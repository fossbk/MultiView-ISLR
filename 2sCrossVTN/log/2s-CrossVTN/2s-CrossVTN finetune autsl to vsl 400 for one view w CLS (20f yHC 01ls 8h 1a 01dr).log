2025-01-18 14:51:02,679 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 1a 01dr)...


2025-01-18 14:51:26,321 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 1a 01dr)...


2025-01-18 14:51:53,993 [INFO] Step[50/2713]: training loss : 6.288181171417237 TRAIN  loss dict:  {'classification_loss': 6.288181171417237}
2025-01-18 14:52:08,996 [INFO] Step[100/2713]: training loss : 6.091262578964233 TRAIN  loss dict:  {'classification_loss': 6.091262578964233}
2025-01-18 14:52:24,065 [INFO] Step[150/2713]: training loss : 6.014632730484009 TRAIN  loss dict:  {'classification_loss': 6.014632730484009}
2025-01-18 14:52:39,107 [INFO] Step[200/2713]: training loss : 5.8932789611816405 TRAIN  loss dict:  {'classification_loss': 5.8932789611816405}
2025-01-18 14:52:54,166 [INFO] Step[250/2713]: training loss : 5.658641500473022 TRAIN  loss dict:  {'classification_loss': 5.658641500473022}
2025-01-18 14:53:09,220 [INFO] Step[300/2713]: training loss : 5.542363996505737 TRAIN  loss dict:  {'classification_loss': 5.542363996505737}
2025-01-18 14:53:24,225 [INFO] Step[350/2713]: training loss : 5.453055181503296 TRAIN  loss dict:  {'classification_loss': 5.453055181503296}
2025-01-18 14:53:39,284 [INFO] Step[400/2713]: training loss : 5.3561778736114505 TRAIN  loss dict:  {'classification_loss': 5.3561778736114505}
2025-01-18 14:53:54,305 [INFO] Step[450/2713]: training loss : 5.183171706199646 TRAIN  loss dict:  {'classification_loss': 5.183171706199646}
2025-01-18 14:54:09,348 [INFO] Step[500/2713]: training loss : 4.992350993156433 TRAIN  loss dict:  {'classification_loss': 4.992350993156433}
2025-01-18 14:54:24,354 [INFO] Step[550/2713]: training loss : 4.864521837234497 TRAIN  loss dict:  {'classification_loss': 4.864521837234497}
2025-01-18 14:54:39,444 [INFO] Step[600/2713]: training loss : 4.723032245635986 TRAIN  loss dict:  {'classification_loss': 4.723032245635986}
2025-01-18 14:54:54,476 [INFO] Step[650/2713]: training loss : 4.621947774887085 TRAIN  loss dict:  {'classification_loss': 4.621947774887085}
2025-01-18 14:55:09,476 [INFO] Step[700/2713]: training loss : 4.590147681236267 TRAIN  loss dict:  {'classification_loss': 4.590147681236267}
2025-01-18 14:55:24,550 [INFO] Step[750/2713]: training loss : 4.287608127593995 TRAIN  loss dict:  {'classification_loss': 4.287608127593995}
2025-01-18 14:55:39,533 [INFO] Step[800/2713]: training loss : 4.411814513206482 TRAIN  loss dict:  {'classification_loss': 4.411814513206482}
2025-01-18 14:55:54,553 [INFO] Step[850/2713]: training loss : 4.267256770133972 TRAIN  loss dict:  {'classification_loss': 4.267256770133972}
2025-01-18 14:56:09,527 [INFO] Step[900/2713]: training loss : 4.125365438461304 TRAIN  loss dict:  {'classification_loss': 4.125365438461304}
2025-01-18 14:56:24,569 [INFO] Step[950/2713]: training loss : 3.901764349937439 TRAIN  loss dict:  {'classification_loss': 3.901764349937439}
2025-01-18 14:56:39,612 [INFO] Step[1000/2713]: training loss : 3.8911973810195923 TRAIN  loss dict:  {'classification_loss': 3.8911973810195923}
2025-01-18 14:56:54,628 [INFO] Step[1050/2713]: training loss : 3.796221766471863 TRAIN  loss dict:  {'classification_loss': 3.796221766471863}
2025-01-18 14:57:09,638 [INFO] Step[1100/2713]: training loss : 3.694493589401245 TRAIN  loss dict:  {'classification_loss': 3.694493589401245}
2025-01-18 14:57:24,754 [INFO] Step[1150/2713]: training loss : 3.437523913383484 TRAIN  loss dict:  {'classification_loss': 3.437523913383484}
2025-01-18 14:57:39,850 [INFO] Step[1200/2713]: training loss : 3.333345856666565 TRAIN  loss dict:  {'classification_loss': 3.333345856666565}
2025-01-18 14:57:54,940 [INFO] Step[1250/2713]: training loss : 3.3546009039878846 TRAIN  loss dict:  {'classification_loss': 3.3546009039878846}
2025-01-18 14:58:09,986 [INFO] Step[1300/2713]: training loss : 3.1853140711784365 TRAIN  loss dict:  {'classification_loss': 3.1853140711784365}
2025-01-18 14:58:25,058 [INFO] Step[1350/2713]: training loss : 3.2261142563819885 TRAIN  loss dict:  {'classification_loss': 3.2261142563819885}
2025-01-18 14:58:40,105 [INFO] Step[1400/2713]: training loss : 3.142151155471802 TRAIN  loss dict:  {'classification_loss': 3.142151155471802}
2025-01-18 14:58:55,161 [INFO] Step[1450/2713]: training loss : 3.2947574758529665 TRAIN  loss dict:  {'classification_loss': 3.2947574758529665}
2025-01-18 14:59:10,202 [INFO] Step[1500/2713]: training loss : 3.075981378555298 TRAIN  loss dict:  {'classification_loss': 3.075981378555298}
2025-01-18 14:59:25,292 [INFO] Step[1550/2713]: training loss : 3.08867479801178 TRAIN  loss dict:  {'classification_loss': 3.08867479801178}
2025-01-18 14:59:40,292 [INFO] Step[1600/2713]: training loss : 2.9753512740135193 TRAIN  loss dict:  {'classification_loss': 2.9753512740135193}
2025-01-18 14:59:55,367 [INFO] Step[1650/2713]: training loss : 2.841924076080322 TRAIN  loss dict:  {'classification_loss': 2.841924076080322}
2025-01-18 15:00:10,414 [INFO] Step[1700/2713]: training loss : 2.762501931190491 TRAIN  loss dict:  {'classification_loss': 2.762501931190491}
2025-01-18 15:00:25,478 [INFO] Step[1750/2713]: training loss : 2.9040610885620115 TRAIN  loss dict:  {'classification_loss': 2.9040610885620115}
2025-01-18 15:00:40,573 [INFO] Step[1800/2713]: training loss : 2.702722864151001 TRAIN  loss dict:  {'classification_loss': 2.702722864151001}
2025-01-18 15:00:55,610 [INFO] Step[1850/2713]: training loss : 2.6671946167945864 TRAIN  loss dict:  {'classification_loss': 2.6671946167945864}
2025-01-18 15:01:10,692 [INFO] Step[1900/2713]: training loss : 2.570720615386963 TRAIN  loss dict:  {'classification_loss': 2.570720615386963}
2025-01-18 15:01:25,778 [INFO] Step[1950/2713]: training loss : 2.592037568092346 TRAIN  loss dict:  {'classification_loss': 2.592037568092346}
2025-01-18 15:01:40,839 [INFO] Step[2000/2713]: training loss : 2.6683932662010195 TRAIN  loss dict:  {'classification_loss': 2.6683932662010195}
2025-01-18 15:01:55,919 [INFO] Step[2050/2713]: training loss : 2.4438446235656737 TRAIN  loss dict:  {'classification_loss': 2.4438446235656737}
2025-01-18 15:02:10,949 [INFO] Step[2100/2713]: training loss : 2.4480758190155028 TRAIN  loss dict:  {'classification_loss': 2.4480758190155028}
2025-01-18 15:02:25,976 [INFO] Step[2150/2713]: training loss : 2.5132748651504517 TRAIN  loss dict:  {'classification_loss': 2.5132748651504517}
2025-01-18 15:02:41,030 [INFO] Step[2200/2713]: training loss : 2.3284109830856323 TRAIN  loss dict:  {'classification_loss': 2.3284109830856323}
2025-01-18 15:02:56,187 [INFO] Step[2250/2713]: training loss : 2.4468339490890503 TRAIN  loss dict:  {'classification_loss': 2.4468339490890503}
2025-01-18 15:03:11,257 [INFO] Step[2300/2713]: training loss : 2.3764333939552307 TRAIN  loss dict:  {'classification_loss': 2.3764333939552307}
2025-01-18 15:03:26,344 [INFO] Step[2350/2713]: training loss : 2.401162507534027 TRAIN  loss dict:  {'classification_loss': 2.401162507534027}
2025-01-18 15:03:41,398 [INFO] Step[2400/2713]: training loss : 2.2888111376762392 TRAIN  loss dict:  {'classification_loss': 2.2888111376762392}
2025-01-18 15:03:56,474 [INFO] Step[2450/2713]: training loss : 2.3984010291099547 TRAIN  loss dict:  {'classification_loss': 2.3984010291099547}
2025-01-18 15:04:11,552 [INFO] Step[2500/2713]: training loss : 2.1798519325256347 TRAIN  loss dict:  {'classification_loss': 2.1798519325256347}
2025-01-18 15:04:26,685 [INFO] Step[2550/2713]: training loss : 2.418891408443451 TRAIN  loss dict:  {'classification_loss': 2.418891408443451}
2025-01-18 15:04:41,783 [INFO] Step[2600/2713]: training loss : 2.167992286682129 TRAIN  loss dict:  {'classification_loss': 2.167992286682129}
2025-01-18 15:04:56,851 [INFO] Step[2650/2713]: training loss : 2.151643340587616 TRAIN  loss dict:  {'classification_loss': 2.151643340587616}
2025-01-18 15:05:11,964 [INFO] Step[2700/2713]: training loss : 2.0514415073394776 TRAIN  loss dict:  {'classification_loss': 2.0514415073394776}
2025-01-18 15:06:32,626 [INFO] Label accuracies statistics:
2025-01-18 15:06:32,626 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.5, 4: 0.25, 5: 0.75, 6: 1.0, 7: 0.75, 8: 0.25, 9: 0.75, 10: 0.75, 11: 1.0, 12: 0.0, 13: 0.75, 14: 0.25, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.5, 25: 0.25, 26: 1.0, 27: 0.25, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.5, 34: 0.75, 35: 0.75, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.25, 42: 1.0, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.25, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.5, 57: 0.75, 58: 0.75, 59: 0.25, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.5, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.25, 89: 0.75, 90: 0.25, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.25, 95: 0.75, 96: 0.25, 97: 0.5, 98: 0.75, 99: 0.8, 100: 1.0, 101: 0.5, 102: 1.0, 103: 0.5, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 0.5, 109: 0.75, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.25, 114: 0.75, 115: 0.75, 116: 1.0, 117: 0.75, 118: 0.75, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.0, 124: 0.25, 125: 0.5, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 0.75, 134: 0.75, 135: 1.0, 136: 0.75, 137: 1.0, 138: 1.0, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 0.75, 144: 0.5, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.75, 158: 1.0, 159: 0.75, 160: 0.25, 161: 0.75, 162: 0.0, 163: 1.0, 164: 0.5, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.25, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 0.75, 178: 0.5, 179: 0.0, 180: 0.5, 181: 0.5, 182: 0.25, 183: 1.0, 184: 0.25, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.0, 190: 1.0, 191: 0.25, 192: 0.5, 193: 1.0, 194: 1.0, 195: 1.0, 196: 0.75, 197: 1.0, 198: 0.75, 199: 0.5, 200: 0.0, 201: 0.5, 202: 0.25, 203: 0.0, 204: 0.0, 205: 0.25, 206: 0.0, 207: 0.25, 208: 0.75, 209: 0.5, 210: 0.5, 211: 0.0, 212: 0.25, 213: 0.0, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.5, 223: 0.5, 224: 0.5, 225: 0.5, 226: 0.0, 227: 0.75, 228: 0.25, 229: 0.0, 230: 0.0, 231: 0.25, 232: 0.25, 233: 0.0, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.0, 238: 0.0, 239: 0.25, 240: 0.75, 241: 0.0, 242: 0.0, 243: 1.0, 244: 0.25, 245: 0.5, 246: 0.75, 247: 0.75, 248: 0.0, 249: 0.0, 250: 1.0, 251: 1.0, 252: 0.5, 253: 0.25, 254: 0.5, 255: 1.0, 256: 0.25, 257: 0.0, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.0, 262: 0.5, 263: 0.0, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.0, 268: 0.0, 269: 0.5, 270: 0.5, 271: 0.5, 272: 0.0, 273: 0.5, 274: 0.0, 275: 0.5, 276: 0.25, 277: 0.25, 278: 1.0, 279: 0.25, 280: 1.0, 281: 0.75, 282: 0.25, 283: 1.0, 284: 0.25, 285: 0.5, 286: 0.0, 287: 1.0, 288: 0.5, 289: 0.25, 290: 0.0, 291: 0.0, 292: 0.75, 293: 0.25, 294: 0.5, 295: 0.25, 296: 0.5, 297: 0.0, 298: 0.75, 299: 0.0, 300: 1.0, 301: 0.5, 302: 0.75, 303: 0.75, 304: 0.5, 305: 0.75, 306: 0.5, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.25, 311: 0.5, 312: 0.5, 313: 0.75, 314: 0.75, 315: 0.0, 316: 0.0, 317: 0.25, 318: 0.0, 319: 0.5, 320: 0.5, 321: 0.75, 322: 0.5, 323: 0.0, 324: 0.0, 325: 0.75, 326: 0.5, 327: 1.0, 328: 1.0, 329: 0.5, 330: 0.25, 331: 0.75, 332: 0.0, 333: 0.0, 334: 0.75, 335: 0.0, 336: 0.75, 337: 0.0, 338: 0.25, 339: 0.5, 340: 0.25, 341: 0.5, 342: 0.75, 343: 0.25, 344: 0.25, 345: 0.0, 346: 0.25, 347: 0.0, 348: 1.0, 349: 0.25, 350: 0.0, 351: 0.5, 352: 1.0, 353: 0.25, 354: 0.0, 355: 0.0, 356: 0.5, 357: 1.0, 358: 0.0, 359: 1.0, 360: 0.75, 361: 0.0, 362: 0.25, 363: 0.25, 364: 0.0, 365: 0.0, 366: 0.5, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.5, 373: 1.0, 374: 0.75, 375: 0.0, 376: 0.0, 377: 0.25, 378: 0.0, 379: 0.25, 380: 1.0, 381: 0.0, 382: 0.5, 383: 0.0, 384: 0.5, 385: 1.0, 386: 0.25, 387: 0.75, 388: 0.5, 389: 1.0, 390: 0.25, 391: 1.0, 392: 0.5, 393: 0.0, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 0.5, 399: 0.75}

2025-01-18 15:06:33,650 [INFO] [1] TRAIN  loss: 3.5876948078575936 acc: 0.43309988942130484
2025-01-18 15:06:33,651 [INFO] [1] TRAIN  loss dict: {'classification_loss': 3.5876948078575936}
2025-01-18 15:06:33,651 [INFO] [1] VALIDATION loss: 2.4693182644091154 VALIDATION acc: 0.5705329153605015
2025-01-18 15:06:33,651 [INFO] [1] VALIDATION loss dict: {'classification_loss': 2.4693182644091154}
2025-01-18 15:06:33,651 [INFO] 
2025-01-18 15:06:54,653 [INFO] Step[50/2713]: training loss : 2.188411686420441 TRAIN  loss dict:  {'classification_loss': 2.188411686420441}
2025-01-18 15:07:09,741 [INFO] Step[100/2713]: training loss : 1.9844570231437684 TRAIN  loss dict:  {'classification_loss': 1.9844570231437684}
2025-01-18 15:07:24,939 [INFO] Step[150/2713]: training loss : 1.8908862566947937 TRAIN  loss dict:  {'classification_loss': 1.8908862566947937}
2025-01-18 15:07:40,097 [INFO] Step[200/2713]: training loss : 1.9814274406433106 TRAIN  loss dict:  {'classification_loss': 1.9814274406433106}
2025-01-18 15:07:55,287 [INFO] Step[250/2713]: training loss : 1.9053155589103699 TRAIN  loss dict:  {'classification_loss': 1.9053155589103699}
2025-01-18 15:08:10,429 [INFO] Step[300/2713]: training loss : 1.9810046434402466 TRAIN  loss dict:  {'classification_loss': 1.9810046434402466}
2025-01-18 15:08:25,569 [INFO] Step[350/2713]: training loss : 1.8588038849830628 TRAIN  loss dict:  {'classification_loss': 1.8588038849830628}
2025-01-18 15:08:40,684 [INFO] Step[400/2713]: training loss : 1.8993964862823487 TRAIN  loss dict:  {'classification_loss': 1.8993964862823487}
2025-01-18 15:08:55,767 [INFO] Step[450/2713]: training loss : 1.9848036527633668 TRAIN  loss dict:  {'classification_loss': 1.9848036527633668}
2025-01-18 15:09:10,907 [INFO] Step[500/2713]: training loss : 1.98299631357193 TRAIN  loss dict:  {'classification_loss': 1.98299631357193}
2025-01-18 15:09:26,053 [INFO] Step[550/2713]: training loss : 1.938837697505951 TRAIN  loss dict:  {'classification_loss': 1.938837697505951}
2025-01-18 15:09:41,217 [INFO] Step[600/2713]: training loss : 1.9363318753242493 TRAIN  loss dict:  {'classification_loss': 1.9363318753242493}
2025-01-18 15:09:56,394 [INFO] Step[650/2713]: training loss : 1.7165294075012207 TRAIN  loss dict:  {'classification_loss': 1.7165294075012207}
2025-01-18 15:10:11,480 [INFO] Step[700/2713]: training loss : 1.9947854781150818 TRAIN  loss dict:  {'classification_loss': 1.9947854781150818}
2025-01-18 15:10:26,593 [INFO] Step[750/2713]: training loss : 1.862520205974579 TRAIN  loss dict:  {'classification_loss': 1.862520205974579}
2025-01-18 15:10:41,770 [INFO] Step[800/2713]: training loss : 1.8508696627616883 TRAIN  loss dict:  {'classification_loss': 1.8508696627616883}
2025-01-18 15:10:56,870 [INFO] Step[850/2713]: training loss : 1.7390504479408264 TRAIN  loss dict:  {'classification_loss': 1.7390504479408264}
2025-01-18 15:11:12,025 [INFO] Step[900/2713]: training loss : 1.7689583563804627 TRAIN  loss dict:  {'classification_loss': 1.7689583563804627}
2025-01-18 15:11:27,199 [INFO] Step[950/2713]: training loss : 1.864529869556427 TRAIN  loss dict:  {'classification_loss': 1.864529869556427}
2025-01-18 15:11:42,328 [INFO] Step[1000/2713]: training loss : 1.7233269500732422 TRAIN  loss dict:  {'classification_loss': 1.7233269500732422}
2025-01-18 15:11:57,512 [INFO] Step[1050/2713]: training loss : 1.9440993404388427 TRAIN  loss dict:  {'classification_loss': 1.9440993404388427}
2025-01-18 15:12:12,674 [INFO] Step[1100/2713]: training loss : 1.8123835301399231 TRAIN  loss dict:  {'classification_loss': 1.8123835301399231}
2025-01-18 15:12:27,848 [INFO] Step[1150/2713]: training loss : 1.9068683338165284 TRAIN  loss dict:  {'classification_loss': 1.9068683338165284}
2025-01-18 15:12:42,983 [INFO] Step[1200/2713]: training loss : 1.8431509971618651 TRAIN  loss dict:  {'classification_loss': 1.8431509971618651}
2025-01-18 15:12:58,042 [INFO] Step[1250/2713]: training loss : 1.8895126056671143 TRAIN  loss dict:  {'classification_loss': 1.8895126056671143}
2025-01-18 15:13:13,215 [INFO] Step[1300/2713]: training loss : 1.80035306930542 TRAIN  loss dict:  {'classification_loss': 1.80035306930542}
2025-01-18 15:13:28,367 [INFO] Step[1350/2713]: training loss : 1.8005638217926025 TRAIN  loss dict:  {'classification_loss': 1.8005638217926025}
2025-01-18 15:13:43,510 [INFO] Step[1400/2713]: training loss : 1.705958318710327 TRAIN  loss dict:  {'classification_loss': 1.705958318710327}
2025-01-18 15:13:58,639 [INFO] Step[1450/2713]: training loss : 1.734548840522766 TRAIN  loss dict:  {'classification_loss': 1.734548840522766}
2025-01-18 15:14:13,811 [INFO] Step[1500/2713]: training loss : 1.7388780522346496 TRAIN  loss dict:  {'classification_loss': 1.7388780522346496}
2025-01-18 15:14:28,991 [INFO] Step[1550/2713]: training loss : 1.7560309410095214 TRAIN  loss dict:  {'classification_loss': 1.7560309410095214}
2025-01-18 15:14:44,133 [INFO] Step[1600/2713]: training loss : 1.7736736631393433 TRAIN  loss dict:  {'classification_loss': 1.7736736631393433}
2025-01-18 15:14:59,269 [INFO] Step[1650/2713]: training loss : 1.6158836698532104 TRAIN  loss dict:  {'classification_loss': 1.6158836698532104}
2025-01-18 15:15:14,428 [INFO] Step[1700/2713]: training loss : 1.655068233013153 TRAIN  loss dict:  {'classification_loss': 1.655068233013153}
2025-01-18 15:15:29,612 [INFO] Step[1750/2713]: training loss : 1.7612048530578612 TRAIN  loss dict:  {'classification_loss': 1.7612048530578612}
2025-01-18 15:15:44,757 [INFO] Step[1800/2713]: training loss : 1.592013657093048 TRAIN  loss dict:  {'classification_loss': 1.592013657093048}
2025-01-18 15:15:59,933 [INFO] Step[1850/2713]: training loss : 1.6281189322471619 TRAIN  loss dict:  {'classification_loss': 1.6281189322471619}
2025-01-18 15:16:15,058 [INFO] Step[1900/2713]: training loss : 1.8287444019317627 TRAIN  loss dict:  {'classification_loss': 1.8287444019317627}
2025-01-18 15:16:30,435 [INFO] Step[1950/2713]: training loss : 1.7121232843399048 TRAIN  loss dict:  {'classification_loss': 1.7121232843399048}
2025-01-18 15:16:45,470 [INFO] Step[2000/2713]: training loss : 1.864101061820984 TRAIN  loss dict:  {'classification_loss': 1.864101061820984}
2025-01-18 15:17:00,586 [INFO] Step[2050/2713]: training loss : 1.8954541993141174 TRAIN  loss dict:  {'classification_loss': 1.8954541993141174}
2025-01-18 15:17:15,702 [INFO] Step[2100/2713]: training loss : 1.6901527285575866 TRAIN  loss dict:  {'classification_loss': 1.6901527285575866}
2025-01-18 15:17:30,754 [INFO] Step[2150/2713]: training loss : 1.7431645607948303 TRAIN  loss dict:  {'classification_loss': 1.7431645607948303}
2025-01-18 15:17:45,862 [INFO] Step[2200/2713]: training loss : 1.877889382839203 TRAIN  loss dict:  {'classification_loss': 1.877889382839203}
2025-01-18 15:18:00,944 [INFO] Step[2250/2713]: training loss : 1.8355026292800902 TRAIN  loss dict:  {'classification_loss': 1.8355026292800902}
2025-01-18 15:18:16,104 [INFO] Step[2300/2713]: training loss : 1.852515697479248 TRAIN  loss dict:  {'classification_loss': 1.852515697479248}
2025-01-18 15:18:31,266 [INFO] Step[2350/2713]: training loss : 1.8005471277236937 TRAIN  loss dict:  {'classification_loss': 1.8005471277236937}
2025-01-18 15:18:46,425 [INFO] Step[2400/2713]: training loss : 1.8243084621429444 TRAIN  loss dict:  {'classification_loss': 1.8243084621429444}
2025-01-18 15:19:01,574 [INFO] Step[2450/2713]: training loss : 1.6952620816230775 TRAIN  loss dict:  {'classification_loss': 1.6952620816230775}
2025-01-18 15:19:16,687 [INFO] Step[2500/2713]: training loss : 1.7059072327613831 TRAIN  loss dict:  {'classification_loss': 1.7059072327613831}
2025-01-18 15:19:31,821 [INFO] Step[2550/2713]: training loss : 1.7373759078979492 TRAIN  loss dict:  {'classification_loss': 1.7373759078979492}
2025-01-18 15:19:46,977 [INFO] Step[2600/2713]: training loss : 1.7093507623672486 TRAIN  loss dict:  {'classification_loss': 1.7093507623672486}
2025-01-18 15:20:02,115 [INFO] Step[2650/2713]: training loss : 1.7991550755500794 TRAIN  loss dict:  {'classification_loss': 1.7991550755500794}
2025-01-18 15:20:17,283 [INFO] Step[2700/2713]: training loss : 1.8296171402931214 TRAIN  loss dict:  {'classification_loss': 1.8296171402931214}
2025-01-18 15:21:37,915 [INFO] Label accuracies statistics:
2025-01-18 15:21:37,915 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.5, 9: 1.0, 10: 0.75, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.25, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.5, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.25, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.5, 56: 1.0, 57: 0.5, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.5, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.25, 69: 0.25, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.5, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.25, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.0, 115: 1.0, 116: 0.75, 117: 0.75, 118: 0.5, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 0.5, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.5, 135: 0.75, 136: 0.75, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.5, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.5, 158: 1.0, 159: 0.25, 160: 0.75, 161: 0.75, 162: 0.25, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.5, 168: 0.5, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.3333333333333333, 180: 0.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.0, 189: 1.0, 190: 0.5, 191: 0.0, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.5, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.25, 203: 0.75, 204: 0.75, 205: 0.5, 206: 0.0, 207: 1.0, 208: 0.75, 209: 0.5, 210: 0.75, 211: 0.5, 212: 0.5, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.0, 218: 0.75, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.0, 224: 0.5, 225: 0.75, 226: 0.25, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.0, 232: 0.5, 233: 0.75, 234: 0.25, 235: 0.5, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.0, 244: 0.25, 245: 0.5, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 0.25, 255: 1.0, 256: 0.25, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.75, 261: 1.0, 262: 0.5, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.5, 278: 0.5, 279: 1.0, 280: 0.5, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.0, 286: 0.25, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.5, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.5, 300: 0.75, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.25, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.0, 317: 0.75, 318: 0.75, 319: 0.5, 320: 0.5, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.75, 328: 0.0, 329: 0.75, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.25, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.25, 338: 1.0, 339: 0.75, 340: 0.25, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.0, 351: 1.0, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.25, 356: 0.25, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.25, 371: 0.0, 372: 1.0, 373: 0.75, 374: 0.25, 375: 1.0, 376: 0.75, 377: 0.0, 378: 0.75, 379: 0.75, 380: 0.5, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.25, 395: 0.5, 396: 0.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 15:21:39,441 [INFO] [2] TRAIN  loss: 1.8229114053612303 acc: 0.7552524880206414
2025-01-18 15:21:39,441 [INFO] [2] TRAIN  loss dict: {'classification_loss': 1.8229114053612303}
2025-01-18 15:21:39,441 [INFO] [2] VALIDATION loss: 2.025097805530505 VALIDATION acc: 0.6877742946708464
2025-01-18 15:21:39,441 [INFO] [2] VALIDATION loss dict: {'classification_loss': 2.025097805530505}
2025-01-18 15:21:39,441 [INFO] 
2025-01-18 15:21:59,892 [INFO] Step[50/2713]: training loss : 1.4609782552719117 TRAIN  loss dict:  {'classification_loss': 1.4609782552719117}
2025-01-18 15:22:14,906 [INFO] Step[100/2713]: training loss : 1.4688401436805725 TRAIN  loss dict:  {'classification_loss': 1.4688401436805725}
2025-01-18 15:22:29,984 [INFO] Step[150/2713]: training loss : 1.6113400149345398 TRAIN  loss dict:  {'classification_loss': 1.6113400149345398}
2025-01-18 15:22:45,014 [INFO] Step[200/2713]: training loss : 1.48244304895401 TRAIN  loss dict:  {'classification_loss': 1.48244304895401}
2025-01-18 15:22:59,996 [INFO] Step[250/2713]: training loss : 1.5216269087791443 TRAIN  loss dict:  {'classification_loss': 1.5216269087791443}
2025-01-18 15:23:15,073 [INFO] Step[300/2713]: training loss : 1.5093352437019347 TRAIN  loss dict:  {'classification_loss': 1.5093352437019347}
2025-01-18 15:23:30,114 [INFO] Step[350/2713]: training loss : 1.5895294809341431 TRAIN  loss dict:  {'classification_loss': 1.5895294809341431}
2025-01-18 15:23:45,169 [INFO] Step[400/2713]: training loss : 1.5762216353416443 TRAIN  loss dict:  {'classification_loss': 1.5762216353416443}
2025-01-18 15:24:00,210 [INFO] Step[450/2713]: training loss : 1.5725036883354186 TRAIN  loss dict:  {'classification_loss': 1.5725036883354186}
2025-01-18 15:24:15,224 [INFO] Step[500/2713]: training loss : 1.5153260850906372 TRAIN  loss dict:  {'classification_loss': 1.5153260850906372}
2025-01-18 15:24:30,161 [INFO] Step[550/2713]: training loss : 1.4201745748519898 TRAIN  loss dict:  {'classification_loss': 1.4201745748519898}
2025-01-18 15:24:45,105 [INFO] Step[600/2713]: training loss : 1.5356980323791505 TRAIN  loss dict:  {'classification_loss': 1.5356980323791505}
2025-01-18 15:25:00,105 [INFO] Step[650/2713]: training loss : 1.621693778038025 TRAIN  loss dict:  {'classification_loss': 1.621693778038025}
2025-01-18 15:25:15,037 [INFO] Step[700/2713]: training loss : 1.660163493156433 TRAIN  loss dict:  {'classification_loss': 1.660163493156433}
2025-01-18 15:25:30,017 [INFO] Step[750/2713]: training loss : 1.6210877394676209 TRAIN  loss dict:  {'classification_loss': 1.6210877394676209}
2025-01-18 15:25:44,934 [INFO] Step[800/2713]: training loss : 1.5665120220184325 TRAIN  loss dict:  {'classification_loss': 1.5665120220184325}
2025-01-18 15:25:59,899 [INFO] Step[850/2713]: training loss : 1.4441475200653076 TRAIN  loss dict:  {'classification_loss': 1.4441475200653076}
2025-01-18 15:26:14,789 [INFO] Step[900/2713]: training loss : 1.6469311904907227 TRAIN  loss dict:  {'classification_loss': 1.6469311904907227}
2025-01-18 15:26:29,767 [INFO] Step[950/2713]: training loss : 1.5578201866149903 TRAIN  loss dict:  {'classification_loss': 1.5578201866149903}
2025-01-18 15:26:44,745 [INFO] Step[1000/2713]: training loss : 1.6666842460632325 TRAIN  loss dict:  {'classification_loss': 1.6666842460632325}
2025-01-18 15:26:59,636 [INFO] Step[1050/2713]: training loss : 1.6324801421165467 TRAIN  loss dict:  {'classification_loss': 1.6324801421165467}
2025-01-18 15:27:14,597 [INFO] Step[1100/2713]: training loss : 1.665480227470398 TRAIN  loss dict:  {'classification_loss': 1.665480227470398}
2025-01-18 15:27:29,615 [INFO] Step[1150/2713]: training loss : 1.5985605192184449 TRAIN  loss dict:  {'classification_loss': 1.5985605192184449}
2025-01-18 15:27:44,609 [INFO] Step[1200/2713]: training loss : 1.6844110894203186 TRAIN  loss dict:  {'classification_loss': 1.6844110894203186}
2025-01-18 15:27:59,532 [INFO] Step[1250/2713]: training loss : 1.5735004687309264 TRAIN  loss dict:  {'classification_loss': 1.5735004687309264}
2025-01-18 15:28:14,524 [INFO] Step[1300/2713]: training loss : 1.6456411361694336 TRAIN  loss dict:  {'classification_loss': 1.6456411361694336}
2025-01-18 15:28:29,473 [INFO] Step[1350/2713]: training loss : 1.4825089526176454 TRAIN  loss dict:  {'classification_loss': 1.4825089526176454}
2025-01-18 15:28:44,440 [INFO] Step[1400/2713]: training loss : 1.7386120057106018 TRAIN  loss dict:  {'classification_loss': 1.7386120057106018}
2025-01-18 15:28:59,351 [INFO] Step[1450/2713]: training loss : 1.4846828961372376 TRAIN  loss dict:  {'classification_loss': 1.4846828961372376}
2025-01-18 15:29:14,412 [INFO] Step[1500/2713]: training loss : 1.7239406085014344 TRAIN  loss dict:  {'classification_loss': 1.7239406085014344}
2025-01-18 15:29:29,400 [INFO] Step[1550/2713]: training loss : 1.517047665119171 TRAIN  loss dict:  {'classification_loss': 1.517047665119171}
2025-01-18 15:29:44,371 [INFO] Step[1600/2713]: training loss : 1.4573004579544067 TRAIN  loss dict:  {'classification_loss': 1.4573004579544067}
2025-01-18 15:29:59,368 [INFO] Step[1650/2713]: training loss : 1.4988581442832947 TRAIN  loss dict:  {'classification_loss': 1.4988581442832947}
2025-01-18 15:30:14,365 [INFO] Step[1700/2713]: training loss : 1.5936793398857116 TRAIN  loss dict:  {'classification_loss': 1.5936793398857116}
2025-01-18 15:30:29,373 [INFO] Step[1750/2713]: training loss : 1.5577663159370423 TRAIN  loss dict:  {'classification_loss': 1.5577663159370423}
2025-01-18 15:30:44,337 [INFO] Step[1800/2713]: training loss : 1.487855350971222 TRAIN  loss dict:  {'classification_loss': 1.487855350971222}
2025-01-18 15:30:59,315 [INFO] Step[1850/2713]: training loss : 1.6135959124565125 TRAIN  loss dict:  {'classification_loss': 1.6135959124565125}
2025-01-18 15:31:14,229 [INFO] Step[1900/2713]: training loss : 1.5699929547309877 TRAIN  loss dict:  {'classification_loss': 1.5699929547309877}
2025-01-18 15:31:29,271 [INFO] Step[1950/2713]: training loss : 1.4907839870452881 TRAIN  loss dict:  {'classification_loss': 1.4907839870452881}
2025-01-18 15:31:44,250 [INFO] Step[2000/2713]: training loss : 1.5794152402877808 TRAIN  loss dict:  {'classification_loss': 1.5794152402877808}
2025-01-18 15:31:59,132 [INFO] Step[2050/2713]: training loss : 1.6495194005966187 TRAIN  loss dict:  {'classification_loss': 1.6495194005966187}
2025-01-18 15:32:14,081 [INFO] Step[2100/2713]: training loss : 1.523429398536682 TRAIN  loss dict:  {'classification_loss': 1.523429398536682}
2025-01-18 15:32:29,014 [INFO] Step[2150/2713]: training loss : 1.5846269702911377 TRAIN  loss dict:  {'classification_loss': 1.5846269702911377}
2025-01-18 15:32:43,997 [INFO] Step[2200/2713]: training loss : 1.5435624885559083 TRAIN  loss dict:  {'classification_loss': 1.5435624885559083}
2025-01-18 15:32:59,101 [INFO] Step[2250/2713]: training loss : 1.463130784034729 TRAIN  loss dict:  {'classification_loss': 1.463130784034729}
2025-01-18 15:33:14,086 [INFO] Step[2300/2713]: training loss : 1.5599875617027283 TRAIN  loss dict:  {'classification_loss': 1.5599875617027283}
2025-01-18 15:33:29,185 [INFO] Step[2350/2713]: training loss : 1.480598247051239 TRAIN  loss dict:  {'classification_loss': 1.480598247051239}
2025-01-18 15:33:44,158 [INFO] Step[2400/2713]: training loss : 1.5319142198562623 TRAIN  loss dict:  {'classification_loss': 1.5319142198562623}
2025-01-18 15:33:59,206 [INFO] Step[2450/2713]: training loss : 1.5166355776786804 TRAIN  loss dict:  {'classification_loss': 1.5166355776786804}
2025-01-18 15:34:14,199 [INFO] Step[2500/2713]: training loss : 1.7176791405677796 TRAIN  loss dict:  {'classification_loss': 1.7176791405677796}
2025-01-18 15:34:29,229 [INFO] Step[2550/2713]: training loss : 1.6260485076904296 TRAIN  loss dict:  {'classification_loss': 1.6260485076904296}
2025-01-18 15:34:44,230 [INFO] Step[2600/2713]: training loss : 1.5849648904800415 TRAIN  loss dict:  {'classification_loss': 1.5849648904800415}
2025-01-18 15:34:59,303 [INFO] Step[2650/2713]: training loss : 1.539229552745819 TRAIN  loss dict:  {'classification_loss': 1.539229552745819}
2025-01-18 15:35:14,317 [INFO] Step[2700/2713]: training loss : 1.5622739148139955 TRAIN  loss dict:  {'classification_loss': 1.5622739148139955}
2025-01-18 15:36:35,173 [INFO] Label accuracies statistics:
2025-01-18 15:36:35,173 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.75, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.25, 19: 0.25, 20: 0.75, 21: 0.5, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.5, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.25, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.5, 42: 1.0, 43: 0.5, 44: 0.5, 45: 0.5, 46: 1.0, 47: 0.75, 48: 0.75, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.5, 56: 0.25, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.5, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.0, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.25, 71: 0.75, 72: 0.5, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.25, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.0, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 0.75, 103: 0.75, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.25, 108: 0.75, 109: 1.0, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.5, 117: 0.75, 118: 0.5, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 0.75, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.25, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.5, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.25, 154: 1.0, 155: 0.75, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.25, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 0.75, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.5, 179: 0.3333333333333333, 180: 0.5, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 1.0, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.25, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.5, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.5, 218: 0.5, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.0, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.0, 230: 1.0, 231: 0.5, 232: 0.25, 233: 0.75, 234: 0.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 0.5, 242: 0.75, 243: 0.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.5, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 0.75, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.25, 262: 0.75, 263: 0.25, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 0.75, 270: 0.5, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.0, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.25, 290: 0.0, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.5, 300: 0.75, 301: 0.75, 302: 0.0, 303: 0.5, 304: 0.0, 305: 0.75, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.25, 316: 0.75, 317: 0.5, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.25, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 0.75, 344: 0.75, 345: 0.25, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.25, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.5, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-18 15:36:36,734 [INFO] [3] TRAIN  loss: 1.5648448837229654 acc: 0.8250399311954786
2025-01-18 15:36:36,734 [INFO] [3] TRAIN  loss dict: {'classification_loss': 1.5648448837229654}
2025-01-18 15:36:36,734 [INFO] [3] VALIDATION loss: 2.0081042481544324 VALIDATION acc: 0.7109717868338558
2025-01-18 15:36:36,735 [INFO] [3] VALIDATION loss dict: {'classification_loss': 2.0081042481544324}
2025-01-18 15:36:36,735 [INFO] 
2025-01-18 15:36:56,961 [INFO] Step[50/2713]: training loss : 1.4808123445510863 TRAIN  loss dict:  {'classification_loss': 1.4808123445510863}
2025-01-18 15:37:12,009 [INFO] Step[100/2713]: training loss : 1.4858470559120178 TRAIN  loss dict:  {'classification_loss': 1.4858470559120178}
2025-01-18 15:37:27,115 [INFO] Step[150/2713]: training loss : 1.3768219828605652 TRAIN  loss dict:  {'classification_loss': 1.3768219828605652}
2025-01-18 15:37:42,225 [INFO] Step[200/2713]: training loss : 1.3608633160591126 TRAIN  loss dict:  {'classification_loss': 1.3608633160591126}
2025-01-18 15:37:57,274 [INFO] Step[250/2713]: training loss : 1.3334908509254455 TRAIN  loss dict:  {'classification_loss': 1.3334908509254455}
2025-01-18 15:38:12,283 [INFO] Step[300/2713]: training loss : 1.51913982629776 TRAIN  loss dict:  {'classification_loss': 1.51913982629776}
2025-01-18 15:38:27,362 [INFO] Step[350/2713]: training loss : 1.4352757740020752 TRAIN  loss dict:  {'classification_loss': 1.4352757740020752}
2025-01-18 15:38:42,414 [INFO] Step[400/2713]: training loss : 1.4469439101219177 TRAIN  loss dict:  {'classification_loss': 1.4469439101219177}
2025-01-18 15:38:57,417 [INFO] Step[450/2713]: training loss : 1.4815501761436463 TRAIN  loss dict:  {'classification_loss': 1.4815501761436463}
2025-01-18 15:39:12,453 [INFO] Step[500/2713]: training loss : 1.4532647943496704 TRAIN  loss dict:  {'classification_loss': 1.4532647943496704}
2025-01-18 15:39:27,538 [INFO] Step[550/2713]: training loss : 1.4575879716873168 TRAIN  loss dict:  {'classification_loss': 1.4575879716873168}
2025-01-18 15:39:42,563 [INFO] Step[600/2713]: training loss : 1.4270247435569763 TRAIN  loss dict:  {'classification_loss': 1.4270247435569763}
2025-01-18 15:39:57,624 [INFO] Step[650/2713]: training loss : 1.4272150993347168 TRAIN  loss dict:  {'classification_loss': 1.4272150993347168}
2025-01-18 15:40:12,718 [INFO] Step[700/2713]: training loss : 1.366061794757843 TRAIN  loss dict:  {'classification_loss': 1.366061794757843}
2025-01-18 15:40:27,847 [INFO] Step[750/2713]: training loss : 1.44251029253006 TRAIN  loss dict:  {'classification_loss': 1.44251029253006}
2025-01-18 15:40:42,949 [INFO] Step[800/2713]: training loss : 1.3676371312141418 TRAIN  loss dict:  {'classification_loss': 1.3676371312141418}
2025-01-18 15:40:58,102 [INFO] Step[850/2713]: training loss : 1.405014865398407 TRAIN  loss dict:  {'classification_loss': 1.405014865398407}
2025-01-18 15:41:13,152 [INFO] Step[900/2713]: training loss : 1.4302497124671936 TRAIN  loss dict:  {'classification_loss': 1.4302497124671936}
2025-01-18 15:41:28,240 [INFO] Step[950/2713]: training loss : 1.3631167435646057 TRAIN  loss dict:  {'classification_loss': 1.3631167435646057}
2025-01-18 15:41:43,323 [INFO] Step[1000/2713]: training loss : 1.353996558189392 TRAIN  loss dict:  {'classification_loss': 1.353996558189392}
2025-01-18 15:41:58,448 [INFO] Step[1050/2713]: training loss : 1.4924150490760804 TRAIN  loss dict:  {'classification_loss': 1.4924150490760804}
2025-01-18 15:42:13,583 [INFO] Step[1100/2713]: training loss : 1.4538698077201844 TRAIN  loss dict:  {'classification_loss': 1.4538698077201844}
2025-01-18 15:42:28,685 [INFO] Step[1150/2713]: training loss : 1.5521729230880736 TRAIN  loss dict:  {'classification_loss': 1.5521729230880736}
2025-01-18 15:42:43,817 [INFO] Step[1200/2713]: training loss : 1.47294908285141 TRAIN  loss dict:  {'classification_loss': 1.47294908285141}
2025-01-18 15:42:58,910 [INFO] Step[1250/2713]: training loss : 1.3764531302452088 TRAIN  loss dict:  {'classification_loss': 1.3764531302452088}
2025-01-18 15:43:14,041 [INFO] Step[1300/2713]: training loss : 1.458021686077118 TRAIN  loss dict:  {'classification_loss': 1.458021686077118}
2025-01-18 15:43:29,102 [INFO] Step[1350/2713]: training loss : 1.381595377922058 TRAIN  loss dict:  {'classification_loss': 1.381595377922058}
2025-01-18 15:43:44,223 [INFO] Step[1400/2713]: training loss : 1.4891374897956848 TRAIN  loss dict:  {'classification_loss': 1.4891374897956848}
2025-01-18 15:43:59,286 [INFO] Step[1450/2713]: training loss : 1.403002700805664 TRAIN  loss dict:  {'classification_loss': 1.403002700805664}
2025-01-18 15:44:14,301 [INFO] Step[1500/2713]: training loss : 1.4014344310760498 TRAIN  loss dict:  {'classification_loss': 1.4014344310760498}
2025-01-18 15:44:29,394 [INFO] Step[1550/2713]: training loss : 1.4610175132751464 TRAIN  loss dict:  {'classification_loss': 1.4610175132751464}
2025-01-18 15:44:44,556 [INFO] Step[1600/2713]: training loss : 1.4981208372116088 TRAIN  loss dict:  {'classification_loss': 1.4981208372116088}
2025-01-18 15:44:59,763 [INFO] Step[1650/2713]: training loss : 1.4501570415496827 TRAIN  loss dict:  {'classification_loss': 1.4501570415496827}
2025-01-18 15:45:14,895 [INFO] Step[1700/2713]: training loss : 1.3570249772071838 TRAIN  loss dict:  {'classification_loss': 1.3570249772071838}
2025-01-18 15:45:30,030 [INFO] Step[1750/2713]: training loss : 1.4044262099266052 TRAIN  loss dict:  {'classification_loss': 1.4044262099266052}
2025-01-18 15:45:45,196 [INFO] Step[1800/2713]: training loss : 1.6170941019058227 TRAIN  loss dict:  {'classification_loss': 1.6170941019058227}
2025-01-18 15:46:00,325 [INFO] Step[1850/2713]: training loss : 1.4616912579536439 TRAIN  loss dict:  {'classification_loss': 1.4616912579536439}
2025-01-18 15:46:15,395 [INFO] Step[1900/2713]: training loss : 1.5023157024383544 TRAIN  loss dict:  {'classification_loss': 1.5023157024383544}
2025-01-18 15:46:30,456 [INFO] Step[1950/2713]: training loss : 1.471281614303589 TRAIN  loss dict:  {'classification_loss': 1.471281614303589}
2025-01-18 15:46:45,605 [INFO] Step[2000/2713]: training loss : 1.5043535804748536 TRAIN  loss dict:  {'classification_loss': 1.5043535804748536}
2025-01-18 15:47:00,746 [INFO] Step[2050/2713]: training loss : 1.433724889755249 TRAIN  loss dict:  {'classification_loss': 1.433724889755249}
2025-01-18 15:47:15,919 [INFO] Step[2100/2713]: training loss : 1.3859472608566283 TRAIN  loss dict:  {'classification_loss': 1.3859472608566283}
2025-01-18 15:47:31,028 [INFO] Step[2150/2713]: training loss : 1.3870839667320252 TRAIN  loss dict:  {'classification_loss': 1.3870839667320252}
2025-01-18 15:47:46,174 [INFO] Step[2200/2713]: training loss : 1.475713505744934 TRAIN  loss dict:  {'classification_loss': 1.475713505744934}
2025-01-18 15:48:01,357 [INFO] Step[2250/2713]: training loss : 1.3825845742225646 TRAIN  loss dict:  {'classification_loss': 1.3825845742225646}
2025-01-18 15:48:16,531 [INFO] Step[2300/2713]: training loss : 1.5038700747489928 TRAIN  loss dict:  {'classification_loss': 1.5038700747489928}
2025-01-18 15:48:31,689 [INFO] Step[2350/2713]: training loss : 1.4142843794822693 TRAIN  loss dict:  {'classification_loss': 1.4142843794822693}
2025-01-18 15:48:46,799 [INFO] Step[2400/2713]: training loss : 1.4046888256072998 TRAIN  loss dict:  {'classification_loss': 1.4046888256072998}
2025-01-18 15:49:01,908 [INFO] Step[2450/2713]: training loss : 1.4820332193374635 TRAIN  loss dict:  {'classification_loss': 1.4820332193374635}
2025-01-18 15:49:17,014 [INFO] Step[2500/2713]: training loss : 1.5328568196296692 TRAIN  loss dict:  {'classification_loss': 1.5328568196296692}
2025-01-18 15:49:32,091 [INFO] Step[2550/2713]: training loss : 1.4933010601997376 TRAIN  loss dict:  {'classification_loss': 1.4933010601997376}
2025-01-18 15:49:47,218 [INFO] Step[2600/2713]: training loss : 1.43728848695755 TRAIN  loss dict:  {'classification_loss': 1.43728848695755}
2025-01-18 15:50:02,363 [INFO] Step[2650/2713]: training loss : 1.4143746328353881 TRAIN  loss dict:  {'classification_loss': 1.4143746328353881}
2025-01-18 15:50:17,447 [INFO] Step[2700/2713]: training loss : 1.4054170989990233 TRAIN  loss dict:  {'classification_loss': 1.4054170989990233}
2025-01-18 15:51:37,981 [INFO] Label accuracies statistics:
2025-01-18 15:51:37,981 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.5, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 0.5, 10: 0.75, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.5, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.75, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.25, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.75, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 0.75, 111: 0.75, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 0.75, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.75, 142: 0.5, 143: 0.75, 144: 1.0, 145: 0.5, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.5, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.5, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.6666666666666666, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.25, 186: 1.0, 187: 1.0, 188: 0.0, 189: 1.0, 190: 1.0, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.25, 209: 1.0, 210: 1.0, 211: 0.0, 212: 1.0, 213: 0.0, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.25, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.0, 230: 1.0, 231: 0.0, 232: 0.5, 233: 0.75, 234: 0.25, 235: 0.25, 236: 1.0, 237: 1.0, 238: 0.75, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.25, 243: 0.5, 244: 1.0, 245: 1.0, 246: 0.75, 247: 0.75, 248: 0.6666666666666666, 249: 0.5, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.5, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.25, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.25, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.25, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 0.75, 330: 0.5, 331: 0.75, 332: 0.25, 333: 1.0, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.25, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.25, 345: 0.25, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.0, 350: 0.25, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.0, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.25, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.25, 378: 0.25, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.5, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.0, 390: 0.25, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.25, 395: 0.5, 396: 0.25, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 15:51:39,381 [INFO] [4] TRAIN  loss: 1.4395691653788418 acc: 0.8671827005774665
2025-01-18 15:51:39,381 [INFO] [4] TRAIN  loss dict: {'classification_loss': 1.4395691653788418}
2025-01-18 15:51:39,381 [INFO] [4] VALIDATION loss: 1.9469868186721229 VALIDATION acc: 0.7360501567398119
2025-01-18 15:51:39,381 [INFO] [4] VALIDATION loss dict: {'classification_loss': 1.9469868186721229}
2025-01-18 15:51:39,381 [INFO] 
2025-01-18 15:51:59,318 [INFO] Step[50/2713]: training loss : 1.3272087478637695 TRAIN  loss dict:  {'classification_loss': 1.3272087478637695}
2025-01-18 15:52:14,433 [INFO] Step[100/2713]: training loss : 1.2865897607803345 TRAIN  loss dict:  {'classification_loss': 1.2865897607803345}
2025-01-18 15:52:29,604 [INFO] Step[150/2713]: training loss : 1.3077823638916015 TRAIN  loss dict:  {'classification_loss': 1.3077823638916015}
2025-01-18 15:52:44,745 [INFO] Step[200/2713]: training loss : 1.248333625793457 TRAIN  loss dict:  {'classification_loss': 1.248333625793457}
2025-01-18 15:52:59,921 [INFO] Step[250/2713]: training loss : 1.301338541507721 TRAIN  loss dict:  {'classification_loss': 1.301338541507721}
2025-01-18 15:53:15,068 [INFO] Step[300/2713]: training loss : 1.3127433252334595 TRAIN  loss dict:  {'classification_loss': 1.3127433252334595}
2025-01-18 15:53:30,251 [INFO] Step[350/2713]: training loss : 1.3249790716171264 TRAIN  loss dict:  {'classification_loss': 1.3249790716171264}
2025-01-18 15:53:45,452 [INFO] Step[400/2713]: training loss : 1.3450532627105714 TRAIN  loss dict:  {'classification_loss': 1.3450532627105714}
2025-01-18 15:54:00,659 [INFO] Step[450/2713]: training loss : 1.365666139125824 TRAIN  loss dict:  {'classification_loss': 1.365666139125824}
2025-01-18 15:54:15,811 [INFO] Step[500/2713]: training loss : 1.2993337440490722 TRAIN  loss dict:  {'classification_loss': 1.2993337440490722}
2025-01-18 15:54:30,963 [INFO] Step[550/2713]: training loss : 1.3078203868865967 TRAIN  loss dict:  {'classification_loss': 1.3078203868865967}
2025-01-18 15:54:46,143 [INFO] Step[600/2713]: training loss : 1.342166018486023 TRAIN  loss dict:  {'classification_loss': 1.342166018486023}
2025-01-18 15:55:01,302 [INFO] Step[650/2713]: training loss : 1.3461609697341919 TRAIN  loss dict:  {'classification_loss': 1.3461609697341919}
2025-01-18 15:55:16,489 [INFO] Step[700/2713]: training loss : 1.3786909461021424 TRAIN  loss dict:  {'classification_loss': 1.3786909461021424}
2025-01-18 15:55:31,662 [INFO] Step[750/2713]: training loss : 1.3405953574180602 TRAIN  loss dict:  {'classification_loss': 1.3405953574180602}
2025-01-18 15:55:46,847 [INFO] Step[800/2713]: training loss : 1.3521060943603516 TRAIN  loss dict:  {'classification_loss': 1.3521060943603516}
2025-01-18 15:56:02,047 [INFO] Step[850/2713]: training loss : 1.337996187210083 TRAIN  loss dict:  {'classification_loss': 1.337996187210083}
2025-01-18 15:56:17,242 [INFO] Step[900/2713]: training loss : 1.3518341708183288 TRAIN  loss dict:  {'classification_loss': 1.3518341708183288}
2025-01-18 15:56:32,416 [INFO] Step[950/2713]: training loss : 1.4116555261611938 TRAIN  loss dict:  {'classification_loss': 1.4116555261611938}
2025-01-18 15:56:47,620 [INFO] Step[1000/2713]: training loss : 1.3968317437171935 TRAIN  loss dict:  {'classification_loss': 1.3968317437171935}
2025-01-18 15:57:02,787 [INFO] Step[1050/2713]: training loss : 1.3777221179008483 TRAIN  loss dict:  {'classification_loss': 1.3777221179008483}
2025-01-18 15:57:17,981 [INFO] Step[1100/2713]: training loss : 1.4872428607940673 TRAIN  loss dict:  {'classification_loss': 1.4872428607940673}
2025-01-18 15:57:33,195 [INFO] Step[1150/2713]: training loss : 1.3704803800582885 TRAIN  loss dict:  {'classification_loss': 1.3704803800582885}
2025-01-18 15:57:48,398 [INFO] Step[1200/2713]: training loss : 1.3558185124397277 TRAIN  loss dict:  {'classification_loss': 1.3558185124397277}
2025-01-18 15:58:03,579 [INFO] Step[1250/2713]: training loss : 1.3427024102210998 TRAIN  loss dict:  {'classification_loss': 1.3427024102210998}
2025-01-18 15:58:18,756 [INFO] Step[1300/2713]: training loss : 1.3985948944091797 TRAIN  loss dict:  {'classification_loss': 1.3985948944091797}
2025-01-18 15:58:33,944 [INFO] Step[1350/2713]: training loss : 1.3794010400772094 TRAIN  loss dict:  {'classification_loss': 1.3794010400772094}
2025-01-18 15:58:49,102 [INFO] Step[1400/2713]: training loss : 1.3026037168502809 TRAIN  loss dict:  {'classification_loss': 1.3026037168502809}
2025-01-18 15:59:04,280 [INFO] Step[1450/2713]: training loss : 1.3778705048561095 TRAIN  loss dict:  {'classification_loss': 1.3778705048561095}
2025-01-18 15:59:19,461 [INFO] Step[1500/2713]: training loss : 1.3348967242240906 TRAIN  loss dict:  {'classification_loss': 1.3348967242240906}
2025-01-18 15:59:34,658 [INFO] Step[1550/2713]: training loss : 1.4517602396011353 TRAIN  loss dict:  {'classification_loss': 1.4517602396011353}
2025-01-18 15:59:49,868 [INFO] Step[1600/2713]: training loss : 1.3595182299613953 TRAIN  loss dict:  {'classification_loss': 1.3595182299613953}
2025-01-18 16:00:05,028 [INFO] Step[1650/2713]: training loss : 1.4061396956443786 TRAIN  loss dict:  {'classification_loss': 1.4061396956443786}
2025-01-18 16:00:20,220 [INFO] Step[1700/2713]: training loss : 1.3018358755111694 TRAIN  loss dict:  {'classification_loss': 1.3018358755111694}
2025-01-18 16:00:35,449 [INFO] Step[1750/2713]: training loss : 1.3752577662467957 TRAIN  loss dict:  {'classification_loss': 1.3752577662467957}
2025-01-18 16:00:50,643 [INFO] Step[1800/2713]: training loss : 1.383092713356018 TRAIN  loss dict:  {'classification_loss': 1.383092713356018}
2025-01-18 16:01:05,858 [INFO] Step[1850/2713]: training loss : 1.4658706021308898 TRAIN  loss dict:  {'classification_loss': 1.4658706021308898}
2025-01-18 16:01:21,001 [INFO] Step[1900/2713]: training loss : 1.3784145426750183 TRAIN  loss dict:  {'classification_loss': 1.3784145426750183}
2025-01-18 16:01:36,219 [INFO] Step[1950/2713]: training loss : 1.3612195754051208 TRAIN  loss dict:  {'classification_loss': 1.3612195754051208}
2025-01-18 16:01:51,618 [INFO] Step[2000/2713]: training loss : 1.3354216980934144 TRAIN  loss dict:  {'classification_loss': 1.3354216980934144}
2025-01-18 16:02:06,904 [INFO] Step[2050/2713]: training loss : 1.390347456932068 TRAIN  loss dict:  {'classification_loss': 1.390347456932068}
2025-01-18 16:02:21,979 [INFO] Step[2100/2713]: training loss : 1.4331266272068024 TRAIN  loss dict:  {'classification_loss': 1.4331266272068024}
2025-01-18 16:02:37,005 [INFO] Step[2150/2713]: training loss : 1.396698353290558 TRAIN  loss dict:  {'classification_loss': 1.396698353290558}
2025-01-18 16:02:52,091 [INFO] Step[2200/2713]: training loss : 1.4368182229995727 TRAIN  loss dict:  {'classification_loss': 1.4368182229995727}
2025-01-18 16:03:07,178 [INFO] Step[2250/2713]: training loss : 1.4501914072036743 TRAIN  loss dict:  {'classification_loss': 1.4501914072036743}
2025-01-18 16:03:22,257 [INFO] Step[2300/2713]: training loss : 1.550867063999176 TRAIN  loss dict:  {'classification_loss': 1.550867063999176}
2025-01-18 16:03:37,334 [INFO] Step[2350/2713]: training loss : 1.4120705652236938 TRAIN  loss dict:  {'classification_loss': 1.4120705652236938}
2025-01-18 16:03:52,487 [INFO] Step[2400/2713]: training loss : 1.461266782283783 TRAIN  loss dict:  {'classification_loss': 1.461266782283783}
2025-01-18 16:04:07,554 [INFO] Step[2450/2713]: training loss : 1.430544126033783 TRAIN  loss dict:  {'classification_loss': 1.430544126033783}
2025-01-18 16:04:22,631 [INFO] Step[2500/2713]: training loss : 1.3230544352531433 TRAIN  loss dict:  {'classification_loss': 1.3230544352531433}
2025-01-18 16:04:37,691 [INFO] Step[2550/2713]: training loss : 1.333302571773529 TRAIN  loss dict:  {'classification_loss': 1.333302571773529}
2025-01-18 16:04:52,758 [INFO] Step[2600/2713]: training loss : 1.3796441912651063 TRAIN  loss dict:  {'classification_loss': 1.3796441912651063}
2025-01-18 16:05:07,871 [INFO] Step[2650/2713]: training loss : 1.3106199240684508 TRAIN  loss dict:  {'classification_loss': 1.3106199240684508}
2025-01-18 16:05:22,920 [INFO] Step[2700/2713]: training loss : 1.4066329765319825 TRAIN  loss dict:  {'classification_loss': 1.4066329765319825}
2025-01-18 16:06:44,075 [INFO] Label accuracies statistics:
2025-01-18 16:06:44,076 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.5, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.5, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 0.5, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.5, 46: 1.0, 47: 0.25, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.25, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.5, 65: 1.0, 66: 0.25, 67: 0.5, 68: 0.75, 69: 0.5, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.5, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.75, 98: 1.0, 99: 0.8, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.5, 124: 0.75, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.5, 146: 0.75, 147: 0.75, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.5, 162: 0.5, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.5, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.5, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.5, 205: 0.5, 206: 0.75, 207: 0.5, 208: 1.0, 209: 0.5, 210: 0.5, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.25, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.5, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.0, 240: 0.75, 241: 0.5, 242: 0.75, 243: 0.75, 244: 0.5, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.3333333333333333, 249: 0.5, 250: 0.75, 251: 0.75, 252: 0.5, 253: 0.5, 254: 0.75, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.25, 260: 0.25, 261: 1.0, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.5, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.5, 277: 0.75, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.25, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.5, 288: 0.75, 289: 0.75, 290: 0.0, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.0, 297: 0.5, 298: 0.75, 299: 0.5, 300: 0.75, 301: 1.0, 302: 0.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.5, 307: 1.0, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.5, 314: 0.75, 315: 0.5, 316: 0.25, 317: 0.5, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.5, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.5, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.5, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.25, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.0, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.5, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.25, 371: 0.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.25, 377: 0.25, 378: 0.75, 379: 0.5, 380: 0.75, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.25, 396: 0.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 16:06:44,078 [INFO] [5] TRAIN  loss: 1.369131869927922 acc: 0.8896670352623173
2025-01-18 16:06:44,078 [INFO] [5] TRAIN  loss dict: {'classification_loss': 1.369131869927922}
2025-01-18 16:06:44,078 [INFO] [5] VALIDATION loss: 2.0453214959094397 VALIDATION acc: 0.7059561128526646
2025-01-18 16:06:44,078 [INFO] [5] VALIDATION loss dict: {'classification_loss': 2.0453214959094397}
2025-01-18 16:06:44,078 [INFO] 
2025-01-18 16:07:04,958 [INFO] Step[50/2713]: training loss : 1.251940779685974 TRAIN  loss dict:  {'classification_loss': 1.251940779685974}
2025-01-18 16:07:20,056 [INFO] Step[100/2713]: training loss : 1.3793677616119384 TRAIN  loss dict:  {'classification_loss': 1.3793677616119384}
2025-01-18 16:07:35,202 [INFO] Step[150/2713]: training loss : 1.2844117999076843 TRAIN  loss dict:  {'classification_loss': 1.2844117999076843}
2025-01-18 16:07:50,331 [INFO] Step[200/2713]: training loss : 1.3404312896728516 TRAIN  loss dict:  {'classification_loss': 1.3404312896728516}
2025-01-18 16:08:05,444 [INFO] Step[250/2713]: training loss : 1.2660209822654724 TRAIN  loss dict:  {'classification_loss': 1.2660209822654724}
2025-01-18 16:08:20,540 [INFO] Step[300/2713]: training loss : 1.2883817195892333 TRAIN  loss dict:  {'classification_loss': 1.2883817195892333}
2025-01-18 16:08:35,660 [INFO] Step[350/2713]: training loss : 1.2416611814498901 TRAIN  loss dict:  {'classification_loss': 1.2416611814498901}
2025-01-18 16:08:50,726 [INFO] Step[400/2713]: training loss : 1.2764593744277954 TRAIN  loss dict:  {'classification_loss': 1.2764593744277954}
2025-01-18 16:09:05,793 [INFO] Step[450/2713]: training loss : 1.2370029044151307 TRAIN  loss dict:  {'classification_loss': 1.2370029044151307}
2025-01-18 16:09:20,880 [INFO] Step[500/2713]: training loss : 1.4209804487228395 TRAIN  loss dict:  {'classification_loss': 1.4209804487228395}
2025-01-18 16:09:35,977 [INFO] Step[550/2713]: training loss : 1.2727256178855897 TRAIN  loss dict:  {'classification_loss': 1.2727256178855897}
2025-01-18 16:09:51,061 [INFO] Step[600/2713]: training loss : 1.329491081237793 TRAIN  loss dict:  {'classification_loss': 1.329491081237793}
2025-01-18 16:10:06,118 [INFO] Step[650/2713]: training loss : 1.302215988636017 TRAIN  loss dict:  {'classification_loss': 1.302215988636017}
2025-01-18 16:10:21,174 [INFO] Step[700/2713]: training loss : 1.2913713622093201 TRAIN  loss dict:  {'classification_loss': 1.2913713622093201}
2025-01-18 16:10:36,260 [INFO] Step[750/2713]: training loss : 1.2131530141830444 TRAIN  loss dict:  {'classification_loss': 1.2131530141830444}
2025-01-18 16:10:51,382 [INFO] Step[800/2713]: training loss : 1.2588581013679505 TRAIN  loss dict:  {'classification_loss': 1.2588581013679505}
2025-01-18 16:11:06,462 [INFO] Step[850/2713]: training loss : 1.2642086124420167 TRAIN  loss dict:  {'classification_loss': 1.2642086124420167}
2025-01-18 16:11:21,549 [INFO] Step[900/2713]: training loss : 1.3187045860290527 TRAIN  loss dict:  {'classification_loss': 1.3187045860290527}
2025-01-18 16:11:36,636 [INFO] Step[950/2713]: training loss : 1.3606513667106628 TRAIN  loss dict:  {'classification_loss': 1.3606513667106628}
2025-01-18 16:11:51,680 [INFO] Step[1000/2713]: training loss : 1.2893169665336608 TRAIN  loss dict:  {'classification_loss': 1.2893169665336608}
2025-01-18 16:12:06,877 [INFO] Step[1050/2713]: training loss : 1.2966486406326294 TRAIN  loss dict:  {'classification_loss': 1.2966486406326294}
2025-01-18 16:12:22,046 [INFO] Step[1100/2713]: training loss : 1.3211471509933472 TRAIN  loss dict:  {'classification_loss': 1.3211471509933472}
2025-01-18 16:12:37,127 [INFO] Step[1150/2713]: training loss : 1.4179705715179443 TRAIN  loss dict:  {'classification_loss': 1.4179705715179443}
2025-01-18 16:12:52,228 [INFO] Step[1200/2713]: training loss : 1.2255111885070802 TRAIN  loss dict:  {'classification_loss': 1.2255111885070802}
2025-01-18 16:13:07,309 [INFO] Step[1250/2713]: training loss : 1.3421099662780762 TRAIN  loss dict:  {'classification_loss': 1.3421099662780762}
2025-01-18 16:13:22,339 [INFO] Step[1300/2713]: training loss : 1.3022616815567016 TRAIN  loss dict:  {'classification_loss': 1.3022616815567016}
2025-01-18 16:13:37,366 [INFO] Step[1350/2713]: training loss : 1.2572588896751404 TRAIN  loss dict:  {'classification_loss': 1.2572588896751404}
2025-01-18 16:13:52,355 [INFO] Step[1400/2713]: training loss : 1.2947874927520753 TRAIN  loss dict:  {'classification_loss': 1.2947874927520753}
2025-01-18 16:14:07,426 [INFO] Step[1450/2713]: training loss : 1.3210426044464112 TRAIN  loss dict:  {'classification_loss': 1.3210426044464112}
2025-01-18 16:14:22,417 [INFO] Step[1500/2713]: training loss : 1.4448937225341796 TRAIN  loss dict:  {'classification_loss': 1.4448937225341796}
2025-01-18 16:14:37,438 [INFO] Step[1550/2713]: training loss : 1.2849884557723998 TRAIN  loss dict:  {'classification_loss': 1.2849884557723998}
2025-01-18 16:14:52,510 [INFO] Step[1600/2713]: training loss : 1.3443300080299379 TRAIN  loss dict:  {'classification_loss': 1.3443300080299379}
2025-01-18 16:15:07,545 [INFO] Step[1650/2713]: training loss : 1.3200164151191711 TRAIN  loss dict:  {'classification_loss': 1.3200164151191711}
2025-01-18 16:15:22,630 [INFO] Step[1700/2713]: training loss : 1.3241896295547486 TRAIN  loss dict:  {'classification_loss': 1.3241896295547486}
2025-01-18 16:15:37,706 [INFO] Step[1750/2713]: training loss : 1.26656898021698 TRAIN  loss dict:  {'classification_loss': 1.26656898021698}
2025-01-18 16:15:52,728 [INFO] Step[1800/2713]: training loss : 1.3540453696250916 TRAIN  loss dict:  {'classification_loss': 1.3540453696250916}
2025-01-18 16:16:07,803 [INFO] Step[1850/2713]: training loss : 1.2454025316238404 TRAIN  loss dict:  {'classification_loss': 1.2454025316238404}
2025-01-18 16:16:22,798 [INFO] Step[1900/2713]: training loss : 1.3695316576957703 TRAIN  loss dict:  {'classification_loss': 1.3695316576957703}
2025-01-18 16:16:37,815 [INFO] Step[1950/2713]: training loss : 1.2730019187927246 TRAIN  loss dict:  {'classification_loss': 1.2730019187927246}
2025-01-18 16:16:52,820 [INFO] Step[2000/2713]: training loss : 1.2222306251525878 TRAIN  loss dict:  {'classification_loss': 1.2222306251525878}
2025-01-18 16:17:07,902 [INFO] Step[2050/2713]: training loss : 1.2958661127090454 TRAIN  loss dict:  {'classification_loss': 1.2958661127090454}
2025-01-18 16:17:22,954 [INFO] Step[2100/2713]: training loss : 1.4472697973251343 TRAIN  loss dict:  {'classification_loss': 1.4472697973251343}
2025-01-18 16:17:37,975 [INFO] Step[2150/2713]: training loss : 1.3471146178245545 TRAIN  loss dict:  {'classification_loss': 1.3471146178245545}
2025-01-18 16:17:52,945 [INFO] Step[2200/2713]: training loss : 1.3029247832298279 TRAIN  loss dict:  {'classification_loss': 1.3029247832298279}
2025-01-18 16:18:08,022 [INFO] Step[2250/2713]: training loss : 1.3009643101692199 TRAIN  loss dict:  {'classification_loss': 1.3009643101692199}
2025-01-18 16:18:23,060 [INFO] Step[2300/2713]: training loss : 1.252417345046997 TRAIN  loss dict:  {'classification_loss': 1.252417345046997}
2025-01-18 16:18:38,131 [INFO] Step[2350/2713]: training loss : 1.2613505125045776 TRAIN  loss dict:  {'classification_loss': 1.2613505125045776}
2025-01-18 16:18:53,142 [INFO] Step[2400/2713]: training loss : 1.3902634739875794 TRAIN  loss dict:  {'classification_loss': 1.3902634739875794}
2025-01-18 16:19:08,165 [INFO] Step[2450/2713]: training loss : 1.3192136573791504 TRAIN  loss dict:  {'classification_loss': 1.3192136573791504}
2025-01-18 16:19:23,240 [INFO] Step[2500/2713]: training loss : 1.3231768941879272 TRAIN  loss dict:  {'classification_loss': 1.3231768941879272}
2025-01-18 16:19:38,272 [INFO] Step[2550/2713]: training loss : 1.2935992407798766 TRAIN  loss dict:  {'classification_loss': 1.2935992407798766}
2025-01-18 16:19:53,411 [INFO] Step[2600/2713]: training loss : 1.3226382637023926 TRAIN  loss dict:  {'classification_loss': 1.3226382637023926}
2025-01-18 16:20:08,491 [INFO] Step[2650/2713]: training loss : 1.3797769355773926 TRAIN  loss dict:  {'classification_loss': 1.3797769355773926}
2025-01-18 16:20:23,574 [INFO] Step[2700/2713]: training loss : 1.256944260597229 TRAIN  loss dict:  {'classification_loss': 1.256944260597229}
2025-01-18 16:21:44,616 [INFO] Label accuracies statistics:
2025-01-18 16:21:44,616 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.5, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 0.75, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.25, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.5, 44: 0.25, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.5, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.5, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.25, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.75, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.5, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 0.5, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.25, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.5, 142: 1.0, 143: 0.5, 144: 1.0, 145: 0.5, 146: 0.75, 147: 0.75, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.25, 189: 0.75, 190: 0.75, 191: 0.25, 192: 0.75, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.5, 210: 1.0, 211: 0.0, 212: 0.5, 213: 0.0, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.25, 218: 1.0, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.75, 229: 0.0, 230: 1.0, 231: 0.25, 232: 0.5, 233: 0.5, 234: 0.5, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 0.5, 241: 1.0, 242: 0.75, 243: 1.0, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.25, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.25, 274: 0.25, 275: 0.75, 276: 0.5, 277: 0.75, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.5, 285: 0.5, 286: 1.0, 287: 0.5, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 0.75, 293: 0.5, 294: 0.75, 295: 0.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.25, 305: 0.75, 306: 0.75, 307: 1.0, 308: 0.75, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.25, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.5, 324: 0.5, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.5, 333: 0.75, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.5, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.5, 363: 0.5, 364: 0.75, 365: 1.0, 366: 0.75, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.0, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.25, 376: 0.5, 377: 0.5, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.0, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-18 16:21:44,618 [INFO] [6] TRAIN  loss: 1.3073589774106114 acc: 0.9120285047303108
2025-01-18 16:21:44,618 [INFO] [6] TRAIN  loss dict: {'classification_loss': 1.3073589774106114}
2025-01-18 16:21:44,618 [INFO] [6] VALIDATION loss: 1.9881496622150105 VALIDATION acc: 0.7197492163009405
2025-01-18 16:21:44,618 [INFO] [6] VALIDATION loss dict: {'classification_loss': 1.9881496622150105}
2025-01-18 16:21:44,618 [INFO] 
2025-01-18 16:22:04,976 [INFO] Step[50/2713]: training loss : 1.3002740669250488 TRAIN  loss dict:  {'classification_loss': 1.3002740669250488}
2025-01-18 16:22:20,023 [INFO] Step[100/2713]: training loss : 1.1837249851226808 TRAIN  loss dict:  {'classification_loss': 1.1837249851226808}
2025-01-18 16:22:35,115 [INFO] Step[150/2713]: training loss : 1.2330742955207825 TRAIN  loss dict:  {'classification_loss': 1.2330742955207825}
2025-01-18 16:22:50,148 [INFO] Step[200/2713]: training loss : 1.2526603770256042 TRAIN  loss dict:  {'classification_loss': 1.2526603770256042}
2025-01-18 16:23:05,240 [INFO] Step[250/2713]: training loss : 1.1990436172485353 TRAIN  loss dict:  {'classification_loss': 1.1990436172485353}
2025-01-18 16:23:20,280 [INFO] Step[300/2713]: training loss : 1.2409989643096924 TRAIN  loss dict:  {'classification_loss': 1.2409989643096924}
2025-01-18 16:23:35,285 [INFO] Step[350/2713]: training loss : 1.228887107372284 TRAIN  loss dict:  {'classification_loss': 1.228887107372284}
2025-01-18 16:23:50,390 [INFO] Step[400/2713]: training loss : 1.2242724609375 TRAIN  loss dict:  {'classification_loss': 1.2242724609375}
2025-01-18 16:24:05,478 [INFO] Step[450/2713]: training loss : 1.264485515356064 TRAIN  loss dict:  {'classification_loss': 1.264485515356064}
2025-01-18 16:24:20,559 [INFO] Step[500/2713]: training loss : 1.2697632431983947 TRAIN  loss dict:  {'classification_loss': 1.2697632431983947}
2025-01-18 16:24:35,559 [INFO] Step[550/2713]: training loss : 1.2130901408195496 TRAIN  loss dict:  {'classification_loss': 1.2130901408195496}
2025-01-18 16:24:50,573 [INFO] Step[600/2713]: training loss : 1.2705397176742554 TRAIN  loss dict:  {'classification_loss': 1.2705397176742554}
2025-01-18 16:25:05,712 [INFO] Step[650/2713]: training loss : 1.2508854603767394 TRAIN  loss dict:  {'classification_loss': 1.2508854603767394}
2025-01-18 16:25:20,759 [INFO] Step[700/2713]: training loss : 1.2636725521087646 TRAIN  loss dict:  {'classification_loss': 1.2636725521087646}
2025-01-18 16:25:35,831 [INFO] Step[750/2713]: training loss : 1.264712414741516 TRAIN  loss dict:  {'classification_loss': 1.264712414741516}
2025-01-18 16:25:50,883 [INFO] Step[800/2713]: training loss : 1.353144087791443 TRAIN  loss dict:  {'classification_loss': 1.353144087791443}
2025-01-18 16:26:06,031 [INFO] Step[850/2713]: training loss : 1.2242582893371583 TRAIN  loss dict:  {'classification_loss': 1.2242582893371583}
2025-01-18 16:26:21,098 [INFO] Step[900/2713]: training loss : 1.215829770565033 TRAIN  loss dict:  {'classification_loss': 1.215829770565033}
2025-01-18 16:26:36,208 [INFO] Step[950/2713]: training loss : 1.24546236038208 TRAIN  loss dict:  {'classification_loss': 1.24546236038208}
2025-01-18 16:26:51,276 [INFO] Step[1000/2713]: training loss : 1.2324640154838562 TRAIN  loss dict:  {'classification_loss': 1.2324640154838562}
2025-01-18 16:27:06,448 [INFO] Step[1050/2713]: training loss : 1.2319442963600158 TRAIN  loss dict:  {'classification_loss': 1.2319442963600158}
2025-01-18 16:27:21,518 [INFO] Step[1100/2713]: training loss : 1.3645821332931518 TRAIN  loss dict:  {'classification_loss': 1.3645821332931518}
2025-01-18 16:27:36,639 [INFO] Step[1150/2713]: training loss : 1.2820566129684448 TRAIN  loss dict:  {'classification_loss': 1.2820566129684448}
2025-01-18 16:27:51,801 [INFO] Step[1200/2713]: training loss : 1.2120917844772339 TRAIN  loss dict:  {'classification_loss': 1.2120917844772339}
2025-01-18 16:28:06,894 [INFO] Step[1250/2713]: training loss : 1.2876570534706115 TRAIN  loss dict:  {'classification_loss': 1.2876570534706115}
2025-01-18 16:28:21,956 [INFO] Step[1300/2713]: training loss : 1.2485013866424561 TRAIN  loss dict:  {'classification_loss': 1.2485013866424561}
2025-01-18 16:28:37,045 [INFO] Step[1350/2713]: training loss : 1.3482327365875244 TRAIN  loss dict:  {'classification_loss': 1.3482327365875244}
2025-01-18 16:28:52,154 [INFO] Step[1400/2713]: training loss : 1.23609628200531 TRAIN  loss dict:  {'classification_loss': 1.23609628200531}
2025-01-18 16:29:07,239 [INFO] Step[1450/2713]: training loss : 1.2422722935676576 TRAIN  loss dict:  {'classification_loss': 1.2422722935676576}
2025-01-18 16:29:22,375 [INFO] Step[1500/2713]: training loss : 1.2823346209526063 TRAIN  loss dict:  {'classification_loss': 1.2823346209526063}
2025-01-18 16:29:37,467 [INFO] Step[1550/2713]: training loss : 1.284850525856018 TRAIN  loss dict:  {'classification_loss': 1.284850525856018}
2025-01-18 16:29:52,584 [INFO] Step[1600/2713]: training loss : 1.2192470741271972 TRAIN  loss dict:  {'classification_loss': 1.2192470741271972}
2025-01-18 16:30:07,906 [INFO] Step[1650/2713]: training loss : 1.3262034034729004 TRAIN  loss dict:  {'classification_loss': 1.3262034034729004}
2025-01-18 16:30:23,079 [INFO] Step[1700/2713]: training loss : 1.315327799320221 TRAIN  loss dict:  {'classification_loss': 1.315327799320221}
2025-01-18 16:30:38,205 [INFO] Step[1750/2713]: training loss : 1.2776289653778077 TRAIN  loss dict:  {'classification_loss': 1.2776289653778077}
2025-01-18 16:30:53,319 [INFO] Step[1800/2713]: training loss : 1.3670757436752319 TRAIN  loss dict:  {'classification_loss': 1.3670757436752319}
2025-01-18 16:31:08,427 [INFO] Step[1850/2713]: training loss : 1.2484058570861816 TRAIN  loss dict:  {'classification_loss': 1.2484058570861816}
2025-01-18 16:31:23,583 [INFO] Step[1900/2713]: training loss : 1.287979907989502 TRAIN  loss dict:  {'classification_loss': 1.287979907989502}
2025-01-18 16:31:38,668 [INFO] Step[1950/2713]: training loss : 1.2626490163803101 TRAIN  loss dict:  {'classification_loss': 1.2626490163803101}
2025-01-18 16:31:53,727 [INFO] Step[2000/2713]: training loss : 1.201786563396454 TRAIN  loss dict:  {'classification_loss': 1.201786563396454}
2025-01-18 16:32:08,862 [INFO] Step[2050/2713]: training loss : 1.2771234130859375 TRAIN  loss dict:  {'classification_loss': 1.2771234130859375}
2025-01-18 16:32:23,993 [INFO] Step[2100/2713]: training loss : 1.2561598491668702 TRAIN  loss dict:  {'classification_loss': 1.2561598491668702}
2025-01-18 16:32:39,163 [INFO] Step[2150/2713]: training loss : 1.260536937713623 TRAIN  loss dict:  {'classification_loss': 1.260536937713623}
2025-01-18 16:32:54,320 [INFO] Step[2200/2713]: training loss : 1.2607679891586303 TRAIN  loss dict:  {'classification_loss': 1.2607679891586303}
2025-01-18 16:33:09,438 [INFO] Step[2250/2713]: training loss : 1.3360655784606934 TRAIN  loss dict:  {'classification_loss': 1.3360655784606934}
2025-01-18 16:33:24,612 [INFO] Step[2300/2713]: training loss : 1.2584929490089416 TRAIN  loss dict:  {'classification_loss': 1.2584929490089416}
2025-01-18 16:33:39,737 [INFO] Step[2350/2713]: training loss : 1.2158343362808228 TRAIN  loss dict:  {'classification_loss': 1.2158343362808228}
2025-01-18 16:33:54,873 [INFO] Step[2400/2713]: training loss : 1.180607476234436 TRAIN  loss dict:  {'classification_loss': 1.180607476234436}
2025-01-18 16:34:10,031 [INFO] Step[2450/2713]: training loss : 1.2963609170913697 TRAIN  loss dict:  {'classification_loss': 1.2963609170913697}
2025-01-18 16:34:25,126 [INFO] Step[2500/2713]: training loss : 1.293438766002655 TRAIN  loss dict:  {'classification_loss': 1.293438766002655}
2025-01-18 16:34:40,243 [INFO] Step[2550/2713]: training loss : 1.2095632910728455 TRAIN  loss dict:  {'classification_loss': 1.2095632910728455}
2025-01-18 16:34:55,278 [INFO] Step[2600/2713]: training loss : 1.3228281736373901 TRAIN  loss dict:  {'classification_loss': 1.3228281736373901}
2025-01-18 16:35:10,372 [INFO] Step[2650/2713]: training loss : 1.3245977187156677 TRAIN  loss dict:  {'classification_loss': 1.3245977187156677}
2025-01-18 16:35:25,409 [INFO] Step[2700/2713]: training loss : 1.322300102710724 TRAIN  loss dict:  {'classification_loss': 1.322300102710724}
2025-01-18 16:36:46,065 [INFO] Label accuracies statistics:
2025-01-18 16:36:46,065 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.5, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.5, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 1.0, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.5, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.5, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 0.75, 66: 0.5, 67: 0.5, 68: 0.25, 69: 0.75, 70: 0.25, 71: 0.5, 72: 1.0, 73: 0.25, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.0, 95: 0.75, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.25, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 0.5, 119: 1.0, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 0.75, 142: 0.5, 143: 0.25, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 0.5, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.25, 192: 0.75, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.5, 200: 0.75, 201: 0.25, 202: 0.25, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.5, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.5, 229: 0.75, 230: 0.0, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.5, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.75, 261: 0.25, 262: 0.75, 263: 0.25, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.5, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.25, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.5, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.25, 314: 1.0, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 1.0, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.25, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.5, 351: 1.0, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.5, 356: 0.25, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 1.0, 372: 0.5, 373: 1.0, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.5, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-18 16:36:47,403 [INFO] [7] TRAIN  loss: 1.2640794902941068 acc: 0.9245607568497358
2025-01-18 16:36:47,403 [INFO] [7] TRAIN  loss dict: {'classification_loss': 1.2640794902941068}
2025-01-18 16:36:47,403 [INFO] [7] VALIDATION loss: 1.900608674027866 VALIDATION acc: 0.7379310344827587
2025-01-18 16:36:47,403 [INFO] [7] VALIDATION loss dict: {'classification_loss': 1.900608674027866}
2025-01-18 16:36:47,404 [INFO] 
2025-01-18 16:37:07,192 [INFO] Step[50/2713]: training loss : 1.221261260509491 TRAIN  loss dict:  {'classification_loss': 1.221261260509491}
2025-01-18 16:37:22,308 [INFO] Step[100/2713]: training loss : 1.2395183801651002 TRAIN  loss dict:  {'classification_loss': 1.2395183801651002}
2025-01-18 16:37:37,428 [INFO] Step[150/2713]: training loss : 1.2314096999168396 TRAIN  loss dict:  {'classification_loss': 1.2314096999168396}
2025-01-18 16:37:52,578 [INFO] Step[200/2713]: training loss : 1.2153528761863708 TRAIN  loss dict:  {'classification_loss': 1.2153528761863708}
2025-01-18 16:38:07,716 [INFO] Step[250/2713]: training loss : 1.1746921682357787 TRAIN  loss dict:  {'classification_loss': 1.1746921682357787}
2025-01-18 16:38:22,866 [INFO] Step[300/2713]: training loss : 1.2455128240585327 TRAIN  loss dict:  {'classification_loss': 1.2455128240585327}
2025-01-18 16:38:37,989 [INFO] Step[350/2713]: training loss : 1.200433759689331 TRAIN  loss dict:  {'classification_loss': 1.200433759689331}
2025-01-18 16:38:53,052 [INFO] Step[400/2713]: training loss : 1.230453097820282 TRAIN  loss dict:  {'classification_loss': 1.230453097820282}
2025-01-18 16:39:08,109 [INFO] Step[450/2713]: training loss : 1.237243993282318 TRAIN  loss dict:  {'classification_loss': 1.237243993282318}
2025-01-18 16:39:23,146 [INFO] Step[500/2713]: training loss : 1.2699860286712648 TRAIN  loss dict:  {'classification_loss': 1.2699860286712648}
2025-01-18 16:39:38,200 [INFO] Step[550/2713]: training loss : 1.1818634867668152 TRAIN  loss dict:  {'classification_loss': 1.1818634867668152}
2025-01-18 16:39:53,227 [INFO] Step[600/2713]: training loss : 1.2769389295578002 TRAIN  loss dict:  {'classification_loss': 1.2769389295578002}
2025-01-18 16:40:08,328 [INFO] Step[650/2713]: training loss : 1.263728952407837 TRAIN  loss dict:  {'classification_loss': 1.263728952407837}
2025-01-18 16:40:23,473 [INFO] Step[700/2713]: training loss : 1.2446769404411315 TRAIN  loss dict:  {'classification_loss': 1.2446769404411315}
2025-01-18 16:40:38,632 [INFO] Step[750/2713]: training loss : 1.2506588411331176 TRAIN  loss dict:  {'classification_loss': 1.2506588411331176}
2025-01-18 16:40:53,788 [INFO] Step[800/2713]: training loss : 1.2588444232940674 TRAIN  loss dict:  {'classification_loss': 1.2588444232940674}
2025-01-18 16:41:08,954 [INFO] Step[850/2713]: training loss : 1.2845854258537293 TRAIN  loss dict:  {'classification_loss': 1.2845854258537293}
2025-01-18 16:41:24,077 [INFO] Step[900/2713]: training loss : 1.179631338119507 TRAIN  loss dict:  {'classification_loss': 1.179631338119507}
2025-01-18 16:41:39,224 [INFO] Step[950/2713]: training loss : 1.2902942323684692 TRAIN  loss dict:  {'classification_loss': 1.2902942323684692}
2025-01-18 16:41:54,361 [INFO] Step[1000/2713]: training loss : 1.198134958744049 TRAIN  loss dict:  {'classification_loss': 1.198134958744049}
2025-01-18 16:42:09,494 [INFO] Step[1050/2713]: training loss : 1.30205974817276 TRAIN  loss dict:  {'classification_loss': 1.30205974817276}
2025-01-18 16:42:24,639 [INFO] Step[1100/2713]: training loss : 1.2755986428260804 TRAIN  loss dict:  {'classification_loss': 1.2755986428260804}
2025-01-18 16:42:39,767 [INFO] Step[1150/2713]: training loss : 1.2813023459911346 TRAIN  loss dict:  {'classification_loss': 1.2813023459911346}
2025-01-18 16:42:54,968 [INFO] Step[1200/2713]: training loss : 1.2241538226604463 TRAIN  loss dict:  {'classification_loss': 1.2241538226604463}
2025-01-18 16:43:10,203 [INFO] Step[1250/2713]: training loss : 1.3250421166419983 TRAIN  loss dict:  {'classification_loss': 1.3250421166419983}
2025-01-18 16:43:25,279 [INFO] Step[1300/2713]: training loss : 1.2603272318840026 TRAIN  loss dict:  {'classification_loss': 1.2603272318840026}
2025-01-18 16:43:40,457 [INFO] Step[1350/2713]: training loss : 1.1930259275436401 TRAIN  loss dict:  {'classification_loss': 1.1930259275436401}
2025-01-18 16:43:55,558 [INFO] Step[1400/2713]: training loss : 1.2326139950752257 TRAIN  loss dict:  {'classification_loss': 1.2326139950752257}
2025-01-18 16:44:10,762 [INFO] Step[1450/2713]: training loss : 1.3356551623344421 TRAIN  loss dict:  {'classification_loss': 1.3356551623344421}
2025-01-18 16:44:25,859 [INFO] Step[1500/2713]: training loss : 1.2122615671157837 TRAIN  loss dict:  {'classification_loss': 1.2122615671157837}
2025-01-18 16:44:41,011 [INFO] Step[1550/2713]: training loss : 1.2509012818336487 TRAIN  loss dict:  {'classification_loss': 1.2509012818336487}
2025-01-18 16:44:56,003 [INFO] Step[1600/2713]: training loss : 1.2280414175987244 TRAIN  loss dict:  {'classification_loss': 1.2280414175987244}
2025-01-18 16:45:10,967 [INFO] Step[1650/2713]: training loss : 1.238605079650879 TRAIN  loss dict:  {'classification_loss': 1.238605079650879}
2025-01-18 16:45:25,890 [INFO] Step[1700/2713]: training loss : 1.2682198810577392 TRAIN  loss dict:  {'classification_loss': 1.2682198810577392}
2025-01-18 16:45:40,945 [INFO] Step[1750/2713]: training loss : 1.2769074821472168 TRAIN  loss dict:  {'classification_loss': 1.2769074821472168}
2025-01-18 16:45:55,951 [INFO] Step[1800/2713]: training loss : 1.2172120165824891 TRAIN  loss dict:  {'classification_loss': 1.2172120165824891}
2025-01-18 16:46:10,916 [INFO] Step[1850/2713]: training loss : 1.1895544123649597 TRAIN  loss dict:  {'classification_loss': 1.1895544123649597}
2025-01-18 16:46:25,868 [INFO] Step[1900/2713]: training loss : 1.2062138175964356 TRAIN  loss dict:  {'classification_loss': 1.2062138175964356}
2025-01-18 16:46:40,808 [INFO] Step[1950/2713]: training loss : 1.2633980369567872 TRAIN  loss dict:  {'classification_loss': 1.2633980369567872}
2025-01-18 16:46:55,738 [INFO] Step[2000/2713]: training loss : 1.3459073102474213 TRAIN  loss dict:  {'classification_loss': 1.3459073102474213}
2025-01-18 16:47:10,685 [INFO] Step[2050/2713]: training loss : 1.219814338684082 TRAIN  loss dict:  {'classification_loss': 1.219814338684082}
2025-01-18 16:47:25,651 [INFO] Step[2100/2713]: training loss : 1.262853753566742 TRAIN  loss dict:  {'classification_loss': 1.262853753566742}
2025-01-18 16:47:40,549 [INFO] Step[2150/2713]: training loss : 1.2789242267608643 TRAIN  loss dict:  {'classification_loss': 1.2789242267608643}
2025-01-18 16:47:55,605 [INFO] Step[2200/2713]: training loss : 1.2192418456077576 TRAIN  loss dict:  {'classification_loss': 1.2192418456077576}
2025-01-18 16:48:10,590 [INFO] Step[2250/2713]: training loss : 1.246542100906372 TRAIN  loss dict:  {'classification_loss': 1.246542100906372}
2025-01-18 16:48:25,580 [INFO] Step[2300/2713]: training loss : 1.2579289412498473 TRAIN  loss dict:  {'classification_loss': 1.2579289412498473}
2025-01-18 16:48:40,518 [INFO] Step[2350/2713]: training loss : 1.223279926776886 TRAIN  loss dict:  {'classification_loss': 1.223279926776886}
2025-01-18 16:48:55,574 [INFO] Step[2400/2713]: training loss : 1.1839759755134582 TRAIN  loss dict:  {'classification_loss': 1.1839759755134582}
2025-01-18 16:49:10,711 [INFO] Step[2450/2713]: training loss : 1.247855192422867 TRAIN  loss dict:  {'classification_loss': 1.247855192422867}
2025-01-18 16:49:25,706 [INFO] Step[2500/2713]: training loss : 1.2585712587833404 TRAIN  loss dict:  {'classification_loss': 1.2585712587833404}
2025-01-18 16:49:40,753 [INFO] Step[2550/2713]: training loss : 1.2517870378494262 TRAIN  loss dict:  {'classification_loss': 1.2517870378494262}
2025-01-18 16:49:55,801 [INFO] Step[2600/2713]: training loss : 1.2206585860252381 TRAIN  loss dict:  {'classification_loss': 1.2206585860252381}
2025-01-18 16:50:10,829 [INFO] Step[2650/2713]: training loss : 1.2419198334217072 TRAIN  loss dict:  {'classification_loss': 1.2419198334217072}
2025-01-18 16:50:25,957 [INFO] Step[2700/2713]: training loss : 1.2544930768013 TRAIN  loss dict:  {'classification_loss': 1.2544930768013}
2025-01-18 16:51:46,489 [INFO] Label accuracies statistics:
2025-01-18 16:51:46,489 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.5, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.25, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 1.0, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 0.75, 126: 1.0, 127: 0.5, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.5, 135: 1.0, 136: 0.75, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.5, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.5, 169: 0.75, 170: 0.75, 171: 0.25, 172: 0.75, 173: 0.25, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.25, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.5, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.0, 207: 0.75, 208: 0.25, 209: 0.75, 210: 1.0, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.5, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.25, 238: 1.0, 239: 0.0, 240: 1.0, 241: 0.75, 242: 0.75, 243: 0.25, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.3333333333333333, 249: 0.75, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.5, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.5, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 1.0, 289: 1.0, 290: 0.0, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 1.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.5, 315: 0.5, 316: 0.25, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.5, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.5, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.25, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.5, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.5, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.25, 377: 0.75, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.25, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.5, 396: 0.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 16:51:47,897 [INFO] [8] TRAIN  loss: 1.244658947342487 acc: 0.9316869394274481
2025-01-18 16:51:47,897 [INFO] [8] TRAIN  loss dict: {'classification_loss': 1.244658947342487}
2025-01-18 16:51:47,897 [INFO] [8] VALIDATION loss: 1.8738721605754436 VALIDATION acc: 0.74858934169279
2025-01-18 16:51:47,897 [INFO] [8] VALIDATION loss dict: {'classification_loss': 1.8738721605754436}
2025-01-18 16:51:47,897 [INFO] 
2025-01-18 16:52:08,530 [INFO] Step[50/2713]: training loss : 1.1483753180503846 TRAIN  loss dict:  {'classification_loss': 1.1483753180503846}
2025-01-18 16:52:23,533 [INFO] Step[100/2713]: training loss : 1.2053694117069245 TRAIN  loss dict:  {'classification_loss': 1.2053694117069245}
2025-01-18 16:52:38,584 [INFO] Step[150/2713]: training loss : 1.1987588262557984 TRAIN  loss dict:  {'classification_loss': 1.1987588262557984}
2025-01-18 16:52:53,693 [INFO] Step[200/2713]: training loss : 1.247850888967514 TRAIN  loss dict:  {'classification_loss': 1.247850888967514}
2025-01-18 16:53:08,823 [INFO] Step[250/2713]: training loss : 1.220553537607193 TRAIN  loss dict:  {'classification_loss': 1.220553537607193}
2025-01-18 16:53:23,892 [INFO] Step[300/2713]: training loss : 1.2090693235397338 TRAIN  loss dict:  {'classification_loss': 1.2090693235397338}
2025-01-18 16:53:38,901 [INFO] Step[350/2713]: training loss : 1.268908908367157 TRAIN  loss dict:  {'classification_loss': 1.268908908367157}
2025-01-18 16:53:53,947 [INFO] Step[400/2713]: training loss : 1.1548986172676086 TRAIN  loss dict:  {'classification_loss': 1.1548986172676086}
2025-01-18 16:54:08,899 [INFO] Step[450/2713]: training loss : 1.2756166744232178 TRAIN  loss dict:  {'classification_loss': 1.2756166744232178}
2025-01-18 16:54:23,839 [INFO] Step[500/2713]: training loss : 1.14955486536026 TRAIN  loss dict:  {'classification_loss': 1.14955486536026}
2025-01-18 16:54:38,894 [INFO] Step[550/2713]: training loss : 1.3617397880554198 TRAIN  loss dict:  {'classification_loss': 1.3617397880554198}
2025-01-18 16:54:54,012 [INFO] Step[600/2713]: training loss : 1.3235315537452699 TRAIN  loss dict:  {'classification_loss': 1.3235315537452699}
2025-01-18 16:55:08,920 [INFO] Step[650/2713]: training loss : 1.178016494512558 TRAIN  loss dict:  {'classification_loss': 1.178016494512558}
2025-01-18 16:55:24,027 [INFO] Step[700/2713]: training loss : 1.1526089203357697 TRAIN  loss dict:  {'classification_loss': 1.1526089203357697}
2025-01-18 16:55:39,083 [INFO] Step[750/2713]: training loss : 1.2220629453659058 TRAIN  loss dict:  {'classification_loss': 1.2220629453659058}
2025-01-18 16:55:54,192 [INFO] Step[800/2713]: training loss : 1.16180762052536 TRAIN  loss dict:  {'classification_loss': 1.16180762052536}
2025-01-18 16:56:09,154 [INFO] Step[850/2713]: training loss : 1.1984723937511443 TRAIN  loss dict:  {'classification_loss': 1.1984723937511443}
2025-01-18 16:56:24,312 [INFO] Step[900/2713]: training loss : 1.1955857944488526 TRAIN  loss dict:  {'classification_loss': 1.1955857944488526}
2025-01-18 16:56:39,345 [INFO] Step[950/2713]: training loss : 1.1761849868297576 TRAIN  loss dict:  {'classification_loss': 1.1761849868297576}
2025-01-18 16:56:54,309 [INFO] Step[1000/2713]: training loss : 1.197741494178772 TRAIN  loss dict:  {'classification_loss': 1.197741494178772}
2025-01-18 16:57:09,343 [INFO] Step[1050/2713]: training loss : 1.2322654223442078 TRAIN  loss dict:  {'classification_loss': 1.2322654223442078}
2025-01-18 16:57:24,319 [INFO] Step[1100/2713]: training loss : 1.1665884780883788 TRAIN  loss dict:  {'classification_loss': 1.1665884780883788}
2025-01-18 16:57:39,354 [INFO] Step[1150/2713]: training loss : 1.2227502405643462 TRAIN  loss dict:  {'classification_loss': 1.2227502405643462}
2025-01-18 16:57:54,397 [INFO] Step[1200/2713]: training loss : 1.2491589295864105 TRAIN  loss dict:  {'classification_loss': 1.2491589295864105}
2025-01-18 16:58:09,316 [INFO] Step[1250/2713]: training loss : 1.1399472439289093 TRAIN  loss dict:  {'classification_loss': 1.1399472439289093}
2025-01-18 16:58:24,380 [INFO] Step[1300/2713]: training loss : 1.2354103207588196 TRAIN  loss dict:  {'classification_loss': 1.2354103207588196}
2025-01-18 16:58:39,339 [INFO] Step[1350/2713]: training loss : 1.1964560317993165 TRAIN  loss dict:  {'classification_loss': 1.1964560317993165}
2025-01-18 16:58:54,261 [INFO] Step[1400/2713]: training loss : 1.2414906120300293 TRAIN  loss dict:  {'classification_loss': 1.2414906120300293}
2025-01-18 16:59:09,349 [INFO] Step[1450/2713]: training loss : 1.2666769301891327 TRAIN  loss dict:  {'classification_loss': 1.2666769301891327}
2025-01-18 16:59:24,471 [INFO] Step[1500/2713]: training loss : 1.1948374843597411 TRAIN  loss dict:  {'classification_loss': 1.1948374843597411}
2025-01-18 16:59:39,576 [INFO] Step[1550/2713]: training loss : 1.205598396062851 TRAIN  loss dict:  {'classification_loss': 1.205598396062851}
2025-01-18 16:59:54,516 [INFO] Step[1600/2713]: training loss : 1.2724094343185426 TRAIN  loss dict:  {'classification_loss': 1.2724094343185426}
2025-01-18 17:00:09,542 [INFO] Step[1650/2713]: training loss : 1.2080217695236206 TRAIN  loss dict:  {'classification_loss': 1.2080217695236206}
2025-01-18 17:00:24,564 [INFO] Step[1700/2713]: training loss : 1.157079907655716 TRAIN  loss dict:  {'classification_loss': 1.157079907655716}
2025-01-18 17:00:39,623 [INFO] Step[1750/2713]: training loss : 1.2126357769966125 TRAIN  loss dict:  {'classification_loss': 1.2126357769966125}
2025-01-18 17:00:54,667 [INFO] Step[1800/2713]: training loss : 1.2440240097045898 TRAIN  loss dict:  {'classification_loss': 1.2440240097045898}
2025-01-18 17:01:09,610 [INFO] Step[1850/2713]: training loss : 1.2167132711410522 TRAIN  loss dict:  {'classification_loss': 1.2167132711410522}
2025-01-18 17:01:24,661 [INFO] Step[1900/2713]: training loss : 1.23000821352005 TRAIN  loss dict:  {'classification_loss': 1.23000821352005}
2025-01-18 17:01:39,830 [INFO] Step[1950/2713]: training loss : 1.3124413895606994 TRAIN  loss dict:  {'classification_loss': 1.3124413895606994}
2025-01-18 17:01:54,875 [INFO] Step[2000/2713]: training loss : 1.2488941740989685 TRAIN  loss dict:  {'classification_loss': 1.2488941740989685}
2025-01-18 17:02:10,057 [INFO] Step[2050/2713]: training loss : 1.1791573715209962 TRAIN  loss dict:  {'classification_loss': 1.1791573715209962}
2025-01-18 17:02:25,079 [INFO] Step[2100/2713]: training loss : 1.2490598094463348 TRAIN  loss dict:  {'classification_loss': 1.2490598094463348}
2025-01-18 17:02:40,175 [INFO] Step[2150/2713]: training loss : 1.1665034735202788 TRAIN  loss dict:  {'classification_loss': 1.1665034735202788}
2025-01-18 17:02:55,154 [INFO] Step[2200/2713]: training loss : 1.2346225714683532 TRAIN  loss dict:  {'classification_loss': 1.2346225714683532}
2025-01-18 17:03:10,123 [INFO] Step[2250/2713]: training loss : 1.135491315126419 TRAIN  loss dict:  {'classification_loss': 1.135491315126419}
2025-01-18 17:03:25,250 [INFO] Step[2300/2713]: training loss : 1.1779505181312562 TRAIN  loss dict:  {'classification_loss': 1.1779505181312562}
2025-01-18 17:03:40,250 [INFO] Step[2350/2713]: training loss : 1.1834825801849365 TRAIN  loss dict:  {'classification_loss': 1.1834825801849365}
2025-01-18 17:03:55,273 [INFO] Step[2400/2713]: training loss : 1.2266937923431396 TRAIN  loss dict:  {'classification_loss': 1.2266937923431396}
2025-01-18 17:04:10,241 [INFO] Step[2450/2713]: training loss : 1.2905189669132233 TRAIN  loss dict:  {'classification_loss': 1.2905189669132233}
2025-01-18 17:04:25,282 [INFO] Step[2500/2713]: training loss : 1.2457750618457795 TRAIN  loss dict:  {'classification_loss': 1.2457750618457795}
2025-01-18 17:04:40,210 [INFO] Step[2550/2713]: training loss : 1.3017496430873872 TRAIN  loss dict:  {'classification_loss': 1.3017496430873872}
2025-01-18 17:04:55,223 [INFO] Step[2600/2713]: training loss : 1.253801028728485 TRAIN  loss dict:  {'classification_loss': 1.253801028728485}
2025-01-18 17:05:10,243 [INFO] Step[2650/2713]: training loss : 1.218271770477295 TRAIN  loss dict:  {'classification_loss': 1.218271770477295}
2025-01-18 17:05:25,189 [INFO] Step[2700/2713]: training loss : 1.2460905528068542 TRAIN  loss dict:  {'classification_loss': 1.2460905528068542}
2025-01-18 17:06:46,415 [INFO] Label accuracies statistics:
2025-01-18 17:06:46,415 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.25, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.25, 69: 0.75, 70: 1.0, 71: 0.5, 72: 0.75, 73: 0.5, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 1.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 0.75, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.5, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.25, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.5, 208: 1.0, 209: 0.5, 210: 1.0, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.25, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.75, 244: 0.75, 245: 0.5, 246: 0.75, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.75, 290: 0.25, 291: 1.0, 292: 0.75, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.5, 326: 0.75, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.5, 334: 1.0, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.5, 342: 1.0, 343: 0.75, 344: 0.75, 345: 0.25, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.5, 356: 0.5, 357: 0.75, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 17:06:47,795 [INFO] [9] TRAIN  loss: 1.218153783921777 acc: 0.9386902567883032
2025-01-18 17:06:47,795 [INFO] [9] TRAIN  loss dict: {'classification_loss': 1.218153783921777}
2025-01-18 17:06:47,795 [INFO] [9] VALIDATION loss: 1.8502085278356881 VALIDATION acc: 0.7717868338557994
2025-01-18 17:06:47,795 [INFO] [9] VALIDATION loss dict: {'classification_loss': 1.8502085278356881}
2025-01-18 17:06:47,795 [INFO] 
2025-01-18 17:07:08,375 [INFO] Step[50/2713]: training loss : 1.1760817956924439 TRAIN  loss dict:  {'classification_loss': 1.1760817956924439}
2025-01-18 17:07:23,317 [INFO] Step[100/2713]: training loss : 1.156585693359375 TRAIN  loss dict:  {'classification_loss': 1.156585693359375}
2025-01-18 17:07:38,349 [INFO] Step[150/2713]: training loss : 1.1707622683048249 TRAIN  loss dict:  {'classification_loss': 1.1707622683048249}
2025-01-18 17:07:53,353 [INFO] Step[200/2713]: training loss : 1.2077410995960236 TRAIN  loss dict:  {'classification_loss': 1.2077410995960236}
2025-01-18 17:08:08,318 [INFO] Step[250/2713]: training loss : 1.17005891084671 TRAIN  loss dict:  {'classification_loss': 1.17005891084671}
2025-01-18 17:08:23,492 [INFO] Step[300/2713]: training loss : 1.165198940038681 TRAIN  loss dict:  {'classification_loss': 1.165198940038681}
2025-01-18 17:08:38,618 [INFO] Step[350/2713]: training loss : 1.1944645357131958 TRAIN  loss dict:  {'classification_loss': 1.1944645357131958}
2025-01-18 17:08:53,719 [INFO] Step[400/2713]: training loss : 1.1472529125213624 TRAIN  loss dict:  {'classification_loss': 1.1472529125213624}
2025-01-18 17:09:08,750 [INFO] Step[450/2713]: training loss : 1.1981803047657014 TRAIN  loss dict:  {'classification_loss': 1.1981803047657014}
2025-01-18 17:09:23,818 [INFO] Step[500/2713]: training loss : 1.1837714743614196 TRAIN  loss dict:  {'classification_loss': 1.1837714743614196}
2025-01-18 17:09:38,768 [INFO] Step[550/2713]: training loss : 1.2211321353912354 TRAIN  loss dict:  {'classification_loss': 1.2211321353912354}
2025-01-18 17:09:53,757 [INFO] Step[600/2713]: training loss : 1.2332539224624635 TRAIN  loss dict:  {'classification_loss': 1.2332539224624635}
2025-01-18 17:10:08,740 [INFO] Step[650/2713]: training loss : 1.193791357278824 TRAIN  loss dict:  {'classification_loss': 1.193791357278824}
2025-01-18 17:10:23,804 [INFO] Step[700/2713]: training loss : 1.1967737460136414 TRAIN  loss dict:  {'classification_loss': 1.1967737460136414}
2025-01-18 17:10:38,887 [INFO] Step[750/2713]: training loss : 1.1811815297603607 TRAIN  loss dict:  {'classification_loss': 1.1811815297603607}
2025-01-18 17:10:53,922 [INFO] Step[800/2713]: training loss : 1.202351063489914 TRAIN  loss dict:  {'classification_loss': 1.202351063489914}
2025-01-18 17:11:08,916 [INFO] Step[850/2713]: training loss : 1.1767967665195465 TRAIN  loss dict:  {'classification_loss': 1.1767967665195465}
2025-01-18 17:11:23,909 [INFO] Step[900/2713]: training loss : 1.1814446413517 TRAIN  loss dict:  {'classification_loss': 1.1814446413517}
2025-01-18 17:11:38,984 [INFO] Step[950/2713]: training loss : 1.2215886414051056 TRAIN  loss dict:  {'classification_loss': 1.2215886414051056}
2025-01-18 17:11:53,999 [INFO] Step[1000/2713]: training loss : 1.227643243074417 TRAIN  loss dict:  {'classification_loss': 1.227643243074417}
2025-01-18 17:12:09,120 [INFO] Step[1050/2713]: training loss : 1.1888801181316375 TRAIN  loss dict:  {'classification_loss': 1.1888801181316375}
2025-01-18 17:12:24,081 [INFO] Step[1100/2713]: training loss : 1.2143119478225708 TRAIN  loss dict:  {'classification_loss': 1.2143119478225708}
2025-01-18 17:12:39,184 [INFO] Step[1150/2713]: training loss : 1.1454105758666993 TRAIN  loss dict:  {'classification_loss': 1.1454105758666993}
2025-01-18 17:12:54,309 [INFO] Step[1200/2713]: training loss : 1.2156122863292693 TRAIN  loss dict:  {'classification_loss': 1.2156122863292693}
2025-01-18 17:13:09,338 [INFO] Step[1250/2713]: training loss : 1.2364917123317718 TRAIN  loss dict:  {'classification_loss': 1.2364917123317718}
2025-01-18 17:13:24,426 [INFO] Step[1300/2713]: training loss : 1.2067342913150787 TRAIN  loss dict:  {'classification_loss': 1.2067342913150787}
2025-01-18 17:13:39,474 [INFO] Step[1350/2713]: training loss : 1.174665094614029 TRAIN  loss dict:  {'classification_loss': 1.174665094614029}
2025-01-18 17:13:54,517 [INFO] Step[1400/2713]: training loss : 1.196174087524414 TRAIN  loss dict:  {'classification_loss': 1.196174087524414}
2025-01-18 17:14:09,576 [INFO] Step[1450/2713]: training loss : 1.1412278342247009 TRAIN  loss dict:  {'classification_loss': 1.1412278342247009}
2025-01-18 17:14:24,664 [INFO] Step[1500/2713]: training loss : 1.2999285662174225 TRAIN  loss dict:  {'classification_loss': 1.2999285662174225}
2025-01-18 17:14:39,766 [INFO] Step[1550/2713]: training loss : 1.2209845781326294 TRAIN  loss dict:  {'classification_loss': 1.2209845781326294}
2025-01-18 17:14:54,789 [INFO] Step[1600/2713]: training loss : 1.1889740645885467 TRAIN  loss dict:  {'classification_loss': 1.1889740645885467}
2025-01-18 17:15:09,842 [INFO] Step[1650/2713]: training loss : 1.2128591752052307 TRAIN  loss dict:  {'classification_loss': 1.2128591752052307}
2025-01-18 17:15:24,966 [INFO] Step[1700/2713]: training loss : 1.1394235694408417 TRAIN  loss dict:  {'classification_loss': 1.1394235694408417}
2025-01-18 17:15:40,039 [INFO] Step[1750/2713]: training loss : 1.1238417840003967 TRAIN  loss dict:  {'classification_loss': 1.1238417840003967}
2025-01-18 17:15:55,089 [INFO] Step[1800/2713]: training loss : 1.1449443817138671 TRAIN  loss dict:  {'classification_loss': 1.1449443817138671}
2025-01-18 17:16:10,095 [INFO] Step[1850/2713]: training loss : 1.1570688343048097 TRAIN  loss dict:  {'classification_loss': 1.1570688343048097}
2025-01-18 17:16:25,050 [INFO] Step[1900/2713]: training loss : 1.242193156480789 TRAIN  loss dict:  {'classification_loss': 1.242193156480789}
2025-01-18 17:16:40,074 [INFO] Step[1950/2713]: training loss : 1.209860347509384 TRAIN  loss dict:  {'classification_loss': 1.209860347509384}
2025-01-18 17:16:55,210 [INFO] Step[2000/2713]: training loss : 1.2208807897567748 TRAIN  loss dict:  {'classification_loss': 1.2208807897567748}
2025-01-18 17:17:10,177 [INFO] Step[2050/2713]: training loss : 1.2437643706798553 TRAIN  loss dict:  {'classification_loss': 1.2437643706798553}
2025-01-18 17:17:25,135 [INFO] Step[2100/2713]: training loss : 1.2416900300979614 TRAIN  loss dict:  {'classification_loss': 1.2416900300979614}
2025-01-18 17:17:40,253 [INFO] Step[2150/2713]: training loss : 1.1854571175575257 TRAIN  loss dict:  {'classification_loss': 1.1854571175575257}
2025-01-18 17:17:55,327 [INFO] Step[2200/2713]: training loss : 1.1571146774291992 TRAIN  loss dict:  {'classification_loss': 1.1571146774291992}
2025-01-18 17:18:10,289 [INFO] Step[2250/2713]: training loss : 1.1729317951202392 TRAIN  loss dict:  {'classification_loss': 1.1729317951202392}
2025-01-18 17:18:25,243 [INFO] Step[2300/2713]: training loss : 1.1962151563167571 TRAIN  loss dict:  {'classification_loss': 1.1962151563167571}
2025-01-18 17:18:40,364 [INFO] Step[2350/2713]: training loss : 1.2100055289268494 TRAIN  loss dict:  {'classification_loss': 1.2100055289268494}
2025-01-18 17:18:55,341 [INFO] Step[2400/2713]: training loss : 1.2286745953559874 TRAIN  loss dict:  {'classification_loss': 1.2286745953559874}
2025-01-18 17:19:10,603 [INFO] Step[2450/2713]: training loss : 1.182514499425888 TRAIN  loss dict:  {'classification_loss': 1.182514499425888}
2025-01-18 17:19:25,693 [INFO] Step[2500/2713]: training loss : 1.2224684309959413 TRAIN  loss dict:  {'classification_loss': 1.2224684309959413}
2025-01-18 17:19:40,763 [INFO] Step[2550/2713]: training loss : 1.279200735092163 TRAIN  loss dict:  {'classification_loss': 1.279200735092163}
2025-01-18 17:19:55,778 [INFO] Step[2600/2713]: training loss : 1.2341568398475646 TRAIN  loss dict:  {'classification_loss': 1.2341568398475646}
2025-01-18 17:20:10,761 [INFO] Step[2650/2713]: training loss : 1.1872383332252503 TRAIN  loss dict:  {'classification_loss': 1.1872383332252503}
2025-01-18 17:20:25,961 [INFO] Step[2700/2713]: training loss : 1.1663138329982758 TRAIN  loss dict:  {'classification_loss': 1.1663138329982758}
2025-01-18 17:21:47,118 [INFO] Label accuracies statistics:
2025-01-18 17:21:47,118 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.25, 67: 0.5, 68: 0.5, 69: 0.75, 70: 0.25, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.5, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 1.0, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 0.75, 137: 0.75, 138: 0.75, 139: 0.75, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.25, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.25, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.5, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.6666666666666666, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.25, 209: 0.5, 210: 1.0, 211: 0.0, 212: 0.25, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.5, 246: 0.5, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.25, 296: 0.75, 297: 0.25, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.25, 303: 1.0, 304: 0.5, 305: 1.0, 306: 0.75, 307: 1.0, 308: 0.75, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 1.0, 326: 0.75, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.5, 356: 0.5, 357: 0.75, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.5, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.25, 371: 1.0, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.5, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.25, 388: 1.0, 389: 0.25, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 17:21:47,120 [INFO] [10] TRAIN  loss: 1.1961524897380988 acc: 0.9459393045828726
2025-01-18 17:21:47,120 [INFO] [10] TRAIN  loss dict: {'classification_loss': 1.1961524897380988}
2025-01-18 17:21:47,120 [INFO] [10] VALIDATION loss: 1.8802879097542369 VALIDATION acc: 0.7504702194357367
2025-01-18 17:21:47,120 [INFO] [10] VALIDATION loss dict: {'classification_loss': 1.8802879097542369}
2025-01-18 17:21:47,121 [INFO] 
2025-01-18 17:22:07,569 [INFO] Step[50/2713]: training loss : 1.1354149091243744 TRAIN  loss dict:  {'classification_loss': 1.1354149091243744}
2025-01-18 17:22:22,512 [INFO] Step[100/2713]: training loss : 1.1541533744335175 TRAIN  loss dict:  {'classification_loss': 1.1541533744335175}
2025-01-18 17:22:37,674 [INFO] Step[150/2713]: training loss : 1.1382036018371582 TRAIN  loss dict:  {'classification_loss': 1.1382036018371582}
2025-01-18 17:22:52,818 [INFO] Step[200/2713]: training loss : 1.1453603422641754 TRAIN  loss dict:  {'classification_loss': 1.1453603422641754}
2025-01-18 17:23:07,921 [INFO] Step[250/2713]: training loss : 1.1314408457279206 TRAIN  loss dict:  {'classification_loss': 1.1314408457279206}
2025-01-18 17:23:22,948 [INFO] Step[300/2713]: training loss : 1.135010496377945 TRAIN  loss dict:  {'classification_loss': 1.135010496377945}
2025-01-18 17:23:37,996 [INFO] Step[350/2713]: training loss : 1.1625037455558778 TRAIN  loss dict:  {'classification_loss': 1.1625037455558778}
2025-01-18 17:23:53,120 [INFO] Step[400/2713]: training loss : 1.1241287696361542 TRAIN  loss dict:  {'classification_loss': 1.1241287696361542}
2025-01-18 17:24:08,199 [INFO] Step[450/2713]: training loss : 1.1002817118167878 TRAIN  loss dict:  {'classification_loss': 1.1002817118167878}
2025-01-18 17:24:23,255 [INFO] Step[500/2713]: training loss : 1.1335875821113586 TRAIN  loss dict:  {'classification_loss': 1.1335875821113586}
2025-01-18 17:24:38,233 [INFO] Step[550/2713]: training loss : 1.1240293669700623 TRAIN  loss dict:  {'classification_loss': 1.1240293669700623}
2025-01-18 17:24:53,270 [INFO] Step[600/2713]: training loss : 1.1282252538204194 TRAIN  loss dict:  {'classification_loss': 1.1282252538204194}
2025-01-18 17:25:08,250 [INFO] Step[650/2713]: training loss : 1.1321075105667113 TRAIN  loss dict:  {'classification_loss': 1.1321075105667113}
2025-01-18 17:25:23,219 [INFO] Step[700/2713]: training loss : 1.1238993668556214 TRAIN  loss dict:  {'classification_loss': 1.1238993668556214}
2025-01-18 17:25:38,308 [INFO] Step[750/2713]: training loss : 1.0865177357196807 TRAIN  loss dict:  {'classification_loss': 1.0865177357196807}
2025-01-18 17:25:53,350 [INFO] Step[800/2713]: training loss : 1.1782138752937317 TRAIN  loss dict:  {'classification_loss': 1.1782138752937317}
2025-01-18 17:26:08,392 [INFO] Step[850/2713]: training loss : 1.1119782149791717 TRAIN  loss dict:  {'classification_loss': 1.1119782149791717}
2025-01-18 17:26:23,402 [INFO] Step[900/2713]: training loss : 1.1290345251560212 TRAIN  loss dict:  {'classification_loss': 1.1290345251560212}
2025-01-18 17:26:38,603 [INFO] Step[950/2713]: training loss : 1.1144271647930146 TRAIN  loss dict:  {'classification_loss': 1.1144271647930146}
2025-01-18 17:26:53,618 [INFO] Step[1000/2713]: training loss : 1.1170890963077544 TRAIN  loss dict:  {'classification_loss': 1.1170890963077544}
2025-01-18 17:27:08,637 [INFO] Step[1050/2713]: training loss : 1.1280780053138733 TRAIN  loss dict:  {'classification_loss': 1.1280780053138733}
2025-01-18 17:27:23,662 [INFO] Step[1100/2713]: training loss : 1.1674233436584474 TRAIN  loss dict:  {'classification_loss': 1.1674233436584474}
2025-01-18 17:27:38,704 [INFO] Step[1150/2713]: training loss : 1.1167890524864197 TRAIN  loss dict:  {'classification_loss': 1.1167890524864197}
2025-01-18 17:27:53,687 [INFO] Step[1200/2713]: training loss : 1.175355772972107 TRAIN  loss dict:  {'classification_loss': 1.175355772972107}
2025-01-18 17:28:08,769 [INFO] Step[1250/2713]: training loss : 1.105137709379196 TRAIN  loss dict:  {'classification_loss': 1.105137709379196}
2025-01-18 17:28:23,866 [INFO] Step[1300/2713]: training loss : 1.1024894428253174 TRAIN  loss dict:  {'classification_loss': 1.1024894428253174}
2025-01-18 17:28:38,835 [INFO] Step[1350/2713]: training loss : 1.2018910872936248 TRAIN  loss dict:  {'classification_loss': 1.2018910872936248}
2025-01-18 17:28:53,858 [INFO] Step[1400/2713]: training loss : 1.1448428559303283 TRAIN  loss dict:  {'classification_loss': 1.1448428559303283}
2025-01-18 17:29:08,965 [INFO] Step[1450/2713]: training loss : 1.134312915802002 TRAIN  loss dict:  {'classification_loss': 1.134312915802002}
2025-01-18 17:29:24,059 [INFO] Step[1500/2713]: training loss : 1.1421790945529937 TRAIN  loss dict:  {'classification_loss': 1.1421790945529937}
2025-01-18 17:29:39,127 [INFO] Step[1550/2713]: training loss : 1.1307653284072876 TRAIN  loss dict:  {'classification_loss': 1.1307653284072876}
2025-01-18 17:29:54,254 [INFO] Step[1600/2713]: training loss : 1.083865101337433 TRAIN  loss dict:  {'classification_loss': 1.083865101337433}
2025-01-18 17:30:09,368 [INFO] Step[1650/2713]: training loss : 1.1432310914993287 TRAIN  loss dict:  {'classification_loss': 1.1432310914993287}
2025-01-18 17:30:24,487 [INFO] Step[1700/2713]: training loss : 1.1251744747161865 TRAIN  loss dict:  {'classification_loss': 1.1251744747161865}
2025-01-18 17:30:39,587 [INFO] Step[1750/2713]: training loss : 1.1904667031764984 TRAIN  loss dict:  {'classification_loss': 1.1904667031764984}
2025-01-18 17:30:54,656 [INFO] Step[1800/2713]: training loss : 1.1275510537624358 TRAIN  loss dict:  {'classification_loss': 1.1275510537624358}
2025-01-18 17:31:09,752 [INFO] Step[1850/2713]: training loss : 1.152640186548233 TRAIN  loss dict:  {'classification_loss': 1.152640186548233}
2025-01-18 17:31:24,810 [INFO] Step[1900/2713]: training loss : 1.1153878247737885 TRAIN  loss dict:  {'classification_loss': 1.1153878247737885}
2025-01-18 17:31:39,966 [INFO] Step[1950/2713]: training loss : 1.2328645706176757 TRAIN  loss dict:  {'classification_loss': 1.2328645706176757}
2025-01-18 17:31:55,073 [INFO] Step[2000/2713]: training loss : 1.0963645660877228 TRAIN  loss dict:  {'classification_loss': 1.0963645660877228}
2025-01-18 17:32:10,190 [INFO] Step[2050/2713]: training loss : 1.151528171300888 TRAIN  loss dict:  {'classification_loss': 1.151528171300888}
2025-01-18 17:32:25,248 [INFO] Step[2100/2713]: training loss : 1.177751168012619 TRAIN  loss dict:  {'classification_loss': 1.177751168012619}
2025-01-18 17:32:40,336 [INFO] Step[2150/2713]: training loss : 1.2230717134475708 TRAIN  loss dict:  {'classification_loss': 1.2230717134475708}
2025-01-18 17:32:55,419 [INFO] Step[2200/2713]: training loss : 1.143655160665512 TRAIN  loss dict:  {'classification_loss': 1.143655160665512}
2025-01-18 17:33:10,487 [INFO] Step[2250/2713]: training loss : 1.1243172585964203 TRAIN  loss dict:  {'classification_loss': 1.1243172585964203}
2025-01-18 17:33:25,606 [INFO] Step[2300/2713]: training loss : 1.1967832112312318 TRAIN  loss dict:  {'classification_loss': 1.1967832112312318}
2025-01-18 17:33:40,689 [INFO] Step[2350/2713]: training loss : 1.1454215300083161 TRAIN  loss dict:  {'classification_loss': 1.1454215300083161}
2025-01-18 17:33:55,770 [INFO] Step[2400/2713]: training loss : 1.166647322177887 TRAIN  loss dict:  {'classification_loss': 1.166647322177887}
2025-01-18 17:34:10,887 [INFO] Step[2450/2713]: training loss : 1.1540206229686738 TRAIN  loss dict:  {'classification_loss': 1.1540206229686738}
2025-01-18 17:34:25,972 [INFO] Step[2500/2713]: training loss : 1.1235912597179414 TRAIN  loss dict:  {'classification_loss': 1.1235912597179414}
2025-01-18 17:34:41,028 [INFO] Step[2550/2713]: training loss : 1.151674530506134 TRAIN  loss dict:  {'classification_loss': 1.151674530506134}
2025-01-18 17:34:55,996 [INFO] Step[2600/2713]: training loss : 1.1634313547611237 TRAIN  loss dict:  {'classification_loss': 1.1634313547611237}
2025-01-18 17:35:11,056 [INFO] Step[2650/2713]: training loss : 1.130632004737854 TRAIN  loss dict:  {'classification_loss': 1.130632004737854}
2025-01-18 17:35:26,227 [INFO] Step[2700/2713]: training loss : 1.1554038441181183 TRAIN  loss dict:  {'classification_loss': 1.1554038441181183}
2025-01-18 17:36:46,836 [INFO] Label accuracies statistics:
2025-01-18 17:36:46,836 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.5, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.25, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.25, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.0, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 0.75, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 0.75, 141: 0.75, 142: 1.0, 143: 0.5, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.5, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.5, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.5, 186: 0.5, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.5, 205: 0.5, 206: 0.75, 207: 0.75, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.25, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.25, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.5, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.5, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.5, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.5, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.5, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.5, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-18 17:36:47,558 [INFO] [11] TRAIN  loss: 1.1411215917808386 acc: 0.964614817545153
2025-01-18 17:36:47,558 [INFO] [11] TRAIN  loss dict: {'classification_loss': 1.1411215917808386}
2025-01-18 17:36:47,559 [INFO] [11] VALIDATION loss: 1.8430897259622587 VALIDATION acc: 0.7561128526645768
2025-01-18 17:36:47,559 [INFO] [11] VALIDATION loss dict: {'classification_loss': 1.8430897259622587}
2025-01-18 17:36:47,559 [INFO] 
2025-01-18 17:37:07,586 [INFO] Step[50/2713]: training loss : 1.0763880252838134 TRAIN  loss dict:  {'classification_loss': 1.0763880252838134}
2025-01-18 17:37:22,565 [INFO] Step[100/2713]: training loss : 1.0757193160057068 TRAIN  loss dict:  {'classification_loss': 1.0757193160057068}
2025-01-18 17:37:37,494 [INFO] Step[150/2713]: training loss : 1.0927580201625824 TRAIN  loss dict:  {'classification_loss': 1.0927580201625824}
2025-01-18 17:37:52,548 [INFO] Step[200/2713]: training loss : 1.1035625958442687 TRAIN  loss dict:  {'classification_loss': 1.1035625958442687}
2025-01-18 17:38:07,607 [INFO] Step[250/2713]: training loss : 1.051375595331192 TRAIN  loss dict:  {'classification_loss': 1.051375595331192}
2025-01-18 17:38:22,614 [INFO] Step[300/2713]: training loss : 1.1322274959087373 TRAIN  loss dict:  {'classification_loss': 1.1322274959087373}
2025-01-18 17:38:37,686 [INFO] Step[350/2713]: training loss : 1.0877988600730897 TRAIN  loss dict:  {'classification_loss': 1.0877988600730897}
2025-01-18 17:38:52,725 [INFO] Step[400/2713]: training loss : 1.0982031452655792 TRAIN  loss dict:  {'classification_loss': 1.0982031452655792}
2025-01-18 17:39:07,664 [INFO] Step[450/2713]: training loss : 1.090657274723053 TRAIN  loss dict:  {'classification_loss': 1.090657274723053}
2025-01-18 17:39:22,691 [INFO] Step[500/2713]: training loss : 1.1026839125156402 TRAIN  loss dict:  {'classification_loss': 1.1026839125156402}
2025-01-18 17:39:37,704 [INFO] Step[550/2713]: training loss : 1.1731286251544952 TRAIN  loss dict:  {'classification_loss': 1.1731286251544952}
2025-01-18 17:39:52,697 [INFO] Step[600/2713]: training loss : 1.1029097259044647 TRAIN  loss dict:  {'classification_loss': 1.1029097259044647}
2025-01-18 17:40:07,773 [INFO] Step[650/2713]: training loss : 1.0772575187683104 TRAIN  loss dict:  {'classification_loss': 1.0772575187683104}
2025-01-18 17:40:22,795 [INFO] Step[700/2713]: training loss : 1.1196995377540588 TRAIN  loss dict:  {'classification_loss': 1.1196995377540588}
2025-01-18 17:40:37,748 [INFO] Step[750/2713]: training loss : 1.1366369676589967 TRAIN  loss dict:  {'classification_loss': 1.1366369676589967}
2025-01-18 17:40:52,737 [INFO] Step[800/2713]: training loss : 1.088737246990204 TRAIN  loss dict:  {'classification_loss': 1.088737246990204}
2025-01-18 17:41:07,718 [INFO] Step[850/2713]: training loss : 1.1095810747146606 TRAIN  loss dict:  {'classification_loss': 1.1095810747146606}
2025-01-18 17:41:22,666 [INFO] Step[900/2713]: training loss : 1.153900878429413 TRAIN  loss dict:  {'classification_loss': 1.153900878429413}
2025-01-18 17:41:37,612 [INFO] Step[950/2713]: training loss : 1.1326979088783264 TRAIN  loss dict:  {'classification_loss': 1.1326979088783264}
2025-01-18 17:41:52,557 [INFO] Step[1000/2713]: training loss : 1.1010779249668121 TRAIN  loss dict:  {'classification_loss': 1.1010779249668121}
2025-01-18 17:42:07,541 [INFO] Step[1050/2713]: training loss : 1.084529871940613 TRAIN  loss dict:  {'classification_loss': 1.084529871940613}
2025-01-18 17:42:22,557 [INFO] Step[1100/2713]: training loss : 1.1098334634304046 TRAIN  loss dict:  {'classification_loss': 1.1098334634304046}
2025-01-18 17:42:37,494 [INFO] Step[1150/2713]: training loss : 1.1140659058094025 TRAIN  loss dict:  {'classification_loss': 1.1140659058094025}
2025-01-18 17:42:52,494 [INFO] Step[1200/2713]: training loss : 1.094636652469635 TRAIN  loss dict:  {'classification_loss': 1.094636652469635}
2025-01-18 17:43:07,467 [INFO] Step[1250/2713]: training loss : 1.1798763537406922 TRAIN  loss dict:  {'classification_loss': 1.1798763537406922}
2025-01-18 17:43:22,460 [INFO] Step[1300/2713]: training loss : 1.1692443120479583 TRAIN  loss dict:  {'classification_loss': 1.1692443120479583}
2025-01-18 17:43:37,544 [INFO] Step[1350/2713]: training loss : 1.0905291211605073 TRAIN  loss dict:  {'classification_loss': 1.0905291211605073}
2025-01-18 17:43:52,656 [INFO] Step[1400/2713]: training loss : 1.1245802927017212 TRAIN  loss dict:  {'classification_loss': 1.1245802927017212}
2025-01-18 17:44:07,768 [INFO] Step[1450/2713]: training loss : 1.146470808982849 TRAIN  loss dict:  {'classification_loss': 1.146470808982849}
2025-01-18 17:44:22,884 [INFO] Step[1500/2713]: training loss : 1.1024884164333344 TRAIN  loss dict:  {'classification_loss': 1.1024884164333344}
2025-01-18 17:44:38,035 [INFO] Step[1550/2713]: training loss : 1.1272161519527435 TRAIN  loss dict:  {'classification_loss': 1.1272161519527435}
2025-01-18 17:44:53,145 [INFO] Step[1600/2713]: training loss : 1.165828869342804 TRAIN  loss dict:  {'classification_loss': 1.165828869342804}
2025-01-18 17:45:08,272 [INFO] Step[1650/2713]: training loss : 1.0655651533603667 TRAIN  loss dict:  {'classification_loss': 1.0655651533603667}
2025-01-18 17:45:23,352 [INFO] Step[1700/2713]: training loss : 1.0747747695446015 TRAIN  loss dict:  {'classification_loss': 1.0747747695446015}
2025-01-18 17:45:38,471 [INFO] Step[1750/2713]: training loss : 1.102101322412491 TRAIN  loss dict:  {'classification_loss': 1.102101322412491}
2025-01-18 17:45:53,574 [INFO] Step[1800/2713]: training loss : 1.1295491778850555 TRAIN  loss dict:  {'classification_loss': 1.1295491778850555}
2025-01-18 17:46:08,672 [INFO] Step[1850/2713]: training loss : 1.161496675014496 TRAIN  loss dict:  {'classification_loss': 1.161496675014496}
2025-01-18 17:46:23,782 [INFO] Step[1900/2713]: training loss : 1.0997374737262726 TRAIN  loss dict:  {'classification_loss': 1.0997374737262726}
2025-01-18 17:46:38,839 [INFO] Step[1950/2713]: training loss : 1.1530267989635468 TRAIN  loss dict:  {'classification_loss': 1.1530267989635468}
2025-01-18 17:46:53,927 [INFO] Step[2000/2713]: training loss : 1.0999435114860534 TRAIN  loss dict:  {'classification_loss': 1.0999435114860534}
2025-01-18 17:47:09,032 [INFO] Step[2050/2713]: training loss : 1.1532626271247863 TRAIN  loss dict:  {'classification_loss': 1.1532626271247863}
2025-01-18 17:47:24,109 [INFO] Step[2100/2713]: training loss : 1.1721231818199158 TRAIN  loss dict:  {'classification_loss': 1.1721231818199158}
2025-01-18 17:47:39,214 [INFO] Step[2150/2713]: training loss : 1.1712247490882874 TRAIN  loss dict:  {'classification_loss': 1.1712247490882874}
2025-01-18 17:47:54,305 [INFO] Step[2200/2713]: training loss : 1.1568124866485596 TRAIN  loss dict:  {'classification_loss': 1.1568124866485596}
2025-01-18 17:48:09,402 [INFO] Step[2250/2713]: training loss : 1.1270832681655885 TRAIN  loss dict:  {'classification_loss': 1.1270832681655885}
2025-01-18 17:48:24,467 [INFO] Step[2300/2713]: training loss : 1.0909075832366943 TRAIN  loss dict:  {'classification_loss': 1.0909075832366943}
2025-01-18 17:48:39,532 [INFO] Step[2350/2713]: training loss : 1.1349488651752473 TRAIN  loss dict:  {'classification_loss': 1.1349488651752473}
2025-01-18 17:48:54,507 [INFO] Step[2400/2713]: training loss : 1.1226159346103668 TRAIN  loss dict:  {'classification_loss': 1.1226159346103668}
2025-01-18 17:49:09,595 [INFO] Step[2450/2713]: training loss : 1.1350064980983734 TRAIN  loss dict:  {'classification_loss': 1.1350064980983734}
2025-01-18 17:49:24,723 [INFO] Step[2500/2713]: training loss : 1.133403903245926 TRAIN  loss dict:  {'classification_loss': 1.133403903245926}
2025-01-18 17:49:39,834 [INFO] Step[2550/2713]: training loss : 1.1410384845733643 TRAIN  loss dict:  {'classification_loss': 1.1410384845733643}
2025-01-18 17:49:54,952 [INFO] Step[2600/2713]: training loss : 1.1389007592201232 TRAIN  loss dict:  {'classification_loss': 1.1389007592201232}
2025-01-18 17:50:09,964 [INFO] Step[2650/2713]: training loss : 1.1378198671340942 TRAIN  loss dict:  {'classification_loss': 1.1378198671340942}
2025-01-18 17:50:24,961 [INFO] Step[2700/2713]: training loss : 1.1292068445682526 TRAIN  loss dict:  {'classification_loss': 1.1292068445682526}
2025-01-18 17:51:45,125 [INFO] Label accuracies statistics:
2025-01-18 17:51:45,125 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.5, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.25, 60: 0.5, 61: 1.0, 62: 0.5, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.25, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.5, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 0.75, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.5, 143: 1.0, 144: 0.75, 145: 0.5, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 0.75, 160: 0.5, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.25, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 0.5, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.25, 209: 0.75, 210: 0.5, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.25, 218: 0.5, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.25, 230: 1.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.5, 245: 0.5, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 1.0, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.5, 300: 1.0, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.0, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.75, 314: 1.0, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.5, 331: 0.75, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.25, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.25, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 0.5, 381: 0.75, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 17:51:45,127 [INFO] [12] TRAIN  loss: 1.1197422948263247 acc: 0.9701437523037229
2025-01-18 17:51:45,127 [INFO] [12] TRAIN  loss dict: {'classification_loss': 1.1197422948263247}
2025-01-18 17:51:45,127 [INFO] [12] VALIDATION loss: 1.852705237896819 VALIDATION acc: 0.7554858934169278
2025-01-18 17:51:45,127 [INFO] [12] VALIDATION loss dict: {'classification_loss': 1.852705237896819}
2025-01-18 17:51:45,128 [INFO] 
2025-01-18 17:52:05,942 [INFO] Step[50/2713]: training loss : 1.0613092231750487 TRAIN  loss dict:  {'classification_loss': 1.0613092231750487}
2025-01-18 17:52:21,054 [INFO] Step[100/2713]: training loss : 1.1143328392505645 TRAIN  loss dict:  {'classification_loss': 1.1143328392505645}
2025-01-18 17:52:36,139 [INFO] Step[150/2713]: training loss : 1.1242387413978576 TRAIN  loss dict:  {'classification_loss': 1.1242387413978576}
2025-01-18 17:52:51,231 [INFO] Step[200/2713]: training loss : 1.1136419916152953 TRAIN  loss dict:  {'classification_loss': 1.1136419916152953}
2025-01-18 17:53:06,338 [INFO] Step[250/2713]: training loss : 1.114127037525177 TRAIN  loss dict:  {'classification_loss': 1.114127037525177}
2025-01-18 17:53:21,415 [INFO] Step[300/2713]: training loss : 1.1193631434440612 TRAIN  loss dict:  {'classification_loss': 1.1193631434440612}
2025-01-18 17:53:36,477 [INFO] Step[350/2713]: training loss : 1.1077164721488952 TRAIN  loss dict:  {'classification_loss': 1.1077164721488952}
2025-01-18 17:53:51,555 [INFO] Step[400/2713]: training loss : 1.097177449464798 TRAIN  loss dict:  {'classification_loss': 1.097177449464798}
2025-01-18 17:54:06,620 [INFO] Step[450/2713]: training loss : 1.1021223342418671 TRAIN  loss dict:  {'classification_loss': 1.1021223342418671}
2025-01-18 17:54:21,723 [INFO] Step[500/2713]: training loss : 1.0901846444606782 TRAIN  loss dict:  {'classification_loss': 1.0901846444606782}
2025-01-18 17:54:36,827 [INFO] Step[550/2713]: training loss : 1.1038678109645843 TRAIN  loss dict:  {'classification_loss': 1.1038678109645843}
2025-01-18 17:54:51,900 [INFO] Step[600/2713]: training loss : 1.1078741657733917 TRAIN  loss dict:  {'classification_loss': 1.1078741657733917}
2025-01-18 17:55:07,086 [INFO] Step[650/2713]: training loss : 1.1113852858543396 TRAIN  loss dict:  {'classification_loss': 1.1113852858543396}
2025-01-18 17:55:22,324 [INFO] Step[700/2713]: training loss : 1.1152896225452422 TRAIN  loss dict:  {'classification_loss': 1.1152896225452422}
2025-01-18 17:55:37,560 [INFO] Step[750/2713]: training loss : 1.0889346563816071 TRAIN  loss dict:  {'classification_loss': 1.0889346563816071}
2025-01-18 17:55:52,769 [INFO] Step[800/2713]: training loss : 1.106307281255722 TRAIN  loss dict:  {'classification_loss': 1.106307281255722}
2025-01-18 17:56:07,911 [INFO] Step[850/2713]: training loss : 1.071719502210617 TRAIN  loss dict:  {'classification_loss': 1.071719502210617}
2025-01-18 17:56:23,088 [INFO] Step[900/2713]: training loss : 1.0907963120937347 TRAIN  loss dict:  {'classification_loss': 1.0907963120937347}
2025-01-18 17:56:38,285 [INFO] Step[950/2713]: training loss : 1.0739235949516297 TRAIN  loss dict:  {'classification_loss': 1.0739235949516297}
2025-01-18 17:56:53,448 [INFO] Step[1000/2713]: training loss : 1.0835833024978638 TRAIN  loss dict:  {'classification_loss': 1.0835833024978638}
2025-01-18 17:57:08,695 [INFO] Step[1050/2713]: training loss : 1.094679307937622 TRAIN  loss dict:  {'classification_loss': 1.094679307937622}
2025-01-18 17:57:23,819 [INFO] Step[1100/2713]: training loss : 1.1030896162986756 TRAIN  loss dict:  {'classification_loss': 1.1030896162986756}
2025-01-18 17:57:38,963 [INFO] Step[1150/2713]: training loss : 1.1367790496349335 TRAIN  loss dict:  {'classification_loss': 1.1367790496349335}
2025-01-18 17:57:54,049 [INFO] Step[1200/2713]: training loss : 1.0985914623737336 TRAIN  loss dict:  {'classification_loss': 1.0985914623737336}
2025-01-18 17:58:09,180 [INFO] Step[1250/2713]: training loss : 1.1101781976222993 TRAIN  loss dict:  {'classification_loss': 1.1101781976222993}
2025-01-18 17:58:24,333 [INFO] Step[1300/2713]: training loss : 1.1395690286159514 TRAIN  loss dict:  {'classification_loss': 1.1395690286159514}
2025-01-18 17:58:39,426 [INFO] Step[1350/2713]: training loss : 1.101806535720825 TRAIN  loss dict:  {'classification_loss': 1.101806535720825}
2025-01-18 17:58:54,513 [INFO] Step[1400/2713]: training loss : 1.1303127121925354 TRAIN  loss dict:  {'classification_loss': 1.1303127121925354}
2025-01-18 17:59:09,687 [INFO] Step[1450/2713]: training loss : 1.1207308185100555 TRAIN  loss dict:  {'classification_loss': 1.1207308185100555}
2025-01-18 17:59:24,836 [INFO] Step[1500/2713]: training loss : 1.0711085021495819 TRAIN  loss dict:  {'classification_loss': 1.0711085021495819}
2025-01-18 17:59:40,020 [INFO] Step[1550/2713]: training loss : 1.1029282319545746 TRAIN  loss dict:  {'classification_loss': 1.1029282319545746}
2025-01-18 17:59:55,100 [INFO] Step[1600/2713]: training loss : 1.1345072078704834 TRAIN  loss dict:  {'classification_loss': 1.1345072078704834}
2025-01-18 18:00:10,201 [INFO] Step[1650/2713]: training loss : 1.1305724453926087 TRAIN  loss dict:  {'classification_loss': 1.1305724453926087}
2025-01-18 18:00:25,391 [INFO] Step[1700/2713]: training loss : 1.1318146538734437 TRAIN  loss dict:  {'classification_loss': 1.1318146538734437}
2025-01-18 18:00:40,549 [INFO] Step[1750/2713]: training loss : 1.1119176292419433 TRAIN  loss dict:  {'classification_loss': 1.1119176292419433}
2025-01-18 18:00:55,663 [INFO] Step[1800/2713]: training loss : 1.1051983296871186 TRAIN  loss dict:  {'classification_loss': 1.1051983296871186}
2025-01-18 18:01:10,728 [INFO] Step[1850/2713]: training loss : 1.0742921257019042 TRAIN  loss dict:  {'classification_loss': 1.0742921257019042}
2025-01-18 18:01:25,767 [INFO] Step[1900/2713]: training loss : 1.1106805753707887 TRAIN  loss dict:  {'classification_loss': 1.1106805753707887}
2025-01-18 18:01:40,881 [INFO] Step[1950/2713]: training loss : 1.1112521541118623 TRAIN  loss dict:  {'classification_loss': 1.1112521541118623}
2025-01-18 18:01:56,045 [INFO] Step[2000/2713]: training loss : 1.137677310705185 TRAIN  loss dict:  {'classification_loss': 1.137677310705185}
2025-01-18 18:02:11,192 [INFO] Step[2050/2713]: training loss : 1.0809986400604248 TRAIN  loss dict:  {'classification_loss': 1.0809986400604248}
2025-01-18 18:02:26,336 [INFO] Step[2100/2713]: training loss : 1.0722397029399873 TRAIN  loss dict:  {'classification_loss': 1.0722397029399873}
2025-01-18 18:02:41,511 [INFO] Step[2150/2713]: training loss : 1.0543088603019715 TRAIN  loss dict:  {'classification_loss': 1.0543088603019715}
2025-01-18 18:02:56,664 [INFO] Step[2200/2713]: training loss : 1.1406745076179505 TRAIN  loss dict:  {'classification_loss': 1.1406745076179505}
2025-01-18 18:03:11,788 [INFO] Step[2250/2713]: training loss : 1.0776717948913574 TRAIN  loss dict:  {'classification_loss': 1.0776717948913574}
2025-01-18 18:03:26,947 [INFO] Step[2300/2713]: training loss : 1.0752330374717713 TRAIN  loss dict:  {'classification_loss': 1.0752330374717713}
2025-01-18 18:03:42,109 [INFO] Step[2350/2713]: training loss : 1.1172058403491973 TRAIN  loss dict:  {'classification_loss': 1.1172058403491973}
2025-01-18 18:03:57,237 [INFO] Step[2400/2713]: training loss : 1.1087923908233643 TRAIN  loss dict:  {'classification_loss': 1.1087923908233643}
2025-01-18 18:04:12,442 [INFO] Step[2450/2713]: training loss : 1.0942266368865967 TRAIN  loss dict:  {'classification_loss': 1.0942266368865967}
2025-01-18 18:04:27,616 [INFO] Step[2500/2713]: training loss : 1.0528275179862976 TRAIN  loss dict:  {'classification_loss': 1.0528275179862976}
2025-01-18 18:04:42,784 [INFO] Step[2550/2713]: training loss : 1.1438593339920045 TRAIN  loss dict:  {'classification_loss': 1.1438593339920045}
2025-01-18 18:04:57,918 [INFO] Step[2600/2713]: training loss : 1.1423824739456176 TRAIN  loss dict:  {'classification_loss': 1.1423824739456176}
2025-01-18 18:05:12,988 [INFO] Step[2650/2713]: training loss : 1.1347193956375121 TRAIN  loss dict:  {'classification_loss': 1.1347193956375121}
2025-01-18 18:05:28,134 [INFO] Step[2700/2713]: training loss : 1.1189728486537933 TRAIN  loss dict:  {'classification_loss': 1.1189728486537933}
2025-01-18 18:06:48,359 [INFO] Label accuracies statistics:
2025-01-18 18:06:48,360 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 0.75, 39: 1.0, 40: 1.0, 41: 0.5, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.25, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.25, 209: 0.5, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.0, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.5, 312: 0.5, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.5, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.5, 378: 0.25, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.0, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 18:06:49,668 [INFO] [13] TRAIN  loss: 1.1054522403702278 acc: 0.9722324609902937
2025-01-18 18:06:49,668 [INFO] [13] TRAIN  loss dict: {'classification_loss': 1.1054522403702278}
2025-01-18 18:06:49,669 [INFO] [13] VALIDATION loss: 1.8022251875328839 VALIDATION acc: 0.774294670846395
2025-01-18 18:06:49,669 [INFO] [13] VALIDATION loss dict: {'classification_loss': 1.8022251875328839}
2025-01-18 18:06:49,669 [INFO] 
2025-01-18 18:07:10,010 [INFO] Step[50/2713]: training loss : 1.0628348660469056 TRAIN  loss dict:  {'classification_loss': 1.0628348660469056}
2025-01-18 18:07:25,102 [INFO] Step[100/2713]: training loss : 1.0483350765705108 TRAIN  loss dict:  {'classification_loss': 1.0483350765705108}
2025-01-18 18:07:40,198 [INFO] Step[150/2713]: training loss : 1.090182249546051 TRAIN  loss dict:  {'classification_loss': 1.090182249546051}
2025-01-18 18:07:55,323 [INFO] Step[200/2713]: training loss : 1.0832283425331115 TRAIN  loss dict:  {'classification_loss': 1.0832283425331115}
2025-01-18 18:08:10,456 [INFO] Step[250/2713]: training loss : 1.1179008424282073 TRAIN  loss dict:  {'classification_loss': 1.1179008424282073}
2025-01-18 18:08:25,613 [INFO] Step[300/2713]: training loss : 1.116426364183426 TRAIN  loss dict:  {'classification_loss': 1.116426364183426}
2025-01-18 18:08:40,722 [INFO] Step[350/2713]: training loss : 1.1448466825485228 TRAIN  loss dict:  {'classification_loss': 1.1448466825485228}
2025-01-18 18:08:55,828 [INFO] Step[400/2713]: training loss : 1.1067741107940674 TRAIN  loss dict:  {'classification_loss': 1.1067741107940674}
2025-01-18 18:09:10,951 [INFO] Step[450/2713]: training loss : 1.1126745223999024 TRAIN  loss dict:  {'classification_loss': 1.1126745223999024}
2025-01-18 18:09:26,013 [INFO] Step[500/2713]: training loss : 1.1251578080654143 TRAIN  loss dict:  {'classification_loss': 1.1251578080654143}
2025-01-18 18:09:41,138 [INFO] Step[550/2713]: training loss : 1.1069852077960969 TRAIN  loss dict:  {'classification_loss': 1.1069852077960969}
2025-01-18 18:09:56,240 [INFO] Step[600/2713]: training loss : 1.1044138085842132 TRAIN  loss dict:  {'classification_loss': 1.1044138085842132}
2025-01-18 18:10:11,295 [INFO] Step[650/2713]: training loss : 1.080204986333847 TRAIN  loss dict:  {'classification_loss': 1.080204986333847}
2025-01-18 18:10:26,411 [INFO] Step[700/2713]: training loss : 1.1304683375358582 TRAIN  loss dict:  {'classification_loss': 1.1304683375358582}
2025-01-18 18:10:41,546 [INFO] Step[750/2713]: training loss : 1.1216474711894988 TRAIN  loss dict:  {'classification_loss': 1.1216474711894988}
2025-01-18 18:10:56,682 [INFO] Step[800/2713]: training loss : 1.1034424102306366 TRAIN  loss dict:  {'classification_loss': 1.1034424102306366}
2025-01-18 18:11:11,744 [INFO] Step[850/2713]: training loss : 1.0804467391967774 TRAIN  loss dict:  {'classification_loss': 1.0804467391967774}
2025-01-18 18:11:26,898 [INFO] Step[900/2713]: training loss : 1.1074020862579346 TRAIN  loss dict:  {'classification_loss': 1.1074020862579346}
2025-01-18 18:11:41,966 [INFO] Step[950/2713]: training loss : 1.0882207655906677 TRAIN  loss dict:  {'classification_loss': 1.0882207655906677}
2025-01-18 18:11:57,008 [INFO] Step[1000/2713]: training loss : 1.1088501906394959 TRAIN  loss dict:  {'classification_loss': 1.1088501906394959}
2025-01-18 18:12:12,086 [INFO] Step[1050/2713]: training loss : 1.1398194253444671 TRAIN  loss dict:  {'classification_loss': 1.1398194253444671}
2025-01-18 18:12:27,230 [INFO] Step[1100/2713]: training loss : 1.0681368470191956 TRAIN  loss dict:  {'classification_loss': 1.0681368470191956}
2025-01-18 18:12:42,337 [INFO] Step[1150/2713]: training loss : 1.0941442000865935 TRAIN  loss dict:  {'classification_loss': 1.0941442000865935}
2025-01-18 18:12:57,430 [INFO] Step[1200/2713]: training loss : 1.1061524200439452 TRAIN  loss dict:  {'classification_loss': 1.1061524200439452}
2025-01-18 18:13:12,461 [INFO] Step[1250/2713]: training loss : 1.0971275579929352 TRAIN  loss dict:  {'classification_loss': 1.0971275579929352}
2025-01-18 18:13:27,517 [INFO] Step[1300/2713]: training loss : 1.1380129528045655 TRAIN  loss dict:  {'classification_loss': 1.1380129528045655}
2025-01-18 18:13:42,614 [INFO] Step[1350/2713]: training loss : 1.067939409017563 TRAIN  loss dict:  {'classification_loss': 1.067939409017563}
2025-01-18 18:13:57,697 [INFO] Step[1400/2713]: training loss : 1.0991555511951447 TRAIN  loss dict:  {'classification_loss': 1.0991555511951447}
2025-01-18 18:14:12,820 [INFO] Step[1450/2713]: training loss : 1.076421810388565 TRAIN  loss dict:  {'classification_loss': 1.076421810388565}
2025-01-18 18:14:27,951 [INFO] Step[1500/2713]: training loss : 1.0866209053993225 TRAIN  loss dict:  {'classification_loss': 1.0866209053993225}
2025-01-18 18:14:43,088 [INFO] Step[1550/2713]: training loss : 1.0901602721214294 TRAIN  loss dict:  {'classification_loss': 1.0901602721214294}
2025-01-18 18:14:58,174 [INFO] Step[1600/2713]: training loss : 1.0972654032707214 TRAIN  loss dict:  {'classification_loss': 1.0972654032707214}
2025-01-18 18:15:13,336 [INFO] Step[1650/2713]: training loss : 1.0810645592212678 TRAIN  loss dict:  {'classification_loss': 1.0810645592212678}
2025-01-18 18:15:28,462 [INFO] Step[1700/2713]: training loss : 1.1214422583580017 TRAIN  loss dict:  {'classification_loss': 1.1214422583580017}
2025-01-18 18:15:43,543 [INFO] Step[1750/2713]: training loss : 1.1188229310512543 TRAIN  loss dict:  {'classification_loss': 1.1188229310512543}
2025-01-18 18:15:58,647 [INFO] Step[1800/2713]: training loss : 1.0999719750881196 TRAIN  loss dict:  {'classification_loss': 1.0999719750881196}
2025-01-18 18:16:13,752 [INFO] Step[1850/2713]: training loss : 1.1439166116714476 TRAIN  loss dict:  {'classification_loss': 1.1439166116714476}
2025-01-18 18:16:28,846 [INFO] Step[1900/2713]: training loss : 1.075038126707077 TRAIN  loss dict:  {'classification_loss': 1.075038126707077}
2025-01-18 18:16:43,964 [INFO] Step[1950/2713]: training loss : 1.118191044330597 TRAIN  loss dict:  {'classification_loss': 1.118191044330597}
2025-01-18 18:16:59,045 [INFO] Step[2000/2713]: training loss : 1.1547943937778473 TRAIN  loss dict:  {'classification_loss': 1.1547943937778473}
2025-01-18 18:17:14,098 [INFO] Step[2050/2713]: training loss : 1.0760512459278107 TRAIN  loss dict:  {'classification_loss': 1.0760512459278107}
2025-01-18 18:17:29,261 [INFO] Step[2100/2713]: training loss : 1.0722716915607453 TRAIN  loss dict:  {'classification_loss': 1.0722716915607453}
2025-01-18 18:17:44,376 [INFO] Step[2150/2713]: training loss : 1.0705650854110718 TRAIN  loss dict:  {'classification_loss': 1.0705650854110718}
2025-01-18 18:17:59,445 [INFO] Step[2200/2713]: training loss : 1.0889634788036346 TRAIN  loss dict:  {'classification_loss': 1.0889634788036346}
2025-01-18 18:18:14,479 [INFO] Step[2250/2713]: training loss : 1.0874136698246002 TRAIN  loss dict:  {'classification_loss': 1.0874136698246002}
2025-01-18 18:18:29,607 [INFO] Step[2300/2713]: training loss : 1.0537700212001802 TRAIN  loss dict:  {'classification_loss': 1.0537700212001802}
2025-01-18 18:18:44,693 [INFO] Step[2350/2713]: training loss : 1.1132863116264344 TRAIN  loss dict:  {'classification_loss': 1.1132863116264344}
2025-01-18 18:18:59,761 [INFO] Step[2400/2713]: training loss : 1.0539069712162017 TRAIN  loss dict:  {'classification_loss': 1.0539069712162017}
2025-01-18 18:19:14,804 [INFO] Step[2450/2713]: training loss : 1.1178707265853882 TRAIN  loss dict:  {'classification_loss': 1.1178707265853882}
2025-01-18 18:19:29,827 [INFO] Step[2500/2713]: training loss : 1.1116085028648377 TRAIN  loss dict:  {'classification_loss': 1.1116085028648377}
2025-01-18 18:19:44,959 [INFO] Step[2550/2713]: training loss : 1.1137815976142884 TRAIN  loss dict:  {'classification_loss': 1.1137815976142884}
2025-01-18 18:20:00,087 [INFO] Step[2600/2713]: training loss : 1.1009015798568726 TRAIN  loss dict:  {'classification_loss': 1.1009015798568726}
2025-01-18 18:20:15,189 [INFO] Step[2650/2713]: training loss : 1.0739226615428925 TRAIN  loss dict:  {'classification_loss': 1.0739226615428925}
2025-01-18 18:20:30,295 [INFO] Step[2700/2713]: training loss : 1.0615627765655518 TRAIN  loss dict:  {'classification_loss': 1.0615627765655518}
2025-01-18 18:21:50,234 [INFO] Label accuracies statistics:
2025-01-18 18:21:50,235 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.0, 17: 0.0, 18: 0.75, 19: 0.25, 20: 0.5, 21: 0.5, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.5, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.25, 60: 0.5, 61: 1.0, 62: 0.5, 63: 0.25, 64: 1.0, 65: 0.75, 66: 0.25, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.25, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 0.5, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.5, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 1.0, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.0, 204: 0.75, 205: 0.5, 206: 0.75, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.5, 228: 0.75, 229: 0.0, 230: 1.0, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 0.75, 242: 0.75, 243: 0.0, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.25, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.5, 288: 1.0, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.25, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.25, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.25, 346: 0.5, 347: 1.0, 348: 0.75, 349: 0.25, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.75, 376: 1.0, 377: 0.5, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.5, 391: 1.0, 392: 1.0, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-18 18:21:50,236 [INFO] [14] TRAIN  loss: 1.0982562684503112 acc: 0.9740754392431502
2025-01-18 18:21:50,236 [INFO] [14] TRAIN  loss dict: {'classification_loss': 1.0982562684503112}
2025-01-18 18:21:50,237 [INFO] [14] VALIDATION loss: 1.9127338166747774 VALIDATION acc: 0.74858934169279
2025-01-18 18:21:50,237 [INFO] [14] VALIDATION loss dict: {'classification_loss': 1.9127338166747774}
2025-01-18 18:21:50,237 [INFO] 
2025-01-18 18:22:10,369 [INFO] Step[50/2713]: training loss : 1.069749232530594 TRAIN  loss dict:  {'classification_loss': 1.069749232530594}
2025-01-18 18:22:25,375 [INFO] Step[100/2713]: training loss : 1.0939084470272065 TRAIN  loss dict:  {'classification_loss': 1.0939084470272065}
2025-01-18 18:22:40,404 [INFO] Step[150/2713]: training loss : 1.0704244732856751 TRAIN  loss dict:  {'classification_loss': 1.0704244732856751}
2025-01-18 18:22:55,406 [INFO] Step[200/2713]: training loss : 1.0845953953266143 TRAIN  loss dict:  {'classification_loss': 1.0845953953266143}
2025-01-18 18:23:10,385 [INFO] Step[250/2713]: training loss : 1.0647580492496491 TRAIN  loss dict:  {'classification_loss': 1.0647580492496491}
2025-01-18 18:23:25,363 [INFO] Step[300/2713]: training loss : 1.0971188056468963 TRAIN  loss dict:  {'classification_loss': 1.0971188056468963}
2025-01-18 18:23:40,362 [INFO] Step[350/2713]: training loss : 1.0899014973640442 TRAIN  loss dict:  {'classification_loss': 1.0899014973640442}
2025-01-18 18:23:55,375 [INFO] Step[400/2713]: training loss : 1.0887030529975892 TRAIN  loss dict:  {'classification_loss': 1.0887030529975892}
2025-01-18 18:24:10,421 [INFO] Step[450/2713]: training loss : 1.052796356678009 TRAIN  loss dict:  {'classification_loss': 1.052796356678009}
2025-01-18 18:24:25,422 [INFO] Step[500/2713]: training loss : 1.0894048476219178 TRAIN  loss dict:  {'classification_loss': 1.0894048476219178}
2025-01-18 18:24:40,419 [INFO] Step[550/2713]: training loss : 1.0903707540035248 TRAIN  loss dict:  {'classification_loss': 1.0903707540035248}
2025-01-18 18:24:55,403 [INFO] Step[600/2713]: training loss : 1.141066508293152 TRAIN  loss dict:  {'classification_loss': 1.141066508293152}
2025-01-18 18:25:10,449 [INFO] Step[650/2713]: training loss : 1.1129184341430665 TRAIN  loss dict:  {'classification_loss': 1.1129184341430665}
2025-01-18 18:25:25,398 [INFO] Step[700/2713]: training loss : 1.0911093723773957 TRAIN  loss dict:  {'classification_loss': 1.0911093723773957}
2025-01-18 18:25:40,432 [INFO] Step[750/2713]: training loss : 1.0341715621948242 TRAIN  loss dict:  {'classification_loss': 1.0341715621948242}
2025-01-18 18:25:55,481 [INFO] Step[800/2713]: training loss : 1.0936280250549317 TRAIN  loss dict:  {'classification_loss': 1.0936280250549317}
2025-01-18 18:26:10,508 [INFO] Step[850/2713]: training loss : 1.0530233681201935 TRAIN  loss dict:  {'classification_loss': 1.0530233681201935}
2025-01-18 18:26:25,517 [INFO] Step[900/2713]: training loss : 1.093711633682251 TRAIN  loss dict:  {'classification_loss': 1.093711633682251}
2025-01-18 18:26:40,467 [INFO] Step[950/2713]: training loss : 1.0825900781154632 TRAIN  loss dict:  {'classification_loss': 1.0825900781154632}
2025-01-18 18:26:55,522 [INFO] Step[1000/2713]: training loss : 1.0462923288345336 TRAIN  loss dict:  {'classification_loss': 1.0462923288345336}
2025-01-18 18:27:10,469 [INFO] Step[1050/2713]: training loss : 1.1243983268737794 TRAIN  loss dict:  {'classification_loss': 1.1243983268737794}
2025-01-18 18:27:25,434 [INFO] Step[1100/2713]: training loss : 1.1122274243831634 TRAIN  loss dict:  {'classification_loss': 1.1122274243831634}
2025-01-18 18:27:40,408 [INFO] Step[1150/2713]: training loss : 1.0804176068305968 TRAIN  loss dict:  {'classification_loss': 1.0804176068305968}
2025-01-18 18:27:55,425 [INFO] Step[1200/2713]: training loss : 1.0348805618286132 TRAIN  loss dict:  {'classification_loss': 1.0348805618286132}
2025-01-18 18:28:10,439 [INFO] Step[1250/2713]: training loss : 1.0850734102725983 TRAIN  loss dict:  {'classification_loss': 1.0850734102725983}
2025-01-18 18:28:25,394 [INFO] Step[1300/2713]: training loss : 1.0870577812194824 TRAIN  loss dict:  {'classification_loss': 1.0870577812194824}
2025-01-18 18:28:40,403 [INFO] Step[1350/2713]: training loss : 1.076023906469345 TRAIN  loss dict:  {'classification_loss': 1.076023906469345}
2025-01-18 18:28:55,385 [INFO] Step[1400/2713]: training loss : 1.0795188629627228 TRAIN  loss dict:  {'classification_loss': 1.0795188629627228}
2025-01-18 18:29:10,337 [INFO] Step[1450/2713]: training loss : 1.0590733897686004 TRAIN  loss dict:  {'classification_loss': 1.0590733897686004}
2025-01-18 18:29:25,364 [INFO] Step[1500/2713]: training loss : 1.081262788772583 TRAIN  loss dict:  {'classification_loss': 1.081262788772583}
2025-01-18 18:29:40,420 [INFO] Step[1550/2713]: training loss : 1.0789339673519134 TRAIN  loss dict:  {'classification_loss': 1.0789339673519134}
2025-01-18 18:29:55,413 [INFO] Step[1600/2713]: training loss : 1.0932096755504608 TRAIN  loss dict:  {'classification_loss': 1.0932096755504608}
2025-01-18 18:30:10,470 [INFO] Step[1650/2713]: training loss : 1.0899737060070038 TRAIN  loss dict:  {'classification_loss': 1.0899737060070038}
2025-01-18 18:30:25,425 [INFO] Step[1700/2713]: training loss : 1.1186471855640412 TRAIN  loss dict:  {'classification_loss': 1.1186471855640412}
2025-01-18 18:30:40,397 [INFO] Step[1750/2713]: training loss : 1.076200751066208 TRAIN  loss dict:  {'classification_loss': 1.076200751066208}
2025-01-18 18:30:55,384 [INFO] Step[1800/2713]: training loss : 1.0996146309375763 TRAIN  loss dict:  {'classification_loss': 1.0996146309375763}
2025-01-18 18:31:10,415 [INFO] Step[1850/2713]: training loss : 1.1121060132980347 TRAIN  loss dict:  {'classification_loss': 1.1121060132980347}
2025-01-18 18:31:25,460 [INFO] Step[1900/2713]: training loss : 1.0976291191577912 TRAIN  loss dict:  {'classification_loss': 1.0976291191577912}
2025-01-18 18:31:40,442 [INFO] Step[1950/2713]: training loss : 1.076198891401291 TRAIN  loss dict:  {'classification_loss': 1.076198891401291}
2025-01-18 18:31:55,456 [INFO] Step[2000/2713]: training loss : 1.106144242286682 TRAIN  loss dict:  {'classification_loss': 1.106144242286682}
2025-01-18 18:32:10,487 [INFO] Step[2050/2713]: training loss : 1.116231645345688 TRAIN  loss dict:  {'classification_loss': 1.116231645345688}
2025-01-18 18:32:25,516 [INFO] Step[2100/2713]: training loss : 1.1144886064529418 TRAIN  loss dict:  {'classification_loss': 1.1144886064529418}
2025-01-18 18:32:40,474 [INFO] Step[2150/2713]: training loss : 1.1070558035373688 TRAIN  loss dict:  {'classification_loss': 1.1070558035373688}
2025-01-18 18:32:55,524 [INFO] Step[2200/2713]: training loss : 1.1031972813606261 TRAIN  loss dict:  {'classification_loss': 1.1031972813606261}
2025-01-18 18:33:10,510 [INFO] Step[2250/2713]: training loss : 1.0988689041137696 TRAIN  loss dict:  {'classification_loss': 1.0988689041137696}
2025-01-18 18:33:25,535 [INFO] Step[2300/2713]: training loss : 1.0785580039024354 TRAIN  loss dict:  {'classification_loss': 1.0785580039024354}
2025-01-18 18:33:40,610 [INFO] Step[2350/2713]: training loss : 1.1720479810237885 TRAIN  loss dict:  {'classification_loss': 1.1720479810237885}
2025-01-18 18:33:55,696 [INFO] Step[2400/2713]: training loss : 1.0760407936573029 TRAIN  loss dict:  {'classification_loss': 1.0760407936573029}
2025-01-18 18:34:10,732 [INFO] Step[2450/2713]: training loss : 1.0486373126506805 TRAIN  loss dict:  {'classification_loss': 1.0486373126506805}
2025-01-18 18:34:25,735 [INFO] Step[2500/2713]: training loss : 1.1323623979091644 TRAIN  loss dict:  {'classification_loss': 1.1323623979091644}
2025-01-18 18:34:40,802 [INFO] Step[2550/2713]: training loss : 1.097483779191971 TRAIN  loss dict:  {'classification_loss': 1.097483779191971}
2025-01-18 18:34:55,846 [INFO] Step[2600/2713]: training loss : 1.063807555437088 TRAIN  loss dict:  {'classification_loss': 1.063807555437088}
2025-01-18 18:35:10,815 [INFO] Step[2650/2713]: training loss : 1.0978020823001862 TRAIN  loss dict:  {'classification_loss': 1.0978020823001862}
2025-01-18 18:35:25,817 [INFO] Step[2700/2713]: training loss : 1.1130970370769502 TRAIN  loss dict:  {'classification_loss': 1.1130970370769502}
2025-01-18 18:36:46,094 [INFO] Label accuracies statistics:
2025-01-18 18:36:46,094 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.5, 4: 0.25, 5: 1.0, 6: 0.25, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.5, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.25, 69: 0.75, 70: 0.25, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 1.0, 122: 1.0, 123: 1.0, 124: 0.5, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 0.75, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.5, 143: 0.75, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 0.5, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 1.0, 190: 0.75, 191: 0.25, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.25, 209: 0.5, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.5, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.25, 235: 0.5, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.25, 244: 0.75, 245: 0.5, 246: 1.0, 247: 1.0, 248: 0.3333333333333333, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.25, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.5, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.5, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.5, 312: 0.75, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.75, 328: 0.25, 329: 0.75, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.25, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.25, 342: 0.75, 343: 1.0, 344: 0.25, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.25, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.5, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.5, 377: 0.5, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.25, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-18 18:36:46,096 [INFO] [15] TRAIN  loss: 1.0893404297050058 acc: 0.9778842609657206
2025-01-18 18:36:46,096 [INFO] [15] TRAIN  loss dict: {'classification_loss': 1.0893404297050058}
2025-01-18 18:36:46,096 [INFO] [15] VALIDATION loss: 1.856990819138692 VALIDATION acc: 0.7573667711598746
2025-01-18 18:36:46,096 [INFO] [15] VALIDATION loss dict: {'classification_loss': 1.856990819138692}
2025-01-18 18:36:46,096 [INFO] 
2025-01-18 18:37:06,326 [INFO] Step[50/2713]: training loss : 1.0773807775974273 TRAIN  loss dict:  {'classification_loss': 1.0773807775974273}
2025-01-18 18:37:21,401 [INFO] Step[100/2713]: training loss : 1.088904651403427 TRAIN  loss dict:  {'classification_loss': 1.088904651403427}
2025-01-18 18:37:36,260 [INFO] Step[150/2713]: training loss : 1.0867451524734497 TRAIN  loss dict:  {'classification_loss': 1.0867451524734497}
2025-01-18 18:37:50,939 [INFO] Step[200/2713]: training loss : 1.0584669137001037 TRAIN  loss dict:  {'classification_loss': 1.0584669137001037}
2025-01-18 18:38:05,621 [INFO] Step[250/2713]: training loss : 1.0730199682712556 TRAIN  loss dict:  {'classification_loss': 1.0730199682712556}
2025-01-18 18:38:20,310 [INFO] Step[300/2713]: training loss : 1.0971918439865112 TRAIN  loss dict:  {'classification_loss': 1.0971918439865112}
2025-01-18 18:38:34,858 [INFO] Step[350/2713]: training loss : 1.05068039894104 TRAIN  loss dict:  {'classification_loss': 1.05068039894104}
2025-01-18 18:38:49,522 [INFO] Step[400/2713]: training loss : 1.0806767010688783 TRAIN  loss dict:  {'classification_loss': 1.0806767010688783}
2025-01-18 18:39:04,153 [INFO] Step[450/2713]: training loss : 1.0711260533332825 TRAIN  loss dict:  {'classification_loss': 1.0711260533332825}
2025-01-18 18:39:18,869 [INFO] Step[500/2713]: training loss : 1.029033167362213 TRAIN  loss dict:  {'classification_loss': 1.029033167362213}
2025-01-18 18:39:33,488 [INFO] Step[550/2713]: training loss : 1.0941039061546325 TRAIN  loss dict:  {'classification_loss': 1.0941039061546325}
2025-01-18 18:39:48,131 [INFO] Step[600/2713]: training loss : 1.054878157377243 TRAIN  loss dict:  {'classification_loss': 1.054878157377243}
2025-01-18 18:40:02,849 [INFO] Step[650/2713]: training loss : 1.0722042441368103 TRAIN  loss dict:  {'classification_loss': 1.0722042441368103}
2025-01-18 18:40:17,482 [INFO] Step[700/2713]: training loss : 1.1084112429618835 TRAIN  loss dict:  {'classification_loss': 1.1084112429618835}
2025-01-18 18:40:32,153 [INFO] Step[750/2713]: training loss : 1.0819580745697022 TRAIN  loss dict:  {'classification_loss': 1.0819580745697022}
2025-01-18 18:40:46,729 [INFO] Step[800/2713]: training loss : 1.0888663959503173 TRAIN  loss dict:  {'classification_loss': 1.0888663959503173}
2025-01-18 18:41:01,387 [INFO] Step[850/2713]: training loss : 1.031096943616867 TRAIN  loss dict:  {'classification_loss': 1.031096943616867}
2025-01-18 18:41:16,050 [INFO] Step[900/2713]: training loss : 1.0722830283641815 TRAIN  loss dict:  {'classification_loss': 1.0722830283641815}
2025-01-18 18:41:30,649 [INFO] Step[950/2713]: training loss : 1.051313420534134 TRAIN  loss dict:  {'classification_loss': 1.051313420534134}
2025-01-18 18:41:45,259 [INFO] Step[1000/2713]: training loss : 1.0980289041996003 TRAIN  loss dict:  {'classification_loss': 1.0980289041996003}
2025-01-18 18:41:59,907 [INFO] Step[1050/2713]: training loss : 1.104359793663025 TRAIN  loss dict:  {'classification_loss': 1.104359793663025}
2025-01-18 18:42:14,561 [INFO] Step[1100/2713]: training loss : 1.0491006910800933 TRAIN  loss dict:  {'classification_loss': 1.0491006910800933}
2025-01-18 18:42:29,191 [INFO] Step[1150/2713]: training loss : 1.0531920671463013 TRAIN  loss dict:  {'classification_loss': 1.0531920671463013}
2025-01-18 18:42:43,832 [INFO] Step[1200/2713]: training loss : 1.0726739859580994 TRAIN  loss dict:  {'classification_loss': 1.0726739859580994}
2025-01-18 18:42:58,438 [INFO] Step[1250/2713]: training loss : 1.0615592229366302 TRAIN  loss dict:  {'classification_loss': 1.0615592229366302}
2025-01-18 18:43:13,054 [INFO] Step[1300/2713]: training loss : 1.1260725557804108 TRAIN  loss dict:  {'classification_loss': 1.1260725557804108}
2025-01-18 18:43:27,672 [INFO] Step[1350/2713]: training loss : 1.1118097150325774 TRAIN  loss dict:  {'classification_loss': 1.1118097150325774}
2025-01-18 18:43:42,352 [INFO] Step[1400/2713]: training loss : 1.085496153831482 TRAIN  loss dict:  {'classification_loss': 1.085496153831482}
2025-01-18 18:43:56,968 [INFO] Step[1450/2713]: training loss : 1.0650416934490203 TRAIN  loss dict:  {'classification_loss': 1.0650416934490203}
2025-01-18 18:44:11,654 [INFO] Step[1500/2713]: training loss : 1.0761206901073457 TRAIN  loss dict:  {'classification_loss': 1.0761206901073457}
2025-01-18 18:44:26,329 [INFO] Step[1550/2713]: training loss : 1.0837320268154145 TRAIN  loss dict:  {'classification_loss': 1.0837320268154145}
2025-01-18 18:44:41,028 [INFO] Step[1600/2713]: training loss : 1.0619591772556305 TRAIN  loss dict:  {'classification_loss': 1.0619591772556305}
2025-01-18 18:44:55,647 [INFO] Step[1650/2713]: training loss : 1.1290584671497346 TRAIN  loss dict:  {'classification_loss': 1.1290584671497346}
2025-01-18 18:45:10,302 [INFO] Step[1700/2713]: training loss : 1.1157804572582244 TRAIN  loss dict:  {'classification_loss': 1.1157804572582244}
2025-01-18 18:45:24,951 [INFO] Step[1750/2713]: training loss : 1.1208478581905366 TRAIN  loss dict:  {'classification_loss': 1.1208478581905366}
2025-01-18 18:45:39,603 [INFO] Step[1800/2713]: training loss : 1.0630960297584533 TRAIN  loss dict:  {'classification_loss': 1.0630960297584533}
2025-01-18 18:45:54,259 [INFO] Step[1850/2713]: training loss : 1.0961034798622131 TRAIN  loss dict:  {'classification_loss': 1.0961034798622131}
2025-01-18 18:46:08,891 [INFO] Step[1900/2713]: training loss : 1.109684418439865 TRAIN  loss dict:  {'classification_loss': 1.109684418439865}
2025-01-18 18:46:23,475 [INFO] Step[1950/2713]: training loss : 1.0751646733283997 TRAIN  loss dict:  {'classification_loss': 1.0751646733283997}
2025-01-18 18:46:38,101 [INFO] Step[2000/2713]: training loss : 1.0723481154441834 TRAIN  loss dict:  {'classification_loss': 1.0723481154441834}
2025-01-18 18:46:52,734 [INFO] Step[2050/2713]: training loss : 1.094728410243988 TRAIN  loss dict:  {'classification_loss': 1.094728410243988}
2025-01-18 18:47:07,357 [INFO] Step[2100/2713]: training loss : 1.0998276448249817 TRAIN  loss dict:  {'classification_loss': 1.0998276448249817}
2025-01-18 18:47:22,097 [INFO] Step[2150/2713]: training loss : 1.1090461373329163 TRAIN  loss dict:  {'classification_loss': 1.1090461373329163}
2025-01-18 18:47:36,755 [INFO] Step[2200/2713]: training loss : 1.0770840811729432 TRAIN  loss dict:  {'classification_loss': 1.0770840811729432}
2025-01-18 18:47:51,367 [INFO] Step[2250/2713]: training loss : 1.0424798214435578 TRAIN  loss dict:  {'classification_loss': 1.0424798214435578}
2025-01-18 18:48:06,270 [INFO] Step[2300/2713]: training loss : 1.0704513454437257 TRAIN  loss dict:  {'classification_loss': 1.0704513454437257}
2025-01-18 18:48:20,820 [INFO] Step[2350/2713]: training loss : 1.1203239262104034 TRAIN  loss dict:  {'classification_loss': 1.1203239262104034}
2025-01-18 18:48:35,435 [INFO] Step[2400/2713]: training loss : 1.1073431527614594 TRAIN  loss dict:  {'classification_loss': 1.1073431527614594}
2025-01-18 18:48:50,077 [INFO] Step[2450/2713]: training loss : 1.1003891038894653 TRAIN  loss dict:  {'classification_loss': 1.1003891038894653}
2025-01-18 18:49:04,629 [INFO] Step[2500/2713]: training loss : 1.0781099200248718 TRAIN  loss dict:  {'classification_loss': 1.0781099200248718}
2025-01-18 18:49:19,220 [INFO] Step[2550/2713]: training loss : 1.1004619991779327 TRAIN  loss dict:  {'classification_loss': 1.1004619991779327}
2025-01-18 18:49:33,849 [INFO] Step[2600/2713]: training loss : 1.081234701871872 TRAIN  loss dict:  {'classification_loss': 1.081234701871872}
2025-01-18 18:49:48,447 [INFO] Step[2650/2713]: training loss : 1.091130142211914 TRAIN  loss dict:  {'classification_loss': 1.091130142211914}
2025-01-18 18:50:03,139 [INFO] Step[2700/2713]: training loss : 1.0494284510612488 TRAIN  loss dict:  {'classification_loss': 1.0494284510612488}
2025-01-18 18:51:23,552 [INFO] Label accuracies statistics:
2025-01-18 18:51:23,553 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.25, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.5, 48: 0.75, 49: 1.0, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.5, 58: 0.5, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.5, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 1.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.75, 115: 0.75, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 0.75, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 0.75, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.75, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.75, 202: 0.75, 203: 1.0, 204: 0.75, 205: 0.5, 206: 0.5, 207: 1.0, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 1.0, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 0.5, 245: 1.0, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.5, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 0.75, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 0.5, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.5, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 1.0, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.25, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.5, 347: 0.75, 348: 1.0, 349: 0.25, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.5, 377: 0.5, 378: 0.5, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.0, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-18 18:51:24,281 [INFO] [16] TRAIN  loss: 1.0818007330897985 acc: 0.9810787566040055
2025-01-18 18:51:24,281 [INFO] [16] TRAIN  loss dict: {'classification_loss': 1.0818007330897985}
2025-01-18 18:51:24,281 [INFO] [16] VALIDATION loss: 1.8454477295167464 VALIDATION acc: 0.7749216300940439
2025-01-18 18:51:24,281 [INFO] [16] VALIDATION loss dict: {'classification_loss': 1.8454477295167464}
2025-01-18 18:51:24,281 [INFO] 
2025-01-18 18:51:44,487 [INFO] Step[50/2713]: training loss : 1.0358295261859893 TRAIN  loss dict:  {'classification_loss': 1.0358295261859893}
2025-01-18 18:51:59,061 [INFO] Step[100/2713]: training loss : 1.0675240421295167 TRAIN  loss dict:  {'classification_loss': 1.0675240421295167}
2025-01-18 18:52:13,610 [INFO] Step[150/2713]: training loss : 1.0795991933345794 TRAIN  loss dict:  {'classification_loss': 1.0795991933345794}
2025-01-18 18:52:28,158 [INFO] Step[200/2713]: training loss : 1.0738587033748628 TRAIN  loss dict:  {'classification_loss': 1.0738587033748628}
2025-01-18 18:52:42,700 [INFO] Step[250/2713]: training loss : 1.0731349229812621 TRAIN  loss dict:  {'classification_loss': 1.0731349229812621}
2025-01-18 18:52:57,296 [INFO] Step[300/2713]: training loss : 1.0781784069538116 TRAIN  loss dict:  {'classification_loss': 1.0781784069538116}
2025-01-18 18:53:11,864 [INFO] Step[350/2713]: training loss : 1.0736526358127594 TRAIN  loss dict:  {'classification_loss': 1.0736526358127594}
2025-01-18 18:53:26,427 [INFO] Step[400/2713]: training loss : 1.0951011431217195 TRAIN  loss dict:  {'classification_loss': 1.0951011431217195}
2025-01-18 18:53:40,976 [INFO] Step[450/2713]: training loss : 1.0651527440547943 TRAIN  loss dict:  {'classification_loss': 1.0651527440547943}
2025-01-18 18:53:55,513 [INFO] Step[500/2713]: training loss : 1.085196589231491 TRAIN  loss dict:  {'classification_loss': 1.085196589231491}
2025-01-18 18:54:10,026 [INFO] Step[550/2713]: training loss : 1.0739452409744263 TRAIN  loss dict:  {'classification_loss': 1.0739452409744263}
2025-01-18 18:54:24,658 [INFO] Step[600/2713]: training loss : 1.0807628262042999 TRAIN  loss dict:  {'classification_loss': 1.0807628262042999}
2025-01-18 18:54:39,198 [INFO] Step[650/2713]: training loss : 1.0678107416629792 TRAIN  loss dict:  {'classification_loss': 1.0678107416629792}
2025-01-18 18:54:53,751 [INFO] Step[700/2713]: training loss : 1.023708622455597 TRAIN  loss dict:  {'classification_loss': 1.023708622455597}
2025-01-18 18:55:08,365 [INFO] Step[750/2713]: training loss : 1.0744055426120758 TRAIN  loss dict:  {'classification_loss': 1.0744055426120758}
2025-01-18 18:55:22,915 [INFO] Step[800/2713]: training loss : 1.055336583852768 TRAIN  loss dict:  {'classification_loss': 1.055336583852768}
2025-01-18 18:55:37,545 [INFO] Step[850/2713]: training loss : 1.0412964725494385 TRAIN  loss dict:  {'classification_loss': 1.0412964725494385}
2025-01-18 18:55:52,098 [INFO] Step[900/2713]: training loss : 1.0879303431510925 TRAIN  loss dict:  {'classification_loss': 1.0879303431510925}
2025-01-18 18:56:06,625 [INFO] Step[950/2713]: training loss : 1.092278035879135 TRAIN  loss dict:  {'classification_loss': 1.092278035879135}
2025-01-18 18:56:21,218 [INFO] Step[1000/2713]: training loss : 1.067326250076294 TRAIN  loss dict:  {'classification_loss': 1.067326250076294}
2025-01-18 18:56:35,819 [INFO] Step[1050/2713]: training loss : 1.075929969549179 TRAIN  loss dict:  {'classification_loss': 1.075929969549179}
2025-01-18 18:56:50,462 [INFO] Step[1100/2713]: training loss : 1.0526203393936158 TRAIN  loss dict:  {'classification_loss': 1.0526203393936158}
2025-01-18 18:57:05,015 [INFO] Step[1150/2713]: training loss : 1.0797393572330476 TRAIN  loss dict:  {'classification_loss': 1.0797393572330476}
2025-01-18 18:57:19,659 [INFO] Step[1200/2713]: training loss : 1.030358395576477 TRAIN  loss dict:  {'classification_loss': 1.030358395576477}
2025-01-18 18:57:34,240 [INFO] Step[1250/2713]: training loss : 1.0428490459918975 TRAIN  loss dict:  {'classification_loss': 1.0428490459918975}
2025-01-18 18:57:48,828 [INFO] Step[1300/2713]: training loss : 1.0588718104362487 TRAIN  loss dict:  {'classification_loss': 1.0588718104362487}
2025-01-18 18:58:03,315 [INFO] Step[1350/2713]: training loss : 1.04572518825531 TRAIN  loss dict:  {'classification_loss': 1.04572518825531}
2025-01-18 18:58:17,923 [INFO] Step[1400/2713]: training loss : 1.0753445625305176 TRAIN  loss dict:  {'classification_loss': 1.0753445625305176}
2025-01-18 18:58:32,527 [INFO] Step[1450/2713]: training loss : 1.1496557462215424 TRAIN  loss dict:  {'classification_loss': 1.1496557462215424}
2025-01-18 18:58:47,097 [INFO] Step[1500/2713]: training loss : 1.0492448830604553 TRAIN  loss dict:  {'classification_loss': 1.0492448830604553}
2025-01-18 18:59:01,648 [INFO] Step[1550/2713]: training loss : 1.0492119884490967 TRAIN  loss dict:  {'classification_loss': 1.0492119884490967}
2025-01-18 18:59:16,247 [INFO] Step[1600/2713]: training loss : 1.0545064997673035 TRAIN  loss dict:  {'classification_loss': 1.0545064997673035}
2025-01-18 18:59:30,887 [INFO] Step[1650/2713]: training loss : 1.0685050523281097 TRAIN  loss dict:  {'classification_loss': 1.0685050523281097}
2025-01-18 18:59:45,503 [INFO] Step[1700/2713]: training loss : 1.0527494955062866 TRAIN  loss dict:  {'classification_loss': 1.0527494955062866}
2025-01-18 19:00:00,096 [INFO] Step[1750/2713]: training loss : 1.0713692808151245 TRAIN  loss dict:  {'classification_loss': 1.0713692808151245}
2025-01-18 19:00:14,701 [INFO] Step[1800/2713]: training loss : 1.050530058145523 TRAIN  loss dict:  {'classification_loss': 1.050530058145523}
2025-01-18 19:00:29,242 [INFO] Step[1850/2713]: training loss : 1.0686975002288819 TRAIN  loss dict:  {'classification_loss': 1.0686975002288819}
2025-01-18 19:00:43,847 [INFO] Step[1900/2713]: training loss : 1.1026631224155425 TRAIN  loss dict:  {'classification_loss': 1.1026631224155425}
2025-01-18 19:00:58,442 [INFO] Step[1950/2713]: training loss : 1.0744621968269348 TRAIN  loss dict:  {'classification_loss': 1.0744621968269348}
2025-01-18 19:01:13,038 [INFO] Step[2000/2713]: training loss : 1.064412109851837 TRAIN  loss dict:  {'classification_loss': 1.064412109851837}
2025-01-18 19:01:27,662 [INFO] Step[2050/2713]: training loss : 1.0647508907318115 TRAIN  loss dict:  {'classification_loss': 1.0647508907318115}
2025-01-18 19:01:42,208 [INFO] Step[2100/2713]: training loss : 1.0639307522773742 TRAIN  loss dict:  {'classification_loss': 1.0639307522773742}
2025-01-18 19:01:56,854 [INFO] Step[2150/2713]: training loss : 1.0614290201663972 TRAIN  loss dict:  {'classification_loss': 1.0614290201663972}
2025-01-18 19:02:11,369 [INFO] Step[2200/2713]: training loss : 1.098030012845993 TRAIN  loss dict:  {'classification_loss': 1.098030012845993}
2025-01-18 19:02:25,962 [INFO] Step[2250/2713]: training loss : 1.0558313930034637 TRAIN  loss dict:  {'classification_loss': 1.0558313930034637}
2025-01-18 19:02:40,490 [INFO] Step[2300/2713]: training loss : 1.1162467396259308 TRAIN  loss dict:  {'classification_loss': 1.1162467396259308}
2025-01-18 19:02:55,076 [INFO] Step[2350/2713]: training loss : 1.0993356668949128 TRAIN  loss dict:  {'classification_loss': 1.0993356668949128}
2025-01-18 19:03:09,684 [INFO] Step[2400/2713]: training loss : 1.0537746739387512 TRAIN  loss dict:  {'classification_loss': 1.0537746739387512}
2025-01-18 19:03:24,260 [INFO] Step[2450/2713]: training loss : 1.0876720654964447 TRAIN  loss dict:  {'classification_loss': 1.0876720654964447}
2025-01-18 19:03:38,894 [INFO] Step[2500/2713]: training loss : 1.0791410100460053 TRAIN  loss dict:  {'classification_loss': 1.0791410100460053}
2025-01-18 19:03:53,494 [INFO] Step[2550/2713]: training loss : 1.0994820296764374 TRAIN  loss dict:  {'classification_loss': 1.0994820296764374}
2025-01-18 19:04:08,125 [INFO] Step[2600/2713]: training loss : 1.0636809659004212 TRAIN  loss dict:  {'classification_loss': 1.0636809659004212}
2025-01-18 19:04:22,687 [INFO] Step[2650/2713]: training loss : 1.081880716085434 TRAIN  loss dict:  {'classification_loss': 1.081880716085434}
2025-01-18 19:04:37,212 [INFO] Step[2700/2713]: training loss : 1.0696282768249512 TRAIN  loss dict:  {'classification_loss': 1.0696282768249512}
2025-01-18 19:05:57,548 [INFO] Label accuracies statistics:
2025-01-18 19:05:57,548 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 0.75, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.75, 58: 0.25, 59: 1.0, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 1.0, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.75, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 0.75, 118: 0.75, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 0.5, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.25, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.5, 228: 0.75, 229: 0.0, 230: 1.0, 231: 0.5, 232: 0.5, 233: 0.25, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.5, 238: 1.0, 239: 0.0, 240: 1.0, 241: 0.5, 242: 0.75, 243: 0.5, 244: 0.5, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.5, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 0.5, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 0.75, 266: 0.75, 267: 0.75, 268: 0.25, 269: 0.5, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.25, 274: 0.0, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.0, 279: 1.0, 280: 1.0, 281: 0.5, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.5, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.25, 333: 0.75, 334: 0.75, 335: 0.25, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.25, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.5, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.0, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 0.75, 399: 0.5}

2025-01-18 19:05:57,551 [INFO] [17] TRAIN  loss: 1.0706813190416617 acc: 0.9829217348568621
2025-01-18 19:05:57,551 [INFO] [17] TRAIN  loss dict: {'classification_loss': 1.0706813190416617}
2025-01-18 19:05:57,552 [INFO] [17] VALIDATION loss: 1.883359381354841 VALIDATION acc: 0.7579937304075235
2025-01-18 19:05:57,552 [INFO] [17] VALIDATION loss dict: {'classification_loss': 1.883359381354841}
2025-01-18 19:05:57,553 [INFO] 
2025-01-18 19:06:17,269 [INFO] Step[50/2713]: training loss : 1.0185511684417725 TRAIN  loss dict:  {'classification_loss': 1.0185511684417725}
2025-01-18 19:06:31,984 [INFO] Step[100/2713]: training loss : 1.0706931471824646 TRAIN  loss dict:  {'classification_loss': 1.0706931471824646}
2025-01-18 19:06:46,621 [INFO] Step[150/2713]: training loss : 1.0827774345874785 TRAIN  loss dict:  {'classification_loss': 1.0827774345874785}
2025-01-18 19:07:01,326 [INFO] Step[200/2713]: training loss : 1.063682222366333 TRAIN  loss dict:  {'classification_loss': 1.063682222366333}
2025-01-18 19:07:16,022 [INFO] Step[250/2713]: training loss : 1.0706183779239655 TRAIN  loss dict:  {'classification_loss': 1.0706183779239655}
2025-01-18 19:07:30,709 [INFO] Step[300/2713]: training loss : 1.0342866456508637 TRAIN  loss dict:  {'classification_loss': 1.0342866456508637}
2025-01-18 19:07:45,370 [INFO] Step[350/2713]: training loss : 1.0404093861579895 TRAIN  loss dict:  {'classification_loss': 1.0404093861579895}
2025-01-18 19:08:00,089 [INFO] Step[400/2713]: training loss : 1.0803549468517304 TRAIN  loss dict:  {'classification_loss': 1.0803549468517304}
2025-01-18 19:08:14,744 [INFO] Step[450/2713]: training loss : 1.0777875351905823 TRAIN  loss dict:  {'classification_loss': 1.0777875351905823}
2025-01-18 19:08:29,380 [INFO] Step[500/2713]: training loss : 1.0590670347213744 TRAIN  loss dict:  {'classification_loss': 1.0590670347213744}
2025-01-18 19:08:44,068 [INFO] Step[550/2713]: training loss : 1.071682848930359 TRAIN  loss dict:  {'classification_loss': 1.071682848930359}
2025-01-18 19:08:58,726 [INFO] Step[600/2713]: training loss : 1.0517030608654023 TRAIN  loss dict:  {'classification_loss': 1.0517030608654023}
2025-01-18 19:09:13,431 [INFO] Step[650/2713]: training loss : 1.1138503062725067 TRAIN  loss dict:  {'classification_loss': 1.1138503062725067}
2025-01-18 19:09:28,105 [INFO] Step[700/2713]: training loss : 1.0566963624954224 TRAIN  loss dict:  {'classification_loss': 1.0566963624954224}
2025-01-18 19:09:42,806 [INFO] Step[750/2713]: training loss : 1.0919311988353728 TRAIN  loss dict:  {'classification_loss': 1.0919311988353728}
2025-01-18 19:09:57,497 [INFO] Step[800/2713]: training loss : 1.0548667705059052 TRAIN  loss dict:  {'classification_loss': 1.0548667705059052}
2025-01-18 19:10:12,239 [INFO] Step[850/2713]: training loss : 1.0812291979789734 TRAIN  loss dict:  {'classification_loss': 1.0812291979789734}
2025-01-18 19:10:26,952 [INFO] Step[900/2713]: training loss : 1.0523128938674926 TRAIN  loss dict:  {'classification_loss': 1.0523128938674926}
2025-01-18 19:10:41,629 [INFO] Step[950/2713]: training loss : 1.0377177429199218 TRAIN  loss dict:  {'classification_loss': 1.0377177429199218}
2025-01-18 19:10:56,267 [INFO] Step[1000/2713]: training loss : 1.0693030619621278 TRAIN  loss dict:  {'classification_loss': 1.0693030619621278}
2025-01-18 19:11:10,935 [INFO] Step[1050/2713]: training loss : 1.079946278333664 TRAIN  loss dict:  {'classification_loss': 1.079946278333664}
2025-01-18 19:11:25,609 [INFO] Step[1100/2713]: training loss : 1.0576429796218871 TRAIN  loss dict:  {'classification_loss': 1.0576429796218871}
2025-01-18 19:11:40,266 [INFO] Step[1150/2713]: training loss : 1.0585094583034516 TRAIN  loss dict:  {'classification_loss': 1.0585094583034516}
2025-01-18 19:11:54,874 [INFO] Step[1200/2713]: training loss : 1.059306756258011 TRAIN  loss dict:  {'classification_loss': 1.059306756258011}
2025-01-18 19:12:09,496 [INFO] Step[1250/2713]: training loss : 1.0783419680595399 TRAIN  loss dict:  {'classification_loss': 1.0783419680595399}
2025-01-18 19:12:24,173 [INFO] Step[1300/2713]: training loss : 1.1058436357975006 TRAIN  loss dict:  {'classification_loss': 1.1058436357975006}
2025-01-18 19:12:38,863 [INFO] Step[1350/2713]: training loss : 1.0755568623542786 TRAIN  loss dict:  {'classification_loss': 1.0755568623542786}
2025-01-18 19:12:53,521 [INFO] Step[1400/2713]: training loss : 1.0839665842056274 TRAIN  loss dict:  {'classification_loss': 1.0839665842056274}
2025-01-18 19:13:08,198 [INFO] Step[1450/2713]: training loss : 1.0517730820178985 TRAIN  loss dict:  {'classification_loss': 1.0517730820178985}
2025-01-18 19:13:22,856 [INFO] Step[1500/2713]: training loss : 1.0845093071460723 TRAIN  loss dict:  {'classification_loss': 1.0845093071460723}
2025-01-18 19:13:37,551 [INFO] Step[1550/2713]: training loss : 1.0697035264968873 TRAIN  loss dict:  {'classification_loss': 1.0697035264968873}
2025-01-18 19:13:52,204 [INFO] Step[1600/2713]: training loss : 1.0743391132354736 TRAIN  loss dict:  {'classification_loss': 1.0743391132354736}
2025-01-18 19:14:06,825 [INFO] Step[1650/2713]: training loss : 1.1063192582130432 TRAIN  loss dict:  {'classification_loss': 1.1063192582130432}
2025-01-18 19:14:21,592 [INFO] Step[1700/2713]: training loss : 1.1101521730422974 TRAIN  loss dict:  {'classification_loss': 1.1101521730422974}
2025-01-18 19:14:36,268 [INFO] Step[1750/2713]: training loss : 1.100156946182251 TRAIN  loss dict:  {'classification_loss': 1.100156946182251}
2025-01-18 19:14:51,036 [INFO] Step[1800/2713]: training loss : 1.0577774405479432 TRAIN  loss dict:  {'classification_loss': 1.0577774405479432}
2025-01-18 19:15:05,750 [INFO] Step[1850/2713]: training loss : 1.070469037294388 TRAIN  loss dict:  {'classification_loss': 1.070469037294388}
2025-01-18 19:15:20,426 [INFO] Step[1900/2713]: training loss : 1.0759793770313264 TRAIN  loss dict:  {'classification_loss': 1.0759793770313264}
2025-01-18 19:15:35,184 [INFO] Step[1950/2713]: training loss : 1.0535459911823273 TRAIN  loss dict:  {'classification_loss': 1.0535459911823273}
2025-01-18 19:15:49,895 [INFO] Step[2000/2713]: training loss : 1.0884919726848603 TRAIN  loss dict:  {'classification_loss': 1.0884919726848603}
2025-01-18 19:16:04,629 [INFO] Step[2050/2713]: training loss : 1.0946829533576965 TRAIN  loss dict:  {'classification_loss': 1.0946829533576965}
2025-01-18 19:16:19,329 [INFO] Step[2100/2713]: training loss : 1.078709055185318 TRAIN  loss dict:  {'classification_loss': 1.078709055185318}
2025-01-18 19:16:34,038 [INFO] Step[2150/2713]: training loss : 1.0914622092247008 TRAIN  loss dict:  {'classification_loss': 1.0914622092247008}
2025-01-18 19:16:48,734 [INFO] Step[2200/2713]: training loss : 1.0599235320091247 TRAIN  loss dict:  {'classification_loss': 1.0599235320091247}
2025-01-18 19:17:03,467 [INFO] Step[2250/2713]: training loss : 1.0440340626239777 TRAIN  loss dict:  {'classification_loss': 1.0440340626239777}
2025-01-18 19:17:18,118 [INFO] Step[2300/2713]: training loss : 1.0397321832180024 TRAIN  loss dict:  {'classification_loss': 1.0397321832180024}
2025-01-18 19:17:32,818 [INFO] Step[2350/2713]: training loss : 1.0573193371295928 TRAIN  loss dict:  {'classification_loss': 1.0573193371295928}
2025-01-18 19:17:47,468 [INFO] Step[2400/2713]: training loss : 1.0927594196796417 TRAIN  loss dict:  {'classification_loss': 1.0927594196796417}
2025-01-18 19:18:02,205 [INFO] Step[2450/2713]: training loss : 1.0937589979171753 TRAIN  loss dict:  {'classification_loss': 1.0937589979171753}
2025-01-18 19:18:16,934 [INFO] Step[2500/2713]: training loss : 1.1501220774650573 TRAIN  loss dict:  {'classification_loss': 1.1501220774650573}
2025-01-18 19:18:31,582 [INFO] Step[2550/2713]: training loss : 1.0595299804210663 TRAIN  loss dict:  {'classification_loss': 1.0595299804210663}
2025-01-18 19:18:46,244 [INFO] Step[2600/2713]: training loss : 1.0840006732940675 TRAIN  loss dict:  {'classification_loss': 1.0840006732940675}
2025-01-18 19:19:00,949 [INFO] Step[2650/2713]: training loss : 1.0504678225517272 TRAIN  loss dict:  {'classification_loss': 1.0504678225517272}
2025-01-18 19:19:15,716 [INFO] Step[2700/2713]: training loss : 1.0491016900539398 TRAIN  loss dict:  {'classification_loss': 1.0491016900539398}
2025-01-18 19:20:35,634 [INFO] Label accuracies statistics:
2025-01-18 19:20:35,634 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.25, 65: 1.0, 66: 0.75, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.5, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 0.25, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 0.75, 137: 1.0, 138: 0.75, 139: 0.5, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.75, 144: 0.75, 145: 0.5, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.0, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.5, 217: 0.25, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.25, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.25, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 1.0, 292: 0.75, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.5, 315: 1.0, 316: 0.25, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 1.0, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.25, 337: 1.0, 338: 1.0, 339: 0.75, 340: 0.5, 341: 0.75, 342: 0.75, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.25, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.25, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.75, 379: 0.5, 380: 0.5, 381: 0.75, 382: 1.0, 383: 1.0, 384: 1.0, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.25, 396: 0.25, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 19:20:35,636 [INFO] [18] TRAIN  loss: 1.0715349053615932 acc: 0.9810787566040055
2025-01-18 19:20:35,636 [INFO] [18] TRAIN  loss dict: {'classification_loss': 1.0715349053615932}
2025-01-18 19:20:35,636 [INFO] [18] VALIDATION loss: 1.8735086101114302 VALIDATION acc: 0.7661442006269592
2025-01-18 19:20:35,636 [INFO] [18] VALIDATION loss dict: {'classification_loss': 1.8735086101114302}
2025-01-18 19:20:35,637 [INFO] 
2025-01-18 19:20:55,928 [INFO] Step[50/2713]: training loss : 1.0856851947307586 TRAIN  loss dict:  {'classification_loss': 1.0856851947307586}
2025-01-18 19:21:10,459 [INFO] Step[100/2713]: training loss : 1.0880658268928527 TRAIN  loss dict:  {'classification_loss': 1.0880658268928527}
2025-01-18 19:21:25,050 [INFO] Step[150/2713]: training loss : 1.0214532828330993 TRAIN  loss dict:  {'classification_loss': 1.0214532828330993}
2025-01-18 19:21:39,633 [INFO] Step[200/2713]: training loss : 1.0418145477771759 TRAIN  loss dict:  {'classification_loss': 1.0418145477771759}
2025-01-18 19:21:54,373 [INFO] Step[250/2713]: training loss : 1.0290578067302705 TRAIN  loss dict:  {'classification_loss': 1.0290578067302705}
2025-01-18 19:22:09,402 [INFO] Step[300/2713]: training loss : 1.074093953371048 TRAIN  loss dict:  {'classification_loss': 1.074093953371048}
2025-01-18 19:22:24,451 [INFO] Step[350/2713]: training loss : 1.066501121520996 TRAIN  loss dict:  {'classification_loss': 1.066501121520996}
2025-01-18 19:22:39,473 [INFO] Step[400/2713]: training loss : 1.069036226272583 TRAIN  loss dict:  {'classification_loss': 1.069036226272583}
2025-01-18 19:22:54,504 [INFO] Step[450/2713]: training loss : 1.0755569624900818 TRAIN  loss dict:  {'classification_loss': 1.0755569624900818}
2025-01-18 19:23:09,574 [INFO] Step[500/2713]: training loss : 1.081717369556427 TRAIN  loss dict:  {'classification_loss': 1.081717369556427}
2025-01-18 19:23:24,599 [INFO] Step[550/2713]: training loss : 1.0615678012371064 TRAIN  loss dict:  {'classification_loss': 1.0615678012371064}
2025-01-18 19:23:39,548 [INFO] Step[600/2713]: training loss : 1.0722847378253937 TRAIN  loss dict:  {'classification_loss': 1.0722847378253937}
2025-01-18 19:23:54,514 [INFO] Step[650/2713]: training loss : 1.0444626545906066 TRAIN  loss dict:  {'classification_loss': 1.0444626545906066}
2025-01-18 19:24:09,556 [INFO] Step[700/2713]: training loss : 1.067880392074585 TRAIN  loss dict:  {'classification_loss': 1.067880392074585}
2025-01-18 19:24:24,591 [INFO] Step[750/2713]: training loss : 1.0719033694267273 TRAIN  loss dict:  {'classification_loss': 1.0719033694267273}
2025-01-18 19:24:39,671 [INFO] Step[800/2713]: training loss : 1.0509997284412385 TRAIN  loss dict:  {'classification_loss': 1.0509997284412385}
2025-01-18 19:24:54,756 [INFO] Step[850/2713]: training loss : 1.0339403712749482 TRAIN  loss dict:  {'classification_loss': 1.0339403712749482}
2025-01-18 19:25:09,784 [INFO] Step[900/2713]: training loss : 1.0668189990520478 TRAIN  loss dict:  {'classification_loss': 1.0668189990520478}
2025-01-18 19:25:24,888 [INFO] Step[950/2713]: training loss : 1.0887806153297424 TRAIN  loss dict:  {'classification_loss': 1.0887806153297424}
2025-01-18 19:25:39,939 [INFO] Step[1000/2713]: training loss : 1.052339689731598 TRAIN  loss dict:  {'classification_loss': 1.052339689731598}
2025-01-18 19:25:55,007 [INFO] Step[1050/2713]: training loss : 1.0504488337039948 TRAIN  loss dict:  {'classification_loss': 1.0504488337039948}
2025-01-18 19:26:10,022 [INFO] Step[1100/2713]: training loss : 1.0493216812610626 TRAIN  loss dict:  {'classification_loss': 1.0493216812610626}
2025-01-18 19:26:25,038 [INFO] Step[1150/2713]: training loss : 1.0662381279468536 TRAIN  loss dict:  {'classification_loss': 1.0662381279468536}
2025-01-18 19:26:39,997 [INFO] Step[1200/2713]: training loss : 1.0418463218212128 TRAIN  loss dict:  {'classification_loss': 1.0418463218212128}
2025-01-18 19:26:55,059 [INFO] Step[1250/2713]: training loss : 1.0841435718536376 TRAIN  loss dict:  {'classification_loss': 1.0841435718536376}
2025-01-18 19:27:10,102 [INFO] Step[1300/2713]: training loss : 1.0437740337848664 TRAIN  loss dict:  {'classification_loss': 1.0437740337848664}
2025-01-18 19:27:25,187 [INFO] Step[1350/2713]: training loss : 1.1016452610492706 TRAIN  loss dict:  {'classification_loss': 1.1016452610492706}
2025-01-18 19:27:40,143 [INFO] Step[1400/2713]: training loss : 1.0897455072402955 TRAIN  loss dict:  {'classification_loss': 1.0897455072402955}
2025-01-18 19:27:55,224 [INFO] Step[1450/2713]: training loss : 1.051732507944107 TRAIN  loss dict:  {'classification_loss': 1.051732507944107}
2025-01-18 19:28:10,325 [INFO] Step[1500/2713]: training loss : 1.0456425940990448 TRAIN  loss dict:  {'classification_loss': 1.0456425940990448}
2025-01-18 19:28:25,371 [INFO] Step[1550/2713]: training loss : 1.0565418708324432 TRAIN  loss dict:  {'classification_loss': 1.0565418708324432}
2025-01-18 19:28:40,419 [INFO] Step[1600/2713]: training loss : 1.0219776821136475 TRAIN  loss dict:  {'classification_loss': 1.0219776821136475}
2025-01-18 19:28:55,453 [INFO] Step[1650/2713]: training loss : 1.0790548813343048 TRAIN  loss dict:  {'classification_loss': 1.0790548813343048}
2025-01-18 19:29:10,446 [INFO] Step[1700/2713]: training loss : 1.073126835823059 TRAIN  loss dict:  {'classification_loss': 1.073126835823059}
2025-01-18 19:29:25,482 [INFO] Step[1750/2713]: training loss : 1.0678815519809723 TRAIN  loss dict:  {'classification_loss': 1.0678815519809723}
2025-01-18 19:29:40,516 [INFO] Step[1800/2713]: training loss : 1.0505623590946198 TRAIN  loss dict:  {'classification_loss': 1.0505623590946198}
2025-01-18 19:29:55,559 [INFO] Step[1850/2713]: training loss : 1.0903614151477814 TRAIN  loss dict:  {'classification_loss': 1.0903614151477814}
2025-01-18 19:30:10,534 [INFO] Step[1900/2713]: training loss : 1.0381622445583343 TRAIN  loss dict:  {'classification_loss': 1.0381622445583343}
2025-01-18 19:30:25,555 [INFO] Step[1950/2713]: training loss : 1.102266640663147 TRAIN  loss dict:  {'classification_loss': 1.102266640663147}
2025-01-18 19:30:40,623 [INFO] Step[2000/2713]: training loss : 1.059087371826172 TRAIN  loss dict:  {'classification_loss': 1.059087371826172}
2025-01-18 19:30:55,697 [INFO] Step[2050/2713]: training loss : 1.0679343032836914 TRAIN  loss dict:  {'classification_loss': 1.0679343032836914}
2025-01-18 19:31:10,712 [INFO] Step[2100/2713]: training loss : 1.0770145618915559 TRAIN  loss dict:  {'classification_loss': 1.0770145618915559}
2025-01-18 19:31:25,750 [INFO] Step[2150/2713]: training loss : 1.0978275001049043 TRAIN  loss dict:  {'classification_loss': 1.0978275001049043}
2025-01-18 19:31:40,700 [INFO] Step[2200/2713]: training loss : 1.0945353710651398 TRAIN  loss dict:  {'classification_loss': 1.0945353710651398}
2025-01-18 19:31:55,637 [INFO] Step[2250/2713]: training loss : 1.0436144840717316 TRAIN  loss dict:  {'classification_loss': 1.0436144840717316}
2025-01-18 19:32:10,634 [INFO] Step[2300/2713]: training loss : 1.0693183040618897 TRAIN  loss dict:  {'classification_loss': 1.0693183040618897}
2025-01-18 19:32:25,647 [INFO] Step[2350/2713]: training loss : 1.0313683199882506 TRAIN  loss dict:  {'classification_loss': 1.0313683199882506}
2025-01-18 19:32:40,593 [INFO] Step[2400/2713]: training loss : 1.0635361802577972 TRAIN  loss dict:  {'classification_loss': 1.0635361802577972}
2025-01-18 19:32:55,516 [INFO] Step[2450/2713]: training loss : 1.1025607705116272 TRAIN  loss dict:  {'classification_loss': 1.1025607705116272}
2025-01-18 19:33:10,446 [INFO] Step[2500/2713]: training loss : 1.0848098003864288 TRAIN  loss dict:  {'classification_loss': 1.0848098003864288}
2025-01-18 19:33:25,413 [INFO] Step[2550/2713]: training loss : 1.0464016020298004 TRAIN  loss dict:  {'classification_loss': 1.0464016020298004}
2025-01-18 19:33:40,324 [INFO] Step[2600/2713]: training loss : 1.1288217556476594 TRAIN  loss dict:  {'classification_loss': 1.1288217556476594}
2025-01-18 19:33:55,291 [INFO] Step[2650/2713]: training loss : 1.052300544977188 TRAIN  loss dict:  {'classification_loss': 1.052300544977188}
2025-01-18 19:34:10,302 [INFO] Step[2700/2713]: training loss : 1.0740450656414031 TRAIN  loss dict:  {'classification_loss': 1.0740450656414031}
2025-01-18 19:35:30,068 [INFO] Label accuracies statistics:
2025-01-18 19:35:30,069 [INFO] {0: 1.0, 1: 1.0, 2: 0.25, 3: 0.5, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.5, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.25, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.5, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 1.0, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.75, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.5, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.25, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 0.5, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.6666666666666666, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 1.0, 208: 0.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.25, 225: 0.75, 226: 1.0, 227: 0.5, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.5, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.75, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.25, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.0, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.5, 318: 0.5, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.25, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 1.0, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 0.75, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.5, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-18 19:35:31,398 [INFO] [19] TRAIN  loss: 1.0656626078417237 acc: 0.9823074087725765
2025-01-18 19:35:31,398 [INFO] [19] TRAIN  loss dict: {'classification_loss': 1.0656626078417237}
2025-01-18 19:35:31,399 [INFO] [19] VALIDATION loss: 1.7708960295395744 VALIDATION acc: 0.7830721003134796
2025-01-18 19:35:31,399 [INFO] [19] VALIDATION loss dict: {'classification_loss': 1.7708960295395744}
2025-01-18 19:35:31,399 [INFO] 
2025-01-18 19:35:51,775 [INFO] Step[50/2713]: training loss : 1.0556651151180267 TRAIN  loss dict:  {'classification_loss': 1.0556651151180267}
2025-01-18 19:36:06,794 [INFO] Step[100/2713]: training loss : 1.090230416059494 TRAIN  loss dict:  {'classification_loss': 1.090230416059494}
2025-01-18 19:36:21,846 [INFO] Step[150/2713]: training loss : 1.0367454075813294 TRAIN  loss dict:  {'classification_loss': 1.0367454075813294}
2025-01-18 19:36:36,817 [INFO] Step[200/2713]: training loss : 1.0530020892620087 TRAIN  loss dict:  {'classification_loss': 1.0530020892620087}
2025-01-18 19:36:51,860 [INFO] Step[250/2713]: training loss : 1.0459262835979461 TRAIN  loss dict:  {'classification_loss': 1.0459262835979461}
2025-01-18 19:37:06,807 [INFO] Step[300/2713]: training loss : 1.0613987922668457 TRAIN  loss dict:  {'classification_loss': 1.0613987922668457}
2025-01-18 19:37:21,780 [INFO] Step[350/2713]: training loss : 1.0603066599369049 TRAIN  loss dict:  {'classification_loss': 1.0603066599369049}
2025-01-18 19:37:36,800 [INFO] Step[400/2713]: training loss : 1.1132357907295227 TRAIN  loss dict:  {'classification_loss': 1.1132357907295227}
2025-01-18 19:37:51,859 [INFO] Step[450/2713]: training loss : 1.0401832199096679 TRAIN  loss dict:  {'classification_loss': 1.0401832199096679}
2025-01-18 19:38:06,920 [INFO] Step[500/2713]: training loss : 1.0499606895446778 TRAIN  loss dict:  {'classification_loss': 1.0499606895446778}
2025-01-18 19:38:21,960 [INFO] Step[550/2713]: training loss : 1.0245837271213531 TRAIN  loss dict:  {'classification_loss': 1.0245837271213531}
2025-01-18 19:38:36,983 [INFO] Step[600/2713]: training loss : 1.070833867788315 TRAIN  loss dict:  {'classification_loss': 1.070833867788315}
2025-01-18 19:38:52,064 [INFO] Step[650/2713]: training loss : 1.0309319484233856 TRAIN  loss dict:  {'classification_loss': 1.0309319484233856}
2025-01-18 19:39:07,115 [INFO] Step[700/2713]: training loss : 1.0433274817466736 TRAIN  loss dict:  {'classification_loss': 1.0433274817466736}
2025-01-18 19:39:22,161 [INFO] Step[750/2713]: training loss : 1.0797488796710968 TRAIN  loss dict:  {'classification_loss': 1.0797488796710968}
2025-01-18 19:39:37,149 [INFO] Step[800/2713]: training loss : 1.0741116619110107 TRAIN  loss dict:  {'classification_loss': 1.0741116619110107}
2025-01-18 19:39:52,210 [INFO] Step[850/2713]: training loss : 1.0661174213886262 TRAIN  loss dict:  {'classification_loss': 1.0661174213886262}
2025-01-18 19:40:07,204 [INFO] Step[900/2713]: training loss : 1.0438217329978943 TRAIN  loss dict:  {'classification_loss': 1.0438217329978943}
2025-01-18 19:40:22,205 [INFO] Step[950/2713]: training loss : 1.0506422448158264 TRAIN  loss dict:  {'classification_loss': 1.0506422448158264}
2025-01-18 19:40:37,270 [INFO] Step[1000/2713]: training loss : 1.0571207559108735 TRAIN  loss dict:  {'classification_loss': 1.0571207559108735}
2025-01-18 19:40:52,363 [INFO] Step[1050/2713]: training loss : 1.058922027349472 TRAIN  loss dict:  {'classification_loss': 1.058922027349472}
2025-01-18 19:41:07,341 [INFO] Step[1100/2713]: training loss : 1.0478077185153962 TRAIN  loss dict:  {'classification_loss': 1.0478077185153962}
2025-01-18 19:41:22,382 [INFO] Step[1150/2713]: training loss : 1.0709702062606812 TRAIN  loss dict:  {'classification_loss': 1.0709702062606812}
2025-01-18 19:41:37,226 [INFO] Step[1200/2713]: training loss : 1.049933340549469 TRAIN  loss dict:  {'classification_loss': 1.049933340549469}
2025-01-18 19:41:52,160 [INFO] Step[1250/2713]: training loss : 1.0637866985797881 TRAIN  loss dict:  {'classification_loss': 1.0637866985797881}
2025-01-18 19:42:07,014 [INFO] Step[1300/2713]: training loss : 1.0392142677307128 TRAIN  loss dict:  {'classification_loss': 1.0392142677307128}
2025-01-18 19:42:21,869 [INFO] Step[1350/2713]: training loss : 1.1665148341655731 TRAIN  loss dict:  {'classification_loss': 1.1665148341655731}
2025-01-18 19:42:36,635 [INFO] Step[1400/2713]: training loss : 1.047225285768509 TRAIN  loss dict:  {'classification_loss': 1.047225285768509}
2025-01-18 19:42:51,506 [INFO] Step[1450/2713]: training loss : 1.0683378469944 TRAIN  loss dict:  {'classification_loss': 1.0683378469944}
2025-01-18 19:43:06,397 [INFO] Step[1500/2713]: training loss : 1.0653053176403047 TRAIN  loss dict:  {'classification_loss': 1.0653053176403047}
2025-01-18 19:43:21,269 [INFO] Step[1550/2713]: training loss : 1.014443975687027 TRAIN  loss dict:  {'classification_loss': 1.014443975687027}
2025-01-18 19:43:36,103 [INFO] Step[1600/2713]: training loss : 1.043532545566559 TRAIN  loss dict:  {'classification_loss': 1.043532545566559}
2025-01-18 19:43:51,011 [INFO] Step[1650/2713]: training loss : 1.0745326256752015 TRAIN  loss dict:  {'classification_loss': 1.0745326256752015}
2025-01-18 19:44:05,846 [INFO] Step[1700/2713]: training loss : 1.0506922030448913 TRAIN  loss dict:  {'classification_loss': 1.0506922030448913}
2025-01-18 19:44:20,711 [INFO] Step[1750/2713]: training loss : 1.0705388247966767 TRAIN  loss dict:  {'classification_loss': 1.0705388247966767}
2025-01-18 19:44:35,603 [INFO] Step[1800/2713]: training loss : 1.0471787226200104 TRAIN  loss dict:  {'classification_loss': 1.0471787226200104}
2025-01-18 19:44:50,467 [INFO] Step[1850/2713]: training loss : 1.0672936379909514 TRAIN  loss dict:  {'classification_loss': 1.0672936379909514}
2025-01-18 19:45:05,333 [INFO] Step[1900/2713]: training loss : 1.0425021600723268 TRAIN  loss dict:  {'classification_loss': 1.0425021600723268}
2025-01-18 19:45:20,208 [INFO] Step[1950/2713]: training loss : 1.0615204179286957 TRAIN  loss dict:  {'classification_loss': 1.0615204179286957}
2025-01-18 19:45:35,070 [INFO] Step[2000/2713]: training loss : 1.0702385652065276 TRAIN  loss dict:  {'classification_loss': 1.0702385652065276}
2025-01-18 19:45:49,983 [INFO] Step[2050/2713]: training loss : 1.0615108549594878 TRAIN  loss dict:  {'classification_loss': 1.0615108549594878}
2025-01-18 19:46:04,821 [INFO] Step[2100/2713]: training loss : 1.0588224720954895 TRAIN  loss dict:  {'classification_loss': 1.0588224720954895}
2025-01-18 19:46:19,719 [INFO] Step[2150/2713]: training loss : 1.04846911072731 TRAIN  loss dict:  {'classification_loss': 1.04846911072731}
2025-01-18 19:46:34,601 [INFO] Step[2200/2713]: training loss : 1.0707252323627472 TRAIN  loss dict:  {'classification_loss': 1.0707252323627472}
2025-01-18 19:46:49,501 [INFO] Step[2250/2713]: training loss : 1.0655362141132354 TRAIN  loss dict:  {'classification_loss': 1.0655362141132354}
2025-01-18 19:47:04,352 [INFO] Step[2300/2713]: training loss : 1.1193834245204926 TRAIN  loss dict:  {'classification_loss': 1.1193834245204926}
2025-01-18 19:47:19,200 [INFO] Step[2350/2713]: training loss : 1.041404526233673 TRAIN  loss dict:  {'classification_loss': 1.041404526233673}
2025-01-18 19:47:34,059 [INFO] Step[2400/2713]: training loss : 1.0897644877433776 TRAIN  loss dict:  {'classification_loss': 1.0897644877433776}
2025-01-18 19:47:49,047 [INFO] Step[2450/2713]: training loss : 1.0340322387218475 TRAIN  loss dict:  {'classification_loss': 1.0340322387218475}
2025-01-18 19:48:03,862 [INFO] Step[2500/2713]: training loss : 1.0928539669513702 TRAIN  loss dict:  {'classification_loss': 1.0928539669513702}
2025-01-18 19:48:18,647 [INFO] Step[2550/2713]: training loss : 1.0315617835521698 TRAIN  loss dict:  {'classification_loss': 1.0315617835521698}
2025-01-18 19:48:33,484 [INFO] Step[2600/2713]: training loss : 1.0462908613681794 TRAIN  loss dict:  {'classification_loss': 1.0462908613681794}
2025-01-18 19:48:48,335 [INFO] Step[2650/2713]: training loss : 1.052476544380188 TRAIN  loss dict:  {'classification_loss': 1.052476544380188}
2025-01-18 19:49:03,222 [INFO] Step[2700/2713]: training loss : 1.036249359846115 TRAIN  loss dict:  {'classification_loss': 1.036249359846115}
2025-01-18 19:50:22,992 [INFO] Label accuracies statistics:
2025-01-18 19:50:22,993 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.75, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.25, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.5, 46: 1.0, 47: 0.5, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 0.75, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.5, 99: 1.0, 100: 0.75, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 0.75, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 0.75, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.25, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 0.75, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.5, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 1.0, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 1.0, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.5, 246: 1.0, 247: 1.0, 248: 0.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 1.0, 259: 0.5, 260: 0.25, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.5, 274: 1.0, 275: 0.75, 276: 1.0, 277: 0.5, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.25, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.25, 315: 0.5, 316: 0.25, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.5, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.5, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.25, 345: 0.25, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.5, 363: 0.5, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.5, 384: 0.5, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-18 19:50:22,994 [INFO] [20] TRAIN  loss: 1.0598097309124148 acc: 0.9825531392062907
2025-01-18 19:50:22,994 [INFO] [20] TRAIN  loss dict: {'classification_loss': 1.0598097309124148}
2025-01-18 19:50:22,995 [INFO] [20] VALIDATION loss: 1.894359660103805 VALIDATION acc: 0.7592476489028214
2025-01-18 19:50:22,995 [INFO] [20] VALIDATION loss dict: {'classification_loss': 1.894359660103805}
2025-01-18 19:50:22,995 [INFO] 
2025-01-18 19:50:43,202 [INFO] Step[50/2713]: training loss : 1.02134223818779 TRAIN  loss dict:  {'classification_loss': 1.02134223818779}
2025-01-18 19:50:58,024 [INFO] Step[100/2713]: training loss : 1.041249623298645 TRAIN  loss dict:  {'classification_loss': 1.041249623298645}
2025-01-18 19:51:12,888 [INFO] Step[150/2713]: training loss : 1.0429895985126496 TRAIN  loss dict:  {'classification_loss': 1.0429895985126496}
2025-01-18 19:51:27,787 [INFO] Step[200/2713]: training loss : 1.038992439508438 TRAIN  loss dict:  {'classification_loss': 1.038992439508438}
2025-01-18 19:51:42,625 [INFO] Step[250/2713]: training loss : 1.0408100283145905 TRAIN  loss dict:  {'classification_loss': 1.0408100283145905}
2025-01-18 19:51:57,456 [INFO] Step[300/2713]: training loss : 1.060626482963562 TRAIN  loss dict:  {'classification_loss': 1.060626482963562}
2025-01-18 19:52:12,345 [INFO] Step[350/2713]: training loss : 1.022146761417389 TRAIN  loss dict:  {'classification_loss': 1.022146761417389}
2025-01-18 19:52:27,184 [INFO] Step[400/2713]: training loss : 1.0377784490585327 TRAIN  loss dict:  {'classification_loss': 1.0377784490585327}
2025-01-18 19:52:42,115 [INFO] Step[450/2713]: training loss : 1.0390946745872498 TRAIN  loss dict:  {'classification_loss': 1.0390946745872498}
2025-01-18 19:52:56,932 [INFO] Step[500/2713]: training loss : 1.0410370814800263 TRAIN  loss dict:  {'classification_loss': 1.0410370814800263}
2025-01-18 19:53:11,819 [INFO] Step[550/2713]: training loss : 1.0291895842552186 TRAIN  loss dict:  {'classification_loss': 1.0291895842552186}
2025-01-18 19:53:26,677 [INFO] Step[600/2713]: training loss : 1.1067981839179992 TRAIN  loss dict:  {'classification_loss': 1.1067981839179992}
2025-01-18 19:53:41,516 [INFO] Step[650/2713]: training loss : 1.0743960642814636 TRAIN  loss dict:  {'classification_loss': 1.0743960642814636}
2025-01-18 19:53:56,321 [INFO] Step[700/2713]: training loss : 1.0514383900165558 TRAIN  loss dict:  {'classification_loss': 1.0514383900165558}
2025-01-18 19:54:11,172 [INFO] Step[750/2713]: training loss : 1.0165534436702728 TRAIN  loss dict:  {'classification_loss': 1.0165534436702728}
2025-01-18 19:54:26,092 [INFO] Step[800/2713]: training loss : 1.0250948858261109 TRAIN  loss dict:  {'classification_loss': 1.0250948858261109}
2025-01-18 19:54:40,961 [INFO] Step[850/2713]: training loss : 1.0287546646595 TRAIN  loss dict:  {'classification_loss': 1.0287546646595}
2025-01-18 19:54:55,773 [INFO] Step[900/2713]: training loss : 1.0456320762634277 TRAIN  loss dict:  {'classification_loss': 1.0456320762634277}
2025-01-18 19:55:10,642 [INFO] Step[950/2713]: training loss : 1.0406795454025268 TRAIN  loss dict:  {'classification_loss': 1.0406795454025268}
2025-01-18 19:55:25,478 [INFO] Step[1000/2713]: training loss : 1.0427273881435395 TRAIN  loss dict:  {'classification_loss': 1.0427273881435395}
2025-01-18 19:55:40,355 [INFO] Step[1050/2713]: training loss : 1.0446187508106233 TRAIN  loss dict:  {'classification_loss': 1.0446187508106233}
2025-01-18 19:55:55,193 [INFO] Step[1100/2713]: training loss : 1.0257007229328154 TRAIN  loss dict:  {'classification_loss': 1.0257007229328154}
2025-01-18 19:56:10,025 [INFO] Step[1150/2713]: training loss : 1.061058976650238 TRAIN  loss dict:  {'classification_loss': 1.061058976650238}
2025-01-18 19:56:24,851 [INFO] Step[1200/2713]: training loss : 1.0278428542613982 TRAIN  loss dict:  {'classification_loss': 1.0278428542613982}
2025-01-18 19:56:39,742 [INFO] Step[1250/2713]: training loss : 1.0288410139083863 TRAIN  loss dict:  {'classification_loss': 1.0288410139083863}
2025-01-18 19:56:54,576 [INFO] Step[1300/2713]: training loss : 1.024864488840103 TRAIN  loss dict:  {'classification_loss': 1.024864488840103}
2025-01-18 19:57:09,459 [INFO] Step[1350/2713]: training loss : 1.0425274050235749 TRAIN  loss dict:  {'classification_loss': 1.0425274050235749}
2025-01-18 19:57:24,297 [INFO] Step[1400/2713]: training loss : 1.011079136133194 TRAIN  loss dict:  {'classification_loss': 1.011079136133194}
2025-01-18 19:57:39,198 [INFO] Step[1450/2713]: training loss : 1.0268871557712556 TRAIN  loss dict:  {'classification_loss': 1.0268871557712556}
2025-01-18 19:57:54,070 [INFO] Step[1500/2713]: training loss : 1.0444783556461334 TRAIN  loss dict:  {'classification_loss': 1.0444783556461334}
2025-01-18 19:58:08,910 [INFO] Step[1550/2713]: training loss : 1.0291446018218995 TRAIN  loss dict:  {'classification_loss': 1.0291446018218995}
2025-01-18 19:58:23,763 [INFO] Step[1600/2713]: training loss : 1.0566665065288543 TRAIN  loss dict:  {'classification_loss': 1.0566665065288543}
2025-01-18 19:58:38,629 [INFO] Step[1650/2713]: training loss : 1.0594726049900054 TRAIN  loss dict:  {'classification_loss': 1.0594726049900054}
2025-01-18 19:58:53,480 [INFO] Step[1700/2713]: training loss : 1.0447601807117461 TRAIN  loss dict:  {'classification_loss': 1.0447601807117461}
2025-01-18 19:59:08,309 [INFO] Step[1750/2713]: training loss : 1.0409083247184754 TRAIN  loss dict:  {'classification_loss': 1.0409083247184754}
2025-01-18 19:59:23,189 [INFO] Step[1800/2713]: training loss : 1.0487167155742645 TRAIN  loss dict:  {'classification_loss': 1.0487167155742645}
2025-01-18 19:59:38,014 [INFO] Step[1850/2713]: training loss : 1.0167783391475678 TRAIN  loss dict:  {'classification_loss': 1.0167783391475678}
2025-01-18 19:59:52,869 [INFO] Step[1900/2713]: training loss : 1.0299922811985016 TRAIN  loss dict:  {'classification_loss': 1.0299922811985016}
2025-01-18 20:00:07,713 [INFO] Step[1950/2713]: training loss : 1.036332561969757 TRAIN  loss dict:  {'classification_loss': 1.036332561969757}
2025-01-18 20:00:22,556 [INFO] Step[2000/2713]: training loss : 1.0281172049045564 TRAIN  loss dict:  {'classification_loss': 1.0281172049045564}
2025-01-18 20:00:37,403 [INFO] Step[2050/2713]: training loss : 1.0171656823158264 TRAIN  loss dict:  {'classification_loss': 1.0171656823158264}
2025-01-18 20:00:52,309 [INFO] Step[2100/2713]: training loss : 1.037195895910263 TRAIN  loss dict:  {'classification_loss': 1.037195895910263}
2025-01-18 20:01:07,163 [INFO] Step[2150/2713]: training loss : 1.0181513786315919 TRAIN  loss dict:  {'classification_loss': 1.0181513786315919}
2025-01-18 20:01:22,030 [INFO] Step[2200/2713]: training loss : 1.0614820170402526 TRAIN  loss dict:  {'classification_loss': 1.0614820170402526}
2025-01-18 20:01:36,931 [INFO] Step[2250/2713]: training loss : 1.0061436104774475 TRAIN  loss dict:  {'classification_loss': 1.0061436104774475}
2025-01-18 20:01:51,778 [INFO] Step[2300/2713]: training loss : 1.0471534383296968 TRAIN  loss dict:  {'classification_loss': 1.0471534383296968}
2025-01-18 20:02:06,613 [INFO] Step[2350/2713]: training loss : 1.03606391787529 TRAIN  loss dict:  {'classification_loss': 1.03606391787529}
2025-01-18 20:02:21,476 [INFO] Step[2400/2713]: training loss : 1.022141227722168 TRAIN  loss dict:  {'classification_loss': 1.022141227722168}
2025-01-18 20:02:36,324 [INFO] Step[2450/2713]: training loss : 1.0450994670391083 TRAIN  loss dict:  {'classification_loss': 1.0450994670391083}
2025-01-18 20:02:51,160 [INFO] Step[2500/2713]: training loss : 1.0157144522666932 TRAIN  loss dict:  {'classification_loss': 1.0157144522666932}
2025-01-18 20:03:05,994 [INFO] Step[2550/2713]: training loss : 1.0561184287071228 TRAIN  loss dict:  {'classification_loss': 1.0561184287071228}
2025-01-18 20:03:20,848 [INFO] Step[2600/2713]: training loss : 1.0026735258102417 TRAIN  loss dict:  {'classification_loss': 1.0026735258102417}
2025-01-18 20:03:35,716 [INFO] Step[2650/2713]: training loss : 1.046277084350586 TRAIN  loss dict:  {'classification_loss': 1.046277084350586}
2025-01-18 20:03:50,645 [INFO] Step[2700/2713]: training loss : 1.035972752571106 TRAIN  loss dict:  {'classification_loss': 1.035972752571106}
2025-01-18 20:05:10,792 [INFO] Label accuracies statistics:
2025-01-18 20:05:10,792 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 0.75, 118: 1.0, 119: 0.75, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 0.75, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 1.0, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.25, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.5, 236: 0.75, 237: 0.75, 238: 1.0, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.5, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.5, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.25, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 1.0, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 0.75, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.25, 333: 0.25, 334: 0.75, 335: 1.0, 336: 0.75, 337: 1.0, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.25, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-18 20:05:10,794 [INFO] [21] TRAIN  loss: 1.0373380457480543 acc: 0.987467747880575
2025-01-18 20:05:10,794 [INFO] [21] TRAIN  loss dict: {'classification_loss': 1.0373380457480543}
2025-01-18 20:05:10,794 [INFO] [21] VALIDATION loss: 1.7991547992354946 VALIDATION acc: 0.7761755485893417
2025-01-18 20:05:10,794 [INFO] [21] VALIDATION loss dict: {'classification_loss': 1.7991547992354946}
2025-01-18 20:05:10,795 [INFO] 
2025-01-18 20:05:31,413 [INFO] Step[50/2713]: training loss : 1.0145283365249633 TRAIN  loss dict:  {'classification_loss': 1.0145283365249633}
2025-01-18 20:05:46,333 [INFO] Step[100/2713]: training loss : 1.0004235804080963 TRAIN  loss dict:  {'classification_loss': 1.0004235804080963}
2025-01-18 20:06:01,285 [INFO] Step[150/2713]: training loss : 1.0222056317329407 TRAIN  loss dict:  {'classification_loss': 1.0222056317329407}
2025-01-18 20:06:16,160 [INFO] Step[200/2713]: training loss : 1.0222757399082183 TRAIN  loss dict:  {'classification_loss': 1.0222757399082183}
2025-01-18 20:06:31,057 [INFO] Step[250/2713]: training loss : 1.0147841703891753 TRAIN  loss dict:  {'classification_loss': 1.0147841703891753}
2025-01-18 20:06:45,871 [INFO] Step[300/2713]: training loss : 1.0225130069255828 TRAIN  loss dict:  {'classification_loss': 1.0225130069255828}
2025-01-18 20:07:00,753 [INFO] Step[350/2713]: training loss : 1.0131653952598572 TRAIN  loss dict:  {'classification_loss': 1.0131653952598572}
2025-01-18 20:07:15,601 [INFO] Step[400/2713]: training loss : 1.0315717899799346 TRAIN  loss dict:  {'classification_loss': 1.0315717899799346}
2025-01-18 20:07:30,393 [INFO] Step[450/2713]: training loss : 1.0029508864879608 TRAIN  loss dict:  {'classification_loss': 1.0029508864879608}
2025-01-18 20:07:45,238 [INFO] Step[500/2713]: training loss : 1.0002387130260468 TRAIN  loss dict:  {'classification_loss': 1.0002387130260468}
2025-01-18 20:08:00,067 [INFO] Step[550/2713]: training loss : 1.050550389289856 TRAIN  loss dict:  {'classification_loss': 1.050550389289856}
2025-01-18 20:08:14,924 [INFO] Step[600/2713]: training loss : 1.0140699303150178 TRAIN  loss dict:  {'classification_loss': 1.0140699303150178}
2025-01-18 20:08:29,845 [INFO] Step[650/2713]: training loss : 1.025832543373108 TRAIN  loss dict:  {'classification_loss': 1.025832543373108}
2025-01-18 20:08:44,664 [INFO] Step[700/2713]: training loss : 1.0271843516826629 TRAIN  loss dict:  {'classification_loss': 1.0271843516826629}
2025-01-18 20:08:59,519 [INFO] Step[750/2713]: training loss : 1.0228822028636932 TRAIN  loss dict:  {'classification_loss': 1.0228822028636932}
2025-01-18 20:09:14,418 [INFO] Step[800/2713]: training loss : 1.0055304098129272 TRAIN  loss dict:  {'classification_loss': 1.0055304098129272}
2025-01-18 20:09:29,299 [INFO] Step[850/2713]: training loss : 1.0002360212802888 TRAIN  loss dict:  {'classification_loss': 1.0002360212802888}
2025-01-18 20:09:44,186 [INFO] Step[900/2713]: training loss : 1.0463250148296357 TRAIN  loss dict:  {'classification_loss': 1.0463250148296357}
2025-01-18 20:09:59,053 [INFO] Step[950/2713]: training loss : 1.0392698621749878 TRAIN  loss dict:  {'classification_loss': 1.0392698621749878}
2025-01-18 20:10:13,883 [INFO] Step[1000/2713]: training loss : 1.0345428216457366 TRAIN  loss dict:  {'classification_loss': 1.0345428216457366}
2025-01-18 20:10:28,746 [INFO] Step[1050/2713]: training loss : 1.0133063757419587 TRAIN  loss dict:  {'classification_loss': 1.0133063757419587}
2025-01-18 20:10:43,602 [INFO] Step[1100/2713]: training loss : 1.0088722884655 TRAIN  loss dict:  {'classification_loss': 1.0088722884655}
2025-01-18 20:10:58,436 [INFO] Step[1150/2713]: training loss : 1.0199207091331481 TRAIN  loss dict:  {'classification_loss': 1.0199207091331481}
2025-01-18 20:11:13,281 [INFO] Step[1200/2713]: training loss : 1.0541867113113403 TRAIN  loss dict:  {'classification_loss': 1.0541867113113403}
2025-01-18 20:11:28,132 [INFO] Step[1250/2713]: training loss : 1.0031840026378631 TRAIN  loss dict:  {'classification_loss': 1.0031840026378631}
2025-01-18 20:11:42,993 [INFO] Step[1300/2713]: training loss : 1.0107821667194365 TRAIN  loss dict:  {'classification_loss': 1.0107821667194365}
2025-01-18 20:11:57,907 [INFO] Step[1350/2713]: training loss : 1.023691827058792 TRAIN  loss dict:  {'classification_loss': 1.023691827058792}
2025-01-18 20:12:12,814 [INFO] Step[1400/2713]: training loss : 1.0037622940540314 TRAIN  loss dict:  {'classification_loss': 1.0037622940540314}
2025-01-18 20:12:27,704 [INFO] Step[1450/2713]: training loss : 1.0109730207920073 TRAIN  loss dict:  {'classification_loss': 1.0109730207920073}
2025-01-18 20:12:42,560 [INFO] Step[1500/2713]: training loss : 1.019383317232132 TRAIN  loss dict:  {'classification_loss': 1.019383317232132}
2025-01-18 20:12:57,447 [INFO] Step[1550/2713]: training loss : 1.0535834205150605 TRAIN  loss dict:  {'classification_loss': 1.0535834205150605}
2025-01-18 20:13:12,292 [INFO] Step[1600/2713]: training loss : 1.0318290758132935 TRAIN  loss dict:  {'classification_loss': 1.0318290758132935}
2025-01-18 20:13:27,218 [INFO] Step[1650/2713]: training loss : 1.032679170370102 TRAIN  loss dict:  {'classification_loss': 1.032679170370102}
2025-01-18 20:13:42,044 [INFO] Step[1700/2713]: training loss : 1.010552055835724 TRAIN  loss dict:  {'classification_loss': 1.010552055835724}
2025-01-18 20:13:56,924 [INFO] Step[1750/2713]: training loss : 1.0442448616027833 TRAIN  loss dict:  {'classification_loss': 1.0442448616027833}
2025-01-18 20:14:11,755 [INFO] Step[1800/2713]: training loss : 1.0128877401351928 TRAIN  loss dict:  {'classification_loss': 1.0128877401351928}
2025-01-18 20:14:26,609 [INFO] Step[1850/2713]: training loss : 1.0219961774349213 TRAIN  loss dict:  {'classification_loss': 1.0219961774349213}
2025-01-18 20:14:41,423 [INFO] Step[1900/2713]: training loss : 1.053287822008133 TRAIN  loss dict:  {'classification_loss': 1.053287822008133}
2025-01-18 20:14:56,264 [INFO] Step[1950/2713]: training loss : 1.0057509636878967 TRAIN  loss dict:  {'classification_loss': 1.0057509636878967}
2025-01-18 20:15:11,125 [INFO] Step[2000/2713]: training loss : 1.0246617913246154 TRAIN  loss dict:  {'classification_loss': 1.0246617913246154}
2025-01-18 20:15:25,970 [INFO] Step[2050/2713]: training loss : 1.0331430149078369 TRAIN  loss dict:  {'classification_loss': 1.0331430149078369}
2025-01-18 20:15:40,862 [INFO] Step[2100/2713]: training loss : 1.0185475492477416 TRAIN  loss dict:  {'classification_loss': 1.0185475492477416}
2025-01-18 20:15:55,740 [INFO] Step[2150/2713]: training loss : 1.0113330936431886 TRAIN  loss dict:  {'classification_loss': 1.0113330936431886}
2025-01-18 20:16:10,593 [INFO] Step[2200/2713]: training loss : 1.0853856134414672 TRAIN  loss dict:  {'classification_loss': 1.0853856134414672}
2025-01-18 20:16:25,445 [INFO] Step[2250/2713]: training loss : 1.0027508234977722 TRAIN  loss dict:  {'classification_loss': 1.0027508234977722}
2025-01-18 20:16:40,289 [INFO] Step[2300/2713]: training loss : 1.012093529701233 TRAIN  loss dict:  {'classification_loss': 1.012093529701233}
2025-01-18 20:16:55,198 [INFO] Step[2350/2713]: training loss : 1.0315708363056182 TRAIN  loss dict:  {'classification_loss': 1.0315708363056182}
2025-01-18 20:17:10,043 [INFO] Step[2400/2713]: training loss : 1.0892850017547608 TRAIN  loss dict:  {'classification_loss': 1.0892850017547608}
2025-01-18 20:17:24,925 [INFO] Step[2450/2713]: training loss : 1.0306867027282716 TRAIN  loss dict:  {'classification_loss': 1.0306867027282716}
2025-01-18 20:17:39,793 [INFO] Step[2500/2713]: training loss : 1.0192642211914062 TRAIN  loss dict:  {'classification_loss': 1.0192642211914062}
2025-01-18 20:17:54,657 [INFO] Step[2550/2713]: training loss : 1.0280185317993165 TRAIN  loss dict:  {'classification_loss': 1.0280185317993165}
2025-01-18 20:18:09,472 [INFO] Step[2600/2713]: training loss : 1.0480799400806426 TRAIN  loss dict:  {'classification_loss': 1.0480799400806426}
2025-01-18 20:18:24,337 [INFO] Step[2650/2713]: training loss : 1.0251374340057373 TRAIN  loss dict:  {'classification_loss': 1.0251374340057373}
2025-01-18 20:18:39,216 [INFO] Step[2700/2713]: training loss : 1.0296008706092834 TRAIN  loss dict:  {'classification_loss': 1.0296008706092834}
2025-01-18 20:19:59,070 [INFO] Label accuracies statistics:
2025-01-18 20:19:59,070 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.5, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 0.75, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 0.75, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.25, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 1.0, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.25, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.5, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 1.0, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 0.75, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.0, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-18 20:19:59,072 [INFO] [22] TRAIN  loss: 1.0248466713640287 acc: 0.9912765696031454
2025-01-18 20:19:59,072 [INFO] [22] TRAIN  loss dict: {'classification_loss': 1.0248466713640287}
2025-01-18 20:19:59,072 [INFO] [22] VALIDATION loss: 1.8125192403121102 VALIDATION acc: 0.7761755485893417
2025-01-18 20:19:59,072 [INFO] [22] VALIDATION loss dict: {'classification_loss': 1.8125192403121102}
2025-01-18 20:19:59,072 [INFO] 
2025-01-18 20:20:19,317 [INFO] Step[50/2713]: training loss : 1.0251256477832795 TRAIN  loss dict:  {'classification_loss': 1.0251256477832795}
2025-01-18 20:20:34,144 [INFO] Step[100/2713]: training loss : 1.0381255698204042 TRAIN  loss dict:  {'classification_loss': 1.0381255698204042}
2025-01-18 20:20:49,078 [INFO] Step[150/2713]: training loss : 1.0334417176246644 TRAIN  loss dict:  {'classification_loss': 1.0334417176246644}
2025-01-18 20:21:04,007 [INFO] Step[200/2713]: training loss : 1.0047277474403382 TRAIN  loss dict:  {'classification_loss': 1.0047277474403382}
2025-01-18 20:21:18,920 [INFO] Step[250/2713]: training loss : 1.0291754293441773 TRAIN  loss dict:  {'classification_loss': 1.0291754293441773}
2025-01-18 20:21:33,883 [INFO] Step[300/2713]: training loss : 1.0161987280845641 TRAIN  loss dict:  {'classification_loss': 1.0161987280845641}
2025-01-18 20:21:48,805 [INFO] Step[350/2713]: training loss : 1.0280502700805665 TRAIN  loss dict:  {'classification_loss': 1.0280502700805665}
2025-01-18 20:22:03,667 [INFO] Step[400/2713]: training loss : 1.0190262353420259 TRAIN  loss dict:  {'classification_loss': 1.0190262353420259}
2025-01-18 20:22:18,573 [INFO] Step[450/2713]: training loss : 1.0000465762615205 TRAIN  loss dict:  {'classification_loss': 1.0000465762615205}
2025-01-18 20:22:33,472 [INFO] Step[500/2713]: training loss : 1.0343670785427093 TRAIN  loss dict:  {'classification_loss': 1.0343670785427093}
2025-01-18 20:22:48,388 [INFO] Step[550/2713]: training loss : 1.0494975638389588 TRAIN  loss dict:  {'classification_loss': 1.0494975638389588}
2025-01-18 20:23:03,307 [INFO] Step[600/2713]: training loss : 1.045172622203827 TRAIN  loss dict:  {'classification_loss': 1.045172622203827}
2025-01-18 20:23:18,211 [INFO] Step[650/2713]: training loss : 1.0196629905700683 TRAIN  loss dict:  {'classification_loss': 1.0196629905700683}
2025-01-18 20:23:33,162 [INFO] Step[700/2713]: training loss : 0.999285659790039 TRAIN  loss dict:  {'classification_loss': 0.999285659790039}
2025-01-18 20:23:48,149 [INFO] Step[750/2713]: training loss : 1.0314159309864044 TRAIN  loss dict:  {'classification_loss': 1.0314159309864044}
2025-01-18 20:24:03,072 [INFO] Step[800/2713]: training loss : 1.0316899144649505 TRAIN  loss dict:  {'classification_loss': 1.0316899144649505}
2025-01-18 20:24:18,028 [INFO] Step[850/2713]: training loss : 1.015971258878708 TRAIN  loss dict:  {'classification_loss': 1.015971258878708}
2025-01-18 20:24:32,960 [INFO] Step[900/2713]: training loss : 1.0128229367733002 TRAIN  loss dict:  {'classification_loss': 1.0128229367733002}
2025-01-18 20:24:47,919 [INFO] Step[950/2713]: training loss : 1.0237817502021789 TRAIN  loss dict:  {'classification_loss': 1.0237817502021789}
2025-01-18 20:25:02,846 [INFO] Step[1000/2713]: training loss : 1.0190853679180145 TRAIN  loss dict:  {'classification_loss': 1.0190853679180145}
2025-01-18 20:25:17,812 [INFO] Step[1050/2713]: training loss : 1.008040121793747 TRAIN  loss dict:  {'classification_loss': 1.008040121793747}
2025-01-18 20:25:32,748 [INFO] Step[1100/2713]: training loss : 1.0150570011138915 TRAIN  loss dict:  {'classification_loss': 1.0150570011138915}
2025-01-18 20:25:47,805 [INFO] Step[1150/2713]: training loss : 1.0276142859458923 TRAIN  loss dict:  {'classification_loss': 1.0276142859458923}
2025-01-18 20:26:02,969 [INFO] Step[1200/2713]: training loss : 1.0136178088188172 TRAIN  loss dict:  {'classification_loss': 1.0136178088188172}
2025-01-18 20:26:18,093 [INFO] Step[1250/2713]: training loss : 1.0180814349651337 TRAIN  loss dict:  {'classification_loss': 1.0180814349651337}
2025-01-18 20:26:33,200 [INFO] Step[1300/2713]: training loss : 1.0028501665592193 TRAIN  loss dict:  {'classification_loss': 1.0028501665592193}
2025-01-18 20:26:48,360 [INFO] Step[1350/2713]: training loss : 1.0057789635658265 TRAIN  loss dict:  {'classification_loss': 1.0057789635658265}
2025-01-18 20:27:03,459 [INFO] Step[1400/2713]: training loss : 1.0138435363769531 TRAIN  loss dict:  {'classification_loss': 1.0138435363769531}
2025-01-18 20:27:18,549 [INFO] Step[1450/2713]: training loss : 1.0298486721515656 TRAIN  loss dict:  {'classification_loss': 1.0298486721515656}
2025-01-18 20:27:33,691 [INFO] Step[1500/2713]: training loss : 1.0289970636367798 TRAIN  loss dict:  {'classification_loss': 1.0289970636367798}
2025-01-18 20:27:48,844 [INFO] Step[1550/2713]: training loss : 1.0441676270961762 TRAIN  loss dict:  {'classification_loss': 1.0441676270961762}
2025-01-18 20:28:03,991 [INFO] Step[1600/2713]: training loss : 1.0230547976493836 TRAIN  loss dict:  {'classification_loss': 1.0230547976493836}
2025-01-18 20:28:19,150 [INFO] Step[1650/2713]: training loss : 1.012155660390854 TRAIN  loss dict:  {'classification_loss': 1.012155660390854}
2025-01-18 20:28:34,284 [INFO] Step[1700/2713]: training loss : 1.0282500934600831 TRAIN  loss dict:  {'classification_loss': 1.0282500934600831}
2025-01-18 20:28:49,440 [INFO] Step[1750/2713]: training loss : 1.0356248211860657 TRAIN  loss dict:  {'classification_loss': 1.0356248211860657}
2025-01-18 20:29:04,559 [INFO] Step[1800/2713]: training loss : 1.019508650302887 TRAIN  loss dict:  {'classification_loss': 1.019508650302887}
2025-01-18 20:29:19,739 [INFO] Step[1850/2713]: training loss : 1.0698196840286256 TRAIN  loss dict:  {'classification_loss': 1.0698196840286256}
2025-01-18 20:29:34,868 [INFO] Step[1900/2713]: training loss : 1.054513293504715 TRAIN  loss dict:  {'classification_loss': 1.054513293504715}
2025-01-18 20:29:50,046 [INFO] Step[1950/2713]: training loss : 1.0131890773773193 TRAIN  loss dict:  {'classification_loss': 1.0131890773773193}
2025-01-18 20:30:05,191 [INFO] Step[2000/2713]: training loss : 1.0169827163219451 TRAIN  loss dict:  {'classification_loss': 1.0169827163219451}
2025-01-18 20:30:20,313 [INFO] Step[2050/2713]: training loss : 1.026838401556015 TRAIN  loss dict:  {'classification_loss': 1.026838401556015}
2025-01-18 20:30:35,473 [INFO] Step[2100/2713]: training loss : 1.0147303938865662 TRAIN  loss dict:  {'classification_loss': 1.0147303938865662}
2025-01-18 20:30:50,632 [INFO] Step[2150/2713]: training loss : 1.0277909886837007 TRAIN  loss dict:  {'classification_loss': 1.0277909886837007}
2025-01-18 20:31:05,745 [INFO] Step[2200/2713]: training loss : 1.0226697635650634 TRAIN  loss dict:  {'classification_loss': 1.0226697635650634}
2025-01-18 20:31:20,853 [INFO] Step[2250/2713]: training loss : 1.0035718035697938 TRAIN  loss dict:  {'classification_loss': 1.0035718035697938}
2025-01-18 20:31:35,984 [INFO] Step[2300/2713]: training loss : 1.006938909292221 TRAIN  loss dict:  {'classification_loss': 1.006938909292221}
2025-01-18 20:31:51,078 [INFO] Step[2350/2713]: training loss : 1.036630493402481 TRAIN  loss dict:  {'classification_loss': 1.036630493402481}
2025-01-18 20:32:06,246 [INFO] Step[2400/2713]: training loss : 1.004733123779297 TRAIN  loss dict:  {'classification_loss': 1.004733123779297}
2025-01-18 20:32:21,389 [INFO] Step[2450/2713]: training loss : 1.0345095574855805 TRAIN  loss dict:  {'classification_loss': 1.0345095574855805}
2025-01-18 20:32:36,504 [INFO] Step[2500/2713]: training loss : 1.049030579328537 TRAIN  loss dict:  {'classification_loss': 1.049030579328537}
2025-01-18 20:32:51,688 [INFO] Step[2550/2713]: training loss : 1.0347680485248565 TRAIN  loss dict:  {'classification_loss': 1.0347680485248565}
2025-01-18 20:33:06,828 [INFO] Step[2600/2713]: training loss : 1.0184966182708741 TRAIN  loss dict:  {'classification_loss': 1.0184966182708741}
2025-01-18 20:33:21,983 [INFO] Step[2650/2713]: training loss : 1.036525058746338 TRAIN  loss dict:  {'classification_loss': 1.036525058746338}
2025-01-18 20:33:37,188 [INFO] Step[2700/2713]: training loss : 0.9965329933166504 TRAIN  loss dict:  {'classification_loss': 0.9965329933166504}
2025-01-18 20:34:57,482 [INFO] Label accuracies statistics:
2025-01-18 20:34:57,483 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.0, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 1.0, 57: 0.75, 58: 0.25, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.5, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.25, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.75, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.75, 122: 1.0, 123: 0.75, 124: 0.75, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.75, 144: 0.75, 145: 0.5, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.5, 206: 0.25, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.25, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 1.0, 233: 0.75, 234: 0.75, 235: 0.25, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.25, 240: 1.0, 241: 0.75, 242: 0.25, 243: 0.75, 244: 0.75, 245: 1.0, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 0.75, 264: 1.0, 265: 0.75, 266: 0.75, 267: 1.0, 268: 0.75, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.5, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.5, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.5, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-18 20:34:57,484 [INFO] [23] TRAIN  loss: 1.0238499192131696 acc: 0.9911537043862882
2025-01-18 20:34:57,484 [INFO] [23] TRAIN  loss dict: {'classification_loss': 1.0238499192131696}
2025-01-18 20:34:57,485 [INFO] [23] VALIDATION loss: 1.8221327194145747 VALIDATION acc: 0.773667711598746
2025-01-18 20:34:57,485 [INFO] [23] VALIDATION loss dict: {'classification_loss': 1.8221327194145747}
2025-01-18 20:34:57,485 [INFO] 
2025-01-18 20:35:17,739 [INFO] Step[50/2713]: training loss : 1.0219825887680054 TRAIN  loss dict:  {'classification_loss': 1.0219825887680054}
2025-01-18 20:35:32,867 [INFO] Step[100/2713]: training loss : 1.038351776599884 TRAIN  loss dict:  {'classification_loss': 1.038351776599884}
2025-01-18 20:35:47,934 [INFO] Step[150/2713]: training loss : 1.0088893938064576 TRAIN  loss dict:  {'classification_loss': 1.0088893938064576}
2025-01-18 20:36:03,088 [INFO] Step[200/2713]: training loss : 1.0242970609664916 TRAIN  loss dict:  {'classification_loss': 1.0242970609664916}
2025-01-18 20:36:18,214 [INFO] Step[250/2713]: training loss : 1.0062624502182007 TRAIN  loss dict:  {'classification_loss': 1.0062624502182007}
2025-01-18 20:36:33,358 [INFO] Step[300/2713]: training loss : 1.003447461128235 TRAIN  loss dict:  {'classification_loss': 1.003447461128235}
2025-01-18 20:36:48,488 [INFO] Step[350/2713]: training loss : 1.0258788418769837 TRAIN  loss dict:  {'classification_loss': 1.0258788418769837}
2025-01-18 20:37:03,597 [INFO] Step[400/2713]: training loss : 1.0093114101886749 TRAIN  loss dict:  {'classification_loss': 1.0093114101886749}
2025-01-18 20:37:18,748 [INFO] Step[450/2713]: training loss : 1.0082392370700837 TRAIN  loss dict:  {'classification_loss': 1.0082392370700837}
2025-01-18 20:37:33,878 [INFO] Step[500/2713]: training loss : 1.0003964376449586 TRAIN  loss dict:  {'classification_loss': 1.0003964376449586}
2025-01-18 20:37:49,019 [INFO] Step[550/2713]: training loss : 0.9887200129032135 TRAIN  loss dict:  {'classification_loss': 0.9887200129032135}
2025-01-18 20:38:04,167 [INFO] Step[600/2713]: training loss : 0.9975696957111359 TRAIN  loss dict:  {'classification_loss': 0.9975696957111359}
2025-01-18 20:38:19,239 [INFO] Step[650/2713]: training loss : 1.01718111038208 TRAIN  loss dict:  {'classification_loss': 1.01718111038208}
2025-01-18 20:38:34,332 [INFO] Step[700/2713]: training loss : 1.005615495443344 TRAIN  loss dict:  {'classification_loss': 1.005615495443344}
2025-01-18 20:38:49,429 [INFO] Step[750/2713]: training loss : 1.007283124923706 TRAIN  loss dict:  {'classification_loss': 1.007283124923706}
2025-01-18 20:39:04,428 [INFO] Step[800/2713]: training loss : 1.0029246509075165 TRAIN  loss dict:  {'classification_loss': 1.0029246509075165}
2025-01-18 20:39:19,555 [INFO] Step[850/2713]: training loss : 0.9985809326171875 TRAIN  loss dict:  {'classification_loss': 0.9985809326171875}
2025-01-18 20:39:34,638 [INFO] Step[900/2713]: training loss : 1.00041663646698 TRAIN  loss dict:  {'classification_loss': 1.00041663646698}
2025-01-18 20:39:49,734 [INFO] Step[950/2713]: training loss : 1.0216614520549774 TRAIN  loss dict:  {'classification_loss': 1.0216614520549774}
2025-01-18 20:40:04,831 [INFO] Step[1000/2713]: training loss : 1.0178078937530517 TRAIN  loss dict:  {'classification_loss': 1.0178078937530517}
2025-01-18 20:40:19,877 [INFO] Step[1050/2713]: training loss : 1.0449310004711152 TRAIN  loss dict:  {'classification_loss': 1.0449310004711152}
2025-01-18 20:40:34,956 [INFO] Step[1100/2713]: training loss : 1.0522643780708314 TRAIN  loss dict:  {'classification_loss': 1.0522643780708314}
2025-01-18 20:40:50,075 [INFO] Step[1150/2713]: training loss : 1.0080672323703765 TRAIN  loss dict:  {'classification_loss': 1.0080672323703765}
2025-01-18 20:41:05,160 [INFO] Step[1200/2713]: training loss : 1.0330335009098053 TRAIN  loss dict:  {'classification_loss': 1.0330335009098053}
2025-01-18 20:41:20,254 [INFO] Step[1250/2713]: training loss : 1.029708195924759 TRAIN  loss dict:  {'classification_loss': 1.029708195924759}
2025-01-18 20:41:35,344 [INFO] Step[1300/2713]: training loss : 1.02770951628685 TRAIN  loss dict:  {'classification_loss': 1.02770951628685}
2025-01-18 20:41:50,474 [INFO] Step[1350/2713]: training loss : 0.9962292897701264 TRAIN  loss dict:  {'classification_loss': 0.9962292897701264}
2025-01-18 20:42:05,593 [INFO] Step[1400/2713]: training loss : 1.0041866779327393 TRAIN  loss dict:  {'classification_loss': 1.0041866779327393}
2025-01-18 20:42:20,714 [INFO] Step[1450/2713]: training loss : 1.065240306854248 TRAIN  loss dict:  {'classification_loss': 1.065240306854248}
2025-01-18 20:42:35,798 [INFO] Step[1500/2713]: training loss : 1.0402758169174193 TRAIN  loss dict:  {'classification_loss': 1.0402758169174193}
2025-01-18 20:42:50,888 [INFO] Step[1550/2713]: training loss : 1.0192090904712676 TRAIN  loss dict:  {'classification_loss': 1.0192090904712676}
2025-01-18 20:43:06,036 [INFO] Step[1600/2713]: training loss : 1.013968094587326 TRAIN  loss dict:  {'classification_loss': 1.013968094587326}
2025-01-18 20:43:21,158 [INFO] Step[1650/2713]: training loss : 1.0504103541374206 TRAIN  loss dict:  {'classification_loss': 1.0504103541374206}
2025-01-18 20:43:36,248 [INFO] Step[1700/2713]: training loss : 1.0012035191059112 TRAIN  loss dict:  {'classification_loss': 1.0012035191059112}
2025-01-18 20:43:51,374 [INFO] Step[1750/2713]: training loss : 1.038335052728653 TRAIN  loss dict:  {'classification_loss': 1.038335052728653}
2025-01-18 20:44:06,448 [INFO] Step[1800/2713]: training loss : 0.9926320397853852 TRAIN  loss dict:  {'classification_loss': 0.9926320397853852}
2025-01-18 20:44:21,605 [INFO] Step[1850/2713]: training loss : 1.0049158227443695 TRAIN  loss dict:  {'classification_loss': 1.0049158227443695}
2025-01-18 20:44:36,668 [INFO] Step[1900/2713]: training loss : 1.0359044110774993 TRAIN  loss dict:  {'classification_loss': 1.0359044110774993}
2025-01-18 20:44:51,763 [INFO] Step[1950/2713]: training loss : 1.0302278745174407 TRAIN  loss dict:  {'classification_loss': 1.0302278745174407}
2025-01-18 20:45:06,883 [INFO] Step[2000/2713]: training loss : 1.00864697098732 TRAIN  loss dict:  {'classification_loss': 1.00864697098732}
2025-01-18 20:45:21,999 [INFO] Step[2050/2713]: training loss : 1.0092444288730622 TRAIN  loss dict:  {'classification_loss': 1.0092444288730622}
2025-01-18 20:45:37,141 [INFO] Step[2100/2713]: training loss : 1.0336278831958772 TRAIN  loss dict:  {'classification_loss': 1.0336278831958772}
2025-01-18 20:45:52,277 [INFO] Step[2150/2713]: training loss : 1.035405123233795 TRAIN  loss dict:  {'classification_loss': 1.035405123233795}
2025-01-18 20:46:07,319 [INFO] Step[2200/2713]: training loss : 0.9976330149173737 TRAIN  loss dict:  {'classification_loss': 0.9976330149173737}
2025-01-18 20:46:22,457 [INFO] Step[2250/2713]: training loss : 1.0190659320354463 TRAIN  loss dict:  {'classification_loss': 1.0190659320354463}
2025-01-18 20:46:37,495 [INFO] Step[2300/2713]: training loss : 1.0360577869415284 TRAIN  loss dict:  {'classification_loss': 1.0360577869415284}
2025-01-18 20:46:52,555 [INFO] Step[2350/2713]: training loss : 1.024332515001297 TRAIN  loss dict:  {'classification_loss': 1.024332515001297}
2025-01-18 20:47:07,654 [INFO] Step[2400/2713]: training loss : 1.0290368175506592 TRAIN  loss dict:  {'classification_loss': 1.0290368175506592}
2025-01-18 20:47:22,799 [INFO] Step[2450/2713]: training loss : 1.0521465182304381 TRAIN  loss dict:  {'classification_loss': 1.0521465182304381}
2025-01-18 20:47:37,910 [INFO] Step[2500/2713]: training loss : 1.028010766506195 TRAIN  loss dict:  {'classification_loss': 1.028010766506195}
2025-01-18 20:47:52,996 [INFO] Step[2550/2713]: training loss : 1.0196142315864563 TRAIN  loss dict:  {'classification_loss': 1.0196142315864563}
2025-01-18 20:48:08,048 [INFO] Step[2600/2713]: training loss : 1.0155629754066466 TRAIN  loss dict:  {'classification_loss': 1.0155629754066466}
2025-01-18 20:48:23,155 [INFO] Step[2650/2713]: training loss : 1.0077320647239685 TRAIN  loss dict:  {'classification_loss': 1.0077320647239685}
2025-01-18 20:48:38,224 [INFO] Step[2700/2713]: training loss : 1.0073051524162293 TRAIN  loss dict:  {'classification_loss': 1.0073051524162293}
2025-01-18 20:49:58,527 [INFO] Label accuracies statistics:
2025-01-18 20:49:58,527 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.25, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.25, 59: 0.75, 60: 0.25, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.5, 68: 1.0, 69: 0.5, 70: 0.25, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.5, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.75, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 0.75, 145: 0.5, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.25, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 1.0, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 1.0, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.25, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.25, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.25, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.0, 305: 1.0, 306: 0.5, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-18 20:49:58,529 [INFO] [24] TRAIN  loss: 1.0187725170353792 acc: 0.992013760904288
2025-01-18 20:49:58,529 [INFO] [24] TRAIN  loss dict: {'classification_loss': 1.0187725170353792}
2025-01-18 20:49:58,529 [INFO] [24] VALIDATION loss: 1.8339989566265191 VALIDATION acc: 0.7780564263322884
2025-01-18 20:49:58,529 [INFO] [24] VALIDATION loss dict: {'classification_loss': 1.8339989566265191}
2025-01-18 20:49:58,530 [INFO] 
2025-01-18 20:50:18,631 [INFO] Step[50/2713]: training loss : 1.0323134517669679 TRAIN  loss dict:  {'classification_loss': 1.0323134517669679}
2025-01-18 20:50:33,653 [INFO] Step[100/2713]: training loss : 1.013216518163681 TRAIN  loss dict:  {'classification_loss': 1.013216518163681}
2025-01-18 20:50:48,754 [INFO] Step[150/2713]: training loss : 1.003568321466446 TRAIN  loss dict:  {'classification_loss': 1.003568321466446}
2025-01-18 20:51:03,818 [INFO] Step[200/2713]: training loss : 1.0140588223934173 TRAIN  loss dict:  {'classification_loss': 1.0140588223934173}
2025-01-18 20:51:18,914 [INFO] Step[250/2713]: training loss : 1.0046002793312072 TRAIN  loss dict:  {'classification_loss': 1.0046002793312072}
2025-01-18 20:51:33,910 [INFO] Step[300/2713]: training loss : 1.026607929468155 TRAIN  loss dict:  {'classification_loss': 1.026607929468155}
2025-01-18 20:51:48,972 [INFO] Step[350/2713]: training loss : 0.9950713610649109 TRAIN  loss dict:  {'classification_loss': 0.9950713610649109}
2025-01-18 20:52:04,032 [INFO] Step[400/2713]: training loss : 1.0135709893703462 TRAIN  loss dict:  {'classification_loss': 1.0135709893703462}
2025-01-18 20:52:19,094 [INFO] Step[450/2713]: training loss : 1.0100190711021424 TRAIN  loss dict:  {'classification_loss': 1.0100190711021424}
2025-01-18 20:52:34,115 [INFO] Step[500/2713]: training loss : 0.9979803895950318 TRAIN  loss dict:  {'classification_loss': 0.9979803895950318}
2025-01-18 20:52:49,205 [INFO] Step[550/2713]: training loss : 1.0103282690048219 TRAIN  loss dict:  {'classification_loss': 1.0103282690048219}
2025-01-18 20:53:04,226 [INFO] Step[600/2713]: training loss : 0.9936044645309449 TRAIN  loss dict:  {'classification_loss': 0.9936044645309449}
2025-01-18 20:53:19,314 [INFO] Step[650/2713]: training loss : 1.0043392837047578 TRAIN  loss dict:  {'classification_loss': 1.0043392837047578}
2025-01-18 20:53:34,411 [INFO] Step[700/2713]: training loss : 1.011392252445221 TRAIN  loss dict:  {'classification_loss': 1.011392252445221}
2025-01-18 20:53:49,481 [INFO] Step[750/2713]: training loss : 1.0108252120018006 TRAIN  loss dict:  {'classification_loss': 1.0108252120018006}
2025-01-18 20:54:04,551 [INFO] Step[800/2713]: training loss : 1.0175510454177856 TRAIN  loss dict:  {'classification_loss': 1.0175510454177856}
2025-01-18 20:54:19,622 [INFO] Step[850/2713]: training loss : 0.9900158560276031 TRAIN  loss dict:  {'classification_loss': 0.9900158560276031}
2025-01-18 20:54:34,705 [INFO] Step[900/2713]: training loss : 0.990195369720459 TRAIN  loss dict:  {'classification_loss': 0.990195369720459}
2025-01-18 20:54:49,836 [INFO] Step[950/2713]: training loss : 1.00150723695755 TRAIN  loss dict:  {'classification_loss': 1.00150723695755}
2025-01-18 20:55:04,900 [INFO] Step[1000/2713]: training loss : 1.017322541475296 TRAIN  loss dict:  {'classification_loss': 1.017322541475296}
2025-01-18 20:55:20,004 [INFO] Step[1050/2713]: training loss : 1.0178813421726227 TRAIN  loss dict:  {'classification_loss': 1.0178813421726227}
2025-01-18 20:55:35,105 [INFO] Step[1100/2713]: training loss : 1.0004399037361145 TRAIN  loss dict:  {'classification_loss': 1.0004399037361145}
2025-01-18 20:55:50,159 [INFO] Step[1150/2713]: training loss : 1.030386644601822 TRAIN  loss dict:  {'classification_loss': 1.030386644601822}
2025-01-18 20:56:05,218 [INFO] Step[1200/2713]: training loss : 1.01694996714592 TRAIN  loss dict:  {'classification_loss': 1.01694996714592}
2025-01-18 20:56:20,297 [INFO] Step[1250/2713]: training loss : 1.0151005935668946 TRAIN  loss dict:  {'classification_loss': 1.0151005935668946}
2025-01-18 20:56:35,211 [INFO] Step[1300/2713]: training loss : 0.9947015821933747 TRAIN  loss dict:  {'classification_loss': 0.9947015821933747}
2025-01-18 20:56:50,063 [INFO] Step[1350/2713]: training loss : 1.0213054037094116 TRAIN  loss dict:  {'classification_loss': 1.0213054037094116}
2025-01-18 20:57:04,970 [INFO] Step[1400/2713]: training loss : 1.0088136005401611 TRAIN  loss dict:  {'classification_loss': 1.0088136005401611}
2025-01-18 20:57:19,894 [INFO] Step[1450/2713]: training loss : 1.014409112930298 TRAIN  loss dict:  {'classification_loss': 1.014409112930298}
2025-01-18 20:57:34,782 [INFO] Step[1500/2713]: training loss : 1.0130661165714263 TRAIN  loss dict:  {'classification_loss': 1.0130661165714263}
2025-01-18 20:57:49,713 [INFO] Step[1550/2713]: training loss : 1.007663778066635 TRAIN  loss dict:  {'classification_loss': 1.007663778066635}
2025-01-18 20:58:04,572 [INFO] Step[1600/2713]: training loss : 1.0054780161380767 TRAIN  loss dict:  {'classification_loss': 1.0054780161380767}
2025-01-18 20:58:19,467 [INFO] Step[1650/2713]: training loss : 1.0015196681022644 TRAIN  loss dict:  {'classification_loss': 1.0015196681022644}
2025-01-18 20:58:34,308 [INFO] Step[1700/2713]: training loss : 1.0068390440940858 TRAIN  loss dict:  {'classification_loss': 1.0068390440940858}
2025-01-18 20:58:49,190 [INFO] Step[1750/2713]: training loss : 1.0080429792404175 TRAIN  loss dict:  {'classification_loss': 1.0080429792404175}
2025-01-18 20:59:04,098 [INFO] Step[1800/2713]: training loss : 1.013624472618103 TRAIN  loss dict:  {'classification_loss': 1.013624472618103}
2025-01-18 20:59:18,987 [INFO] Step[1850/2713]: training loss : 1.0314446079730988 TRAIN  loss dict:  {'classification_loss': 1.0314446079730988}
2025-01-18 20:59:33,897 [INFO] Step[1900/2713]: training loss : 1.0084432411193847 TRAIN  loss dict:  {'classification_loss': 1.0084432411193847}
2025-01-18 20:59:48,709 [INFO] Step[1950/2713]: training loss : 0.9996380591392517 TRAIN  loss dict:  {'classification_loss': 0.9996380591392517}
2025-01-18 21:00:03,579 [INFO] Step[2000/2713]: training loss : 1.0222477614879608 TRAIN  loss dict:  {'classification_loss': 1.0222477614879608}
2025-01-18 21:00:18,477 [INFO] Step[2050/2713]: training loss : 1.0084826481342315 TRAIN  loss dict:  {'classification_loss': 1.0084826481342315}
2025-01-18 21:00:33,353 [INFO] Step[2100/2713]: training loss : 1.002840486764908 TRAIN  loss dict:  {'classification_loss': 1.002840486764908}
2025-01-18 21:00:48,241 [INFO] Step[2150/2713]: training loss : 1.009985694885254 TRAIN  loss dict:  {'classification_loss': 1.009985694885254}
2025-01-18 21:01:03,118 [INFO] Step[2200/2713]: training loss : 1.0139477062225342 TRAIN  loss dict:  {'classification_loss': 1.0139477062225342}
2025-01-18 21:01:17,988 [INFO] Step[2250/2713]: training loss : 1.0256259834766388 TRAIN  loss dict:  {'classification_loss': 1.0256259834766388}
2025-01-18 21:01:32,917 [INFO] Step[2300/2713]: training loss : 1.026713879108429 TRAIN  loss dict:  {'classification_loss': 1.026713879108429}
2025-01-18 21:01:47,866 [INFO] Step[2350/2713]: training loss : 0.9971774697303772 TRAIN  loss dict:  {'classification_loss': 0.9971774697303772}
2025-01-18 21:02:02,750 [INFO] Step[2400/2713]: training loss : 1.0142008817195893 TRAIN  loss dict:  {'classification_loss': 1.0142008817195893}
2025-01-18 21:02:17,674 [INFO] Step[2450/2713]: training loss : 1.03016517162323 TRAIN  loss dict:  {'classification_loss': 1.03016517162323}
2025-01-18 21:02:32,516 [INFO] Step[2500/2713]: training loss : 1.0165747344493865 TRAIN  loss dict:  {'classification_loss': 1.0165747344493865}
2025-01-18 21:02:47,342 [INFO] Step[2550/2713]: training loss : 1.0075947749614715 TRAIN  loss dict:  {'classification_loss': 1.0075947749614715}
2025-01-18 21:03:02,215 [INFO] Step[2600/2713]: training loss : 0.9976581645011902 TRAIN  loss dict:  {'classification_loss': 0.9976581645011902}
2025-01-18 21:03:17,145 [INFO] Step[2650/2713]: training loss : 1.0074342846870423 TRAIN  loss dict:  {'classification_loss': 1.0074342846870423}
2025-01-18 21:03:32,068 [INFO] Step[2700/2713]: training loss : 1.0217017948627471 TRAIN  loss dict:  {'classification_loss': 1.0217017948627471}
2025-01-18 21:04:51,433 [INFO] Label accuracies statistics:
2025-01-18 21:04:51,433 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.5, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.5, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 0.75, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.5, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.0, 230: 1.0, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.5, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.0, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.5, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.5, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.5, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-18 21:04:52,087 [INFO] [25] TRAIN  loss: 1.010814678985344 acc: 0.9929966826391449
2025-01-18 21:04:52,087 [INFO] [25] TRAIN  loss dict: {'classification_loss': 1.010814678985344}
2025-01-18 21:04:52,087 [INFO] [25] VALIDATION loss: 1.8192236115385716 VALIDATION acc: 0.786833855799373
2025-01-18 21:04:52,087 [INFO] [25] VALIDATION loss dict: {'classification_loss': 1.8192236115385716}
2025-01-18 21:04:52,087 [INFO] 
2025-01-18 21:05:11,935 [INFO] Step[50/2713]: training loss : 0.9939556992053986 TRAIN  loss dict:  {'classification_loss': 0.9939556992053986}
2025-01-18 21:05:26,830 [INFO] Step[100/2713]: training loss : 1.0115282475948333 TRAIN  loss dict:  {'classification_loss': 1.0115282475948333}
2025-01-18 21:05:41,785 [INFO] Step[150/2713]: training loss : 1.0413146221637726 TRAIN  loss dict:  {'classification_loss': 1.0413146221637726}
2025-01-18 21:05:56,683 [INFO] Step[200/2713]: training loss : 1.0213565325737 TRAIN  loss dict:  {'classification_loss': 1.0213565325737}
2025-01-18 21:06:11,667 [INFO] Step[250/2713]: training loss : 1.0223588168621063 TRAIN  loss dict:  {'classification_loss': 1.0223588168621063}
2025-01-18 21:06:26,565 [INFO] Step[300/2713]: training loss : 1.0595976936817169 TRAIN  loss dict:  {'classification_loss': 1.0595976936817169}
2025-01-18 21:06:41,506 [INFO] Step[350/2713]: training loss : 1.0108180510997773 TRAIN  loss dict:  {'classification_loss': 1.0108180510997773}
2025-01-18 21:06:56,388 [INFO] Step[400/2713]: training loss : 1.0232027804851531 TRAIN  loss dict:  {'classification_loss': 1.0232027804851531}
2025-01-18 21:07:11,280 [INFO] Step[450/2713]: training loss : 1.0088412058353424 TRAIN  loss dict:  {'classification_loss': 1.0088412058353424}
2025-01-18 21:07:26,183 [INFO] Step[500/2713]: training loss : 1.0219043040275573 TRAIN  loss dict:  {'classification_loss': 1.0219043040275573}
2025-01-18 21:07:41,081 [INFO] Step[550/2713]: training loss : 1.013472933769226 TRAIN  loss dict:  {'classification_loss': 1.013472933769226}
2025-01-18 21:07:56,034 [INFO] Step[600/2713]: training loss : 1.0058435785770417 TRAIN  loss dict:  {'classification_loss': 1.0058435785770417}
2025-01-18 21:08:10,881 [INFO] Step[650/2713]: training loss : 1.0382575988769531 TRAIN  loss dict:  {'classification_loss': 1.0382575988769531}
2025-01-18 21:08:25,768 [INFO] Step[700/2713]: training loss : 1.013897521495819 TRAIN  loss dict:  {'classification_loss': 1.013897521495819}
2025-01-18 21:08:40,706 [INFO] Step[750/2713]: training loss : 1.0029457819461822 TRAIN  loss dict:  {'classification_loss': 1.0029457819461822}
2025-01-18 21:08:55,636 [INFO] Step[800/2713]: training loss : 1.0088031446933747 TRAIN  loss dict:  {'classification_loss': 1.0088031446933747}
2025-01-18 21:09:10,497 [INFO] Step[850/2713]: training loss : 1.002022795677185 TRAIN  loss dict:  {'classification_loss': 1.002022795677185}
2025-01-18 21:09:25,385 [INFO] Step[900/2713]: training loss : 1.0337423825263976 TRAIN  loss dict:  {'classification_loss': 1.0337423825263976}
2025-01-18 21:09:40,352 [INFO] Step[950/2713]: training loss : 1.0074649441242218 TRAIN  loss dict:  {'classification_loss': 1.0074649441242218}
2025-01-18 21:09:55,299 [INFO] Step[1000/2713]: training loss : 1.0312343430519104 TRAIN  loss dict:  {'classification_loss': 1.0312343430519104}
2025-01-18 21:10:10,259 [INFO] Step[1050/2713]: training loss : 1.0050069296360016 TRAIN  loss dict:  {'classification_loss': 1.0050069296360016}
2025-01-18 21:10:25,173 [INFO] Step[1100/2713]: training loss : 1.0248850584030151 TRAIN  loss dict:  {'classification_loss': 1.0248850584030151}
2025-01-18 21:10:40,141 [INFO] Step[1150/2713]: training loss : 0.9973856949806214 TRAIN  loss dict:  {'classification_loss': 0.9973856949806214}
2025-01-18 21:10:55,009 [INFO] Step[1200/2713]: training loss : 0.9894295048713684 TRAIN  loss dict:  {'classification_loss': 0.9894295048713684}
2025-01-18 21:11:09,957 [INFO] Step[1250/2713]: training loss : 1.065209835767746 TRAIN  loss dict:  {'classification_loss': 1.065209835767746}
2025-01-18 21:11:24,848 [INFO] Step[1300/2713]: training loss : 1.014510383605957 TRAIN  loss dict:  {'classification_loss': 1.014510383605957}
2025-01-18 21:11:39,749 [INFO] Step[1350/2713]: training loss : 1.0543956577777862 TRAIN  loss dict:  {'classification_loss': 1.0543956577777862}
2025-01-18 21:11:54,636 [INFO] Step[1400/2713]: training loss : 1.0078179585933684 TRAIN  loss dict:  {'classification_loss': 1.0078179585933684}
2025-01-18 21:12:09,565 [INFO] Step[1450/2713]: training loss : 1.0347993803024291 TRAIN  loss dict:  {'classification_loss': 1.0347993803024291}
2025-01-18 21:12:24,485 [INFO] Step[1500/2713]: training loss : 1.0492159616947174 TRAIN  loss dict:  {'classification_loss': 1.0492159616947174}
2025-01-18 21:12:39,403 [INFO] Step[1550/2713]: training loss : 1.009353289604187 TRAIN  loss dict:  {'classification_loss': 1.009353289604187}
2025-01-18 21:12:54,336 [INFO] Step[1600/2713]: training loss : 1.0120086574554443 TRAIN  loss dict:  {'classification_loss': 1.0120086574554443}
2025-01-18 21:13:09,281 [INFO] Step[1650/2713]: training loss : 1.014748491048813 TRAIN  loss dict:  {'classification_loss': 1.014748491048813}
2025-01-18 21:13:24,276 [INFO] Step[1700/2713]: training loss : 1.0618988168239594 TRAIN  loss dict:  {'classification_loss': 1.0618988168239594}
2025-01-18 21:13:39,160 [INFO] Step[1750/2713]: training loss : 1.0061859786510468 TRAIN  loss dict:  {'classification_loss': 1.0061859786510468}
2025-01-18 21:13:54,107 [INFO] Step[1800/2713]: training loss : 1.0081213557720183 TRAIN  loss dict:  {'classification_loss': 1.0081213557720183}
2025-01-18 21:14:09,080 [INFO] Step[1850/2713]: training loss : 0.9950478279590607 TRAIN  loss dict:  {'classification_loss': 0.9950478279590607}
2025-01-18 21:14:24,048 [INFO] Step[1900/2713]: training loss : 1.035425008535385 TRAIN  loss dict:  {'classification_loss': 1.035425008535385}
2025-01-18 21:14:38,904 [INFO] Step[1950/2713]: training loss : 1.0188776707649232 TRAIN  loss dict:  {'classification_loss': 1.0188776707649232}
2025-01-18 21:14:53,820 [INFO] Step[2000/2713]: training loss : 0.9913830649852753 TRAIN  loss dict:  {'classification_loss': 0.9913830649852753}
2025-01-18 21:15:08,782 [INFO] Step[2050/2713]: training loss : 1.0397450280189515 TRAIN  loss dict:  {'classification_loss': 1.0397450280189515}
2025-01-18 21:15:23,665 [INFO] Step[2100/2713]: training loss : 1.027375648021698 TRAIN  loss dict:  {'classification_loss': 1.027375648021698}
2025-01-18 21:15:38,690 [INFO] Step[2150/2713]: training loss : 1.0325740349292756 TRAIN  loss dict:  {'classification_loss': 1.0325740349292756}
2025-01-18 21:15:53,794 [INFO] Step[2200/2713]: training loss : 0.99075031042099 TRAIN  loss dict:  {'classification_loss': 0.99075031042099}
2025-01-18 21:16:08,872 [INFO] Step[2250/2713]: training loss : 0.9884564113616944 TRAIN  loss dict:  {'classification_loss': 0.9884564113616944}
2025-01-18 21:16:23,943 [INFO] Step[2300/2713]: training loss : 1.0161480462551118 TRAIN  loss dict:  {'classification_loss': 1.0161480462551118}
2025-01-18 21:16:39,047 [INFO] Step[2350/2713]: training loss : 1.0191730618476869 TRAIN  loss dict:  {'classification_loss': 1.0191730618476869}
2025-01-18 21:16:54,135 [INFO] Step[2400/2713]: training loss : 1.0520524632930757 TRAIN  loss dict:  {'classification_loss': 1.0520524632930757}
2025-01-18 21:17:09,292 [INFO] Step[2450/2713]: training loss : 1.0103936171531678 TRAIN  loss dict:  {'classification_loss': 1.0103936171531678}
2025-01-18 21:17:24,464 [INFO] Step[2500/2713]: training loss : 0.9871507358551025 TRAIN  loss dict:  {'classification_loss': 0.9871507358551025}
2025-01-18 21:17:39,616 [INFO] Step[2550/2713]: training loss : 1.0161484229564666 TRAIN  loss dict:  {'classification_loss': 1.0161484229564666}
2025-01-18 21:17:54,780 [INFO] Step[2600/2713]: training loss : 1.0139494240283966 TRAIN  loss dict:  {'classification_loss': 1.0139494240283966}
2025-01-18 21:18:09,929 [INFO] Step[2650/2713]: training loss : 0.9966907668113708 TRAIN  loss dict:  {'classification_loss': 0.9966907668113708}
2025-01-18 21:18:25,101 [INFO] Step[2700/2713]: training loss : 0.9876795470714569 TRAIN  loss dict:  {'classification_loss': 0.9876795470714569}
2025-01-18 21:19:45,422 [INFO] Label accuracies statistics:
2025-01-18 21:19:45,422 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.5, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 1.0, 35: 0.75, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.5, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 0.75, 118: 1.0, 119: 0.5, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 0.75, 142: 0.5, 143: 0.5, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.25, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.5, 208: 0.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.0, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.25, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.25, 240: 1.0, 241: 0.75, 242: 0.5, 243: 0.75, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.25, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 1.0, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.5, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.75, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 0.5, 313: 1.0, 314: 0.75, 315: 1.0, 316: 0.5, 317: 1.0, 318: 0.5, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.25, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.5, 380: 0.75, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.5, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-18 21:19:45,424 [INFO] [26] TRAIN  loss: 1.0173562440184971 acc: 0.9901707826514313
2025-01-18 21:19:45,424 [INFO] [26] TRAIN  loss dict: {'classification_loss': 1.0173562440184971}
2025-01-18 21:19:45,424 [INFO] [26] VALIDATION loss: 1.8286407636968713 VALIDATION acc: 0.7673981191222571
2025-01-18 21:19:45,424 [INFO] [26] VALIDATION loss dict: {'classification_loss': 1.8286407636968713}
2025-01-18 21:19:45,424 [INFO] 
2025-01-18 21:20:06,221 [INFO] Step[50/2713]: training loss : 0.9933146786689758 TRAIN  loss dict:  {'classification_loss': 0.9933146786689758}
2025-01-18 21:20:21,343 [INFO] Step[100/2713]: training loss : 1.0125093054771424 TRAIN  loss dict:  {'classification_loss': 1.0125093054771424}
2025-01-18 21:20:36,504 [INFO] Step[150/2713]: training loss : 1.0255628728866577 TRAIN  loss dict:  {'classification_loss': 1.0255628728866577}
2025-01-18 21:20:51,673 [INFO] Step[200/2713]: training loss : 1.009793210029602 TRAIN  loss dict:  {'classification_loss': 1.009793210029602}
2025-01-18 21:21:06,802 [INFO] Step[250/2713]: training loss : 1.0228919887542725 TRAIN  loss dict:  {'classification_loss': 1.0228919887542725}
2025-01-18 21:21:21,764 [INFO] Step[300/2713]: training loss : 1.021489987373352 TRAIN  loss dict:  {'classification_loss': 1.021489987373352}
2025-01-18 21:21:36,665 [INFO] Step[350/2713]: training loss : 0.9963021457195282 TRAIN  loss dict:  {'classification_loss': 0.9963021457195282}
2025-01-18 21:21:51,630 [INFO] Step[400/2713]: training loss : 1.0139543867111207 TRAIN  loss dict:  {'classification_loss': 1.0139543867111207}
2025-01-18 21:22:06,538 [INFO] Step[450/2713]: training loss : 1.055242636203766 TRAIN  loss dict:  {'classification_loss': 1.055242636203766}
2025-01-18 21:22:21,457 [INFO] Step[500/2713]: training loss : 1.0014135372638702 TRAIN  loss dict:  {'classification_loss': 1.0014135372638702}
2025-01-18 21:22:36,343 [INFO] Step[550/2713]: training loss : 1.0121198987960816 TRAIN  loss dict:  {'classification_loss': 1.0121198987960816}
2025-01-18 21:22:51,213 [INFO] Step[600/2713]: training loss : 1.0023920798301698 TRAIN  loss dict:  {'classification_loss': 1.0023920798301698}
2025-01-18 21:23:06,146 [INFO] Step[650/2713]: training loss : 1.045384681224823 TRAIN  loss dict:  {'classification_loss': 1.045384681224823}
2025-01-18 21:23:21,012 [INFO] Step[700/2713]: training loss : 1.01266326546669 TRAIN  loss dict:  {'classification_loss': 1.01266326546669}
2025-01-18 21:23:35,947 [INFO] Step[750/2713]: training loss : 0.9961619973182678 TRAIN  loss dict:  {'classification_loss': 0.9961619973182678}
2025-01-18 21:23:50,836 [INFO] Step[800/2713]: training loss : 1.0132775819301605 TRAIN  loss dict:  {'classification_loss': 1.0132775819301605}
2025-01-18 21:24:05,755 [INFO] Step[850/2713]: training loss : 0.9945124888420105 TRAIN  loss dict:  {'classification_loss': 0.9945124888420105}
2025-01-18 21:24:20,692 [INFO] Step[900/2713]: training loss : 0.9902898669242859 TRAIN  loss dict:  {'classification_loss': 0.9902898669242859}
2025-01-18 21:24:35,605 [INFO] Step[950/2713]: training loss : 1.0099250125885009 TRAIN  loss dict:  {'classification_loss': 1.0099250125885009}
2025-01-18 21:24:50,470 [INFO] Step[1000/2713]: training loss : 1.0135477328300475 TRAIN  loss dict:  {'classification_loss': 1.0135477328300475}
2025-01-18 21:25:05,400 [INFO] Step[1050/2713]: training loss : 1.0125192260742188 TRAIN  loss dict:  {'classification_loss': 1.0125192260742188}
2025-01-18 21:25:20,322 [INFO] Step[1100/2713]: training loss : 0.9975560891628266 TRAIN  loss dict:  {'classification_loss': 0.9975560891628266}
2025-01-18 21:25:35,229 [INFO] Step[1150/2713]: training loss : 0.9962830770015717 TRAIN  loss dict:  {'classification_loss': 0.9962830770015717}
2025-01-18 21:25:50,054 [INFO] Step[1200/2713]: training loss : 1.004551272392273 TRAIN  loss dict:  {'classification_loss': 1.004551272392273}
2025-01-18 21:26:05,001 [INFO] Step[1250/2713]: training loss : 1.0026171708106995 TRAIN  loss dict:  {'classification_loss': 1.0026171708106995}
2025-01-18 21:26:19,887 [INFO] Step[1300/2713]: training loss : 1.0067973637580871 TRAIN  loss dict:  {'classification_loss': 1.0067973637580871}
2025-01-18 21:26:34,765 [INFO] Step[1350/2713]: training loss : 1.0128137934207917 TRAIN  loss dict:  {'classification_loss': 1.0128137934207917}
2025-01-18 21:26:49,653 [INFO] Step[1400/2713]: training loss : 0.998578063249588 TRAIN  loss dict:  {'classification_loss': 0.998578063249588}
2025-01-18 21:27:04,548 [INFO] Step[1450/2713]: training loss : 1.0283888149261475 TRAIN  loss dict:  {'classification_loss': 1.0283888149261475}
2025-01-18 21:27:19,396 [INFO] Step[1500/2713]: training loss : 0.9900097382068634 TRAIN  loss dict:  {'classification_loss': 0.9900097382068634}
2025-01-18 21:27:34,248 [INFO] Step[1550/2713]: training loss : 1.001914417743683 TRAIN  loss dict:  {'classification_loss': 1.001914417743683}
2025-01-18 21:27:49,166 [INFO] Step[1600/2713]: training loss : 1.0324295675754547 TRAIN  loss dict:  {'classification_loss': 1.0324295675754547}
2025-01-18 21:28:04,064 [INFO] Step[1650/2713]: training loss : 0.9979751837253571 TRAIN  loss dict:  {'classification_loss': 0.9979751837253571}
2025-01-18 21:28:18,971 [INFO] Step[1700/2713]: training loss : 1.0071937811374665 TRAIN  loss dict:  {'classification_loss': 1.0071937811374665}
2025-01-18 21:28:33,881 [INFO] Step[1750/2713]: training loss : 1.0016094624996186 TRAIN  loss dict:  {'classification_loss': 1.0016094624996186}
2025-01-18 21:28:48,712 [INFO] Step[1800/2713]: training loss : 0.9906401669979096 TRAIN  loss dict:  {'classification_loss': 0.9906401669979096}
2025-01-18 21:29:03,583 [INFO] Step[1850/2713]: training loss : 1.0060174942016602 TRAIN  loss dict:  {'classification_loss': 1.0060174942016602}
2025-01-18 21:29:18,477 [INFO] Step[1900/2713]: training loss : 1.0307781147956847 TRAIN  loss dict:  {'classification_loss': 1.0307781147956847}
2025-01-18 21:29:33,368 [INFO] Step[1950/2713]: training loss : 1.009230179786682 TRAIN  loss dict:  {'classification_loss': 1.009230179786682}
2025-01-18 21:29:48,303 [INFO] Step[2000/2713]: training loss : 1.012695950269699 TRAIN  loss dict:  {'classification_loss': 1.012695950269699}
2025-01-18 21:30:03,225 [INFO] Step[2050/2713]: training loss : 1.0291731822490693 TRAIN  loss dict:  {'classification_loss': 1.0291731822490693}
2025-01-18 21:30:18,148 [INFO] Step[2100/2713]: training loss : 0.9906003999710083 TRAIN  loss dict:  {'classification_loss': 0.9906003999710083}
2025-01-18 21:30:33,023 [INFO] Step[2150/2713]: training loss : 0.9969389283657074 TRAIN  loss dict:  {'classification_loss': 0.9969389283657074}
2025-01-18 21:30:47,935 [INFO] Step[2200/2713]: training loss : 1.0086785769462585 TRAIN  loss dict:  {'classification_loss': 1.0086785769462585}
2025-01-18 21:31:02,841 [INFO] Step[2250/2713]: training loss : 1.0118096768856049 TRAIN  loss dict:  {'classification_loss': 1.0118096768856049}
2025-01-18 21:31:17,745 [INFO] Step[2300/2713]: training loss : 1.01365736246109 TRAIN  loss dict:  {'classification_loss': 1.01365736246109}
2025-01-18 21:31:32,647 [INFO] Step[2350/2713]: training loss : 1.0133673441410065 TRAIN  loss dict:  {'classification_loss': 1.0133673441410065}
2025-01-18 21:31:47,558 [INFO] Step[2400/2713]: training loss : 0.9991470408439637 TRAIN  loss dict:  {'classification_loss': 0.9991470408439637}
2025-01-18 21:32:02,442 [INFO] Step[2450/2713]: training loss : 1.0356616961956024 TRAIN  loss dict:  {'classification_loss': 1.0356616961956024}
2025-01-18 21:32:17,351 [INFO] Step[2500/2713]: training loss : 1.0022932505607605 TRAIN  loss dict:  {'classification_loss': 1.0022932505607605}
2025-01-18 21:32:32,238 [INFO] Step[2550/2713]: training loss : 1.0069714486598969 TRAIN  loss dict:  {'classification_loss': 1.0069714486598969}
2025-01-18 21:32:47,119 [INFO] Step[2600/2713]: training loss : 1.0298990058898925 TRAIN  loss dict:  {'classification_loss': 1.0298990058898925}
2025-01-18 21:33:02,020 [INFO] Step[2650/2713]: training loss : 0.9988349282741547 TRAIN  loss dict:  {'classification_loss': 0.9988349282741547}
2025-01-18 21:33:16,909 [INFO] Step[2700/2713]: training loss : 1.0224104273319243 TRAIN  loss dict:  {'classification_loss': 1.0224104273319243}
2025-01-18 21:34:36,430 [INFO] Label accuracies statistics:
2025-01-18 21:34:36,430 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.75, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.5, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.25, 61: 1.0, 62: 0.5, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 0.75, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.25, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.25, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.5, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.5, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.25, 244: 0.5, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.25, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.5, 316: 0.5, 317: 1.0, 318: 0.5, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.25, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.25, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.25, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.25, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.5, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 0.75, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.5}

2025-01-18 21:34:36,432 [INFO] [27] TRAIN  loss: 1.0100261243866944 acc: 0.993119547856002
2025-01-18 21:34:36,432 [INFO] [27] TRAIN  loss dict: {'classification_loss': 1.0100261243866944}
2025-01-18 21:34:36,432 [INFO] [27] VALIDATION loss: 1.8633798544568227 VALIDATION acc: 0.7661442006269592
2025-01-18 21:34:36,432 [INFO] [27] VALIDATION loss dict: {'classification_loss': 1.8633798544568227}
2025-01-18 21:34:36,432 [INFO] 
2025-01-18 21:34:56,058 [INFO] Step[50/2713]: training loss : 1.0072869884967803 TRAIN  loss dict:  {'classification_loss': 1.0072869884967803}
2025-01-18 21:35:10,881 [INFO] Step[100/2713]: training loss : 0.9879274415969849 TRAIN  loss dict:  {'classification_loss': 0.9879274415969849}
2025-01-18 21:35:25,702 [INFO] Step[150/2713]: training loss : 1.0019114899635315 TRAIN  loss dict:  {'classification_loss': 1.0019114899635315}
2025-01-18 21:35:40,508 [INFO] Step[200/2713]: training loss : 0.9841702806949616 TRAIN  loss dict:  {'classification_loss': 0.9841702806949616}
2025-01-18 21:35:55,397 [INFO] Step[250/2713]: training loss : 1.0035263359546662 TRAIN  loss dict:  {'classification_loss': 1.0035263359546662}
2025-01-18 21:36:10,208 [INFO] Step[300/2713]: training loss : 0.9832055902481079 TRAIN  loss dict:  {'classification_loss': 0.9832055902481079}
2025-01-18 21:36:25,079 [INFO] Step[350/2713]: training loss : 1.003309895992279 TRAIN  loss dict:  {'classification_loss': 1.003309895992279}
2025-01-18 21:36:39,918 [INFO] Step[400/2713]: training loss : 1.0031267738342284 TRAIN  loss dict:  {'classification_loss': 1.0031267738342284}
2025-01-18 21:36:54,737 [INFO] Step[450/2713]: training loss : 1.0248606300354004 TRAIN  loss dict:  {'classification_loss': 1.0248606300354004}
2025-01-18 21:37:09,544 [INFO] Step[500/2713]: training loss : 1.0253534138202667 TRAIN  loss dict:  {'classification_loss': 1.0253534138202667}
2025-01-18 21:37:24,392 [INFO] Step[550/2713]: training loss : 1.0030452513694763 TRAIN  loss dict:  {'classification_loss': 1.0030452513694763}
2025-01-18 21:37:39,185 [INFO] Step[600/2713]: training loss : 0.98701615691185 TRAIN  loss dict:  {'classification_loss': 0.98701615691185}
2025-01-18 21:37:53,960 [INFO] Step[650/2713]: training loss : 0.9974022996425629 TRAIN  loss dict:  {'classification_loss': 0.9974022996425629}
2025-01-18 21:38:08,739 [INFO] Step[700/2713]: training loss : 0.9952766156196594 TRAIN  loss dict:  {'classification_loss': 0.9952766156196594}
2025-01-18 21:38:23,520 [INFO] Step[750/2713]: training loss : 1.017475267648697 TRAIN  loss dict:  {'classification_loss': 1.017475267648697}
2025-01-18 21:38:38,312 [INFO] Step[800/2713]: training loss : 0.9918534600734711 TRAIN  loss dict:  {'classification_loss': 0.9918534600734711}
2025-01-18 21:38:53,166 [INFO] Step[850/2713]: training loss : 0.9890481114387513 TRAIN  loss dict:  {'classification_loss': 0.9890481114387513}
2025-01-18 21:39:07,980 [INFO] Step[900/2713]: training loss : 1.0043799662590027 TRAIN  loss dict:  {'classification_loss': 1.0043799662590027}
2025-01-18 21:39:22,825 [INFO] Step[950/2713]: training loss : 0.9987692379951477 TRAIN  loss dict:  {'classification_loss': 0.9987692379951477}
2025-01-18 21:39:37,624 [INFO] Step[1000/2713]: training loss : 0.9936643981933594 TRAIN  loss dict:  {'classification_loss': 0.9936643981933594}
2025-01-18 21:39:52,437 [INFO] Step[1050/2713]: training loss : 1.0248647117614746 TRAIN  loss dict:  {'classification_loss': 1.0248647117614746}
2025-01-18 21:40:07,275 [INFO] Step[1100/2713]: training loss : 1.0012521958351135 TRAIN  loss dict:  {'classification_loss': 1.0012521958351135}
2025-01-18 21:40:22,134 [INFO] Step[1150/2713]: training loss : 1.0069561994075775 TRAIN  loss dict:  {'classification_loss': 1.0069561994075775}
2025-01-18 21:40:36,908 [INFO] Step[1200/2713]: training loss : 1.0124327862262725 TRAIN  loss dict:  {'classification_loss': 1.0124327862262725}
2025-01-18 21:40:51,733 [INFO] Step[1250/2713]: training loss : 0.9998951399326325 TRAIN  loss dict:  {'classification_loss': 0.9998951399326325}
2025-01-18 21:41:06,536 [INFO] Step[1300/2713]: training loss : 1.00024569272995 TRAIN  loss dict:  {'classification_loss': 1.00024569272995}
2025-01-18 21:41:21,323 [INFO] Step[1350/2713]: training loss : 0.9961608636379242 TRAIN  loss dict:  {'classification_loss': 0.9961608636379242}
2025-01-18 21:41:36,164 [INFO] Step[1400/2713]: training loss : 1.0351029109954835 TRAIN  loss dict:  {'classification_loss': 1.0351029109954835}
2025-01-18 21:41:50,969 [INFO] Step[1450/2713]: training loss : 1.0004375410079955 TRAIN  loss dict:  {'classification_loss': 1.0004375410079955}
2025-01-18 21:42:05,806 [INFO] Step[1500/2713]: training loss : 0.9851559889316559 TRAIN  loss dict:  {'classification_loss': 0.9851559889316559}
2025-01-18 21:42:20,801 [INFO] Step[1550/2713]: training loss : 0.9999002504348755 TRAIN  loss dict:  {'classification_loss': 0.9999002504348755}
2025-01-18 21:42:35,760 [INFO] Step[1600/2713]: training loss : 0.9973999714851379 TRAIN  loss dict:  {'classification_loss': 0.9973999714851379}
2025-01-18 21:42:50,789 [INFO] Step[1650/2713]: training loss : 1.032724573612213 TRAIN  loss dict:  {'classification_loss': 1.032724573612213}
2025-01-18 21:43:05,785 [INFO] Step[1700/2713]: training loss : 1.0135277915000915 TRAIN  loss dict:  {'classification_loss': 1.0135277915000915}
2025-01-18 21:43:20,761 [INFO] Step[1750/2713]: training loss : 1.005329781770706 TRAIN  loss dict:  {'classification_loss': 1.005329781770706}
2025-01-18 21:43:35,797 [INFO] Step[1800/2713]: training loss : 1.007762941122055 TRAIN  loss dict:  {'classification_loss': 1.007762941122055}
2025-01-18 21:43:50,834 [INFO] Step[1850/2713]: training loss : 1.0252310025691986 TRAIN  loss dict:  {'classification_loss': 1.0252310025691986}
2025-01-18 21:44:05,794 [INFO] Step[1900/2713]: training loss : 0.9958047544956208 TRAIN  loss dict:  {'classification_loss': 0.9958047544956208}
2025-01-18 21:44:20,837 [INFO] Step[1950/2713]: training loss : 1.0005423653125762 TRAIN  loss dict:  {'classification_loss': 1.0005423653125762}
2025-01-18 21:44:35,811 [INFO] Step[2000/2713]: training loss : 0.9977542507648468 TRAIN  loss dict:  {'classification_loss': 0.9977542507648468}
2025-01-18 21:44:50,815 [INFO] Step[2050/2713]: training loss : 1.0181715750694276 TRAIN  loss dict:  {'classification_loss': 1.0181715750694276}
2025-01-18 21:45:05,792 [INFO] Step[2100/2713]: training loss : 1.008412092924118 TRAIN  loss dict:  {'classification_loss': 1.008412092924118}
2025-01-18 21:45:20,807 [INFO] Step[2150/2713]: training loss : 1.0243742883205413 TRAIN  loss dict:  {'classification_loss': 1.0243742883205413}
2025-01-18 21:45:35,818 [INFO] Step[2200/2713]: training loss : 1.0165300571918487 TRAIN  loss dict:  {'classification_loss': 1.0165300571918487}
2025-01-18 21:45:50,812 [INFO] Step[2250/2713]: training loss : 1.0390291500091553 TRAIN  loss dict:  {'classification_loss': 1.0390291500091553}
2025-01-18 21:46:05,759 [INFO] Step[2300/2713]: training loss : 1.0184036815166473 TRAIN  loss dict:  {'classification_loss': 1.0184036815166473}
2025-01-18 21:46:20,771 [INFO] Step[2350/2713]: training loss : 1.0190504479408264 TRAIN  loss dict:  {'classification_loss': 1.0190504479408264}
2025-01-18 21:46:35,745 [INFO] Step[2400/2713]: training loss : 1.0133264529705048 TRAIN  loss dict:  {'classification_loss': 1.0133264529705048}
2025-01-18 21:46:50,773 [INFO] Step[2450/2713]: training loss : 1.0127730417251586 TRAIN  loss dict:  {'classification_loss': 1.0127730417251586}
2025-01-18 21:47:05,644 [INFO] Step[2500/2713]: training loss : 1.0010492837429046 TRAIN  loss dict:  {'classification_loss': 1.0010492837429046}
2025-01-18 21:47:20,634 [INFO] Step[2550/2713]: training loss : 1.0022063148021698 TRAIN  loss dict:  {'classification_loss': 1.0022063148021698}
2025-01-18 21:47:35,610 [INFO] Step[2600/2713]: training loss : 1.0362099528312683 TRAIN  loss dict:  {'classification_loss': 1.0362099528312683}
2025-01-18 21:47:50,588 [INFO] Step[2650/2713]: training loss : 0.9833807146549225 TRAIN  loss dict:  {'classification_loss': 0.9833807146549225}
2025-01-18 21:48:05,566 [INFO] Step[2700/2713]: training loss : 0.9947606265544892 TRAIN  loss dict:  {'classification_loss': 0.9947606265544892}
2025-01-18 21:49:25,952 [INFO] Label accuracies statistics:
2025-01-18 21:49:25,952 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.5, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.5, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 1.0, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.75, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.5, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.5, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.5, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.25, 333: 0.25, 334: 1.0, 335: 1.0, 336: 0.5, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-18 21:49:25,954 [INFO] [28] TRAIN  loss: 1.0060631413983543 acc: 0.9939796043740017
2025-01-18 21:49:25,954 [INFO] [28] TRAIN  loss dict: {'classification_loss': 1.0060631413983543}
2025-01-18 21:49:25,954 [INFO] [28] VALIDATION loss: 1.7886593140157543 VALIDATION acc: 0.786833855799373
2025-01-18 21:49:25,954 [INFO] [28] VALIDATION loss dict: {'classification_loss': 1.7886593140157543}
2025-01-18 21:49:25,954 [INFO] 
2025-01-18 21:49:45,894 [INFO] Step[50/2713]: training loss : 0.9969134652614593 TRAIN  loss dict:  {'classification_loss': 0.9969134652614593}
2025-01-18 21:50:00,939 [INFO] Step[100/2713]: training loss : 1.0000736570358277 TRAIN  loss dict:  {'classification_loss': 1.0000736570358277}
2025-01-18 21:50:16,010 [INFO] Step[150/2713]: training loss : 0.9882740604877472 TRAIN  loss dict:  {'classification_loss': 0.9882740604877472}
2025-01-18 21:50:31,029 [INFO] Step[200/2713]: training loss : 0.9958503675460816 TRAIN  loss dict:  {'classification_loss': 0.9958503675460816}
2025-01-18 21:50:46,036 [INFO] Step[250/2713]: training loss : 1.0010000085830688 TRAIN  loss dict:  {'classification_loss': 1.0010000085830688}
2025-01-18 21:51:01,090 [INFO] Step[300/2713]: training loss : 1.0177733981609345 TRAIN  loss dict:  {'classification_loss': 1.0177733981609345}
2025-01-18 21:51:16,186 [INFO] Step[350/2713]: training loss : 1.0186227035522462 TRAIN  loss dict:  {'classification_loss': 1.0186227035522462}
2025-01-18 21:51:31,189 [INFO] Step[400/2713]: training loss : 1.0195412993431092 TRAIN  loss dict:  {'classification_loss': 1.0195412993431092}
2025-01-18 21:51:46,280 [INFO] Step[450/2713]: training loss : 1.0224499332904815 TRAIN  loss dict:  {'classification_loss': 1.0224499332904815}
2025-01-18 21:52:01,296 [INFO] Step[500/2713]: training loss : 0.9963133871555329 TRAIN  loss dict:  {'classification_loss': 0.9963133871555329}
2025-01-18 21:52:16,341 [INFO] Step[550/2713]: training loss : 1.0191481983661652 TRAIN  loss dict:  {'classification_loss': 1.0191481983661652}
2025-01-18 21:52:31,391 [INFO] Step[600/2713]: training loss : 1.0233813893795014 TRAIN  loss dict:  {'classification_loss': 1.0233813893795014}
2025-01-18 21:52:46,405 [INFO] Step[650/2713]: training loss : 1.0006592631340028 TRAIN  loss dict:  {'classification_loss': 1.0006592631340028}
2025-01-18 21:53:01,430 [INFO] Step[700/2713]: training loss : 1.0253240859508514 TRAIN  loss dict:  {'classification_loss': 1.0253240859508514}
2025-01-18 21:53:16,469 [INFO] Step[750/2713]: training loss : 1.0123293089866638 TRAIN  loss dict:  {'classification_loss': 1.0123293089866638}
2025-01-18 21:53:31,519 [INFO] Step[800/2713]: training loss : 1.0127925062179566 TRAIN  loss dict:  {'classification_loss': 1.0127925062179566}
2025-01-18 21:53:46,561 [INFO] Step[850/2713]: training loss : 1.005343073606491 TRAIN  loss dict:  {'classification_loss': 1.005343073606491}
2025-01-18 21:54:01,625 [INFO] Step[900/2713]: training loss : 1.009862539768219 TRAIN  loss dict:  {'classification_loss': 1.009862539768219}
2025-01-18 21:54:16,652 [INFO] Step[950/2713]: training loss : 0.9958152878284454 TRAIN  loss dict:  {'classification_loss': 0.9958152878284454}
2025-01-18 21:54:31,728 [INFO] Step[1000/2713]: training loss : 1.0153103828430177 TRAIN  loss dict:  {'classification_loss': 1.0153103828430177}
2025-01-18 21:54:46,810 [INFO] Step[1050/2713]: training loss : 1.0122243714332582 TRAIN  loss dict:  {'classification_loss': 1.0122243714332582}
2025-01-18 21:55:01,873 [INFO] Step[1100/2713]: training loss : 1.028341338634491 TRAIN  loss dict:  {'classification_loss': 1.028341338634491}
2025-01-18 21:55:16,893 [INFO] Step[1150/2713]: training loss : 1.0276243126392364 TRAIN  loss dict:  {'classification_loss': 1.0276243126392364}
2025-01-18 21:55:32,040 [INFO] Step[1200/2713]: training loss : 0.9930386304855346 TRAIN  loss dict:  {'classification_loss': 0.9930386304855346}
2025-01-18 21:55:47,118 [INFO] Step[1250/2713]: training loss : 1.0161602473258973 TRAIN  loss dict:  {'classification_loss': 1.0161602473258973}
2025-01-18 21:56:02,165 [INFO] Step[1300/2713]: training loss : 1.020984332561493 TRAIN  loss dict:  {'classification_loss': 1.020984332561493}
2025-01-18 21:56:17,280 [INFO] Step[1350/2713]: training loss : 0.9958089661598205 TRAIN  loss dict:  {'classification_loss': 0.9958089661598205}
2025-01-18 21:56:32,353 [INFO] Step[1400/2713]: training loss : 0.9924353206157684 TRAIN  loss dict:  {'classification_loss': 0.9924353206157684}
2025-01-18 21:56:47,357 [INFO] Step[1450/2713]: training loss : 1.0057537317276002 TRAIN  loss dict:  {'classification_loss': 1.0057537317276002}
2025-01-18 21:57:02,388 [INFO] Step[1500/2713]: training loss : 1.0201556062698365 TRAIN  loss dict:  {'classification_loss': 1.0201556062698365}
2025-01-18 21:57:17,438 [INFO] Step[1550/2713]: training loss : 0.9819967746734619 TRAIN  loss dict:  {'classification_loss': 0.9819967746734619}
2025-01-18 21:57:32,515 [INFO] Step[1600/2713]: training loss : 1.01946195602417 TRAIN  loss dict:  {'classification_loss': 1.01946195602417}
2025-01-18 21:57:47,610 [INFO] Step[1650/2713]: training loss : 1.007915152311325 TRAIN  loss dict:  {'classification_loss': 1.007915152311325}
2025-01-18 21:58:02,658 [INFO] Step[1700/2713]: training loss : 1.0324654865264893 TRAIN  loss dict:  {'classification_loss': 1.0324654865264893}
2025-01-18 21:58:17,727 [INFO] Step[1750/2713]: training loss : 1.0145232582092285 TRAIN  loss dict:  {'classification_loss': 1.0145232582092285}
2025-01-18 21:58:32,771 [INFO] Step[1800/2713]: training loss : 1.011182028055191 TRAIN  loss dict:  {'classification_loss': 1.011182028055191}
2025-01-18 21:58:47,840 [INFO] Step[1850/2713]: training loss : 1.0051452970504762 TRAIN  loss dict:  {'classification_loss': 1.0051452970504762}
2025-01-18 21:59:02,881 [INFO] Step[1900/2713]: training loss : 0.9976837706565856 TRAIN  loss dict:  {'classification_loss': 0.9976837706565856}
2025-01-18 21:59:17,944 [INFO] Step[1950/2713]: training loss : 0.9951772820949555 TRAIN  loss dict:  {'classification_loss': 0.9951772820949555}
2025-01-18 21:59:32,962 [INFO] Step[2000/2713]: training loss : 1.0331806910037995 TRAIN  loss dict:  {'classification_loss': 1.0331806910037995}
2025-01-18 21:59:48,053 [INFO] Step[2050/2713]: training loss : 0.9955046427249908 TRAIN  loss dict:  {'classification_loss': 0.9955046427249908}
2025-01-18 22:00:03,063 [INFO] Step[2100/2713]: training loss : 0.9896657431125641 TRAIN  loss dict:  {'classification_loss': 0.9896657431125641}
2025-01-18 22:00:18,134 [INFO] Step[2150/2713]: training loss : 0.9935757350921631 TRAIN  loss dict:  {'classification_loss': 0.9935757350921631}
2025-01-18 22:00:33,171 [INFO] Step[2200/2713]: training loss : 0.9932618975639343 TRAIN  loss dict:  {'classification_loss': 0.9932618975639343}
2025-01-18 22:00:48,205 [INFO] Step[2250/2713]: training loss : 0.997518208026886 TRAIN  loss dict:  {'classification_loss': 0.997518208026886}
2025-01-18 22:01:03,197 [INFO] Step[2300/2713]: training loss : 0.9830226445198059 TRAIN  loss dict:  {'classification_loss': 0.9830226445198059}
2025-01-18 22:01:18,171 [INFO] Step[2350/2713]: training loss : 1.0010801005363463 TRAIN  loss dict:  {'classification_loss': 1.0010801005363463}
2025-01-18 22:01:33,142 [INFO] Step[2400/2713]: training loss : 0.9909252202510834 TRAIN  loss dict:  {'classification_loss': 0.9909252202510834}
2025-01-18 22:01:48,214 [INFO] Step[2450/2713]: training loss : 1.000155700445175 TRAIN  loss dict:  {'classification_loss': 1.000155700445175}
2025-01-18 22:02:03,277 [INFO] Step[2500/2713]: training loss : 0.9977235805988312 TRAIN  loss dict:  {'classification_loss': 0.9977235805988312}
2025-01-18 22:02:18,341 [INFO] Step[2550/2713]: training loss : 0.9989483678340911 TRAIN  loss dict:  {'classification_loss': 0.9989483678340911}
2025-01-18 22:02:33,191 [INFO] Step[2600/2713]: training loss : 1.0010427379608153 TRAIN  loss dict:  {'classification_loss': 1.0010427379608153}
2025-01-18 22:02:48,113 [INFO] Step[2650/2713]: training loss : 0.9994653499126435 TRAIN  loss dict:  {'classification_loss': 0.9994653499126435}
2025-01-18 22:03:02,972 [INFO] Step[2700/2713]: training loss : 0.9866253435611725 TRAIN  loss dict:  {'classification_loss': 0.9866253435611725}
2025-01-18 22:04:23,146 [INFO] Label accuracies statistics:
2025-01-18 22:04:23,146 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.25, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 1.0, 173: 0.75, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.5, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 1.0, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.5, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 0.5, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.25, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 1.0, 366: 0.75, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.5, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-18 22:04:24,770 [INFO] [29] TRAIN  loss: 1.0060917480467344 acc: 0.9925052217717164
2025-01-18 22:04:24,770 [INFO] [29] TRAIN  loss dict: {'classification_loss': 1.0060917480467344}
2025-01-18 22:04:24,770 [INFO] [29] VALIDATION loss: 1.756465484213112 VALIDATION acc: 0.7905956112852665
2025-01-18 22:04:24,770 [INFO] [29] VALIDATION loss dict: {'classification_loss': 1.756465484213112}
2025-01-18 22:04:24,770 [INFO] 
2025-01-18 22:04:45,353 [INFO] Step[50/2713]: training loss : 1.0080592966079711 TRAIN  loss dict:  {'classification_loss': 1.0080592966079711}
2025-01-18 22:05:00,314 [INFO] Step[100/2713]: training loss : 1.0048571372032165 TRAIN  loss dict:  {'classification_loss': 1.0048571372032165}
2025-01-18 22:05:15,208 [INFO] Step[150/2713]: training loss : 0.9901722490787506 TRAIN  loss dict:  {'classification_loss': 0.9901722490787506}
2025-01-18 22:05:30,130 [INFO] Step[200/2713]: training loss : 0.9868688774108887 TRAIN  loss dict:  {'classification_loss': 0.9868688774108887}
2025-01-18 22:05:45,021 [INFO] Step[250/2713]: training loss : 1.0024219739437104 TRAIN  loss dict:  {'classification_loss': 1.0024219739437104}
2025-01-18 22:05:59,922 [INFO] Step[300/2713]: training loss : 1.0037186920642853 TRAIN  loss dict:  {'classification_loss': 1.0037186920642853}
2025-01-18 22:06:14,830 [INFO] Step[350/2713]: training loss : 0.99772833943367 TRAIN  loss dict:  {'classification_loss': 0.99772833943367}
2025-01-18 22:06:29,723 [INFO] Step[400/2713]: training loss : 0.9947753417491912 TRAIN  loss dict:  {'classification_loss': 0.9947753417491912}
2025-01-18 22:06:44,600 [INFO] Step[450/2713]: training loss : 1.0108485102653504 TRAIN  loss dict:  {'classification_loss': 1.0108485102653504}
2025-01-18 22:06:59,446 [INFO] Step[500/2713]: training loss : 1.003528128862381 TRAIN  loss dict:  {'classification_loss': 1.003528128862381}
2025-01-18 22:07:14,325 [INFO] Step[550/2713]: training loss : 0.9966390907764435 TRAIN  loss dict:  {'classification_loss': 0.9966390907764435}
2025-01-18 22:07:29,213 [INFO] Step[600/2713]: training loss : 0.9961189103126525 TRAIN  loss dict:  {'classification_loss': 0.9961189103126525}
2025-01-18 22:07:44,140 [INFO] Step[650/2713]: training loss : 0.9959880483150482 TRAIN  loss dict:  {'classification_loss': 0.9959880483150482}
2025-01-18 22:07:59,032 [INFO] Step[700/2713]: training loss : 0.9964927136898041 TRAIN  loss dict:  {'classification_loss': 0.9964927136898041}
2025-01-18 22:08:13,948 [INFO] Step[750/2713]: training loss : 0.9895451307296753 TRAIN  loss dict:  {'classification_loss': 0.9895451307296753}
2025-01-18 22:08:28,818 [INFO] Step[800/2713]: training loss : 0.9849264371395111 TRAIN  loss dict:  {'classification_loss': 0.9849264371395111}
2025-01-18 22:08:43,719 [INFO] Step[850/2713]: training loss : 0.9964711260795593 TRAIN  loss dict:  {'classification_loss': 0.9964711260795593}
2025-01-18 22:08:58,571 [INFO] Step[900/2713]: training loss : 0.9968836975097656 TRAIN  loss dict:  {'classification_loss': 0.9968836975097656}
2025-01-18 22:09:13,449 [INFO] Step[950/2713]: training loss : 0.9885379135608673 TRAIN  loss dict:  {'classification_loss': 0.9885379135608673}
2025-01-18 22:09:28,329 [INFO] Step[1000/2713]: training loss : 1.0081161999702453 TRAIN  loss dict:  {'classification_loss': 1.0081161999702453}
2025-01-18 22:09:43,203 [INFO] Step[1050/2713]: training loss : 1.0092834520339966 TRAIN  loss dict:  {'classification_loss': 1.0092834520339966}
2025-01-18 22:09:58,084 [INFO] Step[1100/2713]: training loss : 1.024018987417221 TRAIN  loss dict:  {'classification_loss': 1.024018987417221}
2025-01-18 22:10:12,946 [INFO] Step[1150/2713]: training loss : 0.9831531870365143 TRAIN  loss dict:  {'classification_loss': 0.9831531870365143}
2025-01-18 22:10:27,820 [INFO] Step[1200/2713]: training loss : 1.021295211315155 TRAIN  loss dict:  {'classification_loss': 1.021295211315155}
2025-01-18 22:10:42,682 [INFO] Step[1250/2713]: training loss : 0.9942474257946015 TRAIN  loss dict:  {'classification_loss': 0.9942474257946015}
2025-01-18 22:10:57,544 [INFO] Step[1300/2713]: training loss : 1.011101324558258 TRAIN  loss dict:  {'classification_loss': 1.011101324558258}
2025-01-18 22:11:12,405 [INFO] Step[1350/2713]: training loss : 1.0281466436386109 TRAIN  loss dict:  {'classification_loss': 1.0281466436386109}
2025-01-18 22:11:27,315 [INFO] Step[1400/2713]: training loss : 0.9777686810493469 TRAIN  loss dict:  {'classification_loss': 0.9777686810493469}
2025-01-18 22:11:42,208 [INFO] Step[1450/2713]: training loss : 1.0323841547966004 TRAIN  loss dict:  {'classification_loss': 1.0323841547966004}
2025-01-18 22:11:57,044 [INFO] Step[1500/2713]: training loss : 1.0143807530403137 TRAIN  loss dict:  {'classification_loss': 1.0143807530403137}
2025-01-18 22:12:11,937 [INFO] Step[1550/2713]: training loss : 0.9940535855293274 TRAIN  loss dict:  {'classification_loss': 0.9940535855293274}
2025-01-18 22:12:26,817 [INFO] Step[1600/2713]: training loss : 0.9948266422748566 TRAIN  loss dict:  {'classification_loss': 0.9948266422748566}
2025-01-18 22:12:41,682 [INFO] Step[1650/2713]: training loss : 0.981692624092102 TRAIN  loss dict:  {'classification_loss': 0.981692624092102}
2025-01-18 22:12:56,577 [INFO] Step[1700/2713]: training loss : 0.9857100021839141 TRAIN  loss dict:  {'classification_loss': 0.9857100021839141}
2025-01-18 22:13:11,421 [INFO] Step[1750/2713]: training loss : 1.0265482020378114 TRAIN  loss dict:  {'classification_loss': 1.0265482020378114}
2025-01-18 22:13:26,344 [INFO] Step[1800/2713]: training loss : 0.9982905256748199 TRAIN  loss dict:  {'classification_loss': 0.9982905256748199}
2025-01-18 22:13:41,253 [INFO] Step[1850/2713]: training loss : 1.0210210239887239 TRAIN  loss dict:  {'classification_loss': 1.0210210239887239}
2025-01-18 22:13:56,112 [INFO] Step[1900/2713]: training loss : 0.9949578821659089 TRAIN  loss dict:  {'classification_loss': 0.9949578821659089}
2025-01-18 22:14:10,989 [INFO] Step[1950/2713]: training loss : 0.9836130249500274 TRAIN  loss dict:  {'classification_loss': 0.9836130249500274}
2025-01-18 22:14:25,911 [INFO] Step[2000/2713]: training loss : 1.0074875628948212 TRAIN  loss dict:  {'classification_loss': 1.0074875628948212}
2025-01-18 22:14:40,840 [INFO] Step[2050/2713]: training loss : 0.997569750547409 TRAIN  loss dict:  {'classification_loss': 0.997569750547409}
2025-01-18 22:14:55,730 [INFO] Step[2100/2713]: training loss : 1.0182583940029144 TRAIN  loss dict:  {'classification_loss': 1.0182583940029144}
2025-01-18 22:15:10,578 [INFO] Step[2150/2713]: training loss : 1.0303098225593568 TRAIN  loss dict:  {'classification_loss': 1.0303098225593568}
2025-01-18 22:15:25,467 [INFO] Step[2200/2713]: training loss : 1.0014127266407014 TRAIN  loss dict:  {'classification_loss': 1.0014127266407014}
2025-01-18 22:15:40,317 [INFO] Step[2250/2713]: training loss : 0.997441246509552 TRAIN  loss dict:  {'classification_loss': 0.997441246509552}
2025-01-18 22:15:55,222 [INFO] Step[2300/2713]: training loss : 1.0273133409023285 TRAIN  loss dict:  {'classification_loss': 1.0273133409023285}
2025-01-18 22:16:10,134 [INFO] Step[2350/2713]: training loss : 1.0015551877021789 TRAIN  loss dict:  {'classification_loss': 1.0015551877021789}
2025-01-18 22:16:25,023 [INFO] Step[2400/2713]: training loss : 1.0045267021656037 TRAIN  loss dict:  {'classification_loss': 1.0045267021656037}
2025-01-18 22:16:39,883 [INFO] Step[2450/2713]: training loss : 1.0169034576416016 TRAIN  loss dict:  {'classification_loss': 1.0169034576416016}
2025-01-18 22:16:54,768 [INFO] Step[2500/2713]: training loss : 0.9981660974025727 TRAIN  loss dict:  {'classification_loss': 0.9981660974025727}
2025-01-18 22:17:09,653 [INFO] Step[2550/2713]: training loss : 1.0409702336788178 TRAIN  loss dict:  {'classification_loss': 1.0409702336788178}
2025-01-18 22:17:24,522 [INFO] Step[2600/2713]: training loss : 1.006095825433731 TRAIN  loss dict:  {'classification_loss': 1.006095825433731}
2025-01-18 22:17:39,409 [INFO] Step[2650/2713]: training loss : 1.0023631846904755 TRAIN  loss dict:  {'classification_loss': 1.0023631846904755}
2025-01-18 22:17:54,313 [INFO] Step[2700/2713]: training loss : 1.022108587026596 TRAIN  loss dict:  {'classification_loss': 1.022108587026596}
2025-01-18 22:19:14,338 [INFO] Label accuracies statistics:
2025-01-18 22:19:14,338 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.5, 22: 0.75, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.5, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 1.0, 143: 0.75, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.25, 300: 0.75, 301: 1.0, 302: 0.25, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 1.0, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.25, 345: 0.5, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 1.0, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.25, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-18 22:19:14,340 [INFO] [30] TRAIN  loss: 1.0040426520109615 acc: 0.9938567391571446
2025-01-18 22:19:14,340 [INFO] [30] TRAIN  loss dict: {'classification_loss': 1.0040426520109615}
2025-01-18 22:19:14,340 [INFO] [30] VALIDATION loss: 1.7763922979733102 VALIDATION acc: 0.7899686520376176
2025-01-18 22:19:14,340 [INFO] [30] VALIDATION loss dict: {'classification_loss': 1.7763922979733102}
2025-01-18 22:19:14,340 [INFO] 
2025-01-18 22:19:34,783 [INFO] Step[50/2713]: training loss : 1.007834632396698 TRAIN  loss dict:  {'classification_loss': 1.007834632396698}
2025-01-18 22:19:49,653 [INFO] Step[100/2713]: training loss : 1.0053002083301543 TRAIN  loss dict:  {'classification_loss': 1.0053002083301543}
2025-01-18 22:20:04,509 [INFO] Step[150/2713]: training loss : 0.9945933949947358 TRAIN  loss dict:  {'classification_loss': 0.9945933949947358}
2025-01-18 22:20:19,415 [INFO] Step[200/2713]: training loss : 0.9821597456932067 TRAIN  loss dict:  {'classification_loss': 0.9821597456932067}
2025-01-18 22:20:34,517 [INFO] Step[250/2713]: training loss : 1.0031468427181245 TRAIN  loss dict:  {'classification_loss': 1.0031468427181245}
2025-01-18 22:20:49,307 [INFO] Step[300/2713]: training loss : 0.9889492607116699 TRAIN  loss dict:  {'classification_loss': 0.9889492607116699}
2025-01-18 22:21:04,552 [INFO] Step[350/2713]: training loss : 0.9836220455169677 TRAIN  loss dict:  {'classification_loss': 0.9836220455169677}
2025-01-18 22:21:19,868 [INFO] Step[400/2713]: training loss : 0.9904994237422943 TRAIN  loss dict:  {'classification_loss': 0.9904994237422943}
2025-01-18 22:21:35,533 [INFO] Step[450/2713]: training loss : 0.9837086117267608 TRAIN  loss dict:  {'classification_loss': 0.9837086117267608}
2025-01-18 22:21:51,278 [INFO] Step[500/2713]: training loss : 0.9907416570186615 TRAIN  loss dict:  {'classification_loss': 0.9907416570186615}
2025-01-18 22:22:07,108 [INFO] Step[550/2713]: training loss : 0.996594889163971 TRAIN  loss dict:  {'classification_loss': 0.996594889163971}
2025-01-18 22:22:22,844 [INFO] Step[600/2713]: training loss : 0.9819872152805328 TRAIN  loss dict:  {'classification_loss': 0.9819872152805328}
2025-01-18 22:22:38,647 [INFO] Step[650/2713]: training loss : 0.977776517868042 TRAIN  loss dict:  {'classification_loss': 0.977776517868042}
2025-01-18 22:22:54,454 [INFO] Step[700/2713]: training loss : 0.9889674270153046 TRAIN  loss dict:  {'classification_loss': 0.9889674270153046}
2025-01-18 22:23:10,311 [INFO] Step[750/2713]: training loss : 0.9779428219795228 TRAIN  loss dict:  {'classification_loss': 0.9779428219795228}
2025-01-18 22:23:26,114 [INFO] Step[800/2713]: training loss : 0.993609004020691 TRAIN  loss dict:  {'classification_loss': 0.993609004020691}
2025-01-18 22:23:41,899 [INFO] Step[850/2713]: training loss : 0.995217159986496 TRAIN  loss dict:  {'classification_loss': 0.995217159986496}
2025-01-18 22:23:57,720 [INFO] Step[900/2713]: training loss : 0.9898591029644013 TRAIN  loss dict:  {'classification_loss': 0.9898591029644013}
2025-01-18 22:24:13,561 [INFO] Step[950/2713]: training loss : 0.9801258432865143 TRAIN  loss dict:  {'classification_loss': 0.9801258432865143}
2025-01-18 22:24:29,370 [INFO] Step[1000/2713]: training loss : 1.0054411935806273 TRAIN  loss dict:  {'classification_loss': 1.0054411935806273}
2025-01-18 22:24:45,246 [INFO] Step[1050/2713]: training loss : 0.9860640430450439 TRAIN  loss dict:  {'classification_loss': 0.9860640430450439}
2025-01-18 22:25:01,081 [INFO] Step[1100/2713]: training loss : 0.9972163605690002 TRAIN  loss dict:  {'classification_loss': 0.9972163605690002}
2025-01-18 22:25:16,902 [INFO] Step[1150/2713]: training loss : 0.9853209114074707 TRAIN  loss dict:  {'classification_loss': 0.9853209114074707}
2025-01-18 22:25:32,742 [INFO] Step[1200/2713]: training loss : 0.9928500878810883 TRAIN  loss dict:  {'classification_loss': 0.9928500878810883}
2025-01-18 22:25:48,601 [INFO] Step[1250/2713]: training loss : 0.9909623754024506 TRAIN  loss dict:  {'classification_loss': 0.9909623754024506}
2025-01-18 22:26:04,489 [INFO] Step[1300/2713]: training loss : 0.9931476783752441 TRAIN  loss dict:  {'classification_loss': 0.9931476783752441}
2025-01-18 22:26:20,301 [INFO] Step[1350/2713]: training loss : 0.9823729467391967 TRAIN  loss dict:  {'classification_loss': 0.9823729467391967}
2025-01-18 22:26:36,114 [INFO] Step[1400/2713]: training loss : 0.9838559687137604 TRAIN  loss dict:  {'classification_loss': 0.9838559687137604}
2025-01-18 22:26:51,950 [INFO] Step[1450/2713]: training loss : 0.9865034449100495 TRAIN  loss dict:  {'classification_loss': 0.9865034449100495}
2025-01-18 22:27:07,806 [INFO] Step[1500/2713]: training loss : 0.9796142637729645 TRAIN  loss dict:  {'classification_loss': 0.9796142637729645}
2025-01-18 22:27:23,680 [INFO] Step[1550/2713]: training loss : 0.9951615738868713 TRAIN  loss dict:  {'classification_loss': 0.9951615738868713}
2025-01-18 22:27:39,520 [INFO] Step[1600/2713]: training loss : 0.9891857719421386 TRAIN  loss dict:  {'classification_loss': 0.9891857719421386}
2025-01-18 22:27:55,320 [INFO] Step[1650/2713]: training loss : 0.9889360678195953 TRAIN  loss dict:  {'classification_loss': 0.9889360678195953}
2025-01-18 22:28:11,143 [INFO] Step[1700/2713]: training loss : 0.9760984122753144 TRAIN  loss dict:  {'classification_loss': 0.9760984122753144}
2025-01-18 22:28:26,988 [INFO] Step[1750/2713]: training loss : 0.9863139653205871 TRAIN  loss dict:  {'classification_loss': 0.9863139653205871}
2025-01-18 22:28:42,847 [INFO] Step[1800/2713]: training loss : 0.9875443124771118 TRAIN  loss dict:  {'classification_loss': 0.9875443124771118}
2025-01-18 22:28:58,646 [INFO] Step[1850/2713]: training loss : 0.9793152213096619 TRAIN  loss dict:  {'classification_loss': 0.9793152213096619}
2025-01-18 22:29:14,420 [INFO] Step[1900/2713]: training loss : 0.977765885591507 TRAIN  loss dict:  {'classification_loss': 0.977765885591507}
2025-01-18 22:29:30,214 [INFO] Step[1950/2713]: training loss : 1.016024899482727 TRAIN  loss dict:  {'classification_loss': 1.016024899482727}
2025-01-18 22:29:46,031 [INFO] Step[2000/2713]: training loss : 0.9829658699035645 TRAIN  loss dict:  {'classification_loss': 0.9829658699035645}
2025-01-18 22:30:01,803 [INFO] Step[2050/2713]: training loss : 0.9807933437824249 TRAIN  loss dict:  {'classification_loss': 0.9807933437824249}
2025-01-18 22:30:17,607 [INFO] Step[2100/2713]: training loss : 0.988621860742569 TRAIN  loss dict:  {'classification_loss': 0.988621860742569}
2025-01-18 22:30:33,441 [INFO] Step[2150/2713]: training loss : 0.9886906731128693 TRAIN  loss dict:  {'classification_loss': 0.9886906731128693}
2025-01-18 22:30:49,238 [INFO] Step[2200/2713]: training loss : 0.9840277707576752 TRAIN  loss dict:  {'classification_loss': 0.9840277707576752}
2025-01-18 22:31:05,084 [INFO] Step[2250/2713]: training loss : 1.0261860394477844 TRAIN  loss dict:  {'classification_loss': 1.0261860394477844}
2025-01-18 22:31:20,864 [INFO] Step[2300/2713]: training loss : 0.9766242575645446 TRAIN  loss dict:  {'classification_loss': 0.9766242575645446}
2025-01-18 22:31:36,646 [INFO] Step[2350/2713]: training loss : 0.9802940964698792 TRAIN  loss dict:  {'classification_loss': 0.9802940964698792}
2025-01-18 22:31:52,438 [INFO] Step[2400/2713]: training loss : 0.9824527740478516 TRAIN  loss dict:  {'classification_loss': 0.9824527740478516}
2025-01-18 22:32:08,251 [INFO] Step[2450/2713]: training loss : 0.973804646730423 TRAIN  loss dict:  {'classification_loss': 0.973804646730423}
2025-01-18 22:32:23,981 [INFO] Step[2500/2713]: training loss : 0.9959492707252502 TRAIN  loss dict:  {'classification_loss': 0.9959492707252502}
2025-01-18 22:32:39,754 [INFO] Step[2550/2713]: training loss : 0.991481339931488 TRAIN  loss dict:  {'classification_loss': 0.991481339931488}
2025-01-18 22:32:55,540 [INFO] Step[2600/2713]: training loss : 0.9854139506816864 TRAIN  loss dict:  {'classification_loss': 0.9854139506816864}
2025-01-18 22:33:11,330 [INFO] Step[2650/2713]: training loss : 0.9934040594100952 TRAIN  loss dict:  {'classification_loss': 0.9934040594100952}
2025-01-18 22:33:27,137 [INFO] Step[2700/2713]: training loss : 0.9856747710704803 TRAIN  loss dict:  {'classification_loss': 0.9856747710704803}
2025-01-18 22:34:49,418 [INFO] Label accuracies statistics:
2025-01-18 22:34:49,418 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.75, 48: 0.5, 49: 1.0, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.5, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.75, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.5, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 190: 1.0, 191: 0.75, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.25, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.5, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.25, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.25, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.25, 345: 0.5, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.5, 362: 1.0, 363: 0.75, 364: 0.25, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.25, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 0.75, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 22:34:49,420 [INFO] [31] TRAIN  loss: 0.9892099369859678 acc: 0.9966826391448581
2025-01-18 22:34:49,420 [INFO] [31] TRAIN  loss dict: {'classification_loss': 0.9892099369859678}
2025-01-18 22:34:49,420 [INFO] [31] VALIDATION loss: 1.7827100319073612 VALIDATION acc: 0.7874608150470219
2025-01-18 22:34:49,420 [INFO] [31] VALIDATION loss dict: {'classification_loss': 1.7827100319073612}
2025-01-18 22:34:49,421 [INFO] 
2025-01-18 22:35:10,344 [INFO] Step[50/2713]: training loss : 0.9751173067092895 TRAIN  loss dict:  {'classification_loss': 0.9751173067092895}
2025-01-18 22:35:26,077 [INFO] Step[100/2713]: training loss : 0.9884010589122773 TRAIN  loss dict:  {'classification_loss': 0.9884010589122773}
2025-01-18 22:35:41,813 [INFO] Step[150/2713]: training loss : 0.9917849576473237 TRAIN  loss dict:  {'classification_loss': 0.9917849576473237}
2025-01-18 22:35:57,596 [INFO] Step[200/2713]: training loss : 0.9840403854846954 TRAIN  loss dict:  {'classification_loss': 0.9840403854846954}
2025-01-18 22:36:13,354 [INFO] Step[250/2713]: training loss : 0.9790295970439911 TRAIN  loss dict:  {'classification_loss': 0.9790295970439911}
2025-01-18 22:36:29,094 [INFO] Step[300/2713]: training loss : 0.9806015920639038 TRAIN  loss dict:  {'classification_loss': 0.9806015920639038}
2025-01-18 22:36:44,892 [INFO] Step[350/2713]: training loss : 0.9884310507774353 TRAIN  loss dict:  {'classification_loss': 0.9884310507774353}
2025-01-18 22:37:00,752 [INFO] Step[400/2713]: training loss : 0.9798684549331665 TRAIN  loss dict:  {'classification_loss': 0.9798684549331665}
2025-01-18 22:37:16,613 [INFO] Step[450/2713]: training loss : 0.9729047954082489 TRAIN  loss dict:  {'classification_loss': 0.9729047954082489}
2025-01-18 22:37:32,439 [INFO] Step[500/2713]: training loss : 0.985902373790741 TRAIN  loss dict:  {'classification_loss': 0.985902373790741}
2025-01-18 22:37:48,298 [INFO] Step[550/2713]: training loss : 0.976675443649292 TRAIN  loss dict:  {'classification_loss': 0.976675443649292}
2025-01-18 22:38:04,177 [INFO] Step[600/2713]: training loss : 0.9816553950309753 TRAIN  loss dict:  {'classification_loss': 0.9816553950309753}
2025-01-18 22:38:20,043 [INFO] Step[650/2713]: training loss : 0.9952474296092987 TRAIN  loss dict:  {'classification_loss': 0.9952474296092987}
2025-01-18 22:38:35,884 [INFO] Step[700/2713]: training loss : 1.0026489901542663 TRAIN  loss dict:  {'classification_loss': 1.0026489901542663}
2025-01-18 22:38:51,754 [INFO] Step[750/2713]: training loss : 1.0147007524967193 TRAIN  loss dict:  {'classification_loss': 1.0147007524967193}
2025-01-18 22:39:07,532 [INFO] Step[800/2713]: training loss : 1.0017018699645996 TRAIN  loss dict:  {'classification_loss': 1.0017018699645996}
2025-01-18 22:39:23,381 [INFO] Step[850/2713]: training loss : 0.9837525689601898 TRAIN  loss dict:  {'classification_loss': 0.9837525689601898}
2025-01-18 22:39:39,254 [INFO] Step[900/2713]: training loss : 0.9798864018917084 TRAIN  loss dict:  {'classification_loss': 0.9798864018917084}
2025-01-18 22:39:55,075 [INFO] Step[950/2713]: training loss : 1.0019703900814056 TRAIN  loss dict:  {'classification_loss': 1.0019703900814056}
2025-01-18 22:40:10,907 [INFO] Step[1000/2713]: training loss : 0.9934701502323151 TRAIN  loss dict:  {'classification_loss': 0.9934701502323151}
2025-01-18 22:40:26,763 [INFO] Step[1050/2713]: training loss : 0.9951494097709656 TRAIN  loss dict:  {'classification_loss': 0.9951494097709656}
2025-01-18 22:40:42,591 [INFO] Step[1100/2713]: training loss : 0.9868850910663605 TRAIN  loss dict:  {'classification_loss': 0.9868850910663605}
2025-01-18 22:40:58,418 [INFO] Step[1150/2713]: training loss : 0.9835466969013215 TRAIN  loss dict:  {'classification_loss': 0.9835466969013215}
2025-01-18 22:41:14,262 [INFO] Step[1200/2713]: training loss : 0.9687803518772126 TRAIN  loss dict:  {'classification_loss': 0.9687803518772126}
2025-01-18 22:41:30,129 [INFO] Step[1250/2713]: training loss : 0.9869982957839966 TRAIN  loss dict:  {'classification_loss': 0.9869982957839966}
2025-01-18 22:41:45,995 [INFO] Step[1300/2713]: training loss : 0.9816745364665985 TRAIN  loss dict:  {'classification_loss': 0.9816745364665985}
2025-01-18 22:42:01,777 [INFO] Step[1350/2713]: training loss : 0.9999799454212188 TRAIN  loss dict:  {'classification_loss': 0.9999799454212188}
2025-01-18 22:42:17,661 [INFO] Step[1400/2713]: training loss : 0.974067667722702 TRAIN  loss dict:  {'classification_loss': 0.974067667722702}
2025-01-18 22:42:33,537 [INFO] Step[1450/2713]: training loss : 0.9983110988140106 TRAIN  loss dict:  {'classification_loss': 0.9983110988140106}
2025-01-18 22:42:49,347 [INFO] Step[1500/2713]: training loss : 0.9764456677436829 TRAIN  loss dict:  {'classification_loss': 0.9764456677436829}
2025-01-18 22:43:05,179 [INFO] Step[1550/2713]: training loss : 1.003632425069809 TRAIN  loss dict:  {'classification_loss': 1.003632425069809}
2025-01-18 22:43:20,985 [INFO] Step[1600/2713]: training loss : 0.9709565091133118 TRAIN  loss dict:  {'classification_loss': 0.9709565091133118}
2025-01-18 22:43:36,818 [INFO] Step[1650/2713]: training loss : 0.9814631807804107 TRAIN  loss dict:  {'classification_loss': 0.9814631807804107}
2025-01-18 22:43:52,651 [INFO] Step[1700/2713]: training loss : 0.9750189113616944 TRAIN  loss dict:  {'classification_loss': 0.9750189113616944}
2025-01-18 22:44:08,553 [INFO] Step[1750/2713]: training loss : 1.0042944574356079 TRAIN  loss dict:  {'classification_loss': 1.0042944574356079}
2025-01-18 22:44:24,407 [INFO] Step[1800/2713]: training loss : 0.9891479432582855 TRAIN  loss dict:  {'classification_loss': 0.9891479432582855}
2025-01-18 22:44:40,294 [INFO] Step[1850/2713]: training loss : 1.011221671104431 TRAIN  loss dict:  {'classification_loss': 1.011221671104431}
2025-01-18 22:44:56,129 [INFO] Step[1900/2713]: training loss : 0.9852356731891632 TRAIN  loss dict:  {'classification_loss': 0.9852356731891632}
2025-01-18 22:45:12,001 [INFO] Step[1950/2713]: training loss : 0.9847235715389252 TRAIN  loss dict:  {'classification_loss': 0.9847235715389252}
2025-01-18 22:45:27,890 [INFO] Step[2000/2713]: training loss : 0.9760892832279205 TRAIN  loss dict:  {'classification_loss': 0.9760892832279205}
2025-01-18 22:45:43,746 [INFO] Step[2050/2713]: training loss : 1.002584878206253 TRAIN  loss dict:  {'classification_loss': 1.002584878206253}
2025-01-18 22:45:59,621 [INFO] Step[2100/2713]: training loss : 0.9762884950637818 TRAIN  loss dict:  {'classification_loss': 0.9762884950637818}
2025-01-18 22:46:15,490 [INFO] Step[2150/2713]: training loss : 0.9904238057136535 TRAIN  loss dict:  {'classification_loss': 0.9904238057136535}
2025-01-18 22:46:31,330 [INFO] Step[2200/2713]: training loss : 0.9859545707702637 TRAIN  loss dict:  {'classification_loss': 0.9859545707702637}
2025-01-18 22:46:47,168 [INFO] Step[2250/2713]: training loss : 0.9739859485626221 TRAIN  loss dict:  {'classification_loss': 0.9739859485626221}
2025-01-18 22:47:03,023 [INFO] Step[2300/2713]: training loss : 1.0068542540073395 TRAIN  loss dict:  {'classification_loss': 1.0068542540073395}
2025-01-18 22:47:18,925 [INFO] Step[2350/2713]: training loss : 0.9813480854034424 TRAIN  loss dict:  {'classification_loss': 0.9813480854034424}
2025-01-18 22:47:34,771 [INFO] Step[2400/2713]: training loss : 0.9799259054660797 TRAIN  loss dict:  {'classification_loss': 0.9799259054660797}
2025-01-18 22:47:50,618 [INFO] Step[2450/2713]: training loss : 1.0058346331119536 TRAIN  loss dict:  {'classification_loss': 1.0058346331119536}
2025-01-18 22:48:06,508 [INFO] Step[2500/2713]: training loss : 0.9842140221595764 TRAIN  loss dict:  {'classification_loss': 0.9842140221595764}
2025-01-18 22:48:22,364 [INFO] Step[2550/2713]: training loss : 0.9777419877052307 TRAIN  loss dict:  {'classification_loss': 0.9777419877052307}
2025-01-18 22:48:38,226 [INFO] Step[2600/2713]: training loss : 0.9874155759811402 TRAIN  loss dict:  {'classification_loss': 0.9874155759811402}
2025-01-18 22:48:54,078 [INFO] Step[2650/2713]: training loss : 1.0017418766021728 TRAIN  loss dict:  {'classification_loss': 1.0017418766021728}
2025-01-18 22:49:09,974 [INFO] Step[2700/2713]: training loss : 0.9761388349533081 TRAIN  loss dict:  {'classification_loss': 0.9761388349533081}
2025-01-18 22:50:34,057 [INFO] Label accuracies statistics:
2025-01-18 22:50:34,057 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.25, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.0, 55: 0.75, 56: 1.0, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 0.75, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.5, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.6666666666666666, 180: 0.5, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.5, 206: 0.25, 207: 0.75, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.25, 235: 0.5, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.5, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.25, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.25, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-18 22:50:34,059 [INFO] [32] TRAIN  loss: 0.9875208012652582 acc: 0.996559773928001
2025-01-18 22:50:34,059 [INFO] [32] TRAIN  loss dict: {'classification_loss': 0.9875208012652582}
2025-01-18 22:50:34,060 [INFO] [32] VALIDATION loss: 1.7593999236149895 VALIDATION acc: 0.7786833855799373
2025-01-18 22:50:34,060 [INFO] [32] VALIDATION loss dict: {'classification_loss': 1.7593999236149895}
2025-01-18 22:50:34,060 [INFO] 
2025-01-18 22:50:54,875 [INFO] Step[50/2713]: training loss : 1.0215989589691161 TRAIN  loss dict:  {'classification_loss': 1.0215989589691161}
2025-01-18 22:51:10,790 [INFO] Step[100/2713]: training loss : 0.9880560207366943 TRAIN  loss dict:  {'classification_loss': 0.9880560207366943}
2025-01-18 22:51:26,725 [INFO] Step[150/2713]: training loss : 0.9748983204364776 TRAIN  loss dict:  {'classification_loss': 0.9748983204364776}
2025-01-18 22:51:42,689 [INFO] Step[200/2713]: training loss : 0.9899633419513703 TRAIN  loss dict:  {'classification_loss': 0.9899633419513703}
2025-01-18 22:51:58,626 [INFO] Step[250/2713]: training loss : 1.0055099976062776 TRAIN  loss dict:  {'classification_loss': 1.0055099976062776}
2025-01-18 22:52:14,554 [INFO] Step[300/2713]: training loss : 0.9788143742084503 TRAIN  loss dict:  {'classification_loss': 0.9788143742084503}
2025-01-18 22:52:30,453 [INFO] Step[350/2713]: training loss : 0.9778645563125611 TRAIN  loss dict:  {'classification_loss': 0.9778645563125611}
2025-01-18 22:52:46,287 [INFO] Step[400/2713]: training loss : 0.9864545607566834 TRAIN  loss dict:  {'classification_loss': 0.9864545607566834}
2025-01-18 22:53:02,190 [INFO] Step[450/2713]: training loss : 0.9801372873783112 TRAIN  loss dict:  {'classification_loss': 0.9801372873783112}
2025-01-18 22:53:18,085 [INFO] Step[500/2713]: training loss : 0.9736536586284638 TRAIN  loss dict:  {'classification_loss': 0.9736536586284638}
2025-01-18 22:53:33,999 [INFO] Step[550/2713]: training loss : 1.0032102262973785 TRAIN  loss dict:  {'classification_loss': 1.0032102262973785}
2025-01-18 22:53:49,874 [INFO] Step[600/2713]: training loss : 0.9874142920970916 TRAIN  loss dict:  {'classification_loss': 0.9874142920970916}
2025-01-18 22:54:05,737 [INFO] Step[650/2713]: training loss : 0.975070424079895 TRAIN  loss dict:  {'classification_loss': 0.975070424079895}
2025-01-18 22:54:21,621 [INFO] Step[700/2713]: training loss : 0.983404996395111 TRAIN  loss dict:  {'classification_loss': 0.983404996395111}
2025-01-18 22:54:37,540 [INFO] Step[750/2713]: training loss : 0.9737227559089661 TRAIN  loss dict:  {'classification_loss': 0.9737227559089661}
2025-01-18 22:54:53,431 [INFO] Step[800/2713]: training loss : 0.987627946138382 TRAIN  loss dict:  {'classification_loss': 0.987627946138382}
2025-01-18 22:55:09,344 [INFO] Step[850/2713]: training loss : 0.9812076711654663 TRAIN  loss dict:  {'classification_loss': 0.9812076711654663}
2025-01-18 22:55:25,228 [INFO] Step[900/2713]: training loss : 0.975726797580719 TRAIN  loss dict:  {'classification_loss': 0.975726797580719}
2025-01-18 22:55:41,071 [INFO] Step[950/2713]: training loss : 0.9862069940567016 TRAIN  loss dict:  {'classification_loss': 0.9862069940567016}
2025-01-18 22:55:56,911 [INFO] Step[1000/2713]: training loss : 0.9818923628330231 TRAIN  loss dict:  {'classification_loss': 0.9818923628330231}
2025-01-18 22:56:12,806 [INFO] Step[1050/2713]: training loss : 0.9795546507835389 TRAIN  loss dict:  {'classification_loss': 0.9795546507835389}
2025-01-18 22:56:28,675 [INFO] Step[1100/2713]: training loss : 0.9907208216190339 TRAIN  loss dict:  {'classification_loss': 0.9907208216190339}
2025-01-18 22:56:44,542 [INFO] Step[1150/2713]: training loss : 0.9751920998096466 TRAIN  loss dict:  {'classification_loss': 0.9751920998096466}
2025-01-18 22:57:00,433 [INFO] Step[1200/2713]: training loss : 0.9798073065280914 TRAIN  loss dict:  {'classification_loss': 0.9798073065280914}
2025-01-18 22:57:16,367 [INFO] Step[1250/2713]: training loss : 0.9825400269031525 TRAIN  loss dict:  {'classification_loss': 0.9825400269031525}
2025-01-18 22:57:32,246 [INFO] Step[1300/2713]: training loss : 0.9792129826545716 TRAIN  loss dict:  {'classification_loss': 0.9792129826545716}
2025-01-18 22:57:48,101 [INFO] Step[1350/2713]: training loss : 0.9715966939926147 TRAIN  loss dict:  {'classification_loss': 0.9715966939926147}
2025-01-18 22:58:03,897 [INFO] Step[1400/2713]: training loss : 0.9824991393089294 TRAIN  loss dict:  {'classification_loss': 0.9824991393089294}
2025-01-18 22:58:19,722 [INFO] Step[1450/2713]: training loss : 0.9873416125774384 TRAIN  loss dict:  {'classification_loss': 0.9873416125774384}
2025-01-18 22:58:35,505 [INFO] Step[1500/2713]: training loss : 0.972046445608139 TRAIN  loss dict:  {'classification_loss': 0.972046445608139}
2025-01-18 22:58:51,383 [INFO] Step[1550/2713]: training loss : 0.9922888684272766 TRAIN  loss dict:  {'classification_loss': 0.9922888684272766}
2025-01-18 22:59:07,282 [INFO] Step[1600/2713]: training loss : 0.995101809501648 TRAIN  loss dict:  {'classification_loss': 0.995101809501648}
2025-01-18 22:59:23,147 [INFO] Step[1650/2713]: training loss : 1.017884030342102 TRAIN  loss dict:  {'classification_loss': 1.017884030342102}
2025-01-18 22:59:38,943 [INFO] Step[1700/2713]: training loss : 1.007713612318039 TRAIN  loss dict:  {'classification_loss': 1.007713612318039}
2025-01-18 22:59:54,689 [INFO] Step[1750/2713]: training loss : 0.9684713888168335 TRAIN  loss dict:  {'classification_loss': 0.9684713888168335}
2025-01-18 23:00:10,468 [INFO] Step[1800/2713]: training loss : 0.9875702893733979 TRAIN  loss dict:  {'classification_loss': 0.9875702893733979}
2025-01-18 23:00:26,262 [INFO] Step[1850/2713]: training loss : 0.9897353529930115 TRAIN  loss dict:  {'classification_loss': 0.9897353529930115}
2025-01-18 23:00:42,067 [INFO] Step[1900/2713]: training loss : 0.9743187117576599 TRAIN  loss dict:  {'classification_loss': 0.9743187117576599}
2025-01-18 23:00:57,914 [INFO] Step[1950/2713]: training loss : 0.9673093712329864 TRAIN  loss dict:  {'classification_loss': 0.9673093712329864}
2025-01-18 23:01:13,725 [INFO] Step[2000/2713]: training loss : 0.9828658103942871 TRAIN  loss dict:  {'classification_loss': 0.9828658103942871}
2025-01-18 23:01:29,502 [INFO] Step[2050/2713]: training loss : 0.9804944717884063 TRAIN  loss dict:  {'classification_loss': 0.9804944717884063}
2025-01-18 23:01:45,253 [INFO] Step[2100/2713]: training loss : 0.9743830847740174 TRAIN  loss dict:  {'classification_loss': 0.9743830847740174}
2025-01-18 23:02:01,003 [INFO] Step[2150/2713]: training loss : 0.9783754754066467 TRAIN  loss dict:  {'classification_loss': 0.9783754754066467}
2025-01-18 23:02:16,719 [INFO] Step[2200/2713]: training loss : 0.9866904377937317 TRAIN  loss dict:  {'classification_loss': 0.9866904377937317}
2025-01-18 23:02:32,499 [INFO] Step[2250/2713]: training loss : 0.9812444794178009 TRAIN  loss dict:  {'classification_loss': 0.9812444794178009}
2025-01-18 23:02:48,239 [INFO] Step[2300/2713]: training loss : 0.9745561516284943 TRAIN  loss dict:  {'classification_loss': 0.9745561516284943}
2025-01-18 23:03:03,980 [INFO] Step[2350/2713]: training loss : 1.016961349248886 TRAIN  loss dict:  {'classification_loss': 1.016961349248886}
2025-01-18 23:03:19,707 [INFO] Step[2400/2713]: training loss : 0.990885694026947 TRAIN  loss dict:  {'classification_loss': 0.990885694026947}
2025-01-18 23:03:35,488 [INFO] Step[2450/2713]: training loss : 0.9910725831985474 TRAIN  loss dict:  {'classification_loss': 0.9910725831985474}
2025-01-18 23:03:51,241 [INFO] Step[2500/2713]: training loss : 0.9836897492408753 TRAIN  loss dict:  {'classification_loss': 0.9836897492408753}
2025-01-18 23:04:07,022 [INFO] Step[2550/2713]: training loss : 0.979801150560379 TRAIN  loss dict:  {'classification_loss': 0.979801150560379}
2025-01-18 23:04:22,793 [INFO] Step[2600/2713]: training loss : 0.9833431923389435 TRAIN  loss dict:  {'classification_loss': 0.9833431923389435}
2025-01-18 23:04:38,583 [INFO] Step[2650/2713]: training loss : 0.9961558270454407 TRAIN  loss dict:  {'classification_loss': 0.9961558270454407}
2025-01-18 23:04:54,302 [INFO] Step[2700/2713]: training loss : 0.9803381025791168 TRAIN  loss dict:  {'classification_loss': 0.9803381025791168}
2025-01-18 23:06:17,068 [INFO] Label accuracies statistics:
2025-01-18 23:06:17,069 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.25, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.25, 230: 1.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.5, 246: 1.0, 247: 0.75, 248: 0.3333333333333333, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.5, 337: 0.5, 338: 1.0, 339: 0.75, 340: 0.25, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-18 23:06:18,621 [INFO] [33] TRAIN  loss: 0.985001560840227 acc: 0.9969283695785723
2025-01-18 23:06:18,621 [INFO] [33] TRAIN  loss dict: {'classification_loss': 0.985001560840227}
2025-01-18 23:06:18,621 [INFO] [33] VALIDATION loss: 1.737078930426361 VALIDATION acc: 0.7981191222570533
2025-01-18 23:06:18,621 [INFO] [33] VALIDATION loss dict: {'classification_loss': 1.737078930426361}
2025-01-18 23:06:18,622 [INFO] 
2025-01-18 23:06:40,178 [INFO] Step[50/2713]: training loss : 0.986603342294693 TRAIN  loss dict:  {'classification_loss': 0.986603342294693}
2025-01-18 23:06:55,930 [INFO] Step[100/2713]: training loss : 0.992403793334961 TRAIN  loss dict:  {'classification_loss': 0.992403793334961}
2025-01-18 23:07:11,673 [INFO] Step[150/2713]: training loss : 0.9750589966773987 TRAIN  loss dict:  {'classification_loss': 0.9750589966773987}
2025-01-18 23:07:27,428 [INFO] Step[200/2713]: training loss : 0.9744493317604065 TRAIN  loss dict:  {'classification_loss': 0.9744493317604065}
2025-01-18 23:07:43,214 [INFO] Step[250/2713]: training loss : 0.998476972579956 TRAIN  loss dict:  {'classification_loss': 0.998476972579956}
2025-01-18 23:07:58,928 [INFO] Step[300/2713]: training loss : 0.9730721056461334 TRAIN  loss dict:  {'classification_loss': 0.9730721056461334}
2025-01-18 23:08:14,671 [INFO] Step[350/2713]: training loss : 0.9852363657951355 TRAIN  loss dict:  {'classification_loss': 0.9852363657951355}
2025-01-18 23:08:30,419 [INFO] Step[400/2713]: training loss : 0.9860124480724335 TRAIN  loss dict:  {'classification_loss': 0.9860124480724335}
2025-01-18 23:08:46,169 [INFO] Step[450/2713]: training loss : 0.9750448942184449 TRAIN  loss dict:  {'classification_loss': 0.9750448942184449}
2025-01-18 23:09:01,874 [INFO] Step[500/2713]: training loss : 0.9877366507053376 TRAIN  loss dict:  {'classification_loss': 0.9877366507053376}
2025-01-18 23:09:17,633 [INFO] Step[550/2713]: training loss : 0.9839601707458496 TRAIN  loss dict:  {'classification_loss': 0.9839601707458496}
2025-01-18 23:09:33,290 [INFO] Step[600/2713]: training loss : 0.97669886469841 TRAIN  loss dict:  {'classification_loss': 0.97669886469841}
2025-01-18 23:09:49,021 [INFO] Step[650/2713]: training loss : 0.982472802400589 TRAIN  loss dict:  {'classification_loss': 0.982472802400589}
2025-01-18 23:10:04,767 [INFO] Step[700/2713]: training loss : 0.9786858880519866 TRAIN  loss dict:  {'classification_loss': 0.9786858880519866}
2025-01-18 23:10:20,507 [INFO] Step[750/2713]: training loss : 0.9642307186126708 TRAIN  loss dict:  {'classification_loss': 0.9642307186126708}
2025-01-18 23:10:36,230 [INFO] Step[800/2713]: training loss : 0.9793434226512909 TRAIN  loss dict:  {'classification_loss': 0.9793434226512909}
2025-01-18 23:10:51,979 [INFO] Step[850/2713]: training loss : 0.9966527903079987 TRAIN  loss dict:  {'classification_loss': 0.9966527903079987}
2025-01-18 23:11:07,694 [INFO] Step[900/2713]: training loss : 0.9730357789993286 TRAIN  loss dict:  {'classification_loss': 0.9730357789993286}
2025-01-18 23:11:23,455 [INFO] Step[950/2713]: training loss : 0.9884992718696595 TRAIN  loss dict:  {'classification_loss': 0.9884992718696595}
2025-01-18 23:11:39,173 [INFO] Step[1000/2713]: training loss : 0.9889701044559479 TRAIN  loss dict:  {'classification_loss': 0.9889701044559479}
2025-01-18 23:11:54,897 [INFO] Step[1050/2713]: training loss : 0.9861611211299897 TRAIN  loss dict:  {'classification_loss': 0.9861611211299897}
2025-01-18 23:12:10,644 [INFO] Step[1100/2713]: training loss : 0.9826515781879425 TRAIN  loss dict:  {'classification_loss': 0.9826515781879425}
2025-01-18 23:12:26,373 [INFO] Step[1150/2713]: training loss : 0.9779024136066437 TRAIN  loss dict:  {'classification_loss': 0.9779024136066437}
2025-01-18 23:12:42,113 [INFO] Step[1200/2713]: training loss : 0.9788314926624299 TRAIN  loss dict:  {'classification_loss': 0.9788314926624299}
2025-01-18 23:12:57,907 [INFO] Step[1250/2713]: training loss : 0.9833756065368653 TRAIN  loss dict:  {'classification_loss': 0.9833756065368653}
2025-01-18 23:13:13,666 [INFO] Step[1300/2713]: training loss : 0.974083821773529 TRAIN  loss dict:  {'classification_loss': 0.974083821773529}
2025-01-18 23:13:29,443 [INFO] Step[1350/2713]: training loss : 0.9861792016029358 TRAIN  loss dict:  {'classification_loss': 0.9861792016029358}
2025-01-18 23:13:45,281 [INFO] Step[1400/2713]: training loss : 0.9802941679954529 TRAIN  loss dict:  {'classification_loss': 0.9802941679954529}
2025-01-18 23:14:01,040 [INFO] Step[1450/2713]: training loss : 0.9783895456790924 TRAIN  loss dict:  {'classification_loss': 0.9783895456790924}
2025-01-18 23:14:16,805 [INFO] Step[1500/2713]: training loss : 1.0121480798721314 TRAIN  loss dict:  {'classification_loss': 1.0121480798721314}
2025-01-18 23:14:32,502 [INFO] Step[1550/2713]: training loss : 0.9882761132717133 TRAIN  loss dict:  {'classification_loss': 0.9882761132717133}
2025-01-18 23:14:48,279 [INFO] Step[1600/2713]: training loss : 0.9701853525638581 TRAIN  loss dict:  {'classification_loss': 0.9701853525638581}
2025-01-18 23:15:04,032 [INFO] Step[1650/2713]: training loss : 0.9747608840465546 TRAIN  loss dict:  {'classification_loss': 0.9747608840465546}
2025-01-18 23:15:19,780 [INFO] Step[1700/2713]: training loss : 0.9692223966121674 TRAIN  loss dict:  {'classification_loss': 0.9692223966121674}
2025-01-18 23:15:35,538 [INFO] Step[1750/2713]: training loss : 0.9980737447738648 TRAIN  loss dict:  {'classification_loss': 0.9980737447738648}
2025-01-18 23:15:51,242 [INFO] Step[1800/2713]: training loss : 0.9904545402526855 TRAIN  loss dict:  {'classification_loss': 0.9904545402526855}
2025-01-18 23:16:07,005 [INFO] Step[1850/2713]: training loss : 0.9929862475395203 TRAIN  loss dict:  {'classification_loss': 0.9929862475395203}
2025-01-18 23:16:22,740 [INFO] Step[1900/2713]: training loss : 0.9841538393497467 TRAIN  loss dict:  {'classification_loss': 0.9841538393497467}
2025-01-18 23:16:38,470 [INFO] Step[1950/2713]: training loss : 0.974404468536377 TRAIN  loss dict:  {'classification_loss': 0.974404468536377}
2025-01-18 23:16:54,115 [INFO] Step[2000/2713]: training loss : 0.9751644587516785 TRAIN  loss dict:  {'classification_loss': 0.9751644587516785}
2025-01-18 23:17:09,874 [INFO] Step[2050/2713]: training loss : 0.9945375919342041 TRAIN  loss dict:  {'classification_loss': 0.9945375919342041}
2025-01-18 23:17:25,576 [INFO] Step[2100/2713]: training loss : 0.9727468180656433 TRAIN  loss dict:  {'classification_loss': 0.9727468180656433}
2025-01-18 23:17:41,325 [INFO] Step[2150/2713]: training loss : 0.9893527591228485 TRAIN  loss dict:  {'classification_loss': 0.9893527591228485}
2025-01-18 23:17:57,061 [INFO] Step[2200/2713]: training loss : 0.9825766170024872 TRAIN  loss dict:  {'classification_loss': 0.9825766170024872}
2025-01-18 23:18:12,786 [INFO] Step[2250/2713]: training loss : 0.9845468318462371 TRAIN  loss dict:  {'classification_loss': 0.9845468318462371}
2025-01-18 23:18:28,532 [INFO] Step[2300/2713]: training loss : 0.9787683534622192 TRAIN  loss dict:  {'classification_loss': 0.9787683534622192}
2025-01-18 23:18:44,358 [INFO] Step[2350/2713]: training loss : 0.9825876653194427 TRAIN  loss dict:  {'classification_loss': 0.9825876653194427}
2025-01-18 23:19:00,097 [INFO] Step[2400/2713]: training loss : 0.9808091568946838 TRAIN  loss dict:  {'classification_loss': 0.9808091568946838}
2025-01-18 23:19:15,848 [INFO] Step[2450/2713]: training loss : 0.9654392087459565 TRAIN  loss dict:  {'classification_loss': 0.9654392087459565}
2025-01-18 23:19:31,622 [INFO] Step[2500/2713]: training loss : 0.9915282380580902 TRAIN  loss dict:  {'classification_loss': 0.9915282380580902}
2025-01-18 23:19:47,319 [INFO] Step[2550/2713]: training loss : 0.9918902170658112 TRAIN  loss dict:  {'classification_loss': 0.9918902170658112}
2025-01-18 23:20:03,087 [INFO] Step[2600/2713]: training loss : 0.9839187574386596 TRAIN  loss dict:  {'classification_loss': 0.9839187574386596}
2025-01-18 23:20:18,797 [INFO] Step[2650/2713]: training loss : 0.9719963765144348 TRAIN  loss dict:  {'classification_loss': 0.9719963765144348}
2025-01-18 23:20:34,631 [INFO] Step[2700/2713]: training loss : 0.9657690489292144 TRAIN  loss dict:  {'classification_loss': 0.9657690489292144}
2025-01-18 23:21:58,154 [INFO] Label accuracies statistics:
2025-01-18 23:21:58,155 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.5, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.5, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.75, 205: 0.5, 206: 0.75, 207: 1.0, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.5, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.5, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.25, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.5, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.25, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 1.0, 393: 0.25, 394: 0.5, 395: 0.75, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-18 23:21:58,156 [INFO] [34] TRAIN  loss: 0.9821333224993986 acc: 0.9964369087111439
2025-01-18 23:21:58,157 [INFO] [34] TRAIN  loss dict: {'classification_loss': 0.9821333224993986}
2025-01-18 23:21:58,157 [INFO] [34] VALIDATION loss: 1.7433235618404876 VALIDATION acc: 0.7949843260188088
2025-01-18 23:21:58,157 [INFO] [34] VALIDATION loss dict: {'classification_loss': 1.7433235618404876}
2025-01-18 23:21:58,157 [INFO] 
2025-01-18 23:22:19,533 [INFO] Step[50/2713]: training loss : 0.9868543100357056 TRAIN  loss dict:  {'classification_loss': 0.9868543100357056}
2025-01-18 23:22:35,251 [INFO] Step[100/2713]: training loss : 0.971671633720398 TRAIN  loss dict:  {'classification_loss': 0.971671633720398}
2025-01-18 23:22:50,995 [INFO] Step[150/2713]: training loss : 0.9806635987758636 TRAIN  loss dict:  {'classification_loss': 0.9806635987758636}
2025-01-18 23:23:06,776 [INFO] Step[200/2713]: training loss : 0.9747037220001221 TRAIN  loss dict:  {'classification_loss': 0.9747037220001221}
2025-01-18 23:23:22,485 [INFO] Step[250/2713]: training loss : 1.0009552347660065 TRAIN  loss dict:  {'classification_loss': 1.0009552347660065}
2025-01-18 23:23:38,204 [INFO] Step[300/2713]: training loss : 0.9732380938529969 TRAIN  loss dict:  {'classification_loss': 0.9732380938529969}
2025-01-18 23:23:53,968 [INFO] Step[350/2713]: training loss : 0.9660974085330963 TRAIN  loss dict:  {'classification_loss': 0.9660974085330963}
2025-01-18 23:24:09,677 [INFO] Step[400/2713]: training loss : 0.9877556312084198 TRAIN  loss dict:  {'classification_loss': 0.9877556312084198}
2025-01-18 23:24:25,415 [INFO] Step[450/2713]: training loss : 0.9761878263950348 TRAIN  loss dict:  {'classification_loss': 0.9761878263950348}
2025-01-18 23:24:41,093 [INFO] Step[500/2713]: training loss : 0.994176117181778 TRAIN  loss dict:  {'classification_loss': 0.994176117181778}
2025-01-18 23:24:56,856 [INFO] Step[550/2713]: training loss : 0.9914087057113647 TRAIN  loss dict:  {'classification_loss': 0.9914087057113647}
2025-01-18 23:25:12,544 [INFO] Step[600/2713]: training loss : 0.9810536050796509 TRAIN  loss dict:  {'classification_loss': 0.9810536050796509}
2025-01-18 23:25:28,225 [INFO] Step[650/2713]: training loss : 0.9728890311717987 TRAIN  loss dict:  {'classification_loss': 0.9728890311717987}
2025-01-18 23:25:43,916 [INFO] Step[700/2713]: training loss : 0.9840994131565094 TRAIN  loss dict:  {'classification_loss': 0.9840994131565094}
2025-01-18 23:25:59,649 [INFO] Step[750/2713]: training loss : 0.9763538753986358 TRAIN  loss dict:  {'classification_loss': 0.9763538753986358}
2025-01-18 23:26:15,337 [INFO] Step[800/2713]: training loss : 0.9726997780799865 TRAIN  loss dict:  {'classification_loss': 0.9726997780799865}
2025-01-18 23:26:31,063 [INFO] Step[850/2713]: training loss : 0.9972498035430908 TRAIN  loss dict:  {'classification_loss': 0.9972498035430908}
2025-01-18 23:26:46,815 [INFO] Step[900/2713]: training loss : 0.9844537270069122 TRAIN  loss dict:  {'classification_loss': 0.9844537270069122}
2025-01-18 23:27:02,565 [INFO] Step[950/2713]: training loss : 0.9767615163326263 TRAIN  loss dict:  {'classification_loss': 0.9767615163326263}
2025-01-18 23:27:18,297 [INFO] Step[1000/2713]: training loss : 0.972412371635437 TRAIN  loss dict:  {'classification_loss': 0.972412371635437}
2025-01-18 23:27:33,995 [INFO] Step[1050/2713]: training loss : 0.9805535781383514 TRAIN  loss dict:  {'classification_loss': 0.9805535781383514}
2025-01-18 23:27:49,706 [INFO] Step[1100/2713]: training loss : 0.9733139145374298 TRAIN  loss dict:  {'classification_loss': 0.9733139145374298}
2025-01-18 23:28:05,414 [INFO] Step[1150/2713]: training loss : 0.983269522190094 TRAIN  loss dict:  {'classification_loss': 0.983269522190094}
2025-01-18 23:28:21,099 [INFO] Step[1200/2713]: training loss : 0.9943250513076782 TRAIN  loss dict:  {'classification_loss': 0.9943250513076782}
2025-01-18 23:28:36,841 [INFO] Step[1250/2713]: training loss : 0.9773985350131988 TRAIN  loss dict:  {'classification_loss': 0.9773985350131988}
2025-01-18 23:28:52,554 [INFO] Step[1300/2713]: training loss : 0.9859874868392944 TRAIN  loss dict:  {'classification_loss': 0.9859874868392944}
2025-01-18 23:29:08,291 [INFO] Step[1350/2713]: training loss : 0.9748243510723114 TRAIN  loss dict:  {'classification_loss': 0.9748243510723114}
2025-01-18 23:29:24,022 [INFO] Step[1400/2713]: training loss : 0.9748626589775086 TRAIN  loss dict:  {'classification_loss': 0.9748626589775086}
2025-01-18 23:29:39,743 [INFO] Step[1450/2713]: training loss : 0.9970672810077668 TRAIN  loss dict:  {'classification_loss': 0.9970672810077668}
2025-01-18 23:29:55,486 [INFO] Step[1500/2713]: training loss : 0.9888922488689422 TRAIN  loss dict:  {'classification_loss': 0.9888922488689422}
2025-01-18 23:30:11,278 [INFO] Step[1550/2713]: training loss : 0.9728572928905487 TRAIN  loss dict:  {'classification_loss': 0.9728572928905487}
2025-01-18 23:30:27,026 [INFO] Step[1600/2713]: training loss : 0.9837712633609772 TRAIN  loss dict:  {'classification_loss': 0.9837712633609772}
2025-01-18 23:30:42,725 [INFO] Step[1650/2713]: training loss : 0.9860176301002502 TRAIN  loss dict:  {'classification_loss': 0.9860176301002502}
2025-01-18 23:30:58,445 [INFO] Step[1700/2713]: training loss : 0.9733071863651276 TRAIN  loss dict:  {'classification_loss': 0.9733071863651276}
2025-01-18 23:31:14,207 [INFO] Step[1750/2713]: training loss : 0.9745217132568359 TRAIN  loss dict:  {'classification_loss': 0.9745217132568359}
2025-01-18 23:31:29,953 [INFO] Step[1800/2713]: training loss : 0.9790394032001495 TRAIN  loss dict:  {'classification_loss': 0.9790394032001495}
2025-01-18 23:31:45,676 [INFO] Step[1850/2713]: training loss : 0.9902462458610535 TRAIN  loss dict:  {'classification_loss': 0.9902462458610535}
2025-01-18 23:32:01,338 [INFO] Step[1900/2713]: training loss : 0.9863318920135498 TRAIN  loss dict:  {'classification_loss': 0.9863318920135498}
2025-01-18 23:32:17,074 [INFO] Step[1950/2713]: training loss : 1.0009891021251678 TRAIN  loss dict:  {'classification_loss': 1.0009891021251678}
2025-01-18 23:32:32,754 [INFO] Step[2000/2713]: training loss : 0.9638786768913269 TRAIN  loss dict:  {'classification_loss': 0.9638786768913269}
2025-01-18 23:32:48,500 [INFO] Step[2050/2713]: training loss : 0.9835843253135681 TRAIN  loss dict:  {'classification_loss': 0.9835843253135681}
2025-01-18 23:33:04,233 [INFO] Step[2100/2713]: training loss : 0.9809150946140289 TRAIN  loss dict:  {'classification_loss': 0.9809150946140289}
2025-01-18 23:33:19,964 [INFO] Step[2150/2713]: training loss : 0.98267671585083 TRAIN  loss dict:  {'classification_loss': 0.98267671585083}
2025-01-18 23:33:35,698 [INFO] Step[2200/2713]: training loss : 0.9781658923625947 TRAIN  loss dict:  {'classification_loss': 0.9781658923625947}
2025-01-18 23:33:51,452 [INFO] Step[2250/2713]: training loss : 0.9724283504486084 TRAIN  loss dict:  {'classification_loss': 0.9724283504486084}
2025-01-18 23:34:07,206 [INFO] Step[2300/2713]: training loss : 0.9727424395084381 TRAIN  loss dict:  {'classification_loss': 0.9727424395084381}
2025-01-18 23:34:22,909 [INFO] Step[2350/2713]: training loss : 0.990663183927536 TRAIN  loss dict:  {'classification_loss': 0.990663183927536}
2025-01-18 23:34:38,607 [INFO] Step[2400/2713]: training loss : 0.9739976847171783 TRAIN  loss dict:  {'classification_loss': 0.9739976847171783}
2025-01-18 23:34:54,294 [INFO] Step[2450/2713]: training loss : 0.97981290102005 TRAIN  loss dict:  {'classification_loss': 0.97981290102005}
2025-01-18 23:35:10,006 [INFO] Step[2500/2713]: training loss : 0.9740617799758912 TRAIN  loss dict:  {'classification_loss': 0.9740617799758912}
2025-01-18 23:35:25,828 [INFO] Step[2550/2713]: training loss : 1.0064772176742554 TRAIN  loss dict:  {'classification_loss': 1.0064772176742554}
2025-01-18 23:35:41,511 [INFO] Step[2600/2713]: training loss : 0.9842389547824859 TRAIN  loss dict:  {'classification_loss': 0.9842389547824859}
2025-01-18 23:35:57,230 [INFO] Step[2650/2713]: training loss : 0.9896995806694031 TRAIN  loss dict:  {'classification_loss': 0.9896995806694031}
2025-01-18 23:36:13,016 [INFO] Step[2700/2713]: training loss : 0.9824427950382233 TRAIN  loss dict:  {'classification_loss': 0.9824427950382233}
2025-01-18 23:37:35,973 [INFO] Label accuracies statistics:
2025-01-18 23:37:35,973 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.5, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 0.75, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.5, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 1.0, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.25, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 1.0, 241: 0.75, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 0.75, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.25, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 1.0, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.5, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-18 23:37:35,975 [INFO] [35] TRAIN  loss: 0.9818411063233997 acc: 0.996559773928001
2025-01-18 23:37:35,975 [INFO] [35] TRAIN  loss dict: {'classification_loss': 0.9818411063233997}
2025-01-18 23:37:35,975 [INFO] [35] VALIDATION loss: 1.7862584846360343 VALIDATION acc: 0.7924764890282132
2025-01-18 23:37:35,975 [INFO] [35] VALIDATION loss dict: {'classification_loss': 1.7862584846360343}
2025-01-18 23:37:35,975 [INFO] 
2025-01-18 23:37:56,757 [INFO] Step[50/2713]: training loss : 0.9899496030807495 TRAIN  loss dict:  {'classification_loss': 0.9899496030807495}
2025-01-18 23:38:12,459 [INFO] Step[100/2713]: training loss : 0.9727962219715118 TRAIN  loss dict:  {'classification_loss': 0.9727962219715118}
2025-01-18 23:38:28,200 [INFO] Step[150/2713]: training loss : 1.0193917047977448 TRAIN  loss dict:  {'classification_loss': 1.0193917047977448}
2025-01-18 23:38:43,941 [INFO] Step[200/2713]: training loss : 0.9763785982131958 TRAIN  loss dict:  {'classification_loss': 0.9763785982131958}
2025-01-18 23:38:59,735 [INFO] Step[250/2713]: training loss : 0.9857920026779174 TRAIN  loss dict:  {'classification_loss': 0.9857920026779174}
2025-01-18 23:39:15,434 [INFO] Step[300/2713]: training loss : 0.9673976862430572 TRAIN  loss dict:  {'classification_loss': 0.9673976862430572}
2025-01-18 23:39:31,194 [INFO] Step[350/2713]: training loss : 0.9786602210998535 TRAIN  loss dict:  {'classification_loss': 0.9786602210998535}
2025-01-18 23:39:47,090 [INFO] Step[400/2713]: training loss : 0.9719090843200684 TRAIN  loss dict:  {'classification_loss': 0.9719090843200684}
2025-01-18 23:40:02,989 [INFO] Step[450/2713]: training loss : 0.9948710215091705 TRAIN  loss dict:  {'classification_loss': 0.9948710215091705}
2025-01-18 23:40:18,845 [INFO] Step[500/2713]: training loss : 0.9823429346084595 TRAIN  loss dict:  {'classification_loss': 0.9823429346084595}
2025-01-18 23:40:34,729 [INFO] Step[550/2713]: training loss : 0.9724134171009063 TRAIN  loss dict:  {'classification_loss': 0.9724134171009063}
2025-01-18 23:40:50,579 [INFO] Step[600/2713]: training loss : 0.9720762979984283 TRAIN  loss dict:  {'classification_loss': 0.9720762979984283}
2025-01-18 23:41:06,447 [INFO] Step[650/2713]: training loss : 0.9995716679096222 TRAIN  loss dict:  {'classification_loss': 0.9995716679096222}
2025-01-18 23:41:22,283 [INFO] Step[700/2713]: training loss : 0.9761071479320527 TRAIN  loss dict:  {'classification_loss': 0.9761071479320527}
2025-01-18 23:41:38,190 [INFO] Step[750/2713]: training loss : 0.9866189241409302 TRAIN  loss dict:  {'classification_loss': 0.9866189241409302}
2025-01-18 23:41:54,058 [INFO] Step[800/2713]: training loss : 0.9675608110427857 TRAIN  loss dict:  {'classification_loss': 0.9675608110427857}
2025-01-18 23:42:09,936 [INFO] Step[850/2713]: training loss : 0.9689162445068359 TRAIN  loss dict:  {'classification_loss': 0.9689162445068359}
2025-01-18 23:42:25,825 [INFO] Step[900/2713]: training loss : 0.9687233185768127 TRAIN  loss dict:  {'classification_loss': 0.9687233185768127}
2025-01-18 23:42:41,796 [INFO] Step[950/2713]: training loss : 0.9776908123493194 TRAIN  loss dict:  {'classification_loss': 0.9776908123493194}
2025-01-18 23:42:57,652 [INFO] Step[1000/2713]: training loss : 0.9640518653392792 TRAIN  loss dict:  {'classification_loss': 0.9640518653392792}
2025-01-18 23:43:13,527 [INFO] Step[1050/2713]: training loss : 0.9831785917282104 TRAIN  loss dict:  {'classification_loss': 0.9831785917282104}
2025-01-18 23:43:29,406 [INFO] Step[1100/2713]: training loss : 0.9809549534320832 TRAIN  loss dict:  {'classification_loss': 0.9809549534320832}
2025-01-18 23:43:45,286 [INFO] Step[1150/2713]: training loss : 0.973354367017746 TRAIN  loss dict:  {'classification_loss': 0.973354367017746}
2025-01-18 23:44:01,167 [INFO] Step[1200/2713]: training loss : 0.9958480405807495 TRAIN  loss dict:  {'classification_loss': 0.9958480405807495}
2025-01-18 23:44:17,070 [INFO] Step[1250/2713]: training loss : 0.9834649240970612 TRAIN  loss dict:  {'classification_loss': 0.9834649240970612}
2025-01-18 23:44:32,987 [INFO] Step[1300/2713]: training loss : 0.9655615425109864 TRAIN  loss dict:  {'classification_loss': 0.9655615425109864}
2025-01-18 23:44:48,870 [INFO] Step[1350/2713]: training loss : 0.9937628901004791 TRAIN  loss dict:  {'classification_loss': 0.9937628901004791}
2025-01-18 23:45:04,648 [INFO] Step[1400/2713]: training loss : 0.9664426696300507 TRAIN  loss dict:  {'classification_loss': 0.9664426696300507}
2025-01-18 23:45:20,524 [INFO] Step[1450/2713]: training loss : 0.9812902760505676 TRAIN  loss dict:  {'classification_loss': 0.9812902760505676}
2025-01-18 23:45:36,389 [INFO] Step[1500/2713]: training loss : 0.98235426902771 TRAIN  loss dict:  {'classification_loss': 0.98235426902771}
2025-01-18 23:45:52,254 [INFO] Step[1550/2713]: training loss : 0.9740420019626618 TRAIN  loss dict:  {'classification_loss': 0.9740420019626618}
2025-01-18 23:46:08,133 [INFO] Step[1600/2713]: training loss : 0.9993593037128449 TRAIN  loss dict:  {'classification_loss': 0.9993593037128449}
2025-01-18 23:46:23,946 [INFO] Step[1650/2713]: training loss : 0.9731187438964843 TRAIN  loss dict:  {'classification_loss': 0.9731187438964843}
2025-01-18 23:46:39,840 [INFO] Step[1700/2713]: training loss : 0.9832463192939759 TRAIN  loss dict:  {'classification_loss': 0.9832463192939759}
2025-01-18 23:46:55,670 [INFO] Step[1750/2713]: training loss : 0.9966882824897766 TRAIN  loss dict:  {'classification_loss': 0.9966882824897766}
2025-01-18 23:47:11,531 [INFO] Step[1800/2713]: training loss : 0.9745207381248474 TRAIN  loss dict:  {'classification_loss': 0.9745207381248474}
2025-01-18 23:47:27,407 [INFO] Step[1850/2713]: training loss : 1.0130645763874053 TRAIN  loss dict:  {'classification_loss': 1.0130645763874053}
2025-01-18 23:47:43,255 [INFO] Step[1900/2713]: training loss : 0.969808474779129 TRAIN  loss dict:  {'classification_loss': 0.969808474779129}
2025-01-18 23:47:59,151 [INFO] Step[1950/2713]: training loss : 0.9729072749614716 TRAIN  loss dict:  {'classification_loss': 0.9729072749614716}
2025-01-18 23:48:15,052 [INFO] Step[2000/2713]: training loss : 0.9757094395160675 TRAIN  loss dict:  {'classification_loss': 0.9757094395160675}
2025-01-18 23:48:30,986 [INFO] Step[2050/2713]: training loss : 0.985114985704422 TRAIN  loss dict:  {'classification_loss': 0.985114985704422}
2025-01-18 23:48:46,862 [INFO] Step[2100/2713]: training loss : 0.9734477722644805 TRAIN  loss dict:  {'classification_loss': 0.9734477722644805}
2025-01-18 23:49:02,723 [INFO] Step[2150/2713]: training loss : 1.0040684258937835 TRAIN  loss dict:  {'classification_loss': 1.0040684258937835}
2025-01-18 23:49:18,566 [INFO] Step[2200/2713]: training loss : 0.9671660125255584 TRAIN  loss dict:  {'classification_loss': 0.9671660125255584}
2025-01-18 23:49:34,479 [INFO] Step[2250/2713]: training loss : 0.979411289691925 TRAIN  loss dict:  {'classification_loss': 0.979411289691925}
2025-01-18 23:49:50,422 [INFO] Step[2300/2713]: training loss : 0.9772802436351776 TRAIN  loss dict:  {'classification_loss': 0.9772802436351776}
2025-01-18 23:50:06,311 [INFO] Step[2350/2713]: training loss : 0.9733875215053558 TRAIN  loss dict:  {'classification_loss': 0.9733875215053558}
2025-01-18 23:50:22,172 [INFO] Step[2400/2713]: training loss : 0.9726783692836761 TRAIN  loss dict:  {'classification_loss': 0.9726783692836761}
2025-01-18 23:50:38,017 [INFO] Step[2450/2713]: training loss : 0.9605719029903412 TRAIN  loss dict:  {'classification_loss': 0.9605719029903412}
2025-01-18 23:50:53,910 [INFO] Step[2500/2713]: training loss : 0.9686690652370453 TRAIN  loss dict:  {'classification_loss': 0.9686690652370453}
2025-01-18 23:51:09,772 [INFO] Step[2550/2713]: training loss : 0.9790741968154907 TRAIN  loss dict:  {'classification_loss': 0.9790741968154907}
2025-01-18 23:51:25,679 [INFO] Step[2600/2713]: training loss : 0.9649199438095093 TRAIN  loss dict:  {'classification_loss': 0.9649199438095093}
2025-01-18 23:51:41,619 [INFO] Step[2650/2713]: training loss : 0.9955798661708832 TRAIN  loss dict:  {'classification_loss': 0.9955798661708832}
2025-01-18 23:51:57,478 [INFO] Step[2700/2713]: training loss : 0.9706914818286896 TRAIN  loss dict:  {'classification_loss': 0.9706914818286896}
2025-01-18 23:53:20,775 [INFO] Label accuracies statistics:
2025-01-18 23:53:20,775 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 1.0, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.75, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.5, 206: 0.5, 207: 0.5, 208: 0.0, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 1.0, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.25, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-18 23:53:22,268 [INFO] [36] TRAIN  loss: 0.979647284607759 acc: 0.996559773928001
2025-01-18 23:53:22,268 [INFO] [36] TRAIN  loss dict: {'classification_loss': 0.979647284607759}
2025-01-18 23:53:22,269 [INFO] [36] VALIDATION loss: 1.715571349164597 VALIDATION acc: 0.8025078369905956
2025-01-18 23:53:22,269 [INFO] [36] VALIDATION loss dict: {'classification_loss': 1.715571349164597}
2025-01-18 23:53:22,269 [INFO] 
2025-01-18 23:53:42,791 [INFO] Step[50/2713]: training loss : 0.9698712813854218 TRAIN  loss dict:  {'classification_loss': 0.9698712813854218}
2025-01-18 23:53:58,714 [INFO] Step[100/2713]: training loss : 0.9722266530990601 TRAIN  loss dict:  {'classification_loss': 0.9722266530990601}
2025-01-18 23:54:14,613 [INFO] Step[150/2713]: training loss : 0.9645214521884918 TRAIN  loss dict:  {'classification_loss': 0.9645214521884918}
2025-01-18 23:54:30,537 [INFO] Step[200/2713]: training loss : 0.9910517156124115 TRAIN  loss dict:  {'classification_loss': 0.9910517156124115}
2025-01-18 23:54:46,406 [INFO] Step[250/2713]: training loss : 0.9807066333293915 TRAIN  loss dict:  {'classification_loss': 0.9807066333293915}
2025-01-18 23:55:02,308 [INFO] Step[300/2713]: training loss : 0.9711931562423706 TRAIN  loss dict:  {'classification_loss': 0.9711931562423706}
2025-01-18 23:55:18,221 [INFO] Step[350/2713]: training loss : 0.9744685435295105 TRAIN  loss dict:  {'classification_loss': 0.9744685435295105}
2025-01-18 23:55:34,097 [INFO] Step[400/2713]: training loss : 0.9675295972824096 TRAIN  loss dict:  {'classification_loss': 0.9675295972824096}
2025-01-18 23:55:49,943 [INFO] Step[450/2713]: training loss : 0.9703284275531768 TRAIN  loss dict:  {'classification_loss': 0.9703284275531768}
2025-01-18 23:56:05,798 [INFO] Step[500/2713]: training loss : 0.972350195646286 TRAIN  loss dict:  {'classification_loss': 0.972350195646286}
2025-01-18 23:56:21,685 [INFO] Step[550/2713]: training loss : 0.970661290884018 TRAIN  loss dict:  {'classification_loss': 0.970661290884018}
2025-01-18 23:56:37,563 [INFO] Step[600/2713]: training loss : 0.972075412273407 TRAIN  loss dict:  {'classification_loss': 0.972075412273407}
2025-01-18 23:56:53,470 [INFO] Step[650/2713]: training loss : 0.9839140427112579 TRAIN  loss dict:  {'classification_loss': 0.9839140427112579}
2025-01-18 23:57:09,460 [INFO] Step[700/2713]: training loss : 0.9830369830131531 TRAIN  loss dict:  {'classification_loss': 0.9830369830131531}
2025-01-18 23:57:25,373 [INFO] Step[750/2713]: training loss : 0.977815968990326 TRAIN  loss dict:  {'classification_loss': 0.977815968990326}
2025-01-18 23:57:41,267 [INFO] Step[800/2713]: training loss : 0.9651816701889038 TRAIN  loss dict:  {'classification_loss': 0.9651816701889038}
2025-01-18 23:57:57,236 [INFO] Step[850/2713]: training loss : 0.9871672523021698 TRAIN  loss dict:  {'classification_loss': 0.9871672523021698}
2025-01-18 23:58:13,214 [INFO] Step[900/2713]: training loss : 0.9727540516853332 TRAIN  loss dict:  {'classification_loss': 0.9727540516853332}
2025-01-18 23:58:29,219 [INFO] Step[950/2713]: training loss : 0.9879294574260712 TRAIN  loss dict:  {'classification_loss': 0.9879294574260712}
2025-01-18 23:58:45,159 [INFO] Step[1000/2713]: training loss : 0.968330112695694 TRAIN  loss dict:  {'classification_loss': 0.968330112695694}
2025-01-18 23:59:01,174 [INFO] Step[1050/2713]: training loss : 0.9702789676189423 TRAIN  loss dict:  {'classification_loss': 0.9702789676189423}
2025-01-18 23:59:17,151 [INFO] Step[1100/2713]: training loss : 0.9731424307823181 TRAIN  loss dict:  {'classification_loss': 0.9731424307823181}
2025-01-18 23:59:33,120 [INFO] Step[1150/2713]: training loss : 0.9768888759613037 TRAIN  loss dict:  {'classification_loss': 0.9768888759613037}
2025-01-18 23:59:49,042 [INFO] Step[1200/2713]: training loss : 0.9718451046943665 TRAIN  loss dict:  {'classification_loss': 0.9718451046943665}
2025-01-19 00:00:05,023 [INFO] Step[1250/2713]: training loss : 0.967041974067688 TRAIN  loss dict:  {'classification_loss': 0.967041974067688}
2025-01-19 00:00:21,015 [INFO] Step[1300/2713]: training loss : 0.9985894322395324 TRAIN  loss dict:  {'classification_loss': 0.9985894322395324}
2025-01-19 00:00:37,010 [INFO] Step[1350/2713]: training loss : 0.9894536364078522 TRAIN  loss dict:  {'classification_loss': 0.9894536364078522}
2025-01-19 00:00:53,026 [INFO] Step[1400/2713]: training loss : 0.9812820148468018 TRAIN  loss dict:  {'classification_loss': 0.9812820148468018}
2025-01-19 00:01:09,013 [INFO] Step[1450/2713]: training loss : 0.9684871482849121 TRAIN  loss dict:  {'classification_loss': 0.9684871482849121}
2025-01-19 00:01:24,994 [INFO] Step[1500/2713]: training loss : 0.9718867886066437 TRAIN  loss dict:  {'classification_loss': 0.9718867886066437}
2025-01-19 00:01:41,008 [INFO] Step[1550/2713]: training loss : 0.9875958824157715 TRAIN  loss dict:  {'classification_loss': 0.9875958824157715}
2025-01-19 00:01:57,010 [INFO] Step[1600/2713]: training loss : 0.9813998770713807 TRAIN  loss dict:  {'classification_loss': 0.9813998770713807}
2025-01-19 00:02:13,016 [INFO] Step[1650/2713]: training loss : 0.9784946703910827 TRAIN  loss dict:  {'classification_loss': 0.9784946703910827}
2025-01-19 00:02:29,008 [INFO] Step[1700/2713]: training loss : 0.9740956330299377 TRAIN  loss dict:  {'classification_loss': 0.9740956330299377}
2025-01-19 00:02:45,023 [INFO] Step[1750/2713]: training loss : 0.9676384544372558 TRAIN  loss dict:  {'classification_loss': 0.9676384544372558}
2025-01-19 00:03:00,990 [INFO] Step[1800/2713]: training loss : 0.9795266950130462 TRAIN  loss dict:  {'classification_loss': 0.9795266950130462}
2025-01-19 00:03:17,004 [INFO] Step[1850/2713]: training loss : 0.9754682981967926 TRAIN  loss dict:  {'classification_loss': 0.9754682981967926}
2025-01-19 00:03:33,064 [INFO] Step[1900/2713]: training loss : 0.9714967894554138 TRAIN  loss dict:  {'classification_loss': 0.9714967894554138}
2025-01-19 00:03:49,124 [INFO] Step[1950/2713]: training loss : 0.9742747879028321 TRAIN  loss dict:  {'classification_loss': 0.9742747879028321}
2025-01-19 00:04:05,085 [INFO] Step[2000/2713]: training loss : 0.9855123710632324 TRAIN  loss dict:  {'classification_loss': 0.9855123710632324}
2025-01-19 00:04:21,163 [INFO] Step[2050/2713]: training loss : 0.9860011065006256 TRAIN  loss dict:  {'classification_loss': 0.9860011065006256}
2025-01-19 00:04:37,140 [INFO] Step[2100/2713]: training loss : 0.9844291841983795 TRAIN  loss dict:  {'classification_loss': 0.9844291841983795}
2025-01-19 00:04:53,129 [INFO] Step[2150/2713]: training loss : 0.9752484333515167 TRAIN  loss dict:  {'classification_loss': 0.9752484333515167}
2025-01-19 00:05:09,128 [INFO] Step[2200/2713]: training loss : 0.9740282809734344 TRAIN  loss dict:  {'classification_loss': 0.9740282809734344}
2025-01-19 00:05:25,109 [INFO] Step[2250/2713]: training loss : 0.9812965774536133 TRAIN  loss dict:  {'classification_loss': 0.9812965774536133}
2025-01-19 00:05:41,037 [INFO] Step[2300/2713]: training loss : 0.987078593969345 TRAIN  loss dict:  {'classification_loss': 0.987078593969345}
2025-01-19 00:05:57,059 [INFO] Step[2350/2713]: training loss : 0.96777219414711 TRAIN  loss dict:  {'classification_loss': 0.96777219414711}
2025-01-19 00:06:13,029 [INFO] Step[2400/2713]: training loss : 0.9862967991828918 TRAIN  loss dict:  {'classification_loss': 0.9862967991828918}
2025-01-19 00:06:29,006 [INFO] Step[2450/2713]: training loss : 0.9822690939903259 TRAIN  loss dict:  {'classification_loss': 0.9822690939903259}
2025-01-19 00:06:44,950 [INFO] Step[2500/2713]: training loss : 0.9859537220001221 TRAIN  loss dict:  {'classification_loss': 0.9859537220001221}
2025-01-19 00:07:00,884 [INFO] Step[2550/2713]: training loss : 0.9701101100444793 TRAIN  loss dict:  {'classification_loss': 0.9701101100444793}
2025-01-19 00:07:16,858 [INFO] Step[2600/2713]: training loss : 0.9786118817329407 TRAIN  loss dict:  {'classification_loss': 0.9786118817329407}
2025-01-19 00:07:32,879 [INFO] Step[2650/2713]: training loss : 0.9869966542720795 TRAIN  loss dict:  {'classification_loss': 0.9869966542720795}
2025-01-19 00:07:48,848 [INFO] Step[2700/2713]: training loss : 0.9730406260490417 TRAIN  loss dict:  {'classification_loss': 0.9730406260490417}
2025-01-19 00:09:11,828 [INFO] Label accuracies statistics:
2025-01-19 00:09:11,829 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.5, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 1.0, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 0.75, 145: 0.75, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.5, 206: 0.75, 207: 1.0, 208: 0.5, 209: 0.75, 210: 0.5, 211: 0.0, 212: 0.75, 213: 0.25, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.5, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.5, 289: 1.0, 290: 1.0, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.5, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.25, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.5, 377: 0.5, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 00:09:11,831 [INFO] [37] TRAIN  loss: 0.9771017265899833 acc: 0.9969283695785723
2025-01-19 00:09:11,831 [INFO] [37] TRAIN  loss dict: {'classification_loss': 0.9771017265899833}
2025-01-19 00:09:11,831 [INFO] [37] VALIDATION loss: 1.8085423684433888 VALIDATION acc: 0.7893416927899687
2025-01-19 00:09:11,831 [INFO] [37] VALIDATION loss dict: {'classification_loss': 1.8085423684433888}
2025-01-19 00:09:11,831 [INFO] 
2025-01-19 00:09:33,396 [INFO] Step[50/2713]: training loss : 0.9609834444522858 TRAIN  loss dict:  {'classification_loss': 0.9609834444522858}
2025-01-19 00:09:49,524 [INFO] Step[100/2713]: training loss : 0.978798758983612 TRAIN  loss dict:  {'classification_loss': 0.978798758983612}
2025-01-19 00:10:05,638 [INFO] Step[150/2713]: training loss : 0.984643474817276 TRAIN  loss dict:  {'classification_loss': 0.984643474817276}
2025-01-19 00:10:21,755 [INFO] Step[200/2713]: training loss : 0.9697871994972229 TRAIN  loss dict:  {'classification_loss': 0.9697871994972229}
2025-01-19 00:10:37,859 [INFO] Step[250/2713]: training loss : 0.9732650983333587 TRAIN  loss dict:  {'classification_loss': 0.9732650983333587}
2025-01-19 00:10:53,927 [INFO] Step[300/2713]: training loss : 0.9657564437389374 TRAIN  loss dict:  {'classification_loss': 0.9657564437389374}
2025-01-19 00:11:10,036 [INFO] Step[350/2713]: training loss : 0.9618467354774475 TRAIN  loss dict:  {'classification_loss': 0.9618467354774475}
2025-01-19 00:11:26,116 [INFO] Step[400/2713]: training loss : 0.9791920912265778 TRAIN  loss dict:  {'classification_loss': 0.9791920912265778}
2025-01-19 00:11:42,235 [INFO] Step[450/2713]: training loss : 0.9665381026268005 TRAIN  loss dict:  {'classification_loss': 0.9665381026268005}
2025-01-19 00:11:58,309 [INFO] Step[500/2713]: training loss : 0.995999948978424 TRAIN  loss dict:  {'classification_loss': 0.995999948978424}
2025-01-19 00:12:14,402 [INFO] Step[550/2713]: training loss : 0.9686131930351257 TRAIN  loss dict:  {'classification_loss': 0.9686131930351257}
2025-01-19 00:12:30,509 [INFO] Step[600/2713]: training loss : 0.9720590353012085 TRAIN  loss dict:  {'classification_loss': 0.9720590353012085}
2025-01-19 00:12:46,626 [INFO] Step[650/2713]: training loss : 0.9678999197483062 TRAIN  loss dict:  {'classification_loss': 0.9678999197483062}
2025-01-19 00:13:02,741 [INFO] Step[700/2713]: training loss : 0.9924062073230744 TRAIN  loss dict:  {'classification_loss': 0.9924062073230744}
2025-01-19 00:13:18,859 [INFO] Step[750/2713]: training loss : 0.9654644644260406 TRAIN  loss dict:  {'classification_loss': 0.9654644644260406}
2025-01-19 00:13:35,002 [INFO] Step[800/2713]: training loss : 0.9971444404125214 TRAIN  loss dict:  {'classification_loss': 0.9971444404125214}
2025-01-19 00:13:51,121 [INFO] Step[850/2713]: training loss : 0.9909322702884674 TRAIN  loss dict:  {'classification_loss': 0.9909322702884674}
2025-01-19 00:14:07,200 [INFO] Step[900/2713]: training loss : 0.971407995223999 TRAIN  loss dict:  {'classification_loss': 0.971407995223999}
2025-01-19 00:14:23,315 [INFO] Step[950/2713]: training loss : 0.9625669085979461 TRAIN  loss dict:  {'classification_loss': 0.9625669085979461}
2025-01-19 00:14:39,395 [INFO] Step[1000/2713]: training loss : 0.9748131573200226 TRAIN  loss dict:  {'classification_loss': 0.9748131573200226}
2025-01-19 00:14:55,453 [INFO] Step[1050/2713]: training loss : 0.9635842716693879 TRAIN  loss dict:  {'classification_loss': 0.9635842716693879}
2025-01-19 00:15:11,538 [INFO] Step[1100/2713]: training loss : 0.97571990609169 TRAIN  loss dict:  {'classification_loss': 0.97571990609169}
2025-01-19 00:15:27,650 [INFO] Step[1150/2713]: training loss : 0.9768057870864868 TRAIN  loss dict:  {'classification_loss': 0.9768057870864868}
2025-01-19 00:15:43,750 [INFO] Step[1200/2713]: training loss : 0.9720232355594635 TRAIN  loss dict:  {'classification_loss': 0.9720232355594635}
2025-01-19 00:15:59,878 [INFO] Step[1250/2713]: training loss : 0.967919020652771 TRAIN  loss dict:  {'classification_loss': 0.967919020652771}
2025-01-19 00:16:15,950 [INFO] Step[1300/2713]: training loss : 0.9848936665058136 TRAIN  loss dict:  {'classification_loss': 0.9848936665058136}
2025-01-19 00:16:31,965 [INFO] Step[1350/2713]: training loss : 0.9819411957263946 TRAIN  loss dict:  {'classification_loss': 0.9819411957263946}
2025-01-19 00:16:48,013 [INFO] Step[1400/2713]: training loss : 0.9724838292598724 TRAIN  loss dict:  {'classification_loss': 0.9724838292598724}
2025-01-19 00:17:04,109 [INFO] Step[1450/2713]: training loss : 0.9853455293178558 TRAIN  loss dict:  {'classification_loss': 0.9853455293178558}
2025-01-19 00:17:19,742 [INFO] Step[1500/2713]: training loss : 0.9892419266700745 TRAIN  loss dict:  {'classification_loss': 0.9892419266700745}
2025-01-19 00:17:35,302 [INFO] Step[1550/2713]: training loss : 0.9787777090072631 TRAIN  loss dict:  {'classification_loss': 0.9787777090072631}
2025-01-19 00:17:50,446 [INFO] Step[1600/2713]: training loss : 0.9668632698059082 TRAIN  loss dict:  {'classification_loss': 0.9668632698059082}
2025-01-19 00:18:05,581 [INFO] Step[1650/2713]: training loss : 0.9671866047382355 TRAIN  loss dict:  {'classification_loss': 0.9671866047382355}
2025-01-19 00:18:20,767 [INFO] Step[1700/2713]: training loss : 0.9850489485263825 TRAIN  loss dict:  {'classification_loss': 0.9850489485263825}
2025-01-19 00:18:35,861 [INFO] Step[1750/2713]: training loss : 0.9785448014736176 TRAIN  loss dict:  {'classification_loss': 0.9785448014736176}
2025-01-19 00:18:50,936 [INFO] Step[1800/2713]: training loss : 0.9923354113101959 TRAIN  loss dict:  {'classification_loss': 0.9923354113101959}
2025-01-19 00:19:05,916 [INFO] Step[1850/2713]: training loss : 0.970562105178833 TRAIN  loss dict:  {'classification_loss': 0.970562105178833}
2025-01-19 00:19:20,974 [INFO] Step[1900/2713]: training loss : 0.99611292719841 TRAIN  loss dict:  {'classification_loss': 0.99611292719841}
2025-01-19 00:19:36,001 [INFO] Step[1950/2713]: training loss : 0.9725136530399322 TRAIN  loss dict:  {'classification_loss': 0.9725136530399322}
2025-01-19 00:19:51,048 [INFO] Step[2000/2713]: training loss : 0.9700516343116761 TRAIN  loss dict:  {'classification_loss': 0.9700516343116761}
2025-01-19 00:20:06,156 [INFO] Step[2050/2713]: training loss : 0.9677680337429047 TRAIN  loss dict:  {'classification_loss': 0.9677680337429047}
2025-01-19 00:20:21,256 [INFO] Step[2100/2713]: training loss : 0.9668369388580322 TRAIN  loss dict:  {'classification_loss': 0.9668369388580322}
2025-01-19 00:20:36,284 [INFO] Step[2150/2713]: training loss : 0.974551876783371 TRAIN  loss dict:  {'classification_loss': 0.974551876783371}
2025-01-19 00:20:51,220 [INFO] Step[2200/2713]: training loss : 0.9971482002735138 TRAIN  loss dict:  {'classification_loss': 0.9971482002735138}
2025-01-19 00:21:06,181 [INFO] Step[2250/2713]: training loss : 0.980851857662201 TRAIN  loss dict:  {'classification_loss': 0.980851857662201}
2025-01-19 00:21:21,172 [INFO] Step[2300/2713]: training loss : 0.9657913637161255 TRAIN  loss dict:  {'classification_loss': 0.9657913637161255}
2025-01-19 00:21:36,154 [INFO] Step[2350/2713]: training loss : 0.9704507613182067 TRAIN  loss dict:  {'classification_loss': 0.9704507613182067}
2025-01-19 00:21:51,114 [INFO] Step[2400/2713]: training loss : 0.9918086731433868 TRAIN  loss dict:  {'classification_loss': 0.9918086731433868}
2025-01-19 00:22:06,151 [INFO] Step[2450/2713]: training loss : 0.9685863065719604 TRAIN  loss dict:  {'classification_loss': 0.9685863065719604}
2025-01-19 00:22:21,123 [INFO] Step[2500/2713]: training loss : 0.9691655302047729 TRAIN  loss dict:  {'classification_loss': 0.9691655302047729}
2025-01-19 00:22:36,091 [INFO] Step[2550/2713]: training loss : 0.9824759125709533 TRAIN  loss dict:  {'classification_loss': 0.9824759125709533}
2025-01-19 00:22:51,120 [INFO] Step[2600/2713]: training loss : 0.9806774699687958 TRAIN  loss dict:  {'classification_loss': 0.9806774699687958}
2025-01-19 00:23:06,113 [INFO] Step[2650/2713]: training loss : 0.9865366053581238 TRAIN  loss dict:  {'classification_loss': 0.9865366053581238}
2025-01-19 00:23:21,144 [INFO] Step[2700/2713]: training loss : 0.9834490883350372 TRAIN  loss dict:  {'classification_loss': 0.9834490883350372}
2025-01-19 00:24:41,241 [INFO] Label accuracies statistics:
2025-01-19 00:24:41,241 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.5, 49: 0.5, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.5, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 1.0, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.5, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.5, 206: 0.75, 207: 0.75, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.5, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.75, 342: 0.75, 343: 1.0, 344: 1.0, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 00:24:41,243 [INFO] [38] TRAIN  loss: 0.9766736642882871 acc: 0.9969283695785723
2025-01-19 00:24:41,243 [INFO] [38] TRAIN  loss dict: {'classification_loss': 0.9766736642882871}
2025-01-19 00:24:41,243 [INFO] [38] VALIDATION loss: 1.789488258666562 VALIDATION acc: 0.7880877742946708
2025-01-19 00:24:41,243 [INFO] [38] VALIDATION loss dict: {'classification_loss': 1.789488258666562}
2025-01-19 00:24:41,243 [INFO] 
2025-01-19 00:25:01,338 [INFO] Step[50/2713]: training loss : 0.9706244170665741 TRAIN  loss dict:  {'classification_loss': 0.9706244170665741}
2025-01-19 00:25:16,345 [INFO] Step[100/2713]: training loss : 0.976921398639679 TRAIN  loss dict:  {'classification_loss': 0.976921398639679}
2025-01-19 00:25:31,379 [INFO] Step[150/2713]: training loss : 1.012481392621994 TRAIN  loss dict:  {'classification_loss': 1.012481392621994}
2025-01-19 00:25:46,343 [INFO] Step[200/2713]: training loss : 0.967411036491394 TRAIN  loss dict:  {'classification_loss': 0.967411036491394}
2025-01-19 00:26:01,230 [INFO] Step[250/2713]: training loss : 0.9639754509925842 TRAIN  loss dict:  {'classification_loss': 0.9639754509925842}
2025-01-19 00:26:16,141 [INFO] Step[300/2713]: training loss : 0.9792278575897216 TRAIN  loss dict:  {'classification_loss': 0.9792278575897216}
2025-01-19 00:26:31,066 [INFO] Step[350/2713]: training loss : 0.995542837381363 TRAIN  loss dict:  {'classification_loss': 0.995542837381363}
2025-01-19 00:26:45,993 [INFO] Step[400/2713]: training loss : 0.9689829933643341 TRAIN  loss dict:  {'classification_loss': 0.9689829933643341}
2025-01-19 00:27:00,942 [INFO] Step[450/2713]: training loss : 0.9851601910591126 TRAIN  loss dict:  {'classification_loss': 0.9851601910591126}
2025-01-19 00:27:15,841 [INFO] Step[500/2713]: training loss : 0.9803520250320434 TRAIN  loss dict:  {'classification_loss': 0.9803520250320434}
2025-01-19 00:27:30,788 [INFO] Step[550/2713]: training loss : 0.9701183354854583 TRAIN  loss dict:  {'classification_loss': 0.9701183354854583}
2025-01-19 00:27:45,686 [INFO] Step[600/2713]: training loss : 0.9783305239677429 TRAIN  loss dict:  {'classification_loss': 0.9783305239677429}
2025-01-19 00:28:00,565 [INFO] Step[650/2713]: training loss : 0.9672190046310425 TRAIN  loss dict:  {'classification_loss': 0.9672190046310425}
2025-01-19 00:28:15,520 [INFO] Step[700/2713]: training loss : 0.9662860178947449 TRAIN  loss dict:  {'classification_loss': 0.9662860178947449}
2025-01-19 00:28:30,438 [INFO] Step[750/2713]: training loss : 0.984953488111496 TRAIN  loss dict:  {'classification_loss': 0.984953488111496}
2025-01-19 00:28:45,422 [INFO] Step[800/2713]: training loss : 0.9670154404640198 TRAIN  loss dict:  {'classification_loss': 0.9670154404640198}
2025-01-19 00:29:00,352 [INFO] Step[850/2713]: training loss : 0.9734673619270324 TRAIN  loss dict:  {'classification_loss': 0.9734673619270324}
2025-01-19 00:29:15,219 [INFO] Step[900/2713]: training loss : 0.9645688390731811 TRAIN  loss dict:  {'classification_loss': 0.9645688390731811}
2025-01-19 00:29:30,137 [INFO] Step[950/2713]: training loss : 0.9735979735851288 TRAIN  loss dict:  {'classification_loss': 0.9735979735851288}
2025-01-19 00:29:45,205 [INFO] Step[1000/2713]: training loss : 0.9883355820178985 TRAIN  loss dict:  {'classification_loss': 0.9883355820178985}
2025-01-19 00:30:00,176 [INFO] Step[1050/2713]: training loss : 1.0117845153808593 TRAIN  loss dict:  {'classification_loss': 1.0117845153808593}
2025-01-19 00:30:15,264 [INFO] Step[1100/2713]: training loss : 0.9818348395824432 TRAIN  loss dict:  {'classification_loss': 0.9818348395824432}
2025-01-19 00:30:30,185 [INFO] Step[1150/2713]: training loss : 0.9760462856292724 TRAIN  loss dict:  {'classification_loss': 0.9760462856292724}
2025-01-19 00:30:45,208 [INFO] Step[1200/2713]: training loss : 0.9820555710792541 TRAIN  loss dict:  {'classification_loss': 0.9820555710792541}
2025-01-19 00:31:00,291 [INFO] Step[1250/2713]: training loss : 0.9820207047462464 TRAIN  loss dict:  {'classification_loss': 0.9820207047462464}
2025-01-19 00:31:15,376 [INFO] Step[1300/2713]: training loss : 0.9649899685382843 TRAIN  loss dict:  {'classification_loss': 0.9649899685382843}
2025-01-19 00:31:30,490 [INFO] Step[1350/2713]: training loss : 0.9843526482582092 TRAIN  loss dict:  {'classification_loss': 0.9843526482582092}
2025-01-19 00:31:45,597 [INFO] Step[1400/2713]: training loss : 0.9728938186168671 TRAIN  loss dict:  {'classification_loss': 0.9728938186168671}
2025-01-19 00:32:00,719 [INFO] Step[1450/2713]: training loss : 0.9710821676254272 TRAIN  loss dict:  {'classification_loss': 0.9710821676254272}
2025-01-19 00:32:15,762 [INFO] Step[1500/2713]: training loss : 0.9665270733833313 TRAIN  loss dict:  {'classification_loss': 0.9665270733833313}
2025-01-19 00:32:30,850 [INFO] Step[1550/2713]: training loss : 0.9596957635879516 TRAIN  loss dict:  {'classification_loss': 0.9596957635879516}
2025-01-19 00:32:45,923 [INFO] Step[1600/2713]: training loss : 0.9657661437988281 TRAIN  loss dict:  {'classification_loss': 0.9657661437988281}
2025-01-19 00:33:00,935 [INFO] Step[1650/2713]: training loss : 0.9684965586662293 TRAIN  loss dict:  {'classification_loss': 0.9684965586662293}
2025-01-19 00:33:15,958 [INFO] Step[1700/2713]: training loss : 0.9747194421291351 TRAIN  loss dict:  {'classification_loss': 0.9747194421291351}
2025-01-19 00:33:31,059 [INFO] Step[1750/2713]: training loss : 0.9693967652320862 TRAIN  loss dict:  {'classification_loss': 0.9693967652320862}
2025-01-19 00:33:46,112 [INFO] Step[1800/2713]: training loss : 0.9821962821483612 TRAIN  loss dict:  {'classification_loss': 0.9821962821483612}
2025-01-19 00:34:01,190 [INFO] Step[1850/2713]: training loss : 0.9858465707302093 TRAIN  loss dict:  {'classification_loss': 0.9858465707302093}
2025-01-19 00:34:16,250 [INFO] Step[1900/2713]: training loss : 0.9719159007072449 TRAIN  loss dict:  {'classification_loss': 0.9719159007072449}
2025-01-19 00:34:31,322 [INFO] Step[1950/2713]: training loss : 0.9630479001998902 TRAIN  loss dict:  {'classification_loss': 0.9630479001998902}
2025-01-19 00:34:46,421 [INFO] Step[2000/2713]: training loss : 0.9865734553337098 TRAIN  loss dict:  {'classification_loss': 0.9865734553337098}
2025-01-19 00:35:01,494 [INFO] Step[2050/2713]: training loss : 0.970677044391632 TRAIN  loss dict:  {'classification_loss': 0.970677044391632}
2025-01-19 00:35:16,603 [INFO] Step[2100/2713]: training loss : 0.9816539084911347 TRAIN  loss dict:  {'classification_loss': 0.9816539084911347}
2025-01-19 00:35:31,707 [INFO] Step[2150/2713]: training loss : 0.9788982951641083 TRAIN  loss dict:  {'classification_loss': 0.9788982951641083}
2025-01-19 00:35:46,783 [INFO] Step[2200/2713]: training loss : 0.9801569032669067 TRAIN  loss dict:  {'classification_loss': 0.9801569032669067}
2025-01-19 00:36:01,870 [INFO] Step[2250/2713]: training loss : 0.9757945597171783 TRAIN  loss dict:  {'classification_loss': 0.9757945597171783}
2025-01-19 00:36:16,912 [INFO] Step[2300/2713]: training loss : 0.9660621464252472 TRAIN  loss dict:  {'classification_loss': 0.9660621464252472}
2025-01-19 00:36:31,934 [INFO] Step[2350/2713]: training loss : 0.9961944019794464 TRAIN  loss dict:  {'classification_loss': 0.9961944019794464}
2025-01-19 00:36:47,008 [INFO] Step[2400/2713]: training loss : 0.978062105178833 TRAIN  loss dict:  {'classification_loss': 0.978062105178833}
2025-01-19 00:37:02,122 [INFO] Step[2450/2713]: training loss : 0.9821307229995727 TRAIN  loss dict:  {'classification_loss': 0.9821307229995727}
2025-01-19 00:37:17,207 [INFO] Step[2500/2713]: training loss : 0.9866827237606048 TRAIN  loss dict:  {'classification_loss': 0.9866827237606048}
2025-01-19 00:37:32,273 [INFO] Step[2550/2713]: training loss : 0.968713995218277 TRAIN  loss dict:  {'classification_loss': 0.968713995218277}
2025-01-19 00:37:47,331 [INFO] Step[2600/2713]: training loss : 0.9861179089546204 TRAIN  loss dict:  {'classification_loss': 0.9861179089546204}
2025-01-19 00:38:02,414 [INFO] Step[2650/2713]: training loss : 0.9938686954975128 TRAIN  loss dict:  {'classification_loss': 0.9938686954975128}
2025-01-19 00:38:17,482 [INFO] Step[2700/2713]: training loss : 0.9851174628734589 TRAIN  loss dict:  {'classification_loss': 0.9851174628734589}
2025-01-19 00:39:37,836 [INFO] Label accuracies statistics:
2025-01-19 00:39:37,837 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.5, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.75, 142: 0.5, 143: 0.5, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 0.75, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.25, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 1.0, 237: 0.5, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 0.5, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.5, 287: 1.0, 288: 0.5, 289: 0.5, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.5, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 00:39:37,838 [INFO] [39] TRAIN  loss: 0.9775160595450593 acc: 0.9968055043617152
2025-01-19 00:39:37,838 [INFO] [39] TRAIN  loss dict: {'classification_loss': 0.9775160595450593}
2025-01-19 00:39:37,838 [INFO] [39] VALIDATION loss: 1.8003696563996767 VALIDATION acc: 0.7849529780564264
2025-01-19 00:39:37,838 [INFO] [39] VALIDATION loss dict: {'classification_loss': 1.8003696563996767}
2025-01-19 00:39:37,839 [INFO] 
2025-01-19 00:39:58,103 [INFO] Step[50/2713]: training loss : 0.9756279134750366 TRAIN  loss dict:  {'classification_loss': 0.9756279134750366}
2025-01-19 00:40:13,177 [INFO] Step[100/2713]: training loss : 0.9851810383796692 TRAIN  loss dict:  {'classification_loss': 0.9851810383796692}
2025-01-19 00:40:28,253 [INFO] Step[150/2713]: training loss : 0.9675837659835815 TRAIN  loss dict:  {'classification_loss': 0.9675837659835815}
2025-01-19 00:40:43,333 [INFO] Step[200/2713]: training loss : 0.9717357516288757 TRAIN  loss dict:  {'classification_loss': 0.9717357516288757}
2025-01-19 00:40:58,428 [INFO] Step[250/2713]: training loss : 0.9619633209705353 TRAIN  loss dict:  {'classification_loss': 0.9619633209705353}
2025-01-19 00:41:13,513 [INFO] Step[300/2713]: training loss : 0.972076176404953 TRAIN  loss dict:  {'classification_loss': 0.972076176404953}
2025-01-19 00:41:28,583 [INFO] Step[350/2713]: training loss : 0.97510498046875 TRAIN  loss dict:  {'classification_loss': 0.97510498046875}
2025-01-19 00:41:43,660 [INFO] Step[400/2713]: training loss : 0.9860743796825409 TRAIN  loss dict:  {'classification_loss': 0.9860743796825409}
2025-01-19 00:41:58,731 [INFO] Step[450/2713]: training loss : 0.9921188914775848 TRAIN  loss dict:  {'classification_loss': 0.9921188914775848}
2025-01-19 00:42:13,814 [INFO] Step[500/2713]: training loss : 0.9635565686225891 TRAIN  loss dict:  {'classification_loss': 0.9635565686225891}
2025-01-19 00:42:28,853 [INFO] Step[550/2713]: training loss : 0.9700524950027466 TRAIN  loss dict:  {'classification_loss': 0.9700524950027466}
2025-01-19 00:42:43,940 [INFO] Step[600/2713]: training loss : 0.9717992389202118 TRAIN  loss dict:  {'classification_loss': 0.9717992389202118}
2025-01-19 00:42:59,048 [INFO] Step[650/2713]: training loss : 0.9577842235565186 TRAIN  loss dict:  {'classification_loss': 0.9577842235565186}
2025-01-19 00:43:14,095 [INFO] Step[700/2713]: training loss : 0.9687925469875336 TRAIN  loss dict:  {'classification_loss': 0.9687925469875336}
2025-01-19 00:43:29,188 [INFO] Step[750/2713]: training loss : 0.9632443451881408 TRAIN  loss dict:  {'classification_loss': 0.9632443451881408}
2025-01-19 00:43:44,262 [INFO] Step[800/2713]: training loss : 0.9733437395095825 TRAIN  loss dict:  {'classification_loss': 0.9733437395095825}
2025-01-19 00:43:59,342 [INFO] Step[850/2713]: training loss : 0.9700982904434204 TRAIN  loss dict:  {'classification_loss': 0.9700982904434204}
2025-01-19 00:44:14,400 [INFO] Step[900/2713]: training loss : 0.9747870671749115 TRAIN  loss dict:  {'classification_loss': 0.9747870671749115}
2025-01-19 00:44:29,471 [INFO] Step[950/2713]: training loss : 0.9678239440917968 TRAIN  loss dict:  {'classification_loss': 0.9678239440917968}
2025-01-19 00:44:44,562 [INFO] Step[1000/2713]: training loss : 0.9658995175361633 TRAIN  loss dict:  {'classification_loss': 0.9658995175361633}
2025-01-19 00:44:59,636 [INFO] Step[1050/2713]: training loss : 0.9637004888057709 TRAIN  loss dict:  {'classification_loss': 0.9637004888057709}
2025-01-19 00:45:14,672 [INFO] Step[1100/2713]: training loss : 0.9728997600078583 TRAIN  loss dict:  {'classification_loss': 0.9728997600078583}
2025-01-19 00:45:29,769 [INFO] Step[1150/2713]: training loss : 0.9950871205329895 TRAIN  loss dict:  {'classification_loss': 0.9950871205329895}
2025-01-19 00:45:44,850 [INFO] Step[1200/2713]: training loss : 0.9712171483039856 TRAIN  loss dict:  {'classification_loss': 0.9712171483039856}
2025-01-19 00:45:59,966 [INFO] Step[1250/2713]: training loss : 0.9691922831535339 TRAIN  loss dict:  {'classification_loss': 0.9691922831535339}
2025-01-19 00:46:15,020 [INFO] Step[1300/2713]: training loss : 0.9715969824790954 TRAIN  loss dict:  {'classification_loss': 0.9715969824790954}
2025-01-19 00:46:30,096 [INFO] Step[1350/2713]: training loss : 0.9824342918395996 TRAIN  loss dict:  {'classification_loss': 0.9824342918395996}
2025-01-19 00:46:45,155 [INFO] Step[1400/2713]: training loss : 0.9709184777736664 TRAIN  loss dict:  {'classification_loss': 0.9709184777736664}
2025-01-19 00:47:00,240 [INFO] Step[1450/2713]: training loss : 0.9628823125362396 TRAIN  loss dict:  {'classification_loss': 0.9628823125362396}
2025-01-19 00:47:15,342 [INFO] Step[1500/2713]: training loss : 0.9671702718734742 TRAIN  loss dict:  {'classification_loss': 0.9671702718734742}
2025-01-19 00:47:30,437 [INFO] Step[1550/2713]: training loss : 0.9609334731101989 TRAIN  loss dict:  {'classification_loss': 0.9609334731101989}
2025-01-19 00:47:45,511 [INFO] Step[1600/2713]: training loss : 0.9801750898361206 TRAIN  loss dict:  {'classification_loss': 0.9801750898361206}
2025-01-19 00:48:00,567 [INFO] Step[1650/2713]: training loss : 0.9701667368412018 TRAIN  loss dict:  {'classification_loss': 0.9701667368412018}
2025-01-19 00:48:15,627 [INFO] Step[1700/2713]: training loss : 0.9882098126411438 TRAIN  loss dict:  {'classification_loss': 0.9882098126411438}
2025-01-19 00:48:30,684 [INFO] Step[1750/2713]: training loss : 0.977263355255127 TRAIN  loss dict:  {'classification_loss': 0.977263355255127}
2025-01-19 00:48:45,739 [INFO] Step[1800/2713]: training loss : 0.9824336087703704 TRAIN  loss dict:  {'classification_loss': 0.9824336087703704}
2025-01-19 00:49:00,807 [INFO] Step[1850/2713]: training loss : 0.9717953848838806 TRAIN  loss dict:  {'classification_loss': 0.9717953848838806}
2025-01-19 00:49:15,834 [INFO] Step[1900/2713]: training loss : 0.9761045408248902 TRAIN  loss dict:  {'classification_loss': 0.9761045408248902}
2025-01-19 00:49:30,929 [INFO] Step[1950/2713]: training loss : 0.9750644600391388 TRAIN  loss dict:  {'classification_loss': 0.9750644600391388}
2025-01-19 00:49:45,946 [INFO] Step[2000/2713]: training loss : 0.9752121150493622 TRAIN  loss dict:  {'classification_loss': 0.9752121150493622}
2025-01-19 00:50:00,973 [INFO] Step[2050/2713]: training loss : 0.975602855682373 TRAIN  loss dict:  {'classification_loss': 0.975602855682373}
2025-01-19 00:50:16,009 [INFO] Step[2100/2713]: training loss : 0.9790238654613495 TRAIN  loss dict:  {'classification_loss': 0.9790238654613495}
2025-01-19 00:50:31,086 [INFO] Step[2150/2713]: training loss : 0.9664000260829926 TRAIN  loss dict:  {'classification_loss': 0.9664000260829926}
2025-01-19 00:50:46,153 [INFO] Step[2200/2713]: training loss : 0.9646313488483429 TRAIN  loss dict:  {'classification_loss': 0.9646313488483429}
2025-01-19 00:51:01,239 [INFO] Step[2250/2713]: training loss : 0.9814788699150085 TRAIN  loss dict:  {'classification_loss': 0.9814788699150085}
2025-01-19 00:51:16,311 [INFO] Step[2300/2713]: training loss : 0.980875986814499 TRAIN  loss dict:  {'classification_loss': 0.980875986814499}
2025-01-19 00:51:31,400 [INFO] Step[2350/2713]: training loss : 0.9766651058197021 TRAIN  loss dict:  {'classification_loss': 0.9766651058197021}
2025-01-19 00:51:46,482 [INFO] Step[2400/2713]: training loss : 0.9694617795944214 TRAIN  loss dict:  {'classification_loss': 0.9694617795944214}
2025-01-19 00:52:01,578 [INFO] Step[2450/2713]: training loss : 1.0075445067882538 TRAIN  loss dict:  {'classification_loss': 1.0075445067882538}
2025-01-19 00:52:16,632 [INFO] Step[2500/2713]: training loss : 0.9631452906131744 TRAIN  loss dict:  {'classification_loss': 0.9631452906131744}
2025-01-19 00:52:31,739 [INFO] Step[2550/2713]: training loss : 0.9741660904884338 TRAIN  loss dict:  {'classification_loss': 0.9741660904884338}
2025-01-19 00:52:46,776 [INFO] Step[2600/2713]: training loss : 0.9798772764205933 TRAIN  loss dict:  {'classification_loss': 0.9798772764205933}
2025-01-19 00:53:01,811 [INFO] Step[2650/2713]: training loss : 0.9783700716495514 TRAIN  loss dict:  {'classification_loss': 0.9783700716495514}
2025-01-19 00:53:16,897 [INFO] Step[2700/2713]: training loss : 0.9757255482673645 TRAIN  loss dict:  {'classification_loss': 0.9757255482673645}
2025-01-19 00:54:37,217 [INFO] Label accuracies statistics:
2025-01-19 00:54:37,217 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.5, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.5, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.5, 206: 0.25, 207: 0.5, 208: 0.25, 209: 1.0, 210: 0.5, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 1.0, 241: 0.75, 242: 0.25, 243: 0.75, 244: 0.5, 245: 0.5, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.25, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.25, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.25, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 00:54:37,219 [INFO] [40] TRAIN  loss: 0.9738788212132199 acc: 0.9972969652291437
2025-01-19 00:54:37,219 [INFO] [40] TRAIN  loss dict: {'classification_loss': 0.9738788212132199}
2025-01-19 00:54:37,219 [INFO] [40] VALIDATION loss: 1.8036253956475652 VALIDATION acc: 0.7774294670846394
2025-01-19 00:54:37,219 [INFO] [40] VALIDATION loss dict: {'classification_loss': 1.8036253956475652}
2025-01-19 00:54:37,219 [INFO] 
2025-01-19 00:54:57,347 [INFO] Step[50/2713]: training loss : 0.9631490409374237 TRAIN  loss dict:  {'classification_loss': 0.9631490409374237}
2025-01-19 00:55:12,395 [INFO] Step[100/2713]: training loss : 0.9680210602283478 TRAIN  loss dict:  {'classification_loss': 0.9680210602283478}
2025-01-19 00:55:27,431 [INFO] Step[150/2713]: training loss : 0.9580490624904633 TRAIN  loss dict:  {'classification_loss': 0.9580490624904633}
2025-01-19 00:55:42,464 [INFO] Step[200/2713]: training loss : 0.9861125910282135 TRAIN  loss dict:  {'classification_loss': 0.9861125910282135}
2025-01-19 00:55:57,513 [INFO] Step[250/2713]: training loss : 0.9682979035377502 TRAIN  loss dict:  {'classification_loss': 0.9682979035377502}
2025-01-19 00:56:12,549 [INFO] Step[300/2713]: training loss : 0.9725584268569947 TRAIN  loss dict:  {'classification_loss': 0.9725584268569947}
2025-01-19 00:56:27,599 [INFO] Step[350/2713]: training loss : 0.9999422788619995 TRAIN  loss dict:  {'classification_loss': 0.9999422788619995}
2025-01-19 00:56:42,656 [INFO] Step[400/2713]: training loss : 0.9739254152774811 TRAIN  loss dict:  {'classification_loss': 0.9739254152774811}
2025-01-19 00:56:57,687 [INFO] Step[450/2713]: training loss : 0.9663625299930573 TRAIN  loss dict:  {'classification_loss': 0.9663625299930573}
2025-01-19 00:57:12,633 [INFO] Step[500/2713]: training loss : 0.9723759734630585 TRAIN  loss dict:  {'classification_loss': 0.9723759734630585}
2025-01-19 00:57:27,621 [INFO] Step[550/2713]: training loss : 0.9723131835460663 TRAIN  loss dict:  {'classification_loss': 0.9723131835460663}
2025-01-19 00:57:42,693 [INFO] Step[600/2713]: training loss : 0.9639862632751465 TRAIN  loss dict:  {'classification_loss': 0.9639862632751465}
2025-01-19 00:57:57,749 [INFO] Step[650/2713]: training loss : 0.9613375091552734 TRAIN  loss dict:  {'classification_loss': 0.9613375091552734}
2025-01-19 00:58:12,746 [INFO] Step[700/2713]: training loss : 0.9617865931987762 TRAIN  loss dict:  {'classification_loss': 0.9617865931987762}
2025-01-19 00:58:27,721 [INFO] Step[750/2713]: training loss : 1.0149672794342042 TRAIN  loss dict:  {'classification_loss': 1.0149672794342042}
2025-01-19 00:58:42,740 [INFO] Step[800/2713]: training loss : 0.9624289274215698 TRAIN  loss dict:  {'classification_loss': 0.9624289274215698}
2025-01-19 00:58:57,688 [INFO] Step[850/2713]: training loss : 0.9726789224147797 TRAIN  loss dict:  {'classification_loss': 0.9726789224147797}
2025-01-19 00:59:12,651 [INFO] Step[900/2713]: training loss : 0.960925304889679 TRAIN  loss dict:  {'classification_loss': 0.960925304889679}
2025-01-19 00:59:27,708 [INFO] Step[950/2713]: training loss : 0.969716717004776 TRAIN  loss dict:  {'classification_loss': 0.969716717004776}
2025-01-19 00:59:42,685 [INFO] Step[1000/2713]: training loss : 0.962519805431366 TRAIN  loss dict:  {'classification_loss': 0.962519805431366}
2025-01-19 00:59:57,725 [INFO] Step[1050/2713]: training loss : 0.9627227175235749 TRAIN  loss dict:  {'classification_loss': 0.9627227175235749}
2025-01-19 01:00:12,727 [INFO] Step[1100/2713]: training loss : 0.9837422251701355 TRAIN  loss dict:  {'classification_loss': 0.9837422251701355}
2025-01-19 01:00:27,770 [INFO] Step[1150/2713]: training loss : 0.9598153126239777 TRAIN  loss dict:  {'classification_loss': 0.9598153126239777}
2025-01-19 01:00:42,790 [INFO] Step[1200/2713]: training loss : 0.9869660246372223 TRAIN  loss dict:  {'classification_loss': 0.9869660246372223}
2025-01-19 01:00:57,759 [INFO] Step[1250/2713]: training loss : 0.9640957152843476 TRAIN  loss dict:  {'classification_loss': 0.9640957152843476}
2025-01-19 01:01:12,800 [INFO] Step[1300/2713]: training loss : 0.9776469719409943 TRAIN  loss dict:  {'classification_loss': 0.9776469719409943}
2025-01-19 01:01:27,816 [INFO] Step[1350/2713]: training loss : 0.9733297622203827 TRAIN  loss dict:  {'classification_loss': 0.9733297622203827}
2025-01-19 01:01:42,779 [INFO] Step[1400/2713]: training loss : 0.9683633482456208 TRAIN  loss dict:  {'classification_loss': 0.9683633482456208}
2025-01-19 01:01:57,852 [INFO] Step[1450/2713]: training loss : 0.9694820010662079 TRAIN  loss dict:  {'classification_loss': 0.9694820010662079}
2025-01-19 01:02:12,893 [INFO] Step[1500/2713]: training loss : 0.9640830421447754 TRAIN  loss dict:  {'classification_loss': 0.9640830421447754}
2025-01-19 01:02:27,896 [INFO] Step[1550/2713]: training loss : 0.9675306415557862 TRAIN  loss dict:  {'classification_loss': 0.9675306415557862}
2025-01-19 01:02:42,881 [INFO] Step[1600/2713]: training loss : 0.9706980633735657 TRAIN  loss dict:  {'classification_loss': 0.9706980633735657}
2025-01-19 01:02:57,933 [INFO] Step[1650/2713]: training loss : 0.9666250741481781 TRAIN  loss dict:  {'classification_loss': 0.9666250741481781}
2025-01-19 01:03:12,923 [INFO] Step[1700/2713]: training loss : 0.9671990621089935 TRAIN  loss dict:  {'classification_loss': 0.9671990621089935}
2025-01-19 01:03:27,927 [INFO] Step[1750/2713]: training loss : 0.9667127859592438 TRAIN  loss dict:  {'classification_loss': 0.9667127859592438}
2025-01-19 01:03:42,918 [INFO] Step[1800/2713]: training loss : 0.9652766358852386 TRAIN  loss dict:  {'classification_loss': 0.9652766358852386}
2025-01-19 01:03:57,912 [INFO] Step[1850/2713]: training loss : 0.9854077231884003 TRAIN  loss dict:  {'classification_loss': 0.9854077231884003}
2025-01-19 01:04:12,893 [INFO] Step[1900/2713]: training loss : 0.9708092963695526 TRAIN  loss dict:  {'classification_loss': 0.9708092963695526}
2025-01-19 01:04:27,840 [INFO] Step[1950/2713]: training loss : 0.9687890422344207 TRAIN  loss dict:  {'classification_loss': 0.9687890422344207}
2025-01-19 01:04:42,793 [INFO] Step[2000/2713]: training loss : 0.9912518322467804 TRAIN  loss dict:  {'classification_loss': 0.9912518322467804}
2025-01-19 01:04:57,765 [INFO] Step[2050/2713]: training loss : 0.9605969715118409 TRAIN  loss dict:  {'classification_loss': 0.9605969715118409}
2025-01-19 01:05:12,757 [INFO] Step[2100/2713]: training loss : 0.962462626695633 TRAIN  loss dict:  {'classification_loss': 0.962462626695633}
2025-01-19 01:05:27,712 [INFO] Step[2150/2713]: training loss : 0.9601144003868103 TRAIN  loss dict:  {'classification_loss': 0.9601144003868103}
2025-01-19 01:05:42,654 [INFO] Step[2200/2713]: training loss : 0.9618750882148742 TRAIN  loss dict:  {'classification_loss': 0.9618750882148742}
2025-01-19 01:05:57,645 [INFO] Step[2250/2713]: training loss : 0.9589406597614288 TRAIN  loss dict:  {'classification_loss': 0.9589406597614288}
2025-01-19 01:06:12,612 [INFO] Step[2300/2713]: training loss : 0.956951676607132 TRAIN  loss dict:  {'classification_loss': 0.956951676607132}
2025-01-19 01:06:27,552 [INFO] Step[2350/2713]: training loss : 0.9597456228733062 TRAIN  loss dict:  {'classification_loss': 0.9597456228733062}
2025-01-19 01:06:42,497 [INFO] Step[2400/2713]: training loss : 0.968048802614212 TRAIN  loss dict:  {'classification_loss': 0.968048802614212}
2025-01-19 01:06:57,462 [INFO] Step[2450/2713]: training loss : 0.9605585634708405 TRAIN  loss dict:  {'classification_loss': 0.9605585634708405}
2025-01-19 01:07:12,483 [INFO] Step[2500/2713]: training loss : 0.9813031780719758 TRAIN  loss dict:  {'classification_loss': 0.9813031780719758}
2025-01-19 01:07:27,434 [INFO] Step[2550/2713]: training loss : 0.9698403334617615 TRAIN  loss dict:  {'classification_loss': 0.9698403334617615}
2025-01-19 01:07:42,377 [INFO] Step[2600/2713]: training loss : 0.9741939091682434 TRAIN  loss dict:  {'classification_loss': 0.9741939091682434}
2025-01-19 01:07:57,411 [INFO] Step[2650/2713]: training loss : 0.9712535536289215 TRAIN  loss dict:  {'classification_loss': 0.9712535536289215}
2025-01-19 01:08:12,380 [INFO] Step[2700/2713]: training loss : 0.9902975153923035 TRAIN  loss dict:  {'classification_loss': 0.9902975153923035}
2025-01-19 01:09:32,538 [INFO] Label accuracies statistics:
2025-01-19 01:09:32,539 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.5, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 1.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.5, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.5, 289: 1.0, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.5, 377: 1.0, 378: 0.25, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 01:09:32,541 [INFO] [41] TRAIN  loss: 0.9705540942082403 acc: 0.997788426096572
2025-01-19 01:09:32,541 [INFO] [41] TRAIN  loss dict: {'classification_loss': 0.9705540942082403}
2025-01-19 01:09:32,541 [INFO] [41] VALIDATION loss: 1.8081330533762623 VALIDATION acc: 0.7855799373040753
2025-01-19 01:09:32,541 [INFO] [41] VALIDATION loss dict: {'classification_loss': 1.8081330533762623}
2025-01-19 01:09:32,541 [INFO] 
2025-01-19 01:09:52,967 [INFO] Step[50/2713]: training loss : 0.9598213255405426 TRAIN  loss dict:  {'classification_loss': 0.9598213255405426}
2025-01-19 01:10:07,918 [INFO] Step[100/2713]: training loss : 0.9588962018489837 TRAIN  loss dict:  {'classification_loss': 0.9588962018489837}
2025-01-19 01:10:22,954 [INFO] Step[150/2713]: training loss : 0.9620918655395507 TRAIN  loss dict:  {'classification_loss': 0.9620918655395507}
2025-01-19 01:10:37,937 [INFO] Step[200/2713]: training loss : 0.9647503507137298 TRAIN  loss dict:  {'classification_loss': 0.9647503507137298}
2025-01-19 01:10:52,912 [INFO] Step[250/2713]: training loss : 0.9642730700969696 TRAIN  loss dict:  {'classification_loss': 0.9642730700969696}
2025-01-19 01:11:07,923 [INFO] Step[300/2713]: training loss : 0.9562634742259979 TRAIN  loss dict:  {'classification_loss': 0.9562634742259979}
2025-01-19 01:11:22,892 [INFO] Step[350/2713]: training loss : 0.971267466545105 TRAIN  loss dict:  {'classification_loss': 0.971267466545105}
2025-01-19 01:11:37,830 [INFO] Step[400/2713]: training loss : 0.9643623352050781 TRAIN  loss dict:  {'classification_loss': 0.9643623352050781}
2025-01-19 01:11:52,777 [INFO] Step[450/2713]: training loss : 0.960869882106781 TRAIN  loss dict:  {'classification_loss': 0.960869882106781}
2025-01-19 01:12:07,751 [INFO] Step[500/2713]: training loss : 0.959047303199768 TRAIN  loss dict:  {'classification_loss': 0.959047303199768}
2025-01-19 01:12:22,700 [INFO] Step[550/2713]: training loss : 0.966755803823471 TRAIN  loss dict:  {'classification_loss': 0.966755803823471}
2025-01-19 01:12:37,719 [INFO] Step[600/2713]: training loss : 0.9638493883609772 TRAIN  loss dict:  {'classification_loss': 0.9638493883609772}
2025-01-19 01:12:52,703 [INFO] Step[650/2713]: training loss : 0.9664843380451202 TRAIN  loss dict:  {'classification_loss': 0.9664843380451202}
2025-01-19 01:13:07,678 [INFO] Step[700/2713]: training loss : 0.9591822183132171 TRAIN  loss dict:  {'classification_loss': 0.9591822183132171}
2025-01-19 01:13:22,604 [INFO] Step[750/2713]: training loss : 0.9650895237922669 TRAIN  loss dict:  {'classification_loss': 0.9650895237922669}
2025-01-19 01:13:37,546 [INFO] Step[800/2713]: training loss : 0.9605523443222046 TRAIN  loss dict:  {'classification_loss': 0.9605523443222046}
2025-01-19 01:13:52,544 [INFO] Step[850/2713]: training loss : 0.9611448812484741 TRAIN  loss dict:  {'classification_loss': 0.9611448812484741}
2025-01-19 01:14:07,493 [INFO] Step[900/2713]: training loss : 0.9578508079051972 TRAIN  loss dict:  {'classification_loss': 0.9578508079051972}
2025-01-19 01:14:22,532 [INFO] Step[950/2713]: training loss : 0.9612852323055268 TRAIN  loss dict:  {'classification_loss': 0.9612852323055268}
2025-01-19 01:14:37,483 [INFO] Step[1000/2713]: training loss : 0.9623578643798828 TRAIN  loss dict:  {'classification_loss': 0.9623578643798828}
2025-01-19 01:14:52,514 [INFO] Step[1050/2713]: training loss : 0.9862721621990204 TRAIN  loss dict:  {'classification_loss': 0.9862721621990204}
2025-01-19 01:15:07,510 [INFO] Step[1100/2713]: training loss : 0.970544136762619 TRAIN  loss dict:  {'classification_loss': 0.970544136762619}
2025-01-19 01:15:22,509 [INFO] Step[1150/2713]: training loss : 0.9627860414981843 TRAIN  loss dict:  {'classification_loss': 0.9627860414981843}
2025-01-19 01:15:37,516 [INFO] Step[1200/2713]: training loss : 0.9624573624134064 TRAIN  loss dict:  {'classification_loss': 0.9624573624134064}
2025-01-19 01:15:52,450 [INFO] Step[1250/2713]: training loss : 0.9771763896942138 TRAIN  loss dict:  {'classification_loss': 0.9771763896942138}
2025-01-19 01:16:07,412 [INFO] Step[1300/2713]: training loss : 0.959072835445404 TRAIN  loss dict:  {'classification_loss': 0.959072835445404}
2025-01-19 01:16:22,439 [INFO] Step[1350/2713]: training loss : 0.9683560061454773 TRAIN  loss dict:  {'classification_loss': 0.9683560061454773}
2025-01-19 01:16:37,409 [INFO] Step[1400/2713]: training loss : 0.9641233742237091 TRAIN  loss dict:  {'classification_loss': 0.9641233742237091}
2025-01-19 01:16:52,385 [INFO] Step[1450/2713]: training loss : 0.9821989679336548 TRAIN  loss dict:  {'classification_loss': 0.9821989679336548}
2025-01-19 01:17:07,384 [INFO] Step[1500/2713]: training loss : 0.9677203786373139 TRAIN  loss dict:  {'classification_loss': 0.9677203786373139}
2025-01-19 01:17:22,391 [INFO] Step[1550/2713]: training loss : 0.9674470102787018 TRAIN  loss dict:  {'classification_loss': 0.9674470102787018}
2025-01-19 01:17:37,398 [INFO] Step[1600/2713]: training loss : 0.9892324268817901 TRAIN  loss dict:  {'classification_loss': 0.9892324268817901}
2025-01-19 01:17:52,360 [INFO] Step[1650/2713]: training loss : 0.968029934167862 TRAIN  loss dict:  {'classification_loss': 0.968029934167862}
2025-01-19 01:18:07,309 [INFO] Step[1700/2713]: training loss : 0.9630803751945496 TRAIN  loss dict:  {'classification_loss': 0.9630803751945496}
2025-01-19 01:18:22,306 [INFO] Step[1750/2713]: training loss : 0.9644206893444062 TRAIN  loss dict:  {'classification_loss': 0.9644206893444062}
2025-01-19 01:18:37,284 [INFO] Step[1800/2713]: training loss : 0.9869719576835633 TRAIN  loss dict:  {'classification_loss': 0.9869719576835633}
2025-01-19 01:18:52,326 [INFO] Step[1850/2713]: training loss : 0.9572734928131104 TRAIN  loss dict:  {'classification_loss': 0.9572734928131104}
2025-01-19 01:19:07,304 [INFO] Step[1900/2713]: training loss : 0.9704809629917145 TRAIN  loss dict:  {'classification_loss': 0.9704809629917145}
2025-01-19 01:19:22,306 [INFO] Step[1950/2713]: training loss : 0.9746094310283661 TRAIN  loss dict:  {'classification_loss': 0.9746094310283661}
2025-01-19 01:19:37,301 [INFO] Step[2000/2713]: training loss : 0.9651142752170563 TRAIN  loss dict:  {'classification_loss': 0.9651142752170563}
2025-01-19 01:19:52,258 [INFO] Step[2050/2713]: training loss : 0.9684995150566101 TRAIN  loss dict:  {'classification_loss': 0.9684995150566101}
2025-01-19 01:20:07,239 [INFO] Step[2100/2713]: training loss : 0.9701455438137054 TRAIN  loss dict:  {'classification_loss': 0.9701455438137054}
2025-01-19 01:20:22,252 [INFO] Step[2150/2713]: training loss : 0.9729773533344269 TRAIN  loss dict:  {'classification_loss': 0.9729773533344269}
2025-01-19 01:20:37,199 [INFO] Step[2200/2713]: training loss : 0.9845074236392974 TRAIN  loss dict:  {'classification_loss': 0.9845074236392974}
2025-01-19 01:20:52,203 [INFO] Step[2250/2713]: training loss : 0.9560132956504822 TRAIN  loss dict:  {'classification_loss': 0.9560132956504822}
2025-01-19 01:21:07,206 [INFO] Step[2300/2713]: training loss : 0.9822113406658173 TRAIN  loss dict:  {'classification_loss': 0.9822113406658173}
2025-01-19 01:21:22,203 [INFO] Step[2350/2713]: training loss : 0.9773671281337738 TRAIN  loss dict:  {'classification_loss': 0.9773671281337738}
2025-01-19 01:21:37,200 [INFO] Step[2400/2713]: training loss : 0.9772763764858245 TRAIN  loss dict:  {'classification_loss': 0.9772763764858245}
2025-01-19 01:21:52,214 [INFO] Step[2450/2713]: training loss : 0.988585616350174 TRAIN  loss dict:  {'classification_loss': 0.988585616350174}
2025-01-19 01:22:07,212 [INFO] Step[2500/2713]: training loss : 0.9713023316860199 TRAIN  loss dict:  {'classification_loss': 0.9713023316860199}
2025-01-19 01:22:22,220 [INFO] Step[2550/2713]: training loss : 0.963042596578598 TRAIN  loss dict:  {'classification_loss': 0.963042596578598}
2025-01-19 01:22:37,197 [INFO] Step[2600/2713]: training loss : 0.9687872803211213 TRAIN  loss dict:  {'classification_loss': 0.9687872803211213}
2025-01-19 01:22:52,172 [INFO] Step[2650/2713]: training loss : 0.9729136955738068 TRAIN  loss dict:  {'classification_loss': 0.9729136955738068}
2025-01-19 01:23:07,201 [INFO] Step[2700/2713]: training loss : 0.968290991783142 TRAIN  loss dict:  {'classification_loss': 0.968290991783142}
2025-01-19 01:24:27,511 [INFO] Label accuracies statistics:
2025-01-19 01:24:27,511 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.5, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 1.0, 205: 0.75, 206: 0.5, 207: 1.0, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 01:24:27,513 [INFO] [42] TRAIN  loss: 0.9680279653932703 acc: 0.9981570217471434
2025-01-19 01:24:27,513 [INFO] [42] TRAIN  loss dict: {'classification_loss': 0.9680279653932703}
2025-01-19 01:24:27,513 [INFO] [42] VALIDATION loss: 1.7566449609690142 VALIDATION acc: 0.7912225705329153
2025-01-19 01:24:27,513 [INFO] [42] VALIDATION loss dict: {'classification_loss': 1.7566449609690142}
2025-01-19 01:24:27,513 [INFO] 
2025-01-19 01:24:47,789 [INFO] Step[50/2713]: training loss : 0.9581266117095947 TRAIN  loss dict:  {'classification_loss': 0.9581266117095947}
2025-01-19 01:25:02,757 [INFO] Step[100/2713]: training loss : 0.9593319237232208 TRAIN  loss dict:  {'classification_loss': 0.9593319237232208}
2025-01-19 01:25:17,818 [INFO] Step[150/2713]: training loss : 0.9717874026298523 TRAIN  loss dict:  {'classification_loss': 0.9717874026298523}
2025-01-19 01:25:32,855 [INFO] Step[200/2713]: training loss : 0.95599942445755 TRAIN  loss dict:  {'classification_loss': 0.95599942445755}
2025-01-19 01:25:47,964 [INFO] Step[250/2713]: training loss : 0.9739656579494477 TRAIN  loss dict:  {'classification_loss': 0.9739656579494477}
2025-01-19 01:26:02,956 [INFO] Step[300/2713]: training loss : 0.9625635623931885 TRAIN  loss dict:  {'classification_loss': 0.9625635623931885}
2025-01-19 01:26:17,980 [INFO] Step[350/2713]: training loss : 0.9683932912349701 TRAIN  loss dict:  {'classification_loss': 0.9683932912349701}
2025-01-19 01:26:33,016 [INFO] Step[400/2713]: training loss : 0.9614968180656434 TRAIN  loss dict:  {'classification_loss': 0.9614968180656434}
2025-01-19 01:26:48,067 [INFO] Step[450/2713]: training loss : 0.9813062846660614 TRAIN  loss dict:  {'classification_loss': 0.9813062846660614}
2025-01-19 01:27:03,119 [INFO] Step[500/2713]: training loss : 0.9673937213420868 TRAIN  loss dict:  {'classification_loss': 0.9673937213420868}
2025-01-19 01:27:18,140 [INFO] Step[550/2713]: training loss : 0.9659976923465728 TRAIN  loss dict:  {'classification_loss': 0.9659976923465728}
2025-01-19 01:27:33,092 [INFO] Step[600/2713]: training loss : 0.9739594352245331 TRAIN  loss dict:  {'classification_loss': 0.9739594352245331}
2025-01-19 01:27:48,148 [INFO] Step[650/2713]: training loss : 0.9837967765331268 TRAIN  loss dict:  {'classification_loss': 0.9837967765331268}
2025-01-19 01:28:03,187 [INFO] Step[700/2713]: training loss : 0.9725260138511658 TRAIN  loss dict:  {'classification_loss': 0.9725260138511658}
2025-01-19 01:28:18,136 [INFO] Step[750/2713]: training loss : 0.969557865858078 TRAIN  loss dict:  {'classification_loss': 0.969557865858078}
2025-01-19 01:28:33,216 [INFO] Step[800/2713]: training loss : 0.9779391765594483 TRAIN  loss dict:  {'classification_loss': 0.9779391765594483}
2025-01-19 01:28:48,227 [INFO] Step[850/2713]: training loss : 0.9680248510837555 TRAIN  loss dict:  {'classification_loss': 0.9680248510837555}
2025-01-19 01:29:03,255 [INFO] Step[900/2713]: training loss : 0.9542888343334198 TRAIN  loss dict:  {'classification_loss': 0.9542888343334198}
2025-01-19 01:29:18,300 [INFO] Step[950/2713]: training loss : 0.9626029288768768 TRAIN  loss dict:  {'classification_loss': 0.9626029288768768}
2025-01-19 01:29:33,274 [INFO] Step[1000/2713]: training loss : 0.9590707886219024 TRAIN  loss dict:  {'classification_loss': 0.9590707886219024}
2025-01-19 01:29:48,331 [INFO] Step[1050/2713]: training loss : 0.9616992771625519 TRAIN  loss dict:  {'classification_loss': 0.9616992771625519}
2025-01-19 01:30:03,368 [INFO] Step[1100/2713]: training loss : 0.960686821937561 TRAIN  loss dict:  {'classification_loss': 0.960686821937561}
2025-01-19 01:30:18,410 [INFO] Step[1150/2713]: training loss : 0.9703726935386657 TRAIN  loss dict:  {'classification_loss': 0.9703726935386657}
2025-01-19 01:30:33,445 [INFO] Step[1200/2713]: training loss : 0.9717135775089264 TRAIN  loss dict:  {'classification_loss': 0.9717135775089264}
2025-01-19 01:30:48,472 [INFO] Step[1250/2713]: training loss : 0.9552408397197724 TRAIN  loss dict:  {'classification_loss': 0.9552408397197724}
2025-01-19 01:31:03,474 [INFO] Step[1300/2713]: training loss : 0.9619828748703003 TRAIN  loss dict:  {'classification_loss': 0.9619828748703003}
2025-01-19 01:31:18,523 [INFO] Step[1350/2713]: training loss : 0.9770525097846985 TRAIN  loss dict:  {'classification_loss': 0.9770525097846985}
2025-01-19 01:31:33,587 [INFO] Step[1400/2713]: training loss : 0.9716582334041596 TRAIN  loss dict:  {'classification_loss': 0.9716582334041596}
2025-01-19 01:31:48,597 [INFO] Step[1450/2713]: training loss : 0.9566540944576264 TRAIN  loss dict:  {'classification_loss': 0.9566540944576264}
2025-01-19 01:32:03,599 [INFO] Step[1500/2713]: training loss : 0.9715750646591187 TRAIN  loss dict:  {'classification_loss': 0.9715750646591187}
2025-01-19 01:32:18,623 [INFO] Step[1550/2713]: training loss : 0.9547531449794769 TRAIN  loss dict:  {'classification_loss': 0.9547531449794769}
2025-01-19 01:32:33,636 [INFO] Step[1600/2713]: training loss : 0.9694518220424652 TRAIN  loss dict:  {'classification_loss': 0.9694518220424652}
2025-01-19 01:32:48,697 [INFO] Step[1650/2713]: training loss : 0.9587348508834839 TRAIN  loss dict:  {'classification_loss': 0.9587348508834839}
2025-01-19 01:33:03,756 [INFO] Step[1700/2713]: training loss : 0.9635099852085114 TRAIN  loss dict:  {'classification_loss': 0.9635099852085114}
2025-01-19 01:33:18,809 [INFO] Step[1750/2713]: training loss : 0.9660199522972107 TRAIN  loss dict:  {'classification_loss': 0.9660199522972107}
2025-01-19 01:33:33,751 [INFO] Step[1800/2713]: training loss : 0.9563716757297516 TRAIN  loss dict:  {'classification_loss': 0.9563716757297516}
2025-01-19 01:33:48,775 [INFO] Step[1850/2713]: training loss : 0.9678379631042481 TRAIN  loss dict:  {'classification_loss': 0.9678379631042481}
2025-01-19 01:34:03,757 [INFO] Step[1900/2713]: training loss : 0.9653534150123596 TRAIN  loss dict:  {'classification_loss': 0.9653534150123596}
2025-01-19 01:34:18,801 [INFO] Step[1950/2713]: training loss : 0.9596164536476135 TRAIN  loss dict:  {'classification_loss': 0.9596164536476135}
2025-01-19 01:34:33,895 [INFO] Step[2000/2713]: training loss : 0.9608441054821014 TRAIN  loss dict:  {'classification_loss': 0.9608441054821014}
2025-01-19 01:34:48,941 [INFO] Step[2050/2713]: training loss : 0.9731410956382751 TRAIN  loss dict:  {'classification_loss': 0.9731410956382751}
2025-01-19 01:35:03,956 [INFO] Step[2100/2713]: training loss : 0.9672280883789063 TRAIN  loss dict:  {'classification_loss': 0.9672280883789063}
2025-01-19 01:35:18,844 [INFO] Step[2150/2713]: training loss : 0.9621344113349914 TRAIN  loss dict:  {'classification_loss': 0.9621344113349914}
2025-01-19 01:35:33,861 [INFO] Step[2200/2713]: training loss : 0.9578240191936493 TRAIN  loss dict:  {'classification_loss': 0.9578240191936493}
2025-01-19 01:35:48,871 [INFO] Step[2250/2713]: training loss : 0.967641065120697 TRAIN  loss dict:  {'classification_loss': 0.967641065120697}
2025-01-19 01:36:03,866 [INFO] Step[2300/2713]: training loss : 0.9645121157169342 TRAIN  loss dict:  {'classification_loss': 0.9645121157169342}
2025-01-19 01:36:18,876 [INFO] Step[2350/2713]: training loss : 0.9588877105712891 TRAIN  loss dict:  {'classification_loss': 0.9588877105712891}
2025-01-19 01:36:33,852 [INFO] Step[2400/2713]: training loss : 0.9689207351207734 TRAIN  loss dict:  {'classification_loss': 0.9689207351207734}
2025-01-19 01:36:48,812 [INFO] Step[2450/2713]: training loss : 0.9592103445529938 TRAIN  loss dict:  {'classification_loss': 0.9592103445529938}
2025-01-19 01:37:03,786 [INFO] Step[2500/2713]: training loss : 0.9629806935787201 TRAIN  loss dict:  {'classification_loss': 0.9629806935787201}
2025-01-19 01:37:18,768 [INFO] Step[2550/2713]: training loss : 0.9650318133831024 TRAIN  loss dict:  {'classification_loss': 0.9650318133831024}
2025-01-19 01:37:33,845 [INFO] Step[2600/2713]: training loss : 0.9564111530780792 TRAIN  loss dict:  {'classification_loss': 0.9564111530780792}
2025-01-19 01:37:48,866 [INFO] Step[2650/2713]: training loss : 0.9628920555114746 TRAIN  loss dict:  {'classification_loss': 0.9628920555114746}
2025-01-19 01:38:03,909 [INFO] Step[2700/2713]: training loss : 0.964423633813858 TRAIN  loss dict:  {'classification_loss': 0.964423633813858}
2025-01-19 01:39:24,047 [INFO] Label accuracies statistics:
2025-01-19 01:39:24,047 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.0, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 01:39:24,049 [INFO] [43] TRAIN  loss: 0.965280558475919 acc: 0.9986484826145718
2025-01-19 01:39:24,049 [INFO] [43] TRAIN  loss dict: {'classification_loss': 0.965280558475919}
2025-01-19 01:39:24,049 [INFO] [43] VALIDATION loss: 1.7346677935885308 VALIDATION acc: 0.7987460815047022
2025-01-19 01:39:24,049 [INFO] [43] VALIDATION loss dict: {'classification_loss': 1.7346677935885308}
2025-01-19 01:39:24,049 [INFO] 
2025-01-19 01:39:44,381 [INFO] Step[50/2713]: training loss : 0.9602660357952117 TRAIN  loss dict:  {'classification_loss': 0.9602660357952117}
2025-01-19 01:39:59,180 [INFO] Step[100/2713]: training loss : 0.961500667333603 TRAIN  loss dict:  {'classification_loss': 0.961500667333603}
2025-01-19 01:40:14,042 [INFO] Step[150/2713]: training loss : 0.9625690817832947 TRAIN  loss dict:  {'classification_loss': 0.9625690817832947}
2025-01-19 01:40:28,861 [INFO] Step[200/2713]: training loss : 0.9576529383659362 TRAIN  loss dict:  {'classification_loss': 0.9576529383659362}
2025-01-19 01:40:43,671 [INFO] Step[250/2713]: training loss : 0.9599469697475433 TRAIN  loss dict:  {'classification_loss': 0.9599469697475433}
2025-01-19 01:40:58,487 [INFO] Step[300/2713]: training loss : 0.9618743944168091 TRAIN  loss dict:  {'classification_loss': 0.9618743944168091}
2025-01-19 01:41:13,291 [INFO] Step[350/2713]: training loss : 0.9783828723430633 TRAIN  loss dict:  {'classification_loss': 0.9783828723430633}
2025-01-19 01:41:28,099 [INFO] Step[400/2713]: training loss : 0.9535690486431122 TRAIN  loss dict:  {'classification_loss': 0.9535690486431122}
2025-01-19 01:41:42,905 [INFO] Step[450/2713]: training loss : 0.9630726420879364 TRAIN  loss dict:  {'classification_loss': 0.9630726420879364}
2025-01-19 01:41:57,762 [INFO] Step[500/2713]: training loss : 0.9571861016750336 TRAIN  loss dict:  {'classification_loss': 0.9571861016750336}
2025-01-19 01:42:12,611 [INFO] Step[550/2713]: training loss : 0.9587910652160645 TRAIN  loss dict:  {'classification_loss': 0.9587910652160645}
2025-01-19 01:42:27,443 [INFO] Step[600/2713]: training loss : 0.9591058921813965 TRAIN  loss dict:  {'classification_loss': 0.9591058921813965}
2025-01-19 01:42:42,317 [INFO] Step[650/2713]: training loss : 0.9579580438137054 TRAIN  loss dict:  {'classification_loss': 0.9579580438137054}
2025-01-19 01:42:57,173 [INFO] Step[700/2713]: training loss : 0.9616014742851258 TRAIN  loss dict:  {'classification_loss': 0.9616014742851258}
2025-01-19 01:43:11,962 [INFO] Step[750/2713]: training loss : 0.9680138206481934 TRAIN  loss dict:  {'classification_loss': 0.9680138206481934}
2025-01-19 01:43:26,765 [INFO] Step[800/2713]: training loss : 0.9831893086433411 TRAIN  loss dict:  {'classification_loss': 0.9831893086433411}
2025-01-19 01:43:41,573 [INFO] Step[850/2713]: training loss : 0.9586494660377503 TRAIN  loss dict:  {'classification_loss': 0.9586494660377503}
2025-01-19 01:43:56,392 [INFO] Step[900/2713]: training loss : 0.96223184466362 TRAIN  loss dict:  {'classification_loss': 0.96223184466362}
2025-01-19 01:44:11,250 [INFO] Step[950/2713]: training loss : 0.9870193421840667 TRAIN  loss dict:  {'classification_loss': 0.9870193421840667}
2025-01-19 01:44:26,057 [INFO] Step[1000/2713]: training loss : 0.9649703598022461 TRAIN  loss dict:  {'classification_loss': 0.9649703598022461}
2025-01-19 01:44:40,832 [INFO] Step[1050/2713]: training loss : 0.9698805618286133 TRAIN  loss dict:  {'classification_loss': 0.9698805618286133}
2025-01-19 01:44:55,611 [INFO] Step[1100/2713]: training loss : 0.9642434000968934 TRAIN  loss dict:  {'classification_loss': 0.9642434000968934}
2025-01-19 01:45:10,405 [INFO] Step[1150/2713]: training loss : 0.9772625911235809 TRAIN  loss dict:  {'classification_loss': 0.9772625911235809}
2025-01-19 01:45:25,225 [INFO] Step[1200/2713]: training loss : 0.9647246289253235 TRAIN  loss dict:  {'classification_loss': 0.9647246289253235}
2025-01-19 01:45:40,050 [INFO] Step[1250/2713]: training loss : 0.9722009205818176 TRAIN  loss dict:  {'classification_loss': 0.9722009205818176}
2025-01-19 01:45:54,811 [INFO] Step[1300/2713]: training loss : 0.9627005589008332 TRAIN  loss dict:  {'classification_loss': 0.9627005589008332}
2025-01-19 01:46:09,638 [INFO] Step[1350/2713]: training loss : 0.9703073632717133 TRAIN  loss dict:  {'classification_loss': 0.9703073632717133}
2025-01-19 01:46:24,453 [INFO] Step[1400/2713]: training loss : 0.9705477201938629 TRAIN  loss dict:  {'classification_loss': 0.9705477201938629}
2025-01-19 01:46:39,265 [INFO] Step[1450/2713]: training loss : 0.9654719746112823 TRAIN  loss dict:  {'classification_loss': 0.9654719746112823}
2025-01-19 01:46:54,091 [INFO] Step[1500/2713]: training loss : 0.9859885430335998 TRAIN  loss dict:  {'classification_loss': 0.9859885430335998}
2025-01-19 01:47:08,898 [INFO] Step[1550/2713]: training loss : 0.9787353122234345 TRAIN  loss dict:  {'classification_loss': 0.9787353122234345}
2025-01-19 01:47:23,689 [INFO] Step[1600/2713]: training loss : 0.9697304844856263 TRAIN  loss dict:  {'classification_loss': 0.9697304844856263}
2025-01-19 01:47:38,492 [INFO] Step[1650/2713]: training loss : 0.9561477720737457 TRAIN  loss dict:  {'classification_loss': 0.9561477720737457}
2025-01-19 01:47:53,355 [INFO] Step[1700/2713]: training loss : 0.9561806404590607 TRAIN  loss dict:  {'classification_loss': 0.9561806404590607}
2025-01-19 01:48:08,157 [INFO] Step[1750/2713]: training loss : 0.963938090801239 TRAIN  loss dict:  {'classification_loss': 0.963938090801239}
2025-01-19 01:48:22,980 [INFO] Step[1800/2713]: training loss : 0.9621776378154755 TRAIN  loss dict:  {'classification_loss': 0.9621776378154755}
2025-01-19 01:48:37,827 [INFO] Step[1850/2713]: training loss : 0.9571851003170013 TRAIN  loss dict:  {'classification_loss': 0.9571851003170013}
2025-01-19 01:48:52,611 [INFO] Step[1900/2713]: training loss : 0.9690349352359772 TRAIN  loss dict:  {'classification_loss': 0.9690349352359772}
2025-01-19 01:49:07,413 [INFO] Step[1950/2713]: training loss : 0.9591527080535889 TRAIN  loss dict:  {'classification_loss': 0.9591527080535889}
2025-01-19 01:49:22,186 [INFO] Step[2000/2713]: training loss : 0.9697393882274628 TRAIN  loss dict:  {'classification_loss': 0.9697393882274628}
2025-01-19 01:49:36,997 [INFO] Step[2050/2713]: training loss : 0.9613199651241302 TRAIN  loss dict:  {'classification_loss': 0.9613199651241302}
2025-01-19 01:49:51,817 [INFO] Step[2100/2713]: training loss : 0.9699799203872681 TRAIN  loss dict:  {'classification_loss': 0.9699799203872681}
2025-01-19 01:50:06,672 [INFO] Step[2150/2713]: training loss : 0.9640935981273651 TRAIN  loss dict:  {'classification_loss': 0.9640935981273651}
2025-01-19 01:50:21,435 [INFO] Step[2200/2713]: training loss : 0.9635105001926422 TRAIN  loss dict:  {'classification_loss': 0.9635105001926422}
2025-01-19 01:50:36,282 [INFO] Step[2250/2713]: training loss : 0.9584002494812012 TRAIN  loss dict:  {'classification_loss': 0.9584002494812012}
2025-01-19 01:50:51,143 [INFO] Step[2300/2713]: training loss : 0.9767553198337555 TRAIN  loss dict:  {'classification_loss': 0.9767553198337555}
2025-01-19 01:51:06,255 [INFO] Step[2350/2713]: training loss : 0.9565635251998902 TRAIN  loss dict:  {'classification_loss': 0.9565635251998902}
2025-01-19 01:51:21,369 [INFO] Step[2400/2713]: training loss : 0.9615444827079773 TRAIN  loss dict:  {'classification_loss': 0.9615444827079773}
2025-01-19 01:51:36,458 [INFO] Step[2450/2713]: training loss : 0.9666023111343384 TRAIN  loss dict:  {'classification_loss': 0.9666023111343384}
2025-01-19 01:51:51,558 [INFO] Step[2500/2713]: training loss : 0.9537925410270691 TRAIN  loss dict:  {'classification_loss': 0.9537925410270691}
2025-01-19 01:52:06,642 [INFO] Step[2550/2713]: training loss : 0.9570800971984863 TRAIN  loss dict:  {'classification_loss': 0.9570800971984863}
2025-01-19 01:52:21,727 [INFO] Step[2600/2713]: training loss : 0.9568767595291138 TRAIN  loss dict:  {'classification_loss': 0.9568767595291138}
2025-01-19 01:52:36,826 [INFO] Step[2650/2713]: training loss : 0.9596319222450256 TRAIN  loss dict:  {'classification_loss': 0.9596319222450256}
2025-01-19 01:52:51,900 [INFO] Step[2700/2713]: training loss : 0.9687258744239807 TRAIN  loss dict:  {'classification_loss': 0.9687258744239807}
2025-01-19 01:54:11,550 [INFO] Label accuracies statistics:
2025-01-19 01:54:11,550 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.5, 33: 0.25, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.5, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.5, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.25, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.25, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 01:54:11,552 [INFO] [44] TRAIN  loss: 0.9647353822307045 acc: 0.9985256173977147
2025-01-19 01:54:11,552 [INFO] [44] TRAIN  loss dict: {'classification_loss': 0.9647353822307045}
2025-01-19 01:54:11,552 [INFO] [44] VALIDATION loss: 1.7718501692651807 VALIDATION acc: 0.7887147335423198
2025-01-19 01:54:11,552 [INFO] [44] VALIDATION loss dict: {'classification_loss': 1.7718501692651807}
2025-01-19 01:54:11,552 [INFO] 
2025-01-19 01:54:31,170 [INFO] Step[50/2713]: training loss : 0.9783590340614319 TRAIN  loss dict:  {'classification_loss': 0.9783590340614319}
2025-01-19 01:54:46,246 [INFO] Step[100/2713]: training loss : 0.9802658712863922 TRAIN  loss dict:  {'classification_loss': 0.9802658712863922}
2025-01-19 01:55:01,368 [INFO] Step[150/2713]: training loss : 0.9583513307571411 TRAIN  loss dict:  {'classification_loss': 0.9583513307571411}
2025-01-19 01:55:16,466 [INFO] Step[200/2713]: training loss : 0.9546499955654144 TRAIN  loss dict:  {'classification_loss': 0.9546499955654144}
2025-01-19 01:55:31,597 [INFO] Step[250/2713]: training loss : 0.9622143328189849 TRAIN  loss dict:  {'classification_loss': 0.9622143328189849}
2025-01-19 01:55:46,682 [INFO] Step[300/2713]: training loss : 0.9536750710010529 TRAIN  loss dict:  {'classification_loss': 0.9536750710010529}
2025-01-19 01:56:01,782 [INFO] Step[350/2713]: training loss : 0.9634331583976745 TRAIN  loss dict:  {'classification_loss': 0.9634331583976745}
2025-01-19 01:56:16,850 [INFO] Step[400/2713]: training loss : 0.9618756949901581 TRAIN  loss dict:  {'classification_loss': 0.9618756949901581}
2025-01-19 01:56:31,952 [INFO] Step[450/2713]: training loss : 0.9584203934669495 TRAIN  loss dict:  {'classification_loss': 0.9584203934669495}
2025-01-19 01:56:47,007 [INFO] Step[500/2713]: training loss : 0.9530250716209412 TRAIN  loss dict:  {'classification_loss': 0.9530250716209412}
2025-01-19 01:57:02,074 [INFO] Step[550/2713]: training loss : 0.9559054732322693 TRAIN  loss dict:  {'classification_loss': 0.9559054732322693}
2025-01-19 01:57:17,168 [INFO] Step[600/2713]: training loss : 0.9581023001670838 TRAIN  loss dict:  {'classification_loss': 0.9581023001670838}
2025-01-19 01:57:32,236 [INFO] Step[650/2713]: training loss : 0.9626964604854584 TRAIN  loss dict:  {'classification_loss': 0.9626964604854584}
2025-01-19 01:57:47,320 [INFO] Step[700/2713]: training loss : 0.9597423887252807 TRAIN  loss dict:  {'classification_loss': 0.9597423887252807}
2025-01-19 01:58:02,424 [INFO] Step[750/2713]: training loss : 0.9621014308929443 TRAIN  loss dict:  {'classification_loss': 0.9621014308929443}
2025-01-19 01:58:17,496 [INFO] Step[800/2713]: training loss : 0.9609814321994782 TRAIN  loss dict:  {'classification_loss': 0.9609814321994782}
2025-01-19 01:58:32,603 [INFO] Step[850/2713]: training loss : 0.9739638423919678 TRAIN  loss dict:  {'classification_loss': 0.9739638423919678}
2025-01-19 01:58:47,688 [INFO] Step[900/2713]: training loss : 0.9826749455928803 TRAIN  loss dict:  {'classification_loss': 0.9826749455928803}
2025-01-19 01:59:02,781 [INFO] Step[950/2713]: training loss : 0.9580199778079986 TRAIN  loss dict:  {'classification_loss': 0.9580199778079986}
2025-01-19 01:59:17,838 [INFO] Step[1000/2713]: training loss : 0.9654094707965851 TRAIN  loss dict:  {'classification_loss': 0.9654094707965851}
2025-01-19 01:59:32,917 [INFO] Step[1050/2713]: training loss : 0.9592544257640838 TRAIN  loss dict:  {'classification_loss': 0.9592544257640838}
2025-01-19 01:59:48,018 [INFO] Step[1100/2713]: training loss : 0.9633781599998474 TRAIN  loss dict:  {'classification_loss': 0.9633781599998474}
2025-01-19 02:00:03,095 [INFO] Step[1150/2713]: training loss : 0.9578889846801758 TRAIN  loss dict:  {'classification_loss': 0.9578889846801758}
2025-01-19 02:00:18,145 [INFO] Step[1200/2713]: training loss : 0.9582112264633179 TRAIN  loss dict:  {'classification_loss': 0.9582112264633179}
2025-01-19 02:00:33,250 [INFO] Step[1250/2713]: training loss : 0.9629705262184143 TRAIN  loss dict:  {'classification_loss': 0.9629705262184143}
2025-01-19 02:00:48,329 [INFO] Step[1300/2713]: training loss : 0.9555600237846374 TRAIN  loss dict:  {'classification_loss': 0.9555600237846374}
2025-01-19 02:01:03,466 [INFO] Step[1350/2713]: training loss : 0.974143522977829 TRAIN  loss dict:  {'classification_loss': 0.974143522977829}
2025-01-19 02:01:18,579 [INFO] Step[1400/2713]: training loss : 0.9622239482402801 TRAIN  loss dict:  {'classification_loss': 0.9622239482402801}
2025-01-19 02:01:33,693 [INFO] Step[1450/2713]: training loss : 0.9575468480587006 TRAIN  loss dict:  {'classification_loss': 0.9575468480587006}
2025-01-19 02:01:48,778 [INFO] Step[1500/2713]: training loss : 0.9569178593158721 TRAIN  loss dict:  {'classification_loss': 0.9569178593158721}
2025-01-19 02:02:03,885 [INFO] Step[1550/2713]: training loss : 0.9653814911842347 TRAIN  loss dict:  {'classification_loss': 0.9653814911842347}
2025-01-19 02:02:18,940 [INFO] Step[1600/2713]: training loss : 0.9636688804626465 TRAIN  loss dict:  {'classification_loss': 0.9636688804626465}
2025-01-19 02:02:34,053 [INFO] Step[1650/2713]: training loss : 0.9634782373905182 TRAIN  loss dict:  {'classification_loss': 0.9634782373905182}
2025-01-19 02:02:49,122 [INFO] Step[1700/2713]: training loss : 0.9662146961688995 TRAIN  loss dict:  {'classification_loss': 0.9662146961688995}
2025-01-19 02:03:04,235 [INFO] Step[1750/2713]: training loss : 0.9611899769306183 TRAIN  loss dict:  {'classification_loss': 0.9611899769306183}
2025-01-19 02:03:19,299 [INFO] Step[1800/2713]: training loss : 0.9594128847122192 TRAIN  loss dict:  {'classification_loss': 0.9594128847122192}
2025-01-19 02:03:34,376 [INFO] Step[1850/2713]: training loss : 0.9767259109020233 TRAIN  loss dict:  {'classification_loss': 0.9767259109020233}
2025-01-19 02:03:49,446 [INFO] Step[1900/2713]: training loss : 0.9668030858039856 TRAIN  loss dict:  {'classification_loss': 0.9668030858039856}
2025-01-19 02:04:04,534 [INFO] Step[1950/2713]: training loss : 0.9626064813137054 TRAIN  loss dict:  {'classification_loss': 0.9626064813137054}
2025-01-19 02:04:19,586 [INFO] Step[2000/2713]: training loss : 0.9543124556541442 TRAIN  loss dict:  {'classification_loss': 0.9543124556541442}
2025-01-19 02:04:34,674 [INFO] Step[2050/2713]: training loss : 0.9682127583026886 TRAIN  loss dict:  {'classification_loss': 0.9682127583026886}
2025-01-19 02:04:49,780 [INFO] Step[2100/2713]: training loss : 0.9667154169082641 TRAIN  loss dict:  {'classification_loss': 0.9667154169082641}
2025-01-19 02:05:04,891 [INFO] Step[2150/2713]: training loss : 0.9570241761207581 TRAIN  loss dict:  {'classification_loss': 0.9570241761207581}
2025-01-19 02:05:19,983 [INFO] Step[2200/2713]: training loss : 0.9586201870441436 TRAIN  loss dict:  {'classification_loss': 0.9586201870441436}
2025-01-19 02:05:35,082 [INFO] Step[2250/2713]: training loss : 0.9664044964313507 TRAIN  loss dict:  {'classification_loss': 0.9664044964313507}
2025-01-19 02:05:50,179 [INFO] Step[2300/2713]: training loss : 0.9614665389060975 TRAIN  loss dict:  {'classification_loss': 0.9614665389060975}
2025-01-19 02:06:05,266 [INFO] Step[2350/2713]: training loss : 0.9689101648330688 TRAIN  loss dict:  {'classification_loss': 0.9689101648330688}
2025-01-19 02:06:20,361 [INFO] Step[2400/2713]: training loss : 0.9550953722000122 TRAIN  loss dict:  {'classification_loss': 0.9550953722000122}
2025-01-19 02:06:35,401 [INFO] Step[2450/2713]: training loss : 0.9886523520946503 TRAIN  loss dict:  {'classification_loss': 0.9886523520946503}
2025-01-19 02:06:50,502 [INFO] Step[2500/2713]: training loss : 0.9520342814922332 TRAIN  loss dict:  {'classification_loss': 0.9520342814922332}
2025-01-19 02:07:05,626 [INFO] Step[2550/2713]: training loss : 0.9698097336292267 TRAIN  loss dict:  {'classification_loss': 0.9698097336292267}
2025-01-19 02:07:20,679 [INFO] Step[2600/2713]: training loss : 0.9972876596450806 TRAIN  loss dict:  {'classification_loss': 0.9972876596450806}
2025-01-19 02:07:35,720 [INFO] Step[2650/2713]: training loss : 0.9692826581001281 TRAIN  loss dict:  {'classification_loss': 0.9692826581001281}
2025-01-19 02:07:50,766 [INFO] Step[2700/2713]: training loss : 0.957993894815445 TRAIN  loss dict:  {'classification_loss': 0.957993894815445}
2025-01-19 02:09:11,001 [INFO] Label accuracies statistics:
2025-01-19 02:09:11,001 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.5, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 0.75, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.5, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.5, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 1.0, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 0.75, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 0.75, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 1.0, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.5, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 02:09:11,003 [INFO] [45] TRAIN  loss: 0.9638925401917516 acc: 0.9984027521808576
2025-01-19 02:09:11,003 [INFO] [45] TRAIN  loss dict: {'classification_loss': 0.9638925401917516}
2025-01-19 02:09:11,003 [INFO] [45] VALIDATION loss: 1.7652931269398309 VALIDATION acc: 0.7968652037617555
2025-01-19 02:09:11,003 [INFO] [45] VALIDATION loss dict: {'classification_loss': 1.7652931269398309}
2025-01-19 02:09:11,004 [INFO] 
2025-01-19 02:09:30,732 [INFO] Step[50/2713]: training loss : 0.9550769805908204 TRAIN  loss dict:  {'classification_loss': 0.9550769805908204}
2025-01-19 02:09:45,867 [INFO] Step[100/2713]: training loss : 0.9570260787010193 TRAIN  loss dict:  {'classification_loss': 0.9570260787010193}
2025-01-19 02:10:00,965 [INFO] Step[150/2713]: training loss : 0.9587681353092193 TRAIN  loss dict:  {'classification_loss': 0.9587681353092193}
2025-01-19 02:10:16,030 [INFO] Step[200/2713]: training loss : 0.9619291591644287 TRAIN  loss dict:  {'classification_loss': 0.9619291591644287}
2025-01-19 02:10:31,111 [INFO] Step[250/2713]: training loss : 0.9584844505786896 TRAIN  loss dict:  {'classification_loss': 0.9584844505786896}
2025-01-19 02:10:46,162 [INFO] Step[300/2713]: training loss : 0.9606455039978027 TRAIN  loss dict:  {'classification_loss': 0.9606455039978027}
2025-01-19 02:11:01,247 [INFO] Step[350/2713]: training loss : 0.9560032963752747 TRAIN  loss dict:  {'classification_loss': 0.9560032963752747}
2025-01-19 02:11:16,316 [INFO] Step[400/2713]: training loss : 0.9566353368759155 TRAIN  loss dict:  {'classification_loss': 0.9566353368759155}
2025-01-19 02:11:31,348 [INFO] Step[450/2713]: training loss : 0.954114224910736 TRAIN  loss dict:  {'classification_loss': 0.954114224910736}
2025-01-19 02:11:46,428 [INFO] Step[500/2713]: training loss : 0.9673755824565887 TRAIN  loss dict:  {'classification_loss': 0.9673755824565887}
2025-01-19 02:12:01,490 [INFO] Step[550/2713]: training loss : 0.953728415966034 TRAIN  loss dict:  {'classification_loss': 0.953728415966034}
2025-01-19 02:12:16,561 [INFO] Step[600/2713]: training loss : 0.9652190113067627 TRAIN  loss dict:  {'classification_loss': 0.9652190113067627}
2025-01-19 02:12:31,617 [INFO] Step[650/2713]: training loss : 0.9520884943008423 TRAIN  loss dict:  {'classification_loss': 0.9520884943008423}
2025-01-19 02:12:46,636 [INFO] Step[700/2713]: training loss : 0.967134325504303 TRAIN  loss dict:  {'classification_loss': 0.967134325504303}
2025-01-19 02:13:01,736 [INFO] Step[750/2713]: training loss : 0.962572330236435 TRAIN  loss dict:  {'classification_loss': 0.962572330236435}
2025-01-19 02:13:16,794 [INFO] Step[800/2713]: training loss : 0.9760620832443238 TRAIN  loss dict:  {'classification_loss': 0.9760620832443238}
2025-01-19 02:13:31,893 [INFO] Step[850/2713]: training loss : 0.9638425815105438 TRAIN  loss dict:  {'classification_loss': 0.9638425815105438}
2025-01-19 02:13:46,985 [INFO] Step[900/2713]: training loss : 0.9559482407569885 TRAIN  loss dict:  {'classification_loss': 0.9559482407569885}
2025-01-19 02:14:02,052 [INFO] Step[950/2713]: training loss : 0.9536664891242981 TRAIN  loss dict:  {'classification_loss': 0.9536664891242981}
2025-01-19 02:14:17,123 [INFO] Step[1000/2713]: training loss : 0.9572822272777557 TRAIN  loss dict:  {'classification_loss': 0.9572822272777557}
2025-01-19 02:14:32,200 [INFO] Step[1050/2713]: training loss : 0.9538999497890472 TRAIN  loss dict:  {'classification_loss': 0.9538999497890472}
2025-01-19 02:14:47,266 [INFO] Step[1100/2713]: training loss : 0.9610912930965424 TRAIN  loss dict:  {'classification_loss': 0.9610912930965424}
2025-01-19 02:15:02,358 [INFO] Step[1150/2713]: training loss : 0.9565623557567596 TRAIN  loss dict:  {'classification_loss': 0.9565623557567596}
2025-01-19 02:15:17,421 [INFO] Step[1200/2713]: training loss : 0.9560032832622528 TRAIN  loss dict:  {'classification_loss': 0.9560032832622528}
2025-01-19 02:15:32,483 [INFO] Step[1250/2713]: training loss : 0.9661532700061798 TRAIN  loss dict:  {'classification_loss': 0.9661532700061798}
2025-01-19 02:15:47,536 [INFO] Step[1300/2713]: training loss : 0.9587875974178314 TRAIN  loss dict:  {'classification_loss': 0.9587875974178314}
2025-01-19 02:16:02,595 [INFO] Step[1350/2713]: training loss : 0.9687669467926026 TRAIN  loss dict:  {'classification_loss': 0.9687669467926026}
2025-01-19 02:16:17,645 [INFO] Step[1400/2713]: training loss : 0.9598286628723145 TRAIN  loss dict:  {'classification_loss': 0.9598286628723145}
2025-01-19 02:16:32,654 [INFO] Step[1450/2713]: training loss : 0.9589174592494965 TRAIN  loss dict:  {'classification_loss': 0.9589174592494965}
2025-01-19 02:16:47,758 [INFO] Step[1500/2713]: training loss : 0.9566096496582032 TRAIN  loss dict:  {'classification_loss': 0.9566096496582032}
2025-01-19 02:17:02,845 [INFO] Step[1550/2713]: training loss : 0.9634146296977997 TRAIN  loss dict:  {'classification_loss': 0.9634146296977997}
2025-01-19 02:17:17,888 [INFO] Step[1600/2713]: training loss : 0.9952474999427795 TRAIN  loss dict:  {'classification_loss': 0.9952474999427795}
2025-01-19 02:17:33,022 [INFO] Step[1650/2713]: training loss : 0.9922521138191223 TRAIN  loss dict:  {'classification_loss': 0.9922521138191223}
2025-01-19 02:17:48,064 [INFO] Step[1700/2713]: training loss : 0.9576980340480804 TRAIN  loss dict:  {'classification_loss': 0.9576980340480804}
2025-01-19 02:18:03,160 [INFO] Step[1750/2713]: training loss : 0.9736472058296204 TRAIN  loss dict:  {'classification_loss': 0.9736472058296204}
2025-01-19 02:18:18,200 [INFO] Step[1800/2713]: training loss : 0.9528681409358978 TRAIN  loss dict:  {'classification_loss': 0.9528681409358978}
2025-01-19 02:18:33,292 [INFO] Step[1850/2713]: training loss : 0.9572052669525146 TRAIN  loss dict:  {'classification_loss': 0.9572052669525146}
2025-01-19 02:18:48,356 [INFO] Step[1900/2713]: training loss : 0.9570419359207153 TRAIN  loss dict:  {'classification_loss': 0.9570419359207153}
2025-01-19 02:19:03,445 [INFO] Step[1950/2713]: training loss : 0.9613216531276703 TRAIN  loss dict:  {'classification_loss': 0.9613216531276703}
2025-01-19 02:19:18,520 [INFO] Step[2000/2713]: training loss : 0.9554274618625641 TRAIN  loss dict:  {'classification_loss': 0.9554274618625641}
2025-01-19 02:19:33,625 [INFO] Step[2050/2713]: training loss : 0.9574349081516266 TRAIN  loss dict:  {'classification_loss': 0.9574349081516266}
2025-01-19 02:19:48,639 [INFO] Step[2100/2713]: training loss : 0.9557373237609863 TRAIN  loss dict:  {'classification_loss': 0.9557373237609863}
2025-01-19 02:20:03,707 [INFO] Step[2150/2713]: training loss : 0.957596960067749 TRAIN  loss dict:  {'classification_loss': 0.957596960067749}
2025-01-19 02:20:18,716 [INFO] Step[2200/2713]: training loss : 0.9599554574489594 TRAIN  loss dict:  {'classification_loss': 0.9599554574489594}
2025-01-19 02:20:33,795 [INFO] Step[2250/2713]: training loss : 0.9549282729625702 TRAIN  loss dict:  {'classification_loss': 0.9549282729625702}
2025-01-19 02:20:48,893 [INFO] Step[2300/2713]: training loss : 0.9602072191238403 TRAIN  loss dict:  {'classification_loss': 0.9602072191238403}
2025-01-19 02:21:03,922 [INFO] Step[2350/2713]: training loss : 0.961321222782135 TRAIN  loss dict:  {'classification_loss': 0.961321222782135}
2025-01-19 02:21:18,952 [INFO] Step[2400/2713]: training loss : 0.9672532498836517 TRAIN  loss dict:  {'classification_loss': 0.9672532498836517}
2025-01-19 02:21:33,985 [INFO] Step[2450/2713]: training loss : 0.9596277332305908 TRAIN  loss dict:  {'classification_loss': 0.9596277332305908}
2025-01-19 02:21:49,043 [INFO] Step[2500/2713]: training loss : 0.963202781677246 TRAIN  loss dict:  {'classification_loss': 0.963202781677246}
2025-01-19 02:22:04,117 [INFO] Step[2550/2713]: training loss : 0.9578769814968109 TRAIN  loss dict:  {'classification_loss': 0.9578769814968109}
2025-01-19 02:22:19,225 [INFO] Step[2600/2713]: training loss : 0.9555869221687316 TRAIN  loss dict:  {'classification_loss': 0.9555869221687316}
2025-01-19 02:22:34,284 [INFO] Step[2650/2713]: training loss : 0.9559663701057434 TRAIN  loss dict:  {'classification_loss': 0.9559663701057434}
2025-01-19 02:22:49,331 [INFO] Step[2700/2713]: training loss : 0.9747474908828735 TRAIN  loss dict:  {'classification_loss': 0.9747474908828735}
2025-01-19 02:24:09,574 [INFO] Label accuracies statistics:
2025-01-19 02:24:09,574 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 1.0, 35: 0.75, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.75, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.25, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 0.75, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.25, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 1.0, 365: 1.0, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 1.0, 378: 0.5, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 02:24:09,576 [INFO] [46] TRAIN  loss: 0.9610721223740133 acc: 0.9990170782651432
2025-01-19 02:24:09,576 [INFO] [46] TRAIN  loss dict: {'classification_loss': 0.9610721223740133}
2025-01-19 02:24:09,576 [INFO] [46] VALIDATION loss: 1.7825297214707037 VALIDATION acc: 0.799373040752351
2025-01-19 02:24:09,576 [INFO] [46] VALIDATION loss dict: {'classification_loss': 1.7825297214707037}
2025-01-19 02:24:09,576 [INFO] 
2025-01-19 02:24:30,266 [INFO] Step[50/2713]: training loss : 0.9617611074447632 TRAIN  loss dict:  {'classification_loss': 0.9617611074447632}
2025-01-19 02:24:45,322 [INFO] Step[100/2713]: training loss : 0.9537229537963867 TRAIN  loss dict:  {'classification_loss': 0.9537229537963867}
2025-01-19 02:25:00,435 [INFO] Step[150/2713]: training loss : 0.9545057034492492 TRAIN  loss dict:  {'classification_loss': 0.9545057034492492}
2025-01-19 02:25:15,501 [INFO] Step[200/2713]: training loss : 0.9571534848213196 TRAIN  loss dict:  {'classification_loss': 0.9571534848213196}
2025-01-19 02:25:30,561 [INFO] Step[250/2713]: training loss : 0.9646985912322998 TRAIN  loss dict:  {'classification_loss': 0.9646985912322998}
2025-01-19 02:25:45,567 [INFO] Step[300/2713]: training loss : 0.9757379400730133 TRAIN  loss dict:  {'classification_loss': 0.9757379400730133}
2025-01-19 02:26:00,605 [INFO] Step[350/2713]: training loss : 0.9695465648174286 TRAIN  loss dict:  {'classification_loss': 0.9695465648174286}
2025-01-19 02:26:15,647 [INFO] Step[400/2713]: training loss : 0.9622566330432892 TRAIN  loss dict:  {'classification_loss': 0.9622566330432892}
2025-01-19 02:26:30,700 [INFO] Step[450/2713]: training loss : 0.9778488945960998 TRAIN  loss dict:  {'classification_loss': 0.9778488945960998}
2025-01-19 02:26:45,720 [INFO] Step[500/2713]: training loss : 0.9599270129203796 TRAIN  loss dict:  {'classification_loss': 0.9599270129203796}
2025-01-19 02:27:00,767 [INFO] Step[550/2713]: training loss : 0.9689241886138916 TRAIN  loss dict:  {'classification_loss': 0.9689241886138916}
2025-01-19 02:27:15,822 [INFO] Step[600/2713]: training loss : 0.957504506111145 TRAIN  loss dict:  {'classification_loss': 0.957504506111145}
2025-01-19 02:27:30,890 [INFO] Step[650/2713]: training loss : 0.958761031627655 TRAIN  loss dict:  {'classification_loss': 0.958761031627655}
2025-01-19 02:27:45,930 [INFO] Step[700/2713]: training loss : 0.9958441543579102 TRAIN  loss dict:  {'classification_loss': 0.9958441543579102}
2025-01-19 02:28:01,005 [INFO] Step[750/2713]: training loss : 0.9572078335285187 TRAIN  loss dict:  {'classification_loss': 0.9572078335285187}
2025-01-19 02:28:16,037 [INFO] Step[800/2713]: training loss : 0.9665372407436371 TRAIN  loss dict:  {'classification_loss': 0.9665372407436371}
2025-01-19 02:28:31,043 [INFO] Step[850/2713]: training loss : 0.9551679384708405 TRAIN  loss dict:  {'classification_loss': 0.9551679384708405}
2025-01-19 02:28:46,117 [INFO] Step[900/2713]: training loss : 0.9559289944171906 TRAIN  loss dict:  {'classification_loss': 0.9559289944171906}
2025-01-19 02:29:01,156 [INFO] Step[950/2713]: training loss : 0.9572587788105011 TRAIN  loss dict:  {'classification_loss': 0.9572587788105011}
2025-01-19 02:29:16,247 [INFO] Step[1000/2713]: training loss : 0.9676343894004822 TRAIN  loss dict:  {'classification_loss': 0.9676343894004822}
2025-01-19 02:29:31,355 [INFO] Step[1050/2713]: training loss : 0.9709071087837219 TRAIN  loss dict:  {'classification_loss': 0.9709071087837219}
2025-01-19 02:29:46,420 [INFO] Step[1100/2713]: training loss : 0.9784449362754821 TRAIN  loss dict:  {'classification_loss': 0.9784449362754821}
2025-01-19 02:30:01,514 [INFO] Step[1150/2713]: training loss : 0.9556153595447541 TRAIN  loss dict:  {'classification_loss': 0.9556153595447541}
2025-01-19 02:30:16,563 [INFO] Step[1200/2713]: training loss : 0.9579998779296875 TRAIN  loss dict:  {'classification_loss': 0.9579998779296875}
2025-01-19 02:30:31,588 [INFO] Step[1250/2713]: training loss : 0.9597177219390869 TRAIN  loss dict:  {'classification_loss': 0.9597177219390869}
2025-01-19 02:30:46,661 [INFO] Step[1300/2713]: training loss : 0.9603460955619813 TRAIN  loss dict:  {'classification_loss': 0.9603460955619813}
2025-01-19 02:31:01,743 [INFO] Step[1350/2713]: training loss : 0.9583631336688996 TRAIN  loss dict:  {'classification_loss': 0.9583631336688996}
2025-01-19 02:31:16,795 [INFO] Step[1400/2713]: training loss : 0.9553064584732056 TRAIN  loss dict:  {'classification_loss': 0.9553064584732056}
2025-01-19 02:31:31,865 [INFO] Step[1450/2713]: training loss : 0.9580049836635589 TRAIN  loss dict:  {'classification_loss': 0.9580049836635589}
2025-01-19 02:31:46,886 [INFO] Step[1500/2713]: training loss : 0.9684395575523377 TRAIN  loss dict:  {'classification_loss': 0.9684395575523377}
2025-01-19 02:32:01,919 [INFO] Step[1550/2713]: training loss : 0.9652123367786407 TRAIN  loss dict:  {'classification_loss': 0.9652123367786407}
2025-01-19 02:32:16,959 [INFO] Step[1600/2713]: training loss : 0.9766682684421539 TRAIN  loss dict:  {'classification_loss': 0.9766682684421539}
2025-01-19 02:32:32,025 [INFO] Step[1650/2713]: training loss : 0.9624759113788605 TRAIN  loss dict:  {'classification_loss': 0.9624759113788605}
2025-01-19 02:32:47,098 [INFO] Step[1700/2713]: training loss : 0.9559558653831481 TRAIN  loss dict:  {'classification_loss': 0.9559558653831481}
2025-01-19 02:33:02,147 [INFO] Step[1750/2713]: training loss : 0.96512948513031 TRAIN  loss dict:  {'classification_loss': 0.96512948513031}
2025-01-19 02:33:17,152 [INFO] Step[1800/2713]: training loss : 0.9595144736766815 TRAIN  loss dict:  {'classification_loss': 0.9595144736766815}
2025-01-19 02:33:32,073 [INFO] Step[1850/2713]: training loss : 0.9672143828868865 TRAIN  loss dict:  {'classification_loss': 0.9672143828868865}
2025-01-19 02:33:47,004 [INFO] Step[1900/2713]: training loss : 0.9636817991733551 TRAIN  loss dict:  {'classification_loss': 0.9636817991733551}
2025-01-19 02:34:02,001 [INFO] Step[1950/2713]: training loss : 0.9538844680786133 TRAIN  loss dict:  {'classification_loss': 0.9538844680786133}
2025-01-19 02:34:16,956 [INFO] Step[2000/2713]: training loss : 0.9544811427593232 TRAIN  loss dict:  {'classification_loss': 0.9544811427593232}
2025-01-19 02:34:31,936 [INFO] Step[2050/2713]: training loss : 0.9590423917770385 TRAIN  loss dict:  {'classification_loss': 0.9590423917770385}
2025-01-19 02:34:46,936 [INFO] Step[2100/2713]: training loss : 0.9505717384815217 TRAIN  loss dict:  {'classification_loss': 0.9505717384815217}
2025-01-19 02:35:01,933 [INFO] Step[2150/2713]: training loss : 0.9599578928947449 TRAIN  loss dict:  {'classification_loss': 0.9599578928947449}
2025-01-19 02:35:16,889 [INFO] Step[2200/2713]: training loss : 0.9697246134281159 TRAIN  loss dict:  {'classification_loss': 0.9697246134281159}
2025-01-19 02:35:31,870 [INFO] Step[2250/2713]: training loss : 0.9593104922771454 TRAIN  loss dict:  {'classification_loss': 0.9593104922771454}
2025-01-19 02:35:46,872 [INFO] Step[2300/2713]: training loss : 0.9613424980640412 TRAIN  loss dict:  {'classification_loss': 0.9613424980640412}
2025-01-19 02:36:01,848 [INFO] Step[2350/2713]: training loss : 0.9600205385684967 TRAIN  loss dict:  {'classification_loss': 0.9600205385684967}
2025-01-19 02:36:16,788 [INFO] Step[2400/2713]: training loss : 0.9770434391498566 TRAIN  loss dict:  {'classification_loss': 0.9770434391498566}
2025-01-19 02:36:31,742 [INFO] Step[2450/2713]: training loss : 0.9747246479988099 TRAIN  loss dict:  {'classification_loss': 0.9747246479988099}
2025-01-19 02:36:46,711 [INFO] Step[2500/2713]: training loss : 0.9606570374965667 TRAIN  loss dict:  {'classification_loss': 0.9606570374965667}
2025-01-19 02:37:01,658 [INFO] Step[2550/2713]: training loss : 0.9691531419754028 TRAIN  loss dict:  {'classification_loss': 0.9691531419754028}
2025-01-19 02:37:16,587 [INFO] Step[2600/2713]: training loss : 0.9649103808403016 TRAIN  loss dict:  {'classification_loss': 0.9649103808403016}
2025-01-19 02:37:31,579 [INFO] Step[2650/2713]: training loss : 0.9606544351577759 TRAIN  loss dict:  {'classification_loss': 0.9606544351577759}
2025-01-19 02:37:46,552 [INFO] Step[2700/2713]: training loss : 0.9770491659641266 TRAIN  loss dict:  {'classification_loss': 0.9770491659641266}
2025-01-19 02:39:06,384 [INFO] Label accuracies statistics:
2025-01-19 02:39:06,384 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.5, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.5, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.5, 209: 0.5, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 1.0, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.25, 230: 0.75, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.5, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 1.0, 340: 0.25, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.25, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.25, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 02:39:06,386 [INFO] [47] TRAIN  loss: 0.9634794081834863 acc: 0.9982798869640005
2025-01-19 02:39:06,386 [INFO] [47] TRAIN  loss dict: {'classification_loss': 0.9634794081834863}
2025-01-19 02:39:06,387 [INFO] [47] VALIDATION loss: 1.7817907056638174 VALIDATION acc: 0.7968652037617555
2025-01-19 02:39:06,387 [INFO] [47] VALIDATION loss dict: {'classification_loss': 1.7817907056638174}
2025-01-19 02:39:06,387 [INFO] 
2025-01-19 02:39:27,050 [INFO] Step[50/2713]: training loss : 0.9563052153587341 TRAIN  loss dict:  {'classification_loss': 0.9563052153587341}
2025-01-19 02:39:41,992 [INFO] Step[100/2713]: training loss : 0.9640445458889008 TRAIN  loss dict:  {'classification_loss': 0.9640445458889008}
2025-01-19 02:39:56,939 [INFO] Step[150/2713]: training loss : 0.9670023477077484 TRAIN  loss dict:  {'classification_loss': 0.9670023477077484}
2025-01-19 02:40:12,011 [INFO] Step[200/2713]: training loss : 0.9572362720966339 TRAIN  loss dict:  {'classification_loss': 0.9572362720966339}
2025-01-19 02:40:27,006 [INFO] Step[250/2713]: training loss : 0.9566789972782135 TRAIN  loss dict:  {'classification_loss': 0.9566789972782135}
2025-01-19 02:40:42,029 [INFO] Step[300/2713]: training loss : 0.9786662232875823 TRAIN  loss dict:  {'classification_loss': 0.9786662232875823}
2025-01-19 02:40:56,962 [INFO] Step[350/2713]: training loss : 0.9600093531608581 TRAIN  loss dict:  {'classification_loss': 0.9600093531608581}
2025-01-19 02:41:11,915 [INFO] Step[400/2713]: training loss : 0.9588562405109405 TRAIN  loss dict:  {'classification_loss': 0.9588562405109405}
2025-01-19 02:41:26,842 [INFO] Step[450/2713]: training loss : 0.9605544972419738 TRAIN  loss dict:  {'classification_loss': 0.9605544972419738}
2025-01-19 02:41:41,778 [INFO] Step[500/2713]: training loss : 0.9656289517879486 TRAIN  loss dict:  {'classification_loss': 0.9656289517879486}
2025-01-19 02:41:56,810 [INFO] Step[550/2713]: training loss : 0.9620751917362214 TRAIN  loss dict:  {'classification_loss': 0.9620751917362214}
2025-01-19 02:42:11,741 [INFO] Step[600/2713]: training loss : 0.9899148058891296 TRAIN  loss dict:  {'classification_loss': 0.9899148058891296}
2025-01-19 02:42:26,722 [INFO] Step[650/2713]: training loss : 0.9569160115718841 TRAIN  loss dict:  {'classification_loss': 0.9569160115718841}
2025-01-19 02:42:41,649 [INFO] Step[700/2713]: training loss : 0.9559132587909699 TRAIN  loss dict:  {'classification_loss': 0.9559132587909699}
2025-01-19 02:42:56,718 [INFO] Step[750/2713]: training loss : 0.9547101795673371 TRAIN  loss dict:  {'classification_loss': 0.9547101795673371}
2025-01-19 02:43:11,662 [INFO] Step[800/2713]: training loss : 0.9605586075782776 TRAIN  loss dict:  {'classification_loss': 0.9605586075782776}
2025-01-19 02:43:26,648 [INFO] Step[850/2713]: training loss : 0.9604716789722443 TRAIN  loss dict:  {'classification_loss': 0.9604716789722443}
2025-01-19 02:43:41,652 [INFO] Step[900/2713]: training loss : 0.957741745710373 TRAIN  loss dict:  {'classification_loss': 0.957741745710373}
2025-01-19 02:43:56,610 [INFO] Step[950/2713]: training loss : 0.9529377496242524 TRAIN  loss dict:  {'classification_loss': 0.9529377496242524}
2025-01-19 02:44:11,570 [INFO] Step[1000/2713]: training loss : 0.9547500348091126 TRAIN  loss dict:  {'classification_loss': 0.9547500348091126}
2025-01-19 02:44:26,569 [INFO] Step[1050/2713]: training loss : 0.9530730819702149 TRAIN  loss dict:  {'classification_loss': 0.9530730819702149}
2025-01-19 02:44:41,486 [INFO] Step[1100/2713]: training loss : 0.958630850315094 TRAIN  loss dict:  {'classification_loss': 0.958630850315094}
2025-01-19 02:44:56,434 [INFO] Step[1150/2713]: training loss : 0.9502767884731292 TRAIN  loss dict:  {'classification_loss': 0.9502767884731292}
2025-01-19 02:45:11,493 [INFO] Step[1200/2713]: training loss : 0.9645866239070893 TRAIN  loss dict:  {'classification_loss': 0.9645866239070893}
2025-01-19 02:45:26,483 [INFO] Step[1250/2713]: training loss : 0.95603165268898 TRAIN  loss dict:  {'classification_loss': 0.95603165268898}
2025-01-19 02:45:41,447 [INFO] Step[1300/2713]: training loss : 0.9608367705345153 TRAIN  loss dict:  {'classification_loss': 0.9608367705345153}
2025-01-19 02:45:56,452 [INFO] Step[1350/2713]: training loss : 0.954613401889801 TRAIN  loss dict:  {'classification_loss': 0.954613401889801}
2025-01-19 02:46:11,420 [INFO] Step[1400/2713]: training loss : 0.9724627423286438 TRAIN  loss dict:  {'classification_loss': 0.9724627423286438}
2025-01-19 02:46:26,390 [INFO] Step[1450/2713]: training loss : 0.9618873405456543 TRAIN  loss dict:  {'classification_loss': 0.9618873405456543}
2025-01-19 02:46:41,370 [INFO] Step[1500/2713]: training loss : 0.9594236695766449 TRAIN  loss dict:  {'classification_loss': 0.9594236695766449}
2025-01-19 02:46:56,389 [INFO] Step[1550/2713]: training loss : 0.9569908213615418 TRAIN  loss dict:  {'classification_loss': 0.9569908213615418}
2025-01-19 02:47:11,357 [INFO] Step[1600/2713]: training loss : 0.9618004870414734 TRAIN  loss dict:  {'classification_loss': 0.9618004870414734}
2025-01-19 02:47:26,353 [INFO] Step[1650/2713]: training loss : 0.9559505486488342 TRAIN  loss dict:  {'classification_loss': 0.9559505486488342}
2025-01-19 02:47:41,347 [INFO] Step[1700/2713]: training loss : 0.9552229642868042 TRAIN  loss dict:  {'classification_loss': 0.9552229642868042}
2025-01-19 02:47:56,322 [INFO] Step[1750/2713]: training loss : 0.9689450716972351 TRAIN  loss dict:  {'classification_loss': 0.9689450716972351}
2025-01-19 02:48:11,264 [INFO] Step[1800/2713]: training loss : 0.9591940808296203 TRAIN  loss dict:  {'classification_loss': 0.9591940808296203}
2025-01-19 02:48:26,210 [INFO] Step[1850/2713]: training loss : 0.9611892795562744 TRAIN  loss dict:  {'classification_loss': 0.9611892795562744}
2025-01-19 02:48:41,193 [INFO] Step[1900/2713]: training loss : 0.9593789398670196 TRAIN  loss dict:  {'classification_loss': 0.9593789398670196}
2025-01-19 02:48:56,211 [INFO] Step[1950/2713]: training loss : 0.9624939286708831 TRAIN  loss dict:  {'classification_loss': 0.9624939286708831}
2025-01-19 02:49:11,131 [INFO] Step[2000/2713]: training loss : 0.9556954264640808 TRAIN  loss dict:  {'classification_loss': 0.9556954264640808}
2025-01-19 02:49:26,146 [INFO] Step[2050/2713]: training loss : 0.9555314087867737 TRAIN  loss dict:  {'classification_loss': 0.9555314087867737}
2025-01-19 02:49:41,101 [INFO] Step[2100/2713]: training loss : 0.982800748348236 TRAIN  loss dict:  {'classification_loss': 0.982800748348236}
2025-01-19 02:49:56,054 [INFO] Step[2150/2713]: training loss : 0.9603221344947815 TRAIN  loss dict:  {'classification_loss': 0.9603221344947815}
2025-01-19 02:50:11,017 [INFO] Step[2200/2713]: training loss : 0.9622385382652283 TRAIN  loss dict:  {'classification_loss': 0.9622385382652283}
2025-01-19 02:50:26,015 [INFO] Step[2250/2713]: training loss : 0.9501217651367188 TRAIN  loss dict:  {'classification_loss': 0.9501217651367188}
2025-01-19 02:50:40,996 [INFO] Step[2300/2713]: training loss : 0.9879082560539245 TRAIN  loss dict:  {'classification_loss': 0.9879082560539245}
2025-01-19 02:50:55,973 [INFO] Step[2350/2713]: training loss : 0.9546983194351196 TRAIN  loss dict:  {'classification_loss': 0.9546983194351196}
2025-01-19 02:51:10,967 [INFO] Step[2400/2713]: training loss : 0.962935711145401 TRAIN  loss dict:  {'classification_loss': 0.962935711145401}
2025-01-19 02:51:25,984 [INFO] Step[2450/2713]: training loss : 0.9529487776756287 TRAIN  loss dict:  {'classification_loss': 0.9529487776756287}
2025-01-19 02:51:40,959 [INFO] Step[2500/2713]: training loss : 0.9575755548477173 TRAIN  loss dict:  {'classification_loss': 0.9575755548477173}
2025-01-19 02:51:55,904 [INFO] Step[2550/2713]: training loss : 0.9634792709350586 TRAIN  loss dict:  {'classification_loss': 0.9634792709350586}
2025-01-19 02:52:10,913 [INFO] Step[2600/2713]: training loss : 0.9745587742328644 TRAIN  loss dict:  {'classification_loss': 0.9745587742328644}
2025-01-19 02:52:25,891 [INFO] Step[2650/2713]: training loss : 0.9617662847042083 TRAIN  loss dict:  {'classification_loss': 0.9617662847042083}
2025-01-19 02:52:40,824 [INFO] Step[2700/2713]: training loss : 0.9696216142177582 TRAIN  loss dict:  {'classification_loss': 0.9696216142177582}
2025-01-19 02:54:00,508 [INFO] Label accuracies statistics:
2025-01-19 02:54:00,508 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.5, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 1.0, 215: 1.0, 216: 0.0, 217: 0.25, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.5, 288: 0.5, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 0.75, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.5, 342: 0.75, 343: 1.0, 344: 1.0, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.5, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 02:54:00,510 [INFO] [48] TRAIN  loss: 0.9613934093149131 acc: 0.9984027521808576
2025-01-19 02:54:00,510 [INFO] [48] TRAIN  loss dict: {'classification_loss': 0.9613934093149131}
2025-01-19 02:54:00,510 [INFO] [48] VALIDATION loss: 1.7741529072137703 VALIDATION acc: 0.7905956112852665
2025-01-19 02:54:00,510 [INFO] [48] VALIDATION loss dict: {'classification_loss': 1.7741529072137703}
2025-01-19 02:54:00,510 [INFO] 
2025-01-19 02:54:20,867 [INFO] Step[50/2713]: training loss : 0.9560403168201447 TRAIN  loss dict:  {'classification_loss': 0.9560403168201447}
2025-01-19 02:54:35,869 [INFO] Step[100/2713]: training loss : 0.9628053116798401 TRAIN  loss dict:  {'classification_loss': 0.9628053116798401}
2025-01-19 02:54:50,890 [INFO] Step[150/2713]: training loss : 0.9635434103012085 TRAIN  loss dict:  {'classification_loss': 0.9635434103012085}
2025-01-19 02:55:05,968 [INFO] Step[200/2713]: training loss : 0.963773854970932 TRAIN  loss dict:  {'classification_loss': 0.963773854970932}
2025-01-19 02:55:21,059 [INFO] Step[250/2713]: training loss : 0.9656941032409668 TRAIN  loss dict:  {'classification_loss': 0.9656941032409668}
2025-01-19 02:55:36,126 [INFO] Step[300/2713]: training loss : 0.9561423981189727 TRAIN  loss dict:  {'classification_loss': 0.9561423981189727}
2025-01-19 02:55:51,167 [INFO] Step[350/2713]: training loss : 0.9627874755859375 TRAIN  loss dict:  {'classification_loss': 0.9627874755859375}
2025-01-19 02:56:06,240 [INFO] Step[400/2713]: training loss : 0.9555807304382324 TRAIN  loss dict:  {'classification_loss': 0.9555807304382324}
2025-01-19 02:56:21,356 [INFO] Step[450/2713]: training loss : 0.9615019106864929 TRAIN  loss dict:  {'classification_loss': 0.9615019106864929}
2025-01-19 02:56:36,453 [INFO] Step[500/2713]: training loss : 0.9551683068275452 TRAIN  loss dict:  {'classification_loss': 0.9551683068275452}
2025-01-19 02:56:51,552 [INFO] Step[550/2713]: training loss : 0.9607527434825898 TRAIN  loss dict:  {'classification_loss': 0.9607527434825898}
2025-01-19 02:57:06,651 [INFO] Step[600/2713]: training loss : 0.9613544011116028 TRAIN  loss dict:  {'classification_loss': 0.9613544011116028}
2025-01-19 02:57:21,694 [INFO] Step[650/2713]: training loss : 0.9641705596446991 TRAIN  loss dict:  {'classification_loss': 0.9641705596446991}
2025-01-19 02:57:36,804 [INFO] Step[700/2713]: training loss : 0.955058856010437 TRAIN  loss dict:  {'classification_loss': 0.955058856010437}
2025-01-19 02:57:51,896 [INFO] Step[750/2713]: training loss : 0.9606279242038727 TRAIN  loss dict:  {'classification_loss': 0.9606279242038727}
2025-01-19 02:58:06,991 [INFO] Step[800/2713]: training loss : 0.9596743035316467 TRAIN  loss dict:  {'classification_loss': 0.9596743035316467}
2025-01-19 02:58:22,107 [INFO] Step[850/2713]: training loss : 0.9549940419197083 TRAIN  loss dict:  {'classification_loss': 0.9549940419197083}
2025-01-19 02:58:37,177 [INFO] Step[900/2713]: training loss : 0.9680539405345917 TRAIN  loss dict:  {'classification_loss': 0.9680539405345917}
2025-01-19 02:58:52,249 [INFO] Step[950/2713]: training loss : 0.9610151124000549 TRAIN  loss dict:  {'classification_loss': 0.9610151124000549}
2025-01-19 02:59:07,328 [INFO] Step[1000/2713]: training loss : 0.9591030824184418 TRAIN  loss dict:  {'classification_loss': 0.9591030824184418}
2025-01-19 02:59:22,392 [INFO] Step[1050/2713]: training loss : 0.9948328912258149 TRAIN  loss dict:  {'classification_loss': 0.9948328912258149}
2025-01-19 02:59:37,487 [INFO] Step[1100/2713]: training loss : 0.9580201065540314 TRAIN  loss dict:  {'classification_loss': 0.9580201065540314}
2025-01-19 02:59:52,579 [INFO] Step[1150/2713]: training loss : 0.9514922785758972 TRAIN  loss dict:  {'classification_loss': 0.9514922785758972}
2025-01-19 03:00:07,672 [INFO] Step[1200/2713]: training loss : 0.9530248987674713 TRAIN  loss dict:  {'classification_loss': 0.9530248987674713}
2025-01-19 03:00:22,792 [INFO] Step[1250/2713]: training loss : 0.9720894110202789 TRAIN  loss dict:  {'classification_loss': 0.9720894110202789}
2025-01-19 03:00:37,872 [INFO] Step[1300/2713]: training loss : 0.9570550310611725 TRAIN  loss dict:  {'classification_loss': 0.9570550310611725}
2025-01-19 03:00:52,935 [INFO] Step[1350/2713]: training loss : 0.9639955914020538 TRAIN  loss dict:  {'classification_loss': 0.9639955914020538}
2025-01-19 03:01:07,971 [INFO] Step[1400/2713]: training loss : 0.960975911617279 TRAIN  loss dict:  {'classification_loss': 0.960975911617279}
2025-01-19 03:01:23,053 [INFO] Step[1450/2713]: training loss : 0.9558987689018249 TRAIN  loss dict:  {'classification_loss': 0.9558987689018249}
2025-01-19 03:01:38,131 [INFO] Step[1500/2713]: training loss : 0.9568973779678345 TRAIN  loss dict:  {'classification_loss': 0.9568973779678345}
2025-01-19 03:01:53,249 [INFO] Step[1550/2713]: training loss : 0.969569878578186 TRAIN  loss dict:  {'classification_loss': 0.969569878578186}
2025-01-19 03:02:08,363 [INFO] Step[1600/2713]: training loss : 0.9595910060405731 TRAIN  loss dict:  {'classification_loss': 0.9595910060405731}
2025-01-19 03:02:23,452 [INFO] Step[1650/2713]: training loss : 0.9506852471828461 TRAIN  loss dict:  {'classification_loss': 0.9506852471828461}
2025-01-19 03:02:38,542 [INFO] Step[1700/2713]: training loss : 0.9563624227046966 TRAIN  loss dict:  {'classification_loss': 0.9563624227046966}
2025-01-19 03:02:53,604 [INFO] Step[1750/2713]: training loss : 0.9556979751586914 TRAIN  loss dict:  {'classification_loss': 0.9556979751586914}
2025-01-19 03:03:08,704 [INFO] Step[1800/2713]: training loss : 0.9534588921070098 TRAIN  loss dict:  {'classification_loss': 0.9534588921070098}
2025-01-19 03:03:23,799 [INFO] Step[1850/2713]: training loss : 0.9489108324050903 TRAIN  loss dict:  {'classification_loss': 0.9489108324050903}
2025-01-19 03:03:38,914 [INFO] Step[1900/2713]: training loss : 0.952813845872879 TRAIN  loss dict:  {'classification_loss': 0.952813845872879}
2025-01-19 03:03:54,003 [INFO] Step[1950/2713]: training loss : 0.9587426555156707 TRAIN  loss dict:  {'classification_loss': 0.9587426555156707}
2025-01-19 03:04:09,099 [INFO] Step[2000/2713]: training loss : 0.9842619466781616 TRAIN  loss dict:  {'classification_loss': 0.9842619466781616}
2025-01-19 03:04:24,172 [INFO] Step[2050/2713]: training loss : 0.950249445438385 TRAIN  loss dict:  {'classification_loss': 0.950249445438385}
2025-01-19 03:04:39,241 [INFO] Step[2100/2713]: training loss : 0.9548190760612488 TRAIN  loss dict:  {'classification_loss': 0.9548190760612488}
2025-01-19 03:04:54,332 [INFO] Step[2150/2713]: training loss : 0.9601139450073242 TRAIN  loss dict:  {'classification_loss': 0.9601139450073242}
2025-01-19 03:05:09,409 [INFO] Step[2200/2713]: training loss : 0.9622959685325623 TRAIN  loss dict:  {'classification_loss': 0.9622959685325623}
2025-01-19 03:05:24,494 [INFO] Step[2250/2713]: training loss : 0.9616926681995391 TRAIN  loss dict:  {'classification_loss': 0.9616926681995391}
2025-01-19 03:05:39,587 [INFO] Step[2300/2713]: training loss : 0.9551083374023438 TRAIN  loss dict:  {'classification_loss': 0.9551083374023438}
2025-01-19 03:05:54,743 [INFO] Step[2350/2713]: training loss : 0.9581155896186828 TRAIN  loss dict:  {'classification_loss': 0.9581155896186828}
2025-01-19 03:06:09,863 [INFO] Step[2400/2713]: training loss : 0.9512553989887238 TRAIN  loss dict:  {'classification_loss': 0.9512553989887238}
2025-01-19 03:06:24,960 [INFO] Step[2450/2713]: training loss : 0.9575189197063446 TRAIN  loss dict:  {'classification_loss': 0.9575189197063446}
2025-01-19 03:06:40,024 [INFO] Step[2500/2713]: training loss : 0.9852419364452362 TRAIN  loss dict:  {'classification_loss': 0.9852419364452362}
2025-01-19 03:06:55,085 [INFO] Step[2550/2713]: training loss : 0.9696779990196228 TRAIN  loss dict:  {'classification_loss': 0.9696779990196228}
2025-01-19 03:07:10,171 [INFO] Step[2600/2713]: training loss : 0.9698601412773132 TRAIN  loss dict:  {'classification_loss': 0.9698601412773132}
2025-01-19 03:07:25,296 [INFO] Step[2650/2713]: training loss : 0.9578179168701172 TRAIN  loss dict:  {'classification_loss': 0.9578179168701172}
2025-01-19 03:07:40,407 [INFO] Step[2700/2713]: training loss : 0.9657522881031037 TRAIN  loss dict:  {'classification_loss': 0.9657522881031037}
2025-01-19 03:09:00,525 [INFO] Label accuracies statistics:
2025-01-19 03:09:00,525 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.5, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.25, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.5, 245: 0.5, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.5, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 03:09:00,527 [INFO] [49] TRAIN  loss: 0.9607200258735181 acc: 0.9984027521808576
2025-01-19 03:09:00,527 [INFO] [49] TRAIN  loss dict: {'classification_loss': 0.9607200258735181}
2025-01-19 03:09:00,527 [INFO] [49] VALIDATION loss: 1.7707063585967946 VALIDATION acc: 0.7943573667711599
2025-01-19 03:09:00,527 [INFO] [49] VALIDATION loss dict: {'classification_loss': 1.7707063585967946}
2025-01-19 03:09:00,527 [INFO] 
2025-01-19 03:09:21,164 [INFO] Step[50/2713]: training loss : 0.9550669765472413 TRAIN  loss dict:  {'classification_loss': 0.9550669765472413}
2025-01-19 03:09:36,278 [INFO] Step[100/2713]: training loss : 0.9519247269630432 TRAIN  loss dict:  {'classification_loss': 0.9519247269630432}
2025-01-19 03:09:51,402 [INFO] Step[150/2713]: training loss : 0.9551714622974395 TRAIN  loss dict:  {'classification_loss': 0.9551714622974395}
2025-01-19 03:10:06,489 [INFO] Step[200/2713]: training loss : 0.9503849017620086 TRAIN  loss dict:  {'classification_loss': 0.9503849017620086}
2025-01-19 03:10:21,574 [INFO] Step[250/2713]: training loss : 0.952163257598877 TRAIN  loss dict:  {'classification_loss': 0.952163257598877}
2025-01-19 03:10:36,654 [INFO] Step[300/2713]: training loss : 0.9538680994510651 TRAIN  loss dict:  {'classification_loss': 0.9538680994510651}
2025-01-19 03:10:51,776 [INFO] Step[350/2713]: training loss : 0.9714817142486573 TRAIN  loss dict:  {'classification_loss': 0.9714817142486573}
2025-01-19 03:11:06,842 [INFO] Step[400/2713]: training loss : 0.9581096339225769 TRAIN  loss dict:  {'classification_loss': 0.9581096339225769}
2025-01-19 03:11:21,909 [INFO] Step[450/2713]: training loss : 0.9537337958812714 TRAIN  loss dict:  {'classification_loss': 0.9537337958812714}
2025-01-19 03:11:37,006 [INFO] Step[500/2713]: training loss : 0.9563186383247375 TRAIN  loss dict:  {'classification_loss': 0.9563186383247375}
2025-01-19 03:11:52,073 [INFO] Step[550/2713]: training loss : 0.9550551974773407 TRAIN  loss dict:  {'classification_loss': 0.9550551974773407}
2025-01-19 03:12:07,128 [INFO] Step[600/2713]: training loss : 0.9587127554416657 TRAIN  loss dict:  {'classification_loss': 0.9587127554416657}
2025-01-19 03:12:22,212 [INFO] Step[650/2713]: training loss : 0.9605017721652984 TRAIN  loss dict:  {'classification_loss': 0.9605017721652984}
2025-01-19 03:12:37,270 [INFO] Step[700/2713]: training loss : 0.966474118232727 TRAIN  loss dict:  {'classification_loss': 0.966474118232727}
2025-01-19 03:12:52,391 [INFO] Step[750/2713]: training loss : 0.9532120656967163 TRAIN  loss dict:  {'classification_loss': 0.9532120656967163}
2025-01-19 03:13:07,487 [INFO] Step[800/2713]: training loss : 0.9690062963962555 TRAIN  loss dict:  {'classification_loss': 0.9690062963962555}
2025-01-19 03:13:22,535 [INFO] Step[850/2713]: training loss : 0.9510642790794372 TRAIN  loss dict:  {'classification_loss': 0.9510642790794372}
2025-01-19 03:13:37,612 [INFO] Step[900/2713]: training loss : 0.9544147729873658 TRAIN  loss dict:  {'classification_loss': 0.9544147729873658}
2025-01-19 03:13:52,696 [INFO] Step[950/2713]: training loss : 0.9545389878749847 TRAIN  loss dict:  {'classification_loss': 0.9545389878749847}
2025-01-19 03:14:07,759 [INFO] Step[1000/2713]: training loss : 0.955364590883255 TRAIN  loss dict:  {'classification_loss': 0.955364590883255}
2025-01-19 03:14:22,864 [INFO] Step[1050/2713]: training loss : 0.9560527896881104 TRAIN  loss dict:  {'classification_loss': 0.9560527896881104}
2025-01-19 03:14:37,968 [INFO] Step[1100/2713]: training loss : 0.955855758190155 TRAIN  loss dict:  {'classification_loss': 0.955855758190155}
2025-01-19 03:14:53,073 [INFO] Step[1150/2713]: training loss : 0.9588379621505737 TRAIN  loss dict:  {'classification_loss': 0.9588379621505737}
2025-01-19 03:15:08,170 [INFO] Step[1200/2713]: training loss : 0.9742569732666015 TRAIN  loss dict:  {'classification_loss': 0.9742569732666015}
2025-01-19 03:15:23,249 [INFO] Step[1250/2713]: training loss : 0.9544153022766113 TRAIN  loss dict:  {'classification_loss': 0.9544153022766113}
2025-01-19 03:15:38,328 [INFO] Step[1300/2713]: training loss : 0.9530750751495362 TRAIN  loss dict:  {'classification_loss': 0.9530750751495362}
2025-01-19 03:15:53,386 [INFO] Step[1350/2713]: training loss : 0.9592304491996765 TRAIN  loss dict:  {'classification_loss': 0.9592304491996765}
2025-01-19 03:16:08,482 [INFO] Step[1400/2713]: training loss : 0.9544997775554657 TRAIN  loss dict:  {'classification_loss': 0.9544997775554657}
2025-01-19 03:16:23,598 [INFO] Step[1450/2713]: training loss : 0.9597640645503998 TRAIN  loss dict:  {'classification_loss': 0.9597640645503998}
2025-01-19 03:16:38,692 [INFO] Step[1500/2713]: training loss : 0.9559840106964111 TRAIN  loss dict:  {'classification_loss': 0.9559840106964111}
2025-01-19 03:16:53,795 [INFO] Step[1550/2713]: training loss : 0.9661904203891755 TRAIN  loss dict:  {'classification_loss': 0.9661904203891755}
2025-01-19 03:17:08,863 [INFO] Step[1600/2713]: training loss : 0.9636991024017334 TRAIN  loss dict:  {'classification_loss': 0.9636991024017334}
2025-01-19 03:17:23,960 [INFO] Step[1650/2713]: training loss : 0.9667266416549682 TRAIN  loss dict:  {'classification_loss': 0.9667266416549682}
2025-01-19 03:17:39,056 [INFO] Step[1700/2713]: training loss : 0.9552011227607727 TRAIN  loss dict:  {'classification_loss': 0.9552011227607727}
2025-01-19 03:17:54,175 [INFO] Step[1750/2713]: training loss : 0.9579377841949462 TRAIN  loss dict:  {'classification_loss': 0.9579377841949462}
2025-01-19 03:18:09,260 [INFO] Step[1800/2713]: training loss : 0.9561375629901886 TRAIN  loss dict:  {'classification_loss': 0.9561375629901886}
2025-01-19 03:18:24,350 [INFO] Step[1850/2713]: training loss : 0.9502570974826813 TRAIN  loss dict:  {'classification_loss': 0.9502570974826813}
2025-01-19 03:18:39,430 [INFO] Step[1900/2713]: training loss : 0.9585819637775421 TRAIN  loss dict:  {'classification_loss': 0.9585819637775421}
2025-01-19 03:18:54,532 [INFO] Step[1950/2713]: training loss : 0.9577260494232178 TRAIN  loss dict:  {'classification_loss': 0.9577260494232178}
2025-01-19 03:19:09,649 [INFO] Step[2000/2713]: training loss : 0.9513120996952057 TRAIN  loss dict:  {'classification_loss': 0.9513120996952057}
2025-01-19 03:19:24,740 [INFO] Step[2050/2713]: training loss : 0.9624965238571167 TRAIN  loss dict:  {'classification_loss': 0.9624965238571167}
2025-01-19 03:19:39,846 [INFO] Step[2100/2713]: training loss : 0.9626345694065094 TRAIN  loss dict:  {'classification_loss': 0.9626345694065094}
2025-01-19 03:19:54,961 [INFO] Step[2150/2713]: training loss : 0.9538859248161315 TRAIN  loss dict:  {'classification_loss': 0.9538859248161315}
2025-01-19 03:20:10,048 [INFO] Step[2200/2713]: training loss : 0.9776153540611268 TRAIN  loss dict:  {'classification_loss': 0.9776153540611268}
2025-01-19 03:20:25,150 [INFO] Step[2250/2713]: training loss : 0.9614262211322785 TRAIN  loss dict:  {'classification_loss': 0.9614262211322785}
2025-01-19 03:20:40,225 [INFO] Step[2300/2713]: training loss : 0.9651909828186035 TRAIN  loss dict:  {'classification_loss': 0.9651909828186035}
2025-01-19 03:20:55,362 [INFO] Step[2350/2713]: training loss : 0.955352441072464 TRAIN  loss dict:  {'classification_loss': 0.955352441072464}
2025-01-19 03:21:10,486 [INFO] Step[2400/2713]: training loss : 0.9517074728012085 TRAIN  loss dict:  {'classification_loss': 0.9517074728012085}
2025-01-19 03:21:25,607 [INFO] Step[2450/2713]: training loss : 0.964525681734085 TRAIN  loss dict:  {'classification_loss': 0.964525681734085}
2025-01-19 03:21:40,710 [INFO] Step[2500/2713]: training loss : 0.9532488083839417 TRAIN  loss dict:  {'classification_loss': 0.9532488083839417}
2025-01-19 03:21:55,815 [INFO] Step[2550/2713]: training loss : 0.9532161784172059 TRAIN  loss dict:  {'classification_loss': 0.9532161784172059}
2025-01-19 03:22:10,923 [INFO] Step[2600/2713]: training loss : 0.9661668705940246 TRAIN  loss dict:  {'classification_loss': 0.9661668705940246}
2025-01-19 03:22:26,009 [INFO] Step[2650/2713]: training loss : 0.9776336276531219 TRAIN  loss dict:  {'classification_loss': 0.9776336276531219}
2025-01-19 03:22:41,046 [INFO] Step[2700/2713]: training loss : 0.9548824429512024 TRAIN  loss dict:  {'classification_loss': 0.9548824429512024}
2025-01-19 03:24:01,711 [INFO] Label accuracies statistics:
2025-01-19 03:24:01,712 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.75, 144: 0.75, 145: 0.5, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 1.0, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.5, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.5, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 0.75, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.5, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 03:24:01,713 [INFO] [50] TRAIN  loss: 0.9585723881347128 acc: 0.9988942130482861
2025-01-19 03:24:01,713 [INFO] [50] TRAIN  loss dict: {'classification_loss': 0.9585723881347128}
2025-01-19 03:24:01,714 [INFO] [50] VALIDATION loss: 1.7568838454054712 VALIDATION acc: 0.7855799373040753
2025-01-19 03:24:01,714 [INFO] [50] VALIDATION loss dict: {'classification_loss': 1.7568838454054712}
2025-01-19 03:24:01,714 [INFO] 
2025-01-19 03:24:21,733 [INFO] Step[50/2713]: training loss : 0.9561619174480438 TRAIN  loss dict:  {'classification_loss': 0.9561619174480438}
2025-01-19 03:24:36,679 [INFO] Step[100/2713]: training loss : 0.9578058683872223 TRAIN  loss dict:  {'classification_loss': 0.9578058683872223}
2025-01-19 03:24:51,671 [INFO] Step[150/2713]: training loss : 0.9496774470806122 TRAIN  loss dict:  {'classification_loss': 0.9496774470806122}
2025-01-19 03:25:06,639 [INFO] Step[200/2713]: training loss : 0.9601561903953553 TRAIN  loss dict:  {'classification_loss': 0.9601561903953553}
2025-01-19 03:25:21,595 [INFO] Step[250/2713]: training loss : 0.9576037001609802 TRAIN  loss dict:  {'classification_loss': 0.9576037001609802}
2025-01-19 03:25:36,563 [INFO] Step[300/2713]: training loss : 0.9697275173664093 TRAIN  loss dict:  {'classification_loss': 0.9697275173664093}
2025-01-19 03:25:51,489 [INFO] Step[350/2713]: training loss : 0.9541605484485626 TRAIN  loss dict:  {'classification_loss': 0.9541605484485626}
2025-01-19 03:26:06,436 [INFO] Step[400/2713]: training loss : 0.954294489622116 TRAIN  loss dict:  {'classification_loss': 0.954294489622116}
2025-01-19 03:26:21,382 [INFO] Step[450/2713]: training loss : 0.9541283893585205 TRAIN  loss dict:  {'classification_loss': 0.9541283893585205}
2025-01-19 03:26:36,369 [INFO] Step[500/2713]: training loss : 0.9520872282981873 TRAIN  loss dict:  {'classification_loss': 0.9520872282981873}
2025-01-19 03:26:51,367 [INFO] Step[550/2713]: training loss : 0.9519439327716828 TRAIN  loss dict:  {'classification_loss': 0.9519439327716828}
2025-01-19 03:27:06,349 [INFO] Step[600/2713]: training loss : 0.955331951379776 TRAIN  loss dict:  {'classification_loss': 0.955331951379776}
2025-01-19 03:27:21,382 [INFO] Step[650/2713]: training loss : 0.9584391820430755 TRAIN  loss dict:  {'classification_loss': 0.9584391820430755}
2025-01-19 03:27:36,354 [INFO] Step[700/2713]: training loss : 0.9526860642433167 TRAIN  loss dict:  {'classification_loss': 0.9526860642433167}
2025-01-19 03:27:51,334 [INFO] Step[750/2713]: training loss : 0.9503226888179779 TRAIN  loss dict:  {'classification_loss': 0.9503226888179779}
2025-01-19 03:28:06,306 [INFO] Step[800/2713]: training loss : 0.9566881728172302 TRAIN  loss dict:  {'classification_loss': 0.9566881728172302}
2025-01-19 03:28:21,272 [INFO] Step[850/2713]: training loss : 0.9640576159954071 TRAIN  loss dict:  {'classification_loss': 0.9640576159954071}
2025-01-19 03:28:36,187 [INFO] Step[900/2713]: training loss : 0.9603385961055756 TRAIN  loss dict:  {'classification_loss': 0.9603385961055756}
2025-01-19 03:28:51,116 [INFO] Step[950/2713]: training loss : 0.9625310850143433 TRAIN  loss dict:  {'classification_loss': 0.9625310850143433}
2025-01-19 03:29:06,064 [INFO] Step[1000/2713]: training loss : 0.954456000328064 TRAIN  loss dict:  {'classification_loss': 0.954456000328064}
2025-01-19 03:29:21,042 [INFO] Step[1050/2713]: training loss : 0.9548422133922577 TRAIN  loss dict:  {'classification_loss': 0.9548422133922577}
2025-01-19 03:29:36,015 [INFO] Step[1100/2713]: training loss : 0.9615372335910797 TRAIN  loss dict:  {'classification_loss': 0.9615372335910797}
2025-01-19 03:29:50,975 [INFO] Step[1150/2713]: training loss : 0.9536785840988159 TRAIN  loss dict:  {'classification_loss': 0.9536785840988159}
2025-01-19 03:30:05,971 [INFO] Step[1200/2713]: training loss : 0.9547793602943421 TRAIN  loss dict:  {'classification_loss': 0.9547793602943421}
2025-01-19 03:30:20,948 [INFO] Step[1250/2713]: training loss : 0.9658924460411071 TRAIN  loss dict:  {'classification_loss': 0.9658924460411071}
2025-01-19 03:30:35,842 [INFO] Step[1300/2713]: training loss : 0.9475225675106048 TRAIN  loss dict:  {'classification_loss': 0.9475225675106048}
2025-01-19 03:30:50,749 [INFO] Step[1350/2713]: training loss : 0.955992056131363 TRAIN  loss dict:  {'classification_loss': 0.955992056131363}
2025-01-19 03:31:05,690 [INFO] Step[1400/2713]: training loss : 0.9567201888561249 TRAIN  loss dict:  {'classification_loss': 0.9567201888561249}
2025-01-19 03:31:20,667 [INFO] Step[1450/2713]: training loss : 0.9476126980781555 TRAIN  loss dict:  {'classification_loss': 0.9476126980781555}
2025-01-19 03:31:35,553 [INFO] Step[1500/2713]: training loss : 0.9599035727977753 TRAIN  loss dict:  {'classification_loss': 0.9599035727977753}
2025-01-19 03:31:50,474 [INFO] Step[1550/2713]: training loss : 0.9755367195606232 TRAIN  loss dict:  {'classification_loss': 0.9755367195606232}
2025-01-19 03:32:05,401 [INFO] Step[1600/2713]: training loss : 0.9696324348449707 TRAIN  loss dict:  {'classification_loss': 0.9696324348449707}
2025-01-19 03:32:20,280 [INFO] Step[1650/2713]: training loss : 0.9564150238037109 TRAIN  loss dict:  {'classification_loss': 0.9564150238037109}
2025-01-19 03:32:35,206 [INFO] Step[1700/2713]: training loss : 0.9527105057239532 TRAIN  loss dict:  {'classification_loss': 0.9527105057239532}
2025-01-19 03:32:50,132 [INFO] Step[1750/2713]: training loss : 0.960832839012146 TRAIN  loss dict:  {'classification_loss': 0.960832839012146}
2025-01-19 03:33:05,083 [INFO] Step[1800/2713]: training loss : 0.9606500315666199 TRAIN  loss dict:  {'classification_loss': 0.9606500315666199}
2025-01-19 03:33:19,943 [INFO] Step[1850/2713]: training loss : 0.9551243722438812 TRAIN  loss dict:  {'classification_loss': 0.9551243722438812}
2025-01-19 03:33:34,866 [INFO] Step[1900/2713]: training loss : 0.9667518830299378 TRAIN  loss dict:  {'classification_loss': 0.9667518830299378}
2025-01-19 03:33:49,816 [INFO] Step[1950/2713]: training loss : 0.951142406463623 TRAIN  loss dict:  {'classification_loss': 0.951142406463623}
2025-01-19 03:34:04,776 [INFO] Step[2000/2713]: training loss : 0.9521778166294098 TRAIN  loss dict:  {'classification_loss': 0.9521778166294098}
2025-01-19 03:34:19,719 [INFO] Step[2050/2713]: training loss : 0.9623020815849305 TRAIN  loss dict:  {'classification_loss': 0.9623020815849305}
2025-01-19 03:34:34,633 [INFO] Step[2100/2713]: training loss : 0.9540961444377899 TRAIN  loss dict:  {'classification_loss': 0.9540961444377899}
2025-01-19 03:34:49,600 [INFO] Step[2150/2713]: training loss : 0.964800295829773 TRAIN  loss dict:  {'classification_loss': 0.964800295829773}
2025-01-19 03:35:04,540 [INFO] Step[2200/2713]: training loss : 0.9570213770866394 TRAIN  loss dict:  {'classification_loss': 0.9570213770866394}
2025-01-19 03:35:19,474 [INFO] Step[2250/2713]: training loss : 0.962768759727478 TRAIN  loss dict:  {'classification_loss': 0.962768759727478}
2025-01-19 03:35:34,432 [INFO] Step[2300/2713]: training loss : 0.9598749625682831 TRAIN  loss dict:  {'classification_loss': 0.9598749625682831}
2025-01-19 03:35:49,354 [INFO] Step[2350/2713]: training loss : 0.9752730715274811 TRAIN  loss dict:  {'classification_loss': 0.9752730715274811}
2025-01-19 03:36:04,250 [INFO] Step[2400/2713]: training loss : 0.9588319265842438 TRAIN  loss dict:  {'classification_loss': 0.9588319265842438}
2025-01-19 03:36:19,193 [INFO] Step[2450/2713]: training loss : 0.951826741695404 TRAIN  loss dict:  {'classification_loss': 0.951826741695404}
2025-01-19 03:36:34,156 [INFO] Step[2500/2713]: training loss : 0.9499690783023834 TRAIN  loss dict:  {'classification_loss': 0.9499690783023834}
2025-01-19 03:36:49,084 [INFO] Step[2550/2713]: training loss : 0.9618685257434845 TRAIN  loss dict:  {'classification_loss': 0.9618685257434845}
2025-01-19 03:37:03,982 [INFO] Step[2600/2713]: training loss : 0.9519865560531616 TRAIN  loss dict:  {'classification_loss': 0.9519865560531616}
2025-01-19 03:37:18,942 [INFO] Step[2650/2713]: training loss : 0.9524054527282715 TRAIN  loss dict:  {'classification_loss': 0.9524054527282715}
2025-01-19 03:37:33,846 [INFO] Step[2700/2713]: training loss : 0.9472251582145691 TRAIN  loss dict:  {'classification_loss': 0.9472251582145691}
2025-01-19 03:38:53,726 [INFO] Label accuracies statistics:
2025-01-19 03:38:53,726 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.5, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.5, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.5, 206: 0.5, 207: 1.0, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.5, 235: 1.0, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 03:38:53,729 [INFO] [51] TRAIN  loss: 0.9573915394038852 acc: 0.9985256173977147
2025-01-19 03:38:53,729 [INFO] [51] TRAIN  loss dict: {'classification_loss': 0.9573915394038852}
2025-01-19 03:38:53,729 [INFO] [51] VALIDATION loss: 1.7654415044121277 VALIDATION acc: 0.7956112852664576
2025-01-19 03:38:53,729 [INFO] [51] VALIDATION loss dict: {'classification_loss': 1.7654415044121277}
2025-01-19 03:38:53,729 [INFO] 
2025-01-19 03:39:14,190 [INFO] Step[50/2713]: training loss : 0.9525608730316162 TRAIN  loss dict:  {'classification_loss': 0.9525608730316162}
2025-01-19 03:39:29,097 [INFO] Step[100/2713]: training loss : 0.9470891737937928 TRAIN  loss dict:  {'classification_loss': 0.9470891737937928}
2025-01-19 03:39:43,992 [INFO] Step[150/2713]: training loss : 0.9516183984279633 TRAIN  loss dict:  {'classification_loss': 0.9516183984279633}
2025-01-19 03:39:58,965 [INFO] Step[200/2713]: training loss : 0.9545275771617889 TRAIN  loss dict:  {'classification_loss': 0.9545275771617889}
2025-01-19 03:40:13,896 [INFO] Step[250/2713]: training loss : 0.9637020909786225 TRAIN  loss dict:  {'classification_loss': 0.9637020909786225}
2025-01-19 03:40:28,774 [INFO] Step[300/2713]: training loss : 0.9506099665164948 TRAIN  loss dict:  {'classification_loss': 0.9506099665164948}
2025-01-19 03:40:43,693 [INFO] Step[350/2713]: training loss : 0.9738740754127503 TRAIN  loss dict:  {'classification_loss': 0.9738740754127503}
2025-01-19 03:40:58,603 [INFO] Step[400/2713]: training loss : 0.9526115322113037 TRAIN  loss dict:  {'classification_loss': 0.9526115322113037}
2025-01-19 03:41:13,587 [INFO] Step[450/2713]: training loss : 0.9543063151836395 TRAIN  loss dict:  {'classification_loss': 0.9543063151836395}
2025-01-19 03:41:28,515 [INFO] Step[500/2713]: training loss : 0.9495970833301545 TRAIN  loss dict:  {'classification_loss': 0.9495970833301545}
2025-01-19 03:41:43,493 [INFO] Step[550/2713]: training loss : 0.9551396787166595 TRAIN  loss dict:  {'classification_loss': 0.9551396787166595}
2025-01-19 03:41:58,421 [INFO] Step[600/2713]: training loss : 0.9477069842815399 TRAIN  loss dict:  {'classification_loss': 0.9477069842815399}
2025-01-19 03:42:13,365 [INFO] Step[650/2713]: training loss : 0.9543918907642365 TRAIN  loss dict:  {'classification_loss': 0.9543918907642365}
2025-01-19 03:42:28,273 [INFO] Step[700/2713]: training loss : 0.9525311267375947 TRAIN  loss dict:  {'classification_loss': 0.9525311267375947}
2025-01-19 03:42:43,143 [INFO] Step[750/2713]: training loss : 0.9506787812709808 TRAIN  loss dict:  {'classification_loss': 0.9506787812709808}
2025-01-19 03:42:58,086 [INFO] Step[800/2713]: training loss : 0.9516476547718048 TRAIN  loss dict:  {'classification_loss': 0.9516476547718048}
2025-01-19 03:43:13,039 [INFO] Step[850/2713]: training loss : 0.9504002916812897 TRAIN  loss dict:  {'classification_loss': 0.9504002916812897}
2025-01-19 03:43:27,954 [INFO] Step[900/2713]: training loss : 0.9523705053329468 TRAIN  loss dict:  {'classification_loss': 0.9523705053329468}
2025-01-19 03:43:42,902 [INFO] Step[950/2713]: training loss : 0.9602440166473388 TRAIN  loss dict:  {'classification_loss': 0.9602440166473388}
2025-01-19 03:43:57,833 [INFO] Step[1000/2713]: training loss : 0.9481546556949616 TRAIN  loss dict:  {'classification_loss': 0.9481546556949616}
2025-01-19 03:44:12,769 [INFO] Step[1050/2713]: training loss : 0.969166465997696 TRAIN  loss dict:  {'classification_loss': 0.969166465997696}
2025-01-19 03:44:27,652 [INFO] Step[1100/2713]: training loss : 0.9624639010429382 TRAIN  loss dict:  {'classification_loss': 0.9624639010429382}
2025-01-19 03:44:42,555 [INFO] Step[1150/2713]: training loss : 0.9538583922386169 TRAIN  loss dict:  {'classification_loss': 0.9538583922386169}
2025-01-19 03:44:57,488 [INFO] Step[1200/2713]: training loss : 0.956648794412613 TRAIN  loss dict:  {'classification_loss': 0.956648794412613}
2025-01-19 03:45:12,465 [INFO] Step[1250/2713]: training loss : 0.949884967803955 TRAIN  loss dict:  {'classification_loss': 0.949884967803955}
2025-01-19 03:45:27,359 [INFO] Step[1300/2713]: training loss : 0.9543942630290985 TRAIN  loss dict:  {'classification_loss': 0.9543942630290985}
2025-01-19 03:45:42,266 [INFO] Step[1350/2713]: training loss : 0.9579137456417084 TRAIN  loss dict:  {'classification_loss': 0.9579137456417084}
2025-01-19 03:45:57,180 [INFO] Step[1400/2713]: training loss : 0.9662225985527039 TRAIN  loss dict:  {'classification_loss': 0.9662225985527039}
2025-01-19 03:46:12,022 [INFO] Step[1450/2713]: training loss : 0.9527432131767273 TRAIN  loss dict:  {'classification_loss': 0.9527432131767273}
2025-01-19 03:46:26,912 [INFO] Step[1500/2713]: training loss : 0.9516577625274658 TRAIN  loss dict:  {'classification_loss': 0.9516577625274658}
2025-01-19 03:46:41,777 [INFO] Step[1550/2713]: training loss : 0.9537947380542755 TRAIN  loss dict:  {'classification_loss': 0.9537947380542755}
2025-01-19 03:46:56,664 [INFO] Step[1600/2713]: training loss : 0.9491285157203674 TRAIN  loss dict:  {'classification_loss': 0.9491285157203674}
2025-01-19 03:47:11,611 [INFO] Step[1650/2713]: training loss : 0.951942789554596 TRAIN  loss dict:  {'classification_loss': 0.951942789554596}
2025-01-19 03:47:26,483 [INFO] Step[1700/2713]: training loss : 0.9511860585212708 TRAIN  loss dict:  {'classification_loss': 0.9511860585212708}
2025-01-19 03:47:41,377 [INFO] Step[1750/2713]: training loss : 0.9519901895523071 TRAIN  loss dict:  {'classification_loss': 0.9519901895523071}
2025-01-19 03:47:56,329 [INFO] Step[1800/2713]: training loss : 0.9629885029792785 TRAIN  loss dict:  {'classification_loss': 0.9629885029792785}
2025-01-19 03:48:11,291 [INFO] Step[1850/2713]: training loss : 0.9468959307670594 TRAIN  loss dict:  {'classification_loss': 0.9468959307670594}
2025-01-19 03:48:26,271 [INFO] Step[1900/2713]: training loss : 0.9519038009643555 TRAIN  loss dict:  {'classification_loss': 0.9519038009643555}
2025-01-19 03:48:41,168 [INFO] Step[1950/2713]: training loss : 0.9547217464447022 TRAIN  loss dict:  {'classification_loss': 0.9547217464447022}
2025-01-19 03:48:56,104 [INFO] Step[2000/2713]: training loss : 0.9514126920700073 TRAIN  loss dict:  {'classification_loss': 0.9514126920700073}
2025-01-19 03:49:11,019 [INFO] Step[2050/2713]: training loss : 0.9487406504154205 TRAIN  loss dict:  {'classification_loss': 0.9487406504154205}
2025-01-19 03:49:25,960 [INFO] Step[2100/2713]: training loss : 0.9492912113666534 TRAIN  loss dict:  {'classification_loss': 0.9492912113666534}
2025-01-19 03:49:40,878 [INFO] Step[2150/2713]: training loss : 0.9514876770973205 TRAIN  loss dict:  {'classification_loss': 0.9514876770973205}
2025-01-19 03:49:55,815 [INFO] Step[2200/2713]: training loss : 0.9526786613464355 TRAIN  loss dict:  {'classification_loss': 0.9526786613464355}
2025-01-19 03:50:10,671 [INFO] Step[2250/2713]: training loss : 0.9566214108467102 TRAIN  loss dict:  {'classification_loss': 0.9566214108467102}
2025-01-19 03:50:25,621 [INFO] Step[2300/2713]: training loss : 0.9510168540477753 TRAIN  loss dict:  {'classification_loss': 0.9510168540477753}
2025-01-19 03:50:40,573 [INFO] Step[2350/2713]: training loss : 0.9584586381912231 TRAIN  loss dict:  {'classification_loss': 0.9584586381912231}
2025-01-19 03:50:55,510 [INFO] Step[2400/2713]: training loss : 0.9509832274913788 TRAIN  loss dict:  {'classification_loss': 0.9509832274913788}
2025-01-19 03:51:10,416 [INFO] Step[2450/2713]: training loss : 0.956834523677826 TRAIN  loss dict:  {'classification_loss': 0.956834523677826}
2025-01-19 03:51:25,350 [INFO] Step[2500/2713]: training loss : 0.9526585447788238 TRAIN  loss dict:  {'classification_loss': 0.9526585447788238}
2025-01-19 03:51:40,380 [INFO] Step[2550/2713]: training loss : 0.9555551373958587 TRAIN  loss dict:  {'classification_loss': 0.9555551373958587}
2025-01-19 03:51:55,503 [INFO] Step[2600/2713]: training loss : 0.9711762154102326 TRAIN  loss dict:  {'classification_loss': 0.9711762154102326}
2025-01-19 03:52:10,618 [INFO] Step[2650/2713]: training loss : 0.948297597169876 TRAIN  loss dict:  {'classification_loss': 0.948297597169876}
2025-01-19 03:52:25,748 [INFO] Step[2700/2713]: training loss : 0.9644589579105377 TRAIN  loss dict:  {'classification_loss': 0.9644589579105377}
2025-01-19 03:53:45,865 [INFO] Label accuracies statistics:
2025-01-19 03:53:45,865 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 0.75, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.5, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 1.0, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.5, 328: 0.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.25, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 0.5, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 03:53:46,517 [INFO] [52] TRAIN  loss: 0.9545025388516933 acc: 0.9993856739157144
2025-01-19 03:53:46,517 [INFO] [52] TRAIN  loss dict: {'classification_loss': 0.9545025388516933}
2025-01-19 03:53:46,518 [INFO] [52] VALIDATION loss: 1.7339026963800417 VALIDATION acc: 0.8037617554858935
2025-01-19 03:53:46,518 [INFO] [52] VALIDATION loss dict: {'classification_loss': 1.7339026963800417}
2025-01-19 03:53:46,518 [INFO] 
2025-01-19 03:54:06,585 [INFO] Step[50/2713]: training loss : 0.9727453136444092 TRAIN  loss dict:  {'classification_loss': 0.9727453136444092}
2025-01-19 03:54:21,709 [INFO] Step[100/2713]: training loss : 0.9513700687885285 TRAIN  loss dict:  {'classification_loss': 0.9513700687885285}
2025-01-19 03:54:36,837 [INFO] Step[150/2713]: training loss : 0.9542658710479737 TRAIN  loss dict:  {'classification_loss': 0.9542658710479737}
2025-01-19 03:54:51,957 [INFO] Step[200/2713]: training loss : 0.9585975098609925 TRAIN  loss dict:  {'classification_loss': 0.9585975098609925}
2025-01-19 03:55:07,098 [INFO] Step[250/2713]: training loss : 0.9799007999897004 TRAIN  loss dict:  {'classification_loss': 0.9799007999897004}
2025-01-19 03:55:22,236 [INFO] Step[300/2713]: training loss : 0.9487195658683777 TRAIN  loss dict:  {'classification_loss': 0.9487195658683777}
2025-01-19 03:55:37,304 [INFO] Step[350/2713]: training loss : 0.9509389102458954 TRAIN  loss dict:  {'classification_loss': 0.9509389102458954}
2025-01-19 03:55:52,415 [INFO] Step[400/2713]: training loss : 0.9543067944049836 TRAIN  loss dict:  {'classification_loss': 0.9543067944049836}
2025-01-19 03:56:07,530 [INFO] Step[450/2713]: training loss : 0.9498049461841583 TRAIN  loss dict:  {'classification_loss': 0.9498049461841583}
2025-01-19 03:56:22,656 [INFO] Step[500/2713]: training loss : 0.956432797908783 TRAIN  loss dict:  {'classification_loss': 0.956432797908783}
2025-01-19 03:56:37,751 [INFO] Step[550/2713]: training loss : 0.9563002729415894 TRAIN  loss dict:  {'classification_loss': 0.9563002729415894}
2025-01-19 03:56:52,906 [INFO] Step[600/2713]: training loss : 0.947416261434555 TRAIN  loss dict:  {'classification_loss': 0.947416261434555}
2025-01-19 03:57:08,022 [INFO] Step[650/2713]: training loss : 0.95542325258255 TRAIN  loss dict:  {'classification_loss': 0.95542325258255}
2025-01-19 03:57:23,129 [INFO] Step[700/2713]: training loss : 0.9521805095672607 TRAIN  loss dict:  {'classification_loss': 0.9521805095672607}
2025-01-19 03:57:38,246 [INFO] Step[750/2713]: training loss : 0.9583278286457062 TRAIN  loss dict:  {'classification_loss': 0.9583278286457062}
2025-01-19 03:57:53,370 [INFO] Step[800/2713]: training loss : 0.9525039005279541 TRAIN  loss dict:  {'classification_loss': 0.9525039005279541}
2025-01-19 03:58:08,455 [INFO] Step[850/2713]: training loss : 0.9531800508499145 TRAIN  loss dict:  {'classification_loss': 0.9531800508499145}
2025-01-19 03:58:23,611 [INFO] Step[900/2713]: training loss : 0.9523726904392242 TRAIN  loss dict:  {'classification_loss': 0.9523726904392242}
2025-01-19 03:58:38,767 [INFO] Step[950/2713]: training loss : 0.950726877450943 TRAIN  loss dict:  {'classification_loss': 0.950726877450943}
2025-01-19 03:58:53,766 [INFO] Step[1000/2713]: training loss : 0.9628660237789154 TRAIN  loss dict:  {'classification_loss': 0.9628660237789154}
2025-01-19 03:59:08,828 [INFO] Step[1050/2713]: training loss : 0.9543301272392273 TRAIN  loss dict:  {'classification_loss': 0.9543301272392273}
2025-01-19 03:59:23,881 [INFO] Step[1100/2713]: training loss : 0.9495025885105133 TRAIN  loss dict:  {'classification_loss': 0.9495025885105133}
2025-01-19 03:59:38,938 [INFO] Step[1150/2713]: training loss : 0.9604317903518677 TRAIN  loss dict:  {'classification_loss': 0.9604317903518677}
2025-01-19 03:59:54,026 [INFO] Step[1200/2713]: training loss : 0.9575994420051575 TRAIN  loss dict:  {'classification_loss': 0.9575994420051575}
2025-01-19 04:00:09,098 [INFO] Step[1250/2713]: training loss : 0.9552919721603393 TRAIN  loss dict:  {'classification_loss': 0.9552919721603393}
2025-01-19 04:00:24,158 [INFO] Step[1300/2713]: training loss : 0.9533584797382355 TRAIN  loss dict:  {'classification_loss': 0.9533584797382355}
2025-01-19 04:00:39,259 [INFO] Step[1350/2713]: training loss : 0.9551507759094239 TRAIN  loss dict:  {'classification_loss': 0.9551507759094239}
2025-01-19 04:00:54,336 [INFO] Step[1400/2713]: training loss : 0.957227600812912 TRAIN  loss dict:  {'classification_loss': 0.957227600812912}
2025-01-19 04:01:09,452 [INFO] Step[1450/2713]: training loss : 0.9516446709632873 TRAIN  loss dict:  {'classification_loss': 0.9516446709632873}
2025-01-19 04:01:24,543 [INFO] Step[1500/2713]: training loss : 0.9542336547374726 TRAIN  loss dict:  {'classification_loss': 0.9542336547374726}
2025-01-19 04:01:39,623 [INFO] Step[1550/2713]: training loss : 0.9515696668624878 TRAIN  loss dict:  {'classification_loss': 0.9515696668624878}
2025-01-19 04:01:54,756 [INFO] Step[1600/2713]: training loss : 0.9542773509025574 TRAIN  loss dict:  {'classification_loss': 0.9542773509025574}
2025-01-19 04:02:09,830 [INFO] Step[1650/2713]: training loss : 0.9530112552642822 TRAIN  loss dict:  {'classification_loss': 0.9530112552642822}
2025-01-19 04:02:24,838 [INFO] Step[1700/2713]: training loss : 0.9495665609836579 TRAIN  loss dict:  {'classification_loss': 0.9495665609836579}
2025-01-19 04:02:39,933 [INFO] Step[1750/2713]: training loss : 0.951401516199112 TRAIN  loss dict:  {'classification_loss': 0.951401516199112}
2025-01-19 04:02:54,891 [INFO] Step[1800/2713]: training loss : 0.9520775198936462 TRAIN  loss dict:  {'classification_loss': 0.9520775198936462}
2025-01-19 04:03:09,965 [INFO] Step[1850/2713]: training loss : 0.9458199906349182 TRAIN  loss dict:  {'classification_loss': 0.9458199906349182}
2025-01-19 04:03:25,023 [INFO] Step[1900/2713]: training loss : 0.9534037303924561 TRAIN  loss dict:  {'classification_loss': 0.9534037303924561}
2025-01-19 04:03:40,096 [INFO] Step[1950/2713]: training loss : 0.9674157989025116 TRAIN  loss dict:  {'classification_loss': 0.9674157989025116}
2025-01-19 04:03:55,176 [INFO] Step[2000/2713]: training loss : 0.9503092050552369 TRAIN  loss dict:  {'classification_loss': 0.9503092050552369}
2025-01-19 04:04:10,251 [INFO] Step[2050/2713]: training loss : 0.9546038937568665 TRAIN  loss dict:  {'classification_loss': 0.9546038937568665}
2025-01-19 04:04:25,352 [INFO] Step[2100/2713]: training loss : 0.9529824805259705 TRAIN  loss dict:  {'classification_loss': 0.9529824805259705}
2025-01-19 04:04:40,350 [INFO] Step[2150/2713]: training loss : 0.9510716140270233 TRAIN  loss dict:  {'classification_loss': 0.9510716140270233}
2025-01-19 04:04:55,437 [INFO] Step[2200/2713]: training loss : 0.950689514875412 TRAIN  loss dict:  {'classification_loss': 0.950689514875412}
2025-01-19 04:05:10,444 [INFO] Step[2250/2713]: training loss : 0.9510483229160309 TRAIN  loss dict:  {'classification_loss': 0.9510483229160309}
2025-01-19 04:05:25,447 [INFO] Step[2300/2713]: training loss : 0.9564716494083405 TRAIN  loss dict:  {'classification_loss': 0.9564716494083405}
2025-01-19 04:05:40,502 [INFO] Step[2350/2713]: training loss : 0.971548569202423 TRAIN  loss dict:  {'classification_loss': 0.971548569202423}
2025-01-19 04:05:55,625 [INFO] Step[2400/2713]: training loss : 0.9507409703731536 TRAIN  loss dict:  {'classification_loss': 0.9507409703731536}
2025-01-19 04:06:10,648 [INFO] Step[2450/2713]: training loss : 0.9655345273017883 TRAIN  loss dict:  {'classification_loss': 0.9655345273017883}
2025-01-19 04:06:25,664 [INFO] Step[2500/2713]: training loss : 0.9522958266735077 TRAIN  loss dict:  {'classification_loss': 0.9522958266735077}
2025-01-19 04:06:40,711 [INFO] Step[2550/2713]: training loss : 0.9602498137950897 TRAIN  loss dict:  {'classification_loss': 0.9602498137950897}
2025-01-19 04:06:55,717 [INFO] Step[2600/2713]: training loss : 0.9549182474613189 TRAIN  loss dict:  {'classification_loss': 0.9549182474613189}
2025-01-19 04:07:10,827 [INFO] Step[2650/2713]: training loss : 0.9484859371185302 TRAIN  loss dict:  {'classification_loss': 0.9484859371185302}
2025-01-19 04:07:25,849 [INFO] Step[2700/2713]: training loss : 0.9552760112285614 TRAIN  loss dict:  {'classification_loss': 0.9552760112285614}
2025-01-19 04:08:46,062 [INFO] Label accuracies statistics:
2025-01-19 04:08:46,062 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.5, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.25, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.75, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 04:08:46,064 [INFO] [53] TRAIN  loss: 0.9550436970232336 acc: 0.9992628086988573
2025-01-19 04:08:46,064 [INFO] [53] TRAIN  loss dict: {'classification_loss': 0.9550436970232336}
2025-01-19 04:08:46,064 [INFO] [53] VALIDATION loss: 1.763599092686983 VALIDATION acc: 0.799373040752351
2025-01-19 04:08:46,064 [INFO] [53] VALIDATION loss dict: {'classification_loss': 1.763599092686983}
2025-01-19 04:08:46,064 [INFO] 
2025-01-19 04:09:06,477 [INFO] Step[50/2713]: training loss : 0.9662925422191619 TRAIN  loss dict:  {'classification_loss': 0.9662925422191619}
2025-01-19 04:09:21,586 [INFO] Step[100/2713]: training loss : 0.9606604588031769 TRAIN  loss dict:  {'classification_loss': 0.9606604588031769}
2025-01-19 04:09:36,699 [INFO] Step[150/2713]: training loss : 0.9530618059635162 TRAIN  loss dict:  {'classification_loss': 0.9530618059635162}
2025-01-19 04:09:51,820 [INFO] Step[200/2713]: training loss : 0.9472414386272431 TRAIN  loss dict:  {'classification_loss': 0.9472414386272431}
2025-01-19 04:10:06,875 [INFO] Step[250/2713]: training loss : 0.95227898478508 TRAIN  loss dict:  {'classification_loss': 0.95227898478508}
2025-01-19 04:10:21,984 [INFO] Step[300/2713]: training loss : 0.9457059967517852 TRAIN  loss dict:  {'classification_loss': 0.9457059967517852}
2025-01-19 04:10:37,117 [INFO] Step[350/2713]: training loss : 0.9477645599842072 TRAIN  loss dict:  {'classification_loss': 0.9477645599842072}
2025-01-19 04:10:52,303 [INFO] Step[400/2713]: training loss : 0.9469629848003387 TRAIN  loss dict:  {'classification_loss': 0.9469629848003387}
2025-01-19 04:11:07,394 [INFO] Step[450/2713]: training loss : 0.949309207201004 TRAIN  loss dict:  {'classification_loss': 0.949309207201004}
2025-01-19 04:11:22,487 [INFO] Step[500/2713]: training loss : 0.9571538090705871 TRAIN  loss dict:  {'classification_loss': 0.9571538090705871}
2025-01-19 04:11:37,603 [INFO] Step[550/2713]: training loss : 0.9516823160648346 TRAIN  loss dict:  {'classification_loss': 0.9516823160648346}
2025-01-19 04:11:52,755 [INFO] Step[600/2713]: training loss : 0.9498802602291108 TRAIN  loss dict:  {'classification_loss': 0.9498802602291108}
2025-01-19 04:12:07,884 [INFO] Step[650/2713]: training loss : 0.9474121272563935 TRAIN  loss dict:  {'classification_loss': 0.9474121272563935}
2025-01-19 04:12:23,004 [INFO] Step[700/2713]: training loss : 0.9526490211486817 TRAIN  loss dict:  {'classification_loss': 0.9526490211486817}
2025-01-19 04:12:38,190 [INFO] Step[750/2713]: training loss : 0.9600004374980926 TRAIN  loss dict:  {'classification_loss': 0.9600004374980926}
2025-01-19 04:12:53,297 [INFO] Step[800/2713]: training loss : 0.9504755043983459 TRAIN  loss dict:  {'classification_loss': 0.9504755043983459}
2025-01-19 04:13:08,378 [INFO] Step[850/2713]: training loss : 0.9493174970149993 TRAIN  loss dict:  {'classification_loss': 0.9493174970149993}
2025-01-19 04:13:23,468 [INFO] Step[900/2713]: training loss : 0.9470348858833313 TRAIN  loss dict:  {'classification_loss': 0.9470348858833313}
2025-01-19 04:13:38,605 [INFO] Step[950/2713]: training loss : 0.9471337187290192 TRAIN  loss dict:  {'classification_loss': 0.9471337187290192}
2025-01-19 04:13:53,691 [INFO] Step[1000/2713]: training loss : 0.9490780925750733 TRAIN  loss dict:  {'classification_loss': 0.9490780925750733}
2025-01-19 04:14:08,792 [INFO] Step[1050/2713]: training loss : 0.9491384279727936 TRAIN  loss dict:  {'classification_loss': 0.9491384279727936}
2025-01-19 04:14:23,981 [INFO] Step[1100/2713]: training loss : 0.9519427800178528 TRAIN  loss dict:  {'classification_loss': 0.9519427800178528}
2025-01-19 04:14:39,156 [INFO] Step[1150/2713]: training loss : 0.9521235597133636 TRAIN  loss dict:  {'classification_loss': 0.9521235597133636}
2025-01-19 04:14:54,264 [INFO] Step[1200/2713]: training loss : 0.9504349553585052 TRAIN  loss dict:  {'classification_loss': 0.9504349553585052}
2025-01-19 04:15:09,382 [INFO] Step[1250/2713]: training loss : 0.9505795979499817 TRAIN  loss dict:  {'classification_loss': 0.9505795979499817}
2025-01-19 04:15:24,463 [INFO] Step[1300/2713]: training loss : 0.952810879945755 TRAIN  loss dict:  {'classification_loss': 0.952810879945755}
2025-01-19 04:15:39,610 [INFO] Step[1350/2713]: training loss : 0.9506856489181519 TRAIN  loss dict:  {'classification_loss': 0.9506856489181519}
2025-01-19 04:15:54,739 [INFO] Step[1400/2713]: training loss : 0.951130200624466 TRAIN  loss dict:  {'classification_loss': 0.951130200624466}
2025-01-19 04:16:09,885 [INFO] Step[1450/2713]: training loss : 0.9511230385303497 TRAIN  loss dict:  {'classification_loss': 0.9511230385303497}
2025-01-19 04:16:24,905 [INFO] Step[1500/2713]: training loss : 0.9508084666728973 TRAIN  loss dict:  {'classification_loss': 0.9508084666728973}
2025-01-19 04:16:39,975 [INFO] Step[1550/2713]: training loss : 0.9544346833229065 TRAIN  loss dict:  {'classification_loss': 0.9544346833229065}
2025-01-19 04:16:55,075 [INFO] Step[1600/2713]: training loss : 0.9468750596046448 TRAIN  loss dict:  {'classification_loss': 0.9468750596046448}
2025-01-19 04:17:10,258 [INFO] Step[1650/2713]: training loss : 0.950116058588028 TRAIN  loss dict:  {'classification_loss': 0.950116058588028}
2025-01-19 04:17:25,381 [INFO] Step[1700/2713]: training loss : 0.9507499527931214 TRAIN  loss dict:  {'classification_loss': 0.9507499527931214}
2025-01-19 04:17:40,556 [INFO] Step[1750/2713]: training loss : 0.9573518872261048 TRAIN  loss dict:  {'classification_loss': 0.9573518872261048}
2025-01-19 04:17:55,641 [INFO] Step[1800/2713]: training loss : 0.9479677104949951 TRAIN  loss dict:  {'classification_loss': 0.9479677104949951}
2025-01-19 04:18:10,735 [INFO] Step[1850/2713]: training loss : 0.9531696021556855 TRAIN  loss dict:  {'classification_loss': 0.9531696021556855}
2025-01-19 04:18:25,872 [INFO] Step[1900/2713]: training loss : 0.9523516464233398 TRAIN  loss dict:  {'classification_loss': 0.9523516464233398}
2025-01-19 04:18:40,986 [INFO] Step[1950/2713]: training loss : 0.9487013959884644 TRAIN  loss dict:  {'classification_loss': 0.9487013959884644}
2025-01-19 04:18:56,069 [INFO] Step[2000/2713]: training loss : 0.9553065800666809 TRAIN  loss dict:  {'classification_loss': 0.9553065800666809}
2025-01-19 04:19:11,160 [INFO] Step[2050/2713]: training loss : 0.9464140737056732 TRAIN  loss dict:  {'classification_loss': 0.9464140737056732}
2025-01-19 04:19:26,295 [INFO] Step[2100/2713]: training loss : 0.9587993967533112 TRAIN  loss dict:  {'classification_loss': 0.9587993967533112}
2025-01-19 04:19:41,383 [INFO] Step[2150/2713]: training loss : 0.9485026860237121 TRAIN  loss dict:  {'classification_loss': 0.9485026860237121}
2025-01-19 04:19:56,422 [INFO] Step[2200/2713]: training loss : 0.9579605257511139 TRAIN  loss dict:  {'classification_loss': 0.9579605257511139}
2025-01-19 04:20:11,469 [INFO] Step[2250/2713]: training loss : 0.9531753194332123 TRAIN  loss dict:  {'classification_loss': 0.9531753194332123}
2025-01-19 04:20:26,465 [INFO] Step[2300/2713]: training loss : 0.9605987870693207 TRAIN  loss dict:  {'classification_loss': 0.9605987870693207}
2025-01-19 04:20:41,482 [INFO] Step[2350/2713]: training loss : 0.9504769849777222 TRAIN  loss dict:  {'classification_loss': 0.9504769849777222}
2025-01-19 04:20:56,558 [INFO] Step[2400/2713]: training loss : 0.9485331213474274 TRAIN  loss dict:  {'classification_loss': 0.9485331213474274}
2025-01-19 04:21:11,609 [INFO] Step[2450/2713]: training loss : 0.9526189470291138 TRAIN  loss dict:  {'classification_loss': 0.9526189470291138}
2025-01-19 04:21:26,645 [INFO] Step[2500/2713]: training loss : 0.9577927684783936 TRAIN  loss dict:  {'classification_loss': 0.9577927684783936}
2025-01-19 04:21:41,671 [INFO] Step[2550/2713]: training loss : 0.9639554703235627 TRAIN  loss dict:  {'classification_loss': 0.9639554703235627}
2025-01-19 04:21:56,711 [INFO] Step[2600/2713]: training loss : 0.9536713254451752 TRAIN  loss dict:  {'classification_loss': 0.9536713254451752}
2025-01-19 04:22:11,807 [INFO] Step[2650/2713]: training loss : 0.9514902436733246 TRAIN  loss dict:  {'classification_loss': 0.9514902436733246}
2025-01-19 04:22:26,920 [INFO] Step[2700/2713]: training loss : 0.9467038643360138 TRAIN  loss dict:  {'classification_loss': 0.9467038643360138}
2025-01-19 04:23:46,636 [INFO] Label accuracies statistics:
2025-01-19 04:23:46,636 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 1.0, 205: 1.0, 206: 0.5, 207: 1.0, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.25, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 04:23:46,638 [INFO] [54] TRAIN  loss: 0.9521185906224263 acc: 0.9993856739157144
2025-01-19 04:23:46,638 [INFO] [54] TRAIN  loss dict: {'classification_loss': 0.9521185906224263}
2025-01-19 04:23:46,639 [INFO] [54] VALIDATION loss: 1.7479858315528785 VALIDATION acc: 0.7981191222570533
2025-01-19 04:23:46,639 [INFO] [54] VALIDATION loss dict: {'classification_loss': 1.7479858315528785}
2025-01-19 04:23:46,639 [INFO] 
2025-01-19 04:24:06,822 [INFO] Step[50/2713]: training loss : 0.9526253581047058 TRAIN  loss dict:  {'classification_loss': 0.9526253581047058}
2025-01-19 04:24:21,743 [INFO] Step[100/2713]: training loss : 0.9491207957267761 TRAIN  loss dict:  {'classification_loss': 0.9491207957267761}
2025-01-19 04:24:36,714 [INFO] Step[150/2713]: training loss : 0.9481900870800019 TRAIN  loss dict:  {'classification_loss': 0.9481900870800019}
2025-01-19 04:24:51,718 [INFO] Step[200/2713]: training loss : 0.9541418814659118 TRAIN  loss dict:  {'classification_loss': 0.9541418814659118}
2025-01-19 04:25:06,678 [INFO] Step[250/2713]: training loss : 0.9515804648399353 TRAIN  loss dict:  {'classification_loss': 0.9515804648399353}
2025-01-19 04:25:21,642 [INFO] Step[300/2713]: training loss : 0.9515516722202301 TRAIN  loss dict:  {'classification_loss': 0.9515516722202301}
2025-01-19 04:25:36,652 [INFO] Step[350/2713]: training loss : 0.9660993540287017 TRAIN  loss dict:  {'classification_loss': 0.9660993540287017}
2025-01-19 04:25:51,668 [INFO] Step[400/2713]: training loss : 0.9444162952899933 TRAIN  loss dict:  {'classification_loss': 0.9444162952899933}
2025-01-19 04:26:06,622 [INFO] Step[450/2713]: training loss : 0.9478456270694733 TRAIN  loss dict:  {'classification_loss': 0.9478456270694733}
2025-01-19 04:26:21,633 [INFO] Step[500/2713]: training loss : 0.9534991478919983 TRAIN  loss dict:  {'classification_loss': 0.9534991478919983}
2025-01-19 04:26:36,570 [INFO] Step[550/2713]: training loss : 0.9522962057590485 TRAIN  loss dict:  {'classification_loss': 0.9522962057590485}
2025-01-19 04:26:51,573 [INFO] Step[600/2713]: training loss : 0.967660300731659 TRAIN  loss dict:  {'classification_loss': 0.967660300731659}
2025-01-19 04:27:06,578 [INFO] Step[650/2713]: training loss : 0.9555553805828094 TRAIN  loss dict:  {'classification_loss': 0.9555553805828094}
2025-01-19 04:27:21,573 [INFO] Step[700/2713]: training loss : 0.9506018924713134 TRAIN  loss dict:  {'classification_loss': 0.9506018924713134}
2025-01-19 04:27:36,552 [INFO] Step[750/2713]: training loss : 0.9467413592338562 TRAIN  loss dict:  {'classification_loss': 0.9467413592338562}
2025-01-19 04:27:51,529 [INFO] Step[800/2713]: training loss : 0.9496770012378692 TRAIN  loss dict:  {'classification_loss': 0.9496770012378692}
2025-01-19 04:28:06,599 [INFO] Step[850/2713]: training loss : 0.9602398562431336 TRAIN  loss dict:  {'classification_loss': 0.9602398562431336}
2025-01-19 04:28:21,570 [INFO] Step[900/2713]: training loss : 0.9462765634059906 TRAIN  loss dict:  {'classification_loss': 0.9462765634059906}
2025-01-19 04:28:36,600 [INFO] Step[950/2713]: training loss : 0.9573346400260925 TRAIN  loss dict:  {'classification_loss': 0.9573346400260925}
2025-01-19 04:28:51,603 [INFO] Step[1000/2713]: training loss : 0.950683559179306 TRAIN  loss dict:  {'classification_loss': 0.950683559179306}
2025-01-19 04:29:06,610 [INFO] Step[1050/2713]: training loss : 0.9661716544628143 TRAIN  loss dict:  {'classification_loss': 0.9661716544628143}
2025-01-19 04:29:21,560 [INFO] Step[1100/2713]: training loss : 0.9469890022277831 TRAIN  loss dict:  {'classification_loss': 0.9469890022277831}
2025-01-19 04:29:36,517 [INFO] Step[1150/2713]: training loss : 0.9580719494819641 TRAIN  loss dict:  {'classification_loss': 0.9580719494819641}
2025-01-19 04:29:51,561 [INFO] Step[1200/2713]: training loss : 0.9476862740516663 TRAIN  loss dict:  {'classification_loss': 0.9476862740516663}
2025-01-19 04:30:06,645 [INFO] Step[1250/2713]: training loss : 0.9478670847415924 TRAIN  loss dict:  {'classification_loss': 0.9478670847415924}
2025-01-19 04:30:21,641 [INFO] Step[1300/2713]: training loss : 0.9542140412330627 TRAIN  loss dict:  {'classification_loss': 0.9542140412330627}
2025-01-19 04:30:36,597 [INFO] Step[1350/2713]: training loss : 0.9786521255970001 TRAIN  loss dict:  {'classification_loss': 0.9786521255970001}
2025-01-19 04:30:51,562 [INFO] Step[1400/2713]: training loss : 0.9537515223026276 TRAIN  loss dict:  {'classification_loss': 0.9537515223026276}
2025-01-19 04:31:06,566 [INFO] Step[1450/2713]: training loss : 0.9509338510036468 TRAIN  loss dict:  {'classification_loss': 0.9509338510036468}
2025-01-19 04:31:21,558 [INFO] Step[1500/2713]: training loss : 0.9581965315341949 TRAIN  loss dict:  {'classification_loss': 0.9581965315341949}
2025-01-19 04:31:36,492 [INFO] Step[1550/2713]: training loss : 0.9548393440246582 TRAIN  loss dict:  {'classification_loss': 0.9548393440246582}
2025-01-19 04:31:51,462 [INFO] Step[1600/2713]: training loss : 0.9550725436210632 TRAIN  loss dict:  {'classification_loss': 0.9550725436210632}
2025-01-19 04:32:06,463 [INFO] Step[1650/2713]: training loss : 0.9626935577392578 TRAIN  loss dict:  {'classification_loss': 0.9626935577392578}
2025-01-19 04:32:21,420 [INFO] Step[1700/2713]: training loss : 0.9495251858234406 TRAIN  loss dict:  {'classification_loss': 0.9495251858234406}
2025-01-19 04:32:36,428 [INFO] Step[1750/2713]: training loss : 0.9578375792503357 TRAIN  loss dict:  {'classification_loss': 0.9578375792503357}
2025-01-19 04:32:51,372 [INFO] Step[1800/2713]: training loss : 0.9581778192520142 TRAIN  loss dict:  {'classification_loss': 0.9581778192520142}
2025-01-19 04:33:06,361 [INFO] Step[1850/2713]: training loss : 0.9507914292812347 TRAIN  loss dict:  {'classification_loss': 0.9507914292812347}
2025-01-19 04:33:21,336 [INFO] Step[1900/2713]: training loss : 0.950088928937912 TRAIN  loss dict:  {'classification_loss': 0.950088928937912}
2025-01-19 04:33:36,319 [INFO] Step[1950/2713]: training loss : 0.9462748980522155 TRAIN  loss dict:  {'classification_loss': 0.9462748980522155}
2025-01-19 04:33:51,266 [INFO] Step[2000/2713]: training loss : 0.96378413438797 TRAIN  loss dict:  {'classification_loss': 0.96378413438797}
2025-01-19 04:34:06,253 [INFO] Step[2050/2713]: training loss : 0.9589492809772492 TRAIN  loss dict:  {'classification_loss': 0.9589492809772492}
2025-01-19 04:34:21,211 [INFO] Step[2100/2713]: training loss : 0.9493037486076354 TRAIN  loss dict:  {'classification_loss': 0.9493037486076354}
2025-01-19 04:34:36,229 [INFO] Step[2150/2713]: training loss : 0.9456192743778229 TRAIN  loss dict:  {'classification_loss': 0.9456192743778229}
2025-01-19 04:34:51,202 [INFO] Step[2200/2713]: training loss : 0.9504546427726746 TRAIN  loss dict:  {'classification_loss': 0.9504546427726746}
2025-01-19 04:35:06,149 [INFO] Step[2250/2713]: training loss : 0.9512488126754761 TRAIN  loss dict:  {'classification_loss': 0.9512488126754761}
2025-01-19 04:35:21,073 [INFO] Step[2300/2713]: training loss : 0.9710341954231262 TRAIN  loss dict:  {'classification_loss': 0.9710341954231262}
2025-01-19 04:35:36,020 [INFO] Step[2350/2713]: training loss : 0.951415981054306 TRAIN  loss dict:  {'classification_loss': 0.951415981054306}
2025-01-19 04:35:50,969 [INFO] Step[2400/2713]: training loss : 0.9487040841579437 TRAIN  loss dict:  {'classification_loss': 0.9487040841579437}
2025-01-19 04:36:05,976 [INFO] Step[2450/2713]: training loss : 0.9476868665218353 TRAIN  loss dict:  {'classification_loss': 0.9476868665218353}
2025-01-19 04:36:20,926 [INFO] Step[2500/2713]: training loss : 0.9522724568843841 TRAIN  loss dict:  {'classification_loss': 0.9522724568843841}
2025-01-19 04:36:35,928 [INFO] Step[2550/2713]: training loss : 0.9486287832260132 TRAIN  loss dict:  {'classification_loss': 0.9486287832260132}
2025-01-19 04:36:51,009 [INFO] Step[2600/2713]: training loss : 0.9586998319625855 TRAIN  loss dict:  {'classification_loss': 0.9586998319625855}
2025-01-19 04:37:06,082 [INFO] Step[2650/2713]: training loss : 0.9498231732845306 TRAIN  loss dict:  {'classification_loss': 0.9498231732845306}
2025-01-19 04:37:21,069 [INFO] Step[2700/2713]: training loss : 0.9514832973480225 TRAIN  loss dict:  {'classification_loss': 0.9514832973480225}
2025-01-19 04:38:41,612 [INFO] Label accuracies statistics:
2025-01-19 04:38:41,612 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.5, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 1.0, 207: 1.0, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 1.0, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.25, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 04:38:53,910 [INFO] [55] TRAIN  loss: 0.9537555568120685 acc: 0.9985256173977147
2025-01-19 04:38:53,910 [INFO] [55] TRAIN  loss dict: {'classification_loss': 0.9537555568120685}
2025-01-19 04:38:53,910 [INFO] [55] VALIDATION loss: 1.7236678416567637 VALIDATION acc: 0.8087774294670846
2025-01-19 04:38:53,911 [INFO] [55] VALIDATION loss dict: {'classification_loss': 1.7236678416567637}
2025-01-19 04:38:53,911 [INFO] 
2025-01-19 04:39:13,713 [INFO] Step[50/2713]: training loss : 0.9496285378932953 TRAIN  loss dict:  {'classification_loss': 0.9496285378932953}
2025-01-19 04:39:28,719 [INFO] Step[100/2713]: training loss : 0.9497350525856018 TRAIN  loss dict:  {'classification_loss': 0.9497350525856018}
2025-01-19 04:39:43,770 [INFO] Step[150/2713]: training loss : 0.9512337267398834 TRAIN  loss dict:  {'classification_loss': 0.9512337267398834}
2025-01-19 04:39:58,723 [INFO] Step[200/2713]: training loss : 0.9533757245540619 TRAIN  loss dict:  {'classification_loss': 0.9533757245540619}
2025-01-19 04:40:13,759 [INFO] Step[250/2713]: training loss : 0.9480408906936646 TRAIN  loss dict:  {'classification_loss': 0.9480408906936646}
2025-01-19 04:40:28,737 [INFO] Step[300/2713]: training loss : 0.9465968108177185 TRAIN  loss dict:  {'classification_loss': 0.9465968108177185}
2025-01-19 04:40:43,748 [INFO] Step[350/2713]: training loss : 0.9494482409954071 TRAIN  loss dict:  {'classification_loss': 0.9494482409954071}
2025-01-19 04:40:58,772 [INFO] Step[400/2713]: training loss : 0.9463856935501098 TRAIN  loss dict:  {'classification_loss': 0.9463856935501098}
2025-01-19 04:41:13,828 [INFO] Step[450/2713]: training loss : 0.9452813065052033 TRAIN  loss dict:  {'classification_loss': 0.9452813065052033}
2025-01-19 04:41:28,792 [INFO] Step[500/2713]: training loss : 0.9534130156040191 TRAIN  loss dict:  {'classification_loss': 0.9534130156040191}
2025-01-19 04:41:43,721 [INFO] Step[550/2713]: training loss : 0.94839519739151 TRAIN  loss dict:  {'classification_loss': 0.94839519739151}
2025-01-19 04:41:58,682 [INFO] Step[600/2713]: training loss : 0.950857093334198 TRAIN  loss dict:  {'classification_loss': 0.950857093334198}
2025-01-19 04:42:13,657 [INFO] Step[650/2713]: training loss : 0.9457034099102021 TRAIN  loss dict:  {'classification_loss': 0.9457034099102021}
2025-01-19 04:42:28,598 [INFO] Step[700/2713]: training loss : 0.9551508891582489 TRAIN  loss dict:  {'classification_loss': 0.9551508891582489}
2025-01-19 04:42:43,585 [INFO] Step[750/2713]: training loss : 0.9598993945121765 TRAIN  loss dict:  {'classification_loss': 0.9598993945121765}
2025-01-19 04:42:58,563 [INFO] Step[800/2713]: training loss : 0.9486641049385071 TRAIN  loss dict:  {'classification_loss': 0.9486641049385071}
2025-01-19 04:43:13,683 [INFO] Step[850/2713]: training loss : 0.9520306289196014 TRAIN  loss dict:  {'classification_loss': 0.9520306289196014}
2025-01-19 04:43:28,761 [INFO] Step[900/2713]: training loss : 0.9466410827636719 TRAIN  loss dict:  {'classification_loss': 0.9466410827636719}
2025-01-19 04:43:43,868 [INFO] Step[950/2713]: training loss : 0.9495397257804871 TRAIN  loss dict:  {'classification_loss': 0.9495397257804871}
2025-01-19 04:43:58,970 [INFO] Step[1000/2713]: training loss : 0.9472835576534271 TRAIN  loss dict:  {'classification_loss': 0.9472835576534271}
2025-01-19 04:44:14,092 [INFO] Step[1050/2713]: training loss : 0.9496794617176056 TRAIN  loss dict:  {'classification_loss': 0.9496794617176056}
2025-01-19 04:44:29,213 [INFO] Step[1100/2713]: training loss : 0.9462271428108215 TRAIN  loss dict:  {'classification_loss': 0.9462271428108215}
2025-01-19 04:44:44,305 [INFO] Step[1150/2713]: training loss : 0.9500250506401062 TRAIN  loss dict:  {'classification_loss': 0.9500250506401062}
2025-01-19 04:44:59,456 [INFO] Step[1200/2713]: training loss : 0.9526396632194519 TRAIN  loss dict:  {'classification_loss': 0.9526396632194519}
2025-01-19 04:45:14,521 [INFO] Step[1250/2713]: training loss : 0.9486717772483826 TRAIN  loss dict:  {'classification_loss': 0.9486717772483826}
2025-01-19 04:45:29,513 [INFO] Step[1300/2713]: training loss : 0.9474074876308441 TRAIN  loss dict:  {'classification_loss': 0.9474074876308441}
2025-01-19 04:45:44,608 [INFO] Step[1350/2713]: training loss : 0.9448748075962067 TRAIN  loss dict:  {'classification_loss': 0.9448748075962067}
2025-01-19 04:45:59,650 [INFO] Step[1400/2713]: training loss : 0.9607151758670807 TRAIN  loss dict:  {'classification_loss': 0.9607151758670807}
2025-01-19 04:46:14,746 [INFO] Step[1450/2713]: training loss : 0.9480426394939423 TRAIN  loss dict:  {'classification_loss': 0.9480426394939423}
2025-01-19 04:46:29,809 [INFO] Step[1500/2713]: training loss : 0.9520086419582366 TRAIN  loss dict:  {'classification_loss': 0.9520086419582366}
2025-01-19 04:46:44,842 [INFO] Step[1550/2713]: training loss : 0.9512519562244415 TRAIN  loss dict:  {'classification_loss': 0.9512519562244415}
2025-01-19 04:46:59,896 [INFO] Step[1600/2713]: training loss : 0.9468893134593963 TRAIN  loss dict:  {'classification_loss': 0.9468893134593963}
2025-01-19 04:47:15,009 [INFO] Step[1650/2713]: training loss : 0.9520977187156677 TRAIN  loss dict:  {'classification_loss': 0.9520977187156677}
2025-01-19 04:47:30,076 [INFO] Step[1700/2713]: training loss : 0.9470854985713959 TRAIN  loss dict:  {'classification_loss': 0.9470854985713959}
2025-01-19 04:47:45,117 [INFO] Step[1750/2713]: training loss : 0.945099573135376 TRAIN  loss dict:  {'classification_loss': 0.945099573135376}
2025-01-19 04:48:00,230 [INFO] Step[1800/2713]: training loss : 0.951221718788147 TRAIN  loss dict:  {'classification_loss': 0.951221718788147}
2025-01-19 04:48:15,301 [INFO] Step[1850/2713]: training loss : 0.9523972010612488 TRAIN  loss dict:  {'classification_loss': 0.9523972010612488}
2025-01-19 04:48:30,390 [INFO] Step[1900/2713]: training loss : 0.9783236169815064 TRAIN  loss dict:  {'classification_loss': 0.9783236169815064}
2025-01-19 04:48:45,472 [INFO] Step[1950/2713]: training loss : 0.9591900432109832 TRAIN  loss dict:  {'classification_loss': 0.9591900432109832}
2025-01-19 04:49:00,477 [INFO] Step[2000/2713]: training loss : 0.9531577444076538 TRAIN  loss dict:  {'classification_loss': 0.9531577444076538}
2025-01-19 04:49:15,525 [INFO] Step[2050/2713]: training loss : 0.9640809881687165 TRAIN  loss dict:  {'classification_loss': 0.9640809881687165}
2025-01-19 04:49:30,558 [INFO] Step[2100/2713]: training loss : 0.9522598040103912 TRAIN  loss dict:  {'classification_loss': 0.9522598040103912}
2025-01-19 04:49:45,586 [INFO] Step[2150/2713]: training loss : 0.9587276887893676 TRAIN  loss dict:  {'classification_loss': 0.9587276887893676}
2025-01-19 04:50:00,611 [INFO] Step[2200/2713]: training loss : 0.9599209988117218 TRAIN  loss dict:  {'classification_loss': 0.9599209988117218}
2025-01-19 04:50:15,623 [INFO] Step[2250/2713]: training loss : 0.95070587515831 TRAIN  loss dict:  {'classification_loss': 0.95070587515831}
2025-01-19 04:50:30,571 [INFO] Step[2300/2713]: training loss : 0.9449762606620788 TRAIN  loss dict:  {'classification_loss': 0.9449762606620788}
2025-01-19 04:50:45,521 [INFO] Step[2350/2713]: training loss : 0.9478024137020111 TRAIN  loss dict:  {'classification_loss': 0.9478024137020111}
2025-01-19 04:51:00,534 [INFO] Step[2400/2713]: training loss : 0.945052969455719 TRAIN  loss dict:  {'classification_loss': 0.945052969455719}
2025-01-19 04:51:15,550 [INFO] Step[2450/2713]: training loss : 0.94845632314682 TRAIN  loss dict:  {'classification_loss': 0.94845632314682}
2025-01-19 04:51:30,512 [INFO] Step[2500/2713]: training loss : 0.9562888729572296 TRAIN  loss dict:  {'classification_loss': 0.9562888729572296}
2025-01-19 04:51:45,496 [INFO] Step[2550/2713]: training loss : 0.9467686557769776 TRAIN  loss dict:  {'classification_loss': 0.9467686557769776}
2025-01-19 04:52:00,495 [INFO] Step[2600/2713]: training loss : 0.961939948797226 TRAIN  loss dict:  {'classification_loss': 0.961939948797226}
2025-01-19 04:52:15,495 [INFO] Step[2650/2713]: training loss : 0.9477078652381897 TRAIN  loss dict:  {'classification_loss': 0.9477078652381897}
2025-01-19 04:52:30,513 [INFO] Step[2700/2713]: training loss : 0.9491688930988311 TRAIN  loss dict:  {'classification_loss': 0.9491688930988311}
2025-01-19 04:53:50,820 [INFO] Label accuracies statistics:
2025-01-19 04:53:50,820 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.75, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 0.75, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 1.0, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.5, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 0.75, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 04:53:50,822 [INFO] [56] TRAIN  loss: 0.9512442811977543 acc: 0.9990170782651432
2025-01-19 04:53:50,822 [INFO] [56] TRAIN  loss dict: {'classification_loss': 0.9512442811977543}
2025-01-19 04:53:50,822 [INFO] [56] VALIDATION loss: 1.7687008573596639 VALIDATION acc: 0.8
2025-01-19 04:53:50,822 [INFO] [56] VALIDATION loss dict: {'classification_loss': 1.7687008573596639}
2025-01-19 04:53:50,823 [INFO] 
2025-01-19 04:54:10,575 [INFO] Step[50/2713]: training loss : 0.9467669570446015 TRAIN  loss dict:  {'classification_loss': 0.9467669570446015}
2025-01-19 04:54:25,504 [INFO] Step[100/2713]: training loss : 0.9606266462802887 TRAIN  loss dict:  {'classification_loss': 0.9606266462802887}
2025-01-19 04:54:40,522 [INFO] Step[150/2713]: training loss : 0.9545136034488678 TRAIN  loss dict:  {'classification_loss': 0.9545136034488678}
2025-01-19 04:54:55,596 [INFO] Step[200/2713]: training loss : 0.9522043907642365 TRAIN  loss dict:  {'classification_loss': 0.9522043907642365}
2025-01-19 04:55:10,583 [INFO] Step[250/2713]: training loss : 0.9497460949420929 TRAIN  loss dict:  {'classification_loss': 0.9497460949420929}
2025-01-19 04:55:25,603 [INFO] Step[300/2713]: training loss : 0.9495589339733124 TRAIN  loss dict:  {'classification_loss': 0.9495589339733124}
2025-01-19 04:55:40,617 [INFO] Step[350/2713]: training loss : 0.9466820085048675 TRAIN  loss dict:  {'classification_loss': 0.9466820085048675}
2025-01-19 04:55:55,613 [INFO] Step[400/2713]: training loss : 0.9469730329513549 TRAIN  loss dict:  {'classification_loss': 0.9469730329513549}
2025-01-19 04:56:10,595 [INFO] Step[450/2713]: training loss : 0.9490185582637787 TRAIN  loss dict:  {'classification_loss': 0.9490185582637787}
2025-01-19 04:56:25,584 [INFO] Step[500/2713]: training loss : 0.9554387986660003 TRAIN  loss dict:  {'classification_loss': 0.9554387986660003}
2025-01-19 04:56:40,558 [INFO] Step[550/2713]: training loss : 0.9492382133007049 TRAIN  loss dict:  {'classification_loss': 0.9492382133007049}
2025-01-19 04:56:55,543 [INFO] Step[600/2713]: training loss : 0.9531151723861694 TRAIN  loss dict:  {'classification_loss': 0.9531151723861694}
2025-01-19 04:57:10,560 [INFO] Step[650/2713]: training loss : 0.9470020568370819 TRAIN  loss dict:  {'classification_loss': 0.9470020568370819}
2025-01-19 04:57:25,562 [INFO] Step[700/2713]: training loss : 0.9462372195720673 TRAIN  loss dict:  {'classification_loss': 0.9462372195720673}
2025-01-19 04:57:40,564 [INFO] Step[750/2713]: training loss : 0.9533513641357422 TRAIN  loss dict:  {'classification_loss': 0.9533513641357422}
2025-01-19 04:57:55,604 [INFO] Step[800/2713]: training loss : 0.9473842358589173 TRAIN  loss dict:  {'classification_loss': 0.9473842358589173}
2025-01-19 04:58:10,629 [INFO] Step[850/2713]: training loss : 0.9528019559383393 TRAIN  loss dict:  {'classification_loss': 0.9528019559383393}
2025-01-19 04:58:25,611 [INFO] Step[900/2713]: training loss : 0.951545238494873 TRAIN  loss dict:  {'classification_loss': 0.951545238494873}
2025-01-19 04:58:40,612 [INFO] Step[950/2713]: training loss : 0.9507412326335907 TRAIN  loss dict:  {'classification_loss': 0.9507412326335907}
2025-01-19 04:58:55,591 [INFO] Step[1000/2713]: training loss : 0.9476008820533752 TRAIN  loss dict:  {'classification_loss': 0.9476008820533752}
2025-01-19 04:59:10,574 [INFO] Step[1050/2713]: training loss : 0.9512938988208771 TRAIN  loss dict:  {'classification_loss': 0.9512938988208771}
2025-01-19 04:59:25,571 [INFO] Step[1100/2713]: training loss : 0.9518229043483735 TRAIN  loss dict:  {'classification_loss': 0.9518229043483735}
2025-01-19 04:59:40,605 [INFO] Step[1150/2713]: training loss : 0.9653727602958679 TRAIN  loss dict:  {'classification_loss': 0.9653727602958679}
2025-01-19 04:59:55,559 [INFO] Step[1200/2713]: training loss : 0.9487786221504212 TRAIN  loss dict:  {'classification_loss': 0.9487786221504212}
2025-01-19 05:00:10,571 [INFO] Step[1250/2713]: training loss : 0.9468168807029724 TRAIN  loss dict:  {'classification_loss': 0.9468168807029724}
2025-01-19 05:00:25,531 [INFO] Step[1300/2713]: training loss : 0.9542903828620911 TRAIN  loss dict:  {'classification_loss': 0.9542903828620911}
2025-01-19 05:00:40,580 [INFO] Step[1350/2713]: training loss : 0.9480018198490143 TRAIN  loss dict:  {'classification_loss': 0.9480018198490143}
2025-01-19 05:00:55,570 [INFO] Step[1400/2713]: training loss : 0.9503442764282226 TRAIN  loss dict:  {'classification_loss': 0.9503442764282226}
2025-01-19 05:01:10,583 [INFO] Step[1450/2713]: training loss : 0.9468396663665771 TRAIN  loss dict:  {'classification_loss': 0.9468396663665771}
2025-01-19 05:01:25,573 [INFO] Step[1500/2713]: training loss : 0.9583597719669342 TRAIN  loss dict:  {'classification_loss': 0.9583597719669342}
2025-01-19 05:01:40,631 [INFO] Step[1550/2713]: training loss : 0.9574038076400757 TRAIN  loss dict:  {'classification_loss': 0.9574038076400757}
2025-01-19 05:01:55,587 [INFO] Step[1600/2713]: training loss : 0.953158472776413 TRAIN  loss dict:  {'classification_loss': 0.953158472776413}
2025-01-19 05:02:10,508 [INFO] Step[1650/2713]: training loss : 0.9499222683906555 TRAIN  loss dict:  {'classification_loss': 0.9499222683906555}
2025-01-19 05:02:25,418 [INFO] Step[1700/2713]: training loss : 0.9631043016910553 TRAIN  loss dict:  {'classification_loss': 0.9631043016910553}
2025-01-19 05:02:40,362 [INFO] Step[1750/2713]: training loss : 0.9471247851848602 TRAIN  loss dict:  {'classification_loss': 0.9471247851848602}
2025-01-19 05:02:55,265 [INFO] Step[1800/2713]: training loss : 0.9479479146003723 TRAIN  loss dict:  {'classification_loss': 0.9479479146003723}
2025-01-19 05:03:10,191 [INFO] Step[1850/2713]: training loss : 0.9547470510005951 TRAIN  loss dict:  {'classification_loss': 0.9547470510005951}
2025-01-19 05:03:25,157 [INFO] Step[1900/2713]: training loss : 0.9465734207630158 TRAIN  loss dict:  {'classification_loss': 0.9465734207630158}
2025-01-19 05:03:40,142 [INFO] Step[1950/2713]: training loss : 0.9476911962032318 TRAIN  loss dict:  {'classification_loss': 0.9476911962032318}
2025-01-19 05:03:55,151 [INFO] Step[2000/2713]: training loss : 0.952341034412384 TRAIN  loss dict:  {'classification_loss': 0.952341034412384}
2025-01-19 05:04:10,176 [INFO] Step[2050/2713]: training loss : 0.947337988615036 TRAIN  loss dict:  {'classification_loss': 0.947337988615036}
2025-01-19 05:04:25,127 [INFO] Step[2100/2713]: training loss : 0.9533127617835998 TRAIN  loss dict:  {'classification_loss': 0.9533127617835998}
2025-01-19 05:04:40,107 [INFO] Step[2150/2713]: training loss : 0.9478820502758026 TRAIN  loss dict:  {'classification_loss': 0.9478820502758026}
2025-01-19 05:04:55,036 [INFO] Step[2200/2713]: training loss : 0.9498636293411254 TRAIN  loss dict:  {'classification_loss': 0.9498636293411254}
2025-01-19 05:05:09,984 [INFO] Step[2250/2713]: training loss : 0.945700968503952 TRAIN  loss dict:  {'classification_loss': 0.945700968503952}
2025-01-19 05:05:24,998 [INFO] Step[2300/2713]: training loss : 0.9469392919540405 TRAIN  loss dict:  {'classification_loss': 0.9469392919540405}
2025-01-19 05:05:39,967 [INFO] Step[2350/2713]: training loss : 0.9479171931743622 TRAIN  loss dict:  {'classification_loss': 0.9479171931743622}
2025-01-19 05:05:54,951 [INFO] Step[2400/2713]: training loss : 0.9463751137256622 TRAIN  loss dict:  {'classification_loss': 0.9463751137256622}
2025-01-19 05:06:09,987 [INFO] Step[2450/2713]: training loss : 0.9543461096286774 TRAIN  loss dict:  {'classification_loss': 0.9543461096286774}
2025-01-19 05:06:24,982 [INFO] Step[2500/2713]: training loss : 0.9506384885311127 TRAIN  loss dict:  {'classification_loss': 0.9506384885311127}
2025-01-19 05:06:39,968 [INFO] Step[2550/2713]: training loss : 0.9462572979927063 TRAIN  loss dict:  {'classification_loss': 0.9462572979927063}
2025-01-19 05:06:54,933 [INFO] Step[2600/2713]: training loss : 0.9472366011142731 TRAIN  loss dict:  {'classification_loss': 0.9472366011142731}
2025-01-19 05:07:09,917 [INFO] Step[2650/2713]: training loss : 0.951170779466629 TRAIN  loss dict:  {'classification_loss': 0.951170779466629}
2025-01-19 05:07:24,933 [INFO] Step[2700/2713]: training loss : 0.9661195158958436 TRAIN  loss dict:  {'classification_loss': 0.9661195158958436}
2025-01-19 05:08:45,205 [INFO] Label accuracies statistics:
2025-01-19 05:08:45,205 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.5, 59: 1.0, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.5, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.5, 207: 1.0, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.25, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.5, 289: 0.5, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.25, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.5, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 05:08:45,211 [INFO] [57] TRAIN  loss: 0.9510076863054767 acc: 0.9996314043494287
2025-01-19 05:08:45,211 [INFO] [57] TRAIN  loss dict: {'classification_loss': 0.9510076863054767}
2025-01-19 05:08:45,211 [INFO] [57] VALIDATION loss: 1.748948142716759 VALIDATION acc: 0.8
2025-01-19 05:08:45,211 [INFO] [57] VALIDATION loss dict: {'classification_loss': 1.748948142716759}
2025-01-19 05:08:45,211 [INFO] 
2025-01-19 05:09:05,357 [INFO] Step[50/2713]: training loss : 0.9475025033950806 TRAIN  loss dict:  {'classification_loss': 0.9475025033950806}
2025-01-19 05:09:20,276 [INFO] Step[100/2713]: training loss : 0.9481203508377075 TRAIN  loss dict:  {'classification_loss': 0.9481203508377075}
2025-01-19 05:09:35,197 [INFO] Step[150/2713]: training loss : 0.9485637044906616 TRAIN  loss dict:  {'classification_loss': 0.9485637044906616}
2025-01-19 05:09:50,079 [INFO] Step[200/2713]: training loss : 0.9464811539649963 TRAIN  loss dict:  {'classification_loss': 0.9464811539649963}
2025-01-19 05:10:04,991 [INFO] Step[250/2713]: training loss : 0.9610417592525482 TRAIN  loss dict:  {'classification_loss': 0.9610417592525482}
2025-01-19 05:10:19,861 [INFO] Step[300/2713]: training loss : 0.9525943398475647 TRAIN  loss dict:  {'classification_loss': 0.9525943398475647}
2025-01-19 05:10:34,806 [INFO] Step[350/2713]: training loss : 0.9474112010002136 TRAIN  loss dict:  {'classification_loss': 0.9474112010002136}
2025-01-19 05:10:49,685 [INFO] Step[400/2713]: training loss : 0.9468858444690704 TRAIN  loss dict:  {'classification_loss': 0.9468858444690704}
2025-01-19 05:11:04,604 [INFO] Step[450/2713]: training loss : 0.9491855502128601 TRAIN  loss dict:  {'classification_loss': 0.9491855502128601}
2025-01-19 05:11:19,434 [INFO] Step[500/2713]: training loss : 0.9497489249706268 TRAIN  loss dict:  {'classification_loss': 0.9497489249706268}
2025-01-19 05:11:34,373 [INFO] Step[550/2713]: training loss : 0.9677071607112885 TRAIN  loss dict:  {'classification_loss': 0.9677071607112885}
2025-01-19 05:11:49,276 [INFO] Step[600/2713]: training loss : 0.9575091576576233 TRAIN  loss dict:  {'classification_loss': 0.9575091576576233}
2025-01-19 05:12:04,126 [INFO] Step[650/2713]: training loss : 0.9613357877731323 TRAIN  loss dict:  {'classification_loss': 0.9613357877731323}
2025-01-19 05:12:18,992 [INFO] Step[700/2713]: training loss : 0.9482941830158234 TRAIN  loss dict:  {'classification_loss': 0.9482941830158234}
2025-01-19 05:12:33,881 [INFO] Step[750/2713]: training loss : 0.9502177488803863 TRAIN  loss dict:  {'classification_loss': 0.9502177488803863}
2025-01-19 05:12:48,827 [INFO] Step[800/2713]: training loss : 0.951133725643158 TRAIN  loss dict:  {'classification_loss': 0.951133725643158}
2025-01-19 05:13:03,807 [INFO] Step[850/2713]: training loss : 0.9482060205936432 TRAIN  loss dict:  {'classification_loss': 0.9482060205936432}
2025-01-19 05:13:18,827 [INFO] Step[900/2713]: training loss : 0.9528952503204345 TRAIN  loss dict:  {'classification_loss': 0.9528952503204345}
2025-01-19 05:13:33,736 [INFO] Step[950/2713]: training loss : 0.9460138607025147 TRAIN  loss dict:  {'classification_loss': 0.9460138607025147}
2025-01-19 05:13:48,748 [INFO] Step[1000/2713]: training loss : 0.946923770904541 TRAIN  loss dict:  {'classification_loss': 0.946923770904541}
2025-01-19 05:14:03,848 [INFO] Step[1050/2713]: training loss : 0.9534548091888427 TRAIN  loss dict:  {'classification_loss': 0.9534548091888427}
2025-01-19 05:14:18,868 [INFO] Step[1100/2713]: training loss : 0.9484539270401001 TRAIN  loss dict:  {'classification_loss': 0.9484539270401001}
2025-01-19 05:14:33,830 [INFO] Step[1150/2713]: training loss : 0.9447939431667328 TRAIN  loss dict:  {'classification_loss': 0.9447939431667328}
2025-01-19 05:14:48,756 [INFO] Step[1200/2713]: training loss : 0.9448690855503082 TRAIN  loss dict:  {'classification_loss': 0.9448690855503082}
2025-01-19 05:15:03,734 [INFO] Step[1250/2713]: training loss : 0.9551221668720246 TRAIN  loss dict:  {'classification_loss': 0.9551221668720246}
2025-01-19 05:15:18,645 [INFO] Step[1300/2713]: training loss : 0.9515025365352631 TRAIN  loss dict:  {'classification_loss': 0.9515025365352631}
2025-01-19 05:15:33,696 [INFO] Step[1350/2713]: training loss : 0.9507674384117126 TRAIN  loss dict:  {'classification_loss': 0.9507674384117126}
2025-01-19 05:15:48,652 [INFO] Step[1400/2713]: training loss : 0.9459947896003723 TRAIN  loss dict:  {'classification_loss': 0.9459947896003723}
2025-01-19 05:16:03,748 [INFO] Step[1450/2713]: training loss : 0.9493914663791656 TRAIN  loss dict:  {'classification_loss': 0.9493914663791656}
2025-01-19 05:16:18,787 [INFO] Step[1500/2713]: training loss : 0.9531860816478729 TRAIN  loss dict:  {'classification_loss': 0.9531860816478729}
2025-01-19 05:16:33,877 [INFO] Step[1550/2713]: training loss : 0.9511858153343201 TRAIN  loss dict:  {'classification_loss': 0.9511858153343201}
2025-01-19 05:16:48,907 [INFO] Step[1600/2713]: training loss : 0.9485334467887878 TRAIN  loss dict:  {'classification_loss': 0.9485334467887878}
2025-01-19 05:17:03,971 [INFO] Step[1650/2713]: training loss : 0.9470478475093842 TRAIN  loss dict:  {'classification_loss': 0.9470478475093842}
2025-01-19 05:17:19,024 [INFO] Step[1700/2713]: training loss : 0.953429081439972 TRAIN  loss dict:  {'classification_loss': 0.953429081439972}
2025-01-19 05:17:34,110 [INFO] Step[1750/2713]: training loss : 0.9495952761173249 TRAIN  loss dict:  {'classification_loss': 0.9495952761173249}
2025-01-19 05:17:49,182 [INFO] Step[1800/2713]: training loss : 0.957009414434433 TRAIN  loss dict:  {'classification_loss': 0.957009414434433}
2025-01-19 05:18:04,272 [INFO] Step[1850/2713]: training loss : 0.9510934126377105 TRAIN  loss dict:  {'classification_loss': 0.9510934126377105}
2025-01-19 05:18:19,339 [INFO] Step[1900/2713]: training loss : 0.9463650190830231 TRAIN  loss dict:  {'classification_loss': 0.9463650190830231}
2025-01-19 05:18:34,332 [INFO] Step[1950/2713]: training loss : 0.9495942687988281 TRAIN  loss dict:  {'classification_loss': 0.9495942687988281}
2025-01-19 05:18:49,312 [INFO] Step[2000/2713]: training loss : 0.951519615650177 TRAIN  loss dict:  {'classification_loss': 0.951519615650177}
2025-01-19 05:19:04,318 [INFO] Step[2050/2713]: training loss : 0.9601509296894073 TRAIN  loss dict:  {'classification_loss': 0.9601509296894073}
2025-01-19 05:19:19,300 [INFO] Step[2100/2713]: training loss : 0.9467494809627532 TRAIN  loss dict:  {'classification_loss': 0.9467494809627532}
2025-01-19 05:19:34,271 [INFO] Step[2150/2713]: training loss : 0.945371196269989 TRAIN  loss dict:  {'classification_loss': 0.945371196269989}
2025-01-19 05:19:49,281 [INFO] Step[2200/2713]: training loss : 0.9466897022724151 TRAIN  loss dict:  {'classification_loss': 0.9466897022724151}
2025-01-19 05:20:04,299 [INFO] Step[2250/2713]: training loss : 0.9678956556320191 TRAIN  loss dict:  {'classification_loss': 0.9678956556320191}
2025-01-19 05:20:19,242 [INFO] Step[2300/2713]: training loss : 0.9496971344947815 TRAIN  loss dict:  {'classification_loss': 0.9496971344947815}
2025-01-19 05:20:34,187 [INFO] Step[2350/2713]: training loss : 0.9475979316234588 TRAIN  loss dict:  {'classification_loss': 0.9475979316234588}
2025-01-19 05:20:49,203 [INFO] Step[2400/2713]: training loss : 0.9480908024311065 TRAIN  loss dict:  {'classification_loss': 0.9480908024311065}
2025-01-19 05:21:04,211 [INFO] Step[2450/2713]: training loss : 0.9447800087928772 TRAIN  loss dict:  {'classification_loss': 0.9447800087928772}
2025-01-19 05:21:19,190 [INFO] Step[2500/2713]: training loss : 0.9469688189029694 TRAIN  loss dict:  {'classification_loss': 0.9469688189029694}
2025-01-19 05:21:34,152 [INFO] Step[2550/2713]: training loss : 0.9506046140193939 TRAIN  loss dict:  {'classification_loss': 0.9506046140193939}
2025-01-19 05:21:49,185 [INFO] Step[2600/2713]: training loss : 0.9465708458423614 TRAIN  loss dict:  {'classification_loss': 0.9465708458423614}
2025-01-19 05:22:04,228 [INFO] Step[2650/2713]: training loss : 0.9516743659973145 TRAIN  loss dict:  {'classification_loss': 0.9516743659973145}
2025-01-19 05:22:19,212 [INFO] Step[2700/2713]: training loss : 0.9487168204784393 TRAIN  loss dict:  {'classification_loss': 0.9487168204784393}
2025-01-19 05:23:39,372 [INFO] Label accuracies statistics:
2025-01-19 05:23:39,372 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.25, 68: 1.0, 69: 0.75, 70: 1.0, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.25, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.5, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.5, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 05:23:39,374 [INFO] [58] TRAIN  loss: 0.9505919461271396 acc: 0.9990170782651432
2025-01-19 05:23:39,374 [INFO] [58] TRAIN  loss dict: {'classification_loss': 0.9505919461271396}
2025-01-19 05:23:39,374 [INFO] [58] VALIDATION loss: 1.7297146739368152 VALIDATION acc: 0.8075235109717869
2025-01-19 05:23:39,374 [INFO] [58] VALIDATION loss dict: {'classification_loss': 1.7297146739368152}
2025-01-19 05:23:39,374 [INFO] 
2025-01-19 05:23:59,369 [INFO] Step[50/2713]: training loss : 0.9508419275283814 TRAIN  loss dict:  {'classification_loss': 0.9508419275283814}
2025-01-19 05:24:14,411 [INFO] Step[100/2713]: training loss : 0.9493082427978515 TRAIN  loss dict:  {'classification_loss': 0.9493082427978515}
2025-01-19 05:24:29,474 [INFO] Step[150/2713]: training loss : 0.9487104511260986 TRAIN  loss dict:  {'classification_loss': 0.9487104511260986}
2025-01-19 05:24:44,585 [INFO] Step[200/2713]: training loss : 0.94883664727211 TRAIN  loss dict:  {'classification_loss': 0.94883664727211}
2025-01-19 05:24:59,654 [INFO] Step[250/2713]: training loss : 0.9449971163272858 TRAIN  loss dict:  {'classification_loss': 0.9449971163272858}
2025-01-19 05:25:14,680 [INFO] Step[300/2713]: training loss : 0.9504936909675599 TRAIN  loss dict:  {'classification_loss': 0.9504936909675599}
2025-01-19 05:25:29,791 [INFO] Step[350/2713]: training loss : 0.9494205963611603 TRAIN  loss dict:  {'classification_loss': 0.9494205963611603}
2025-01-19 05:25:44,861 [INFO] Step[400/2713]: training loss : 0.9453148090839386 TRAIN  loss dict:  {'classification_loss': 0.9453148090839386}
2025-01-19 05:25:59,959 [INFO] Step[450/2713]: training loss : 0.9494435954093933 TRAIN  loss dict:  {'classification_loss': 0.9494435954093933}
2025-01-19 05:26:14,995 [INFO] Step[500/2713]: training loss : 0.9455823540687561 TRAIN  loss dict:  {'classification_loss': 0.9455823540687561}
2025-01-19 05:26:30,063 [INFO] Step[550/2713]: training loss : 0.9495177638530731 TRAIN  loss dict:  {'classification_loss': 0.9495177638530731}
2025-01-19 05:26:45,205 [INFO] Step[600/2713]: training loss : 0.9496095216274262 TRAIN  loss dict:  {'classification_loss': 0.9496095216274262}
2025-01-19 05:27:00,312 [INFO] Step[650/2713]: training loss : 0.9547300887107849 TRAIN  loss dict:  {'classification_loss': 0.9547300887107849}
2025-01-19 05:27:15,356 [INFO] Step[700/2713]: training loss : 0.9477975690364837 TRAIN  loss dict:  {'classification_loss': 0.9477975690364837}
2025-01-19 05:27:30,449 [INFO] Step[750/2713]: training loss : 0.9530205798149108 TRAIN  loss dict:  {'classification_loss': 0.9530205798149108}
2025-01-19 05:27:45,519 [INFO] Step[800/2713]: training loss : 0.948374570608139 TRAIN  loss dict:  {'classification_loss': 0.948374570608139}
2025-01-19 05:28:00,643 [INFO] Step[850/2713]: training loss : 0.9458281660079956 TRAIN  loss dict:  {'classification_loss': 0.9458281660079956}
2025-01-19 05:28:15,778 [INFO] Step[900/2713]: training loss : 0.9481552004814148 TRAIN  loss dict:  {'classification_loss': 0.9481552004814148}
2025-01-19 05:28:30,921 [INFO] Step[950/2713]: training loss : 0.9482194638252258 TRAIN  loss dict:  {'classification_loss': 0.9482194638252258}
2025-01-19 05:28:46,049 [INFO] Step[1000/2713]: training loss : 0.9479708039760589 TRAIN  loss dict:  {'classification_loss': 0.9479708039760589}
2025-01-19 05:29:01,175 [INFO] Step[1050/2713]: training loss : 0.9465206909179688 TRAIN  loss dict:  {'classification_loss': 0.9465206909179688}
2025-01-19 05:29:16,338 [INFO] Step[1100/2713]: training loss : 0.9506071639060975 TRAIN  loss dict:  {'classification_loss': 0.9506071639060975}
2025-01-19 05:29:31,459 [INFO] Step[1150/2713]: training loss : 0.9478188192844391 TRAIN  loss dict:  {'classification_loss': 0.9478188192844391}
2025-01-19 05:29:46,630 [INFO] Step[1200/2713]: training loss : 0.9539072394371033 TRAIN  loss dict:  {'classification_loss': 0.9539072394371033}
2025-01-19 05:30:01,823 [INFO] Step[1250/2713]: training loss : 0.9557474219799041 TRAIN  loss dict:  {'classification_loss': 0.9557474219799041}
2025-01-19 05:30:16,980 [INFO] Step[1300/2713]: training loss : 0.9489264988899231 TRAIN  loss dict:  {'classification_loss': 0.9489264988899231}
2025-01-19 05:30:32,097 [INFO] Step[1350/2713]: training loss : 0.9518652594089508 TRAIN  loss dict:  {'classification_loss': 0.9518652594089508}
2025-01-19 05:30:47,172 [INFO] Step[1400/2713]: training loss : 0.9535526967048645 TRAIN  loss dict:  {'classification_loss': 0.9535526967048645}
2025-01-19 05:31:02,232 [INFO] Step[1450/2713]: training loss : 0.9469135439395905 TRAIN  loss dict:  {'classification_loss': 0.9469135439395905}
2025-01-19 05:31:17,292 [INFO] Step[1500/2713]: training loss : 0.9543922674655915 TRAIN  loss dict:  {'classification_loss': 0.9543922674655915}
2025-01-19 05:31:32,428 [INFO] Step[1550/2713]: training loss : 0.9477652621269226 TRAIN  loss dict:  {'classification_loss': 0.9477652621269226}
2025-01-19 05:31:47,563 [INFO] Step[1600/2713]: training loss : 0.9585239124298096 TRAIN  loss dict:  {'classification_loss': 0.9585239124298096}
2025-01-19 05:32:02,639 [INFO] Step[1650/2713]: training loss : 0.96254545211792 TRAIN  loss dict:  {'classification_loss': 0.96254545211792}
2025-01-19 05:32:17,780 [INFO] Step[1700/2713]: training loss : 0.9497440469264984 TRAIN  loss dict:  {'classification_loss': 0.9497440469264984}
2025-01-19 05:32:32,951 [INFO] Step[1750/2713]: training loss : 0.9495870435237884 TRAIN  loss dict:  {'classification_loss': 0.9495870435237884}
2025-01-19 05:32:48,107 [INFO] Step[1800/2713]: training loss : 0.9520004415512084 TRAIN  loss dict:  {'classification_loss': 0.9520004415512084}
2025-01-19 05:33:03,272 [INFO] Step[1850/2713]: training loss : 0.94758828997612 TRAIN  loss dict:  {'classification_loss': 0.94758828997612}
2025-01-19 05:33:18,421 [INFO] Step[1900/2713]: training loss : 0.9498526704311371 TRAIN  loss dict:  {'classification_loss': 0.9498526704311371}
2025-01-19 05:33:33,596 [INFO] Step[1950/2713]: training loss : 0.9482934367656708 TRAIN  loss dict:  {'classification_loss': 0.9482934367656708}
2025-01-19 05:33:48,723 [INFO] Step[2000/2713]: training loss : 0.955544673204422 TRAIN  loss dict:  {'classification_loss': 0.955544673204422}
2025-01-19 05:34:03,874 [INFO] Step[2050/2713]: training loss : 0.9706693410873413 TRAIN  loss dict:  {'classification_loss': 0.9706693410873413}
2025-01-19 05:34:18,999 [INFO] Step[2100/2713]: training loss : 0.9534353709220886 TRAIN  loss dict:  {'classification_loss': 0.9534353709220886}
2025-01-19 05:34:34,137 [INFO] Step[2150/2713]: training loss : 0.9479371440410614 TRAIN  loss dict:  {'classification_loss': 0.9479371440410614}
2025-01-19 05:34:49,320 [INFO] Step[2200/2713]: training loss : 0.9540420627593994 TRAIN  loss dict:  {'classification_loss': 0.9540420627593994}
2025-01-19 05:35:04,516 [INFO] Step[2250/2713]: training loss : 0.9561900568008422 TRAIN  loss dict:  {'classification_loss': 0.9561900568008422}
2025-01-19 05:35:19,687 [INFO] Step[2300/2713]: training loss : 0.9533445847034454 TRAIN  loss dict:  {'classification_loss': 0.9533445847034454}
2025-01-19 05:35:34,831 [INFO] Step[2350/2713]: training loss : 0.9489842224121093 TRAIN  loss dict:  {'classification_loss': 0.9489842224121093}
2025-01-19 05:35:49,988 [INFO] Step[2400/2713]: training loss : 0.9505586612224579 TRAIN  loss dict:  {'classification_loss': 0.9505586612224579}
2025-01-19 05:36:05,130 [INFO] Step[2450/2713]: training loss : 0.9506424498558045 TRAIN  loss dict:  {'classification_loss': 0.9506424498558045}
2025-01-19 05:36:20,279 [INFO] Step[2500/2713]: training loss : 0.9490947270393372 TRAIN  loss dict:  {'classification_loss': 0.9490947270393372}
2025-01-19 05:36:35,416 [INFO] Step[2550/2713]: training loss : 0.9492196440696716 TRAIN  loss dict:  {'classification_loss': 0.9492196440696716}
2025-01-19 05:36:50,332 [INFO] Step[2600/2713]: training loss : 0.9507415294647217 TRAIN  loss dict:  {'classification_loss': 0.9507415294647217}
2025-01-19 05:37:05,187 [INFO] Step[2650/2713]: training loss : 0.9581919646263123 TRAIN  loss dict:  {'classification_loss': 0.9581919646263123}
2025-01-19 05:37:20,135 [INFO] Step[2700/2713]: training loss : 0.9574999463558197 TRAIN  loss dict:  {'classification_loss': 0.9574999463558197}
2025-01-19 05:38:39,629 [INFO] Label accuracies statistics:
2025-01-19 05:38:39,630 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 0.75, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.25, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 05:38:39,631 [INFO] [59] TRAIN  loss: 0.951055382016924 acc: 0.9995085391325715
2025-01-19 05:38:39,631 [INFO] [59] TRAIN  loss dict: {'classification_loss': 0.951055382016924}
2025-01-19 05:38:39,632 [INFO] [59] VALIDATION loss: 1.7505283785941905 VALIDATION acc: 0.7956112852664576
2025-01-19 05:38:39,632 [INFO] [59] VALIDATION loss dict: {'classification_loss': 1.7505283785941905}
2025-01-19 05:38:39,632 [INFO] 
2025-01-19 05:38:59,836 [INFO] Step[50/2713]: training loss : 0.9578678107261658 TRAIN  loss dict:  {'classification_loss': 0.9578678107261658}
2025-01-19 05:39:14,641 [INFO] Step[100/2713]: training loss : 0.9523326194286347 TRAIN  loss dict:  {'classification_loss': 0.9523326194286347}
2025-01-19 05:39:29,457 [INFO] Step[150/2713]: training loss : 0.9485589802265167 TRAIN  loss dict:  {'classification_loss': 0.9485589802265167}
2025-01-19 05:39:44,244 [INFO] Step[200/2713]: training loss : 0.9504593443870545 TRAIN  loss dict:  {'classification_loss': 0.9504593443870545}
2025-01-19 05:39:59,056 [INFO] Step[250/2713]: training loss : 0.9523352098464966 TRAIN  loss dict:  {'classification_loss': 0.9523352098464966}
2025-01-19 05:40:13,810 [INFO] Step[300/2713]: training loss : 0.952679443359375 TRAIN  loss dict:  {'classification_loss': 0.952679443359375}
2025-01-19 05:40:28,618 [INFO] Step[350/2713]: training loss : 0.9451818227767944 TRAIN  loss dict:  {'classification_loss': 0.9451818227767944}
2025-01-19 05:40:43,454 [INFO] Step[400/2713]: training loss : 0.9437434542179107 TRAIN  loss dict:  {'classification_loss': 0.9437434542179107}
2025-01-19 05:40:58,247 [INFO] Step[450/2713]: training loss : 0.947467782497406 TRAIN  loss dict:  {'classification_loss': 0.947467782497406}
2025-01-19 05:41:13,047 [INFO] Step[500/2713]: training loss : 0.9433770537376404 TRAIN  loss dict:  {'classification_loss': 0.9433770537376404}
2025-01-19 05:41:27,851 [INFO] Step[550/2713]: training loss : 0.9504319310188294 TRAIN  loss dict:  {'classification_loss': 0.9504319310188294}
2025-01-19 05:41:42,664 [INFO] Step[600/2713]: training loss : 0.9488438844680787 TRAIN  loss dict:  {'classification_loss': 0.9488438844680787}
2025-01-19 05:41:57,502 [INFO] Step[650/2713]: training loss : 0.942757054567337 TRAIN  loss dict:  {'classification_loss': 0.942757054567337}
2025-01-19 05:42:12,349 [INFO] Step[700/2713]: training loss : 0.9479116535186768 TRAIN  loss dict:  {'classification_loss': 0.9479116535186768}
2025-01-19 05:42:27,185 [INFO] Step[750/2713]: training loss : 0.9506322824954987 TRAIN  loss dict:  {'classification_loss': 0.9506322824954987}
2025-01-19 05:42:41,993 [INFO] Step[800/2713]: training loss : 0.9510987985134125 TRAIN  loss dict:  {'classification_loss': 0.9510987985134125}
2025-01-19 05:42:56,809 [INFO] Step[850/2713]: training loss : 0.9482158291339874 TRAIN  loss dict:  {'classification_loss': 0.9482158291339874}
2025-01-19 05:43:11,639 [INFO] Step[900/2713]: training loss : 0.9527174496650695 TRAIN  loss dict:  {'classification_loss': 0.9527174496650695}
2025-01-19 05:43:26,471 [INFO] Step[950/2713]: training loss : 0.9557035624980926 TRAIN  loss dict:  {'classification_loss': 0.9557035624980926}
2025-01-19 05:43:41,220 [INFO] Step[1000/2713]: training loss : 0.9505086421966553 TRAIN  loss dict:  {'classification_loss': 0.9505086421966553}
2025-01-19 05:43:56,052 [INFO] Step[1050/2713]: training loss : 0.9470329201221466 TRAIN  loss dict:  {'classification_loss': 0.9470329201221466}
2025-01-19 05:44:10,845 [INFO] Step[1100/2713]: training loss : 0.9526153731346131 TRAIN  loss dict:  {'classification_loss': 0.9526153731346131}
2025-01-19 05:44:25,575 [INFO] Step[1150/2713]: training loss : 0.9559939062595367 TRAIN  loss dict:  {'classification_loss': 0.9559939062595367}
2025-01-19 05:44:40,378 [INFO] Step[1200/2713]: training loss : 0.9539745080471039 TRAIN  loss dict:  {'classification_loss': 0.9539745080471039}
2025-01-19 05:44:55,165 [INFO] Step[1250/2713]: training loss : 0.9499025297164917 TRAIN  loss dict:  {'classification_loss': 0.9499025297164917}
2025-01-19 05:45:09,998 [INFO] Step[1300/2713]: training loss : 0.9515653777122498 TRAIN  loss dict:  {'classification_loss': 0.9515653777122498}
2025-01-19 05:45:24,827 [INFO] Step[1350/2713]: training loss : 0.9460901033878326 TRAIN  loss dict:  {'classification_loss': 0.9460901033878326}
2025-01-19 05:45:39,640 [INFO] Step[1400/2713]: training loss : 0.9466430997848511 TRAIN  loss dict:  {'classification_loss': 0.9466430997848511}
2025-01-19 05:45:54,478 [INFO] Step[1450/2713]: training loss : 0.9460351037979126 TRAIN  loss dict:  {'classification_loss': 0.9460351037979126}
2025-01-19 05:46:09,289 [INFO] Step[1500/2713]: training loss : 0.9469151020050048 TRAIN  loss dict:  {'classification_loss': 0.9469151020050048}
2025-01-19 05:46:24,102 [INFO] Step[1550/2713]: training loss : 0.9566103827953338 TRAIN  loss dict:  {'classification_loss': 0.9566103827953338}
2025-01-19 05:46:38,930 [INFO] Step[1600/2713]: training loss : 0.9626836967468262 TRAIN  loss dict:  {'classification_loss': 0.9626836967468262}
2025-01-19 05:46:53,715 [INFO] Step[1650/2713]: training loss : 0.9550622177124023 TRAIN  loss dict:  {'classification_loss': 0.9550622177124023}
2025-01-19 05:47:08,509 [INFO] Step[1700/2713]: training loss : 0.9452106535434723 TRAIN  loss dict:  {'classification_loss': 0.9452106535434723}
2025-01-19 05:47:23,346 [INFO] Step[1750/2713]: training loss : 0.9480262804031372 TRAIN  loss dict:  {'classification_loss': 0.9480262804031372}
2025-01-19 05:47:38,172 [INFO] Step[1800/2713]: training loss : 0.9467183482646943 TRAIN  loss dict:  {'classification_loss': 0.9467183482646943}
2025-01-19 05:47:52,993 [INFO] Step[1850/2713]: training loss : 0.9449477076530457 TRAIN  loss dict:  {'classification_loss': 0.9449477076530457}
2025-01-19 05:48:07,805 [INFO] Step[1900/2713]: training loss : 0.9464426136016846 TRAIN  loss dict:  {'classification_loss': 0.9464426136016846}
2025-01-19 05:48:22,647 [INFO] Step[1950/2713]: training loss : 0.9501024472713471 TRAIN  loss dict:  {'classification_loss': 0.9501024472713471}
2025-01-19 05:48:37,443 [INFO] Step[2000/2713]: training loss : 0.9483190131187439 TRAIN  loss dict:  {'classification_loss': 0.9483190131187439}
2025-01-19 05:48:52,302 [INFO] Step[2050/2713]: training loss : 0.9484044384956359 TRAIN  loss dict:  {'classification_loss': 0.9484044384956359}
2025-01-19 05:49:07,121 [INFO] Step[2100/2713]: training loss : 0.9458751177787781 TRAIN  loss dict:  {'classification_loss': 0.9458751177787781}
2025-01-19 05:49:21,932 [INFO] Step[2150/2713]: training loss : 0.9455411386489868 TRAIN  loss dict:  {'classification_loss': 0.9455411386489868}
2025-01-19 05:49:36,703 [INFO] Step[2200/2713]: training loss : 0.9513550698757172 TRAIN  loss dict:  {'classification_loss': 0.9513550698757172}
2025-01-19 05:49:51,530 [INFO] Step[2250/2713]: training loss : 0.9443807220458984 TRAIN  loss dict:  {'classification_loss': 0.9443807220458984}
2025-01-19 05:50:06,338 [INFO] Step[2300/2713]: training loss : 0.9516277897357941 TRAIN  loss dict:  {'classification_loss': 0.9516277897357941}
2025-01-19 05:50:21,133 [INFO] Step[2350/2713]: training loss : 0.9513059461116791 TRAIN  loss dict:  {'classification_loss': 0.9513059461116791}
2025-01-19 05:50:35,932 [INFO] Step[2400/2713]: training loss : 0.9476555728912354 TRAIN  loss dict:  {'classification_loss': 0.9476555728912354}
2025-01-19 05:50:50,803 [INFO] Step[2450/2713]: training loss : 0.9471664714813233 TRAIN  loss dict:  {'classification_loss': 0.9471664714813233}
2025-01-19 05:51:05,592 [INFO] Step[2500/2713]: training loss : 0.9575456869602204 TRAIN  loss dict:  {'classification_loss': 0.9575456869602204}
2025-01-19 05:51:20,397 [INFO] Step[2550/2713]: training loss : 0.9449004638195038 TRAIN  loss dict:  {'classification_loss': 0.9449004638195038}
2025-01-19 05:51:35,214 [INFO] Step[2600/2713]: training loss : 0.9479993379116058 TRAIN  loss dict:  {'classification_loss': 0.9479993379116058}
2025-01-19 05:51:50,045 [INFO] Step[2650/2713]: training loss : 0.9464733529090882 TRAIN  loss dict:  {'classification_loss': 0.9464733529090882}
2025-01-19 05:52:04,926 [INFO] Step[2700/2713]: training loss : 0.9456801986694336 TRAIN  loss dict:  {'classification_loss': 0.9456801986694336}
2025-01-19 05:53:24,784 [INFO] Label accuracies statistics:
2025-01-19 05:53:24,785 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 1.0, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.5, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 1.0, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 1.0, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 1.0, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 05:53:24,787 [INFO] [60] TRAIN  loss: 0.9494734734483544 acc: 0.9998771347831429
2025-01-19 05:53:24,787 [INFO] [60] TRAIN  loss dict: {'classification_loss': 0.9494734734483544}
2025-01-19 05:53:24,787 [INFO] [60] VALIDATION loss: 1.7231168865709376 VALIDATION acc: 0.8037617554858935
2025-01-19 05:53:24,787 [INFO] [60] VALIDATION loss dict: {'classification_loss': 1.7231168865709376}
2025-01-19 05:53:24,787 [INFO] 
2025-01-19 05:53:45,407 [INFO] Step[50/2713]: training loss : 0.94800008893013 TRAIN  loss dict:  {'classification_loss': 0.94800008893013}
2025-01-19 05:54:00,269 [INFO] Step[100/2713]: training loss : 0.9469667875766754 TRAIN  loss dict:  {'classification_loss': 0.9469667875766754}
2025-01-19 05:54:15,093 [INFO] Step[150/2713]: training loss : 0.9476893305778503 TRAIN  loss dict:  {'classification_loss': 0.9476893305778503}
2025-01-19 05:54:29,926 [INFO] Step[200/2713]: training loss : 0.9487821269035339 TRAIN  loss dict:  {'classification_loss': 0.9487821269035339}
2025-01-19 05:54:44,785 [INFO] Step[250/2713]: training loss : 0.9468420541286469 TRAIN  loss dict:  {'classification_loss': 0.9468420541286469}
2025-01-19 05:54:59,625 [INFO] Step[300/2713]: training loss : 0.9430903804302215 TRAIN  loss dict:  {'classification_loss': 0.9430903804302215}
2025-01-19 05:55:14,436 [INFO] Step[350/2713]: training loss : 0.9438417983055115 TRAIN  loss dict:  {'classification_loss': 0.9438417983055115}
2025-01-19 05:55:29,262 [INFO] Step[400/2713]: training loss : 0.9454461979866028 TRAIN  loss dict:  {'classification_loss': 0.9454461979866028}
2025-01-19 05:55:44,019 [INFO] Step[450/2713]: training loss : 0.9466818785667419 TRAIN  loss dict:  {'classification_loss': 0.9466818785667419}
2025-01-19 05:55:58,883 [INFO] Step[500/2713]: training loss : 0.9428008699417114 TRAIN  loss dict:  {'classification_loss': 0.9428008699417114}
2025-01-19 05:56:13,665 [INFO] Step[550/2713]: training loss : 0.9507586038112641 TRAIN  loss dict:  {'classification_loss': 0.9507586038112641}
2025-01-19 05:56:28,445 [INFO] Step[600/2713]: training loss : 0.9501665186882019 TRAIN  loss dict:  {'classification_loss': 0.9501665186882019}
2025-01-19 05:56:43,259 [INFO] Step[650/2713]: training loss : 0.9468960225582123 TRAIN  loss dict:  {'classification_loss': 0.9468960225582123}
2025-01-19 05:56:58,079 [INFO] Step[700/2713]: training loss : 0.9450323522090912 TRAIN  loss dict:  {'classification_loss': 0.9450323522090912}
2025-01-19 05:57:12,852 [INFO] Step[750/2713]: training loss : 0.9538263618946076 TRAIN  loss dict:  {'classification_loss': 0.9538263618946076}
2025-01-19 05:57:27,669 [INFO] Step[800/2713]: training loss : 0.9624988985061645 TRAIN  loss dict:  {'classification_loss': 0.9624988985061645}
2025-01-19 05:57:42,470 [INFO] Step[850/2713]: training loss : 0.9446124339103699 TRAIN  loss dict:  {'classification_loss': 0.9446124339103699}
2025-01-19 05:57:57,268 [INFO] Step[900/2713]: training loss : 0.9475035548210144 TRAIN  loss dict:  {'classification_loss': 0.9475035548210144}
2025-01-19 05:58:12,082 [INFO] Step[950/2713]: training loss : 0.9430959129333496 TRAIN  loss dict:  {'classification_loss': 0.9430959129333496}
2025-01-19 05:58:26,966 [INFO] Step[1000/2713]: training loss : 0.9489266979694366 TRAIN  loss dict:  {'classification_loss': 0.9489266979694366}
2025-01-19 05:58:41,770 [INFO] Step[1050/2713]: training loss : 0.9460767424106598 TRAIN  loss dict:  {'classification_loss': 0.9460767424106598}
2025-01-19 05:58:56,590 [INFO] Step[1100/2713]: training loss : 0.9445933365821838 TRAIN  loss dict:  {'classification_loss': 0.9445933365821838}
2025-01-19 05:59:11,414 [INFO] Step[1150/2713]: training loss : 0.9502697086334229 TRAIN  loss dict:  {'classification_loss': 0.9502697086334229}
2025-01-19 05:59:26,247 [INFO] Step[1200/2713]: training loss : 0.9450667309761047 TRAIN  loss dict:  {'classification_loss': 0.9450667309761047}
2025-01-19 05:59:41,081 [INFO] Step[1250/2713]: training loss : 0.9439778041839599 TRAIN  loss dict:  {'classification_loss': 0.9439778041839599}
2025-01-19 05:59:55,894 [INFO] Step[1300/2713]: training loss : 0.9437731647491455 TRAIN  loss dict:  {'classification_loss': 0.9437731647491455}
2025-01-19 06:00:10,734 [INFO] Step[1350/2713]: training loss : 0.9492377674579621 TRAIN  loss dict:  {'classification_loss': 0.9492377674579621}
2025-01-19 06:00:25,553 [INFO] Step[1400/2713]: training loss : 0.9444037079811096 TRAIN  loss dict:  {'classification_loss': 0.9444037079811096}
2025-01-19 06:00:40,375 [INFO] Step[1450/2713]: training loss : 0.9421502256393433 TRAIN  loss dict:  {'classification_loss': 0.9421502256393433}
2025-01-19 06:00:55,190 [INFO] Step[1500/2713]: training loss : 0.9452349889278412 TRAIN  loss dict:  {'classification_loss': 0.9452349889278412}
2025-01-19 06:01:10,007 [INFO] Step[1550/2713]: training loss : 0.9549603068828583 TRAIN  loss dict:  {'classification_loss': 0.9549603068828583}
2025-01-19 06:01:24,824 [INFO] Step[1600/2713]: training loss : 0.9520814824104309 TRAIN  loss dict:  {'classification_loss': 0.9520814824104309}
2025-01-19 06:01:39,623 [INFO] Step[1650/2713]: training loss : 0.9452406752109528 TRAIN  loss dict:  {'classification_loss': 0.9452406752109528}
2025-01-19 06:01:54,445 [INFO] Step[1700/2713]: training loss : 0.946111216545105 TRAIN  loss dict:  {'classification_loss': 0.946111216545105}
2025-01-19 06:02:09,323 [INFO] Step[1750/2713]: training loss : 0.9472612345218658 TRAIN  loss dict:  {'classification_loss': 0.9472612345218658}
2025-01-19 06:02:24,217 [INFO] Step[1800/2713]: training loss : 0.946120730638504 TRAIN  loss dict:  {'classification_loss': 0.946120730638504}
2025-01-19 06:02:39,009 [INFO] Step[1850/2713]: training loss : 0.9434602689743042 TRAIN  loss dict:  {'classification_loss': 0.9434602689743042}
2025-01-19 06:02:53,831 [INFO] Step[1900/2713]: training loss : 0.9465470778942108 TRAIN  loss dict:  {'classification_loss': 0.9465470778942108}
2025-01-19 06:03:08,612 [INFO] Step[1950/2713]: training loss : 0.9512462735176086 TRAIN  loss dict:  {'classification_loss': 0.9512462735176086}
2025-01-19 06:03:23,432 [INFO] Step[2000/2713]: training loss : 0.9459498131275177 TRAIN  loss dict:  {'classification_loss': 0.9459498131275177}
2025-01-19 06:03:38,318 [INFO] Step[2050/2713]: training loss : 0.9444937825202941 TRAIN  loss dict:  {'classification_loss': 0.9444937825202941}
2025-01-19 06:03:53,139 [INFO] Step[2100/2713]: training loss : 0.9587564384937286 TRAIN  loss dict:  {'classification_loss': 0.9587564384937286}
2025-01-19 06:04:07,940 [INFO] Step[2150/2713]: training loss : 0.9459336960315704 TRAIN  loss dict:  {'classification_loss': 0.9459336960315704}
2025-01-19 06:04:22,732 [INFO] Step[2200/2713]: training loss : 0.9442289388179779 TRAIN  loss dict:  {'classification_loss': 0.9442289388179779}
2025-01-19 06:04:37,572 [INFO] Step[2250/2713]: training loss : 0.9424283874034881 TRAIN  loss dict:  {'classification_loss': 0.9424283874034881}
2025-01-19 06:04:52,363 [INFO] Step[2300/2713]: training loss : 0.9458727097511291 TRAIN  loss dict:  {'classification_loss': 0.9458727097511291}
2025-01-19 06:05:07,182 [INFO] Step[2350/2713]: training loss : 0.9482281243801117 TRAIN  loss dict:  {'classification_loss': 0.9482281243801117}
2025-01-19 06:05:21,982 [INFO] Step[2400/2713]: training loss : 0.9472430527210236 TRAIN  loss dict:  {'classification_loss': 0.9472430527210236}
2025-01-19 06:05:36,802 [INFO] Step[2450/2713]: training loss : 0.9525975406169891 TRAIN  loss dict:  {'classification_loss': 0.9525975406169891}
2025-01-19 06:05:51,618 [INFO] Step[2500/2713]: training loss : 0.951397488117218 TRAIN  loss dict:  {'classification_loss': 0.951397488117218}
2025-01-19 06:06:06,409 [INFO] Step[2550/2713]: training loss : 0.9444827008247375 TRAIN  loss dict:  {'classification_loss': 0.9444827008247375}
2025-01-19 06:06:21,180 [INFO] Step[2600/2713]: training loss : 0.9520133507251739 TRAIN  loss dict:  {'classification_loss': 0.9520133507251739}
2025-01-19 06:06:35,979 [INFO] Step[2650/2713]: training loss : 0.9512335121631622 TRAIN  loss dict:  {'classification_loss': 0.9512335121631622}
2025-01-19 06:06:50,825 [INFO] Step[2700/2713]: training loss : 0.9472192907333374 TRAIN  loss dict:  {'classification_loss': 0.9472192907333374}
2025-01-19 06:08:10,405 [INFO] Label accuracies statistics:
2025-01-19 06:08:10,405 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 1.0, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 0.75, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 06:08:10,407 [INFO] [61] TRAIN  loss: 0.9474325098764663 acc: 0.9995085391325715
2025-01-19 06:08:10,407 [INFO] [61] TRAIN  loss dict: {'classification_loss': 0.9474325098764663}
2025-01-19 06:08:10,407 [INFO] [61] VALIDATION loss: 1.7310789966941775 VALIDATION acc: 0.7987460815047022
2025-01-19 06:08:10,408 [INFO] [61] VALIDATION loss dict: {'classification_loss': 1.7310789966941775}
2025-01-19 06:08:10,408 [INFO] 
2025-01-19 06:08:30,440 [INFO] Step[50/2713]: training loss : 0.9460539436340332 TRAIN  loss dict:  {'classification_loss': 0.9460539436340332}
2025-01-19 06:08:45,251 [INFO] Step[100/2713]: training loss : 0.9468651437759399 TRAIN  loss dict:  {'classification_loss': 0.9468651437759399}
2025-01-19 06:09:00,106 [INFO] Step[150/2713]: training loss : 0.944617657661438 TRAIN  loss dict:  {'classification_loss': 0.944617657661438}
2025-01-19 06:09:14,976 [INFO] Step[200/2713]: training loss : 0.9493080854415894 TRAIN  loss dict:  {'classification_loss': 0.9493080854415894}
2025-01-19 06:09:29,879 [INFO] Step[250/2713]: training loss : 0.9532473278045654 TRAIN  loss dict:  {'classification_loss': 0.9532473278045654}
2025-01-19 06:09:44,778 [INFO] Step[300/2713]: training loss : 0.9435067069530487 TRAIN  loss dict:  {'classification_loss': 0.9435067069530487}
2025-01-19 06:09:59,649 [INFO] Step[350/2713]: training loss : 0.9443100738525391 TRAIN  loss dict:  {'classification_loss': 0.9443100738525391}
2025-01-19 06:10:14,620 [INFO] Step[400/2713]: training loss : 0.9471437072753907 TRAIN  loss dict:  {'classification_loss': 0.9471437072753907}
2025-01-19 06:10:29,774 [INFO] Step[450/2713]: training loss : 0.9481642007827759 TRAIN  loss dict:  {'classification_loss': 0.9481642007827759}
2025-01-19 06:10:44,838 [INFO] Step[500/2713]: training loss : 0.9492994892597199 TRAIN  loss dict:  {'classification_loss': 0.9492994892597199}
2025-01-19 06:10:59,971 [INFO] Step[550/2713]: training loss : 0.9547083020210266 TRAIN  loss dict:  {'classification_loss': 0.9547083020210266}
2025-01-19 06:11:15,112 [INFO] Step[600/2713]: training loss : 0.9459601736068726 TRAIN  loss dict:  {'classification_loss': 0.9459601736068726}
2025-01-19 06:11:30,240 [INFO] Step[650/2713]: training loss : 0.9479867696762085 TRAIN  loss dict:  {'classification_loss': 0.9479867696762085}
2025-01-19 06:11:45,339 [INFO] Step[700/2713]: training loss : 0.9465942823886871 TRAIN  loss dict:  {'classification_loss': 0.9465942823886871}
2025-01-19 06:12:00,463 [INFO] Step[750/2713]: training loss : 0.9467233157157898 TRAIN  loss dict:  {'classification_loss': 0.9467233157157898}
2025-01-19 06:12:15,545 [INFO] Step[800/2713]: training loss : 0.945927814245224 TRAIN  loss dict:  {'classification_loss': 0.945927814245224}
2025-01-19 06:12:30,625 [INFO] Step[850/2713]: training loss : 0.9640430903434754 TRAIN  loss dict:  {'classification_loss': 0.9640430903434754}
2025-01-19 06:12:45,747 [INFO] Step[900/2713]: training loss : 0.9446585524082184 TRAIN  loss dict:  {'classification_loss': 0.9446585524082184}
2025-01-19 06:13:00,884 [INFO] Step[950/2713]: training loss : 0.9450430297851562 TRAIN  loss dict:  {'classification_loss': 0.9450430297851562}
2025-01-19 06:13:15,998 [INFO] Step[1000/2713]: training loss : 0.9446601271629333 TRAIN  loss dict:  {'classification_loss': 0.9446601271629333}
2025-01-19 06:13:31,083 [INFO] Step[1050/2713]: training loss : 0.9494030499458312 TRAIN  loss dict:  {'classification_loss': 0.9494030499458312}
2025-01-19 06:13:46,194 [INFO] Step[1100/2713]: training loss : 0.9500504601001739 TRAIN  loss dict:  {'classification_loss': 0.9500504601001739}
2025-01-19 06:14:01,263 [INFO] Step[1150/2713]: training loss : 0.9561688125133514 TRAIN  loss dict:  {'classification_loss': 0.9561688125133514}
2025-01-19 06:14:16,357 [INFO] Step[1200/2713]: training loss : 0.9472194159030914 TRAIN  loss dict:  {'classification_loss': 0.9472194159030914}
2025-01-19 06:14:31,420 [INFO] Step[1250/2713]: training loss : 0.9411023676395416 TRAIN  loss dict:  {'classification_loss': 0.9411023676395416}
2025-01-19 06:14:46,538 [INFO] Step[1300/2713]: training loss : 0.9435448455810547 TRAIN  loss dict:  {'classification_loss': 0.9435448455810547}
2025-01-19 06:15:01,648 [INFO] Step[1350/2713]: training loss : 0.9457888841629029 TRAIN  loss dict:  {'classification_loss': 0.9457888841629029}
2025-01-19 06:15:16,791 [INFO] Step[1400/2713]: training loss : 0.9464603912830353 TRAIN  loss dict:  {'classification_loss': 0.9464603912830353}
2025-01-19 06:15:31,933 [INFO] Step[1450/2713]: training loss : 0.9478587257862091 TRAIN  loss dict:  {'classification_loss': 0.9478587257862091}
2025-01-19 06:15:47,051 [INFO] Step[1500/2713]: training loss : 0.9454841756820679 TRAIN  loss dict:  {'classification_loss': 0.9454841756820679}
2025-01-19 06:16:02,212 [INFO] Step[1550/2713]: training loss : 0.9508101677894593 TRAIN  loss dict:  {'classification_loss': 0.9508101677894593}
2025-01-19 06:16:17,285 [INFO] Step[1600/2713]: training loss : 0.949021828174591 TRAIN  loss dict:  {'classification_loss': 0.949021828174591}
2025-01-19 06:16:32,336 [INFO] Step[1650/2713]: training loss : 0.9430149424076081 TRAIN  loss dict:  {'classification_loss': 0.9430149424076081}
2025-01-19 06:16:47,400 [INFO] Step[1700/2713]: training loss : 0.9504171538352967 TRAIN  loss dict:  {'classification_loss': 0.9504171538352967}
2025-01-19 06:17:02,490 [INFO] Step[1750/2713]: training loss : 0.9436532199382782 TRAIN  loss dict:  {'classification_loss': 0.9436532199382782}
2025-01-19 06:17:17,566 [INFO] Step[1800/2713]: training loss : 0.9469754111766815 TRAIN  loss dict:  {'classification_loss': 0.9469754111766815}
2025-01-19 06:17:32,661 [INFO] Step[1850/2713]: training loss : 0.9556176340579987 TRAIN  loss dict:  {'classification_loss': 0.9556176340579987}
2025-01-19 06:17:47,660 [INFO] Step[1900/2713]: training loss : 0.9546865081787109 TRAIN  loss dict:  {'classification_loss': 0.9546865081787109}
2025-01-19 06:18:02,778 [INFO] Step[1950/2713]: training loss : 0.9473265600204468 TRAIN  loss dict:  {'classification_loss': 0.9473265600204468}
2025-01-19 06:18:17,888 [INFO] Step[2000/2713]: training loss : 0.9605366289615631 TRAIN  loss dict:  {'classification_loss': 0.9605366289615631}
2025-01-19 06:18:33,026 [INFO] Step[2050/2713]: training loss : 0.9447220683097839 TRAIN  loss dict:  {'classification_loss': 0.9447220683097839}
2025-01-19 06:18:48,110 [INFO] Step[2100/2713]: training loss : 0.9552117943763733 TRAIN  loss dict:  {'classification_loss': 0.9552117943763733}
2025-01-19 06:19:03,256 [INFO] Step[2150/2713]: training loss : 0.9441589033603668 TRAIN  loss dict:  {'classification_loss': 0.9441589033603668}
2025-01-19 06:19:18,304 [INFO] Step[2200/2713]: training loss : 0.9464189660549164 TRAIN  loss dict:  {'classification_loss': 0.9464189660549164}
2025-01-19 06:19:33,417 [INFO] Step[2250/2713]: training loss : 0.9416357743740081 TRAIN  loss dict:  {'classification_loss': 0.9416357743740081}
2025-01-19 06:19:48,543 [INFO] Step[2300/2713]: training loss : 0.9433682429790496 TRAIN  loss dict:  {'classification_loss': 0.9433682429790496}
2025-01-19 06:20:03,675 [INFO] Step[2350/2713]: training loss : 0.9455421531200409 TRAIN  loss dict:  {'classification_loss': 0.9455421531200409}
2025-01-19 06:20:18,783 [INFO] Step[2400/2713]: training loss : 0.9466638708114624 TRAIN  loss dict:  {'classification_loss': 0.9466638708114624}
2025-01-19 06:20:33,848 [INFO] Step[2450/2713]: training loss : 0.9431930029392243 TRAIN  loss dict:  {'classification_loss': 0.9431930029392243}
2025-01-19 06:20:48,915 [INFO] Step[2500/2713]: training loss : 0.9463215935230255 TRAIN  loss dict:  {'classification_loss': 0.9463215935230255}
2025-01-19 06:21:03,971 [INFO] Step[2550/2713]: training loss : 0.9493121576309204 TRAIN  loss dict:  {'classification_loss': 0.9493121576309204}
2025-01-19 06:21:19,039 [INFO] Step[2600/2713]: training loss : 0.9429077124595642 TRAIN  loss dict:  {'classification_loss': 0.9429077124595642}
2025-01-19 06:21:34,120 [INFO] Step[2650/2713]: training loss : 0.9422431099414825 TRAIN  loss dict:  {'classification_loss': 0.9422431099414825}
2025-01-19 06:21:49,299 [INFO] Step[2700/2713]: training loss : 0.9464347469806671 TRAIN  loss dict:  {'classification_loss': 0.9464347469806671}
2025-01-19 06:23:09,083 [INFO] Label accuracies statistics:
2025-01-19 06:23:09,083 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.25, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.5, 207: 1.0, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.25, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 06:23:09,090 [INFO] [62] TRAIN  loss: 0.9476258861943887 acc: 0.9997542695662858
2025-01-19 06:23:09,090 [INFO] [62] TRAIN  loss dict: {'classification_loss': 0.9476258861943887}
2025-01-19 06:23:09,090 [INFO] [62] VALIDATION loss: 1.732327744700855 VALIDATION acc: 0.8068965517241379
2025-01-19 06:23:09,090 [INFO] [62] VALIDATION loss dict: {'classification_loss': 1.732327744700855}
2025-01-19 06:23:09,090 [INFO] 
2025-01-19 06:23:29,313 [INFO] Step[50/2713]: training loss : 0.9421318840980529 TRAIN  loss dict:  {'classification_loss': 0.9421318840980529}
2025-01-19 06:23:44,325 [INFO] Step[100/2713]: training loss : 0.9475285625457763 TRAIN  loss dict:  {'classification_loss': 0.9475285625457763}
2025-01-19 06:23:59,327 [INFO] Step[150/2713]: training loss : 0.9432778537273407 TRAIN  loss dict:  {'classification_loss': 0.9432778537273407}
2025-01-19 06:24:14,328 [INFO] Step[200/2713]: training loss : 0.9476568734645844 TRAIN  loss dict:  {'classification_loss': 0.9476568734645844}
2025-01-19 06:24:29,344 [INFO] Step[250/2713]: training loss : 0.9459968447685242 TRAIN  loss dict:  {'classification_loss': 0.9459968447685242}
2025-01-19 06:24:44,274 [INFO] Step[300/2713]: training loss : 0.9707604384422303 TRAIN  loss dict:  {'classification_loss': 0.9707604384422303}
2025-01-19 06:24:59,325 [INFO] Step[350/2713]: training loss : 0.947769147157669 TRAIN  loss dict:  {'classification_loss': 0.947769147157669}
2025-01-19 06:25:14,267 [INFO] Step[400/2713]: training loss : 0.9467303586006165 TRAIN  loss dict:  {'classification_loss': 0.9467303586006165}
2025-01-19 06:25:29,239 [INFO] Step[450/2713]: training loss : 0.9448654901981354 TRAIN  loss dict:  {'classification_loss': 0.9448654901981354}
2025-01-19 06:25:44,253 [INFO] Step[500/2713]: training loss : 0.9459078478813171 TRAIN  loss dict:  {'classification_loss': 0.9459078478813171}
2025-01-19 06:25:59,211 [INFO] Step[550/2713]: training loss : 0.9470326662063598 TRAIN  loss dict:  {'classification_loss': 0.9470326662063598}
2025-01-19 06:26:14,008 [INFO] Step[600/2713]: training loss : 0.9411350321769715 TRAIN  loss dict:  {'classification_loss': 0.9411350321769715}
2025-01-19 06:26:28,821 [INFO] Step[650/2713]: training loss : 0.9524554800987244 TRAIN  loss dict:  {'classification_loss': 0.9524554800987244}
2025-01-19 06:26:43,639 [INFO] Step[700/2713]: training loss : 0.9459372365474701 TRAIN  loss dict:  {'classification_loss': 0.9459372365474701}
2025-01-19 06:26:58,431 [INFO] Step[750/2713]: training loss : 0.9449787425994873 TRAIN  loss dict:  {'classification_loss': 0.9449787425994873}
2025-01-19 06:27:13,194 [INFO] Step[800/2713]: training loss : 0.9433728086948395 TRAIN  loss dict:  {'classification_loss': 0.9433728086948395}
2025-01-19 06:27:28,011 [INFO] Step[850/2713]: training loss : 0.9416488349437714 TRAIN  loss dict:  {'classification_loss': 0.9416488349437714}
2025-01-19 06:27:42,772 [INFO] Step[900/2713]: training loss : 0.9505135643482209 TRAIN  loss dict:  {'classification_loss': 0.9505135643482209}
2025-01-19 06:27:57,545 [INFO] Step[950/2713]: training loss : 0.9421986615657807 TRAIN  loss dict:  {'classification_loss': 0.9421986615657807}
2025-01-19 06:28:12,323 [INFO] Step[1000/2713]: training loss : 0.9462645173072814 TRAIN  loss dict:  {'classification_loss': 0.9462645173072814}
2025-01-19 06:28:27,107 [INFO] Step[1050/2713]: training loss : 0.944928982257843 TRAIN  loss dict:  {'classification_loss': 0.944928982257843}
2025-01-19 06:28:41,890 [INFO] Step[1100/2713]: training loss : 0.9476763737201691 TRAIN  loss dict:  {'classification_loss': 0.9476763737201691}
2025-01-19 06:28:56,708 [INFO] Step[1150/2713]: training loss : 0.946771366596222 TRAIN  loss dict:  {'classification_loss': 0.946771366596222}
2025-01-19 06:29:11,509 [INFO] Step[1200/2713]: training loss : 0.9469916260242462 TRAIN  loss dict:  {'classification_loss': 0.9469916260242462}
2025-01-19 06:29:26,308 [INFO] Step[1250/2713]: training loss : 0.9532102000713348 TRAIN  loss dict:  {'classification_loss': 0.9532102000713348}
2025-01-19 06:29:41,063 [INFO] Step[1300/2713]: training loss : 0.9473882830142974 TRAIN  loss dict:  {'classification_loss': 0.9473882830142974}
2025-01-19 06:29:55,886 [INFO] Step[1350/2713]: training loss : 0.9438222968578338 TRAIN  loss dict:  {'classification_loss': 0.9438222968578338}
2025-01-19 06:30:10,686 [INFO] Step[1400/2713]: training loss : 0.9447335147857666 TRAIN  loss dict:  {'classification_loss': 0.9447335147857666}
2025-01-19 06:30:25,499 [INFO] Step[1450/2713]: training loss : 0.9460038769245148 TRAIN  loss dict:  {'classification_loss': 0.9460038769245148}
2025-01-19 06:30:40,216 [INFO] Step[1500/2713]: training loss : 0.9440020763874054 TRAIN  loss dict:  {'classification_loss': 0.9440020763874054}
2025-01-19 06:30:55,046 [INFO] Step[1550/2713]: training loss : 0.951333018541336 TRAIN  loss dict:  {'classification_loss': 0.951333018541336}
2025-01-19 06:31:09,830 [INFO] Step[1600/2713]: training loss : 0.9483242774009705 TRAIN  loss dict:  {'classification_loss': 0.9483242774009705}
2025-01-19 06:31:24,586 [INFO] Step[1650/2713]: training loss : 0.9448011970520019 TRAIN  loss dict:  {'classification_loss': 0.9448011970520019}
2025-01-19 06:31:39,417 [INFO] Step[1700/2713]: training loss : 0.9474548387527466 TRAIN  loss dict:  {'classification_loss': 0.9474548387527466}
2025-01-19 06:31:54,239 [INFO] Step[1750/2713]: training loss : 0.9481145703792572 TRAIN  loss dict:  {'classification_loss': 0.9481145703792572}
2025-01-19 06:32:09,023 [INFO] Step[1800/2713]: training loss : 0.9470537877082825 TRAIN  loss dict:  {'classification_loss': 0.9470537877082825}
2025-01-19 06:32:23,804 [INFO] Step[1850/2713]: training loss : 0.9461750066280366 TRAIN  loss dict:  {'classification_loss': 0.9461750066280366}
2025-01-19 06:32:38,570 [INFO] Step[1900/2713]: training loss : 0.9463663160800934 TRAIN  loss dict:  {'classification_loss': 0.9463663160800934}
2025-01-19 06:32:53,352 [INFO] Step[1950/2713]: training loss : 0.9495754861831665 TRAIN  loss dict:  {'classification_loss': 0.9495754861831665}
2025-01-19 06:33:08,133 [INFO] Step[2000/2713]: training loss : 0.9499720120429993 TRAIN  loss dict:  {'classification_loss': 0.9499720120429993}
2025-01-19 06:33:22,948 [INFO] Step[2050/2713]: training loss : 0.9477846801280976 TRAIN  loss dict:  {'classification_loss': 0.9477846801280976}
2025-01-19 06:33:37,753 [INFO] Step[2100/2713]: training loss : 0.944366283416748 TRAIN  loss dict:  {'classification_loss': 0.944366283416748}
2025-01-19 06:33:52,503 [INFO] Step[2150/2713]: training loss : 0.9528917014598847 TRAIN  loss dict:  {'classification_loss': 0.9528917014598847}
2025-01-19 06:34:07,340 [INFO] Step[2200/2713]: training loss : 0.9432422995567322 TRAIN  loss dict:  {'classification_loss': 0.9432422995567322}
2025-01-19 06:34:22,145 [INFO] Step[2250/2713]: training loss : 0.9451393461227418 TRAIN  loss dict:  {'classification_loss': 0.9451393461227418}
2025-01-19 06:34:36,931 [INFO] Step[2300/2713]: training loss : 0.9453908121585846 TRAIN  loss dict:  {'classification_loss': 0.9453908121585846}
2025-01-19 06:34:51,725 [INFO] Step[2350/2713]: training loss : 0.9459870541095734 TRAIN  loss dict:  {'classification_loss': 0.9459870541095734}
2025-01-19 06:35:06,485 [INFO] Step[2400/2713]: training loss : 0.9486530029773712 TRAIN  loss dict:  {'classification_loss': 0.9486530029773712}
2025-01-19 06:35:21,254 [INFO] Step[2450/2713]: training loss : 0.9454401755332946 TRAIN  loss dict:  {'classification_loss': 0.9454401755332946}
2025-01-19 06:35:36,040 [INFO] Step[2500/2713]: training loss : 0.9417621195316315 TRAIN  loss dict:  {'classification_loss': 0.9417621195316315}
2025-01-19 06:35:50,861 [INFO] Step[2550/2713]: training loss : 0.9565123581886291 TRAIN  loss dict:  {'classification_loss': 0.9565123581886291}
2025-01-19 06:36:05,624 [INFO] Step[2600/2713]: training loss : 0.9454416048526764 TRAIN  loss dict:  {'classification_loss': 0.9454416048526764}
2025-01-19 06:36:20,453 [INFO] Step[2650/2713]: training loss : 0.9458988118171692 TRAIN  loss dict:  {'classification_loss': 0.9458988118171692}
2025-01-19 06:36:35,265 [INFO] Step[2700/2713]: training loss : 0.944825040102005 TRAIN  loss dict:  {'classification_loss': 0.944825040102005}
2025-01-19 06:37:54,723 [INFO] Label accuracies statistics:
2025-01-19 06:37:54,723 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.25, 214: 1.0, 215: 1.0, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 0.75, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 1.0, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.5, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.5, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 1.0, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 06:37:56,190 [INFO] [63] TRAIN  loss: 0.9469663920632773 acc: 0.9997542695662858
2025-01-19 06:37:56,190 [INFO] [63] TRAIN  loss dict: {'classification_loss': 0.9469663920632773}
2025-01-19 06:37:56,190 [INFO] [63] VALIDATION loss: 1.708629244252255 VALIDATION acc: 0.8163009404388715
2025-01-19 06:37:56,190 [INFO] [63] VALIDATION loss dict: {'classification_loss': 1.708629244252255}
2025-01-19 06:37:56,190 [INFO] 
2025-01-19 06:38:16,793 [INFO] Step[50/2713]: training loss : 0.9431572675704956 TRAIN  loss dict:  {'classification_loss': 0.9431572675704956}
2025-01-19 06:38:31,573 [INFO] Step[100/2713]: training loss : 0.9434031963348388 TRAIN  loss dict:  {'classification_loss': 0.9434031963348388}
2025-01-19 06:38:46,414 [INFO] Step[150/2713]: training loss : 0.9483473169803619 TRAIN  loss dict:  {'classification_loss': 0.9483473169803619}
2025-01-19 06:39:01,217 [INFO] Step[200/2713]: training loss : 0.9425103390216827 TRAIN  loss dict:  {'classification_loss': 0.9425103390216827}
2025-01-19 06:39:16,036 [INFO] Step[250/2713]: training loss : 0.9546424436569214 TRAIN  loss dict:  {'classification_loss': 0.9546424436569214}
2025-01-19 06:39:30,808 [INFO] Step[300/2713]: training loss : 0.9468544685840606 TRAIN  loss dict:  {'classification_loss': 0.9468544685840606}
2025-01-19 06:39:45,632 [INFO] Step[350/2713]: training loss : 0.943365296125412 TRAIN  loss dict:  {'classification_loss': 0.943365296125412}
2025-01-19 06:40:00,459 [INFO] Step[400/2713]: training loss : 0.9449558019638061 TRAIN  loss dict:  {'classification_loss': 0.9449558019638061}
2025-01-19 06:40:15,239 [INFO] Step[450/2713]: training loss : 0.9420141327381134 TRAIN  loss dict:  {'classification_loss': 0.9420141327381134}
2025-01-19 06:40:30,033 [INFO] Step[500/2713]: training loss : 0.9441404485702515 TRAIN  loss dict:  {'classification_loss': 0.9441404485702515}
2025-01-19 06:40:44,839 [INFO] Step[550/2713]: training loss : 0.9450990521907806 TRAIN  loss dict:  {'classification_loss': 0.9450990521907806}
2025-01-19 06:40:59,703 [INFO] Step[600/2713]: training loss : 0.9470938193798065 TRAIN  loss dict:  {'classification_loss': 0.9470938193798065}
2025-01-19 06:41:14,506 [INFO] Step[650/2713]: training loss : 0.9474973833560943 TRAIN  loss dict:  {'classification_loss': 0.9474973833560943}
2025-01-19 06:41:29,281 [INFO] Step[700/2713]: training loss : 0.9520812499523162 TRAIN  loss dict:  {'classification_loss': 0.9520812499523162}
2025-01-19 06:41:44,147 [INFO] Step[750/2713]: training loss : 0.9445368456840515 TRAIN  loss dict:  {'classification_loss': 0.9445368456840515}
2025-01-19 06:41:58,920 [INFO] Step[800/2713]: training loss : 0.9463624024391174 TRAIN  loss dict:  {'classification_loss': 0.9463624024391174}
2025-01-19 06:42:13,757 [INFO] Step[850/2713]: training loss : 0.9453421604633331 TRAIN  loss dict:  {'classification_loss': 0.9453421604633331}
2025-01-19 06:42:28,550 [INFO] Step[900/2713]: training loss : 0.94445143699646 TRAIN  loss dict:  {'classification_loss': 0.94445143699646}
2025-01-19 06:42:43,362 [INFO] Step[950/2713]: training loss : 0.9477449476718902 TRAIN  loss dict:  {'classification_loss': 0.9477449476718902}
2025-01-19 06:42:58,161 [INFO] Step[1000/2713]: training loss : 0.9471871972084045 TRAIN  loss dict:  {'classification_loss': 0.9471871972084045}
2025-01-19 06:43:13,013 [INFO] Step[1050/2713]: training loss : 0.9450473499298095 TRAIN  loss dict:  {'classification_loss': 0.9450473499298095}
2025-01-19 06:43:27,824 [INFO] Step[1100/2713]: training loss : 0.9469949007034302 TRAIN  loss dict:  {'classification_loss': 0.9469949007034302}
2025-01-19 06:43:42,596 [INFO] Step[1150/2713]: training loss : 0.944682377576828 TRAIN  loss dict:  {'classification_loss': 0.944682377576828}
2025-01-19 06:43:57,417 [INFO] Step[1200/2713]: training loss : 0.9447162795066834 TRAIN  loss dict:  {'classification_loss': 0.9447162795066834}
2025-01-19 06:44:12,220 [INFO] Step[1250/2713]: training loss : 0.9477758145332337 TRAIN  loss dict:  {'classification_loss': 0.9477758145332337}
2025-01-19 06:44:27,041 [INFO] Step[1300/2713]: training loss : 0.9449410808086395 TRAIN  loss dict:  {'classification_loss': 0.9449410808086395}
2025-01-19 06:44:41,830 [INFO] Step[1350/2713]: training loss : 0.9496120297908783 TRAIN  loss dict:  {'classification_loss': 0.9496120297908783}
2025-01-19 06:44:56,671 [INFO] Step[1400/2713]: training loss : 0.947090038061142 TRAIN  loss dict:  {'classification_loss': 0.947090038061142}
2025-01-19 06:45:11,524 [INFO] Step[1450/2713]: training loss : 0.9445214390754699 TRAIN  loss dict:  {'classification_loss': 0.9445214390754699}
2025-01-19 06:45:26,364 [INFO] Step[1500/2713]: training loss : 0.945902806520462 TRAIN  loss dict:  {'classification_loss': 0.945902806520462}
2025-01-19 06:45:41,210 [INFO] Step[1550/2713]: training loss : 0.9457117903232575 TRAIN  loss dict:  {'classification_loss': 0.9457117903232575}
2025-01-19 06:45:55,974 [INFO] Step[1600/2713]: training loss : 0.9463916492462158 TRAIN  loss dict:  {'classification_loss': 0.9463916492462158}
2025-01-19 06:46:10,782 [INFO] Step[1650/2713]: training loss : 0.9458809292316437 TRAIN  loss dict:  {'classification_loss': 0.9458809292316437}
2025-01-19 06:46:25,585 [INFO] Step[1700/2713]: training loss : 0.9513697779178619 TRAIN  loss dict:  {'classification_loss': 0.9513697779178619}
2025-01-19 06:46:40,401 [INFO] Step[1750/2713]: training loss : 0.9686640059947967 TRAIN  loss dict:  {'classification_loss': 0.9686640059947967}
2025-01-19 06:46:55,189 [INFO] Step[1800/2713]: training loss : 0.9454919111728668 TRAIN  loss dict:  {'classification_loss': 0.9454919111728668}
2025-01-19 06:47:10,035 [INFO] Step[1850/2713]: training loss : 0.9431586790084839 TRAIN  loss dict:  {'classification_loss': 0.9431586790084839}
2025-01-19 06:47:24,820 [INFO] Step[1900/2713]: training loss : 0.9448878145217896 TRAIN  loss dict:  {'classification_loss': 0.9448878145217896}
2025-01-19 06:47:39,670 [INFO] Step[1950/2713]: training loss : 0.9436367440223694 TRAIN  loss dict:  {'classification_loss': 0.9436367440223694}
2025-01-19 06:47:54,500 [INFO] Step[2000/2713]: training loss : 0.9433087980747223 TRAIN  loss dict:  {'classification_loss': 0.9433087980747223}
2025-01-19 06:48:09,366 [INFO] Step[2050/2713]: training loss : 0.9446315741539002 TRAIN  loss dict:  {'classification_loss': 0.9446315741539002}
2025-01-19 06:48:24,132 [INFO] Step[2100/2713]: training loss : 0.9524893498420716 TRAIN  loss dict:  {'classification_loss': 0.9524893498420716}
2025-01-19 06:48:38,925 [INFO] Step[2150/2713]: training loss : 0.946850860118866 TRAIN  loss dict:  {'classification_loss': 0.946850860118866}
2025-01-19 06:48:53,645 [INFO] Step[2200/2713]: training loss : 0.943128844499588 TRAIN  loss dict:  {'classification_loss': 0.943128844499588}
2025-01-19 06:49:08,493 [INFO] Step[2250/2713]: training loss : 0.9480755925178528 TRAIN  loss dict:  {'classification_loss': 0.9480755925178528}
2025-01-19 06:49:23,306 [INFO] Step[2300/2713]: training loss : 0.9431558859348297 TRAIN  loss dict:  {'classification_loss': 0.9431558859348297}
2025-01-19 06:49:38,102 [INFO] Step[2350/2713]: training loss : 0.9443011546134948 TRAIN  loss dict:  {'classification_loss': 0.9443011546134948}
2025-01-19 06:49:52,965 [INFO] Step[2400/2713]: training loss : 0.9437376117706299 TRAIN  loss dict:  {'classification_loss': 0.9437376117706299}
2025-01-19 06:50:07,728 [INFO] Step[2450/2713]: training loss : 0.9452181375026703 TRAIN  loss dict:  {'classification_loss': 0.9452181375026703}
2025-01-19 06:50:22,487 [INFO] Step[2500/2713]: training loss : 0.9456318461894989 TRAIN  loss dict:  {'classification_loss': 0.9456318461894989}
2025-01-19 06:50:37,298 [INFO] Step[2550/2713]: training loss : 0.942257250547409 TRAIN  loss dict:  {'classification_loss': 0.942257250547409}
2025-01-19 06:50:52,116 [INFO] Step[2600/2713]: training loss : 0.9441694247722626 TRAIN  loss dict:  {'classification_loss': 0.9441694247722626}
2025-01-19 06:51:06,881 [INFO] Step[2650/2713]: training loss : 0.9445358657836914 TRAIN  loss dict:  {'classification_loss': 0.9445358657836914}
2025-01-19 06:51:21,734 [INFO] Step[2700/2713]: training loss : 0.945226628780365 TRAIN  loss dict:  {'classification_loss': 0.945226628780365}
2025-01-19 06:52:41,194 [INFO] Label accuracies statistics:
2025-01-19 06:52:41,194 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.75, 86: 1.0, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.75, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 06:52:41,843 [INFO] [64] TRAIN  loss: 0.946168015374937 acc: 0.9996314043494287
2025-01-19 06:52:41,843 [INFO] [64] TRAIN  loss dict: {'classification_loss': 0.946168015374937}
2025-01-19 06:52:41,843 [INFO] [64] VALIDATION loss: 1.673409067374423 VALIDATION acc: 0.8150470219435737
2025-01-19 06:52:41,844 [INFO] [64] VALIDATION loss dict: {'classification_loss': 1.673409067374423}
2025-01-19 06:52:41,844 [INFO] 
2025-01-19 06:53:02,145 [INFO] Step[50/2713]: training loss : 0.9417798113822937 TRAIN  loss dict:  {'classification_loss': 0.9417798113822937}
2025-01-19 06:53:16,986 [INFO] Step[100/2713]: training loss : 0.9486277770996093 TRAIN  loss dict:  {'classification_loss': 0.9486277770996093}
2025-01-19 06:53:31,853 [INFO] Step[150/2713]: training loss : 0.9448687219619751 TRAIN  loss dict:  {'classification_loss': 0.9448687219619751}
2025-01-19 06:53:46,672 [INFO] Step[200/2713]: training loss : 0.9433031070232392 TRAIN  loss dict:  {'classification_loss': 0.9433031070232392}
2025-01-19 06:54:01,422 [INFO] Step[250/2713]: training loss : 0.944488799571991 TRAIN  loss dict:  {'classification_loss': 0.944488799571991}
2025-01-19 06:54:16,217 [INFO] Step[300/2713]: training loss : 0.9439336335659028 TRAIN  loss dict:  {'classification_loss': 0.9439336335659028}
2025-01-19 06:54:31,009 [INFO] Step[350/2713]: training loss : 0.9458012533187866 TRAIN  loss dict:  {'classification_loss': 0.9458012533187866}
2025-01-19 06:54:45,808 [INFO] Step[400/2713]: training loss : 0.945574882030487 TRAIN  loss dict:  {'classification_loss': 0.945574882030487}
2025-01-19 06:55:00,620 [INFO] Step[450/2713]: training loss : 0.9460172641277313 TRAIN  loss dict:  {'classification_loss': 0.9460172641277313}
2025-01-19 06:55:15,430 [INFO] Step[500/2713]: training loss : 0.9479090094566345 TRAIN  loss dict:  {'classification_loss': 0.9479090094566345}
2025-01-19 06:55:30,211 [INFO] Step[550/2713]: training loss : 0.9436452317237854 TRAIN  loss dict:  {'classification_loss': 0.9436452317237854}
2025-01-19 06:55:45,051 [INFO] Step[600/2713]: training loss : 0.9457841801643372 TRAIN  loss dict:  {'classification_loss': 0.9457841801643372}
2025-01-19 06:55:59,864 [INFO] Step[650/2713]: training loss : 0.9442734575271606 TRAIN  loss dict:  {'classification_loss': 0.9442734575271606}
2025-01-19 06:56:14,660 [INFO] Step[700/2713]: training loss : 0.9465164482593537 TRAIN  loss dict:  {'classification_loss': 0.9465164482593537}
2025-01-19 06:56:29,446 [INFO] Step[750/2713]: training loss : 0.9447227072715759 TRAIN  loss dict:  {'classification_loss': 0.9447227072715759}
2025-01-19 06:56:44,201 [INFO] Step[800/2713]: training loss : 0.9431396865844727 TRAIN  loss dict:  {'classification_loss': 0.9431396865844727}
2025-01-19 06:56:58,934 [INFO] Step[850/2713]: training loss : 0.9470707321166992 TRAIN  loss dict:  {'classification_loss': 0.9470707321166992}
2025-01-19 06:57:13,748 [INFO] Step[900/2713]: training loss : 0.9486554455757141 TRAIN  loss dict:  {'classification_loss': 0.9486554455757141}
2025-01-19 06:57:28,552 [INFO] Step[950/2713]: training loss : 0.9446655690670014 TRAIN  loss dict:  {'classification_loss': 0.9446655690670014}
2025-01-19 06:57:43,350 [INFO] Step[1000/2713]: training loss : 0.943812073469162 TRAIN  loss dict:  {'classification_loss': 0.943812073469162}
2025-01-19 06:57:58,158 [INFO] Step[1050/2713]: training loss : 0.9474551951885224 TRAIN  loss dict:  {'classification_loss': 0.9474551951885224}
2025-01-19 06:58:12,939 [INFO] Step[1100/2713]: training loss : 0.9448587107658386 TRAIN  loss dict:  {'classification_loss': 0.9448587107658386}
2025-01-19 06:58:27,713 [INFO] Step[1150/2713]: training loss : 0.9447701108455658 TRAIN  loss dict:  {'classification_loss': 0.9447701108455658}
2025-01-19 06:58:42,513 [INFO] Step[1200/2713]: training loss : 0.9461969125270844 TRAIN  loss dict:  {'classification_loss': 0.9461969125270844}
2025-01-19 06:58:57,369 [INFO] Step[1250/2713]: training loss : 0.9397154307365417 TRAIN  loss dict:  {'classification_loss': 0.9397154307365417}
2025-01-19 06:59:12,137 [INFO] Step[1300/2713]: training loss : 0.944094158411026 TRAIN  loss dict:  {'classification_loss': 0.944094158411026}
2025-01-19 06:59:26,973 [INFO] Step[1350/2713]: training loss : 0.9433688294887542 TRAIN  loss dict:  {'classification_loss': 0.9433688294887542}
2025-01-19 06:59:41,768 [INFO] Step[1400/2713]: training loss : 0.9506570565700531 TRAIN  loss dict:  {'classification_loss': 0.9506570565700531}
2025-01-19 06:59:56,592 [INFO] Step[1450/2713]: training loss : 0.9500572681427002 TRAIN  loss dict:  {'classification_loss': 0.9500572681427002}
2025-01-19 07:00:11,412 [INFO] Step[1500/2713]: training loss : 0.9425233268737793 TRAIN  loss dict:  {'classification_loss': 0.9425233268737793}
2025-01-19 07:00:26,224 [INFO] Step[1550/2713]: training loss : 0.9610328507423401 TRAIN  loss dict:  {'classification_loss': 0.9610328507423401}
2025-01-19 07:00:41,055 [INFO] Step[1600/2713]: training loss : 0.9459431540966033 TRAIN  loss dict:  {'classification_loss': 0.9459431540966033}
2025-01-19 07:00:55,856 [INFO] Step[1650/2713]: training loss : 0.9426405394077301 TRAIN  loss dict:  {'classification_loss': 0.9426405394077301}
2025-01-19 07:01:10,641 [INFO] Step[1700/2713]: training loss : 0.9624871408939362 TRAIN  loss dict:  {'classification_loss': 0.9624871408939362}
2025-01-19 07:01:25,436 [INFO] Step[1750/2713]: training loss : 0.9507677233219147 TRAIN  loss dict:  {'classification_loss': 0.9507677233219147}
2025-01-19 07:01:40,254 [INFO] Step[1800/2713]: training loss : 0.9597034764289856 TRAIN  loss dict:  {'classification_loss': 0.9597034764289856}
2025-01-19 07:01:55,035 [INFO] Step[1850/2713]: training loss : 0.9445762073993683 TRAIN  loss dict:  {'classification_loss': 0.9445762073993683}
2025-01-19 07:02:09,823 [INFO] Step[1900/2713]: training loss : 0.945065884590149 TRAIN  loss dict:  {'classification_loss': 0.945065884590149}
2025-01-19 07:02:24,677 [INFO] Step[1950/2713]: training loss : 0.9458048176765442 TRAIN  loss dict:  {'classification_loss': 0.9458048176765442}
2025-01-19 07:02:39,486 [INFO] Step[2000/2713]: training loss : 0.9418666577339172 TRAIN  loss dict:  {'classification_loss': 0.9418666577339172}
2025-01-19 07:02:54,291 [INFO] Step[2050/2713]: training loss : 0.943373898267746 TRAIN  loss dict:  {'classification_loss': 0.943373898267746}
2025-01-19 07:03:09,119 [INFO] Step[2100/2713]: training loss : 0.9471655905246734 TRAIN  loss dict:  {'classification_loss': 0.9471655905246734}
2025-01-19 07:03:23,917 [INFO] Step[2150/2713]: training loss : 0.9461163794994354 TRAIN  loss dict:  {'classification_loss': 0.9461163794994354}
2025-01-19 07:03:38,664 [INFO] Step[2200/2713]: training loss : 0.9429838943481446 TRAIN  loss dict:  {'classification_loss': 0.9429838943481446}
2025-01-19 07:03:53,480 [INFO] Step[2250/2713]: training loss : 0.9440555930137634 TRAIN  loss dict:  {'classification_loss': 0.9440555930137634}
2025-01-19 07:04:08,237 [INFO] Step[2300/2713]: training loss : 0.9434167766571044 TRAIN  loss dict:  {'classification_loss': 0.9434167766571044}
2025-01-19 07:04:23,018 [INFO] Step[2350/2713]: training loss : 0.9445450067520141 TRAIN  loss dict:  {'classification_loss': 0.9445450067520141}
2025-01-19 07:04:37,787 [INFO] Step[2400/2713]: training loss : 0.9438814377784729 TRAIN  loss dict:  {'classification_loss': 0.9438814377784729}
2025-01-19 07:04:52,601 [INFO] Step[2450/2713]: training loss : 0.94390251994133 TRAIN  loss dict:  {'classification_loss': 0.94390251994133}
2025-01-19 07:05:07,356 [INFO] Step[2500/2713]: training loss : 0.9578198993206024 TRAIN  loss dict:  {'classification_loss': 0.9578198993206024}
2025-01-19 07:05:22,163 [INFO] Step[2550/2713]: training loss : 0.9482995235919952 TRAIN  loss dict:  {'classification_loss': 0.9482995235919952}
2025-01-19 07:05:36,947 [INFO] Step[2600/2713]: training loss : 0.9440399491786957 TRAIN  loss dict:  {'classification_loss': 0.9440399491786957}
2025-01-19 07:05:51,727 [INFO] Step[2650/2713]: training loss : 0.9412743473052978 TRAIN  loss dict:  {'classification_loss': 0.9412743473052978}
2025-01-19 07:06:06,533 [INFO] Step[2700/2713]: training loss : 0.9417614614963532 TRAIN  loss dict:  {'classification_loss': 0.9417614614963532}
2025-01-19 07:07:40,220 [INFO] Label accuracies statistics:
2025-01-19 07:07:40,220 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.5, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.75, 86: 1.0, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 0.75, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.5, 288: 1.0, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.25, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.25, 345: 0.25, 346: 0.75, 347: 0.75, 348: 0.5, 349: 0.5, 350: 0.25, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 07:07:40,222 [INFO] [65] TRAIN  loss: 0.946109960758506 acc: 0.9992628086988573
2025-01-19 07:07:40,222 [INFO] [65] TRAIN  loss dict: {'classification_loss': 0.946109960758506}
2025-01-19 07:07:40,222 [INFO] [65] VALIDATION loss: 1.7557583008717774 VALIDATION acc: 0.8018808777429467
2025-01-19 07:07:40,222 [INFO] [65] VALIDATION loss dict: {'classification_loss': 1.7557583008717774}
2025-01-19 07:07:40,222 [INFO] 
2025-01-19 07:08:00,803 [INFO] Step[50/2713]: training loss : 0.9454639971256256 TRAIN  loss dict:  {'classification_loss': 0.9454639971256256}
2025-01-19 07:08:15,851 [INFO] Step[100/2713]: training loss : 0.9567775297164917 TRAIN  loss dict:  {'classification_loss': 0.9567775297164917}
2025-01-19 07:08:30,929 [INFO] Step[150/2713]: training loss : 0.9509049999713898 TRAIN  loss dict:  {'classification_loss': 0.9509049999713898}
2025-01-19 07:08:46,000 [INFO] Step[200/2713]: training loss : 0.9514532017707825 TRAIN  loss dict:  {'classification_loss': 0.9514532017707825}
2025-01-19 07:09:01,097 [INFO] Step[250/2713]: training loss : 0.9442890369892121 TRAIN  loss dict:  {'classification_loss': 0.9442890369892121}
2025-01-19 07:09:16,149 [INFO] Step[300/2713]: training loss : 0.9440952169895173 TRAIN  loss dict:  {'classification_loss': 0.9440952169895173}
2025-01-19 07:09:31,207 [INFO] Step[350/2713]: training loss : 0.9449349534511566 TRAIN  loss dict:  {'classification_loss': 0.9449349534511566}
2025-01-19 07:09:46,265 [INFO] Step[400/2713]: training loss : 0.9430789637565613 TRAIN  loss dict:  {'classification_loss': 0.9430789637565613}
2025-01-19 07:10:01,292 [INFO] Step[450/2713]: training loss : 0.9400528931617737 TRAIN  loss dict:  {'classification_loss': 0.9400528931617737}
2025-01-19 07:10:16,342 [INFO] Step[500/2713]: training loss : 0.9438603293895721 TRAIN  loss dict:  {'classification_loss': 0.9438603293895721}
2025-01-19 07:10:31,449 [INFO] Step[550/2713]: training loss : 0.9419064998626709 TRAIN  loss dict:  {'classification_loss': 0.9419064998626709}
2025-01-19 07:10:46,499 [INFO] Step[600/2713]: training loss : 0.9433943355083465 TRAIN  loss dict:  {'classification_loss': 0.9433943355083465}
2025-01-19 07:11:01,606 [INFO] Step[650/2713]: training loss : 0.9414763736724854 TRAIN  loss dict:  {'classification_loss': 0.9414763736724854}
2025-01-19 07:11:16,684 [INFO] Step[700/2713]: training loss : 0.9449774706363678 TRAIN  loss dict:  {'classification_loss': 0.9449774706363678}
2025-01-19 07:11:31,761 [INFO] Step[750/2713]: training loss : 0.9462768507003784 TRAIN  loss dict:  {'classification_loss': 0.9462768507003784}
2025-01-19 07:11:46,858 [INFO] Step[800/2713]: training loss : 0.9462541258335113 TRAIN  loss dict:  {'classification_loss': 0.9462541258335113}
2025-01-19 07:12:01,952 [INFO] Step[850/2713]: training loss : 0.9541500198841095 TRAIN  loss dict:  {'classification_loss': 0.9541500198841095}
2025-01-19 07:12:17,008 [INFO] Step[900/2713]: training loss : 0.944840167760849 TRAIN  loss dict:  {'classification_loss': 0.944840167760849}
2025-01-19 07:12:32,050 [INFO] Step[950/2713]: training loss : 0.9464450299739837 TRAIN  loss dict:  {'classification_loss': 0.9464450299739837}
2025-01-19 07:12:47,127 [INFO] Step[1000/2713]: training loss : 0.9440756344795227 TRAIN  loss dict:  {'classification_loss': 0.9440756344795227}
2025-01-19 07:13:02,192 [INFO] Step[1050/2713]: training loss : 0.9432276976108551 TRAIN  loss dict:  {'classification_loss': 0.9432276976108551}
2025-01-19 07:13:17,256 [INFO] Step[1100/2713]: training loss : 0.9461500883102417 TRAIN  loss dict:  {'classification_loss': 0.9461500883102417}
2025-01-19 07:13:32,366 [INFO] Step[1150/2713]: training loss : 0.9481659042835235 TRAIN  loss dict:  {'classification_loss': 0.9481659042835235}
2025-01-19 07:13:47,463 [INFO] Step[1200/2713]: training loss : 0.9423778367042541 TRAIN  loss dict:  {'classification_loss': 0.9423778367042541}
2025-01-19 07:14:02,580 [INFO] Step[1250/2713]: training loss : 0.9479360580444336 TRAIN  loss dict:  {'classification_loss': 0.9479360580444336}
2025-01-19 07:14:17,653 [INFO] Step[1300/2713]: training loss : 0.9441798281669617 TRAIN  loss dict:  {'classification_loss': 0.9441798281669617}
2025-01-19 07:14:32,705 [INFO] Step[1350/2713]: training loss : 0.9429680550098419 TRAIN  loss dict:  {'classification_loss': 0.9429680550098419}
2025-01-19 07:14:47,741 [INFO] Step[1400/2713]: training loss : 0.9454778003692627 TRAIN  loss dict:  {'classification_loss': 0.9454778003692627}
2025-01-19 07:15:02,800 [INFO] Step[1450/2713]: training loss : 0.943073627948761 TRAIN  loss dict:  {'classification_loss': 0.943073627948761}
2025-01-19 07:15:17,858 [INFO] Step[1500/2713]: training loss : 0.9412976813316345 TRAIN  loss dict:  {'classification_loss': 0.9412976813316345}
2025-01-19 07:15:32,931 [INFO] Step[1550/2713]: training loss : 0.9459444403648376 TRAIN  loss dict:  {'classification_loss': 0.9459444403648376}
2025-01-19 07:15:47,989 [INFO] Step[1600/2713]: training loss : 0.9505117094516754 TRAIN  loss dict:  {'classification_loss': 0.9505117094516754}
2025-01-19 07:16:03,071 [INFO] Step[1650/2713]: training loss : 0.9584199464321137 TRAIN  loss dict:  {'classification_loss': 0.9584199464321137}
2025-01-19 07:16:18,112 [INFO] Step[1700/2713]: training loss : 0.946222585439682 TRAIN  loss dict:  {'classification_loss': 0.946222585439682}
2025-01-19 07:16:33,171 [INFO] Step[1750/2713]: training loss : 0.9432415819168091 TRAIN  loss dict:  {'classification_loss': 0.9432415819168091}
2025-01-19 07:16:48,258 [INFO] Step[1800/2713]: training loss : 0.9442544460296631 TRAIN  loss dict:  {'classification_loss': 0.9442544460296631}
2025-01-19 07:17:03,327 [INFO] Step[1850/2713]: training loss : 0.9454205989837646 TRAIN  loss dict:  {'classification_loss': 0.9454205989837646}
2025-01-19 07:17:18,396 [INFO] Step[1900/2713]: training loss : 0.9439825367927551 TRAIN  loss dict:  {'classification_loss': 0.9439825367927551}
2025-01-19 07:17:33,507 [INFO] Step[1950/2713]: training loss : 0.9445885705947876 TRAIN  loss dict:  {'classification_loss': 0.9445885705947876}
2025-01-19 07:17:48,613 [INFO] Step[2000/2713]: training loss : 0.9424387335777282 TRAIN  loss dict:  {'classification_loss': 0.9424387335777282}
2025-01-19 07:18:03,647 [INFO] Step[2050/2713]: training loss : 0.9422117233276367 TRAIN  loss dict:  {'classification_loss': 0.9422117233276367}
2025-01-19 07:18:18,651 [INFO] Step[2100/2713]: training loss : 0.9451229131221771 TRAIN  loss dict:  {'classification_loss': 0.9451229131221771}
2025-01-19 07:18:33,738 [INFO] Step[2150/2713]: training loss : 0.9471953225135803 TRAIN  loss dict:  {'classification_loss': 0.9471953225135803}
2025-01-19 07:18:48,792 [INFO] Step[2200/2713]: training loss : 0.9458929884433747 TRAIN  loss dict:  {'classification_loss': 0.9458929884433747}
2025-01-19 07:19:03,881 [INFO] Step[2250/2713]: training loss : 0.9499080073833466 TRAIN  loss dict:  {'classification_loss': 0.9499080073833466}
2025-01-19 07:19:18,942 [INFO] Step[2300/2713]: training loss : 0.9457347786426544 TRAIN  loss dict:  {'classification_loss': 0.9457347786426544}
2025-01-19 07:19:33,989 [INFO] Step[2350/2713]: training loss : 0.9430916571617126 TRAIN  loss dict:  {'classification_loss': 0.9430916571617126}
2025-01-19 07:19:49,071 [INFO] Step[2400/2713]: training loss : 0.9596809959411621 TRAIN  loss dict:  {'classification_loss': 0.9596809959411621}
2025-01-19 07:20:04,134 [INFO] Step[2450/2713]: training loss : 0.9466858732700348 TRAIN  loss dict:  {'classification_loss': 0.9466858732700348}
2025-01-19 07:20:19,176 [INFO] Step[2500/2713]: training loss : 0.943856588602066 TRAIN  loss dict:  {'classification_loss': 0.943856588602066}
2025-01-19 07:20:34,235 [INFO] Step[2550/2713]: training loss : 0.9438717865943909 TRAIN  loss dict:  {'classification_loss': 0.9438717865943909}
2025-01-19 07:20:49,317 [INFO] Step[2600/2713]: training loss : 0.9450442969799042 TRAIN  loss dict:  {'classification_loss': 0.9450442969799042}
2025-01-19 07:21:04,381 [INFO] Step[2650/2713]: training loss : 0.9467482089996337 TRAIN  loss dict:  {'classification_loss': 0.9467482089996337}
2025-01-19 07:21:19,487 [INFO] Step[2700/2713]: training loss : 0.9430223214626312 TRAIN  loss dict:  {'classification_loss': 0.9430223214626312}
2025-01-19 07:22:40,096 [INFO] Label accuracies statistics:
2025-01-19 07:22:40,097 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 1.0, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.5, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 07:22:40,732 [INFO] [66] TRAIN  loss: 0.9458475687870275 acc: 0.9995085391325715
2025-01-19 07:22:40,732 [INFO] [66] TRAIN  loss dict: {'classification_loss': 0.9458475687870275}
2025-01-19 07:22:40,732 [INFO] [66] VALIDATION loss: 1.698094783084733 VALIDATION acc: 0.8206896551724138
2025-01-19 07:22:40,732 [INFO] [66] VALIDATION loss dict: {'classification_loss': 1.698094783084733}
2025-01-19 07:22:40,732 [INFO] 
2025-01-19 07:23:01,195 [INFO] Step[50/2713]: training loss : 0.9423219919204712 TRAIN  loss dict:  {'classification_loss': 0.9423219919204712}
2025-01-19 07:23:16,320 [INFO] Step[100/2713]: training loss : 0.9440809559822082 TRAIN  loss dict:  {'classification_loss': 0.9440809559822082}
2025-01-19 07:23:31,451 [INFO] Step[150/2713]: training loss : 0.9462770736217498 TRAIN  loss dict:  {'classification_loss': 0.9462770736217498}
2025-01-19 07:23:46,558 [INFO] Step[200/2713]: training loss : 0.9437423253059387 TRAIN  loss dict:  {'classification_loss': 0.9437423253059387}
2025-01-19 07:24:01,676 [INFO] Step[250/2713]: training loss : 0.946259251832962 TRAIN  loss dict:  {'classification_loss': 0.946259251832962}
2025-01-19 07:24:16,784 [INFO] Step[300/2713]: training loss : 0.9412737345695495 TRAIN  loss dict:  {'classification_loss': 0.9412737345695495}
2025-01-19 07:24:31,915 [INFO] Step[350/2713]: training loss : 0.9435934090614319 TRAIN  loss dict:  {'classification_loss': 0.9435934090614319}
2025-01-19 07:24:47,007 [INFO] Step[400/2713]: training loss : 0.9459277367591858 TRAIN  loss dict:  {'classification_loss': 0.9459277367591858}
2025-01-19 07:25:02,116 [INFO] Step[450/2713]: training loss : 0.9425895833969116 TRAIN  loss dict:  {'classification_loss': 0.9425895833969116}
2025-01-19 07:25:17,256 [INFO] Step[500/2713]: training loss : 0.943902564048767 TRAIN  loss dict:  {'classification_loss': 0.943902564048767}
2025-01-19 07:25:32,383 [INFO] Step[550/2713]: training loss : 0.9420149827003479 TRAIN  loss dict:  {'classification_loss': 0.9420149827003479}
2025-01-19 07:25:47,514 [INFO] Step[600/2713]: training loss : 0.9409019589424134 TRAIN  loss dict:  {'classification_loss': 0.9409019589424134}
2025-01-19 07:26:02,620 [INFO] Step[650/2713]: training loss : 0.947950439453125 TRAIN  loss dict:  {'classification_loss': 0.947950439453125}
2025-01-19 07:26:17,702 [INFO] Step[700/2713]: training loss : 0.9454189455509185 TRAIN  loss dict:  {'classification_loss': 0.9454189455509185}
2025-01-19 07:26:32,825 [INFO] Step[750/2713]: training loss : 0.9434314382076263 TRAIN  loss dict:  {'classification_loss': 0.9434314382076263}
2025-01-19 07:26:47,948 [INFO] Step[800/2713]: training loss : 0.9406867563724518 TRAIN  loss dict:  {'classification_loss': 0.9406867563724518}
2025-01-19 07:27:02,971 [INFO] Step[850/2713]: training loss : 0.9420054519176483 TRAIN  loss dict:  {'classification_loss': 0.9420054519176483}
2025-01-19 07:27:17,985 [INFO] Step[900/2713]: training loss : 0.9479669141769409 TRAIN  loss dict:  {'classification_loss': 0.9479669141769409}
2025-01-19 07:27:32,990 [INFO] Step[950/2713]: training loss : 0.9449754595756531 TRAIN  loss dict:  {'classification_loss': 0.9449754595756531}
2025-01-19 07:27:47,960 [INFO] Step[1000/2713]: training loss : 0.9475295543670654 TRAIN  loss dict:  {'classification_loss': 0.9475295543670654}
2025-01-19 07:28:02,972 [INFO] Step[1050/2713]: training loss : 0.9441523921489715 TRAIN  loss dict:  {'classification_loss': 0.9441523921489715}
2025-01-19 07:28:17,982 [INFO] Step[1100/2713]: training loss : 0.9429055690765381 TRAIN  loss dict:  {'classification_loss': 0.9429055690765381}
2025-01-19 07:28:32,976 [INFO] Step[1150/2713]: training loss : 0.9414591646194458 TRAIN  loss dict:  {'classification_loss': 0.9414591646194458}
2025-01-19 07:28:47,920 [INFO] Step[1200/2713]: training loss : 0.9424415862560273 TRAIN  loss dict:  {'classification_loss': 0.9424415862560273}
2025-01-19 07:29:02,908 [INFO] Step[1250/2713]: training loss : 0.9465285015106201 TRAIN  loss dict:  {'classification_loss': 0.9465285015106201}
2025-01-19 07:29:17,926 [INFO] Step[1300/2713]: training loss : 0.9416424143314361 TRAIN  loss dict:  {'classification_loss': 0.9416424143314361}
2025-01-19 07:29:32,944 [INFO] Step[1350/2713]: training loss : 0.9407014870643615 TRAIN  loss dict:  {'classification_loss': 0.9407014870643615}
2025-01-19 07:29:47,924 [INFO] Step[1400/2713]: training loss : 0.9455533003807068 TRAIN  loss dict:  {'classification_loss': 0.9455533003807068}
2025-01-19 07:30:02,883 [INFO] Step[1450/2713]: training loss : 0.9499436974525451 TRAIN  loss dict:  {'classification_loss': 0.9499436974525451}
2025-01-19 07:30:17,887 [INFO] Step[1500/2713]: training loss : 0.9424903750419616 TRAIN  loss dict:  {'classification_loss': 0.9424903750419616}
2025-01-19 07:30:32,883 [INFO] Step[1550/2713]: training loss : 0.9436566257476806 TRAIN  loss dict:  {'classification_loss': 0.9436566257476806}
2025-01-19 07:30:47,833 [INFO] Step[1600/2713]: training loss : 0.9622528958320617 TRAIN  loss dict:  {'classification_loss': 0.9622528958320617}
2025-01-19 07:31:02,839 [INFO] Step[1650/2713]: training loss : 0.9492602658271789 TRAIN  loss dict:  {'classification_loss': 0.9492602658271789}
2025-01-19 07:31:17,802 [INFO] Step[1700/2713]: training loss : 0.9451657676696777 TRAIN  loss dict:  {'classification_loss': 0.9451657676696777}
2025-01-19 07:31:32,830 [INFO] Step[1750/2713]: training loss : 0.9421672642230987 TRAIN  loss dict:  {'classification_loss': 0.9421672642230987}
2025-01-19 07:31:47,817 [INFO] Step[1800/2713]: training loss : 0.9536785745620727 TRAIN  loss dict:  {'classification_loss': 0.9536785745620727}
2025-01-19 07:32:02,828 [INFO] Step[1850/2713]: training loss : 0.9419329190254211 TRAIN  loss dict:  {'classification_loss': 0.9419329190254211}
2025-01-19 07:32:17,839 [INFO] Step[1900/2713]: training loss : 0.9458889198303223 TRAIN  loss dict:  {'classification_loss': 0.9458889198303223}
2025-01-19 07:32:32,832 [INFO] Step[1950/2713]: training loss : 0.9422567629814148 TRAIN  loss dict:  {'classification_loss': 0.9422567629814148}
2025-01-19 07:32:47,847 [INFO] Step[2000/2713]: training loss : 0.9422310173511506 TRAIN  loss dict:  {'classification_loss': 0.9422310173511506}
2025-01-19 07:33:02,833 [INFO] Step[2050/2713]: training loss : 0.9454163038730621 TRAIN  loss dict:  {'classification_loss': 0.9454163038730621}
2025-01-19 07:33:17,838 [INFO] Step[2100/2713]: training loss : 0.9485718846321106 TRAIN  loss dict:  {'classification_loss': 0.9485718846321106}
2025-01-19 07:33:32,848 [INFO] Step[2150/2713]: training loss : 0.9433364200592042 TRAIN  loss dict:  {'classification_loss': 0.9433364200592042}
2025-01-19 07:33:47,843 [INFO] Step[2200/2713]: training loss : 0.9454476451873779 TRAIN  loss dict:  {'classification_loss': 0.9454476451873779}
2025-01-19 07:34:02,835 [INFO] Step[2250/2713]: training loss : 0.9421761846542358 TRAIN  loss dict:  {'classification_loss': 0.9421761846542358}
2025-01-19 07:34:17,830 [INFO] Step[2300/2713]: training loss : 0.9464757144451141 TRAIN  loss dict:  {'classification_loss': 0.9464757144451141}
2025-01-19 07:34:32,833 [INFO] Step[2350/2713]: training loss : 0.9491428923606873 TRAIN  loss dict:  {'classification_loss': 0.9491428923606873}
2025-01-19 07:34:47,825 [INFO] Step[2400/2713]: training loss : 0.9459718775749206 TRAIN  loss dict:  {'classification_loss': 0.9459718775749206}
2025-01-19 07:35:02,826 [INFO] Step[2450/2713]: training loss : 0.9425469052791595 TRAIN  loss dict:  {'classification_loss': 0.9425469052791595}
2025-01-19 07:35:17,868 [INFO] Step[2500/2713]: training loss : 0.9410359263420105 TRAIN  loss dict:  {'classification_loss': 0.9410359263420105}
2025-01-19 07:35:32,843 [INFO] Step[2550/2713]: training loss : 0.9409440290927887 TRAIN  loss dict:  {'classification_loss': 0.9409440290927887}
2025-01-19 07:35:47,826 [INFO] Step[2600/2713]: training loss : 0.9484524559974671 TRAIN  loss dict:  {'classification_loss': 0.9484524559974671}
2025-01-19 07:36:02,848 [INFO] Step[2650/2713]: training loss : 0.948590693473816 TRAIN  loss dict:  {'classification_loss': 0.948590693473816}
2025-01-19 07:36:17,886 [INFO] Step[2700/2713]: training loss : 0.9455773782730102 TRAIN  loss dict:  {'classification_loss': 0.9455773782730102}
2025-01-19 07:37:37,362 [INFO] Label accuracies statistics:
2025-01-19 07:37:37,362 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 0.75, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.25, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 1.0, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.5, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 07:37:37,365 [INFO] [67] TRAIN  loss: 0.944874416252717 acc: 0.9998771347831429
2025-01-19 07:37:37,365 [INFO] [67] TRAIN  loss dict: {'classification_loss': 0.944874416252717}
2025-01-19 07:37:37,365 [INFO] [67] VALIDATION loss: 1.7005177747486229 VALIDATION acc: 0.8144200626959248
2025-01-19 07:37:37,365 [INFO] [67] VALIDATION loss dict: {'classification_loss': 1.7005177747486229}
2025-01-19 07:37:37,365 [INFO] 
2025-01-19 07:37:57,858 [INFO] Step[50/2713]: training loss : 0.9506212711334229 TRAIN  loss dict:  {'classification_loss': 0.9506212711334229}
2025-01-19 07:38:12,839 [INFO] Step[100/2713]: training loss : 0.9486859321594239 TRAIN  loss dict:  {'classification_loss': 0.9486859321594239}
2025-01-19 07:38:27,837 [INFO] Step[150/2713]: training loss : 0.9479546213150024 TRAIN  loss dict:  {'classification_loss': 0.9479546213150024}
2025-01-19 07:38:42,869 [INFO] Step[200/2713]: training loss : 0.9436031186580658 TRAIN  loss dict:  {'classification_loss': 0.9436031186580658}
2025-01-19 07:38:57,889 [INFO] Step[250/2713]: training loss : 0.9434463918209076 TRAIN  loss dict:  {'classification_loss': 0.9434463918209076}
2025-01-19 07:39:12,902 [INFO] Step[300/2713]: training loss : 0.9430194783210755 TRAIN  loss dict:  {'classification_loss': 0.9430194783210755}
2025-01-19 07:39:27,885 [INFO] Step[350/2713]: training loss : 0.9429398310184479 TRAIN  loss dict:  {'classification_loss': 0.9429398310184479}
2025-01-19 07:39:42,879 [INFO] Step[400/2713]: training loss : 0.9474584186077117 TRAIN  loss dict:  {'classification_loss': 0.9474584186077117}
2025-01-19 07:39:57,868 [INFO] Step[450/2713]: training loss : 0.9439828372001648 TRAIN  loss dict:  {'classification_loss': 0.9439828372001648}
2025-01-19 07:40:12,917 [INFO] Step[500/2713]: training loss : 0.9419240045547486 TRAIN  loss dict:  {'classification_loss': 0.9419240045547486}
2025-01-19 07:40:27,940 [INFO] Step[550/2713]: training loss : 0.9471645081043243 TRAIN  loss dict:  {'classification_loss': 0.9471645081043243}
2025-01-19 07:40:43,014 [INFO] Step[600/2713]: training loss : 0.9409849452972412 TRAIN  loss dict:  {'classification_loss': 0.9409849452972412}
2025-01-19 07:40:58,004 [INFO] Step[650/2713]: training loss : 0.9433342850208283 TRAIN  loss dict:  {'classification_loss': 0.9433342850208283}
2025-01-19 07:41:13,018 [INFO] Step[700/2713]: training loss : 0.9434281241893768 TRAIN  loss dict:  {'classification_loss': 0.9434281241893768}
2025-01-19 07:41:28,065 [INFO] Step[750/2713]: training loss : 0.9541798579692841 TRAIN  loss dict:  {'classification_loss': 0.9541798579692841}
2025-01-19 07:41:43,030 [INFO] Step[800/2713]: training loss : 0.9439891648292541 TRAIN  loss dict:  {'classification_loss': 0.9439891648292541}
2025-01-19 07:41:58,003 [INFO] Step[850/2713]: training loss : 0.9435536038875579 TRAIN  loss dict:  {'classification_loss': 0.9435536038875579}
2025-01-19 07:42:12,994 [INFO] Step[900/2713]: training loss : 0.9438706314563752 TRAIN  loss dict:  {'classification_loss': 0.9438706314563752}
2025-01-19 07:42:27,982 [INFO] Step[950/2713]: training loss : 0.9420719313621521 TRAIN  loss dict:  {'classification_loss': 0.9420719313621521}
2025-01-19 07:42:42,979 [INFO] Step[1000/2713]: training loss : 0.9415575897693634 TRAIN  loss dict:  {'classification_loss': 0.9415575897693634}
2025-01-19 07:42:57,976 [INFO] Step[1050/2713]: training loss : 0.9433481657505035 TRAIN  loss dict:  {'classification_loss': 0.9433481657505035}
2025-01-19 07:43:12,957 [INFO] Step[1100/2713]: training loss : 0.9484096443653107 TRAIN  loss dict:  {'classification_loss': 0.9484096443653107}
2025-01-19 07:43:27,955 [INFO] Step[1150/2713]: training loss : 0.9416161060333252 TRAIN  loss dict:  {'classification_loss': 0.9416161060333252}
2025-01-19 07:43:42,911 [INFO] Step[1200/2713]: training loss : 0.9461694800853729 TRAIN  loss dict:  {'classification_loss': 0.9461694800853729}
2025-01-19 07:43:57,931 [INFO] Step[1250/2713]: training loss : 0.9435376560688019 TRAIN  loss dict:  {'classification_loss': 0.9435376560688019}
2025-01-19 07:44:13,002 [INFO] Step[1300/2713]: training loss : 0.9531665658950805 TRAIN  loss dict:  {'classification_loss': 0.9531665658950805}
2025-01-19 07:44:28,045 [INFO] Step[1350/2713]: training loss : 0.9449196529388427 TRAIN  loss dict:  {'classification_loss': 0.9449196529388427}
2025-01-19 07:44:43,077 [INFO] Step[1400/2713]: training loss : 0.9421222484111786 TRAIN  loss dict:  {'classification_loss': 0.9421222484111786}
2025-01-19 07:44:58,069 [INFO] Step[1450/2713]: training loss : 0.9416809308528901 TRAIN  loss dict:  {'classification_loss': 0.9416809308528901}
2025-01-19 07:45:13,144 [INFO] Step[1500/2713]: training loss : 0.9463162159919739 TRAIN  loss dict:  {'classification_loss': 0.9463162159919739}
2025-01-19 07:45:28,153 [INFO] Step[1550/2713]: training loss : 0.9439608383178711 TRAIN  loss dict:  {'classification_loss': 0.9439608383178711}
2025-01-19 07:45:43,193 [INFO] Step[1600/2713]: training loss : 0.9447816479206085 TRAIN  loss dict:  {'classification_loss': 0.9447816479206085}
2025-01-19 07:45:58,318 [INFO] Step[1650/2713]: training loss : 0.9487443423271179 TRAIN  loss dict:  {'classification_loss': 0.9487443423271179}
2025-01-19 07:46:13,382 [INFO] Step[1700/2713]: training loss : 0.942749845981598 TRAIN  loss dict:  {'classification_loss': 0.942749845981598}
2025-01-19 07:46:28,432 [INFO] Step[1750/2713]: training loss : 0.9420606708526611 TRAIN  loss dict:  {'classification_loss': 0.9420606708526611}
2025-01-19 07:46:43,397 [INFO] Step[1800/2713]: training loss : 0.9422573041915894 TRAIN  loss dict:  {'classification_loss': 0.9422573041915894}
2025-01-19 07:46:58,414 [INFO] Step[1850/2713]: training loss : 0.945862147808075 TRAIN  loss dict:  {'classification_loss': 0.945862147808075}
2025-01-19 07:47:13,443 [INFO] Step[1900/2713]: training loss : 0.9426156747341156 TRAIN  loss dict:  {'classification_loss': 0.9426156747341156}
2025-01-19 07:47:28,469 [INFO] Step[1950/2713]: training loss : 0.9427180302143097 TRAIN  loss dict:  {'classification_loss': 0.9427180302143097}
2025-01-19 07:47:43,505 [INFO] Step[2000/2713]: training loss : 0.9411081993579864 TRAIN  loss dict:  {'classification_loss': 0.9411081993579864}
2025-01-19 07:47:58,525 [INFO] Step[2050/2713]: training loss : 0.9431742680072784 TRAIN  loss dict:  {'classification_loss': 0.9431742680072784}
2025-01-19 07:48:13,471 [INFO] Step[2100/2713]: training loss : 0.9426402592658997 TRAIN  loss dict:  {'classification_loss': 0.9426402592658997}
2025-01-19 07:48:28,520 [INFO] Step[2150/2713]: training loss : 0.9455351805686951 TRAIN  loss dict:  {'classification_loss': 0.9455351805686951}
2025-01-19 07:48:43,556 [INFO] Step[2200/2713]: training loss : 0.9465176022052765 TRAIN  loss dict:  {'classification_loss': 0.9465176022052765}
2025-01-19 07:48:58,576 [INFO] Step[2250/2713]: training loss : 0.9440800893306732 TRAIN  loss dict:  {'classification_loss': 0.9440800893306732}
2025-01-19 07:49:13,583 [INFO] Step[2300/2713]: training loss : 0.9424780309200287 TRAIN  loss dict:  {'classification_loss': 0.9424780309200287}
2025-01-19 07:49:28,678 [INFO] Step[2350/2713]: training loss : 0.9428183460235595 TRAIN  loss dict:  {'classification_loss': 0.9428183460235595}
2025-01-19 07:49:43,811 [INFO] Step[2400/2713]: training loss : 0.942488044500351 TRAIN  loss dict:  {'classification_loss': 0.942488044500351}
2025-01-19 07:49:58,943 [INFO] Step[2450/2713]: training loss : 0.9516130936145782 TRAIN  loss dict:  {'classification_loss': 0.9516130936145782}
2025-01-19 07:50:14,045 [INFO] Step[2500/2713]: training loss : 0.942604374885559 TRAIN  loss dict:  {'classification_loss': 0.942604374885559}
2025-01-19 07:50:29,125 [INFO] Step[2550/2713]: training loss : 0.9451842200756073 TRAIN  loss dict:  {'classification_loss': 0.9451842200756073}
2025-01-19 07:50:44,276 [INFO] Step[2600/2713]: training loss : 0.9486035931110383 TRAIN  loss dict:  {'classification_loss': 0.9486035931110383}
2025-01-19 07:50:59,415 [INFO] Step[2650/2713]: training loss : 0.9525069892406464 TRAIN  loss dict:  {'classification_loss': 0.9525069892406464}
2025-01-19 07:51:14,551 [INFO] Step[2700/2713]: training loss : 0.9452491986751557 TRAIN  loss dict:  {'classification_loss': 0.9452491986751557}
2025-01-19 07:52:33,997 [INFO] Label accuracies statistics:
2025-01-19 07:52:33,997 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.25, 207: 1.0, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 1.0, 289: 1.0, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.5, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 07:52:33,999 [INFO] [68] TRAIN  loss: 0.9448843608032589 acc: 0.9996314043494287
2025-01-19 07:52:33,999 [INFO] [68] TRAIN  loss dict: {'classification_loss': 0.9448843608032589}
2025-01-19 07:52:34,000 [INFO] [68] VALIDATION loss: 1.7433885617139644 VALIDATION acc: 0.8025078369905956
2025-01-19 07:52:34,000 [INFO] [68] VALIDATION loss dict: {'classification_loss': 1.7433885617139644}
2025-01-19 07:52:34,000 [INFO] 
2025-01-19 07:52:54,881 [INFO] Step[50/2713]: training loss : 0.9440276646614074 TRAIN  loss dict:  {'classification_loss': 0.9440276646614074}
2025-01-19 07:53:09,898 [INFO] Step[100/2713]: training loss : 0.9396482300758362 TRAIN  loss dict:  {'classification_loss': 0.9396482300758362}
2025-01-19 07:53:24,896 [INFO] Step[150/2713]: training loss : 0.9524259352684021 TRAIN  loss dict:  {'classification_loss': 0.9524259352684021}
2025-01-19 07:53:39,965 [INFO] Step[200/2713]: training loss : 0.947586487531662 TRAIN  loss dict:  {'classification_loss': 0.947586487531662}
2025-01-19 07:53:54,979 [INFO] Step[250/2713]: training loss : 0.95047438621521 TRAIN  loss dict:  {'classification_loss': 0.95047438621521}
2025-01-19 07:54:10,027 [INFO] Step[300/2713]: training loss : 0.9421446144580841 TRAIN  loss dict:  {'classification_loss': 0.9421446144580841}
2025-01-19 07:54:25,010 [INFO] Step[350/2713]: training loss : 0.9563438105583191 TRAIN  loss dict:  {'classification_loss': 0.9563438105583191}
2025-01-19 07:54:40,075 [INFO] Step[400/2713]: training loss : 0.9413217973709106 TRAIN  loss dict:  {'classification_loss': 0.9413217973709106}
2025-01-19 07:54:55,029 [INFO] Step[450/2713]: training loss : 0.9431657981872559 TRAIN  loss dict:  {'classification_loss': 0.9431657981872559}
2025-01-19 07:55:09,997 [INFO] Step[500/2713]: training loss : 0.9414374828338623 TRAIN  loss dict:  {'classification_loss': 0.9414374828338623}
2025-01-19 07:55:25,039 [INFO] Step[550/2713]: training loss : 0.9418471741676331 TRAIN  loss dict:  {'classification_loss': 0.9418471741676331}
2025-01-19 07:55:40,025 [INFO] Step[600/2713]: training loss : 0.9419431793689728 TRAIN  loss dict:  {'classification_loss': 0.9419431793689728}
2025-01-19 07:55:55,033 [INFO] Step[650/2713]: training loss : 0.9419586086273193 TRAIN  loss dict:  {'classification_loss': 0.9419586086273193}
2025-01-19 07:56:09,959 [INFO] Step[700/2713]: training loss : 0.9458282387256622 TRAIN  loss dict:  {'classification_loss': 0.9458282387256622}
2025-01-19 07:56:24,989 [INFO] Step[750/2713]: training loss : 0.9428196513652801 TRAIN  loss dict:  {'classification_loss': 0.9428196513652801}
2025-01-19 07:56:39,995 [INFO] Step[800/2713]: training loss : 0.9473819017410279 TRAIN  loss dict:  {'classification_loss': 0.9473819017410279}
2025-01-19 07:56:55,025 [INFO] Step[850/2713]: training loss : 0.9422080600261689 TRAIN  loss dict:  {'classification_loss': 0.9422080600261689}
2025-01-19 07:57:09,981 [INFO] Step[900/2713]: training loss : 0.954617406129837 TRAIN  loss dict:  {'classification_loss': 0.954617406129837}
2025-01-19 07:57:24,960 [INFO] Step[950/2713]: training loss : 0.9434498035907746 TRAIN  loss dict:  {'classification_loss': 0.9434498035907746}
2025-01-19 07:57:39,903 [INFO] Step[1000/2713]: training loss : 0.9431408274173737 TRAIN  loss dict:  {'classification_loss': 0.9431408274173737}
2025-01-19 07:57:54,910 [INFO] Step[1050/2713]: training loss : 0.9496655142307282 TRAIN  loss dict:  {'classification_loss': 0.9496655142307282}
2025-01-19 07:58:09,824 [INFO] Step[1100/2713]: training loss : 0.9423306894302368 TRAIN  loss dict:  {'classification_loss': 0.9423306894302368}
2025-01-19 07:58:24,778 [INFO] Step[1150/2713]: training loss : 0.942045146226883 TRAIN  loss dict:  {'classification_loss': 0.942045146226883}
2025-01-19 07:58:39,721 [INFO] Step[1200/2713]: training loss : 0.9446799027919769 TRAIN  loss dict:  {'classification_loss': 0.9446799027919769}
2025-01-19 07:58:54,710 [INFO] Step[1250/2713]: training loss : 0.9444592416286468 TRAIN  loss dict:  {'classification_loss': 0.9444592416286468}
2025-01-19 07:59:09,710 [INFO] Step[1300/2713]: training loss : 0.943755761384964 TRAIN  loss dict:  {'classification_loss': 0.943755761384964}
2025-01-19 07:59:24,694 [INFO] Step[1350/2713]: training loss : 0.9428779661655426 TRAIN  loss dict:  {'classification_loss': 0.9428779661655426}
2025-01-19 07:59:39,681 [INFO] Step[1400/2713]: training loss : 0.942761470079422 TRAIN  loss dict:  {'classification_loss': 0.942761470079422}
2025-01-19 07:59:54,728 [INFO] Step[1450/2713]: training loss : 0.9458924543857574 TRAIN  loss dict:  {'classification_loss': 0.9458924543857574}
2025-01-19 08:00:09,692 [INFO] Step[1500/2713]: training loss : 0.9498783314228058 TRAIN  loss dict:  {'classification_loss': 0.9498783314228058}
2025-01-19 08:00:24,672 [INFO] Step[1550/2713]: training loss : 0.9441397368907929 TRAIN  loss dict:  {'classification_loss': 0.9441397368907929}
2025-01-19 08:00:39,623 [INFO] Step[1600/2713]: training loss : 0.9424179756641388 TRAIN  loss dict:  {'classification_loss': 0.9424179756641388}
2025-01-19 08:00:54,643 [INFO] Step[1650/2713]: training loss : 0.9439259696006775 TRAIN  loss dict:  {'classification_loss': 0.9439259696006775}
2025-01-19 08:01:09,630 [INFO] Step[1700/2713]: training loss : 0.9435370147228241 TRAIN  loss dict:  {'classification_loss': 0.9435370147228241}
2025-01-19 08:01:24,635 [INFO] Step[1750/2713]: training loss : 0.963647176027298 TRAIN  loss dict:  {'classification_loss': 0.963647176027298}
2025-01-19 08:01:39,566 [INFO] Step[1800/2713]: training loss : 0.9440413773059845 TRAIN  loss dict:  {'classification_loss': 0.9440413773059845}
2025-01-19 08:01:54,567 [INFO] Step[1850/2713]: training loss : 0.9439703464508057 TRAIN  loss dict:  {'classification_loss': 0.9439703464508057}
2025-01-19 08:02:09,599 [INFO] Step[1900/2713]: training loss : 0.9420746862888336 TRAIN  loss dict:  {'classification_loss': 0.9420746862888336}
2025-01-19 08:02:24,554 [INFO] Step[1950/2713]: training loss : 0.9495099675655365 TRAIN  loss dict:  {'classification_loss': 0.9495099675655365}
2025-01-19 08:02:39,555 [INFO] Step[2000/2713]: training loss : 0.9433735620975494 TRAIN  loss dict:  {'classification_loss': 0.9433735620975494}
2025-01-19 08:02:54,569 [INFO] Step[2050/2713]: training loss : 0.9442694664001465 TRAIN  loss dict:  {'classification_loss': 0.9442694664001465}
2025-01-19 08:03:09,536 [INFO] Step[2100/2713]: training loss : 0.9499229705333709 TRAIN  loss dict:  {'classification_loss': 0.9499229705333709}
2025-01-19 08:03:24,534 [INFO] Step[2150/2713]: training loss : 0.9582752156257629 TRAIN  loss dict:  {'classification_loss': 0.9582752156257629}
2025-01-19 08:03:39,549 [INFO] Step[2200/2713]: training loss : 0.9476537740230561 TRAIN  loss dict:  {'classification_loss': 0.9476537740230561}
2025-01-19 08:03:54,568 [INFO] Step[2250/2713]: training loss : 0.9401370966434479 TRAIN  loss dict:  {'classification_loss': 0.9401370966434479}
2025-01-19 08:04:09,602 [INFO] Step[2300/2713]: training loss : 0.9418106412887574 TRAIN  loss dict:  {'classification_loss': 0.9418106412887574}
2025-01-19 08:04:24,629 [INFO] Step[2350/2713]: training loss : 0.9415180432796478 TRAIN  loss dict:  {'classification_loss': 0.9415180432796478}
2025-01-19 08:04:39,571 [INFO] Step[2400/2713]: training loss : 0.9504172801971436 TRAIN  loss dict:  {'classification_loss': 0.9504172801971436}
2025-01-19 08:04:54,571 [INFO] Step[2450/2713]: training loss : 0.9420849514007569 TRAIN  loss dict:  {'classification_loss': 0.9420849514007569}
2025-01-19 08:05:09,589 [INFO] Step[2500/2713]: training loss : 0.9540837633609772 TRAIN  loss dict:  {'classification_loss': 0.9540837633609772}
2025-01-19 08:05:24,625 [INFO] Step[2550/2713]: training loss : 0.9425014352798462 TRAIN  loss dict:  {'classification_loss': 0.9425014352798462}
2025-01-19 08:05:39,624 [INFO] Step[2600/2713]: training loss : 0.9450508368015289 TRAIN  loss dict:  {'classification_loss': 0.9450508368015289}
2025-01-19 08:05:54,600 [INFO] Step[2650/2713]: training loss : 0.943707538843155 TRAIN  loss dict:  {'classification_loss': 0.943707538843155}
2025-01-19 08:06:09,623 [INFO] Step[2700/2713]: training loss : 0.945204690694809 TRAIN  loss dict:  {'classification_loss': 0.945204690694809}
2025-01-19 08:07:29,731 [INFO] Label accuracies statistics:
2025-01-19 08:07:29,731 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 1.0, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.5, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 1.0, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.25, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 08:07:29,733 [INFO] [69] TRAIN  loss: 0.9455110508930714 acc: 0.9993856739157144
2025-01-19 08:07:29,733 [INFO] [69] TRAIN  loss dict: {'classification_loss': 0.9455110508930714}
2025-01-19 08:07:29,733 [INFO] [69] VALIDATION loss: 1.7265883783872862 VALIDATION acc: 0.8100313479623824
2025-01-19 08:07:29,733 [INFO] [69] VALIDATION loss dict: {'classification_loss': 1.7265883783872862}
2025-01-19 08:07:29,733 [INFO] 
2025-01-19 08:07:49,919 [INFO] Step[50/2713]: training loss : 0.9456187963485718 TRAIN  loss dict:  {'classification_loss': 0.9456187963485718}
2025-01-19 08:08:04,910 [INFO] Step[100/2713]: training loss : 0.9488338422775269 TRAIN  loss dict:  {'classification_loss': 0.9488338422775269}
2025-01-19 08:08:20,032 [INFO] Step[150/2713]: training loss : 0.9405696165561676 TRAIN  loss dict:  {'classification_loss': 0.9405696165561676}
2025-01-19 08:08:35,104 [INFO] Step[200/2713]: training loss : 0.9410578954219818 TRAIN  loss dict:  {'classification_loss': 0.9410578954219818}
2025-01-19 08:08:50,157 [INFO] Step[250/2713]: training loss : 0.941121072769165 TRAIN  loss dict:  {'classification_loss': 0.941121072769165}
2025-01-19 08:09:05,205 [INFO] Step[300/2713]: training loss : 0.9442110705375671 TRAIN  loss dict:  {'classification_loss': 0.9442110705375671}
2025-01-19 08:09:20,303 [INFO] Step[350/2713]: training loss : 0.9462394654750824 TRAIN  loss dict:  {'classification_loss': 0.9462394654750824}
2025-01-19 08:09:35,320 [INFO] Step[400/2713]: training loss : 0.9410907745361328 TRAIN  loss dict:  {'classification_loss': 0.9410907745361328}
2025-01-19 08:09:50,401 [INFO] Step[450/2713]: training loss : 0.9435527563095093 TRAIN  loss dict:  {'classification_loss': 0.9435527563095093}
2025-01-19 08:10:05,464 [INFO] Step[500/2713]: training loss : 0.9421487081050873 TRAIN  loss dict:  {'classification_loss': 0.9421487081050873}
2025-01-19 08:10:20,526 [INFO] Step[550/2713]: training loss : 0.9439566242694855 TRAIN  loss dict:  {'classification_loss': 0.9439566242694855}
2025-01-19 08:10:35,552 [INFO] Step[600/2713]: training loss : 0.9541941571235657 TRAIN  loss dict:  {'classification_loss': 0.9541941571235657}
2025-01-19 08:10:50,688 [INFO] Step[650/2713]: training loss : 0.9417919135093689 TRAIN  loss dict:  {'classification_loss': 0.9417919135093689}
2025-01-19 08:11:05,810 [INFO] Step[700/2713]: training loss : 0.9441096007823944 TRAIN  loss dict:  {'classification_loss': 0.9441096007823944}
2025-01-19 08:11:20,839 [INFO] Step[750/2713]: training loss : 0.9457010102272033 TRAIN  loss dict:  {'classification_loss': 0.9457010102272033}
2025-01-19 08:11:35,906 [INFO] Step[800/2713]: training loss : 0.9412196397781372 TRAIN  loss dict:  {'classification_loss': 0.9412196397781372}
2025-01-19 08:11:50,924 [INFO] Step[850/2713]: training loss : 0.9521748578548431 TRAIN  loss dict:  {'classification_loss': 0.9521748578548431}
2025-01-19 08:12:05,983 [INFO] Step[900/2713]: training loss : 0.9459474945068359 TRAIN  loss dict:  {'classification_loss': 0.9459474945068359}
2025-01-19 08:12:21,016 [INFO] Step[950/2713]: training loss : 0.9398527026176453 TRAIN  loss dict:  {'classification_loss': 0.9398527026176453}
2025-01-19 08:12:36,086 [INFO] Step[1000/2713]: training loss : 0.9476325392723084 TRAIN  loss dict:  {'classification_loss': 0.9476325392723084}
2025-01-19 08:12:51,133 [INFO] Step[1050/2713]: training loss : 0.9457016563415528 TRAIN  loss dict:  {'classification_loss': 0.9457016563415528}
2025-01-19 08:13:06,223 [INFO] Step[1100/2713]: training loss : 0.9427266085147857 TRAIN  loss dict:  {'classification_loss': 0.9427266085147857}
2025-01-19 08:13:21,293 [INFO] Step[1150/2713]: training loss : 0.9409404253959656 TRAIN  loss dict:  {'classification_loss': 0.9409404253959656}
2025-01-19 08:13:36,376 [INFO] Step[1200/2713]: training loss : 0.9427062892913818 TRAIN  loss dict:  {'classification_loss': 0.9427062892913818}
2025-01-19 08:13:51,456 [INFO] Step[1250/2713]: training loss : 0.9407950425148011 TRAIN  loss dict:  {'classification_loss': 0.9407950425148011}
2025-01-19 08:14:06,592 [INFO] Step[1300/2713]: training loss : 0.941036936044693 TRAIN  loss dict:  {'classification_loss': 0.941036936044693}
2025-01-19 08:14:21,651 [INFO] Step[1350/2713]: training loss : 0.9419250571727753 TRAIN  loss dict:  {'classification_loss': 0.9419250571727753}
2025-01-19 08:14:36,693 [INFO] Step[1400/2713]: training loss : 0.9419839942455291 TRAIN  loss dict:  {'classification_loss': 0.9419839942455291}
2025-01-19 08:14:51,745 [INFO] Step[1450/2713]: training loss : 0.9422011268138886 TRAIN  loss dict:  {'classification_loss': 0.9422011268138886}
2025-01-19 08:15:06,777 [INFO] Step[1500/2713]: training loss : 0.9405536425113677 TRAIN  loss dict:  {'classification_loss': 0.9405536425113677}
2025-01-19 08:15:21,837 [INFO] Step[1550/2713]: training loss : 0.9450581204891205 TRAIN  loss dict:  {'classification_loss': 0.9450581204891205}
2025-01-19 08:15:36,858 [INFO] Step[1600/2713]: training loss : 0.9433917510509491 TRAIN  loss dict:  {'classification_loss': 0.9433917510509491}
2025-01-19 08:15:51,888 [INFO] Step[1650/2713]: training loss : 0.9456618118286133 TRAIN  loss dict:  {'classification_loss': 0.9456618118286133}
2025-01-19 08:16:06,879 [INFO] Step[1700/2713]: training loss : 0.946163889169693 TRAIN  loss dict:  {'classification_loss': 0.946163889169693}
2025-01-19 08:16:21,977 [INFO] Step[1750/2713]: training loss : 0.9423307847976684 TRAIN  loss dict:  {'classification_loss': 0.9423307847976684}
2025-01-19 08:16:36,984 [INFO] Step[1800/2713]: training loss : 0.9449123775959015 TRAIN  loss dict:  {'classification_loss': 0.9449123775959015}
2025-01-19 08:16:52,079 [INFO] Step[1850/2713]: training loss : 0.9420592117309571 TRAIN  loss dict:  {'classification_loss': 0.9420592117309571}
2025-01-19 08:17:07,107 [INFO] Step[1900/2713]: training loss : 0.9431084966659546 TRAIN  loss dict:  {'classification_loss': 0.9431084966659546}
2025-01-19 08:17:22,185 [INFO] Step[1950/2713]: training loss : 0.9432979786396026 TRAIN  loss dict:  {'classification_loss': 0.9432979786396026}
2025-01-19 08:17:37,188 [INFO] Step[2000/2713]: training loss : 0.942733371257782 TRAIN  loss dict:  {'classification_loss': 0.942733371257782}
2025-01-19 08:17:52,234 [INFO] Step[2050/2713]: training loss : 0.946956079006195 TRAIN  loss dict:  {'classification_loss': 0.946956079006195}
2025-01-19 08:18:07,268 [INFO] Step[2100/2713]: training loss : 0.9437928891181946 TRAIN  loss dict:  {'classification_loss': 0.9437928891181946}
2025-01-19 08:18:22,318 [INFO] Step[2150/2713]: training loss : 0.9411599707603454 TRAIN  loss dict:  {'classification_loss': 0.9411599707603454}
2025-01-19 08:18:37,365 [INFO] Step[2200/2713]: training loss : 0.9391681122779846 TRAIN  loss dict:  {'classification_loss': 0.9391681122779846}
2025-01-19 08:18:52,504 [INFO] Step[2250/2713]: training loss : 0.9415013837814331 TRAIN  loss dict:  {'classification_loss': 0.9415013837814331}
2025-01-19 08:19:07,596 [INFO] Step[2300/2713]: training loss : 0.9418733322620392 TRAIN  loss dict:  {'classification_loss': 0.9418733322620392}
2025-01-19 08:19:22,664 [INFO] Step[2350/2713]: training loss : 0.9421081507205963 TRAIN  loss dict:  {'classification_loss': 0.9421081507205963}
2025-01-19 08:19:37,721 [INFO] Step[2400/2713]: training loss : 0.9464005649089813 TRAIN  loss dict:  {'classification_loss': 0.9464005649089813}
2025-01-19 08:19:52,796 [INFO] Step[2450/2713]: training loss : 0.9436741650104523 TRAIN  loss dict:  {'classification_loss': 0.9436741650104523}
2025-01-19 08:20:07,941 [INFO] Step[2500/2713]: training loss : 0.9435251915454864 TRAIN  loss dict:  {'classification_loss': 0.9435251915454864}
2025-01-19 08:20:23,035 [INFO] Step[2550/2713]: training loss : 0.9441345715522766 TRAIN  loss dict:  {'classification_loss': 0.9441345715522766}
2025-01-19 08:20:38,102 [INFO] Step[2600/2713]: training loss : 0.9410459458827972 TRAIN  loss dict:  {'classification_loss': 0.9410459458827972}
2025-01-19 08:20:53,173 [INFO] Step[2650/2713]: training loss : 0.9449925196170806 TRAIN  loss dict:  {'classification_loss': 0.9449925196170806}
2025-01-19 08:21:08,246 [INFO] Step[2700/2713]: training loss : 0.9434338998794556 TRAIN  loss dict:  {'classification_loss': 0.9434338998794556}
2025-01-19 08:22:28,465 [INFO] Label accuracies statistics:
2025-01-19 08:22:28,465 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 1.0, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.5, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 08:22:28,468 [INFO] [70] TRAIN  loss: 0.9436099437871104 acc: 0.9998771347831429
2025-01-19 08:22:28,468 [INFO] [70] TRAIN  loss dict: {'classification_loss': 0.9436099437871104}
2025-01-19 08:22:28,468 [INFO] [70] VALIDATION loss: 1.6933163128849258 VALIDATION acc: 0.8131661442006269
2025-01-19 08:22:28,468 [INFO] [70] VALIDATION loss dict: {'classification_loss': 1.6933163128849258}
2025-01-19 08:22:28,468 [INFO] 
2025-01-19 08:22:48,527 [INFO] Step[50/2713]: training loss : 0.9468412530422211 TRAIN  loss dict:  {'classification_loss': 0.9468412530422211}
2025-01-19 08:23:03,619 [INFO] Step[100/2713]: training loss : 0.9446290564537049 TRAIN  loss dict:  {'classification_loss': 0.9446290564537049}
2025-01-19 08:23:18,765 [INFO] Step[150/2713]: training loss : 0.9484240102767945 TRAIN  loss dict:  {'classification_loss': 0.9484240102767945}
2025-01-19 08:23:33,931 [INFO] Step[200/2713]: training loss : 0.9557511913776398 TRAIN  loss dict:  {'classification_loss': 0.9557511913776398}
2025-01-19 08:23:49,126 [INFO] Step[250/2713]: training loss : 0.9418634521961212 TRAIN  loss dict:  {'classification_loss': 0.9418634521961212}
2025-01-19 08:24:04,303 [INFO] Step[300/2713]: training loss : 0.9434314632415771 TRAIN  loss dict:  {'classification_loss': 0.9434314632415771}
2025-01-19 08:24:19,499 [INFO] Step[350/2713]: training loss : 0.9427448713779449 TRAIN  loss dict:  {'classification_loss': 0.9427448713779449}
2025-01-19 08:24:34,635 [INFO] Step[400/2713]: training loss : 0.9406438422203064 TRAIN  loss dict:  {'classification_loss': 0.9406438422203064}
2025-01-19 08:24:49,790 [INFO] Step[450/2713]: training loss : 0.9431974351406097 TRAIN  loss dict:  {'classification_loss': 0.9431974351406097}
2025-01-19 08:25:04,956 [INFO] Step[500/2713]: training loss : 0.9425884127616883 TRAIN  loss dict:  {'classification_loss': 0.9425884127616883}
2025-01-19 08:25:20,169 [INFO] Step[550/2713]: training loss : 0.946136794090271 TRAIN  loss dict:  {'classification_loss': 0.946136794090271}
2025-01-19 08:25:35,370 [INFO] Step[600/2713]: training loss : 0.9457958269119263 TRAIN  loss dict:  {'classification_loss': 0.9457958269119263}
2025-01-19 08:25:50,560 [INFO] Step[650/2713]: training loss : 0.9440845012664795 TRAIN  loss dict:  {'classification_loss': 0.9440845012664795}
2025-01-19 08:26:05,714 [INFO] Step[700/2713]: training loss : 0.9422088634967803 TRAIN  loss dict:  {'classification_loss': 0.9422088634967803}
2025-01-19 08:26:20,875 [INFO] Step[750/2713]: training loss : 0.9406119430065155 TRAIN  loss dict:  {'classification_loss': 0.9406119430065155}
2025-01-19 08:26:36,046 [INFO] Step[800/2713]: training loss : 0.9424900817871094 TRAIN  loss dict:  {'classification_loss': 0.9424900817871094}
2025-01-19 08:26:51,187 [INFO] Step[850/2713]: training loss : 0.9454618322849274 TRAIN  loss dict:  {'classification_loss': 0.9454618322849274}
2025-01-19 08:27:06,303 [INFO] Step[900/2713]: training loss : 0.9453731524944305 TRAIN  loss dict:  {'classification_loss': 0.9453731524944305}
2025-01-19 08:27:21,458 [INFO] Step[950/2713]: training loss : 0.9432636380195618 TRAIN  loss dict:  {'classification_loss': 0.9432636380195618}
2025-01-19 08:27:36,577 [INFO] Step[1000/2713]: training loss : 0.9425583279132843 TRAIN  loss dict:  {'classification_loss': 0.9425583279132843}
2025-01-19 08:27:51,732 [INFO] Step[1050/2713]: training loss : 0.9406919753551484 TRAIN  loss dict:  {'classification_loss': 0.9406919753551484}
2025-01-19 08:28:06,913 [INFO] Step[1100/2713]: training loss : 0.9429907882213593 TRAIN  loss dict:  {'classification_loss': 0.9429907882213593}
2025-01-19 08:28:22,074 [INFO] Step[1150/2713]: training loss : 0.9449471962451935 TRAIN  loss dict:  {'classification_loss': 0.9449471962451935}
2025-01-19 08:28:37,234 [INFO] Step[1200/2713]: training loss : 0.9418992936611176 TRAIN  loss dict:  {'classification_loss': 0.9418992936611176}
2025-01-19 08:28:52,398 [INFO] Step[1250/2713]: training loss : 0.9420705449581146 TRAIN  loss dict:  {'classification_loss': 0.9420705449581146}
2025-01-19 08:29:07,476 [INFO] Step[1300/2713]: training loss : 0.9527165150642395 TRAIN  loss dict:  {'classification_loss': 0.9527165150642395}
2025-01-19 08:29:22,614 [INFO] Step[1350/2713]: training loss : 0.9411798405647278 TRAIN  loss dict:  {'classification_loss': 0.9411798405647278}
2025-01-19 08:29:37,767 [INFO] Step[1400/2713]: training loss : 0.9416844546794891 TRAIN  loss dict:  {'classification_loss': 0.9416844546794891}
2025-01-19 08:29:52,938 [INFO] Step[1450/2713]: training loss : 0.9433172929286957 TRAIN  loss dict:  {'classification_loss': 0.9433172929286957}
2025-01-19 08:30:08,120 [INFO] Step[1500/2713]: training loss : 0.9469145786762238 TRAIN  loss dict:  {'classification_loss': 0.9469145786762238}
2025-01-19 08:30:23,300 [INFO] Step[1550/2713]: training loss : 0.9416271626949311 TRAIN  loss dict:  {'classification_loss': 0.9416271626949311}
2025-01-19 08:30:38,468 [INFO] Step[1600/2713]: training loss : 0.9434072971343994 TRAIN  loss dict:  {'classification_loss': 0.9434072971343994}
2025-01-19 08:30:53,652 [INFO] Step[1650/2713]: training loss : 0.9435418677330017 TRAIN  loss dict:  {'classification_loss': 0.9435418677330017}
2025-01-19 08:31:08,842 [INFO] Step[1700/2713]: training loss : 0.941990134716034 TRAIN  loss dict:  {'classification_loss': 0.941990134716034}
2025-01-19 08:31:24,016 [INFO] Step[1750/2713]: training loss : 0.941033605337143 TRAIN  loss dict:  {'classification_loss': 0.941033605337143}
2025-01-19 08:31:39,146 [INFO] Step[1800/2713]: training loss : 0.9429427099227905 TRAIN  loss dict:  {'classification_loss': 0.9429427099227905}
2025-01-19 08:31:54,265 [INFO] Step[1850/2713]: training loss : 0.9436601912975311 TRAIN  loss dict:  {'classification_loss': 0.9436601912975311}
2025-01-19 08:32:09,374 [INFO] Step[1900/2713]: training loss : 0.9403441393375397 TRAIN  loss dict:  {'classification_loss': 0.9403441393375397}
2025-01-19 08:32:24,540 [INFO] Step[1950/2713]: training loss : 0.941431473493576 TRAIN  loss dict:  {'classification_loss': 0.941431473493576}
2025-01-19 08:32:39,748 [INFO] Step[2000/2713]: training loss : 0.939701691865921 TRAIN  loss dict:  {'classification_loss': 0.939701691865921}
2025-01-19 08:32:54,921 [INFO] Step[2050/2713]: training loss : 0.9395248687267304 TRAIN  loss dict:  {'classification_loss': 0.9395248687267304}
2025-01-19 08:33:10,039 [INFO] Step[2100/2713]: training loss : 0.943847119808197 TRAIN  loss dict:  {'classification_loss': 0.943847119808197}
2025-01-19 08:33:25,162 [INFO] Step[2150/2713]: training loss : 0.9411701369285583 TRAIN  loss dict:  {'classification_loss': 0.9411701369285583}
2025-01-19 08:33:40,355 [INFO] Step[2200/2713]: training loss : 0.9407600557804108 TRAIN  loss dict:  {'classification_loss': 0.9407600557804108}
2025-01-19 08:33:55,494 [INFO] Step[2250/2713]: training loss : 0.9408354246616364 TRAIN  loss dict:  {'classification_loss': 0.9408354246616364}
2025-01-19 08:34:10,644 [INFO] Step[2300/2713]: training loss : 0.9471561801433563 TRAIN  loss dict:  {'classification_loss': 0.9471561801433563}
2025-01-19 08:34:25,804 [INFO] Step[2350/2713]: training loss : 0.9404558563232421 TRAIN  loss dict:  {'classification_loss': 0.9404558563232421}
2025-01-19 08:34:40,924 [INFO] Step[2400/2713]: training loss : 0.9410263347625732 TRAIN  loss dict:  {'classification_loss': 0.9410263347625732}
2025-01-19 08:34:56,093 [INFO] Step[2450/2713]: training loss : 0.9409616684913635 TRAIN  loss dict:  {'classification_loss': 0.9409616684913635}
2025-01-19 08:35:11,203 [INFO] Step[2500/2713]: training loss : 0.965657000541687 TRAIN  loss dict:  {'classification_loss': 0.965657000541687}
2025-01-19 08:35:26,329 [INFO] Step[2550/2713]: training loss : 0.9395250606536866 TRAIN  loss dict:  {'classification_loss': 0.9395250606536866}
2025-01-19 08:35:41,443 [INFO] Step[2600/2713]: training loss : 0.9387644755840302 TRAIN  loss dict:  {'classification_loss': 0.9387644755840302}
2025-01-19 08:35:56,589 [INFO] Step[2650/2713]: training loss : 0.9425281417369843 TRAIN  loss dict:  {'classification_loss': 0.9425281417369843}
2025-01-19 08:36:11,635 [INFO] Step[2700/2713]: training loss : 0.9441170644760132 TRAIN  loss dict:  {'classification_loss': 0.9441170644760132}
2025-01-19 08:37:31,408 [INFO] Label accuracies statistics:
2025-01-19 08:37:31,409 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 1.0, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 08:37:31,410 [INFO] [71] TRAIN  loss: 0.9435637933145447 acc: 0.9996314043494287
2025-01-19 08:37:31,410 [INFO] [71] TRAIN  loss dict: {'classification_loss': 0.9435637933145447}
2025-01-19 08:37:31,411 [INFO] [71] VALIDATION loss: 1.7067732133139344 VALIDATION acc: 0.8163009404388715
2025-01-19 08:37:31,411 [INFO] [71] VALIDATION loss dict: {'classification_loss': 1.7067732133139344}
2025-01-19 08:37:31,411 [INFO] 
2025-01-19 08:37:51,860 [INFO] Step[50/2713]: training loss : 0.9410686552524566 TRAIN  loss dict:  {'classification_loss': 0.9410686552524566}
2025-01-19 08:38:06,799 [INFO] Step[100/2713]: training loss : 0.9397416532039642 TRAIN  loss dict:  {'classification_loss': 0.9397416532039642}
2025-01-19 08:38:21,782 [INFO] Step[150/2713]: training loss : 0.9405360555648804 TRAIN  loss dict:  {'classification_loss': 0.9405360555648804}
2025-01-19 08:38:36,810 [INFO] Step[200/2713]: training loss : 0.9598634207248687 TRAIN  loss dict:  {'classification_loss': 0.9598634207248687}
2025-01-19 08:38:51,828 [INFO] Step[250/2713]: training loss : 0.9425449049472809 TRAIN  loss dict:  {'classification_loss': 0.9425449049472809}
2025-01-19 08:39:06,874 [INFO] Step[300/2713]: training loss : 0.9392710292339325 TRAIN  loss dict:  {'classification_loss': 0.9392710292339325}
2025-01-19 08:39:21,852 [INFO] Step[350/2713]: training loss : 0.9454103660583496 TRAIN  loss dict:  {'classification_loss': 0.9454103660583496}
2025-01-19 08:39:36,859 [INFO] Step[400/2713]: training loss : 0.94085906624794 TRAIN  loss dict:  {'classification_loss': 0.94085906624794}
2025-01-19 08:39:51,858 [INFO] Step[450/2713]: training loss : 0.9403778684139251 TRAIN  loss dict:  {'classification_loss': 0.9403778684139251}
2025-01-19 08:40:06,864 [INFO] Step[500/2713]: training loss : 0.9443115055561065 TRAIN  loss dict:  {'classification_loss': 0.9443115055561065}
2025-01-19 08:40:21,971 [INFO] Step[550/2713]: training loss : 0.9408757376670838 TRAIN  loss dict:  {'classification_loss': 0.9408757376670838}
2025-01-19 08:40:37,034 [INFO] Step[600/2713]: training loss : 0.9423335123062134 TRAIN  loss dict:  {'classification_loss': 0.9423335123062134}
2025-01-19 08:40:52,088 [INFO] Step[650/2713]: training loss : 0.9450053358078003 TRAIN  loss dict:  {'classification_loss': 0.9450053358078003}
2025-01-19 08:41:07,189 [INFO] Step[700/2713]: training loss : 0.9397057604789734 TRAIN  loss dict:  {'classification_loss': 0.9397057604789734}
2025-01-19 08:41:22,221 [INFO] Step[750/2713]: training loss : 0.9431574714183807 TRAIN  loss dict:  {'classification_loss': 0.9431574714183807}
2025-01-19 08:41:37,159 [INFO] Step[800/2713]: training loss : 0.9408403182029724 TRAIN  loss dict:  {'classification_loss': 0.9408403182029724}
2025-01-19 08:41:52,244 [INFO] Step[850/2713]: training loss : 0.9421258115768433 TRAIN  loss dict:  {'classification_loss': 0.9421258115768433}
2025-01-19 08:42:07,325 [INFO] Step[900/2713]: training loss : 0.9402514386177063 TRAIN  loss dict:  {'classification_loss': 0.9402514386177063}
2025-01-19 08:42:22,370 [INFO] Step[950/2713]: training loss : 0.9414906418323516 TRAIN  loss dict:  {'classification_loss': 0.9414906418323516}
2025-01-19 08:42:37,339 [INFO] Step[1000/2713]: training loss : 0.9382032442092896 TRAIN  loss dict:  {'classification_loss': 0.9382032442092896}
2025-01-19 08:42:52,293 [INFO] Step[1050/2713]: training loss : 0.940860720872879 TRAIN  loss dict:  {'classification_loss': 0.940860720872879}
2025-01-19 08:43:07,309 [INFO] Step[1100/2713]: training loss : 0.9418448746204376 TRAIN  loss dict:  {'classification_loss': 0.9418448746204376}
2025-01-19 08:43:22,271 [INFO] Step[1150/2713]: training loss : 0.9407815289497375 TRAIN  loss dict:  {'classification_loss': 0.9407815289497375}
2025-01-19 08:43:37,208 [INFO] Step[1200/2713]: training loss : 0.9508270573616028 TRAIN  loss dict:  {'classification_loss': 0.9508270573616028}
2025-01-19 08:43:52,232 [INFO] Step[1250/2713]: training loss : 0.9460950672626496 TRAIN  loss dict:  {'classification_loss': 0.9460950672626496}
2025-01-19 08:44:07,264 [INFO] Step[1300/2713]: training loss : 0.9413719046115875 TRAIN  loss dict:  {'classification_loss': 0.9413719046115875}
2025-01-19 08:44:22,285 [INFO] Step[1350/2713]: training loss : 0.9486015498638153 TRAIN  loss dict:  {'classification_loss': 0.9486015498638153}
2025-01-19 08:44:37,239 [INFO] Step[1400/2713]: training loss : 0.9427185285091401 TRAIN  loss dict:  {'classification_loss': 0.9427185285091401}
2025-01-19 08:44:52,278 [INFO] Step[1450/2713]: training loss : 0.9436499428749084 TRAIN  loss dict:  {'classification_loss': 0.9436499428749084}
2025-01-19 08:45:07,339 [INFO] Step[1500/2713]: training loss : 0.9433661723136901 TRAIN  loss dict:  {'classification_loss': 0.9433661723136901}
2025-01-19 08:45:22,373 [INFO] Step[1550/2713]: training loss : 0.9414553463459014 TRAIN  loss dict:  {'classification_loss': 0.9414553463459014}
2025-01-19 08:45:37,475 [INFO] Step[1600/2713]: training loss : 0.952777247428894 TRAIN  loss dict:  {'classification_loss': 0.952777247428894}
2025-01-19 08:45:52,577 [INFO] Step[1650/2713]: training loss : 0.9459633326530457 TRAIN  loss dict:  {'classification_loss': 0.9459633326530457}
2025-01-19 08:46:07,645 [INFO] Step[1700/2713]: training loss : 0.9402885437011719 TRAIN  loss dict:  {'classification_loss': 0.9402885437011719}
2025-01-19 08:46:22,728 [INFO] Step[1750/2713]: training loss : 0.9422855043411255 TRAIN  loss dict:  {'classification_loss': 0.9422855043411255}
2025-01-19 08:46:37,772 [INFO] Step[1800/2713]: training loss : 0.940869003534317 TRAIN  loss dict:  {'classification_loss': 0.940869003534317}
2025-01-19 08:46:52,842 [INFO] Step[1850/2713]: training loss : 0.9391819441318512 TRAIN  loss dict:  {'classification_loss': 0.9391819441318512}
2025-01-19 08:47:07,851 [INFO] Step[1900/2713]: training loss : 0.9383398067951202 TRAIN  loss dict:  {'classification_loss': 0.9383398067951202}
2025-01-19 08:47:22,904 [INFO] Step[1950/2713]: training loss : 0.9392593288421631 TRAIN  loss dict:  {'classification_loss': 0.9392593288421631}
2025-01-19 08:47:37,971 [INFO] Step[2000/2713]: training loss : 0.9409681606292725 TRAIN  loss dict:  {'classification_loss': 0.9409681606292725}
2025-01-19 08:47:53,040 [INFO] Step[2050/2713]: training loss : 0.9396018767356873 TRAIN  loss dict:  {'classification_loss': 0.9396018767356873}
2025-01-19 08:48:08,113 [INFO] Step[2100/2713]: training loss : 0.9390116691589355 TRAIN  loss dict:  {'classification_loss': 0.9390116691589355}
2025-01-19 08:48:23,127 [INFO] Step[2150/2713]: training loss : 0.9394422113895416 TRAIN  loss dict:  {'classification_loss': 0.9394422113895416}
2025-01-19 08:48:38,201 [INFO] Step[2200/2713]: training loss : 0.9417364704608917 TRAIN  loss dict:  {'classification_loss': 0.9417364704608917}
2025-01-19 08:48:53,237 [INFO] Step[2250/2713]: training loss : 0.9439441967010498 TRAIN  loss dict:  {'classification_loss': 0.9439441967010498}
2025-01-19 08:49:08,263 [INFO] Step[2300/2713]: training loss : 0.9400721824169159 TRAIN  loss dict:  {'classification_loss': 0.9400721824169159}
2025-01-19 08:49:23,297 [INFO] Step[2350/2713]: training loss : 0.948605568408966 TRAIN  loss dict:  {'classification_loss': 0.948605568408966}
2025-01-19 08:49:38,376 [INFO] Step[2400/2713]: training loss : 0.9406953942775726 TRAIN  loss dict:  {'classification_loss': 0.9406953942775726}
2025-01-19 08:49:53,443 [INFO] Step[2450/2713]: training loss : 0.9413269519805908 TRAIN  loss dict:  {'classification_loss': 0.9413269519805908}
2025-01-19 08:50:08,494 [INFO] Step[2500/2713]: training loss : 0.9470326948165894 TRAIN  loss dict:  {'classification_loss': 0.9470326948165894}
2025-01-19 08:50:23,568 [INFO] Step[2550/2713]: training loss : 0.9598039388656616 TRAIN  loss dict:  {'classification_loss': 0.9598039388656616}
2025-01-19 08:50:38,636 [INFO] Step[2600/2713]: training loss : 0.9403467690944671 TRAIN  loss dict:  {'classification_loss': 0.9403467690944671}
2025-01-19 08:50:53,698 [INFO] Step[2650/2713]: training loss : 0.9433651089668273 TRAIN  loss dict:  {'classification_loss': 0.9433651089668273}
2025-01-19 08:51:08,783 [INFO] Step[2700/2713]: training loss : 0.9420142650604248 TRAIN  loss dict:  {'classification_loss': 0.9420142650604248}
2025-01-19 08:52:29,205 [INFO] Label accuracies statistics:
2025-01-19 08:52:29,205 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 1.0, 208: 0.5, 209: 0.5, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 1.0, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 1.0, 340: 0.75, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.5, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 08:52:29,207 [INFO] [72] TRAIN  loss: 0.9429224375601057 acc: 0.9995085391325715
2025-01-19 08:52:29,207 [INFO] [72] TRAIN  loss dict: {'classification_loss': 0.9429224375601057}
2025-01-19 08:52:29,207 [INFO] [72] VALIDATION loss: 1.6972973859847937 VALIDATION acc: 0.8175548589341692
2025-01-19 08:52:29,207 [INFO] [72] VALIDATION loss dict: {'classification_loss': 1.6972973859847937}
2025-01-19 08:52:29,207 [INFO] 
2025-01-19 08:52:49,441 [INFO] Step[50/2713]: training loss : 0.939295243024826 TRAIN  loss dict:  {'classification_loss': 0.939295243024826}
2025-01-19 08:53:04,590 [INFO] Step[100/2713]: training loss : 0.9430406033992768 TRAIN  loss dict:  {'classification_loss': 0.9430406033992768}
2025-01-19 08:53:19,796 [INFO] Step[150/2713]: training loss : 0.9430914974212646 TRAIN  loss dict:  {'classification_loss': 0.9430914974212646}
2025-01-19 08:53:34,946 [INFO] Step[200/2713]: training loss : 0.9410224068164825 TRAIN  loss dict:  {'classification_loss': 0.9410224068164825}
2025-01-19 08:53:50,136 [INFO] Step[250/2713]: training loss : 0.9393040370941163 TRAIN  loss dict:  {'classification_loss': 0.9393040370941163}
2025-01-19 08:54:05,326 [INFO] Step[300/2713]: training loss : 0.9515240359306335 TRAIN  loss dict:  {'classification_loss': 0.9515240359306335}
2025-01-19 08:54:20,486 [INFO] Step[350/2713]: training loss : 0.9444896030426025 TRAIN  loss dict:  {'classification_loss': 0.9444896030426025}
2025-01-19 08:54:35,682 [INFO] Step[400/2713]: training loss : 0.9445935714244843 TRAIN  loss dict:  {'classification_loss': 0.9445935714244843}
2025-01-19 08:54:50,873 [INFO] Step[450/2713]: training loss : 0.9456382012367248 TRAIN  loss dict:  {'classification_loss': 0.9456382012367248}
2025-01-19 08:55:06,029 [INFO] Step[500/2713]: training loss : 0.9473550283908844 TRAIN  loss dict:  {'classification_loss': 0.9473550283908844}
2025-01-19 08:55:21,226 [INFO] Step[550/2713]: training loss : 0.9407255101203919 TRAIN  loss dict:  {'classification_loss': 0.9407255101203919}
2025-01-19 08:55:36,377 [INFO] Step[600/2713]: training loss : 0.9458651041984558 TRAIN  loss dict:  {'classification_loss': 0.9458651041984558}
2025-01-19 08:55:51,549 [INFO] Step[650/2713]: training loss : 0.942476612329483 TRAIN  loss dict:  {'classification_loss': 0.942476612329483}
2025-01-19 08:56:06,694 [INFO] Step[700/2713]: training loss : 0.9401219439506531 TRAIN  loss dict:  {'classification_loss': 0.9401219439506531}
2025-01-19 08:56:21,859 [INFO] Step[750/2713]: training loss : 0.9415694761276245 TRAIN  loss dict:  {'classification_loss': 0.9415694761276245}
2025-01-19 08:56:37,006 [INFO] Step[800/2713]: training loss : 0.9418474221229554 TRAIN  loss dict:  {'classification_loss': 0.9418474221229554}
2025-01-19 08:56:52,196 [INFO] Step[850/2713]: training loss : 0.9413490760326385 TRAIN  loss dict:  {'classification_loss': 0.9413490760326385}
2025-01-19 08:57:07,385 [INFO] Step[900/2713]: training loss : 0.9412477517127991 TRAIN  loss dict:  {'classification_loss': 0.9412477517127991}
2025-01-19 08:57:22,548 [INFO] Step[950/2713]: training loss : 0.9698684489727021 TRAIN  loss dict:  {'classification_loss': 0.9698684489727021}
2025-01-19 08:57:37,708 [INFO] Step[1000/2713]: training loss : 0.9402943456172943 TRAIN  loss dict:  {'classification_loss': 0.9402943456172943}
2025-01-19 08:57:52,863 [INFO] Step[1050/2713]: training loss : 0.9407791876792908 TRAIN  loss dict:  {'classification_loss': 0.9407791876792908}
2025-01-19 08:58:08,016 [INFO] Step[1100/2713]: training loss : 0.9430458998680115 TRAIN  loss dict:  {'classification_loss': 0.9430458998680115}
2025-01-19 08:58:23,214 [INFO] Step[1150/2713]: training loss : 0.9424786746501923 TRAIN  loss dict:  {'classification_loss': 0.9424786746501923}
2025-01-19 08:58:38,384 [INFO] Step[1200/2713]: training loss : 0.9416936039924622 TRAIN  loss dict:  {'classification_loss': 0.9416936039924622}
2025-01-19 08:58:53,568 [INFO] Step[1250/2713]: training loss : 0.9403002452850342 TRAIN  loss dict:  {'classification_loss': 0.9403002452850342}
2025-01-19 08:59:08,718 [INFO] Step[1300/2713]: training loss : 0.9438424205780029 TRAIN  loss dict:  {'classification_loss': 0.9438424205780029}
2025-01-19 08:59:23,850 [INFO] Step[1350/2713]: training loss : 0.9415061104297638 TRAIN  loss dict:  {'classification_loss': 0.9415061104297638}
2025-01-19 08:59:39,067 [INFO] Step[1400/2713]: training loss : 0.9404183876514435 TRAIN  loss dict:  {'classification_loss': 0.9404183876514435}
2025-01-19 08:59:54,269 [INFO] Step[1450/2713]: training loss : 0.9395528483390808 TRAIN  loss dict:  {'classification_loss': 0.9395528483390808}
2025-01-19 09:00:09,478 [INFO] Step[1500/2713]: training loss : 0.9402146220207215 TRAIN  loss dict:  {'classification_loss': 0.9402146220207215}
2025-01-19 09:00:24,651 [INFO] Step[1550/2713]: training loss : 0.9446360921859741 TRAIN  loss dict:  {'classification_loss': 0.9446360921859741}
2025-01-19 09:00:39,808 [INFO] Step[1600/2713]: training loss : 0.9439464354515076 TRAIN  loss dict:  {'classification_loss': 0.9439464354515076}
2025-01-19 09:00:54,987 [INFO] Step[1650/2713]: training loss : 0.9415881884098053 TRAIN  loss dict:  {'classification_loss': 0.9415881884098053}
2025-01-19 09:01:10,174 [INFO] Step[1700/2713]: training loss : 0.9440018630027771 TRAIN  loss dict:  {'classification_loss': 0.9440018630027771}
2025-01-19 09:01:25,359 [INFO] Step[1750/2713]: training loss : 0.9414183402061462 TRAIN  loss dict:  {'classification_loss': 0.9414183402061462}
2025-01-19 09:01:40,535 [INFO] Step[1800/2713]: training loss : 0.9383344912528991 TRAIN  loss dict:  {'classification_loss': 0.9383344912528991}
2025-01-19 09:01:55,722 [INFO] Step[1850/2713]: training loss : 0.943044946193695 TRAIN  loss dict:  {'classification_loss': 0.943044946193695}
2025-01-19 09:02:10,847 [INFO] Step[1900/2713]: training loss : 0.9400745785236359 TRAIN  loss dict:  {'classification_loss': 0.9400745785236359}
2025-01-19 09:02:26,064 [INFO] Step[1950/2713]: training loss : 0.9429108846187592 TRAIN  loss dict:  {'classification_loss': 0.9429108846187592}
2025-01-19 09:02:41,226 [INFO] Step[2000/2713]: training loss : 0.9419086444377899 TRAIN  loss dict:  {'classification_loss': 0.9419086444377899}
2025-01-19 09:02:56,419 [INFO] Step[2050/2713]: training loss : 0.9472986817359924 TRAIN  loss dict:  {'classification_loss': 0.9472986817359924}
2025-01-19 09:03:11,542 [INFO] Step[2100/2713]: training loss : 0.9393252730369568 TRAIN  loss dict:  {'classification_loss': 0.9393252730369568}
2025-01-19 09:03:26,750 [INFO] Step[2150/2713]: training loss : 0.9414425909519195 TRAIN  loss dict:  {'classification_loss': 0.9414425909519195}
2025-01-19 09:03:41,909 [INFO] Step[2200/2713]: training loss : 0.9432656276226044 TRAIN  loss dict:  {'classification_loss': 0.9432656276226044}
2025-01-19 09:03:57,074 [INFO] Step[2250/2713]: training loss : 0.9428037965297699 TRAIN  loss dict:  {'classification_loss': 0.9428037965297699}
2025-01-19 09:04:12,241 [INFO] Step[2300/2713]: training loss : 0.9395775139331818 TRAIN  loss dict:  {'classification_loss': 0.9395775139331818}
2025-01-19 09:04:27,414 [INFO] Step[2350/2713]: training loss : 0.9387647461891174 TRAIN  loss dict:  {'classification_loss': 0.9387647461891174}
2025-01-19 09:04:42,569 [INFO] Step[2400/2713]: training loss : 0.9382262802124024 TRAIN  loss dict:  {'classification_loss': 0.9382262802124024}
2025-01-19 09:04:57,747 [INFO] Step[2450/2713]: training loss : 0.9434326040744782 TRAIN  loss dict:  {'classification_loss': 0.9434326040744782}
2025-01-19 09:05:12,952 [INFO] Step[2500/2713]: training loss : 0.943628054857254 TRAIN  loss dict:  {'classification_loss': 0.943628054857254}
2025-01-19 09:05:28,123 [INFO] Step[2550/2713]: training loss : 0.9420705699920654 TRAIN  loss dict:  {'classification_loss': 0.9420705699920654}
2025-01-19 09:05:43,314 [INFO] Step[2600/2713]: training loss : 0.9463192665576935 TRAIN  loss dict:  {'classification_loss': 0.9463192665576935}
2025-01-19 09:05:58,496 [INFO] Step[2650/2713]: training loss : 0.9390765190124511 TRAIN  loss dict:  {'classification_loss': 0.9390765190124511}
2025-01-19 09:06:13,672 [INFO] Step[2700/2713]: training loss : 0.9440184259414672 TRAIN  loss dict:  {'classification_loss': 0.9440184259414672}
2025-01-19 09:07:34,338 [INFO] Label accuracies statistics:
2025-01-19 09:07:34,338 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 1.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 09:07:34,340 [INFO] [73] TRAIN  loss: 0.9427418948933509 acc: 0.9997542695662858
2025-01-19 09:07:34,340 [INFO] [73] TRAIN  loss dict: {'classification_loss': 0.9427418948933509}
2025-01-19 09:07:34,340 [INFO] [73] VALIDATION loss: 1.7315534994118196 VALIDATION acc: 0.8056426332288401
2025-01-19 09:07:34,340 [INFO] [73] VALIDATION loss dict: {'classification_loss': 1.7315534994118196}
2025-01-19 09:07:34,340 [INFO] 
2025-01-19 09:07:54,512 [INFO] Step[50/2713]: training loss : 0.9426520204544068 TRAIN  loss dict:  {'classification_loss': 0.9426520204544068}
2025-01-19 09:08:09,626 [INFO] Step[100/2713]: training loss : 0.9402527141571045 TRAIN  loss dict:  {'classification_loss': 0.9402527141571045}
2025-01-19 09:08:24,696 [INFO] Step[150/2713]: training loss : 0.9448638033866882 TRAIN  loss dict:  {'classification_loss': 0.9448638033866882}
2025-01-19 09:08:39,759 [INFO] Step[200/2713]: training loss : 0.9457880938053131 TRAIN  loss dict:  {'classification_loss': 0.9457880938053131}
2025-01-19 09:08:54,819 [INFO] Step[250/2713]: training loss : 0.9387579488754273 TRAIN  loss dict:  {'classification_loss': 0.9387579488754273}
2025-01-19 09:09:09,935 [INFO] Step[300/2713]: training loss : 0.9441712379455567 TRAIN  loss dict:  {'classification_loss': 0.9441712379455567}
2025-01-19 09:09:25,032 [INFO] Step[350/2713]: training loss : 0.9425976514816284 TRAIN  loss dict:  {'classification_loss': 0.9425976514816284}
2025-01-19 09:09:40,077 [INFO] Step[400/2713]: training loss : 0.9389892053604126 TRAIN  loss dict:  {'classification_loss': 0.9389892053604126}
2025-01-19 09:09:55,223 [INFO] Step[450/2713]: training loss : 0.9407144558429718 TRAIN  loss dict:  {'classification_loss': 0.9407144558429718}
2025-01-19 09:10:10,210 [INFO] Step[500/2713]: training loss : 0.9420054352283478 TRAIN  loss dict:  {'classification_loss': 0.9420054352283478}
2025-01-19 09:10:25,297 [INFO] Step[550/2713]: training loss : 0.9402041041851044 TRAIN  loss dict:  {'classification_loss': 0.9402041041851044}
2025-01-19 09:10:40,344 [INFO] Step[600/2713]: training loss : 0.9418795549869537 TRAIN  loss dict:  {'classification_loss': 0.9418795549869537}
2025-01-19 09:10:55,453 [INFO] Step[650/2713]: training loss : 0.93891348361969 TRAIN  loss dict:  {'classification_loss': 0.93891348361969}
2025-01-19 09:11:10,521 [INFO] Step[700/2713]: training loss : 0.9455629622936249 TRAIN  loss dict:  {'classification_loss': 0.9455629622936249}
2025-01-19 09:11:25,577 [INFO] Step[750/2713]: training loss : 0.9413645458221436 TRAIN  loss dict:  {'classification_loss': 0.9413645458221436}
2025-01-19 09:11:40,626 [INFO] Step[800/2713]: training loss : 0.9402270185947418 TRAIN  loss dict:  {'classification_loss': 0.9402270185947418}
2025-01-19 09:11:55,711 [INFO] Step[850/2713]: training loss : 0.9428239679336547 TRAIN  loss dict:  {'classification_loss': 0.9428239679336547}
2025-01-19 09:12:10,760 [INFO] Step[900/2713]: training loss : 0.9432293820381165 TRAIN  loss dict:  {'classification_loss': 0.9432293820381165}
2025-01-19 09:12:25,830 [INFO] Step[950/2713]: training loss : 0.9451183652877808 TRAIN  loss dict:  {'classification_loss': 0.9451183652877808}
2025-01-19 09:12:40,870 [INFO] Step[1000/2713]: training loss : 0.9433732759952546 TRAIN  loss dict:  {'classification_loss': 0.9433732759952546}
2025-01-19 09:12:55,937 [INFO] Step[1050/2713]: training loss : 0.9413005971908569 TRAIN  loss dict:  {'classification_loss': 0.9413005971908569}
2025-01-19 09:13:11,005 [INFO] Step[1100/2713]: training loss : 0.9437721514701843 TRAIN  loss dict:  {'classification_loss': 0.9437721514701843}
2025-01-19 09:13:26,077 [INFO] Step[1150/2713]: training loss : 0.9380278360843658 TRAIN  loss dict:  {'classification_loss': 0.9380278360843658}
2025-01-19 09:13:41,098 [INFO] Step[1200/2713]: training loss : 0.9398049712181091 TRAIN  loss dict:  {'classification_loss': 0.9398049712181091}
2025-01-19 09:13:56,143 [INFO] Step[1250/2713]: training loss : 0.9416170752048493 TRAIN  loss dict:  {'classification_loss': 0.9416170752048493}
2025-01-19 09:14:11,152 [INFO] Step[1300/2713]: training loss : 0.939162220954895 TRAIN  loss dict:  {'classification_loss': 0.939162220954895}
2025-01-19 09:14:26,209 [INFO] Step[1350/2713]: training loss : 0.9402821743488312 TRAIN  loss dict:  {'classification_loss': 0.9402821743488312}
2025-01-19 09:14:41,274 [INFO] Step[1400/2713]: training loss : 0.9407476127147675 TRAIN  loss dict:  {'classification_loss': 0.9407476127147675}
2025-01-19 09:14:56,323 [INFO] Step[1450/2713]: training loss : 0.9413857793807984 TRAIN  loss dict:  {'classification_loss': 0.9413857793807984}
2025-01-19 09:15:11,351 [INFO] Step[1500/2713]: training loss : 0.9486316990852356 TRAIN  loss dict:  {'classification_loss': 0.9486316990852356}
2025-01-19 09:15:26,440 [INFO] Step[1550/2713]: training loss : 0.9419036996364594 TRAIN  loss dict:  {'classification_loss': 0.9419036996364594}
2025-01-19 09:15:41,503 [INFO] Step[1600/2713]: training loss : 0.940783931016922 TRAIN  loss dict:  {'classification_loss': 0.940783931016922}
2025-01-19 09:15:56,587 [INFO] Step[1650/2713]: training loss : 0.9389594435691834 TRAIN  loss dict:  {'classification_loss': 0.9389594435691834}
2025-01-19 09:16:11,635 [INFO] Step[1700/2713]: training loss : 0.939979898929596 TRAIN  loss dict:  {'classification_loss': 0.939979898929596}
2025-01-19 09:16:26,730 [INFO] Step[1750/2713]: training loss : 0.9397919523715973 TRAIN  loss dict:  {'classification_loss': 0.9397919523715973}
2025-01-19 09:16:41,812 [INFO] Step[1800/2713]: training loss : 0.938880330324173 TRAIN  loss dict:  {'classification_loss': 0.938880330324173}
2025-01-19 09:16:56,881 [INFO] Step[1850/2713]: training loss : 0.9407947993278504 TRAIN  loss dict:  {'classification_loss': 0.9407947993278504}
2025-01-19 09:17:11,976 [INFO] Step[1900/2713]: training loss : 0.9426998662948608 TRAIN  loss dict:  {'classification_loss': 0.9426998662948608}
2025-01-19 09:17:27,053 [INFO] Step[1950/2713]: training loss : 0.9399851584434509 TRAIN  loss dict:  {'classification_loss': 0.9399851584434509}
2025-01-19 09:17:42,148 [INFO] Step[2000/2713]: training loss : 0.9386512386798859 TRAIN  loss dict:  {'classification_loss': 0.9386512386798859}
2025-01-19 09:17:57,220 [INFO] Step[2050/2713]: training loss : 0.9424750185012818 TRAIN  loss dict:  {'classification_loss': 0.9424750185012818}
2025-01-19 09:18:12,304 [INFO] Step[2100/2713]: training loss : 0.9401665306091309 TRAIN  loss dict:  {'classification_loss': 0.9401665306091309}
2025-01-19 09:18:27,369 [INFO] Step[2150/2713]: training loss : 0.9384406077861785 TRAIN  loss dict:  {'classification_loss': 0.9384406077861785}
2025-01-19 09:18:42,431 [INFO] Step[2200/2713]: training loss : 0.9391108644008637 TRAIN  loss dict:  {'classification_loss': 0.9391108644008637}
2025-01-19 09:18:57,483 [INFO] Step[2250/2713]: training loss : 0.9551346266269684 TRAIN  loss dict:  {'classification_loss': 0.9551346266269684}
2025-01-19 09:19:12,576 [INFO] Step[2300/2713]: training loss : 0.9400089871883393 TRAIN  loss dict:  {'classification_loss': 0.9400089871883393}
2025-01-19 09:19:27,658 [INFO] Step[2350/2713]: training loss : 0.9595780348777772 TRAIN  loss dict:  {'classification_loss': 0.9595780348777772}
2025-01-19 09:19:42,729 [INFO] Step[2400/2713]: training loss : 0.9393689906597138 TRAIN  loss dict:  {'classification_loss': 0.9393689906597138}
2025-01-19 09:19:57,738 [INFO] Step[2450/2713]: training loss : 0.9386045968532563 TRAIN  loss dict:  {'classification_loss': 0.9386045968532563}
2025-01-19 09:20:12,792 [INFO] Step[2500/2713]: training loss : 0.9425828135013581 TRAIN  loss dict:  {'classification_loss': 0.9425828135013581}
2025-01-19 09:20:27,855 [INFO] Step[2550/2713]: training loss : 0.9391557824611664 TRAIN  loss dict:  {'classification_loss': 0.9391557824611664}
2025-01-19 09:20:42,943 [INFO] Step[2600/2713]: training loss : 0.9481290125846863 TRAIN  loss dict:  {'classification_loss': 0.9481290125846863}
2025-01-19 09:20:57,977 [INFO] Step[2650/2713]: training loss : 0.9459075820446015 TRAIN  loss dict:  {'classification_loss': 0.9459075820446015}
2025-01-19 09:21:13,023 [INFO] Step[2700/2713]: training loss : 0.9399226069450378 TRAIN  loss dict:  {'classification_loss': 0.9399226069450378}
2025-01-19 09:22:32,952 [INFO] Label accuracies statistics:
2025-01-19 09:22:32,952 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.75, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 1.0, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 1.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 09:22:32,954 [INFO] [74] TRAIN  loss: 0.9420227257914531 acc: 0.9995085391325715
2025-01-19 09:22:32,954 [INFO] [74] TRAIN  loss dict: {'classification_loss': 0.9420227257914531}
2025-01-19 09:22:32,954 [INFO] [74] VALIDATION loss: 1.7066474018016256 VALIDATION acc: 0.8181818181818182
2025-01-19 09:22:32,954 [INFO] [74] VALIDATION loss dict: {'classification_loss': 1.7066474018016256}
2025-01-19 09:22:32,954 [INFO] 
2025-01-19 09:22:53,839 [INFO] Step[50/2713]: training loss : 0.942922658920288 TRAIN  loss dict:  {'classification_loss': 0.942922658920288}
2025-01-19 09:23:08,930 [INFO] Step[100/2713]: training loss : 0.94328941822052 TRAIN  loss dict:  {'classification_loss': 0.94328941822052}
2025-01-19 09:23:24,034 [INFO] Step[150/2713]: training loss : 0.9402649068832397 TRAIN  loss dict:  {'classification_loss': 0.9402649068832397}
2025-01-19 09:23:39,111 [INFO] Step[200/2713]: training loss : 0.9501076245307922 TRAIN  loss dict:  {'classification_loss': 0.9501076245307922}
2025-01-19 09:23:54,210 [INFO] Step[250/2713]: training loss : 0.9413528156280517 TRAIN  loss dict:  {'classification_loss': 0.9413528156280517}
2025-01-19 09:24:09,315 [INFO] Step[300/2713]: training loss : 0.9415885698795319 TRAIN  loss dict:  {'classification_loss': 0.9415885698795319}
2025-01-19 09:24:24,446 [INFO] Step[350/2713]: training loss : 0.9402244639396667 TRAIN  loss dict:  {'classification_loss': 0.9402244639396667}
2025-01-19 09:24:39,518 [INFO] Step[400/2713]: training loss : 0.9406199312210083 TRAIN  loss dict:  {'classification_loss': 0.9406199312210083}
2025-01-19 09:24:54,669 [INFO] Step[450/2713]: training loss : 0.9410395610332489 TRAIN  loss dict:  {'classification_loss': 0.9410395610332489}
2025-01-19 09:25:09,728 [INFO] Step[500/2713]: training loss : 0.9439535057544708 TRAIN  loss dict:  {'classification_loss': 0.9439535057544708}
2025-01-19 09:25:24,802 [INFO] Step[550/2713]: training loss : 0.9399507462978363 TRAIN  loss dict:  {'classification_loss': 0.9399507462978363}
2025-01-19 09:25:39,882 [INFO] Step[600/2713]: training loss : 0.9516814005374908 TRAIN  loss dict:  {'classification_loss': 0.9516814005374908}
2025-01-19 09:25:54,970 [INFO] Step[650/2713]: training loss : 0.9401063561439514 TRAIN  loss dict:  {'classification_loss': 0.9401063561439514}
2025-01-19 09:26:10,025 [INFO] Step[700/2713]: training loss : 0.9417687475681304 TRAIN  loss dict:  {'classification_loss': 0.9417687475681304}
2025-01-19 09:26:25,128 [INFO] Step[750/2713]: training loss : 0.9403974795341492 TRAIN  loss dict:  {'classification_loss': 0.9403974795341492}
2025-01-19 09:26:40,184 [INFO] Step[800/2713]: training loss : 0.9415955054759979 TRAIN  loss dict:  {'classification_loss': 0.9415955054759979}
2025-01-19 09:26:55,299 [INFO] Step[850/2713]: training loss : 0.9419903814792633 TRAIN  loss dict:  {'classification_loss': 0.9419903814792633}
2025-01-19 09:27:10,238 [INFO] Step[900/2713]: training loss : 0.9409868371486664 TRAIN  loss dict:  {'classification_loss': 0.9409868371486664}
2025-01-19 09:27:25,103 [INFO] Step[950/2713]: training loss : 0.9412295353412629 TRAIN  loss dict:  {'classification_loss': 0.9412295353412629}
2025-01-19 09:27:39,924 [INFO] Step[1000/2713]: training loss : 0.9389158856868743 TRAIN  loss dict:  {'classification_loss': 0.9389158856868743}
2025-01-19 09:27:54,779 [INFO] Step[1050/2713]: training loss : 0.9393202900886536 TRAIN  loss dict:  {'classification_loss': 0.9393202900886536}
2025-01-19 09:28:09,600 [INFO] Step[1100/2713]: training loss : 0.9470047235488892 TRAIN  loss dict:  {'classification_loss': 0.9470047235488892}
2025-01-19 09:28:24,496 [INFO] Step[1150/2713]: training loss : 0.9399114418029785 TRAIN  loss dict:  {'classification_loss': 0.9399114418029785}
2025-01-19 09:28:39,324 [INFO] Step[1200/2713]: training loss : 0.9418121266365052 TRAIN  loss dict:  {'classification_loss': 0.9418121266365052}
2025-01-19 09:28:54,141 [INFO] Step[1250/2713]: training loss : 0.9426125371456147 TRAIN  loss dict:  {'classification_loss': 0.9426125371456147}
2025-01-19 09:29:08,917 [INFO] Step[1300/2713]: training loss : 0.9404946410655975 TRAIN  loss dict:  {'classification_loss': 0.9404946410655975}
2025-01-19 09:29:23,753 [INFO] Step[1350/2713]: training loss : 0.9391530275344848 TRAIN  loss dict:  {'classification_loss': 0.9391530275344848}
2025-01-19 09:29:38,555 [INFO] Step[1400/2713]: training loss : 0.9383674025535583 TRAIN  loss dict:  {'classification_loss': 0.9383674025535583}
2025-01-19 09:29:53,425 [INFO] Step[1450/2713]: training loss : 0.9395291221141815 TRAIN  loss dict:  {'classification_loss': 0.9395291221141815}
2025-01-19 09:30:08,256 [INFO] Step[1500/2713]: training loss : 0.9421801722049713 TRAIN  loss dict:  {'classification_loss': 0.9421801722049713}
2025-01-19 09:30:23,121 [INFO] Step[1550/2713]: training loss : 0.9417605781555176 TRAIN  loss dict:  {'classification_loss': 0.9417605781555176}
2025-01-19 09:30:37,984 [INFO] Step[1600/2713]: training loss : 0.9442698681354522 TRAIN  loss dict:  {'classification_loss': 0.9442698681354522}
2025-01-19 09:30:52,827 [INFO] Step[1650/2713]: training loss : 0.951681661605835 TRAIN  loss dict:  {'classification_loss': 0.951681661605835}
2025-01-19 09:31:07,638 [INFO] Step[1700/2713]: training loss : 0.9394762122631073 TRAIN  loss dict:  {'classification_loss': 0.9394762122631073}
2025-01-19 09:31:22,516 [INFO] Step[1750/2713]: training loss : 0.9389546620845795 TRAIN  loss dict:  {'classification_loss': 0.9389546620845795}
2025-01-19 09:31:37,344 [INFO] Step[1800/2713]: training loss : 0.9408884906768799 TRAIN  loss dict:  {'classification_loss': 0.9408884906768799}
2025-01-19 09:31:52,213 [INFO] Step[1850/2713]: training loss : 0.9431997811794282 TRAIN  loss dict:  {'classification_loss': 0.9431997811794282}
2025-01-19 09:32:07,047 [INFO] Step[1900/2713]: training loss : 0.9714598608016968 TRAIN  loss dict:  {'classification_loss': 0.9714598608016968}
2025-01-19 09:32:21,922 [INFO] Step[1950/2713]: training loss : 0.9405370378494262 TRAIN  loss dict:  {'classification_loss': 0.9405370378494262}
2025-01-19 09:32:36,737 [INFO] Step[2000/2713]: training loss : 0.9414773035049439 TRAIN  loss dict:  {'classification_loss': 0.9414773035049439}
2025-01-19 09:32:51,562 [INFO] Step[2050/2713]: training loss : 0.9393989634513855 TRAIN  loss dict:  {'classification_loss': 0.9393989634513855}
2025-01-19 09:33:06,431 [INFO] Step[2100/2713]: training loss : 0.9370610976219177 TRAIN  loss dict:  {'classification_loss': 0.9370610976219177}
2025-01-19 09:33:21,270 [INFO] Step[2150/2713]: training loss : 0.941804871559143 TRAIN  loss dict:  {'classification_loss': 0.941804871559143}
2025-01-19 09:33:36,071 [INFO] Step[2200/2713]: training loss : 0.940176614522934 TRAIN  loss dict:  {'classification_loss': 0.940176614522934}
2025-01-19 09:33:50,950 [INFO] Step[2250/2713]: training loss : 0.9410249042510986 TRAIN  loss dict:  {'classification_loss': 0.9410249042510986}
2025-01-19 09:34:05,779 [INFO] Step[2300/2713]: training loss : 0.939130095243454 TRAIN  loss dict:  {'classification_loss': 0.939130095243454}
2025-01-19 09:34:20,636 [INFO] Step[2350/2713]: training loss : 0.9395591890811921 TRAIN  loss dict:  {'classification_loss': 0.9395591890811921}
2025-01-19 09:34:35,403 [INFO] Step[2400/2713]: training loss : 0.93994837641716 TRAIN  loss dict:  {'classification_loss': 0.93994837641716}
2025-01-19 09:34:50,227 [INFO] Step[2450/2713]: training loss : 0.9422301518917083 TRAIN  loss dict:  {'classification_loss': 0.9422301518917083}
2025-01-19 09:35:05,097 [INFO] Step[2500/2713]: training loss : 0.9435011196136475 TRAIN  loss dict:  {'classification_loss': 0.9435011196136475}
2025-01-19 09:35:19,910 [INFO] Step[2550/2713]: training loss : 0.9422271049022675 TRAIN  loss dict:  {'classification_loss': 0.9422271049022675}
2025-01-19 09:35:34,759 [INFO] Step[2600/2713]: training loss : 0.9378767454624176 TRAIN  loss dict:  {'classification_loss': 0.9378767454624176}
2025-01-19 09:35:49,579 [INFO] Step[2650/2713]: training loss : 0.9399330031871795 TRAIN  loss dict:  {'classification_loss': 0.9399330031871795}
2025-01-19 09:36:04,413 [INFO] Step[2700/2713]: training loss : 0.9398305332660675 TRAIN  loss dict:  {'classification_loss': 0.9398305332660675}
2025-01-19 09:37:24,709 [INFO] Label accuracies statistics:
2025-01-19 09:37:24,709 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.5, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 1.0, 205: 0.75, 206: 0.5, 207: 1.0, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 0.75, 389: 0.75, 390: 0.75, 391: 0.75, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 09:37:24,714 [INFO] [75] TRAIN  loss: 0.9420534199865112 acc: 0.9995085391325715
2025-01-19 09:37:24,714 [INFO] [75] TRAIN  loss dict: {'classification_loss': 0.9420534199865112}
2025-01-19 09:37:24,714 [INFO] [75] VALIDATION loss: 1.7345816237585885 VALIDATION acc: 0.8131661442006269
2025-01-19 09:37:24,714 [INFO] [75] VALIDATION loss dict: {'classification_loss': 1.7345816237585885}
2025-01-19 09:37:24,714 [INFO] 
2025-01-19 09:37:49,796 [INFO] Step[50/2713]: training loss : 0.9441127371788025 TRAIN  loss dict:  {'classification_loss': 0.9441127371788025}
2025-01-19 09:38:04,620 [INFO] Step[100/2713]: training loss : 0.9414877581596375 TRAIN  loss dict:  {'classification_loss': 0.9414877581596375}
2025-01-19 09:38:19,420 [INFO] Step[150/2713]: training loss : 0.944725558757782 TRAIN  loss dict:  {'classification_loss': 0.944725558757782}
2025-01-19 09:38:34,219 [INFO] Step[200/2713]: training loss : 0.9399220085144043 TRAIN  loss dict:  {'classification_loss': 0.9399220085144043}
2025-01-19 09:38:49,064 [INFO] Step[250/2713]: training loss : 0.9395588254928589 TRAIN  loss dict:  {'classification_loss': 0.9395588254928589}
2025-01-19 09:39:03,836 [INFO] Step[300/2713]: training loss : 0.9384867584705353 TRAIN  loss dict:  {'classification_loss': 0.9384867584705353}
2025-01-19 09:39:18,668 [INFO] Step[350/2713]: training loss : 0.9383509457111359 TRAIN  loss dict:  {'classification_loss': 0.9383509457111359}
2025-01-19 09:39:33,478 [INFO] Step[400/2713]: training loss : 0.9397070038318635 TRAIN  loss dict:  {'classification_loss': 0.9397070038318635}
2025-01-19 09:39:48,310 [INFO] Step[450/2713]: training loss : 0.9398815953731536 TRAIN  loss dict:  {'classification_loss': 0.9398815953731536}
2025-01-19 09:40:03,150 [INFO] Step[500/2713]: training loss : 0.942705899477005 TRAIN  loss dict:  {'classification_loss': 0.942705899477005}
2025-01-19 09:40:17,976 [INFO] Step[550/2713]: training loss : 0.9509975457191467 TRAIN  loss dict:  {'classification_loss': 0.9509975457191467}
2025-01-19 09:40:32,770 [INFO] Step[600/2713]: training loss : 0.9385860073566437 TRAIN  loss dict:  {'classification_loss': 0.9385860073566437}
2025-01-19 09:40:47,579 [INFO] Step[650/2713]: training loss : 0.938431762456894 TRAIN  loss dict:  {'classification_loss': 0.938431762456894}
2025-01-19 09:41:02,406 [INFO] Step[700/2713]: training loss : 0.9412985670566559 TRAIN  loss dict:  {'classification_loss': 0.9412985670566559}
2025-01-19 09:41:17,243 [INFO] Step[750/2713]: training loss : 0.9416490328311921 TRAIN  loss dict:  {'classification_loss': 0.9416490328311921}
2025-01-19 09:41:32,091 [INFO] Step[800/2713]: training loss : 0.9582703721523285 TRAIN  loss dict:  {'classification_loss': 0.9582703721523285}
2025-01-19 09:41:46,930 [INFO] Step[850/2713]: training loss : 0.9403908669948577 TRAIN  loss dict:  {'classification_loss': 0.9403908669948577}
2025-01-19 09:42:01,753 [INFO] Step[900/2713]: training loss : 0.9376387143135071 TRAIN  loss dict:  {'classification_loss': 0.9376387143135071}
2025-01-19 09:42:16,591 [INFO] Step[950/2713]: training loss : 0.9386676037311554 TRAIN  loss dict:  {'classification_loss': 0.9386676037311554}
2025-01-19 09:42:31,395 [INFO] Step[1000/2713]: training loss : 0.9366160833835602 TRAIN  loss dict:  {'classification_loss': 0.9366160833835602}
2025-01-19 09:42:46,227 [INFO] Step[1050/2713]: training loss : 0.9420432317256927 TRAIN  loss dict:  {'classification_loss': 0.9420432317256927}
2025-01-19 09:43:00,963 [INFO] Step[1100/2713]: training loss : 0.9400280237197876 TRAIN  loss dict:  {'classification_loss': 0.9400280237197876}
2025-01-19 09:43:15,854 [INFO] Step[1150/2713]: training loss : 0.9392637467384338 TRAIN  loss dict:  {'classification_loss': 0.9392637467384338}
2025-01-19 09:43:30,684 [INFO] Step[1200/2713]: training loss : 0.9390405917167663 TRAIN  loss dict:  {'classification_loss': 0.9390405917167663}
2025-01-19 09:43:45,518 [INFO] Step[1250/2713]: training loss : 0.9409003710746765 TRAIN  loss dict:  {'classification_loss': 0.9409003710746765}
2025-01-19 09:44:00,280 [INFO] Step[1300/2713]: training loss : 0.9397521126270294 TRAIN  loss dict:  {'classification_loss': 0.9397521126270294}
2025-01-19 09:44:15,117 [INFO] Step[1350/2713]: training loss : 0.9382381296157837 TRAIN  loss dict:  {'classification_loss': 0.9382381296157837}
2025-01-19 09:44:29,876 [INFO] Step[1400/2713]: training loss : 0.9393557643890381 TRAIN  loss dict:  {'classification_loss': 0.9393557643890381}
2025-01-19 09:44:44,725 [INFO] Step[1450/2713]: training loss : 0.9406156265735626 TRAIN  loss dict:  {'classification_loss': 0.9406156265735626}
2025-01-19 09:44:59,565 [INFO] Step[1500/2713]: training loss : 0.9440591514110566 TRAIN  loss dict:  {'classification_loss': 0.9440591514110566}
2025-01-19 09:45:14,356 [INFO] Step[1550/2713]: training loss : 0.9414345133304596 TRAIN  loss dict:  {'classification_loss': 0.9414345133304596}
2025-01-19 09:45:29,182 [INFO] Step[1600/2713]: training loss : 0.9409434044361115 TRAIN  loss dict:  {'classification_loss': 0.9409434044361115}
2025-01-19 09:45:44,043 [INFO] Step[1650/2713]: training loss : 0.9405315756797791 TRAIN  loss dict:  {'classification_loss': 0.9405315756797791}
2025-01-19 09:45:58,837 [INFO] Step[1700/2713]: training loss : 0.9411338031291961 TRAIN  loss dict:  {'classification_loss': 0.9411338031291961}
2025-01-19 09:46:13,675 [INFO] Step[1750/2713]: training loss : 0.9388625800609589 TRAIN  loss dict:  {'classification_loss': 0.9388625800609589}
2025-01-19 09:46:28,481 [INFO] Step[1800/2713]: training loss : 0.9413347542285919 TRAIN  loss dict:  {'classification_loss': 0.9413347542285919}
2025-01-19 09:46:43,277 [INFO] Step[1850/2713]: training loss : 0.9412108552455902 TRAIN  loss dict:  {'classification_loss': 0.9412108552455902}
2025-01-19 09:46:58,108 [INFO] Step[1900/2713]: training loss : 0.9460055994987487 TRAIN  loss dict:  {'classification_loss': 0.9460055994987487}
2025-01-19 09:47:12,942 [INFO] Step[1950/2713]: training loss : 0.9393045651912689 TRAIN  loss dict:  {'classification_loss': 0.9393045651912689}
2025-01-19 09:47:27,731 [INFO] Step[2000/2713]: training loss : 0.9443345975875854 TRAIN  loss dict:  {'classification_loss': 0.9443345975875854}
2025-01-19 09:47:42,530 [INFO] Step[2050/2713]: training loss : 0.9395675575733184 TRAIN  loss dict:  {'classification_loss': 0.9395675575733184}
2025-01-19 09:47:57,346 [INFO] Step[2100/2713]: training loss : 0.9398682546615601 TRAIN  loss dict:  {'classification_loss': 0.9398682546615601}
2025-01-19 09:48:12,127 [INFO] Step[2150/2713]: training loss : 0.9411122334003449 TRAIN  loss dict:  {'classification_loss': 0.9411122334003449}
2025-01-19 09:48:26,950 [INFO] Step[2200/2713]: training loss : 0.9405160295963287 TRAIN  loss dict:  {'classification_loss': 0.9405160295963287}
2025-01-19 09:48:41,758 [INFO] Step[2250/2713]: training loss : 0.9426557958126068 TRAIN  loss dict:  {'classification_loss': 0.9426557958126068}
2025-01-19 09:48:56,596 [INFO] Step[2300/2713]: training loss : 0.9474972224235535 TRAIN  loss dict:  {'classification_loss': 0.9474972224235535}
2025-01-19 09:49:11,427 [INFO] Step[2350/2713]: training loss : 0.9417831325531005 TRAIN  loss dict:  {'classification_loss': 0.9417831325531005}
2025-01-19 09:49:26,262 [INFO] Step[2400/2713]: training loss : 0.9440964770317077 TRAIN  loss dict:  {'classification_loss': 0.9440964770317077}
2025-01-19 09:49:41,134 [INFO] Step[2450/2713]: training loss : 0.938732043504715 TRAIN  loss dict:  {'classification_loss': 0.938732043504715}
2025-01-19 09:49:55,962 [INFO] Step[2500/2713]: training loss : 0.9464811146259308 TRAIN  loss dict:  {'classification_loss': 0.9464811146259308}
2025-01-19 09:50:10,776 [INFO] Step[2550/2713]: training loss : 0.9411927831172943 TRAIN  loss dict:  {'classification_loss': 0.9411927831172943}
2025-01-19 09:50:25,551 [INFO] Step[2600/2713]: training loss : 0.9484984290599823 TRAIN  loss dict:  {'classification_loss': 0.9484984290599823}
2025-01-19 09:50:40,388 [INFO] Step[2650/2713]: training loss : 0.9393147814273834 TRAIN  loss dict:  {'classification_loss': 0.9393147814273834}
2025-01-19 09:50:55,266 [INFO] Step[2700/2713]: training loss : 0.9406094217300415 TRAIN  loss dict:  {'classification_loss': 0.9406094217300415}
2025-01-19 09:52:14,839 [INFO] Label accuracies statistics:
2025-01-19 09:52:14,839 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.5, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 0.75, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 1.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 09:52:14,841 [INFO] [76] TRAIN  loss: 0.9415415818955772 acc: 0.9996314043494287
2025-01-19 09:52:14,841 [INFO] [76] TRAIN  loss dict: {'classification_loss': 0.9415415818955772}
2025-01-19 09:52:14,841 [INFO] [76] VALIDATION loss: 1.707233174963105 VALIDATION acc: 0.8188087774294671
2025-01-19 09:52:14,841 [INFO] [76] VALIDATION loss dict: {'classification_loss': 1.707233174963105}
2025-01-19 09:52:14,841 [INFO] 
2025-01-19 09:52:35,664 [INFO] Step[50/2713]: training loss : 0.9436982381343841 TRAIN  loss dict:  {'classification_loss': 0.9436982381343841}
2025-01-19 09:52:50,724 [INFO] Step[100/2713]: training loss : 0.9401453983783722 TRAIN  loss dict:  {'classification_loss': 0.9401453983783722}
2025-01-19 09:53:05,812 [INFO] Step[150/2713]: training loss : 0.9419196653366089 TRAIN  loss dict:  {'classification_loss': 0.9419196653366089}
2025-01-19 09:53:20,887 [INFO] Step[200/2713]: training loss : 0.9434868955612182 TRAIN  loss dict:  {'classification_loss': 0.9434868955612182}
2025-01-19 09:53:35,987 [INFO] Step[250/2713]: training loss : 0.9409495174884797 TRAIN  loss dict:  {'classification_loss': 0.9409495174884797}
2025-01-19 09:53:51,054 [INFO] Step[300/2713]: training loss : 0.9429469847679138 TRAIN  loss dict:  {'classification_loss': 0.9429469847679138}
2025-01-19 09:54:06,132 [INFO] Step[350/2713]: training loss : 0.9409182274341583 TRAIN  loss dict:  {'classification_loss': 0.9409182274341583}
2025-01-19 09:54:21,229 [INFO] Step[400/2713]: training loss : 0.9425279378890992 TRAIN  loss dict:  {'classification_loss': 0.9425279378890992}
2025-01-19 09:54:36,238 [INFO] Step[450/2713]: training loss : 0.9384637880325317 TRAIN  loss dict:  {'classification_loss': 0.9384637880325317}
2025-01-19 09:54:51,281 [INFO] Step[500/2713]: training loss : 0.9387677896022797 TRAIN  loss dict:  {'classification_loss': 0.9387677896022797}
2025-01-19 09:55:06,295 [INFO] Step[550/2713]: training loss : 0.9394308924674988 TRAIN  loss dict:  {'classification_loss': 0.9394308924674988}
2025-01-19 09:55:21,262 [INFO] Step[600/2713]: training loss : 0.9410950660705566 TRAIN  loss dict:  {'classification_loss': 0.9410950660705566}
2025-01-19 09:55:36,309 [INFO] Step[650/2713]: training loss : 0.9392777669429779 TRAIN  loss dict:  {'classification_loss': 0.9392777669429779}
2025-01-19 09:55:51,306 [INFO] Step[700/2713]: training loss : 0.9412483096122741 TRAIN  loss dict:  {'classification_loss': 0.9412483096122741}
2025-01-19 09:56:06,369 [INFO] Step[750/2713]: training loss : 0.9409642004966736 TRAIN  loss dict:  {'classification_loss': 0.9409642004966736}
2025-01-19 09:56:21,462 [INFO] Step[800/2713]: training loss : 0.9423306882381439 TRAIN  loss dict:  {'classification_loss': 0.9423306882381439}
2025-01-19 09:56:36,564 [INFO] Step[850/2713]: training loss : 0.9408950650691986 TRAIN  loss dict:  {'classification_loss': 0.9408950650691986}
2025-01-19 09:56:51,627 [INFO] Step[900/2713]: training loss : 0.939881044626236 TRAIN  loss dict:  {'classification_loss': 0.939881044626236}
2025-01-19 09:57:06,752 [INFO] Step[950/2713]: training loss : 0.9433950352668762 TRAIN  loss dict:  {'classification_loss': 0.9433950352668762}
2025-01-19 09:57:21,800 [INFO] Step[1000/2713]: training loss : 0.9413024175167084 TRAIN  loss dict:  {'classification_loss': 0.9413024175167084}
2025-01-19 09:57:36,864 [INFO] Step[1050/2713]: training loss : 0.9456042945384979 TRAIN  loss dict:  {'classification_loss': 0.9456042945384979}
2025-01-19 09:57:51,881 [INFO] Step[1100/2713]: training loss : 0.9405045795440674 TRAIN  loss dict:  {'classification_loss': 0.9405045795440674}
2025-01-19 09:58:06,924 [INFO] Step[1150/2713]: training loss : 0.9376875221729278 TRAIN  loss dict:  {'classification_loss': 0.9376875221729278}
2025-01-19 09:58:22,014 [INFO] Step[1200/2713]: training loss : 0.9516335451602935 TRAIN  loss dict:  {'classification_loss': 0.9516335451602935}
2025-01-19 09:58:37,110 [INFO] Step[1250/2713]: training loss : 0.9418797516822814 TRAIN  loss dict:  {'classification_loss': 0.9418797516822814}
2025-01-19 09:58:52,158 [INFO] Step[1300/2713]: training loss : 0.9417597472667694 TRAIN  loss dict:  {'classification_loss': 0.9417597472667694}
2025-01-19 09:59:07,178 [INFO] Step[1350/2713]: training loss : 0.9424531519412994 TRAIN  loss dict:  {'classification_loss': 0.9424531519412994}
2025-01-19 09:59:22,222 [INFO] Step[1400/2713]: training loss : 0.9403800904750824 TRAIN  loss dict:  {'classification_loss': 0.9403800904750824}
2025-01-19 09:59:37,196 [INFO] Step[1450/2713]: training loss : 0.9384500122070313 TRAIN  loss dict:  {'classification_loss': 0.9384500122070313}
2025-01-19 09:59:52,190 [INFO] Step[1500/2713]: training loss : 0.9396146512031556 TRAIN  loss dict:  {'classification_loss': 0.9396146512031556}
2025-01-19 10:00:07,225 [INFO] Step[1550/2713]: training loss : 0.9385611319541931 TRAIN  loss dict:  {'classification_loss': 0.9385611319541931}
2025-01-19 10:00:22,191 [INFO] Step[1600/2713]: training loss : 0.9441555523872376 TRAIN  loss dict:  {'classification_loss': 0.9441555523872376}
2025-01-19 10:00:37,118 [INFO] Step[1650/2713]: training loss : 0.9396857655048371 TRAIN  loss dict:  {'classification_loss': 0.9396857655048371}
2025-01-19 10:00:52,034 [INFO] Step[1700/2713]: training loss : 0.940822446346283 TRAIN  loss dict:  {'classification_loss': 0.940822446346283}
2025-01-19 10:01:06,993 [INFO] Step[1750/2713]: training loss : 0.9394239091873169 TRAIN  loss dict:  {'classification_loss': 0.9394239091873169}
2025-01-19 10:01:21,956 [INFO] Step[1800/2713]: training loss : 0.9425968837738037 TRAIN  loss dict:  {'classification_loss': 0.9425968837738037}
2025-01-19 10:01:36,923 [INFO] Step[1850/2713]: training loss : 0.9390754568576812 TRAIN  loss dict:  {'classification_loss': 0.9390754568576812}
2025-01-19 10:01:51,880 [INFO] Step[1900/2713]: training loss : 0.9435046768188476 TRAIN  loss dict:  {'classification_loss': 0.9435046768188476}
2025-01-19 10:02:06,808 [INFO] Step[1950/2713]: training loss : 0.941167358160019 TRAIN  loss dict:  {'classification_loss': 0.941167358160019}
2025-01-19 10:02:21,780 [INFO] Step[2000/2713]: training loss : 0.9432586598396301 TRAIN  loss dict:  {'classification_loss': 0.9432586598396301}
2025-01-19 10:02:36,826 [INFO] Step[2050/2713]: training loss : 0.9390438687801361 TRAIN  loss dict:  {'classification_loss': 0.9390438687801361}
2025-01-19 10:02:51,860 [INFO] Step[2100/2713]: training loss : 0.9415442728996277 TRAIN  loss dict:  {'classification_loss': 0.9415442728996277}
2025-01-19 10:03:06,814 [INFO] Step[2150/2713]: training loss : 0.9414176726341248 TRAIN  loss dict:  {'classification_loss': 0.9414176726341248}
2025-01-19 10:03:21,854 [INFO] Step[2200/2713]: training loss : 0.94002494931221 TRAIN  loss dict:  {'classification_loss': 0.94002494931221}
2025-01-19 10:03:36,910 [INFO] Step[2250/2713]: training loss : 0.9409184396266937 TRAIN  loss dict:  {'classification_loss': 0.9409184396266937}
2025-01-19 10:03:52,035 [INFO] Step[2300/2713]: training loss : 0.9405216348171234 TRAIN  loss dict:  {'classification_loss': 0.9405216348171234}
2025-01-19 10:04:07,158 [INFO] Step[2350/2713]: training loss : 0.942751657962799 TRAIN  loss dict:  {'classification_loss': 0.942751657962799}
2025-01-19 10:04:22,202 [INFO] Step[2400/2713]: training loss : 0.9528596067428589 TRAIN  loss dict:  {'classification_loss': 0.9528596067428589}
2025-01-19 10:04:37,300 [INFO] Step[2450/2713]: training loss : 0.9384427392482757 TRAIN  loss dict:  {'classification_loss': 0.9384427392482757}
2025-01-19 10:04:52,404 [INFO] Step[2500/2713]: training loss : 0.9513706254959107 TRAIN  loss dict:  {'classification_loss': 0.9513706254959107}
2025-01-19 10:05:07,515 [INFO] Step[2550/2713]: training loss : 0.9459933245182037 TRAIN  loss dict:  {'classification_loss': 0.9459933245182037}
2025-01-19 10:05:22,612 [INFO] Step[2600/2713]: training loss : 0.9394978177547455 TRAIN  loss dict:  {'classification_loss': 0.9394978177547455}
2025-01-19 10:05:37,733 [INFO] Step[2650/2713]: training loss : 0.9498683428764343 TRAIN  loss dict:  {'classification_loss': 0.9498683428764343}
2025-01-19 10:05:52,859 [INFO] Step[2700/2713]: training loss : 0.9390855073928833 TRAIN  loss dict:  {'classification_loss': 0.9390855073928833}
2025-01-19 10:07:12,954 [INFO] Label accuracies statistics:
2025-01-19 10:07:12,954 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 1.0, 61: 1.0, 62: 0.5, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 0.75, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 190: 1.0, 191: 0.75, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 0.75, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 10:07:12,956 [INFO] [77] TRAIN  loss: 0.9418274626668599 acc: 0.9997542695662858
2025-01-19 10:07:12,956 [INFO] [77] TRAIN  loss dict: {'classification_loss': 0.9418274626668599}
2025-01-19 10:07:12,956 [INFO] [77] VALIDATION loss: 1.7297162912169795 VALIDATION acc: 0.8163009404388715
2025-01-19 10:07:12,956 [INFO] [77] VALIDATION loss dict: {'classification_loss': 1.7297162912169795}
2025-01-19 10:07:12,956 [INFO] 
2025-01-19 10:07:32,682 [INFO] Step[50/2713]: training loss : 0.9638136100769042 TRAIN  loss dict:  {'classification_loss': 0.9638136100769042}
2025-01-19 10:07:47,809 [INFO] Step[100/2713]: training loss : 0.9438363361358643 TRAIN  loss dict:  {'classification_loss': 0.9438363361358643}
2025-01-19 10:08:02,921 [INFO] Step[150/2713]: training loss : 0.9411237227916718 TRAIN  loss dict:  {'classification_loss': 0.9411237227916718}
2025-01-19 10:08:18,052 [INFO] Step[200/2713]: training loss : 0.9394913232326507 TRAIN  loss dict:  {'classification_loss': 0.9394913232326507}
2025-01-19 10:08:33,220 [INFO] Step[250/2713]: training loss : 0.9376771914958953 TRAIN  loss dict:  {'classification_loss': 0.9376771914958953}
2025-01-19 10:08:48,364 [INFO] Step[300/2713]: training loss : 0.9414635372161865 TRAIN  loss dict:  {'classification_loss': 0.9414635372161865}
2025-01-19 10:09:03,511 [INFO] Step[350/2713]: training loss : 0.9399130380153656 TRAIN  loss dict:  {'classification_loss': 0.9399130380153656}
2025-01-19 10:09:18,641 [INFO] Step[400/2713]: training loss : 0.9414002501964569 TRAIN  loss dict:  {'classification_loss': 0.9414002501964569}
2025-01-19 10:09:33,772 [INFO] Step[450/2713]: training loss : 0.9383301484584808 TRAIN  loss dict:  {'classification_loss': 0.9383301484584808}
2025-01-19 10:09:48,987 [INFO] Step[500/2713]: training loss : 0.9378259587287903 TRAIN  loss dict:  {'classification_loss': 0.9378259587287903}
2025-01-19 10:10:04,226 [INFO] Step[550/2713]: training loss : 0.9609405946731567 TRAIN  loss dict:  {'classification_loss': 0.9609405946731567}
2025-01-19 10:10:19,447 [INFO] Step[600/2713]: training loss : 0.9395441150665284 TRAIN  loss dict:  {'classification_loss': 0.9395441150665284}
2025-01-19 10:10:34,633 [INFO] Step[650/2713]: training loss : 0.9381513452529907 TRAIN  loss dict:  {'classification_loss': 0.9381513452529907}
2025-01-19 10:10:49,697 [INFO] Step[700/2713]: training loss : 0.940225350856781 TRAIN  loss dict:  {'classification_loss': 0.940225350856781}
2025-01-19 10:11:05,018 [INFO] Step[750/2713]: training loss : 0.9379778814315796 TRAIN  loss dict:  {'classification_loss': 0.9379778814315796}
2025-01-19 10:11:20,349 [INFO] Step[800/2713]: training loss : 0.9501314294338227 TRAIN  loss dict:  {'classification_loss': 0.9501314294338227}
2025-01-19 10:11:35,520 [INFO] Step[850/2713]: training loss : 0.941289256811142 TRAIN  loss dict:  {'classification_loss': 0.941289256811142}
2025-01-19 10:11:50,761 [INFO] Step[900/2713]: training loss : 0.9399613881111145 TRAIN  loss dict:  {'classification_loss': 0.9399613881111145}
2025-01-19 10:12:06,297 [INFO] Step[950/2713]: training loss : 0.9486049747467041 TRAIN  loss dict:  {'classification_loss': 0.9486049747467041}
2025-01-19 10:12:21,535 [INFO] Step[1000/2713]: training loss : 0.9390581178665162 TRAIN  loss dict:  {'classification_loss': 0.9390581178665162}
2025-01-19 10:12:36,939 [INFO] Step[1050/2713]: training loss : 0.9408696699142456 TRAIN  loss dict:  {'classification_loss': 0.9408696699142456}
2025-01-19 10:12:52,215 [INFO] Step[1100/2713]: training loss : 0.9434405326843261 TRAIN  loss dict:  {'classification_loss': 0.9434405326843261}
2025-01-19 10:13:07,624 [INFO] Step[1150/2713]: training loss : 0.9370667779445648 TRAIN  loss dict:  {'classification_loss': 0.9370667779445648}
2025-01-19 10:13:22,949 [INFO] Step[1200/2713]: training loss : 0.9403852939605712 TRAIN  loss dict:  {'classification_loss': 0.9403852939605712}
2025-01-19 10:13:38,040 [INFO] Step[1250/2713]: training loss : 0.9375648248195648 TRAIN  loss dict:  {'classification_loss': 0.9375648248195648}
2025-01-19 10:13:53,260 [INFO] Step[1300/2713]: training loss : 0.9422028172016144 TRAIN  loss dict:  {'classification_loss': 0.9422028172016144}
2025-01-19 10:14:08,543 [INFO] Step[1350/2713]: training loss : 0.9389580810070037 TRAIN  loss dict:  {'classification_loss': 0.9389580810070037}
2025-01-19 10:14:23,268 [INFO] Step[1400/2713]: training loss : 0.9408727777004242 TRAIN  loss dict:  {'classification_loss': 0.9408727777004242}
2025-01-19 10:14:39,791 [INFO] Step[1450/2713]: training loss : 0.9489645349979401 TRAIN  loss dict:  {'classification_loss': 0.9489645349979401}
2025-01-19 10:15:05,492 [INFO] Step[1500/2713]: training loss : 0.9411792004108429 TRAIN  loss dict:  {'classification_loss': 0.9411792004108429}
2025-01-19 10:15:31,696 [INFO] Step[1550/2713]: training loss : 0.9414217400550843 TRAIN  loss dict:  {'classification_loss': 0.9414217400550843}
2025-01-19 10:15:54,554 [INFO] Step[1600/2713]: training loss : 0.9384651553630828 TRAIN  loss dict:  {'classification_loss': 0.9384651553630828}
2025-01-19 10:16:15,791 [INFO] Step[1650/2713]: training loss : 0.9385379433631897 TRAIN  loss dict:  {'classification_loss': 0.9385379433631897}
2025-01-19 10:16:34,877 [INFO] Step[1700/2713]: training loss : 0.9432482957839966 TRAIN  loss dict:  {'classification_loss': 0.9432482957839966}
2025-01-19 10:17:01,562 [INFO] Step[1750/2713]: training loss : 0.9396734440326691 TRAIN  loss dict:  {'classification_loss': 0.9396734440326691}
2025-01-19 10:17:28,575 [INFO] Step[1800/2713]: training loss : 0.9407947111129761 TRAIN  loss dict:  {'classification_loss': 0.9407947111129761}
2025-01-19 10:17:49,818 [INFO] Step[1850/2713]: training loss : 0.9420011270046235 TRAIN  loss dict:  {'classification_loss': 0.9420011270046235}
2025-01-19 10:18:08,379 [INFO] Step[1900/2713]: training loss : 0.9420234787464142 TRAIN  loss dict:  {'classification_loss': 0.9420234787464142}
2025-01-19 10:18:32,703 [INFO] Step[1950/2713]: training loss : 0.9391611576080322 TRAIN  loss dict:  {'classification_loss': 0.9391611576080322}
2025-01-19 10:18:58,274 [INFO] Step[2000/2713]: training loss : 0.9375312626361847 TRAIN  loss dict:  {'classification_loss': 0.9375312626361847}
2025-01-19 10:19:21,398 [INFO] Step[2050/2713]: training loss : 0.9458673918247222 TRAIN  loss dict:  {'classification_loss': 0.9458673918247222}
2025-01-19 10:19:42,098 [INFO] Step[2100/2713]: training loss : 0.9405937230587006 TRAIN  loss dict:  {'classification_loss': 0.9405937230587006}
2025-01-19 10:20:02,232 [INFO] Step[2150/2713]: training loss : 0.9409608483314514 TRAIN  loss dict:  {'classification_loss': 0.9409608483314514}
2025-01-19 10:20:27,843 [INFO] Step[2200/2713]: training loss : 0.9411902892589569 TRAIN  loss dict:  {'classification_loss': 0.9411902892589569}
2025-01-19 10:20:53,441 [INFO] Step[2250/2713]: training loss : 0.9406213879585266 TRAIN  loss dict:  {'classification_loss': 0.9406213879585266}
2025-01-19 10:21:13,621 [INFO] Step[2300/2713]: training loss : 0.9378999781608581 TRAIN  loss dict:  {'classification_loss': 0.9378999781608581}
2025-01-19 10:21:31,919 [INFO] Step[2350/2713]: training loss : 0.9425334191322327 TRAIN  loss dict:  {'classification_loss': 0.9425334191322327}
2025-01-19 10:21:55,928 [INFO] Step[2400/2713]: training loss : 0.9399560880661011 TRAIN  loss dict:  {'classification_loss': 0.9399560880661011}
2025-01-19 10:22:21,486 [INFO] Step[2450/2713]: training loss : 0.9406643462181091 TRAIN  loss dict:  {'classification_loss': 0.9406643462181091}
2025-01-19 10:22:44,638 [INFO] Step[2500/2713]: training loss : 0.9391943228244781 TRAIN  loss dict:  {'classification_loss': 0.9391943228244781}
2025-01-19 10:23:05,276 [INFO] Step[2550/2713]: training loss : 0.9399636614322663 TRAIN  loss dict:  {'classification_loss': 0.9399636614322663}
2025-01-19 10:23:25,682 [INFO] Step[2600/2713]: training loss : 0.9396077013015747 TRAIN  loss dict:  {'classification_loss': 0.9396077013015747}
2025-01-19 10:23:51,476 [INFO] Step[2650/2713]: training loss : 0.9394716191291809 TRAIN  loss dict:  {'classification_loss': 0.9394716191291809}
2025-01-19 10:24:17,394 [INFO] Step[2700/2713]: training loss : 0.9390318787097931 TRAIN  loss dict:  {'classification_loss': 0.9390318787097931}
2025-01-19 10:26:30,286 [INFO] Label accuracies statistics:
2025-01-19 10:26:30,286 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.5, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.5, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 10:26:30,291 [INFO] [78] TRAIN  loss: 0.9415085184763069 acc: 0.9996314043494287
2025-01-19 10:26:30,291 [INFO] [78] TRAIN  loss dict: {'classification_loss': 0.9415085184763069}
2025-01-19 10:26:30,291 [INFO] [78] VALIDATION loss: 1.76320953465494 VALIDATION acc: 0.8094043887147335
2025-01-19 10:26:30,292 [INFO] [78] VALIDATION loss dict: {'classification_loss': 1.76320953465494}
2025-01-19 10:26:30,292 [INFO] 
2025-01-19 10:26:57,279 [INFO] Step[50/2713]: training loss : 0.9586035537719727 TRAIN  loss dict:  {'classification_loss': 0.9586035537719727}
2025-01-19 10:27:22,666 [INFO] Step[100/2713]: training loss : 0.9552158081531524 TRAIN  loss dict:  {'classification_loss': 0.9552158081531524}
2025-01-19 10:27:49,000 [INFO] Step[150/2713]: training loss : 0.9380396735668183 TRAIN  loss dict:  {'classification_loss': 0.9380396735668183}
2025-01-19 10:28:08,791 [INFO] Step[200/2713]: training loss : 0.9485080540180206 TRAIN  loss dict:  {'classification_loss': 0.9485080540180206}
2025-01-19 10:28:27,908 [INFO] Step[250/2713]: training loss : 0.9397627890110016 TRAIN  loss dict:  {'classification_loss': 0.9397627890110016}
2025-01-19 10:28:51,078 [INFO] Step[300/2713]: training loss : 0.9388890135288238 TRAIN  loss dict:  {'classification_loss': 0.9388890135288238}
2025-01-19 10:29:16,162 [INFO] Step[350/2713]: training loss : 0.9431161499023437 TRAIN  loss dict:  {'classification_loss': 0.9431161499023437}
2025-01-19 10:29:40,489 [INFO] Step[400/2713]: training loss : 0.9433554077148437 TRAIN  loss dict:  {'classification_loss': 0.9433554077148437}
2025-01-19 10:30:02,687 [INFO] Step[450/2713]: training loss : 0.9387291026115417 TRAIN  loss dict:  {'classification_loss': 0.9387291026115417}
2025-01-19 10:30:19,930 [INFO] Step[500/2713]: training loss : 0.94729731798172 TRAIN  loss dict:  {'classification_loss': 0.94729731798172}
2025-01-19 10:30:44,888 [INFO] Step[550/2713]: training loss : 0.9410730767250061 TRAIN  loss dict:  {'classification_loss': 0.9410730767250061}
2025-01-19 10:31:11,036 [INFO] Step[600/2713]: training loss : 0.9409550440311432 TRAIN  loss dict:  {'classification_loss': 0.9409550440311432}
2025-01-19 10:31:31,087 [INFO] Step[650/2713]: training loss : 0.9399596309661865 TRAIN  loss dict:  {'classification_loss': 0.9399596309661865}
2025-01-19 10:31:50,559 [INFO] Step[700/2713]: training loss : 0.9402275776863098 TRAIN  loss dict:  {'classification_loss': 0.9402275776863098}
2025-01-19 10:32:05,518 [INFO] Step[750/2713]: training loss : 0.9405072498321533 TRAIN  loss dict:  {'classification_loss': 0.9405072498321533}
2025-01-19 10:32:20,564 [INFO] Step[800/2713]: training loss : 0.9408971536159515 TRAIN  loss dict:  {'classification_loss': 0.9408971536159515}
2025-01-19 10:32:35,519 [INFO] Step[850/2713]: training loss : 0.9396319150924682 TRAIN  loss dict:  {'classification_loss': 0.9396319150924682}
2025-01-19 10:32:50,475 [INFO] Step[900/2713]: training loss : 0.9399276292324066 TRAIN  loss dict:  {'classification_loss': 0.9399276292324066}
2025-01-19 10:33:05,421 [INFO] Step[950/2713]: training loss : 0.9461634266376495 TRAIN  loss dict:  {'classification_loss': 0.9461634266376495}
2025-01-19 10:33:20,410 [INFO] Step[1000/2713]: training loss : 0.9402214503288269 TRAIN  loss dict:  {'classification_loss': 0.9402214503288269}
2025-01-19 10:33:35,384 [INFO] Step[1050/2713]: training loss : 0.938246865272522 TRAIN  loss dict:  {'classification_loss': 0.938246865272522}
2025-01-19 10:33:50,350 [INFO] Step[1100/2713]: training loss : 0.9377111780643463 TRAIN  loss dict:  {'classification_loss': 0.9377111780643463}
2025-01-19 10:34:05,293 [INFO] Step[1150/2713]: training loss : 0.939803638458252 TRAIN  loss dict:  {'classification_loss': 0.939803638458252}
2025-01-19 10:34:20,247 [INFO] Step[1200/2713]: training loss : 0.9465252888202668 TRAIN  loss dict:  {'classification_loss': 0.9465252888202668}
2025-01-19 10:34:35,191 [INFO] Step[1250/2713]: training loss : 0.9416855525970459 TRAIN  loss dict:  {'classification_loss': 0.9416855525970459}
2025-01-19 10:34:50,129 [INFO] Step[1300/2713]: training loss : 0.9399744951725006 TRAIN  loss dict:  {'classification_loss': 0.9399744951725006}
2025-01-19 10:35:05,096 [INFO] Step[1350/2713]: training loss : 0.9387971389293671 TRAIN  loss dict:  {'classification_loss': 0.9387971389293671}
2025-01-19 10:35:19,978 [INFO] Step[1400/2713]: training loss : 0.9418891775608063 TRAIN  loss dict:  {'classification_loss': 0.9418891775608063}
2025-01-19 10:35:34,959 [INFO] Step[1450/2713]: training loss : 0.9413446664810181 TRAIN  loss dict:  {'classification_loss': 0.9413446664810181}
2025-01-19 10:35:49,937 [INFO] Step[1500/2713]: training loss : 0.9462715148925781 TRAIN  loss dict:  {'classification_loss': 0.9462715148925781}
2025-01-19 10:36:04,927 [INFO] Step[1550/2713]: training loss : 0.9407627284526825 TRAIN  loss dict:  {'classification_loss': 0.9407627284526825}
2025-01-19 10:36:19,852 [INFO] Step[1600/2713]: training loss : 0.9396476101875305 TRAIN  loss dict:  {'classification_loss': 0.9396476101875305}
2025-01-19 10:36:34,808 [INFO] Step[1650/2713]: training loss : 0.9406726861000061 TRAIN  loss dict:  {'classification_loss': 0.9406726861000061}
2025-01-19 10:36:49,782 [INFO] Step[1700/2713]: training loss : 0.9424729490280152 TRAIN  loss dict:  {'classification_loss': 0.9424729490280152}
2025-01-19 10:37:04,758 [INFO] Step[1750/2713]: training loss : 0.9402613413333892 TRAIN  loss dict:  {'classification_loss': 0.9402613413333892}
2025-01-19 10:37:19,702 [INFO] Step[1800/2713]: training loss : 0.9393206822872162 TRAIN  loss dict:  {'classification_loss': 0.9393206822872162}
2025-01-19 10:37:34,640 [INFO] Step[1850/2713]: training loss : 0.9449197840690613 TRAIN  loss dict:  {'classification_loss': 0.9449197840690613}
2025-01-19 10:37:49,512 [INFO] Step[1900/2713]: training loss : 0.9417003118991851 TRAIN  loss dict:  {'classification_loss': 0.9417003118991851}
2025-01-19 10:38:04,486 [INFO] Step[1950/2713]: training loss : 0.9395184338092804 TRAIN  loss dict:  {'classification_loss': 0.9395184338092804}
2025-01-19 10:38:19,475 [INFO] Step[2000/2713]: training loss : 0.9409049141407013 TRAIN  loss dict:  {'classification_loss': 0.9409049141407013}
2025-01-19 10:38:34,480 [INFO] Step[2050/2713]: training loss : 0.9380291652679443 TRAIN  loss dict:  {'classification_loss': 0.9380291652679443}
2025-01-19 10:38:49,444 [INFO] Step[2100/2713]: training loss : 0.939444146156311 TRAIN  loss dict:  {'classification_loss': 0.939444146156311}
2025-01-19 10:39:04,551 [INFO] Step[2150/2713]: training loss : 0.9416100895404815 TRAIN  loss dict:  {'classification_loss': 0.9416100895404815}
2025-01-19 10:39:19,539 [INFO] Step[2200/2713]: training loss : 0.9386218774318695 TRAIN  loss dict:  {'classification_loss': 0.9386218774318695}
2025-01-19 10:39:34,519 [INFO] Step[2250/2713]: training loss : 0.9377294492721557 TRAIN  loss dict:  {'classification_loss': 0.9377294492721557}
2025-01-19 10:39:49,478 [INFO] Step[2300/2713]: training loss : 0.9422537040710449 TRAIN  loss dict:  {'classification_loss': 0.9422537040710449}
2025-01-19 10:40:04,489 [INFO] Step[2350/2713]: training loss : 0.9404179894924164 TRAIN  loss dict:  {'classification_loss': 0.9404179894924164}
2025-01-19 10:40:19,422 [INFO] Step[2400/2713]: training loss : 0.9416358470916748 TRAIN  loss dict:  {'classification_loss': 0.9416358470916748}
2025-01-19 10:40:34,369 [INFO] Step[2450/2713]: training loss : 0.9415170323848724 TRAIN  loss dict:  {'classification_loss': 0.9415170323848724}
2025-01-19 10:40:49,349 [INFO] Step[2500/2713]: training loss : 0.9404916822910309 TRAIN  loss dict:  {'classification_loss': 0.9404916822910309}
2025-01-19 10:41:04,299 [INFO] Step[2550/2713]: training loss : 0.9382695889472962 TRAIN  loss dict:  {'classification_loss': 0.9382695889472962}
2025-01-19 10:41:19,274 [INFO] Step[2600/2713]: training loss : 0.9386044430732727 TRAIN  loss dict:  {'classification_loss': 0.9386044430732727}
2025-01-19 10:41:34,268 [INFO] Step[2650/2713]: training loss : 0.943661093711853 TRAIN  loss dict:  {'classification_loss': 0.943661093711853}
2025-01-19 10:41:49,292 [INFO] Step[2700/2713]: training loss : 0.93806361079216 TRAIN  loss dict:  {'classification_loss': 0.93806361079216}
2025-01-19 10:43:10,420 [INFO] Label accuracies statistics:
2025-01-19 10:43:10,420 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.25, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.25, 305: 0.75, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 10:43:10,427 [INFO] [79] TRAIN  loss: 0.9415388975994111 acc: 0.9997542695662858
2025-01-19 10:43:10,427 [INFO] [79] TRAIN  loss dict: {'classification_loss': 0.9415388975994111}
2025-01-19 10:43:10,427 [INFO] [79] VALIDATION loss: 1.7743900950465883 VALIDATION acc: 0.8031347962382445
2025-01-19 10:43:10,427 [INFO] [79] VALIDATION loss dict: {'classification_loss': 1.7743900950465883}
2025-01-19 10:43:10,427 [INFO] 
2025-01-19 10:43:30,773 [INFO] Step[50/2713]: training loss : 0.9455585622787476 TRAIN  loss dict:  {'classification_loss': 0.9455585622787476}
2025-01-19 10:43:45,796 [INFO] Step[100/2713]: training loss : 0.9430447936058044 TRAIN  loss dict:  {'classification_loss': 0.9430447936058044}
2025-01-19 10:44:00,881 [INFO] Step[150/2713]: training loss : 0.9375428664684295 TRAIN  loss dict:  {'classification_loss': 0.9375428664684295}
2025-01-19 10:44:15,897 [INFO] Step[200/2713]: training loss : 0.9372353601455689 TRAIN  loss dict:  {'classification_loss': 0.9372353601455689}
2025-01-19 10:44:31,063 [INFO] Step[250/2713]: training loss : 0.9403505373001099 TRAIN  loss dict:  {'classification_loss': 0.9403505373001099}
2025-01-19 10:44:46,210 [INFO] Step[300/2713]: training loss : 0.9428489005565643 TRAIN  loss dict:  {'classification_loss': 0.9428489005565643}
2025-01-19 10:45:01,296 [INFO] Step[350/2713]: training loss : 0.9451827359199524 TRAIN  loss dict:  {'classification_loss': 0.9451827359199524}
2025-01-19 10:45:16,353 [INFO] Step[400/2713]: training loss : 0.9379569005966186 TRAIN  loss dict:  {'classification_loss': 0.9379569005966186}
2025-01-19 10:45:31,402 [INFO] Step[450/2713]: training loss : 0.9398757433891296 TRAIN  loss dict:  {'classification_loss': 0.9398757433891296}
2025-01-19 10:45:46,412 [INFO] Step[500/2713]: training loss : 0.937516587972641 TRAIN  loss dict:  {'classification_loss': 0.937516587972641}
2025-01-19 10:46:01,505 [INFO] Step[550/2713]: training loss : 0.9416427886486054 TRAIN  loss dict:  {'classification_loss': 0.9416427886486054}
2025-01-19 10:46:16,505 [INFO] Step[600/2713]: training loss : 0.9449315655231476 TRAIN  loss dict:  {'classification_loss': 0.9449315655231476}
2025-01-19 10:46:31,508 [INFO] Step[650/2713]: training loss : 0.9406648814678192 TRAIN  loss dict:  {'classification_loss': 0.9406648814678192}
2025-01-19 10:46:46,622 [INFO] Step[700/2713]: training loss : 0.938459368944168 TRAIN  loss dict:  {'classification_loss': 0.938459368944168}
2025-01-19 10:47:01,609 [INFO] Step[750/2713]: training loss : 0.9383032393455505 TRAIN  loss dict:  {'classification_loss': 0.9383032393455505}
2025-01-19 10:47:16,652 [INFO] Step[800/2713]: training loss : 0.9400260949134827 TRAIN  loss dict:  {'classification_loss': 0.9400260949134827}
2025-01-19 10:47:31,603 [INFO] Step[850/2713]: training loss : 0.9422276604175568 TRAIN  loss dict:  {'classification_loss': 0.9422276604175568}
2025-01-19 10:47:46,591 [INFO] Step[900/2713]: training loss : 0.9408140325546265 TRAIN  loss dict:  {'classification_loss': 0.9408140325546265}
2025-01-19 10:48:01,565 [INFO] Step[950/2713]: training loss : 0.9379144096374512 TRAIN  loss dict:  {'classification_loss': 0.9379144096374512}
2025-01-19 10:48:16,569 [INFO] Step[1000/2713]: training loss : 0.9408624470233917 TRAIN  loss dict:  {'classification_loss': 0.9408624470233917}
2025-01-19 10:48:31,530 [INFO] Step[1050/2713]: training loss : 0.9393239796161652 TRAIN  loss dict:  {'classification_loss': 0.9393239796161652}
2025-01-19 10:48:46,579 [INFO] Step[1100/2713]: training loss : 0.9386956346035004 TRAIN  loss dict:  {'classification_loss': 0.9386956346035004}
2025-01-19 10:49:01,523 [INFO] Step[1150/2713]: training loss : 0.9416144239902496 TRAIN  loss dict:  {'classification_loss': 0.9416144239902496}
2025-01-19 10:49:16,473 [INFO] Step[1200/2713]: training loss : 0.9378004539012909 TRAIN  loss dict:  {'classification_loss': 0.9378004539012909}
2025-01-19 10:49:31,434 [INFO] Step[1250/2713]: training loss : 0.9379358148574829 TRAIN  loss dict:  {'classification_loss': 0.9379358148574829}
2025-01-19 10:49:46,413 [INFO] Step[1300/2713]: training loss : 0.9408733916282653 TRAIN  loss dict:  {'classification_loss': 0.9408733916282653}
2025-01-19 10:50:01,437 [INFO] Step[1350/2713]: training loss : 0.9382132399082184 TRAIN  loss dict:  {'classification_loss': 0.9382132399082184}
2025-01-19 10:50:16,730 [INFO] Step[1400/2713]: training loss : 0.9380566704273224 TRAIN  loss dict:  {'classification_loss': 0.9380566704273224}
2025-01-19 10:50:31,711 [INFO] Step[1450/2713]: training loss : 0.9380273377895355 TRAIN  loss dict:  {'classification_loss': 0.9380273377895355}
2025-01-19 10:50:46,718 [INFO] Step[1500/2713]: training loss : 0.9383665490150451 TRAIN  loss dict:  {'classification_loss': 0.9383665490150451}
2025-01-19 10:51:01,683 [INFO] Step[1550/2713]: training loss : 0.9393001866340637 TRAIN  loss dict:  {'classification_loss': 0.9393001866340637}
2025-01-19 10:51:16,687 [INFO] Step[1600/2713]: training loss : 0.9379706573486328 TRAIN  loss dict:  {'classification_loss': 0.9379706573486328}
2025-01-19 10:51:31,742 [INFO] Step[1650/2713]: training loss : 0.9362247228622437 TRAIN  loss dict:  {'classification_loss': 0.9362247228622437}
2025-01-19 10:51:46,702 [INFO] Step[1700/2713]: training loss : 0.9398439300060272 TRAIN  loss dict:  {'classification_loss': 0.9398439300060272}
2025-01-19 10:52:01,646 [INFO] Step[1750/2713]: training loss : 0.9469366812705994 TRAIN  loss dict:  {'classification_loss': 0.9469366812705994}
2025-01-19 10:52:16,650 [INFO] Step[1800/2713]: training loss : 0.941035293340683 TRAIN  loss dict:  {'classification_loss': 0.941035293340683}
2025-01-19 10:52:31,612 [INFO] Step[1850/2713]: training loss : 0.9391976451873779 TRAIN  loss dict:  {'classification_loss': 0.9391976451873779}
2025-01-19 10:52:46,568 [INFO] Step[1900/2713]: training loss : 0.9380892300605774 TRAIN  loss dict:  {'classification_loss': 0.9380892300605774}
2025-01-19 10:53:01,587 [INFO] Step[1950/2713]: training loss : 0.9393438923358918 TRAIN  loss dict:  {'classification_loss': 0.9393438923358918}
2025-01-19 10:53:16,595 [INFO] Step[2000/2713]: training loss : 0.9394731414318085 TRAIN  loss dict:  {'classification_loss': 0.9394731414318085}
2025-01-19 10:53:31,633 [INFO] Step[2050/2713]: training loss : 0.9423290622234345 TRAIN  loss dict:  {'classification_loss': 0.9423290622234345}
2025-01-19 10:53:46,612 [INFO] Step[2100/2713]: training loss : 0.9410376334190369 TRAIN  loss dict:  {'classification_loss': 0.9410376334190369}
2025-01-19 10:54:01,671 [INFO] Step[2150/2713]: training loss : 0.9403808546066285 TRAIN  loss dict:  {'classification_loss': 0.9403808546066285}
2025-01-19 10:54:16,727 [INFO] Step[2200/2713]: training loss : 0.9423071587085724 TRAIN  loss dict:  {'classification_loss': 0.9423071587085724}
2025-01-19 10:54:31,782 [INFO] Step[2250/2713]: training loss : 0.9398933553695679 TRAIN  loss dict:  {'classification_loss': 0.9398933553695679}
2025-01-19 10:54:46,811 [INFO] Step[2300/2713]: training loss : 0.9377536404132844 TRAIN  loss dict:  {'classification_loss': 0.9377536404132844}
2025-01-19 10:55:01,813 [INFO] Step[2350/2713]: training loss : 0.9394875705242157 TRAIN  loss dict:  {'classification_loss': 0.9394875705242157}
2025-01-19 10:55:16,852 [INFO] Step[2400/2713]: training loss : 0.9435683834552765 TRAIN  loss dict:  {'classification_loss': 0.9435683834552765}
2025-01-19 10:55:31,869 [INFO] Step[2450/2713]: training loss : 0.9379352021217346 TRAIN  loss dict:  {'classification_loss': 0.9379352021217346}
2025-01-19 10:55:46,898 [INFO] Step[2500/2713]: training loss : 0.9385954403877258 TRAIN  loss dict:  {'classification_loss': 0.9385954403877258}
2025-01-19 10:56:01,896 [INFO] Step[2550/2713]: training loss : 0.9378511726856231 TRAIN  loss dict:  {'classification_loss': 0.9378511726856231}
2025-01-19 10:56:16,913 [INFO] Step[2600/2713]: training loss : 0.9429706799983978 TRAIN  loss dict:  {'classification_loss': 0.9429706799983978}
2025-01-19 10:56:31,930 [INFO] Step[2650/2713]: training loss : 0.939668926000595 TRAIN  loss dict:  {'classification_loss': 0.939668926000595}
2025-01-19 10:56:46,971 [INFO] Step[2700/2713]: training loss : 0.9397859656810761 TRAIN  loss dict:  {'classification_loss': 0.9397859656810761}
2025-01-19 10:58:07,282 [INFO] Label accuracies statistics:
2025-01-19 10:58:07,282 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 1.0, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.25, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.25, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.5, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 10:58:07,284 [INFO] [80] TRAIN  loss: 0.9400410303935941 acc: 0.9997542695662858
2025-01-19 10:58:07,284 [INFO] [80] TRAIN  loss dict: {'classification_loss': 0.9400410303935941}
2025-01-19 10:58:07,284 [INFO] [80] VALIDATION loss: 1.7425256189995242 VALIDATION acc: 0.806269592476489
2025-01-19 10:58:07,284 [INFO] [80] VALIDATION loss dict: {'classification_loss': 1.7425256189995242}
2025-01-19 10:58:07,284 [INFO] 
2025-01-19 10:59:00,847 [INFO] Step[50/2713]: training loss : 0.9412743830680848 TRAIN  loss dict:  {'classification_loss': 0.9412743830680848}
2025-01-19 10:59:15,813 [INFO] Step[100/2713]: training loss : 0.9396443176269531 TRAIN  loss dict:  {'classification_loss': 0.9396443176269531}
2025-01-19 10:59:30,761 [INFO] Step[150/2713]: training loss : 0.9392029905319214 TRAIN  loss dict:  {'classification_loss': 0.9392029905319214}
2025-01-19 10:59:45,745 [INFO] Step[200/2713]: training loss : 0.9396527373790741 TRAIN  loss dict:  {'classification_loss': 0.9396527373790741}
2025-01-19 11:00:00,740 [INFO] Step[250/2713]: training loss : 0.9399634575843812 TRAIN  loss dict:  {'classification_loss': 0.9399634575843812}
2025-01-19 11:00:15,726 [INFO] Step[300/2713]: training loss : 0.9395597267150879 TRAIN  loss dict:  {'classification_loss': 0.9395597267150879}
2025-01-19 11:00:30,710 [INFO] Step[350/2713]: training loss : 0.9396204125881195 TRAIN  loss dict:  {'classification_loss': 0.9396204125881195}
2025-01-19 11:00:45,672 [INFO] Step[400/2713]: training loss : 0.9412092626094818 TRAIN  loss dict:  {'classification_loss': 0.9412092626094818}
2025-01-19 11:01:00,648 [INFO] Step[450/2713]: training loss : 0.9387901699543 TRAIN  loss dict:  {'classification_loss': 0.9387901699543}
2025-01-19 11:01:15,694 [INFO] Step[500/2713]: training loss : 0.9375155401229859 TRAIN  loss dict:  {'classification_loss': 0.9375155401229859}
2025-01-19 11:01:30,815 [INFO] Step[550/2713]: training loss : 0.9374631857872009 TRAIN  loss dict:  {'classification_loss': 0.9374631857872009}
2025-01-19 11:01:45,917 [INFO] Step[600/2713]: training loss : 0.939408837556839 TRAIN  loss dict:  {'classification_loss': 0.939408837556839}
2025-01-19 11:02:01,071 [INFO] Step[650/2713]: training loss : 0.9383267080783844 TRAIN  loss dict:  {'classification_loss': 0.9383267080783844}
2025-01-19 11:02:16,170 [INFO] Step[700/2713]: training loss : 0.942420060634613 TRAIN  loss dict:  {'classification_loss': 0.942420060634613}
2025-01-19 11:02:31,336 [INFO] Step[750/2713]: training loss : 0.9426488363742829 TRAIN  loss dict:  {'classification_loss': 0.9426488363742829}
2025-01-19 11:02:46,467 [INFO] Step[800/2713]: training loss : 0.9518672943115234 TRAIN  loss dict:  {'classification_loss': 0.9518672943115234}
2025-01-19 11:03:01,594 [INFO] Step[850/2713]: training loss : 0.9376607608795166 TRAIN  loss dict:  {'classification_loss': 0.9376607608795166}
2025-01-19 11:03:16,687 [INFO] Step[900/2713]: training loss : 0.937667818069458 TRAIN  loss dict:  {'classification_loss': 0.937667818069458}
2025-01-19 11:03:31,775 [INFO] Step[950/2713]: training loss : 0.9445466029644013 TRAIN  loss dict:  {'classification_loss': 0.9445466029644013}
2025-01-19 11:03:46,918 [INFO] Step[1000/2713]: training loss : 0.9375032424926758 TRAIN  loss dict:  {'classification_loss': 0.9375032424926758}
2025-01-19 11:04:01,977 [INFO] Step[1050/2713]: training loss : 0.9384393775463105 TRAIN  loss dict:  {'classification_loss': 0.9384393775463105}
2025-01-19 11:04:17,121 [INFO] Step[1100/2713]: training loss : 0.9390635550022125 TRAIN  loss dict:  {'classification_loss': 0.9390635550022125}
2025-01-19 11:04:32,217 [INFO] Step[1150/2713]: training loss : 0.9383367955684662 TRAIN  loss dict:  {'classification_loss': 0.9383367955684662}
2025-01-19 11:04:47,345 [INFO] Step[1200/2713]: training loss : 0.9393605589866638 TRAIN  loss dict:  {'classification_loss': 0.9393605589866638}
2025-01-19 11:05:02,470 [INFO] Step[1250/2713]: training loss : 0.9401804971694946 TRAIN  loss dict:  {'classification_loss': 0.9401804971694946}
2025-01-19 11:05:17,590 [INFO] Step[1300/2713]: training loss : 0.938128422498703 TRAIN  loss dict:  {'classification_loss': 0.938128422498703}
2025-01-19 11:05:32,688 [INFO] Step[1350/2713]: training loss : 0.9373933577537537 TRAIN  loss dict:  {'classification_loss': 0.9373933577537537}
2025-01-19 11:05:47,724 [INFO] Step[1400/2713]: training loss : 0.9385629606246948 TRAIN  loss dict:  {'classification_loss': 0.9385629606246948}
2025-01-19 11:06:02,814 [INFO] Step[1450/2713]: training loss : 0.9395926475524903 TRAIN  loss dict:  {'classification_loss': 0.9395926475524903}
2025-01-19 11:06:17,854 [INFO] Step[1500/2713]: training loss : 0.9373298251628875 TRAIN  loss dict:  {'classification_loss': 0.9373298251628875}
2025-01-19 11:06:32,934 [INFO] Step[1550/2713]: training loss : 0.9380121433734894 TRAIN  loss dict:  {'classification_loss': 0.9380121433734894}
2025-01-19 11:06:47,947 [INFO] Step[1600/2713]: training loss : 0.9378073310852051 TRAIN  loss dict:  {'classification_loss': 0.9378073310852051}
2025-01-19 11:07:02,997 [INFO] Step[1650/2713]: training loss : 0.9370543992519379 TRAIN  loss dict:  {'classification_loss': 0.9370543992519379}
2025-01-19 11:07:18,098 [INFO] Step[1700/2713]: training loss : 0.9388222944736481 TRAIN  loss dict:  {'classification_loss': 0.9388222944736481}
2025-01-19 11:07:33,153 [INFO] Step[1750/2713]: training loss : 0.942889347076416 TRAIN  loss dict:  {'classification_loss': 0.942889347076416}
2025-01-19 11:07:48,265 [INFO] Step[1800/2713]: training loss : 0.9394614338874817 TRAIN  loss dict:  {'classification_loss': 0.9394614338874817}
2025-01-19 11:08:03,311 [INFO] Step[1850/2713]: training loss : 0.9392757439613342 TRAIN  loss dict:  {'classification_loss': 0.9392757439613342}
2025-01-19 11:08:18,347 [INFO] Step[1900/2713]: training loss : 0.9370272874832153 TRAIN  loss dict:  {'classification_loss': 0.9370272874832153}
2025-01-19 11:08:33,446 [INFO] Step[1950/2713]: training loss : 0.9391586029529572 TRAIN  loss dict:  {'classification_loss': 0.9391586029529572}
2025-01-19 11:08:48,544 [INFO] Step[2000/2713]: training loss : 0.9395105493068695 TRAIN  loss dict:  {'classification_loss': 0.9395105493068695}
2025-01-19 11:09:03,665 [INFO] Step[2050/2713]: training loss : 0.9400660383701325 TRAIN  loss dict:  {'classification_loss': 0.9400660383701325}
2025-01-19 11:09:18,772 [INFO] Step[2100/2713]: training loss : 0.938136578798294 TRAIN  loss dict:  {'classification_loss': 0.938136578798294}
2025-01-19 11:09:33,821 [INFO] Step[2150/2713]: training loss : 0.9389278650283813 TRAIN  loss dict:  {'classification_loss': 0.9389278650283813}
2025-01-19 11:09:48,901 [INFO] Step[2200/2713]: training loss : 0.9383637762069702 TRAIN  loss dict:  {'classification_loss': 0.9383637762069702}
2025-01-19 11:10:03,988 [INFO] Step[2250/2713]: training loss : 0.9375526475906372 TRAIN  loss dict:  {'classification_loss': 0.9375526475906372}
2025-01-19 11:10:19,077 [INFO] Step[2300/2713]: training loss : 0.9391901564598083 TRAIN  loss dict:  {'classification_loss': 0.9391901564598083}
2025-01-19 11:10:34,158 [INFO] Step[2350/2713]: training loss : 0.9386262679100037 TRAIN  loss dict:  {'classification_loss': 0.9386262679100037}
2025-01-19 11:10:49,291 [INFO] Step[2400/2713]: training loss : 0.9398739540576935 TRAIN  loss dict:  {'classification_loss': 0.9398739540576935}
2025-01-19 11:11:04,380 [INFO] Step[2450/2713]: training loss : 0.9372237050533294 TRAIN  loss dict:  {'classification_loss': 0.9372237050533294}
2025-01-19 11:11:19,419 [INFO] Step[2500/2713]: training loss : 0.939413845539093 TRAIN  loss dict:  {'classification_loss': 0.939413845539093}
2025-01-19 11:11:34,499 [INFO] Step[2550/2713]: training loss : 0.9395690727233886 TRAIN  loss dict:  {'classification_loss': 0.9395690727233886}
2025-01-19 11:11:49,645 [INFO] Step[2600/2713]: training loss : 0.9383040046691895 TRAIN  loss dict:  {'classification_loss': 0.9383040046691895}
2025-01-19 11:12:04,738 [INFO] Step[2650/2713]: training loss : 0.9381614804267884 TRAIN  loss dict:  {'classification_loss': 0.9381614804267884}
2025-01-19 11:12:19,800 [INFO] Step[2700/2713]: training loss : 0.9373039317131042 TRAIN  loss dict:  {'classification_loss': 0.9373039317131042}
2025-01-19 11:13:40,513 [INFO] Label accuracies statistics:
2025-01-19 11:13:40,514 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 0.75, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 11:13:40,515 [INFO] [81] TRAIN  loss: 0.9392874105320265 acc: 0.9998771347831429
2025-01-19 11:13:40,515 [INFO] [81] TRAIN  loss dict: {'classification_loss': 0.9392874105320265}
2025-01-19 11:13:40,515 [INFO] [81] VALIDATION loss: 1.7430408926386582 VALIDATION acc: 0.8087774294670846
2025-01-19 11:13:40,515 [INFO] [81] VALIDATION loss dict: {'classification_loss': 1.7430408926386582}
2025-01-19 11:13:40,515 [INFO] 
2025-01-19 11:14:00,705 [INFO] Step[50/2713]: training loss : 0.9398690962791443 TRAIN  loss dict:  {'classification_loss': 0.9398690962791443}
2025-01-19 11:14:15,698 [INFO] Step[100/2713]: training loss : 0.9380481445789337 TRAIN  loss dict:  {'classification_loss': 0.9380481445789337}
2025-01-19 11:14:30,717 [INFO] Step[150/2713]: training loss : 0.9401261496543885 TRAIN  loss dict:  {'classification_loss': 0.9401261496543885}
2025-01-19 11:14:45,747 [INFO] Step[200/2713]: training loss : 0.939456832408905 TRAIN  loss dict:  {'classification_loss': 0.939456832408905}
2025-01-19 11:15:00,737 [INFO] Step[250/2713]: training loss : 0.9362924420833587 TRAIN  loss dict:  {'classification_loss': 0.9362924420833587}
2025-01-19 11:15:15,690 [INFO] Step[300/2713]: training loss : 0.9397514593601227 TRAIN  loss dict:  {'classification_loss': 0.9397514593601227}
2025-01-19 11:15:30,671 [INFO] Step[350/2713]: training loss : 0.9372576642036438 TRAIN  loss dict:  {'classification_loss': 0.9372576642036438}
2025-01-19 11:15:45,659 [INFO] Step[400/2713]: training loss : 0.9402110397815704 TRAIN  loss dict:  {'classification_loss': 0.9402110397815704}
2025-01-19 11:16:00,617 [INFO] Step[450/2713]: training loss : 0.9394142389297485 TRAIN  loss dict:  {'classification_loss': 0.9394142389297485}
2025-01-19 11:16:15,529 [INFO] Step[500/2713]: training loss : 0.9386056101322174 TRAIN  loss dict:  {'classification_loss': 0.9386056101322174}
2025-01-19 11:16:30,538 [INFO] Step[550/2713]: training loss : 0.9394326388835907 TRAIN  loss dict:  {'classification_loss': 0.9394326388835907}
2025-01-19 11:16:45,525 [INFO] Step[600/2713]: training loss : 0.9389688086509704 TRAIN  loss dict:  {'classification_loss': 0.9389688086509704}
2025-01-19 11:17:00,494 [INFO] Step[650/2713]: training loss : 0.9371875298023223 TRAIN  loss dict:  {'classification_loss': 0.9371875298023223}
2025-01-19 11:17:15,541 [INFO] Step[700/2713]: training loss : 0.9381997191905975 TRAIN  loss dict:  {'classification_loss': 0.9381997191905975}
2025-01-19 11:17:30,520 [INFO] Step[750/2713]: training loss : 0.9364192056655883 TRAIN  loss dict:  {'classification_loss': 0.9364192056655883}
2025-01-19 11:17:45,483 [INFO] Step[800/2713]: training loss : 0.9378800988197327 TRAIN  loss dict:  {'classification_loss': 0.9378800988197327}
2025-01-19 11:18:00,476 [INFO] Step[850/2713]: training loss : 0.9493302607536316 TRAIN  loss dict:  {'classification_loss': 0.9493302607536316}
2025-01-19 11:18:15,462 [INFO] Step[900/2713]: training loss : 0.9376732647418976 TRAIN  loss dict:  {'classification_loss': 0.9376732647418976}
2025-01-19 11:18:30,542 [INFO] Step[950/2713]: training loss : 0.9393854439258575 TRAIN  loss dict:  {'classification_loss': 0.9393854439258575}
2025-01-19 11:18:45,498 [INFO] Step[1000/2713]: training loss : 0.9363645911216736 TRAIN  loss dict:  {'classification_loss': 0.9363645911216736}
2025-01-19 11:19:00,480 [INFO] Step[1050/2713]: training loss : 0.937227840423584 TRAIN  loss dict:  {'classification_loss': 0.937227840423584}
2025-01-19 11:19:15,445 [INFO] Step[1100/2713]: training loss : 0.9411121678352355 TRAIN  loss dict:  {'classification_loss': 0.9411121678352355}
2025-01-19 11:19:30,378 [INFO] Step[1150/2713]: training loss : 0.9372664713859558 TRAIN  loss dict:  {'classification_loss': 0.9372664713859558}
2025-01-19 11:19:45,321 [INFO] Step[1200/2713]: training loss : 0.9368358433246613 TRAIN  loss dict:  {'classification_loss': 0.9368358433246613}
2025-01-19 11:20:00,312 [INFO] Step[1250/2713]: training loss : 0.9394275295734406 TRAIN  loss dict:  {'classification_loss': 0.9394275295734406}
2025-01-19 11:20:15,280 [INFO] Step[1300/2713]: training loss : 0.937153834104538 TRAIN  loss dict:  {'classification_loss': 0.937153834104538}
2025-01-19 11:20:30,330 [INFO] Step[1350/2713]: training loss : 0.9400231516361237 TRAIN  loss dict:  {'classification_loss': 0.9400231516361237}
2025-01-19 11:20:45,316 [INFO] Step[1400/2713]: training loss : 0.9391468858718872 TRAIN  loss dict:  {'classification_loss': 0.9391468858718872}
2025-01-19 11:21:00,356 [INFO] Step[1450/2713]: training loss : 0.9359804773330689 TRAIN  loss dict:  {'classification_loss': 0.9359804773330689}
2025-01-19 11:21:15,291 [INFO] Step[1500/2713]: training loss : 0.9382748448848724 TRAIN  loss dict:  {'classification_loss': 0.9382748448848724}
2025-01-19 11:21:30,268 [INFO] Step[1550/2713]: training loss : 0.9441696000099182 TRAIN  loss dict:  {'classification_loss': 0.9441696000099182}
2025-01-19 11:21:45,230 [INFO] Step[1600/2713]: training loss : 0.9480877578258514 TRAIN  loss dict:  {'classification_loss': 0.9480877578258514}
2025-01-19 11:22:00,218 [INFO] Step[1650/2713]: training loss : 0.9401365005970002 TRAIN  loss dict:  {'classification_loss': 0.9401365005970002}
2025-01-19 11:22:15,180 [INFO] Step[1700/2713]: training loss : 0.9389667332172393 TRAIN  loss dict:  {'classification_loss': 0.9389667332172393}
2025-01-19 11:22:30,182 [INFO] Step[1750/2713]: training loss : 0.9408980762958526 TRAIN  loss dict:  {'classification_loss': 0.9408980762958526}
2025-01-19 11:22:45,192 [INFO] Step[1800/2713]: training loss : 0.9393322563171387 TRAIN  loss dict:  {'classification_loss': 0.9393322563171387}
2025-01-19 11:23:00,126 [INFO] Step[1850/2713]: training loss : 0.9407256352901459 TRAIN  loss dict:  {'classification_loss': 0.9407256352901459}
2025-01-19 11:23:15,190 [INFO] Step[1900/2713]: training loss : 0.9372307658195496 TRAIN  loss dict:  {'classification_loss': 0.9372307658195496}
2025-01-19 11:23:30,197 [INFO] Step[1950/2713]: training loss : 0.9375240516662597 TRAIN  loss dict:  {'classification_loss': 0.9375240516662597}
2025-01-19 11:23:45,208 [INFO] Step[2000/2713]: training loss : 0.9373432421684265 TRAIN  loss dict:  {'classification_loss': 0.9373432421684265}
2025-01-19 11:24:00,175 [INFO] Step[2050/2713]: training loss : 0.9380866944789886 TRAIN  loss dict:  {'classification_loss': 0.9380866944789886}
2025-01-19 11:24:15,076 [INFO] Step[2100/2713]: training loss : 0.9427226328849793 TRAIN  loss dict:  {'classification_loss': 0.9427226328849793}
2025-01-19 11:24:30,049 [INFO] Step[2150/2713]: training loss : 0.944969699382782 TRAIN  loss dict:  {'classification_loss': 0.944969699382782}
2025-01-19 11:24:44,995 [INFO] Step[2200/2713]: training loss : 0.9401685905456543 TRAIN  loss dict:  {'classification_loss': 0.9401685905456543}
2025-01-19 11:24:59,986 [INFO] Step[2250/2713]: training loss : 0.941003258228302 TRAIN  loss dict:  {'classification_loss': 0.941003258228302}
2025-01-19 11:25:14,954 [INFO] Step[2300/2713]: training loss : 0.9541357290744782 TRAIN  loss dict:  {'classification_loss': 0.9541357290744782}
2025-01-19 11:25:29,945 [INFO] Step[2350/2713]: training loss : 0.9378223097324372 TRAIN  loss dict:  {'classification_loss': 0.9378223097324372}
2025-01-19 11:25:44,920 [INFO] Step[2400/2713]: training loss : 0.9386507654190064 TRAIN  loss dict:  {'classification_loss': 0.9386507654190064}
2025-01-19 11:25:59,896 [INFO] Step[2450/2713]: training loss : 0.943766541481018 TRAIN  loss dict:  {'classification_loss': 0.943766541481018}
2025-01-19 11:26:14,899 [INFO] Step[2500/2713]: training loss : 0.9374089181423187 TRAIN  loss dict:  {'classification_loss': 0.9374089181423187}
2025-01-19 11:26:29,889 [INFO] Step[2550/2713]: training loss : 0.9407504665851593 TRAIN  loss dict:  {'classification_loss': 0.9407504665851593}
2025-01-19 11:26:44,871 [INFO] Step[2600/2713]: training loss : 0.9410186445713044 TRAIN  loss dict:  {'classification_loss': 0.9410186445713044}
2025-01-19 11:26:59,829 [INFO] Step[2650/2713]: training loss : 0.9385765945911407 TRAIN  loss dict:  {'classification_loss': 0.9385765945911407}
2025-01-19 11:27:14,858 [INFO] Step[2700/2713]: training loss : 0.9396975553035736 TRAIN  loss dict:  {'classification_loss': 0.9396975553035736}
2025-01-19 11:28:35,562 [INFO] Label accuracies statistics:
2025-01-19 11:28:35,562 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.5, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.25, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 1.0, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 11:28:35,564 [INFO] [82] TRAIN  loss: 0.9397387542805459 acc: 0.9997542695662858
2025-01-19 11:28:35,564 [INFO] [82] TRAIN  loss dict: {'classification_loss': 0.9397387542805459}
2025-01-19 11:28:35,564 [INFO] [82] VALIDATION loss: 1.7391244687308045 VALIDATION acc: 0.812539184952978
2025-01-19 11:28:35,564 [INFO] [82] VALIDATION loss dict: {'classification_loss': 1.7391244687308045}
2025-01-19 11:28:35,564 [INFO] 
2025-01-19 11:28:56,421 [INFO] Step[50/2713]: training loss : 0.9371212804317475 TRAIN  loss dict:  {'classification_loss': 0.9371212804317475}
2025-01-19 11:29:11,441 [INFO] Step[100/2713]: training loss : 0.940010472536087 TRAIN  loss dict:  {'classification_loss': 0.940010472536087}
2025-01-19 11:29:26,483 [INFO] Step[150/2713]: training loss : 0.9384394466876984 TRAIN  loss dict:  {'classification_loss': 0.9384394466876984}
2025-01-19 11:29:41,460 [INFO] Step[200/2713]: training loss : 0.9402326905727386 TRAIN  loss dict:  {'classification_loss': 0.9402326905727386}
2025-01-19 11:29:56,482 [INFO] Step[250/2713]: training loss : 0.9400008869171143 TRAIN  loss dict:  {'classification_loss': 0.9400008869171143}
2025-01-19 11:30:11,450 [INFO] Step[300/2713]: training loss : 0.9375562679767608 TRAIN  loss dict:  {'classification_loss': 0.9375562679767608}
2025-01-19 11:30:26,450 [INFO] Step[350/2713]: training loss : 0.9392466485500336 TRAIN  loss dict:  {'classification_loss': 0.9392466485500336}
2025-01-19 11:30:41,449 [INFO] Step[400/2713]: training loss : 0.9372691810131073 TRAIN  loss dict:  {'classification_loss': 0.9372691810131073}
2025-01-19 11:30:56,465 [INFO] Step[450/2713]: training loss : 0.9390406501293183 TRAIN  loss dict:  {'classification_loss': 0.9390406501293183}
2025-01-19 11:31:11,446 [INFO] Step[500/2713]: training loss : 0.9395717310905457 TRAIN  loss dict:  {'classification_loss': 0.9395717310905457}
2025-01-19 11:31:26,537 [INFO] Step[550/2713]: training loss : 0.9386601400375366 TRAIN  loss dict:  {'classification_loss': 0.9386601400375366}
2025-01-19 11:31:41,592 [INFO] Step[600/2713]: training loss : 0.9386600577831268 TRAIN  loss dict:  {'classification_loss': 0.9386600577831268}
2025-01-19 11:31:56,643 [INFO] Step[650/2713]: training loss : 0.9377245473861694 TRAIN  loss dict:  {'classification_loss': 0.9377245473861694}
2025-01-19 11:32:11,657 [INFO] Step[700/2713]: training loss : 0.9385451877117157 TRAIN  loss dict:  {'classification_loss': 0.9385451877117157}
2025-01-19 11:32:26,743 [INFO] Step[750/2713]: training loss : 0.9407467472553254 TRAIN  loss dict:  {'classification_loss': 0.9407467472553254}
2025-01-19 11:32:41,830 [INFO] Step[800/2713]: training loss : 0.9387740671634675 TRAIN  loss dict:  {'classification_loss': 0.9387740671634675}
2025-01-19 11:32:56,918 [INFO] Step[850/2713]: training loss : 0.9388412690162659 TRAIN  loss dict:  {'classification_loss': 0.9388412690162659}
2025-01-19 11:33:11,970 [INFO] Step[900/2713]: training loss : 0.9391510713100434 TRAIN  loss dict:  {'classification_loss': 0.9391510713100434}
2025-01-19 11:33:27,005 [INFO] Step[950/2713]: training loss : 0.9386345374584198 TRAIN  loss dict:  {'classification_loss': 0.9386345374584198}
2025-01-19 11:33:42,037 [INFO] Step[1000/2713]: training loss : 0.9393857753276825 TRAIN  loss dict:  {'classification_loss': 0.9393857753276825}
2025-01-19 11:33:57,113 [INFO] Step[1050/2713]: training loss : 0.9437007772922515 TRAIN  loss dict:  {'classification_loss': 0.9437007772922515}
2025-01-19 11:34:12,185 [INFO] Step[1100/2713]: training loss : 0.9398165559768676 TRAIN  loss dict:  {'classification_loss': 0.9398165559768676}
2025-01-19 11:34:27,242 [INFO] Step[1150/2713]: training loss : 0.9376604580879211 TRAIN  loss dict:  {'classification_loss': 0.9376604580879211}
2025-01-19 11:34:42,331 [INFO] Step[1200/2713]: training loss : 0.941145703792572 TRAIN  loss dict:  {'classification_loss': 0.941145703792572}
2025-01-19 11:34:57,407 [INFO] Step[1250/2713]: training loss : 0.9393585479259491 TRAIN  loss dict:  {'classification_loss': 0.9393585479259491}
2025-01-19 11:35:12,484 [INFO] Step[1300/2713]: training loss : 0.9372734093666076 TRAIN  loss dict:  {'classification_loss': 0.9372734093666076}
2025-01-19 11:35:27,555 [INFO] Step[1350/2713]: training loss : 0.9413909757137299 TRAIN  loss dict:  {'classification_loss': 0.9413909757137299}
2025-01-19 11:35:42,625 [INFO] Step[1400/2713]: training loss : 0.9375756192207336 TRAIN  loss dict:  {'classification_loss': 0.9375756192207336}
2025-01-19 11:35:57,706 [INFO] Step[1450/2713]: training loss : 0.938223307132721 TRAIN  loss dict:  {'classification_loss': 0.938223307132721}
2025-01-19 11:36:12,774 [INFO] Step[1500/2713]: training loss : 0.9545158886909485 TRAIN  loss dict:  {'classification_loss': 0.9545158886909485}
2025-01-19 11:36:27,778 [INFO] Step[1550/2713]: training loss : 0.941578220129013 TRAIN  loss dict:  {'classification_loss': 0.941578220129013}
2025-01-19 11:36:42,867 [INFO] Step[1600/2713]: training loss : 0.9368235206604004 TRAIN  loss dict:  {'classification_loss': 0.9368235206604004}
2025-01-19 11:36:57,933 [INFO] Step[1650/2713]: training loss : 0.9373561215400695 TRAIN  loss dict:  {'classification_loss': 0.9373561215400695}
2025-01-19 11:37:12,990 [INFO] Step[1700/2713]: training loss : 0.9387940609455109 TRAIN  loss dict:  {'classification_loss': 0.9387940609455109}
2025-01-19 11:37:28,041 [INFO] Step[1750/2713]: training loss : 0.9364127767086029 TRAIN  loss dict:  {'classification_loss': 0.9364127767086029}
2025-01-19 11:37:43,063 [INFO] Step[1800/2713]: training loss : 0.9390268433094024 TRAIN  loss dict:  {'classification_loss': 0.9390268433094024}
2025-01-19 11:37:58,134 [INFO] Step[1850/2713]: training loss : 0.9370243501663208 TRAIN  loss dict:  {'classification_loss': 0.9370243501663208}
2025-01-19 11:38:13,196 [INFO] Step[1900/2713]: training loss : 0.9393661856651306 TRAIN  loss dict:  {'classification_loss': 0.9393661856651306}
2025-01-19 11:38:28,279 [INFO] Step[1950/2713]: training loss : 0.9378871321678162 TRAIN  loss dict:  {'classification_loss': 0.9378871321678162}
2025-01-19 11:38:43,329 [INFO] Step[2000/2713]: training loss : 0.9367879998683929 TRAIN  loss dict:  {'classification_loss': 0.9367879998683929}
2025-01-19 11:38:58,401 [INFO] Step[2050/2713]: training loss : 0.9396887457370758 TRAIN  loss dict:  {'classification_loss': 0.9396887457370758}
2025-01-19 11:39:13,475 [INFO] Step[2100/2713]: training loss : 0.9380291140079499 TRAIN  loss dict:  {'classification_loss': 0.9380291140079499}
2025-01-19 11:39:28,558 [INFO] Step[2150/2713]: training loss : 0.9373878347873688 TRAIN  loss dict:  {'classification_loss': 0.9373878347873688}
2025-01-19 11:39:43,629 [INFO] Step[2200/2713]: training loss : 0.950691808462143 TRAIN  loss dict:  {'classification_loss': 0.950691808462143}
2025-01-19 11:39:58,727 [INFO] Step[2250/2713]: training loss : 0.9386505496501922 TRAIN  loss dict:  {'classification_loss': 0.9386505496501922}
2025-01-19 11:40:13,740 [INFO] Step[2300/2713]: training loss : 0.9360733687877655 TRAIN  loss dict:  {'classification_loss': 0.9360733687877655}
2025-01-19 11:40:28,825 [INFO] Step[2350/2713]: training loss : 0.9396121823787689 TRAIN  loss dict:  {'classification_loss': 0.9396121823787689}
2025-01-19 11:40:43,943 [INFO] Step[2400/2713]: training loss : 0.9387706661224365 TRAIN  loss dict:  {'classification_loss': 0.9387706661224365}
2025-01-19 11:40:59,018 [INFO] Step[2450/2713]: training loss : 0.9380287408828736 TRAIN  loss dict:  {'classification_loss': 0.9380287408828736}
2025-01-19 11:41:14,088 [INFO] Step[2500/2713]: training loss : 0.9412352764606475 TRAIN  loss dict:  {'classification_loss': 0.9412352764606475}
2025-01-19 11:41:29,156 [INFO] Step[2550/2713]: training loss : 0.9386406707763671 TRAIN  loss dict:  {'classification_loss': 0.9386406707763671}
2025-01-19 11:41:44,242 [INFO] Step[2600/2713]: training loss : 0.9393299949169159 TRAIN  loss dict:  {'classification_loss': 0.9393299949169159}
2025-01-19 11:41:59,335 [INFO] Step[2650/2713]: training loss : 0.9369021666049957 TRAIN  loss dict:  {'classification_loss': 0.9369021666049957}
2025-01-19 11:42:14,394 [INFO] Step[2700/2713]: training loss : 0.9412511694431305 TRAIN  loss dict:  {'classification_loss': 0.9412511694431305}
2025-01-19 11:43:34,691 [INFO] Label accuracies statistics:
2025-01-19 11:43:34,691 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 1.0, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.5, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 11:43:34,693 [INFO] [83] TRAIN  loss: 0.9393229153470012 acc: 0.9997542695662858
2025-01-19 11:43:34,693 [INFO] [83] TRAIN  loss dict: {'classification_loss': 0.9393229153470012}
2025-01-19 11:43:34,693 [INFO] [83] VALIDATION loss: 1.746216716510909 VALIDATION acc: 0.8100313479623824
2025-01-19 11:43:34,693 [INFO] [83] VALIDATION loss dict: {'classification_loss': 1.746216716510909}
2025-01-19 11:43:34,693 [INFO] 
2025-01-19 11:43:55,253 [INFO] Step[50/2713]: training loss : 0.9375804650783539 TRAIN  loss dict:  {'classification_loss': 0.9375804650783539}
2025-01-19 11:44:10,355 [INFO] Step[100/2713]: training loss : 0.9392866265773773 TRAIN  loss dict:  {'classification_loss': 0.9392866265773773}
2025-01-19 11:44:25,464 [INFO] Step[150/2713]: training loss : 0.9396420323848724 TRAIN  loss dict:  {'classification_loss': 0.9396420323848724}
2025-01-19 11:44:40,569 [INFO] Step[200/2713]: training loss : 0.9397435629367829 TRAIN  loss dict:  {'classification_loss': 0.9397435629367829}
2025-01-19 11:44:55,672 [INFO] Step[250/2713]: training loss : 0.9383891320228577 TRAIN  loss dict:  {'classification_loss': 0.9383891320228577}
2025-01-19 11:45:10,785 [INFO] Step[300/2713]: training loss : 0.9393800818920135 TRAIN  loss dict:  {'classification_loss': 0.9393800818920135}
2025-01-19 11:45:25,896 [INFO] Step[350/2713]: training loss : 0.9383501172065735 TRAIN  loss dict:  {'classification_loss': 0.9383501172065735}
2025-01-19 11:45:40,983 [INFO] Step[400/2713]: training loss : 0.9412801480293274 TRAIN  loss dict:  {'classification_loss': 0.9412801480293274}
2025-01-19 11:45:56,079 [INFO] Step[450/2713]: training loss : 0.9373175311088562 TRAIN  loss dict:  {'classification_loss': 0.9373175311088562}
2025-01-19 11:46:10,962 [INFO] Step[500/2713]: training loss : 0.9381336975097656 TRAIN  loss dict:  {'classification_loss': 0.9381336975097656}
2025-01-19 11:46:25,811 [INFO] Step[550/2713]: training loss : 0.9378587472438812 TRAIN  loss dict:  {'classification_loss': 0.9378587472438812}
2025-01-19 11:46:40,766 [INFO] Step[600/2713]: training loss : 0.9366995346546173 TRAIN  loss dict:  {'classification_loss': 0.9366995346546173}
2025-01-19 11:46:55,641 [INFO] Step[650/2713]: training loss : 0.9373134684562683 TRAIN  loss dict:  {'classification_loss': 0.9373134684562683}
2025-01-19 11:47:10,556 [INFO] Step[700/2713]: training loss : 0.9391751325130463 TRAIN  loss dict:  {'classification_loss': 0.9391751325130463}
2025-01-19 11:47:25,499 [INFO] Step[750/2713]: training loss : 0.9384221243858337 TRAIN  loss dict:  {'classification_loss': 0.9384221243858337}
2025-01-19 11:47:40,371 [INFO] Step[800/2713]: training loss : 0.9390882277488708 TRAIN  loss dict:  {'classification_loss': 0.9390882277488708}
2025-01-19 11:47:55,310 [INFO] Step[850/2713]: training loss : 0.9396785628795624 TRAIN  loss dict:  {'classification_loss': 0.9396785628795624}
2025-01-19 11:48:10,209 [INFO] Step[900/2713]: training loss : 0.9384277212619782 TRAIN  loss dict:  {'classification_loss': 0.9384277212619782}
2025-01-19 11:48:25,087 [INFO] Step[950/2713]: training loss : 0.9374270129203797 TRAIN  loss dict:  {'classification_loss': 0.9374270129203797}
2025-01-19 11:48:39,956 [INFO] Step[1000/2713]: training loss : 0.9398774087429047 TRAIN  loss dict:  {'classification_loss': 0.9398774087429047}
2025-01-19 11:48:54,929 [INFO] Step[1050/2713]: training loss : 0.9360965430736542 TRAIN  loss dict:  {'classification_loss': 0.9360965430736542}
2025-01-19 11:49:09,918 [INFO] Step[1100/2713]: training loss : 0.9413210010528564 TRAIN  loss dict:  {'classification_loss': 0.9413210010528564}
2025-01-19 11:49:24,808 [INFO] Step[1150/2713]: training loss : 0.9448914396762848 TRAIN  loss dict:  {'classification_loss': 0.9448914396762848}
2025-01-19 11:49:39,690 [INFO] Step[1200/2713]: training loss : 0.9393825244903564 TRAIN  loss dict:  {'classification_loss': 0.9393825244903564}
2025-01-19 11:49:54,623 [INFO] Step[1250/2713]: training loss : 0.9424569690227509 TRAIN  loss dict:  {'classification_loss': 0.9424569690227509}
2025-01-19 11:50:09,606 [INFO] Step[1300/2713]: training loss : 0.9444683885574341 TRAIN  loss dict:  {'classification_loss': 0.9444683885574341}
2025-01-19 11:50:24,487 [INFO] Step[1350/2713]: training loss : 0.9406954288482666 TRAIN  loss dict:  {'classification_loss': 0.9406954288482666}
2025-01-19 11:50:39,362 [INFO] Step[1400/2713]: training loss : 0.9399080276489258 TRAIN  loss dict:  {'classification_loss': 0.9399080276489258}
2025-01-19 11:50:54,258 [INFO] Step[1450/2713]: training loss : 0.9389896667003632 TRAIN  loss dict:  {'classification_loss': 0.9389896667003632}
2025-01-19 11:51:09,184 [INFO] Step[1500/2713]: training loss : 0.9394472801685333 TRAIN  loss dict:  {'classification_loss': 0.9394472801685333}
2025-01-19 11:51:24,151 [INFO] Step[1550/2713]: training loss : 0.9375142240524292 TRAIN  loss dict:  {'classification_loss': 0.9375142240524292}
2025-01-19 11:51:39,079 [INFO] Step[1600/2713]: training loss : 0.9378008878231049 TRAIN  loss dict:  {'classification_loss': 0.9378008878231049}
2025-01-19 11:51:54,017 [INFO] Step[1650/2713]: training loss : 0.9382896459102631 TRAIN  loss dict:  {'classification_loss': 0.9382896459102631}
2025-01-19 11:52:08,977 [INFO] Step[1700/2713]: training loss : 0.9359516382217408 TRAIN  loss dict:  {'classification_loss': 0.9359516382217408}
2025-01-19 11:52:23,836 [INFO] Step[1750/2713]: training loss : 0.9398217749595642 TRAIN  loss dict:  {'classification_loss': 0.9398217749595642}
2025-01-19 11:52:38,740 [INFO] Step[1800/2713]: training loss : 0.9369223320484161 TRAIN  loss dict:  {'classification_loss': 0.9369223320484161}
2025-01-19 11:52:53,685 [INFO] Step[1850/2713]: training loss : 0.9364195954799652 TRAIN  loss dict:  {'classification_loss': 0.9364195954799652}
2025-01-19 11:53:08,542 [INFO] Step[1900/2713]: training loss : 0.9375041961669922 TRAIN  loss dict:  {'classification_loss': 0.9375041961669922}
2025-01-19 11:53:23,455 [INFO] Step[1950/2713]: training loss : 0.9369483423233033 TRAIN  loss dict:  {'classification_loss': 0.9369483423233033}
2025-01-19 11:53:38,275 [INFO] Step[2000/2713]: training loss : 0.936534411907196 TRAIN  loss dict:  {'classification_loss': 0.936534411907196}
2025-01-19 11:53:53,161 [INFO] Step[2050/2713]: training loss : 0.9378380262851715 TRAIN  loss dict:  {'classification_loss': 0.9378380262851715}
2025-01-19 11:54:08,120 [INFO] Step[2100/2713]: training loss : 0.9375018072128296 TRAIN  loss dict:  {'classification_loss': 0.9375018072128296}
2025-01-19 11:54:23,049 [INFO] Step[2150/2713]: training loss : 0.9389275312423706 TRAIN  loss dict:  {'classification_loss': 0.9389275312423706}
2025-01-19 11:54:37,903 [INFO] Step[2200/2713]: training loss : 0.9382165336608886 TRAIN  loss dict:  {'classification_loss': 0.9382165336608886}
2025-01-19 11:54:52,832 [INFO] Step[2250/2713]: training loss : 0.9370338034629822 TRAIN  loss dict:  {'classification_loss': 0.9370338034629822}
2025-01-19 11:55:07,725 [INFO] Step[2300/2713]: training loss : 0.9393276095390319 TRAIN  loss dict:  {'classification_loss': 0.9393276095390319}
2025-01-19 11:55:22,613 [INFO] Step[2350/2713]: training loss : 0.939481292963028 TRAIN  loss dict:  {'classification_loss': 0.939481292963028}
2025-01-19 11:55:37,562 [INFO] Step[2400/2713]: training loss : 0.9371093714237213 TRAIN  loss dict:  {'classification_loss': 0.9371093714237213}
2025-01-19 11:55:52,494 [INFO] Step[2450/2713]: training loss : 0.9373742616176606 TRAIN  loss dict:  {'classification_loss': 0.9373742616176606}
2025-01-19 11:56:07,335 [INFO] Step[2500/2713]: training loss : 0.9418797671794892 TRAIN  loss dict:  {'classification_loss': 0.9418797671794892}
2025-01-19 11:56:22,228 [INFO] Step[2550/2713]: training loss : 0.9378004944324494 TRAIN  loss dict:  {'classification_loss': 0.9378004944324494}
2025-01-19 11:56:37,052 [INFO] Step[2600/2713]: training loss : 0.9381821060180664 TRAIN  loss dict:  {'classification_loss': 0.9381821060180664}
2025-01-19 11:56:51,982 [INFO] Step[2650/2713]: training loss : 0.938229193687439 TRAIN  loss dict:  {'classification_loss': 0.938229193687439}
2025-01-19 11:57:06,972 [INFO] Step[2700/2713]: training loss : 0.9398161256313324 TRAIN  loss dict:  {'classification_loss': 0.9398161256313324}
2025-01-19 11:58:27,063 [INFO] Label accuracies statistics:
2025-01-19 11:58:27,063 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 11:58:27,065 [INFO] [84] TRAIN  loss: 0.9387651213279651 acc: 1.0
2025-01-19 11:58:27,065 [INFO] [84] TRAIN  loss dict: {'classification_loss': 0.9387651213279651}
2025-01-19 11:58:27,065 [INFO] [84] VALIDATION loss: 1.7512991697268379 VALIDATION acc: 0.8050156739811912
2025-01-19 11:58:27,065 [INFO] [84] VALIDATION loss dict: {'classification_loss': 1.7512991697268379}
2025-01-19 11:58:27,065 [INFO] 
2025-01-19 11:58:47,395 [INFO] Step[50/2713]: training loss : 0.9390866994857788 TRAIN  loss dict:  {'classification_loss': 0.9390866994857788}
2025-01-19 11:59:02,299 [INFO] Step[100/2713]: training loss : 0.9538885414600372 TRAIN  loss dict:  {'classification_loss': 0.9538885414600372}
2025-01-19 11:59:17,252 [INFO] Step[150/2713]: training loss : 0.9367912292480469 TRAIN  loss dict:  {'classification_loss': 0.9367912292480469}
2025-01-19 11:59:32,149 [INFO] Step[200/2713]: training loss : 0.9380512857437133 TRAIN  loss dict:  {'classification_loss': 0.9380512857437133}
2025-01-19 11:59:47,117 [INFO] Step[250/2713]: training loss : 0.9410995602607727 TRAIN  loss dict:  {'classification_loss': 0.9410995602607727}
2025-01-19 12:00:02,086 [INFO] Step[300/2713]: training loss : 0.9386448109149933 TRAIN  loss dict:  {'classification_loss': 0.9386448109149933}
2025-01-19 12:00:17,040 [INFO] Step[350/2713]: training loss : 0.9419357800483703 TRAIN  loss dict:  {'classification_loss': 0.9419357800483703}
2025-01-19 12:00:32,003 [INFO] Step[400/2713]: training loss : 0.938329428434372 TRAIN  loss dict:  {'classification_loss': 0.938329428434372}
2025-01-19 12:00:46,983 [INFO] Step[450/2713]: training loss : 0.937733381986618 TRAIN  loss dict:  {'classification_loss': 0.937733381986618}
2025-01-19 12:01:01,951 [INFO] Step[500/2713]: training loss : 0.9414568662643432 TRAIN  loss dict:  {'classification_loss': 0.9414568662643432}
2025-01-19 12:01:16,865 [INFO] Step[550/2713]: training loss : 0.9378745663166046 TRAIN  loss dict:  {'classification_loss': 0.9378745663166046}
2025-01-19 12:01:31,756 [INFO] Step[600/2713]: training loss : 0.9382919526100159 TRAIN  loss dict:  {'classification_loss': 0.9382919526100159}
2025-01-19 12:01:46,688 [INFO] Step[650/2713]: training loss : 0.9368298900127411 TRAIN  loss dict:  {'classification_loss': 0.9368298900127411}
2025-01-19 12:02:01,648 [INFO] Step[700/2713]: training loss : 0.935782846212387 TRAIN  loss dict:  {'classification_loss': 0.935782846212387}
2025-01-19 12:02:16,620 [INFO] Step[750/2713]: training loss : 0.9385858643054962 TRAIN  loss dict:  {'classification_loss': 0.9385858643054962}
2025-01-19 12:02:31,568 [INFO] Step[800/2713]: training loss : 0.9376397287845611 TRAIN  loss dict:  {'classification_loss': 0.9376397287845611}
2025-01-19 12:02:46,502 [INFO] Step[850/2713]: training loss : 0.9374160325527191 TRAIN  loss dict:  {'classification_loss': 0.9374160325527191}
2025-01-19 12:03:01,373 [INFO] Step[900/2713]: training loss : 0.9352629089355469 TRAIN  loss dict:  {'classification_loss': 0.9352629089355469}
2025-01-19 12:03:16,327 [INFO] Step[950/2713]: training loss : 0.9381237435340881 TRAIN  loss dict:  {'classification_loss': 0.9381237435340881}
2025-01-19 12:03:31,314 [INFO] Step[1000/2713]: training loss : 0.9366279637813568 TRAIN  loss dict:  {'classification_loss': 0.9366279637813568}
2025-01-19 12:03:46,238 [INFO] Step[1050/2713]: training loss : 0.9378057813644409 TRAIN  loss dict:  {'classification_loss': 0.9378057813644409}
2025-01-19 12:04:01,189 [INFO] Step[1100/2713]: training loss : 0.9367017662525177 TRAIN  loss dict:  {'classification_loss': 0.9367017662525177}
2025-01-19 12:04:16,199 [INFO] Step[1150/2713]: training loss : 0.9396890997886658 TRAIN  loss dict:  {'classification_loss': 0.9396890997886658}
2025-01-19 12:04:31,166 [INFO] Step[1200/2713]: training loss : 0.9391065835952759 TRAIN  loss dict:  {'classification_loss': 0.9391065835952759}
2025-01-19 12:04:46,145 [INFO] Step[1250/2713]: training loss : 0.9386564779281616 TRAIN  loss dict:  {'classification_loss': 0.9386564779281616}
2025-01-19 12:05:01,085 [INFO] Step[1300/2713]: training loss : 0.9387836647033692 TRAIN  loss dict:  {'classification_loss': 0.9387836647033692}
2025-01-19 12:05:16,036 [INFO] Step[1350/2713]: training loss : 0.9373777329921722 TRAIN  loss dict:  {'classification_loss': 0.9373777329921722}
2025-01-19 12:05:30,978 [INFO] Step[1400/2713]: training loss : 0.9442568945884705 TRAIN  loss dict:  {'classification_loss': 0.9442568945884705}
2025-01-19 12:05:46,030 [INFO] Step[1450/2713]: training loss : 0.9368629586696625 TRAIN  loss dict:  {'classification_loss': 0.9368629586696625}
2025-01-19 12:06:00,951 [INFO] Step[1500/2713]: training loss : 0.9397110450267792 TRAIN  loss dict:  {'classification_loss': 0.9397110450267792}
2025-01-19 12:06:15,879 [INFO] Step[1550/2713]: training loss : 0.9403311765193939 TRAIN  loss dict:  {'classification_loss': 0.9403311765193939}
2025-01-19 12:06:30,849 [INFO] Step[1600/2713]: training loss : 0.938523736000061 TRAIN  loss dict:  {'classification_loss': 0.938523736000061}
2025-01-19 12:06:45,853 [INFO] Step[1650/2713]: training loss : 0.9399496400356293 TRAIN  loss dict:  {'classification_loss': 0.9399496400356293}
2025-01-19 12:07:00,761 [INFO] Step[1700/2713]: training loss : 0.9367675232887268 TRAIN  loss dict:  {'classification_loss': 0.9367675232887268}
2025-01-19 12:07:15,843 [INFO] Step[1750/2713]: training loss : 0.9427234184741974 TRAIN  loss dict:  {'classification_loss': 0.9427234184741974}
2025-01-19 12:07:30,924 [INFO] Step[1800/2713]: training loss : 0.9390744030475616 TRAIN  loss dict:  {'classification_loss': 0.9390744030475616}
2025-01-19 12:07:46,049 [INFO] Step[1850/2713]: training loss : 0.9385025537014008 TRAIN  loss dict:  {'classification_loss': 0.9385025537014008}
2025-01-19 12:08:01,120 [INFO] Step[1900/2713]: training loss : 0.9370361220836639 TRAIN  loss dict:  {'classification_loss': 0.9370361220836639}
2025-01-19 12:08:16,214 [INFO] Step[1950/2713]: training loss : 0.9373620140552521 TRAIN  loss dict:  {'classification_loss': 0.9373620140552521}
2025-01-19 12:08:31,286 [INFO] Step[2000/2713]: training loss : 0.9369080710411072 TRAIN  loss dict:  {'classification_loss': 0.9369080710411072}
2025-01-19 12:08:46,347 [INFO] Step[2050/2713]: training loss : 0.9379043757915497 TRAIN  loss dict:  {'classification_loss': 0.9379043757915497}
2025-01-19 12:09:01,415 [INFO] Step[2100/2713]: training loss : 0.9378225040435791 TRAIN  loss dict:  {'classification_loss': 0.9378225040435791}
2025-01-19 12:09:16,522 [INFO] Step[2150/2713]: training loss : 0.9376056289672852 TRAIN  loss dict:  {'classification_loss': 0.9376056289672852}
2025-01-19 12:09:31,621 [INFO] Step[2200/2713]: training loss : 0.9381030941009522 TRAIN  loss dict:  {'classification_loss': 0.9381030941009522}
2025-01-19 12:09:46,712 [INFO] Step[2250/2713]: training loss : 0.9379127514362335 TRAIN  loss dict:  {'classification_loss': 0.9379127514362335}
2025-01-19 12:10:01,744 [INFO] Step[2300/2713]: training loss : 0.9369987952709198 TRAIN  loss dict:  {'classification_loss': 0.9369987952709198}
2025-01-19 12:10:16,873 [INFO] Step[2350/2713]: training loss : 0.9402914679050446 TRAIN  loss dict:  {'classification_loss': 0.9402914679050446}
2025-01-19 12:10:31,934 [INFO] Step[2400/2713]: training loss : 0.9424074876308441 TRAIN  loss dict:  {'classification_loss': 0.9424074876308441}
2025-01-19 12:10:47,036 [INFO] Step[2450/2713]: training loss : 0.9377376627922058 TRAIN  loss dict:  {'classification_loss': 0.9377376627922058}
2025-01-19 12:11:02,114 [INFO] Step[2500/2713]: training loss : 0.9379138910770416 TRAIN  loss dict:  {'classification_loss': 0.9379138910770416}
2025-01-19 12:11:17,185 [INFO] Step[2550/2713]: training loss : 0.9386095404624939 TRAIN  loss dict:  {'classification_loss': 0.9386095404624939}
2025-01-19 12:11:32,300 [INFO] Step[2600/2713]: training loss : 0.938647928237915 TRAIN  loss dict:  {'classification_loss': 0.938647928237915}
2025-01-19 12:11:47,393 [INFO] Step[2650/2713]: training loss : 0.9536026251316071 TRAIN  loss dict:  {'classification_loss': 0.9536026251316071}
2025-01-19 12:12:02,520 [INFO] Step[2700/2713]: training loss : 0.9374238288402558 TRAIN  loss dict:  {'classification_loss': 0.9374238288402558}
2025-01-19 12:13:23,087 [INFO] Label accuracies statistics:
2025-01-19 12:13:23,088 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.25, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 12:13:23,093 [INFO] [85] TRAIN  loss: 0.9390432788268517 acc: 0.9997542695662858
2025-01-19 12:13:23,094 [INFO] [85] TRAIN  loss dict: {'classification_loss': 0.9390432788268517}
2025-01-19 12:13:23,094 [INFO] [85] VALIDATION loss: 1.730586215853691 VALIDATION acc: 0.8119122257053292
2025-01-19 12:13:23,094 [INFO] [85] VALIDATION loss dict: {'classification_loss': 1.730586215853691}
2025-01-19 12:13:23,094 [INFO] 
2025-01-19 12:13:43,694 [INFO] Step[50/2713]: training loss : 0.9368245232105256 TRAIN  loss dict:  {'classification_loss': 0.9368245232105256}
2025-01-19 12:13:58,810 [INFO] Step[100/2713]: training loss : 0.9362185776233674 TRAIN  loss dict:  {'classification_loss': 0.9362185776233674}
2025-01-19 12:14:13,891 [INFO] Step[150/2713]: training loss : 0.9378013253211975 TRAIN  loss dict:  {'classification_loss': 0.9378013253211975}
2025-01-19 12:14:29,078 [INFO] Step[200/2713]: training loss : 0.9398113799095154 TRAIN  loss dict:  {'classification_loss': 0.9398113799095154}
2025-01-19 12:14:44,180 [INFO] Step[250/2713]: training loss : 0.9387726926803589 TRAIN  loss dict:  {'classification_loss': 0.9387726926803589}
2025-01-19 12:14:59,278 [INFO] Step[300/2713]: training loss : 0.9409495210647583 TRAIN  loss dict:  {'classification_loss': 0.9409495210647583}
2025-01-19 12:15:14,398 [INFO] Step[350/2713]: training loss : 0.938246042728424 TRAIN  loss dict:  {'classification_loss': 0.938246042728424}
2025-01-19 12:15:29,479 [INFO] Step[400/2713]: training loss : 0.9381036829948425 TRAIN  loss dict:  {'classification_loss': 0.9381036829948425}
2025-01-19 12:15:44,593 [INFO] Step[450/2713]: training loss : 0.9398938524723053 TRAIN  loss dict:  {'classification_loss': 0.9398938524723053}
2025-01-19 12:15:59,698 [INFO] Step[500/2713]: training loss : 0.9392878806591034 TRAIN  loss dict:  {'classification_loss': 0.9392878806591034}
2025-01-19 12:16:14,863 [INFO] Step[550/2713]: training loss : 0.9400362014770508 TRAIN  loss dict:  {'classification_loss': 0.9400362014770508}
2025-01-19 12:16:29,982 [INFO] Step[600/2713]: training loss : 0.9388190388679505 TRAIN  loss dict:  {'classification_loss': 0.9388190388679505}
2025-01-19 12:16:45,116 [INFO] Step[650/2713]: training loss : 0.939211459159851 TRAIN  loss dict:  {'classification_loss': 0.939211459159851}
2025-01-19 12:17:00,241 [INFO] Step[700/2713]: training loss : 0.9409499549865723 TRAIN  loss dict:  {'classification_loss': 0.9409499549865723}
2025-01-19 12:17:15,392 [INFO] Step[750/2713]: training loss : 0.9369774401187897 TRAIN  loss dict:  {'classification_loss': 0.9369774401187897}
2025-01-19 12:17:30,505 [INFO] Step[800/2713]: training loss : 0.935868787765503 TRAIN  loss dict:  {'classification_loss': 0.935868787765503}
2025-01-19 12:17:45,649 [INFO] Step[850/2713]: training loss : 0.9381106090545654 TRAIN  loss dict:  {'classification_loss': 0.9381106090545654}
2025-01-19 12:18:00,702 [INFO] Step[900/2713]: training loss : 0.9371806812286377 TRAIN  loss dict:  {'classification_loss': 0.9371806812286377}
2025-01-19 12:18:15,742 [INFO] Step[950/2713]: training loss : 0.9394790494441986 TRAIN  loss dict:  {'classification_loss': 0.9394790494441986}
2025-01-19 12:18:30,883 [INFO] Step[1000/2713]: training loss : 0.9378730928897858 TRAIN  loss dict:  {'classification_loss': 0.9378730928897858}
2025-01-19 12:18:46,017 [INFO] Step[1050/2713]: training loss : 0.9367121827602386 TRAIN  loss dict:  {'classification_loss': 0.9367121827602386}
2025-01-19 12:19:01,145 [INFO] Step[1100/2713]: training loss : 0.9373845517635345 TRAIN  loss dict:  {'classification_loss': 0.9373845517635345}
2025-01-19 12:19:16,270 [INFO] Step[1150/2713]: training loss : 0.9393078064918519 TRAIN  loss dict:  {'classification_loss': 0.9393078064918519}
2025-01-19 12:19:31,431 [INFO] Step[1200/2713]: training loss : 0.9430583083629608 TRAIN  loss dict:  {'classification_loss': 0.9430583083629608}
2025-01-19 12:19:46,524 [INFO] Step[1250/2713]: training loss : 0.9371021568775177 TRAIN  loss dict:  {'classification_loss': 0.9371021568775177}
2025-01-19 12:20:01,662 [INFO] Step[1300/2713]: training loss : 0.9373747050762177 TRAIN  loss dict:  {'classification_loss': 0.9373747050762177}
2025-01-19 12:20:16,756 [INFO] Step[1350/2713]: training loss : 0.9391833734512329 TRAIN  loss dict:  {'classification_loss': 0.9391833734512329}
2025-01-19 12:20:31,877 [INFO] Step[1400/2713]: training loss : 0.9366688776016235 TRAIN  loss dict:  {'classification_loss': 0.9366688776016235}
2025-01-19 12:20:47,006 [INFO] Step[1450/2713]: training loss : 0.939189670085907 TRAIN  loss dict:  {'classification_loss': 0.939189670085907}
2025-01-19 12:21:02,157 [INFO] Step[1500/2713]: training loss : 0.9386570453643799 TRAIN  loss dict:  {'classification_loss': 0.9386570453643799}
2025-01-19 12:21:17,289 [INFO] Step[1550/2713]: training loss : 0.9372250890731811 TRAIN  loss dict:  {'classification_loss': 0.9372250890731811}
2025-01-19 12:21:32,454 [INFO] Step[1600/2713]: training loss : 0.9377321183681488 TRAIN  loss dict:  {'classification_loss': 0.9377321183681488}
2025-01-19 12:21:47,574 [INFO] Step[1650/2713]: training loss : 0.9381629264354706 TRAIN  loss dict:  {'classification_loss': 0.9381629264354706}
2025-01-19 12:22:02,707 [INFO] Step[1700/2713]: training loss : 0.940223103761673 TRAIN  loss dict:  {'classification_loss': 0.940223103761673}
2025-01-19 12:22:17,808 [INFO] Step[1750/2713]: training loss : 0.9354864990711212 TRAIN  loss dict:  {'classification_loss': 0.9354864990711212}
2025-01-19 12:22:32,898 [INFO] Step[1800/2713]: training loss : 0.9397251796722412 TRAIN  loss dict:  {'classification_loss': 0.9397251796722412}
2025-01-19 12:22:47,969 [INFO] Step[1850/2713]: training loss : 0.9370956230163574 TRAIN  loss dict:  {'classification_loss': 0.9370956230163574}
2025-01-19 12:23:03,076 [INFO] Step[1900/2713]: training loss : 0.9381074905395508 TRAIN  loss dict:  {'classification_loss': 0.9381074905395508}
2025-01-19 12:23:18,176 [INFO] Step[1950/2713]: training loss : 0.9386151599884033 TRAIN  loss dict:  {'classification_loss': 0.9386151599884033}
2025-01-19 12:23:33,259 [INFO] Step[2000/2713]: training loss : 0.9405783903598786 TRAIN  loss dict:  {'classification_loss': 0.9405783903598786}
2025-01-19 12:23:48,390 [INFO] Step[2050/2713]: training loss : 0.9518117463588714 TRAIN  loss dict:  {'classification_loss': 0.9518117463588714}
2025-01-19 12:24:03,509 [INFO] Step[2100/2713]: training loss : 0.9376667010784149 TRAIN  loss dict:  {'classification_loss': 0.9376667010784149}
2025-01-19 12:24:18,624 [INFO] Step[2150/2713]: training loss : 0.9361791896820069 TRAIN  loss dict:  {'classification_loss': 0.9361791896820069}
2025-01-19 12:24:33,770 [INFO] Step[2200/2713]: training loss : 0.9367507886886597 TRAIN  loss dict:  {'classification_loss': 0.9367507886886597}
2025-01-19 12:24:48,909 [INFO] Step[2250/2713]: training loss : 0.9376910078525543 TRAIN  loss dict:  {'classification_loss': 0.9376910078525543}
2025-01-19 12:25:04,031 [INFO] Step[2300/2713]: training loss : 0.9396058344841003 TRAIN  loss dict:  {'classification_loss': 0.9396058344841003}
2025-01-19 12:25:19,193 [INFO] Step[2350/2713]: training loss : 0.9458769977092742 TRAIN  loss dict:  {'classification_loss': 0.9458769977092742}
2025-01-19 12:25:34,329 [INFO] Step[2400/2713]: training loss : 0.9413539898395539 TRAIN  loss dict:  {'classification_loss': 0.9413539898395539}
2025-01-19 12:25:49,491 [INFO] Step[2450/2713]: training loss : 0.9429812335968017 TRAIN  loss dict:  {'classification_loss': 0.9429812335968017}
2025-01-19 12:26:04,633 [INFO] Step[2500/2713]: training loss : 0.9459037041664123 TRAIN  loss dict:  {'classification_loss': 0.9459037041664123}
2025-01-19 12:26:19,762 [INFO] Step[2550/2713]: training loss : 0.9372554981708526 TRAIN  loss dict:  {'classification_loss': 0.9372554981708526}
2025-01-19 12:26:34,884 [INFO] Step[2600/2713]: training loss : 0.9374334263801575 TRAIN  loss dict:  {'classification_loss': 0.9374334263801575}
2025-01-19 12:26:49,973 [INFO] Step[2650/2713]: training loss : 0.9362451791763305 TRAIN  loss dict:  {'classification_loss': 0.9362451791763305}
2025-01-19 12:27:05,117 [INFO] Step[2700/2713]: training loss : 0.9383082592487335 TRAIN  loss dict:  {'classification_loss': 0.9383082592487335}
2025-01-19 12:28:25,402 [INFO] Label accuracies statistics:
2025-01-19 12:28:25,402 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.75, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.25, 396: 1.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 12:28:25,422 [INFO] [86] TRAIN  loss: 0.9389463109456874 acc: 0.9997542695662858
2025-01-19 12:28:25,422 [INFO] [86] TRAIN  loss dict: {'classification_loss': 0.9389463109456874}
2025-01-19 12:28:25,422 [INFO] [86] VALIDATION loss: 1.7210548319538732 VALIDATION acc: 0.8144200626959248
2025-01-19 12:28:25,422 [INFO] [86] VALIDATION loss dict: {'classification_loss': 1.7210548319538732}
2025-01-19 12:28:25,423 [INFO] 
2025-01-19 12:28:45,342 [INFO] Step[50/2713]: training loss : 0.9388629281520844 TRAIN  loss dict:  {'classification_loss': 0.9388629281520844}
2025-01-19 12:29:00,382 [INFO] Step[100/2713]: training loss : 0.9391227054595948 TRAIN  loss dict:  {'classification_loss': 0.9391227054595948}
2025-01-19 12:29:15,503 [INFO] Step[150/2713]: training loss : 0.9368819403648376 TRAIN  loss dict:  {'classification_loss': 0.9368819403648376}
2025-01-19 12:29:30,591 [INFO] Step[200/2713]: training loss : 0.9364376556873322 TRAIN  loss dict:  {'classification_loss': 0.9364376556873322}
2025-01-19 12:29:45,666 [INFO] Step[250/2713]: training loss : 0.9397245287895203 TRAIN  loss dict:  {'classification_loss': 0.9397245287895203}
2025-01-19 12:30:00,781 [INFO] Step[300/2713]: training loss : 0.9379621171951293 TRAIN  loss dict:  {'classification_loss': 0.9379621171951293}
2025-01-19 12:30:15,903 [INFO] Step[350/2713]: training loss : 0.9370414459705353 TRAIN  loss dict:  {'classification_loss': 0.9370414459705353}
2025-01-19 12:30:30,964 [INFO] Step[400/2713]: training loss : 0.9394743824005127 TRAIN  loss dict:  {'classification_loss': 0.9394743824005127}
2025-01-19 12:30:46,032 [INFO] Step[450/2713]: training loss : 0.9362966763973236 TRAIN  loss dict:  {'classification_loss': 0.9362966763973236}
2025-01-19 12:31:01,121 [INFO] Step[500/2713]: training loss : 0.9382560503482819 TRAIN  loss dict:  {'classification_loss': 0.9382560503482819}
2025-01-19 12:31:16,193 [INFO] Step[550/2713]: training loss : 0.946903463602066 TRAIN  loss dict:  {'classification_loss': 0.946903463602066}
2025-01-19 12:31:31,267 [INFO] Step[600/2713]: training loss : 0.9366692829132081 TRAIN  loss dict:  {'classification_loss': 0.9366692829132081}
2025-01-19 12:31:46,327 [INFO] Step[650/2713]: training loss : 0.9426801860332489 TRAIN  loss dict:  {'classification_loss': 0.9426801860332489}
2025-01-19 12:32:01,420 [INFO] Step[700/2713]: training loss : 0.9369409334659576 TRAIN  loss dict:  {'classification_loss': 0.9369409334659576}
2025-01-19 12:32:16,487 [INFO] Step[750/2713]: training loss : 0.9379462027549743 TRAIN  loss dict:  {'classification_loss': 0.9379462027549743}
2025-01-19 12:32:31,559 [INFO] Step[800/2713]: training loss : 0.9381140327453613 TRAIN  loss dict:  {'classification_loss': 0.9381140327453613}
2025-01-19 12:32:46,661 [INFO] Step[850/2713]: training loss : 0.9396533417701721 TRAIN  loss dict:  {'classification_loss': 0.9396533417701721}
2025-01-19 12:33:01,734 [INFO] Step[900/2713]: training loss : 0.9364725804328918 TRAIN  loss dict:  {'classification_loss': 0.9364725804328918}
2025-01-19 12:33:15,316 [INFO] Step[950/2713]: training loss : 0.9387189018726348 TRAIN  loss dict:  {'classification_loss': 0.9387189018726348}
2025-01-19 12:33:26,797 [INFO] Step[1000/2713]: training loss : 0.9354599142074584 TRAIN  loss dict:  {'classification_loss': 0.9354599142074584}
2025-01-19 12:33:38,304 [INFO] Step[1050/2713]: training loss : 0.9374569284915925 TRAIN  loss dict:  {'classification_loss': 0.9374569284915925}
2025-01-19 12:33:49,784 [INFO] Step[1100/2713]: training loss : 0.9358348906040191 TRAIN  loss dict:  {'classification_loss': 0.9358348906040191}
2025-01-19 12:34:01,242 [INFO] Step[1150/2713]: training loss : 0.9361496698856354 TRAIN  loss dict:  {'classification_loss': 0.9361496698856354}
2025-01-19 12:34:12,739 [INFO] Step[1200/2713]: training loss : 0.9390249991416931 TRAIN  loss dict:  {'classification_loss': 0.9390249991416931}
2025-01-19 12:34:24,189 [INFO] Step[1250/2713]: training loss : 0.9406550240516662 TRAIN  loss dict:  {'classification_loss': 0.9406550240516662}
2025-01-19 12:34:35,655 [INFO] Step[1300/2713]: training loss : 0.9391877925395966 TRAIN  loss dict:  {'classification_loss': 0.9391877925395966}
2025-01-19 12:34:47,149 [INFO] Step[1350/2713]: training loss : 0.937564697265625 TRAIN  loss dict:  {'classification_loss': 0.937564697265625}
2025-01-19 12:34:58,620 [INFO] Step[1400/2713]: training loss : 0.9378213107585907 TRAIN  loss dict:  {'classification_loss': 0.9378213107585907}
2025-01-19 12:35:10,095 [INFO] Step[1450/2713]: training loss : 0.942124695777893 TRAIN  loss dict:  {'classification_loss': 0.942124695777893}
2025-01-19 12:35:21,533 [INFO] Step[1500/2713]: training loss : 0.9379681384563446 TRAIN  loss dict:  {'classification_loss': 0.9379681384563446}
2025-01-19 12:35:33,003 [INFO] Step[1550/2713]: training loss : 0.9385139167308807 TRAIN  loss dict:  {'classification_loss': 0.9385139167308807}
2025-01-19 12:35:44,454 [INFO] Step[1600/2713]: training loss : 0.9440209925174713 TRAIN  loss dict:  {'classification_loss': 0.9440209925174713}
2025-01-19 12:35:55,856 [INFO] Step[1650/2713]: training loss : 0.9376238775253296 TRAIN  loss dict:  {'classification_loss': 0.9376238775253296}
2025-01-19 12:36:07,340 [INFO] Step[1700/2713]: training loss : 0.9381584227085114 TRAIN  loss dict:  {'classification_loss': 0.9381584227085114}
2025-01-19 12:36:18,782 [INFO] Step[1750/2713]: training loss : 0.9380494463443756 TRAIN  loss dict:  {'classification_loss': 0.9380494463443756}
2025-01-19 12:36:30,214 [INFO] Step[1800/2713]: training loss : 0.9369393420219422 TRAIN  loss dict:  {'classification_loss': 0.9369393420219422}
2025-01-19 12:36:41,690 [INFO] Step[1850/2713]: training loss : 0.9391092610359192 TRAIN  loss dict:  {'classification_loss': 0.9391092610359192}
2025-01-19 12:36:53,104 [INFO] Step[1900/2713]: training loss : 0.9388340318202972 TRAIN  loss dict:  {'classification_loss': 0.9388340318202972}
2025-01-19 12:37:04,586 [INFO] Step[1950/2713]: training loss : 0.9390703785419464 TRAIN  loss dict:  {'classification_loss': 0.9390703785419464}
2025-01-19 12:37:16,039 [INFO] Step[2000/2713]: training loss : 0.9380840730667114 TRAIN  loss dict:  {'classification_loss': 0.9380840730667114}
2025-01-19 12:37:27,483 [INFO] Step[2050/2713]: training loss : 0.9382921171188354 TRAIN  loss dict:  {'classification_loss': 0.9382921171188354}
2025-01-19 12:37:38,948 [INFO] Step[2100/2713]: training loss : 0.9543986415863037 TRAIN  loss dict:  {'classification_loss': 0.9543986415863037}
2025-01-19 12:37:50,379 [INFO] Step[2150/2713]: training loss : 0.9397449254989624 TRAIN  loss dict:  {'classification_loss': 0.9397449254989624}
2025-01-19 12:38:01,867 [INFO] Step[2200/2713]: training loss : 0.941299340724945 TRAIN  loss dict:  {'classification_loss': 0.941299340724945}
2025-01-19 12:38:13,334 [INFO] Step[2250/2713]: training loss : 0.9380018234252929 TRAIN  loss dict:  {'classification_loss': 0.9380018234252929}
2025-01-19 12:38:24,814 [INFO] Step[2300/2713]: training loss : 0.9481769621372222 TRAIN  loss dict:  {'classification_loss': 0.9481769621372222}
2025-01-19 12:38:36,254 [INFO] Step[2350/2713]: training loss : 0.9461708509922028 TRAIN  loss dict:  {'classification_loss': 0.9461708509922028}
2025-01-19 12:38:47,709 [INFO] Step[2400/2713]: training loss : 0.9373325216770172 TRAIN  loss dict:  {'classification_loss': 0.9373325216770172}
2025-01-19 12:38:59,170 [INFO] Step[2450/2713]: training loss : 0.9378008127212525 TRAIN  loss dict:  {'classification_loss': 0.9378008127212525}
2025-01-19 12:39:10,649 [INFO] Step[2500/2713]: training loss : 0.9392277753353119 TRAIN  loss dict:  {'classification_loss': 0.9392277753353119}
2025-01-19 12:39:22,135 [INFO] Step[2550/2713]: training loss : 0.9393145787715912 TRAIN  loss dict:  {'classification_loss': 0.9393145787715912}
2025-01-19 12:39:33,623 [INFO] Step[2600/2713]: training loss : 0.937576893568039 TRAIN  loss dict:  {'classification_loss': 0.937576893568039}
2025-01-19 12:39:45,077 [INFO] Step[2650/2713]: training loss : 0.935815315246582 TRAIN  loss dict:  {'classification_loss': 0.935815315246582}
2025-01-19 12:39:56,492 [INFO] Step[2700/2713]: training loss : 0.9382804799079895 TRAIN  loss dict:  {'classification_loss': 0.9382804799079895}
2025-01-19 12:41:02,262 [INFO] Label accuracies statistics:
2025-01-19 12:41:02,262 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.0, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 12:41:02,264 [INFO] [87] TRAIN  loss: 0.9391199278295413 acc: 0.9995085391325715
2025-01-19 12:41:02,264 [INFO] [87] TRAIN  loss dict: {'classification_loss': 0.9391199278295413}
2025-01-19 12:41:02,264 [INFO] [87] VALIDATION loss: 1.7023726786885942 VALIDATION acc: 0.8175548589341692
2025-01-19 12:41:02,264 [INFO] [87] VALIDATION loss dict: {'classification_loss': 1.7023726786885942}
2025-01-19 12:41:02,264 [INFO] 
2025-01-19 12:41:18,662 [INFO] Step[50/2713]: training loss : 0.9362781918048859 TRAIN  loss dict:  {'classification_loss': 0.9362781918048859}
2025-01-19 12:41:30,178 [INFO] Step[100/2713]: training loss : 0.9369781970977783 TRAIN  loss dict:  {'classification_loss': 0.9369781970977783}
2025-01-19 12:41:41,641 [INFO] Step[150/2713]: training loss : 0.9372726142406463 TRAIN  loss dict:  {'classification_loss': 0.9372726142406463}
2025-01-19 12:41:53,117 [INFO] Step[200/2713]: training loss : 0.9381842994689942 TRAIN  loss dict:  {'classification_loss': 0.9381842994689942}
2025-01-19 12:42:04,633 [INFO] Step[250/2713]: training loss : 0.9376660311222076 TRAIN  loss dict:  {'classification_loss': 0.9376660311222076}
2025-01-19 12:42:16,115 [INFO] Step[300/2713]: training loss : 0.9389246594905853 TRAIN  loss dict:  {'classification_loss': 0.9389246594905853}
2025-01-19 12:42:27,653 [INFO] Step[350/2713]: training loss : 0.937340134382248 TRAIN  loss dict:  {'classification_loss': 0.937340134382248}
2025-01-19 12:42:39,158 [INFO] Step[400/2713]: training loss : 0.9415660881996155 TRAIN  loss dict:  {'classification_loss': 0.9415660881996155}
2025-01-19 12:42:50,652 [INFO] Step[450/2713]: training loss : 0.9367466986179351 TRAIN  loss dict:  {'classification_loss': 0.9367466986179351}
2025-01-19 12:43:02,140 [INFO] Step[500/2713]: training loss : 0.9371990942955017 TRAIN  loss dict:  {'classification_loss': 0.9371990942955017}
2025-01-19 12:43:13,637 [INFO] Step[550/2713]: training loss : 0.9404369664192199 TRAIN  loss dict:  {'classification_loss': 0.9404369664192199}
2025-01-19 12:43:25,110 [INFO] Step[600/2713]: training loss : 0.9365461933612823 TRAIN  loss dict:  {'classification_loss': 0.9365461933612823}
2025-01-19 12:43:36,624 [INFO] Step[650/2713]: training loss : 0.9371199905872345 TRAIN  loss dict:  {'classification_loss': 0.9371199905872345}
2025-01-19 12:43:48,139 [INFO] Step[700/2713]: training loss : 0.951324006319046 TRAIN  loss dict:  {'classification_loss': 0.951324006319046}
2025-01-19 12:43:59,662 [INFO] Step[750/2713]: training loss : 0.9367095041275024 TRAIN  loss dict:  {'classification_loss': 0.9367095041275024}
2025-01-19 12:44:11,141 [INFO] Step[800/2713]: training loss : 0.9393385875225068 TRAIN  loss dict:  {'classification_loss': 0.9393385875225068}
2025-01-19 12:44:22,621 [INFO] Step[850/2713]: training loss : 0.9407498347759247 TRAIN  loss dict:  {'classification_loss': 0.9407498347759247}
2025-01-19 12:44:34,120 [INFO] Step[900/2713]: training loss : 0.9370504832267761 TRAIN  loss dict:  {'classification_loss': 0.9370504832267761}
2025-01-19 12:44:45,598 [INFO] Step[950/2713]: training loss : 0.9369681048393249 TRAIN  loss dict:  {'classification_loss': 0.9369681048393249}
2025-01-19 12:44:57,102 [INFO] Step[1000/2713]: training loss : 0.9493920207023621 TRAIN  loss dict:  {'classification_loss': 0.9493920207023621}
2025-01-19 12:45:08,584 [INFO] Step[1050/2713]: training loss : 0.9519913840293884 TRAIN  loss dict:  {'classification_loss': 0.9519913840293884}
2025-01-19 12:45:20,034 [INFO] Step[1100/2713]: training loss : 0.9366688501834869 TRAIN  loss dict:  {'classification_loss': 0.9366688501834869}
2025-01-19 12:45:31,547 [INFO] Step[1150/2713]: training loss : 0.9371907758712769 TRAIN  loss dict:  {'classification_loss': 0.9371907758712769}
2025-01-19 12:45:43,021 [INFO] Step[1200/2713]: training loss : 0.9373704993724823 TRAIN  loss dict:  {'classification_loss': 0.9373704993724823}
2025-01-19 12:45:54,535 [INFO] Step[1250/2713]: training loss : 0.9371613097190857 TRAIN  loss dict:  {'classification_loss': 0.9371613097190857}
2025-01-19 12:46:06,056 [INFO] Step[1300/2713]: training loss : 0.9389722514152526 TRAIN  loss dict:  {'classification_loss': 0.9389722514152526}
2025-01-19 12:46:17,550 [INFO] Step[1350/2713]: training loss : 0.9361783707141876 TRAIN  loss dict:  {'classification_loss': 0.9361783707141876}
2025-01-19 12:46:29,048 [INFO] Step[1400/2713]: training loss : 0.9372278892993927 TRAIN  loss dict:  {'classification_loss': 0.9372278892993927}
2025-01-19 12:46:40,564 [INFO] Step[1450/2713]: training loss : 0.9381889867782592 TRAIN  loss dict:  {'classification_loss': 0.9381889867782592}
2025-01-19 12:46:52,057 [INFO] Step[1500/2713]: training loss : 0.9369612467288971 TRAIN  loss dict:  {'classification_loss': 0.9369612467288971}
2025-01-19 12:47:03,553 [INFO] Step[1550/2713]: training loss : 0.937818193435669 TRAIN  loss dict:  {'classification_loss': 0.937818193435669}
2025-01-19 12:47:15,062 [INFO] Step[1600/2713]: training loss : 0.9396465969085693 TRAIN  loss dict:  {'classification_loss': 0.9396465969085693}
2025-01-19 12:47:26,562 [INFO] Step[1650/2713]: training loss : 0.9379978215694428 TRAIN  loss dict:  {'classification_loss': 0.9379978215694428}
2025-01-19 12:47:38,047 [INFO] Step[1700/2713]: training loss : 0.9380740678310394 TRAIN  loss dict:  {'classification_loss': 0.9380740678310394}
2025-01-19 12:47:49,531 [INFO] Step[1750/2713]: training loss : 0.9362162399291992 TRAIN  loss dict:  {'classification_loss': 0.9362162399291992}
2025-01-19 12:48:01,034 [INFO] Step[1800/2713]: training loss : 0.9391058373451233 TRAIN  loss dict:  {'classification_loss': 0.9391058373451233}
2025-01-19 12:48:12,536 [INFO] Step[1850/2713]: training loss : 0.9377417528629303 TRAIN  loss dict:  {'classification_loss': 0.9377417528629303}
2025-01-19 12:48:24,018 [INFO] Step[1900/2713]: training loss : 0.9368915522098541 TRAIN  loss dict:  {'classification_loss': 0.9368915522098541}
2025-01-19 12:48:35,496 [INFO] Step[1950/2713]: training loss : 0.9366695868968964 TRAIN  loss dict:  {'classification_loss': 0.9366695868968964}
2025-01-19 12:48:46,990 [INFO] Step[2000/2713]: training loss : 0.9364920318126678 TRAIN  loss dict:  {'classification_loss': 0.9364920318126678}
2025-01-19 12:48:58,465 [INFO] Step[2050/2713]: training loss : 0.9386308240890503 TRAIN  loss dict:  {'classification_loss': 0.9386308240890503}
2025-01-19 12:49:09,917 [INFO] Step[2100/2713]: training loss : 0.9374505758285523 TRAIN  loss dict:  {'classification_loss': 0.9374505758285523}
2025-01-19 12:49:21,398 [INFO] Step[2150/2713]: training loss : 0.9418715703487396 TRAIN  loss dict:  {'classification_loss': 0.9418715703487396}
2025-01-19 12:49:32,918 [INFO] Step[2200/2713]: training loss : 0.9388668942451477 TRAIN  loss dict:  {'classification_loss': 0.9388668942451477}
2025-01-19 12:49:44,427 [INFO] Step[2250/2713]: training loss : 0.9385448706150055 TRAIN  loss dict:  {'classification_loss': 0.9385448706150055}
2025-01-19 12:49:55,913 [INFO] Step[2300/2713]: training loss : 0.9360098993778229 TRAIN  loss dict:  {'classification_loss': 0.9360098993778229}
2025-01-19 12:50:07,346 [INFO] Step[2350/2713]: training loss : 0.9377303755283356 TRAIN  loss dict:  {'classification_loss': 0.9377303755283356}
2025-01-19 12:50:18,779 [INFO] Step[2400/2713]: training loss : 0.9383903396129608 TRAIN  loss dict:  {'classification_loss': 0.9383903396129608}
2025-01-19 12:50:30,218 [INFO] Step[2450/2713]: training loss : 0.9383366942405701 TRAIN  loss dict:  {'classification_loss': 0.9383366942405701}
2025-01-19 12:50:41,749 [INFO] Step[2500/2713]: training loss : 0.9385008919239044 TRAIN  loss dict:  {'classification_loss': 0.9385008919239044}
2025-01-19 12:50:53,225 [INFO] Step[2550/2713]: training loss : 0.9407891571521759 TRAIN  loss dict:  {'classification_loss': 0.9407891571521759}
2025-01-19 12:51:04,671 [INFO] Step[2600/2713]: training loss : 0.936590176820755 TRAIN  loss dict:  {'classification_loss': 0.936590176820755}
2025-01-19 12:51:16,211 [INFO] Step[2650/2713]: training loss : 0.9364037632942199 TRAIN  loss dict:  {'classification_loss': 0.9364037632942199}
2025-01-19 12:51:27,703 [INFO] Step[2700/2713]: training loss : 0.9365536439418792 TRAIN  loss dict:  {'classification_loss': 0.9365536439418792}
2025-01-19 12:52:33,258 [INFO] Label accuracies statistics:
2025-01-19 12:52:33,258 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.75, 204: 1.0, 205: 1.0, 206: 0.5, 207: 1.0, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 12:52:33,260 [INFO] [88] TRAIN  loss: 0.9385676109127308 acc: 0.9996314043494287
2025-01-19 12:52:33,260 [INFO] [88] TRAIN  loss dict: {'classification_loss': 0.9385676109127308}
2025-01-19 12:52:33,260 [INFO] [88] VALIDATION loss: 1.7324159924024927 VALIDATION acc: 0.8150470219435737
2025-01-19 12:52:33,260 [INFO] [88] VALIDATION loss dict: {'classification_loss': 1.7324159924024927}
2025-01-19 12:52:33,260 [INFO] 
2025-01-19 12:52:49,796 [INFO] Step[50/2713]: training loss : 0.9422653090953826 TRAIN  loss dict:  {'classification_loss': 0.9422653090953826}
2025-01-19 12:53:01,317 [INFO] Step[100/2713]: training loss : 0.9360113024711609 TRAIN  loss dict:  {'classification_loss': 0.9360113024711609}
2025-01-19 12:53:12,812 [INFO] Step[150/2713]: training loss : 0.9385651552677154 TRAIN  loss dict:  {'classification_loss': 0.9385651552677154}
2025-01-19 12:53:24,249 [INFO] Step[200/2713]: training loss : 0.9372371685504913 TRAIN  loss dict:  {'classification_loss': 0.9372371685504913}
2025-01-19 12:53:35,728 [INFO] Step[250/2713]: training loss : 0.9376367998123168 TRAIN  loss dict:  {'classification_loss': 0.9376367998123168}
2025-01-19 12:53:47,246 [INFO] Step[300/2713]: training loss : 0.936641639471054 TRAIN  loss dict:  {'classification_loss': 0.936641639471054}
2025-01-19 12:53:58,756 [INFO] Step[350/2713]: training loss : 0.9392218542098999 TRAIN  loss dict:  {'classification_loss': 0.9392218542098999}
2025-01-19 12:54:10,256 [INFO] Step[400/2713]: training loss : 0.9368403160572052 TRAIN  loss dict:  {'classification_loss': 0.9368403160572052}
2025-01-19 12:54:21,774 [INFO] Step[450/2713]: training loss : 0.9361845934391022 TRAIN  loss dict:  {'classification_loss': 0.9361845934391022}
2025-01-19 12:54:33,263 [INFO] Step[500/2713]: training loss : 0.9377295315265656 TRAIN  loss dict:  {'classification_loss': 0.9377295315265656}
2025-01-19 12:54:44,797 [INFO] Step[550/2713]: training loss : 0.9368399906158448 TRAIN  loss dict:  {'classification_loss': 0.9368399906158448}
2025-01-19 12:54:56,233 [INFO] Step[600/2713]: training loss : 0.9378034925460815 TRAIN  loss dict:  {'classification_loss': 0.9378034925460815}
2025-01-19 12:55:07,728 [INFO] Step[650/2713]: training loss : 0.9383117437362671 TRAIN  loss dict:  {'classification_loss': 0.9383117437362671}
2025-01-19 12:55:19,213 [INFO] Step[700/2713]: training loss : 0.9380536389350891 TRAIN  loss dict:  {'classification_loss': 0.9380536389350891}
2025-01-19 12:55:30,716 [INFO] Step[750/2713]: training loss : 0.9409995007514954 TRAIN  loss dict:  {'classification_loss': 0.9409995007514954}
2025-01-19 12:55:42,200 [INFO] Step[800/2713]: training loss : 0.9362799215316773 TRAIN  loss dict:  {'classification_loss': 0.9362799215316773}
2025-01-19 12:55:53,659 [INFO] Step[850/2713]: training loss : 0.9391605830192566 TRAIN  loss dict:  {'classification_loss': 0.9391605830192566}
2025-01-19 12:56:05,125 [INFO] Step[900/2713]: training loss : 0.9483214485645294 TRAIN  loss dict:  {'classification_loss': 0.9483214485645294}
2025-01-19 12:56:16,610 [INFO] Step[950/2713]: training loss : 0.9426669764518738 TRAIN  loss dict:  {'classification_loss': 0.9426669764518738}
2025-01-19 12:56:28,085 [INFO] Step[1000/2713]: training loss : 0.935096333026886 TRAIN  loss dict:  {'classification_loss': 0.935096333026886}
2025-01-19 12:56:39,578 [INFO] Step[1050/2713]: training loss : 0.9389893412590027 TRAIN  loss dict:  {'classification_loss': 0.9389893412590027}
2025-01-19 12:56:51,065 [INFO] Step[1100/2713]: training loss : 0.9355083417892456 TRAIN  loss dict:  {'classification_loss': 0.9355083417892456}
2025-01-19 12:57:02,547 [INFO] Step[1150/2713]: training loss : 0.936533192396164 TRAIN  loss dict:  {'classification_loss': 0.936533192396164}
2025-01-19 12:57:14,053 [INFO] Step[1200/2713]: training loss : 0.9369128715991973 TRAIN  loss dict:  {'classification_loss': 0.9369128715991973}
2025-01-19 12:57:25,569 [INFO] Step[1250/2713]: training loss : 0.9373915040493012 TRAIN  loss dict:  {'classification_loss': 0.9373915040493012}
2025-01-19 12:57:37,044 [INFO] Step[1300/2713]: training loss : 0.9370652937889099 TRAIN  loss dict:  {'classification_loss': 0.9370652937889099}
2025-01-19 12:57:48,569 [INFO] Step[1350/2713]: training loss : 0.9376429438591003 TRAIN  loss dict:  {'classification_loss': 0.9376429438591003}
2025-01-19 12:58:00,059 [INFO] Step[1400/2713]: training loss : 0.9358939003944396 TRAIN  loss dict:  {'classification_loss': 0.9358939003944396}
2025-01-19 12:58:11,534 [INFO] Step[1450/2713]: training loss : 0.9360831725597382 TRAIN  loss dict:  {'classification_loss': 0.9360831725597382}
2025-01-19 12:58:23,048 [INFO] Step[1500/2713]: training loss : 0.938416200876236 TRAIN  loss dict:  {'classification_loss': 0.938416200876236}
2025-01-19 12:58:34,574 [INFO] Step[1550/2713]: training loss : 0.9393725454807281 TRAIN  loss dict:  {'classification_loss': 0.9393725454807281}
2025-01-19 12:58:46,048 [INFO] Step[1600/2713]: training loss : 0.9403152227401733 TRAIN  loss dict:  {'classification_loss': 0.9403152227401733}
2025-01-19 12:58:57,579 [INFO] Step[1650/2713]: training loss : 0.9388284420967102 TRAIN  loss dict:  {'classification_loss': 0.9388284420967102}
2025-01-19 12:59:09,106 [INFO] Step[1700/2713]: training loss : 0.9372203361988067 TRAIN  loss dict:  {'classification_loss': 0.9372203361988067}
2025-01-19 12:59:20,594 [INFO] Step[1750/2713]: training loss : 0.9358219969272613 TRAIN  loss dict:  {'classification_loss': 0.9358219969272613}
2025-01-19 12:59:32,053 [INFO] Step[1800/2713]: training loss : 0.9604443657398224 TRAIN  loss dict:  {'classification_loss': 0.9604443657398224}
2025-01-19 12:59:43,538 [INFO] Step[1850/2713]: training loss : 0.9371828198432922 TRAIN  loss dict:  {'classification_loss': 0.9371828198432922}
2025-01-19 12:59:55,029 [INFO] Step[1900/2713]: training loss : 0.9374403536319733 TRAIN  loss dict:  {'classification_loss': 0.9374403536319733}
2025-01-19 13:00:06,539 [INFO] Step[1950/2713]: training loss : 0.9375279152393341 TRAIN  loss dict:  {'classification_loss': 0.9375279152393341}
2025-01-19 13:00:17,998 [INFO] Step[2000/2713]: training loss : 0.9372343993186951 TRAIN  loss dict:  {'classification_loss': 0.9372343993186951}
2025-01-19 13:00:29,468 [INFO] Step[2050/2713]: training loss : 0.9364463984966278 TRAIN  loss dict:  {'classification_loss': 0.9364463984966278}
2025-01-19 13:00:40,972 [INFO] Step[2100/2713]: training loss : 0.9384023976325989 TRAIN  loss dict:  {'classification_loss': 0.9384023976325989}
2025-01-19 13:00:52,470 [INFO] Step[2150/2713]: training loss : 0.9366739583015442 TRAIN  loss dict:  {'classification_loss': 0.9366739583015442}
2025-01-19 13:01:03,936 [INFO] Step[2200/2713]: training loss : 0.9385356092453003 TRAIN  loss dict:  {'classification_loss': 0.9385356092453003}
2025-01-19 13:01:15,414 [INFO] Step[2250/2713]: training loss : 0.93743532538414 TRAIN  loss dict:  {'classification_loss': 0.93743532538414}
2025-01-19 13:01:26,909 [INFO] Step[2300/2713]: training loss : 0.9372736883163452 TRAIN  loss dict:  {'classification_loss': 0.9372736883163452}
2025-01-19 13:01:38,392 [INFO] Step[2350/2713]: training loss : 0.937940479516983 TRAIN  loss dict:  {'classification_loss': 0.937940479516983}
2025-01-19 13:01:49,858 [INFO] Step[2400/2713]: training loss : 0.9376135396957398 TRAIN  loss dict:  {'classification_loss': 0.9376135396957398}
2025-01-19 13:02:01,330 [INFO] Step[2450/2713]: training loss : 0.9369632923603057 TRAIN  loss dict:  {'classification_loss': 0.9369632923603057}
2025-01-19 13:02:12,815 [INFO] Step[2500/2713]: training loss : 0.9388775062561036 TRAIN  loss dict:  {'classification_loss': 0.9388775062561036}
2025-01-19 13:02:24,321 [INFO] Step[2550/2713]: training loss : 0.9364115452766418 TRAIN  loss dict:  {'classification_loss': 0.9364115452766418}
2025-01-19 13:02:35,839 [INFO] Step[2600/2713]: training loss : 0.937180655002594 TRAIN  loss dict:  {'classification_loss': 0.937180655002594}
2025-01-19 13:02:47,335 [INFO] Step[2650/2713]: training loss : 0.9365917587280274 TRAIN  loss dict:  {'classification_loss': 0.9365917587280274}
2025-01-19 13:02:58,836 [INFO] Step[2700/2713]: training loss : 0.9394586980342865 TRAIN  loss dict:  {'classification_loss': 0.9394586980342865}
2025-01-19 13:04:04,860 [INFO] Label accuracies statistics:
2025-01-19 13:04:04,860 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 1.0, 378: 1.0, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 13:04:04,862 [INFO] [89] TRAIN  loss: 0.9383277905809013 acc: 0.9997542695662858
2025-01-19 13:04:04,862 [INFO] [89] TRAIN  loss dict: {'classification_loss': 0.9383277905809013}
2025-01-19 13:04:04,862 [INFO] [89] VALIDATION loss: 1.7145745587304122 VALIDATION acc: 0.8144200626959248
2025-01-19 13:04:04,862 [INFO] [89] VALIDATION loss dict: {'classification_loss': 1.7145745587304122}
2025-01-19 13:04:04,862 [INFO] 
2025-01-19 13:04:21,359 [INFO] Step[50/2713]: training loss : 0.939214881658554 TRAIN  loss dict:  {'classification_loss': 0.939214881658554}
2025-01-19 13:04:32,839 [INFO] Step[100/2713]: training loss : 0.9368380331993102 TRAIN  loss dict:  {'classification_loss': 0.9368380331993102}
2025-01-19 13:04:44,360 [INFO] Step[150/2713]: training loss : 0.9353080761432647 TRAIN  loss dict:  {'classification_loss': 0.9353080761432647}
2025-01-19 13:04:55,854 [INFO] Step[200/2713]: training loss : 0.9387595844268799 TRAIN  loss dict:  {'classification_loss': 0.9387595844268799}
2025-01-19 13:05:07,368 [INFO] Step[250/2713]: training loss : 0.9373898327350616 TRAIN  loss dict:  {'classification_loss': 0.9373898327350616}
2025-01-19 13:05:18,870 [INFO] Step[300/2713]: training loss : 0.9381672334671021 TRAIN  loss dict:  {'classification_loss': 0.9381672334671021}
2025-01-19 13:05:30,343 [INFO] Step[350/2713]: training loss : 0.9376265025138855 TRAIN  loss dict:  {'classification_loss': 0.9376265025138855}
2025-01-19 13:05:41,867 [INFO] Step[400/2713]: training loss : 0.9374790275096894 TRAIN  loss dict:  {'classification_loss': 0.9374790275096894}
2025-01-19 13:05:53,374 [INFO] Step[450/2713]: training loss : 0.9361376595497132 TRAIN  loss dict:  {'classification_loss': 0.9361376595497132}
2025-01-19 13:06:04,852 [INFO] Step[500/2713]: training loss : 0.9370341110229492 TRAIN  loss dict:  {'classification_loss': 0.9370341110229492}
2025-01-19 13:06:16,340 [INFO] Step[550/2713]: training loss : 0.9363535940647125 TRAIN  loss dict:  {'classification_loss': 0.9363535940647125}
2025-01-19 13:06:27,814 [INFO] Step[600/2713]: training loss : 0.9381606757640839 TRAIN  loss dict:  {'classification_loss': 0.9381606757640839}
2025-01-19 13:06:39,273 [INFO] Step[650/2713]: training loss : 0.9370305526256562 TRAIN  loss dict:  {'classification_loss': 0.9370305526256562}
2025-01-19 13:06:50,744 [INFO] Step[700/2713]: training loss : 0.9376510167121888 TRAIN  loss dict:  {'classification_loss': 0.9376510167121888}
2025-01-19 13:07:02,261 [INFO] Step[750/2713]: training loss : 0.9380923211574554 TRAIN  loss dict:  {'classification_loss': 0.9380923211574554}
2025-01-19 13:07:13,758 [INFO] Step[800/2713]: training loss : 0.9403553438186646 TRAIN  loss dict:  {'classification_loss': 0.9403553438186646}
2025-01-19 13:07:25,268 [INFO] Step[850/2713]: training loss : 0.9366893768310547 TRAIN  loss dict:  {'classification_loss': 0.9366893768310547}
2025-01-19 13:07:36,763 [INFO] Step[900/2713]: training loss : 0.937032585144043 TRAIN  loss dict:  {'classification_loss': 0.937032585144043}
2025-01-19 13:07:48,274 [INFO] Step[950/2713]: training loss : 0.9405927991867066 TRAIN  loss dict:  {'classification_loss': 0.9405927991867066}
2025-01-19 13:07:59,796 [INFO] Step[1000/2713]: training loss : 0.9706365060806275 TRAIN  loss dict:  {'classification_loss': 0.9706365060806275}
2025-01-19 13:08:11,306 [INFO] Step[1050/2713]: training loss : 0.9371962440013886 TRAIN  loss dict:  {'classification_loss': 0.9371962440013886}
2025-01-19 13:08:22,811 [INFO] Step[1100/2713]: training loss : 0.9366602241992951 TRAIN  loss dict:  {'classification_loss': 0.9366602241992951}
2025-01-19 13:08:34,314 [INFO] Step[1150/2713]: training loss : 0.9375251007080078 TRAIN  loss dict:  {'classification_loss': 0.9375251007080078}
2025-01-19 13:08:45,814 [INFO] Step[1200/2713]: training loss : 0.936287716627121 TRAIN  loss dict:  {'classification_loss': 0.936287716627121}
2025-01-19 13:08:57,316 [INFO] Step[1250/2713]: training loss : 0.939200439453125 TRAIN  loss dict:  {'classification_loss': 0.939200439453125}
2025-01-19 13:09:08,841 [INFO] Step[1300/2713]: training loss : 0.9365295529365539 TRAIN  loss dict:  {'classification_loss': 0.9365295529365539}
2025-01-19 13:09:20,282 [INFO] Step[1350/2713]: training loss : 0.9383368229866028 TRAIN  loss dict:  {'classification_loss': 0.9383368229866028}
2025-01-19 13:09:31,765 [INFO] Step[1400/2713]: training loss : 0.9361098647117615 TRAIN  loss dict:  {'classification_loss': 0.9361098647117615}
2025-01-19 13:09:43,293 [INFO] Step[1450/2713]: training loss : 0.9363642454147338 TRAIN  loss dict:  {'classification_loss': 0.9363642454147338}
2025-01-19 13:09:54,774 [INFO] Step[1500/2713]: training loss : 0.9370883572101593 TRAIN  loss dict:  {'classification_loss': 0.9370883572101593}
2025-01-19 13:10:06,285 [INFO] Step[1550/2713]: training loss : 0.9370307946205139 TRAIN  loss dict:  {'classification_loss': 0.9370307946205139}
2025-01-19 13:10:17,779 [INFO] Step[1600/2713]: training loss : 0.9360490334033966 TRAIN  loss dict:  {'classification_loss': 0.9360490334033966}
2025-01-19 13:10:29,275 [INFO] Step[1650/2713]: training loss : 0.9349588644504547 TRAIN  loss dict:  {'classification_loss': 0.9349588644504547}
2025-01-19 13:10:40,810 [INFO] Step[1700/2713]: training loss : 0.9398567116260529 TRAIN  loss dict:  {'classification_loss': 0.9398567116260529}
2025-01-19 13:10:52,315 [INFO] Step[1750/2713]: training loss : 0.9382437610626221 TRAIN  loss dict:  {'classification_loss': 0.9382437610626221}
2025-01-19 13:11:03,808 [INFO] Step[1800/2713]: training loss : 0.9364196038246155 TRAIN  loss dict:  {'classification_loss': 0.9364196038246155}
2025-01-19 13:11:15,324 [INFO] Step[1850/2713]: training loss : 0.9360242283344269 TRAIN  loss dict:  {'classification_loss': 0.9360242283344269}
2025-01-19 13:11:26,795 [INFO] Step[1900/2713]: training loss : 0.9365617024898529 TRAIN  loss dict:  {'classification_loss': 0.9365617024898529}
2025-01-19 13:11:38,319 [INFO] Step[1950/2713]: training loss : 0.9371761023998261 TRAIN  loss dict:  {'classification_loss': 0.9371761023998261}
2025-01-19 13:11:49,843 [INFO] Step[2000/2713]: training loss : 0.9477654838562012 TRAIN  loss dict:  {'classification_loss': 0.9477654838562012}
2025-01-19 13:12:01,372 [INFO] Step[2050/2713]: training loss : 0.9396934449672699 TRAIN  loss dict:  {'classification_loss': 0.9396934449672699}
2025-01-19 13:12:12,890 [INFO] Step[2100/2713]: training loss : 0.9367682003974914 TRAIN  loss dict:  {'classification_loss': 0.9367682003974914}
2025-01-19 13:12:24,417 [INFO] Step[2150/2713]: training loss : 0.9385712492465973 TRAIN  loss dict:  {'classification_loss': 0.9385712492465973}
2025-01-19 13:12:35,928 [INFO] Step[2200/2713]: training loss : 0.9382197666168213 TRAIN  loss dict:  {'classification_loss': 0.9382197666168213}
2025-01-19 13:12:47,459 [INFO] Step[2250/2713]: training loss : 0.9417320716381073 TRAIN  loss dict:  {'classification_loss': 0.9417320716381073}
2025-01-19 13:12:58,968 [INFO] Step[2300/2713]: training loss : 0.9359678459167481 TRAIN  loss dict:  {'classification_loss': 0.9359678459167481}
2025-01-19 13:13:10,475 [INFO] Step[2350/2713]: training loss : 0.9392854785919189 TRAIN  loss dict:  {'classification_loss': 0.9392854785919189}
2025-01-19 13:13:21,961 [INFO] Step[2400/2713]: training loss : 0.9366393542289734 TRAIN  loss dict:  {'classification_loss': 0.9366393542289734}
2025-01-19 13:13:33,436 [INFO] Step[2450/2713]: training loss : 0.9387528038024903 TRAIN  loss dict:  {'classification_loss': 0.9387528038024903}
2025-01-19 13:13:44,912 [INFO] Step[2500/2713]: training loss : 0.939118617773056 TRAIN  loss dict:  {'classification_loss': 0.939118617773056}
2025-01-19 13:13:56,449 [INFO] Step[2550/2713]: training loss : 0.9371816539764404 TRAIN  loss dict:  {'classification_loss': 0.9371816539764404}
2025-01-19 13:14:07,950 [INFO] Step[2600/2713]: training loss : 0.9351325356960296 TRAIN  loss dict:  {'classification_loss': 0.9351325356960296}
2025-01-19 13:14:19,458 [INFO] Step[2650/2713]: training loss : 0.9391773092746735 TRAIN  loss dict:  {'classification_loss': 0.9391773092746735}
2025-01-19 13:14:30,974 [INFO] Step[2700/2713]: training loss : 0.9365990185737609 TRAIN  loss dict:  {'classification_loss': 0.9365990185737609}
2025-01-19 13:15:36,384 [INFO] Label accuracies statistics:
2025-01-19 13:15:36,385 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 1.0, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.75, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 1.0, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 13:15:36,386 [INFO] [90] TRAIN  loss: 0.9383480129890498 acc: 0.9997542695662858
2025-01-19 13:15:36,386 [INFO] [90] TRAIN  loss dict: {'classification_loss': 0.9383480129890498}
2025-01-19 13:15:36,386 [INFO] [90] VALIDATION loss: 1.7003350153677446 VALIDATION acc: 0.8163009404388715
2025-01-19 13:15:36,387 [INFO] [90] VALIDATION loss dict: {'classification_loss': 1.7003350153677446}
2025-01-19 13:15:36,387 [INFO] 
2025-01-19 13:15:52,486 [INFO] Step[50/2713]: training loss : 0.9372244346141815 TRAIN  loss dict:  {'classification_loss': 0.9372244346141815}
2025-01-19 13:16:03,948 [INFO] Step[100/2713]: training loss : 0.9379531741142273 TRAIN  loss dict:  {'classification_loss': 0.9379531741142273}
2025-01-19 13:16:15,434 [INFO] Step[150/2713]: training loss : 0.9353929543495179 TRAIN  loss dict:  {'classification_loss': 0.9353929543495179}
2025-01-19 13:16:26,949 [INFO] Step[200/2713]: training loss : 0.9577412855625153 TRAIN  loss dict:  {'classification_loss': 0.9577412855625153}
2025-01-19 13:16:38,497 [INFO] Step[250/2713]: training loss : 0.9347442388534546 TRAIN  loss dict:  {'classification_loss': 0.9347442388534546}
2025-01-19 13:16:50,002 [INFO] Step[300/2713]: training loss : 0.9364706873893738 TRAIN  loss dict:  {'classification_loss': 0.9364706873893738}
2025-01-19 13:17:01,509 [INFO] Step[350/2713]: training loss : 0.9359782814979554 TRAIN  loss dict:  {'classification_loss': 0.9359782814979554}
2025-01-19 13:17:12,982 [INFO] Step[400/2713]: training loss : 0.9420909380912781 TRAIN  loss dict:  {'classification_loss': 0.9420909380912781}
2025-01-19 13:17:24,408 [INFO] Step[450/2713]: training loss : 0.9382136690616608 TRAIN  loss dict:  {'classification_loss': 0.9382136690616608}
2025-01-19 13:17:35,866 [INFO] Step[500/2713]: training loss : 0.9376191747188568 TRAIN  loss dict:  {'classification_loss': 0.9376191747188568}
2025-01-19 13:17:47,314 [INFO] Step[550/2713]: training loss : 0.9375323581695557 TRAIN  loss dict:  {'classification_loss': 0.9375323581695557}
2025-01-19 13:17:58,735 [INFO] Step[600/2713]: training loss : 0.9359043633937836 TRAIN  loss dict:  {'classification_loss': 0.9359043633937836}
2025-01-19 13:18:10,198 [INFO] Step[650/2713]: training loss : 0.9397060310840607 TRAIN  loss dict:  {'classification_loss': 0.9397060310840607}
2025-01-19 13:18:21,690 [INFO] Step[700/2713]: training loss : 0.9360933113098144 TRAIN  loss dict:  {'classification_loss': 0.9360933113098144}
2025-01-19 13:18:33,185 [INFO] Step[750/2713]: training loss : 0.9358809924125672 TRAIN  loss dict:  {'classification_loss': 0.9358809924125672}
2025-01-19 13:18:44,655 [INFO] Step[800/2713]: training loss : 0.9380428457260132 TRAIN  loss dict:  {'classification_loss': 0.9380428457260132}
2025-01-19 13:18:56,121 [INFO] Step[850/2713]: training loss : 0.9355618882179261 TRAIN  loss dict:  {'classification_loss': 0.9355618882179261}
2025-01-19 13:19:07,570 [INFO] Step[900/2713]: training loss : 0.9399424958229065 TRAIN  loss dict:  {'classification_loss': 0.9399424958229065}
2025-01-19 13:19:19,092 [INFO] Step[950/2713]: training loss : 0.9375045287609101 TRAIN  loss dict:  {'classification_loss': 0.9375045287609101}
2025-01-19 13:19:30,593 [INFO] Step[1000/2713]: training loss : 0.9401368737220764 TRAIN  loss dict:  {'classification_loss': 0.9401368737220764}
2025-01-19 13:19:42,094 [INFO] Step[1050/2713]: training loss : 0.9370721328258514 TRAIN  loss dict:  {'classification_loss': 0.9370721328258514}
2025-01-19 13:19:53,584 [INFO] Step[1100/2713]: training loss : 0.9382496404647828 TRAIN  loss dict:  {'classification_loss': 0.9382496404647828}
2025-01-19 13:20:05,054 [INFO] Step[1150/2713]: training loss : 0.9405627417564392 TRAIN  loss dict:  {'classification_loss': 0.9405627417564392}
2025-01-19 13:20:16,532 [INFO] Step[1200/2713]: training loss : 0.941659175157547 TRAIN  loss dict:  {'classification_loss': 0.941659175157547}
2025-01-19 13:20:27,988 [INFO] Step[1250/2713]: training loss : 0.9354284548759461 TRAIN  loss dict:  {'classification_loss': 0.9354284548759461}
2025-01-19 13:20:39,460 [INFO] Step[1300/2713]: training loss : 0.9361015856266022 TRAIN  loss dict:  {'classification_loss': 0.9361015856266022}
2025-01-19 13:20:50,887 [INFO] Step[1350/2713]: training loss : 0.9376298892498016 TRAIN  loss dict:  {'classification_loss': 0.9376298892498016}
2025-01-19 13:21:02,369 [INFO] Step[1400/2713]: training loss : 0.9407786476612091 TRAIN  loss dict:  {'classification_loss': 0.9407786476612091}
2025-01-19 13:21:13,845 [INFO] Step[1450/2713]: training loss : 0.9342080581188202 TRAIN  loss dict:  {'classification_loss': 0.9342080581188202}
2025-01-19 13:21:25,341 [INFO] Step[1500/2713]: training loss : 0.935420343875885 TRAIN  loss dict:  {'classification_loss': 0.935420343875885}
2025-01-19 13:21:36,807 [INFO] Step[1550/2713]: training loss : 0.9372537672519684 TRAIN  loss dict:  {'classification_loss': 0.9372537672519684}
2025-01-19 13:21:48,323 [INFO] Step[1600/2713]: training loss : 0.9394282972812653 TRAIN  loss dict:  {'classification_loss': 0.9394282972812653}
2025-01-19 13:21:59,811 [INFO] Step[1650/2713]: training loss : 0.9355330896377564 TRAIN  loss dict:  {'classification_loss': 0.9355330896377564}
2025-01-19 13:22:11,310 [INFO] Step[1700/2713]: training loss : 0.9361445248126984 TRAIN  loss dict:  {'classification_loss': 0.9361445248126984}
2025-01-19 13:22:22,802 [INFO] Step[1750/2713]: training loss : 0.9365306973457337 TRAIN  loss dict:  {'classification_loss': 0.9365306973457337}
2025-01-19 13:22:34,271 [INFO] Step[1800/2713]: training loss : 0.9354598474502563 TRAIN  loss dict:  {'classification_loss': 0.9354598474502563}
2025-01-19 13:22:45,732 [INFO] Step[1850/2713]: training loss : 0.93624351978302 TRAIN  loss dict:  {'classification_loss': 0.93624351978302}
2025-01-19 13:22:57,213 [INFO] Step[1900/2713]: training loss : 0.9371774137020111 TRAIN  loss dict:  {'classification_loss': 0.9371774137020111}
2025-01-19 13:23:08,683 [INFO] Step[1950/2713]: training loss : 0.9360088765621185 TRAIN  loss dict:  {'classification_loss': 0.9360088765621185}
2025-01-19 13:23:20,177 [INFO] Step[2000/2713]: training loss : 0.9366157901287079 TRAIN  loss dict:  {'classification_loss': 0.9366157901287079}
2025-01-19 13:23:31,639 [INFO] Step[2050/2713]: training loss : 0.9353742742538452 TRAIN  loss dict:  {'classification_loss': 0.9353742742538452}
2025-01-19 13:23:43,067 [INFO] Step[2100/2713]: training loss : 0.9395955288410187 TRAIN  loss dict:  {'classification_loss': 0.9395955288410187}
2025-01-19 13:23:54,524 [INFO] Step[2150/2713]: training loss : 0.9366988849639892 TRAIN  loss dict:  {'classification_loss': 0.9366988849639892}
2025-01-19 13:24:05,993 [INFO] Step[2200/2713]: training loss : 0.9364026951789856 TRAIN  loss dict:  {'classification_loss': 0.9364026951789856}
2025-01-19 13:24:17,470 [INFO] Step[2250/2713]: training loss : 0.9352873587608337 TRAIN  loss dict:  {'classification_loss': 0.9352873587608337}
2025-01-19 13:24:28,900 [INFO] Step[2300/2713]: training loss : 0.935786247253418 TRAIN  loss dict:  {'classification_loss': 0.935786247253418}
2025-01-19 13:24:40,346 [INFO] Step[2350/2713]: training loss : 0.9353146290779114 TRAIN  loss dict:  {'classification_loss': 0.9353146290779114}
2025-01-19 13:24:51,784 [INFO] Step[2400/2713]: training loss : 0.9363480520248413 TRAIN  loss dict:  {'classification_loss': 0.9363480520248413}
2025-01-19 13:25:03,263 [INFO] Step[2450/2713]: training loss : 0.9354667496681214 TRAIN  loss dict:  {'classification_loss': 0.9354667496681214}
2025-01-19 13:25:14,723 [INFO] Step[2500/2713]: training loss : 0.9370912230014801 TRAIN  loss dict:  {'classification_loss': 0.9370912230014801}
2025-01-19 13:25:26,238 [INFO] Step[2550/2713]: training loss : 0.9366736793518067 TRAIN  loss dict:  {'classification_loss': 0.9366736793518067}
2025-01-19 13:25:37,696 [INFO] Step[2600/2713]: training loss : 0.9400631761550904 TRAIN  loss dict:  {'classification_loss': 0.9400631761550904}
2025-01-19 13:25:49,186 [INFO] Step[2650/2713]: training loss : 0.937483378648758 TRAIN  loss dict:  {'classification_loss': 0.937483378648758}
2025-01-19 13:26:00,666 [INFO] Step[2700/2713]: training loss : 0.9365073704719543 TRAIN  loss dict:  {'classification_loss': 0.9365073704719543}
2025-01-19 13:27:05,847 [INFO] Label accuracies statistics:
2025-01-19 13:27:05,847 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 13:27:05,849 [INFO] [91] TRAIN  loss: 0.9375378662618025 acc: 0.9998771347831429
2025-01-19 13:27:05,849 [INFO] [91] TRAIN  loss dict: {'classification_loss': 0.9375378662618025}
2025-01-19 13:27:05,849 [INFO] [91] VALIDATION loss: 1.6873782421637298 VALIDATION acc: 0.819435736677116
2025-01-19 13:27:05,849 [INFO] [91] VALIDATION loss dict: {'classification_loss': 1.6873782421637298}
2025-01-19 13:27:05,849 [INFO] 
2025-01-19 13:27:22,037 [INFO] Step[50/2713]: training loss : 0.9362265002727509 TRAIN  loss dict:  {'classification_loss': 0.9362265002727509}
2025-01-19 13:27:33,508 [INFO] Step[100/2713]: training loss : 0.9367942881584167 TRAIN  loss dict:  {'classification_loss': 0.9367942881584167}
2025-01-19 13:27:44,985 [INFO] Step[150/2713]: training loss : 0.9372671627998352 TRAIN  loss dict:  {'classification_loss': 0.9372671627998352}
2025-01-19 13:27:56,478 [INFO] Step[200/2713]: training loss : 0.9364459419250488 TRAIN  loss dict:  {'classification_loss': 0.9364459419250488}
2025-01-19 13:28:07,959 [INFO] Step[250/2713]: training loss : 0.9440332806110382 TRAIN  loss dict:  {'classification_loss': 0.9440332806110382}
2025-01-19 13:28:19,428 [INFO] Step[300/2713]: training loss : 0.9370164215564728 TRAIN  loss dict:  {'classification_loss': 0.9370164215564728}
2025-01-19 13:28:30,844 [INFO] Step[350/2713]: training loss : 0.941427036523819 TRAIN  loss dict:  {'classification_loss': 0.941427036523819}
2025-01-19 13:28:42,280 [INFO] Step[400/2713]: training loss : 0.9358357739448547 TRAIN  loss dict:  {'classification_loss': 0.9358357739448547}
2025-01-19 13:28:53,774 [INFO] Step[450/2713]: training loss : 0.9360354113578796 TRAIN  loss dict:  {'classification_loss': 0.9360354113578796}
2025-01-19 13:29:05,242 [INFO] Step[500/2713]: training loss : 0.9357039165496827 TRAIN  loss dict:  {'classification_loss': 0.9357039165496827}
2025-01-19 13:29:16,666 [INFO] Step[550/2713]: training loss : 0.9363405871391296 TRAIN  loss dict:  {'classification_loss': 0.9363405871391296}
2025-01-19 13:29:28,058 [INFO] Step[600/2713]: training loss : 0.9359655022621155 TRAIN  loss dict:  {'classification_loss': 0.9359655022621155}
2025-01-19 13:29:39,535 [INFO] Step[650/2713]: training loss : 0.9371258664131165 TRAIN  loss dict:  {'classification_loss': 0.9371258664131165}
2025-01-19 13:29:51,028 [INFO] Step[700/2713]: training loss : 0.937732435464859 TRAIN  loss dict:  {'classification_loss': 0.937732435464859}
2025-01-19 13:30:02,543 [INFO] Step[750/2713]: training loss : 0.9350659716129303 TRAIN  loss dict:  {'classification_loss': 0.9350659716129303}
2025-01-19 13:30:14,005 [INFO] Step[800/2713]: training loss : 0.9363723456859588 TRAIN  loss dict:  {'classification_loss': 0.9363723456859588}
2025-01-19 13:30:25,514 [INFO] Step[850/2713]: training loss : 0.9357633340358734 TRAIN  loss dict:  {'classification_loss': 0.9357633340358734}
2025-01-19 13:30:37,016 [INFO] Step[900/2713]: training loss : 0.9377054154872895 TRAIN  loss dict:  {'classification_loss': 0.9377054154872895}
2025-01-19 13:30:48,493 [INFO] Step[950/2713]: training loss : 0.9358757638931274 TRAIN  loss dict:  {'classification_loss': 0.9358757638931274}
2025-01-19 13:30:59,951 [INFO] Step[1000/2713]: training loss : 0.9354017317295075 TRAIN  loss dict:  {'classification_loss': 0.9354017317295075}
2025-01-19 13:31:11,450 [INFO] Step[1050/2713]: training loss : 0.9381826460361481 TRAIN  loss dict:  {'classification_loss': 0.9381826460361481}
2025-01-19 13:31:22,901 [INFO] Step[1100/2713]: training loss : 0.937300353050232 TRAIN  loss dict:  {'classification_loss': 0.937300353050232}
2025-01-19 13:31:34,398 [INFO] Step[1150/2713]: training loss : 0.9367878425121308 TRAIN  loss dict:  {'classification_loss': 0.9367878425121308}
2025-01-19 13:31:45,833 [INFO] Step[1200/2713]: training loss : 0.9390298104286194 TRAIN  loss dict:  {'classification_loss': 0.9390298104286194}
2025-01-19 13:31:57,296 [INFO] Step[1250/2713]: training loss : 0.9367478775978089 TRAIN  loss dict:  {'classification_loss': 0.9367478775978089}
2025-01-19 13:32:08,795 [INFO] Step[1300/2713]: training loss : 0.9386536812782288 TRAIN  loss dict:  {'classification_loss': 0.9386536812782288}
2025-01-19 13:32:20,323 [INFO] Step[1350/2713]: training loss : 0.9383250319957733 TRAIN  loss dict:  {'classification_loss': 0.9383250319957733}
2025-01-19 13:32:31,776 [INFO] Step[1400/2713]: training loss : 0.9371754348278045 TRAIN  loss dict:  {'classification_loss': 0.9371754348278045}
2025-01-19 13:32:43,254 [INFO] Step[1450/2713]: training loss : 0.9361087346076965 TRAIN  loss dict:  {'classification_loss': 0.9361087346076965}
2025-01-19 13:32:54,777 [INFO] Step[1500/2713]: training loss : 0.9355218970775604 TRAIN  loss dict:  {'classification_loss': 0.9355218970775604}
2025-01-19 13:33:06,286 [INFO] Step[1550/2713]: training loss : 0.9431991422176361 TRAIN  loss dict:  {'classification_loss': 0.9431991422176361}
2025-01-19 13:33:17,761 [INFO] Step[1600/2713]: training loss : 0.9362143874168396 TRAIN  loss dict:  {'classification_loss': 0.9362143874168396}
2025-01-19 13:33:29,205 [INFO] Step[1650/2713]: training loss : 0.937210932970047 TRAIN  loss dict:  {'classification_loss': 0.937210932970047}
2025-01-19 13:33:40,665 [INFO] Step[1700/2713]: training loss : 0.9377857387065888 TRAIN  loss dict:  {'classification_loss': 0.9377857387065888}
2025-01-19 13:33:52,136 [INFO] Step[1750/2713]: training loss : 0.9363594079017639 TRAIN  loss dict:  {'classification_loss': 0.9363594079017639}
2025-01-19 13:34:03,617 [INFO] Step[1800/2713]: training loss : 0.9365798830986023 TRAIN  loss dict:  {'classification_loss': 0.9365798830986023}
2025-01-19 13:34:15,130 [INFO] Step[1850/2713]: training loss : 0.9367359435558319 TRAIN  loss dict:  {'classification_loss': 0.9367359435558319}
2025-01-19 13:34:26,575 [INFO] Step[1900/2713]: training loss : 0.9380063080787658 TRAIN  loss dict:  {'classification_loss': 0.9380063080787658}
2025-01-19 13:34:38,036 [INFO] Step[1950/2713]: training loss : 0.9352422308921814 TRAIN  loss dict:  {'classification_loss': 0.9352422308921814}
2025-01-19 13:34:49,465 [INFO] Step[2000/2713]: training loss : 0.934770084619522 TRAIN  loss dict:  {'classification_loss': 0.934770084619522}
2025-01-19 13:35:00,928 [INFO] Step[2050/2713]: training loss : 0.9386649596691131 TRAIN  loss dict:  {'classification_loss': 0.9386649596691131}
2025-01-19 13:35:12,398 [INFO] Step[2100/2713]: training loss : 0.9371988499164581 TRAIN  loss dict:  {'classification_loss': 0.9371988499164581}
2025-01-19 13:35:23,898 [INFO] Step[2150/2713]: training loss : 0.9376234591007233 TRAIN  loss dict:  {'classification_loss': 0.9376234591007233}
2025-01-19 13:35:35,392 [INFO] Step[2200/2713]: training loss : 0.9370530998706817 TRAIN  loss dict:  {'classification_loss': 0.9370530998706817}
2025-01-19 13:35:46,852 [INFO] Step[2250/2713]: training loss : 0.9383252787590027 TRAIN  loss dict:  {'classification_loss': 0.9383252787590027}
2025-01-19 13:35:58,353 [INFO] Step[2300/2713]: training loss : 0.9379119312763214 TRAIN  loss dict:  {'classification_loss': 0.9379119312763214}
2025-01-19 13:36:09,849 [INFO] Step[2350/2713]: training loss : 0.9396913933753968 TRAIN  loss dict:  {'classification_loss': 0.9396913933753968}
2025-01-19 13:36:21,281 [INFO] Step[2400/2713]: training loss : 0.9347405457496643 TRAIN  loss dict:  {'classification_loss': 0.9347405457496643}
2025-01-19 13:36:32,764 [INFO] Step[2450/2713]: training loss : 0.9390172982215881 TRAIN  loss dict:  {'classification_loss': 0.9390172982215881}
2025-01-19 13:36:44,262 [INFO] Step[2500/2713]: training loss : 0.9368187999725341 TRAIN  loss dict:  {'classification_loss': 0.9368187999725341}
2025-01-19 13:36:55,716 [INFO] Step[2550/2713]: training loss : 0.9366816365718842 TRAIN  loss dict:  {'classification_loss': 0.9366816365718842}
2025-01-19 13:37:07,186 [INFO] Step[2600/2713]: training loss : 0.9361267876625061 TRAIN  loss dict:  {'classification_loss': 0.9361267876625061}
2025-01-19 13:37:18,654 [INFO] Step[2650/2713]: training loss : 0.9375925981998443 TRAIN  loss dict:  {'classification_loss': 0.9375925981998443}
2025-01-19 13:37:30,112 [INFO] Step[2700/2713]: training loss : 0.935192242860794 TRAIN  loss dict:  {'classification_loss': 0.935192242860794}
2025-01-19 13:38:35,588 [INFO] Label accuracies statistics:
2025-01-19 13:38:35,588 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 13:38:35,589 [INFO] [92] TRAIN  loss: 0.9371861477773479 acc: 1.0
2025-01-19 13:38:35,589 [INFO] [92] TRAIN  loss dict: {'classification_loss': 0.9371861477773479}
2025-01-19 13:38:35,589 [INFO] [92] VALIDATION loss: 1.6977821278168743 VALIDATION acc: 0.819435736677116
2025-01-19 13:38:35,589 [INFO] [92] VALIDATION loss dict: {'classification_loss': 1.6977821278168743}
2025-01-19 13:38:35,590 [INFO] 
2025-01-19 13:38:52,318 [INFO] Step[50/2713]: training loss : 0.9366251015663147 TRAIN  loss dict:  {'classification_loss': 0.9366251015663147}
2025-01-19 13:39:03,763 [INFO] Step[100/2713]: training loss : 0.9365476191043853 TRAIN  loss dict:  {'classification_loss': 0.9365476191043853}
2025-01-19 13:39:15,247 [INFO] Step[150/2713]: training loss : 0.9354452896118164 TRAIN  loss dict:  {'classification_loss': 0.9354452896118164}
2025-01-19 13:39:26,718 [INFO] Step[200/2713]: training loss : 0.9374276280403138 TRAIN  loss dict:  {'classification_loss': 0.9374276280403138}
2025-01-19 13:39:38,171 [INFO] Step[250/2713]: training loss : 0.9447557175159454 TRAIN  loss dict:  {'classification_loss': 0.9447557175159454}
2025-01-19 13:39:49,687 [INFO] Step[300/2713]: training loss : 0.9375462532043457 TRAIN  loss dict:  {'classification_loss': 0.9375462532043457}
2025-01-19 13:40:01,172 [INFO] Step[350/2713]: training loss : 0.94540163397789 TRAIN  loss dict:  {'classification_loss': 0.94540163397789}
2025-01-19 13:40:12,629 [INFO] Step[400/2713]: training loss : 0.9364509117603302 TRAIN  loss dict:  {'classification_loss': 0.9364509117603302}
2025-01-19 13:40:24,149 [INFO] Step[450/2713]: training loss : 0.9370168888568878 TRAIN  loss dict:  {'classification_loss': 0.9370168888568878}
2025-01-19 13:40:35,647 [INFO] Step[500/2713]: training loss : 0.9365243029594421 TRAIN  loss dict:  {'classification_loss': 0.9365243029594421}
2025-01-19 13:40:47,161 [INFO] Step[550/2713]: training loss : 0.9358432292938232 TRAIN  loss dict:  {'classification_loss': 0.9358432292938232}
2025-01-19 13:40:58,664 [INFO] Step[600/2713]: training loss : 0.9426089739799499 TRAIN  loss dict:  {'classification_loss': 0.9426089739799499}
2025-01-19 13:41:10,160 [INFO] Step[650/2713]: training loss : 0.9360487782955169 TRAIN  loss dict:  {'classification_loss': 0.9360487782955169}
2025-01-19 13:41:21,641 [INFO] Step[700/2713]: training loss : 0.9362700891494751 TRAIN  loss dict:  {'classification_loss': 0.9362700891494751}
2025-01-19 13:41:33,147 [INFO] Step[750/2713]: training loss : 0.9383827006816864 TRAIN  loss dict:  {'classification_loss': 0.9383827006816864}
2025-01-19 13:41:44,633 [INFO] Step[800/2713]: training loss : 0.9378339505195618 TRAIN  loss dict:  {'classification_loss': 0.9378339505195618}
2025-01-19 13:41:56,107 [INFO] Step[850/2713]: training loss : 0.9358213329315186 TRAIN  loss dict:  {'classification_loss': 0.9358213329315186}
2025-01-19 13:42:07,570 [INFO] Step[900/2713]: training loss : 0.9367965519428253 TRAIN  loss dict:  {'classification_loss': 0.9367965519428253}
2025-01-19 13:42:19,092 [INFO] Step[950/2713]: training loss : 0.9379022586345672 TRAIN  loss dict:  {'classification_loss': 0.9379022586345672}
2025-01-19 13:42:30,576 [INFO] Step[1000/2713]: training loss : 0.9363322293758393 TRAIN  loss dict:  {'classification_loss': 0.9363322293758393}
2025-01-19 13:42:42,017 [INFO] Step[1050/2713]: training loss : 0.935564615726471 TRAIN  loss dict:  {'classification_loss': 0.935564615726471}
2025-01-19 13:42:53,469 [INFO] Step[1100/2713]: training loss : 0.9436077964305878 TRAIN  loss dict:  {'classification_loss': 0.9436077964305878}
2025-01-19 13:43:04,933 [INFO] Step[1150/2713]: training loss : 0.9413360977172851 TRAIN  loss dict:  {'classification_loss': 0.9413360977172851}
2025-01-19 13:43:16,431 [INFO] Step[1200/2713]: training loss : 0.9360402941703796 TRAIN  loss dict:  {'classification_loss': 0.9360402941703796}
2025-01-19 13:43:27,894 [INFO] Step[1250/2713]: training loss : 0.9383615136146546 TRAIN  loss dict:  {'classification_loss': 0.9383615136146546}
2025-01-19 13:43:39,393 [INFO] Step[1300/2713]: training loss : 0.9352052783966065 TRAIN  loss dict:  {'classification_loss': 0.9352052783966065}
2025-01-19 13:43:50,879 [INFO] Step[1350/2713]: training loss : 0.9355954766273499 TRAIN  loss dict:  {'classification_loss': 0.9355954766273499}
2025-01-19 13:44:02,353 [INFO] Step[1400/2713]: training loss : 0.9428108489513397 TRAIN  loss dict:  {'classification_loss': 0.9428108489513397}
2025-01-19 13:44:13,826 [INFO] Step[1450/2713]: training loss : 0.9368248569965363 TRAIN  loss dict:  {'classification_loss': 0.9368248569965363}
2025-01-19 13:44:25,326 [INFO] Step[1500/2713]: training loss : 0.9358817231655121 TRAIN  loss dict:  {'classification_loss': 0.9358817231655121}
2025-01-19 13:44:36,838 [INFO] Step[1550/2713]: training loss : 0.9356509923934937 TRAIN  loss dict:  {'classification_loss': 0.9356509923934937}
2025-01-19 13:44:48,337 [INFO] Step[1600/2713]: training loss : 0.9369090807437896 TRAIN  loss dict:  {'classification_loss': 0.9369090807437896}
2025-01-19 13:44:59,831 [INFO] Step[1650/2713]: training loss : 0.937156183719635 TRAIN  loss dict:  {'classification_loss': 0.937156183719635}
2025-01-19 13:45:11,325 [INFO] Step[1700/2713]: training loss : 0.9355815243721008 TRAIN  loss dict:  {'classification_loss': 0.9355815243721008}
2025-01-19 13:45:22,816 [INFO] Step[1750/2713]: training loss : 0.9349714839458465 TRAIN  loss dict:  {'classification_loss': 0.9349714839458465}
2025-01-19 13:45:34,279 [INFO] Step[1800/2713]: training loss : 0.941039547920227 TRAIN  loss dict:  {'classification_loss': 0.941039547920227}
2025-01-19 13:45:45,776 [INFO] Step[1850/2713]: training loss : 0.9366671872138977 TRAIN  loss dict:  {'classification_loss': 0.9366671872138977}
2025-01-19 13:45:57,258 [INFO] Step[1900/2713]: training loss : 0.9358231794834136 TRAIN  loss dict:  {'classification_loss': 0.9358231794834136}
2025-01-19 13:46:08,720 [INFO] Step[1950/2713]: training loss : 0.9359707260131835 TRAIN  loss dict:  {'classification_loss': 0.9359707260131835}
2025-01-19 13:46:20,226 [INFO] Step[2000/2713]: training loss : 0.9357062470912934 TRAIN  loss dict:  {'classification_loss': 0.9357062470912934}
2025-01-19 13:46:31,728 [INFO] Step[2050/2713]: training loss : 0.9347465717792511 TRAIN  loss dict:  {'classification_loss': 0.9347465717792511}
2025-01-19 13:46:43,228 [INFO] Step[2100/2713]: training loss : 0.9355190813541412 TRAIN  loss dict:  {'classification_loss': 0.9355190813541412}
2025-01-19 13:46:54,745 [INFO] Step[2150/2713]: training loss : 0.9352202630043029 TRAIN  loss dict:  {'classification_loss': 0.9352202630043029}
2025-01-19 13:47:06,174 [INFO] Step[2200/2713]: training loss : 0.9387564241886139 TRAIN  loss dict:  {'classification_loss': 0.9387564241886139}
2025-01-19 13:47:17,650 [INFO] Step[2250/2713]: training loss : 0.9390213394165039 TRAIN  loss dict:  {'classification_loss': 0.9390213394165039}
2025-01-19 13:47:29,101 [INFO] Step[2300/2713]: training loss : 0.9411586654186249 TRAIN  loss dict:  {'classification_loss': 0.9411586654186249}
2025-01-19 13:47:40,557 [INFO] Step[2350/2713]: training loss : 0.9387079691886902 TRAIN  loss dict:  {'classification_loss': 0.9387079691886902}
2025-01-19 13:47:52,053 [INFO] Step[2400/2713]: training loss : 0.9364662325382233 TRAIN  loss dict:  {'classification_loss': 0.9364662325382233}
2025-01-19 13:48:03,553 [INFO] Step[2450/2713]: training loss : 0.9374911093711853 TRAIN  loss dict:  {'classification_loss': 0.9374911093711853}
2025-01-19 13:48:15,031 [INFO] Step[2500/2713]: training loss : 0.9377213084697723 TRAIN  loss dict:  {'classification_loss': 0.9377213084697723}
2025-01-19 13:48:26,542 [INFO] Step[2550/2713]: training loss : 0.9378560912609101 TRAIN  loss dict:  {'classification_loss': 0.9378560912609101}
2025-01-19 13:48:38,016 [INFO] Step[2600/2713]: training loss : 0.9363929235935211 TRAIN  loss dict:  {'classification_loss': 0.9363929235935211}
2025-01-19 13:48:49,517 [INFO] Step[2650/2713]: training loss : 0.9361343502998352 TRAIN  loss dict:  {'classification_loss': 0.9361343502998352}
2025-01-19 13:49:00,999 [INFO] Step[2700/2713]: training loss : 0.9417775535583496 TRAIN  loss dict:  {'classification_loss': 0.9417775535583496}
2025-01-19 13:50:07,537 [INFO] Label accuracies statistics:
2025-01-19 13:50:07,537 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.5, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 1.0, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 1.0, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 13:50:07,538 [INFO] [93] TRAIN  loss: 0.9376085838224015 acc: 1.0
2025-01-19 13:50:07,538 [INFO] [93] TRAIN  loss dict: {'classification_loss': 0.9376085838224015}
2025-01-19 13:50:07,539 [INFO] [93] VALIDATION loss: 1.7255195733299828 VALIDATION acc: 0.8087774294670846
2025-01-19 13:50:07,539 [INFO] [93] VALIDATION loss dict: {'classification_loss': 1.7255195733299828}
2025-01-19 13:50:07,539 [INFO] 
2025-01-19 13:50:24,342 [INFO] Step[50/2713]: training loss : 0.9369937062263489 TRAIN  loss dict:  {'classification_loss': 0.9369937062263489}
2025-01-19 13:50:35,791 [INFO] Step[100/2713]: training loss : 0.936614396572113 TRAIN  loss dict:  {'classification_loss': 0.936614396572113}
2025-01-19 13:50:47,291 [INFO] Step[150/2713]: training loss : 0.9361720287799835 TRAIN  loss dict:  {'classification_loss': 0.9361720287799835}
2025-01-19 13:50:58,788 [INFO] Step[200/2713]: training loss : 0.9355822920799255 TRAIN  loss dict:  {'classification_loss': 0.9355822920799255}
2025-01-19 13:51:10,300 [INFO] Step[250/2713]: training loss : 0.9352380931377411 TRAIN  loss dict:  {'classification_loss': 0.9352380931377411}
2025-01-19 13:51:21,763 [INFO] Step[300/2713]: training loss : 0.9368819403648376 TRAIN  loss dict:  {'classification_loss': 0.9368819403648376}
2025-01-19 13:51:33,241 [INFO] Step[350/2713]: training loss : 0.9372574985027313 TRAIN  loss dict:  {'classification_loss': 0.9372574985027313}
2025-01-19 13:51:44,756 [INFO] Step[400/2713]: training loss : 0.9376950800418854 TRAIN  loss dict:  {'classification_loss': 0.9376950800418854}
2025-01-19 13:51:56,237 [INFO] Step[450/2713]: training loss : 0.936293226480484 TRAIN  loss dict:  {'classification_loss': 0.936293226480484}
2025-01-19 13:52:07,709 [INFO] Step[500/2713]: training loss : 0.9353339517116547 TRAIN  loss dict:  {'classification_loss': 0.9353339517116547}
2025-01-19 13:52:19,225 [INFO] Step[550/2713]: training loss : 0.9380795121192932 TRAIN  loss dict:  {'classification_loss': 0.9380795121192932}
2025-01-19 13:52:30,713 [INFO] Step[600/2713]: training loss : 0.9356736838817596 TRAIN  loss dict:  {'classification_loss': 0.9356736838817596}
2025-01-19 13:52:42,169 [INFO] Step[650/2713]: training loss : 0.9361501431465149 TRAIN  loss dict:  {'classification_loss': 0.9361501431465149}
2025-01-19 13:52:53,647 [INFO] Step[700/2713]: training loss : 0.9363147711753845 TRAIN  loss dict:  {'classification_loss': 0.9363147711753845}
2025-01-19 13:53:05,156 [INFO] Step[750/2713]: training loss : 0.9364231812953949 TRAIN  loss dict:  {'classification_loss': 0.9364231812953949}
2025-01-19 13:53:16,660 [INFO] Step[800/2713]: training loss : 0.9359629988670349 TRAIN  loss dict:  {'classification_loss': 0.9359629988670349}
2025-01-19 13:53:28,157 [INFO] Step[850/2713]: training loss : 0.9356340444087983 TRAIN  loss dict:  {'classification_loss': 0.9356340444087983}
2025-01-19 13:53:39,665 [INFO] Step[900/2713]: training loss : 0.9353060698509217 TRAIN  loss dict:  {'classification_loss': 0.9353060698509217}
2025-01-19 13:53:51,148 [INFO] Step[950/2713]: training loss : 0.9372435617446899 TRAIN  loss dict:  {'classification_loss': 0.9372435617446899}
2025-01-19 13:54:02,639 [INFO] Step[1000/2713]: training loss : 0.9386359739303589 TRAIN  loss dict:  {'classification_loss': 0.9386359739303589}
2025-01-19 13:54:14,093 [INFO] Step[1050/2713]: training loss : 0.9375036489963532 TRAIN  loss dict:  {'classification_loss': 0.9375036489963532}
2025-01-19 13:54:25,587 [INFO] Step[1100/2713]: training loss : 0.9376536405086517 TRAIN  loss dict:  {'classification_loss': 0.9376536405086517}
2025-01-19 13:54:37,068 [INFO] Step[1150/2713]: training loss : 0.9378545749187469 TRAIN  loss dict:  {'classification_loss': 0.9378545749187469}
2025-01-19 13:54:48,527 [INFO] Step[1200/2713]: training loss : 0.9356131851673126 TRAIN  loss dict:  {'classification_loss': 0.9356131851673126}
2025-01-19 13:55:00,030 [INFO] Step[1250/2713]: training loss : 0.9371213793754578 TRAIN  loss dict:  {'classification_loss': 0.9371213793754578}
2025-01-19 13:55:11,508 [INFO] Step[1300/2713]: training loss : 0.9359464144706726 TRAIN  loss dict:  {'classification_loss': 0.9359464144706726}
2025-01-19 13:55:22,991 [INFO] Step[1350/2713]: training loss : 0.937012140750885 TRAIN  loss dict:  {'classification_loss': 0.937012140750885}
2025-01-19 13:55:34,472 [INFO] Step[1400/2713]: training loss : 0.9357858955860138 TRAIN  loss dict:  {'classification_loss': 0.9357858955860138}
2025-01-19 13:55:45,919 [INFO] Step[1450/2713]: training loss : 0.9433919191360474 TRAIN  loss dict:  {'classification_loss': 0.9433919191360474}
2025-01-19 13:55:57,384 [INFO] Step[1500/2713]: training loss : 0.9376626002788544 TRAIN  loss dict:  {'classification_loss': 0.9376626002788544}
2025-01-19 13:56:08,861 [INFO] Step[1550/2713]: training loss : 0.9367875826358795 TRAIN  loss dict:  {'classification_loss': 0.9367875826358795}
2025-01-19 13:56:20,320 [INFO] Step[1600/2713]: training loss : 0.938038101196289 TRAIN  loss dict:  {'classification_loss': 0.938038101196289}
2025-01-19 13:56:31,788 [INFO] Step[1650/2713]: training loss : 0.9351918315887451 TRAIN  loss dict:  {'classification_loss': 0.9351918315887451}
2025-01-19 13:56:43,216 [INFO] Step[1700/2713]: training loss : 0.9365539252758026 TRAIN  loss dict:  {'classification_loss': 0.9365539252758026}
2025-01-19 13:56:54,692 [INFO] Step[1750/2713]: training loss : 0.9424530231952667 TRAIN  loss dict:  {'classification_loss': 0.9424530231952667}
2025-01-19 13:57:06,181 [INFO] Step[1800/2713]: training loss : 0.944522978067398 TRAIN  loss dict:  {'classification_loss': 0.944522978067398}
2025-01-19 13:57:17,663 [INFO] Step[1850/2713]: training loss : 0.9356141996383667 TRAIN  loss dict:  {'classification_loss': 0.9356141996383667}
2025-01-19 13:57:29,095 [INFO] Step[1900/2713]: training loss : 0.9363823533058167 TRAIN  loss dict:  {'classification_loss': 0.9363823533058167}
2025-01-19 13:57:40,585 [INFO] Step[1950/2713]: training loss : 0.9363913786411285 TRAIN  loss dict:  {'classification_loss': 0.9363913786411285}
2025-01-19 13:57:52,090 [INFO] Step[2000/2713]: training loss : 0.936812653541565 TRAIN  loss dict:  {'classification_loss': 0.936812653541565}
2025-01-19 13:58:03,594 [INFO] Step[2050/2713]: training loss : 0.9367927682399749 TRAIN  loss dict:  {'classification_loss': 0.9367927682399749}
2025-01-19 13:58:15,061 [INFO] Step[2100/2713]: training loss : 0.9390055787563324 TRAIN  loss dict:  {'classification_loss': 0.9390055787563324}
2025-01-19 13:58:26,530 [INFO] Step[2150/2713]: training loss : 0.9345513045787811 TRAIN  loss dict:  {'classification_loss': 0.9345513045787811}
2025-01-19 13:58:38,016 [INFO] Step[2200/2713]: training loss : 0.9381048285961151 TRAIN  loss dict:  {'classification_loss': 0.9381048285961151}
2025-01-19 13:58:49,513 [INFO] Step[2250/2713]: training loss : 0.9488310623168945 TRAIN  loss dict:  {'classification_loss': 0.9488310623168945}
2025-01-19 13:59:00,946 [INFO] Step[2300/2713]: training loss : 0.9357231521606445 TRAIN  loss dict:  {'classification_loss': 0.9357231521606445}
2025-01-19 13:59:12,425 [INFO] Step[2350/2713]: training loss : 0.9348794543743133 TRAIN  loss dict:  {'classification_loss': 0.9348794543743133}
2025-01-19 13:59:23,901 [INFO] Step[2400/2713]: training loss : 0.9363196563720703 TRAIN  loss dict:  {'classification_loss': 0.9363196563720703}
2025-01-19 13:59:35,363 [INFO] Step[2450/2713]: training loss : 0.9355183064937591 TRAIN  loss dict:  {'classification_loss': 0.9355183064937591}
2025-01-19 13:59:46,820 [INFO] Step[2500/2713]: training loss : 0.9359993946552276 TRAIN  loss dict:  {'classification_loss': 0.9359993946552276}
2025-01-19 13:59:58,297 [INFO] Step[2550/2713]: training loss : 0.942780704498291 TRAIN  loss dict:  {'classification_loss': 0.942780704498291}
2025-01-19 14:00:09,776 [INFO] Step[2600/2713]: training loss : 0.9374706757068634 TRAIN  loss dict:  {'classification_loss': 0.9374706757068634}
2025-01-19 14:00:21,233 [INFO] Step[2650/2713]: training loss : 0.9390934669971466 TRAIN  loss dict:  {'classification_loss': 0.9390934669971466}
2025-01-19 14:00:32,684 [INFO] Step[2700/2713]: training loss : 0.9361203515529632 TRAIN  loss dict:  {'classification_loss': 0.9361203515529632}
2025-01-19 14:01:38,737 [INFO] Label accuracies statistics:
2025-01-19 14:01:38,737 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 1.0, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.5, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 14:01:38,739 [INFO] [94] TRAIN  loss: 0.9373112800505343 acc: 0.9998771347831429
2025-01-19 14:01:38,739 [INFO] [94] TRAIN  loss dict: {'classification_loss': 0.9373112800505343}
2025-01-19 14:01:38,739 [INFO] [94] VALIDATION loss: 1.73909351666619 VALIDATION acc: 0.8075235109717869
2025-01-19 14:01:38,739 [INFO] [94] VALIDATION loss dict: {'classification_loss': 1.73909351666619}
2025-01-19 14:01:38,739 [INFO] 
2025-01-19 14:01:54,668 [INFO] Step[50/2713]: training loss : 0.9373100888729096 TRAIN  loss dict:  {'classification_loss': 0.9373100888729096}
2025-01-19 14:02:06,165 [INFO] Step[100/2713]: training loss : 0.9359310948848725 TRAIN  loss dict:  {'classification_loss': 0.9359310948848725}
2025-01-19 14:02:17,657 [INFO] Step[150/2713]: training loss : 0.9378817987442016 TRAIN  loss dict:  {'classification_loss': 0.9378817987442016}
2025-01-19 14:02:29,167 [INFO] Step[200/2713]: training loss : 0.9349825406074523 TRAIN  loss dict:  {'classification_loss': 0.9349825406074523}
2025-01-19 14:02:40,652 [INFO] Step[250/2713]: training loss : 0.9384756469726563 TRAIN  loss dict:  {'classification_loss': 0.9384756469726563}
2025-01-19 14:02:52,144 [INFO] Step[300/2713]: training loss : 0.939785099029541 TRAIN  loss dict:  {'classification_loss': 0.939785099029541}
2025-01-19 14:03:03,623 [INFO] Step[350/2713]: training loss : 0.9383316206932067 TRAIN  loss dict:  {'classification_loss': 0.9383316206932067}
2025-01-19 14:03:15,113 [INFO] Step[400/2713]: training loss : 0.9364549267292023 TRAIN  loss dict:  {'classification_loss': 0.9364549267292023}
2025-01-19 14:03:26,648 [INFO] Step[450/2713]: training loss : 0.9357902443408966 TRAIN  loss dict:  {'classification_loss': 0.9357902443408966}
2025-01-19 14:03:38,086 [INFO] Step[500/2713]: training loss : 0.9365226340293884 TRAIN  loss dict:  {'classification_loss': 0.9365226340293884}
2025-01-19 14:03:49,590 [INFO] Step[550/2713]: training loss : 0.9367422533035278 TRAIN  loss dict:  {'classification_loss': 0.9367422533035278}
2025-01-19 14:04:01,056 [INFO] Step[600/2713]: training loss : 0.9348386311531067 TRAIN  loss dict:  {'classification_loss': 0.9348386311531067}
2025-01-19 14:04:12,536 [INFO] Step[650/2713]: training loss : 0.9348221325874329 TRAIN  loss dict:  {'classification_loss': 0.9348221325874329}
2025-01-19 14:04:24,023 [INFO] Step[700/2713]: training loss : 0.9344020581245422 TRAIN  loss dict:  {'classification_loss': 0.9344020581245422}
2025-01-19 14:04:35,527 [INFO] Step[750/2713]: training loss : 0.9366269814968109 TRAIN  loss dict:  {'classification_loss': 0.9366269814968109}
2025-01-19 14:04:46,999 [INFO] Step[800/2713]: training loss : 0.9359749150276184 TRAIN  loss dict:  {'classification_loss': 0.9359749150276184}
2025-01-19 14:04:58,450 [INFO] Step[850/2713]: training loss : 0.9343899273872376 TRAIN  loss dict:  {'classification_loss': 0.9343899273872376}
2025-01-19 14:05:09,910 [INFO] Step[900/2713]: training loss : 0.9356532847881317 TRAIN  loss dict:  {'classification_loss': 0.9356532847881317}
2025-01-19 14:05:21,373 [INFO] Step[950/2713]: training loss : 0.9359314978122711 TRAIN  loss dict:  {'classification_loss': 0.9359314978122711}
2025-01-19 14:05:32,807 [INFO] Step[1000/2713]: training loss : 0.9379315376281738 TRAIN  loss dict:  {'classification_loss': 0.9379315376281738}
2025-01-19 14:05:44,271 [INFO] Step[1050/2713]: training loss : 0.9355288016796112 TRAIN  loss dict:  {'classification_loss': 0.9355288016796112}
2025-01-19 14:05:55,771 [INFO] Step[1100/2713]: training loss : 0.9374069237709045 TRAIN  loss dict:  {'classification_loss': 0.9374069237709045}
2025-01-19 14:06:07,285 [INFO] Step[1150/2713]: training loss : 0.9368511044979095 TRAIN  loss dict:  {'classification_loss': 0.9368511044979095}
2025-01-19 14:06:18,729 [INFO] Step[1200/2713]: training loss : 0.9380879354476929 TRAIN  loss dict:  {'classification_loss': 0.9380879354476929}
2025-01-19 14:06:30,217 [INFO] Step[1250/2713]: training loss : 0.9374876618385315 TRAIN  loss dict:  {'classification_loss': 0.9374876618385315}
2025-01-19 14:06:41,673 [INFO] Step[1300/2713]: training loss : 0.9436944890022277 TRAIN  loss dict:  {'classification_loss': 0.9436944890022277}
2025-01-19 14:06:53,196 [INFO] Step[1350/2713]: training loss : 0.9354190099239349 TRAIN  loss dict:  {'classification_loss': 0.9354190099239349}
2025-01-19 14:07:04,717 [INFO] Step[1400/2713]: training loss : 0.9357350826263428 TRAIN  loss dict:  {'classification_loss': 0.9357350826263428}
2025-01-19 14:07:16,228 [INFO] Step[1450/2713]: training loss : 0.934951479434967 TRAIN  loss dict:  {'classification_loss': 0.934951479434967}
2025-01-19 14:07:27,736 [INFO] Step[1500/2713]: training loss : 0.9368528985977173 TRAIN  loss dict:  {'classification_loss': 0.9368528985977173}
2025-01-19 14:07:39,244 [INFO] Step[1550/2713]: training loss : 0.9358451521396637 TRAIN  loss dict:  {'classification_loss': 0.9358451521396637}
2025-01-19 14:07:50,730 [INFO] Step[1600/2713]: training loss : 0.9374392318725586 TRAIN  loss dict:  {'classification_loss': 0.9374392318725586}
2025-01-19 14:08:02,176 [INFO] Step[1650/2713]: training loss : 0.9356425762176513 TRAIN  loss dict:  {'classification_loss': 0.9356425762176513}
2025-01-19 14:08:13,672 [INFO] Step[1700/2713]: training loss : 0.9364955711364746 TRAIN  loss dict:  {'classification_loss': 0.9364955711364746}
2025-01-19 14:08:25,153 [INFO] Step[1750/2713]: training loss : 0.9361148571968079 TRAIN  loss dict:  {'classification_loss': 0.9361148571968079}
2025-01-19 14:08:36,649 [INFO] Step[1800/2713]: training loss : 0.9374808180332184 TRAIN  loss dict:  {'classification_loss': 0.9374808180332184}
2025-01-19 14:08:48,123 [INFO] Step[1850/2713]: training loss : 0.9355390763282776 TRAIN  loss dict:  {'classification_loss': 0.9355390763282776}
2025-01-19 14:08:59,621 [INFO] Step[1900/2713]: training loss : 0.9347325587272644 TRAIN  loss dict:  {'classification_loss': 0.9347325587272644}
2025-01-19 14:09:11,104 [INFO] Step[1950/2713]: training loss : 0.9385347378253937 TRAIN  loss dict:  {'classification_loss': 0.9385347378253937}
2025-01-19 14:09:22,619 [INFO] Step[2000/2713]: training loss : 0.9361052882671356 TRAIN  loss dict:  {'classification_loss': 0.9361052882671356}
2025-01-19 14:09:34,078 [INFO] Step[2050/2713]: training loss : 0.9356464576721192 TRAIN  loss dict:  {'classification_loss': 0.9356464576721192}
2025-01-19 14:09:45,510 [INFO] Step[2100/2713]: training loss : 0.9386696016788483 TRAIN  loss dict:  {'classification_loss': 0.9386696016788483}
2025-01-19 14:09:56,990 [INFO] Step[2150/2713]: training loss : 0.9368018364906311 TRAIN  loss dict:  {'classification_loss': 0.9368018364906311}
2025-01-19 14:10:08,453 [INFO] Step[2200/2713]: training loss : 0.9362796008586883 TRAIN  loss dict:  {'classification_loss': 0.9362796008586883}
2025-01-19 14:10:19,984 [INFO] Step[2250/2713]: training loss : 0.9365879547595978 TRAIN  loss dict:  {'classification_loss': 0.9365879547595978}
2025-01-19 14:10:31,474 [INFO] Step[2300/2713]: training loss : 0.9354588949680328 TRAIN  loss dict:  {'classification_loss': 0.9354588949680328}
2025-01-19 14:10:42,942 [INFO] Step[2350/2713]: training loss : 0.9353840148448944 TRAIN  loss dict:  {'classification_loss': 0.9353840148448944}
2025-01-19 14:10:54,374 [INFO] Step[2400/2713]: training loss : 0.9372336745262146 TRAIN  loss dict:  {'classification_loss': 0.9372336745262146}
2025-01-19 14:11:05,820 [INFO] Step[2450/2713]: training loss : 0.9358863639831543 TRAIN  loss dict:  {'classification_loss': 0.9358863639831543}
2025-01-19 14:11:17,305 [INFO] Step[2500/2713]: training loss : 0.9352286469936371 TRAIN  loss dict:  {'classification_loss': 0.9352286469936371}
2025-01-19 14:11:28,798 [INFO] Step[2550/2713]: training loss : 0.9381826341152191 TRAIN  loss dict:  {'classification_loss': 0.9381826341152191}
2025-01-19 14:11:40,290 [INFO] Step[2600/2713]: training loss : 0.9366753387451172 TRAIN  loss dict:  {'classification_loss': 0.9366753387451172}
2025-01-19 14:11:51,744 [INFO] Step[2650/2713]: training loss : 0.935499757528305 TRAIN  loss dict:  {'classification_loss': 0.935499757528305}
2025-01-19 14:12:03,221 [INFO] Step[2700/2713]: training loss : 0.9372378063201904 TRAIN  loss dict:  {'classification_loss': 0.9372378063201904}
2025-01-19 14:13:09,324 [INFO] Label accuracies statistics:
2025-01-19 14:13:09,324 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 1.0, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 1.0, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.75, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 14:13:09,326 [INFO] [95] TRAIN  loss: 0.936564730951982 acc: 1.0
2025-01-19 14:13:09,326 [INFO] [95] TRAIN  loss dict: {'classification_loss': 0.936564730951982}
2025-01-19 14:13:09,326 [INFO] [95] VALIDATION loss: 1.7011107904346365 VALIDATION acc: 0.812539184952978
2025-01-19 14:13:09,326 [INFO] [95] VALIDATION loss dict: {'classification_loss': 1.7011107904346365}
2025-01-19 14:13:09,326 [INFO] 
2025-01-19 14:13:26,082 [INFO] Step[50/2713]: training loss : 0.9357206058502198 TRAIN  loss dict:  {'classification_loss': 0.9357206058502198}
2025-01-19 14:13:37,544 [INFO] Step[100/2713]: training loss : 0.9349501740932464 TRAIN  loss dict:  {'classification_loss': 0.9349501740932464}
2025-01-19 14:13:49,061 [INFO] Step[150/2713]: training loss : 0.9360112464427948 TRAIN  loss dict:  {'classification_loss': 0.9360112464427948}
2025-01-19 14:14:00,575 [INFO] Step[200/2713]: training loss : 0.935108517408371 TRAIN  loss dict:  {'classification_loss': 0.935108517408371}
2025-01-19 14:14:12,093 [INFO] Step[250/2713]: training loss : 0.9386781203746796 TRAIN  loss dict:  {'classification_loss': 0.9386781203746796}
2025-01-19 14:14:23,571 [INFO] Step[300/2713]: training loss : 0.9372294855117798 TRAIN  loss dict:  {'classification_loss': 0.9372294855117798}
2025-01-19 14:14:35,098 [INFO] Step[350/2713]: training loss : 0.9360707783699036 TRAIN  loss dict:  {'classification_loss': 0.9360707783699036}
2025-01-19 14:14:46,585 [INFO] Step[400/2713]: training loss : 0.9371653187274933 TRAIN  loss dict:  {'classification_loss': 0.9371653187274933}
2025-01-19 14:14:58,079 [INFO] Step[450/2713]: training loss : 0.9363850176334381 TRAIN  loss dict:  {'classification_loss': 0.9363850176334381}
2025-01-19 14:15:09,533 [INFO] Step[500/2713]: training loss : 0.9364990901947021 TRAIN  loss dict:  {'classification_loss': 0.9364990901947021}
2025-01-19 14:15:20,999 [INFO] Step[550/2713]: training loss : 0.9419322371482849 TRAIN  loss dict:  {'classification_loss': 0.9419322371482849}
2025-01-19 14:15:32,477 [INFO] Step[600/2713]: training loss : 0.9372897171974182 TRAIN  loss dict:  {'classification_loss': 0.9372897171974182}
2025-01-19 14:15:43,997 [INFO] Step[650/2713]: training loss : 0.9392780804634094 TRAIN  loss dict:  {'classification_loss': 0.9392780804634094}
2025-01-19 14:15:55,514 [INFO] Step[700/2713]: training loss : 0.9361941480636596 TRAIN  loss dict:  {'classification_loss': 0.9361941480636596}
2025-01-19 14:16:06,982 [INFO] Step[750/2713]: training loss : 0.9378752040863038 TRAIN  loss dict:  {'classification_loss': 0.9378752040863038}
2025-01-19 14:16:18,448 [INFO] Step[800/2713]: training loss : 0.9359821724891663 TRAIN  loss dict:  {'classification_loss': 0.9359821724891663}
2025-01-19 14:16:29,883 [INFO] Step[850/2713]: training loss : 0.9362227368354797 TRAIN  loss dict:  {'classification_loss': 0.9362227368354797}
2025-01-19 14:16:41,370 [INFO] Step[900/2713]: training loss : 0.936818345785141 TRAIN  loss dict:  {'classification_loss': 0.936818345785141}
2025-01-19 14:16:52,879 [INFO] Step[950/2713]: training loss : 0.9374411225318908 TRAIN  loss dict:  {'classification_loss': 0.9374411225318908}
2025-01-19 14:17:04,366 [INFO] Step[1000/2713]: training loss : 0.9390810489654541 TRAIN  loss dict:  {'classification_loss': 0.9390810489654541}
2025-01-19 14:17:15,878 [INFO] Step[1050/2713]: training loss : 0.9351641595363617 TRAIN  loss dict:  {'classification_loss': 0.9351641595363617}
2025-01-19 14:17:27,338 [INFO] Step[1100/2713]: training loss : 0.937084070444107 TRAIN  loss dict:  {'classification_loss': 0.937084070444107}
2025-01-19 14:17:38,779 [INFO] Step[1150/2713]: training loss : 0.9385447955131531 TRAIN  loss dict:  {'classification_loss': 0.9385447955131531}
2025-01-19 14:17:50,292 [INFO] Step[1200/2713]: training loss : 0.938084626197815 TRAIN  loss dict:  {'classification_loss': 0.938084626197815}
2025-01-19 14:18:01,755 [INFO] Step[1250/2713]: training loss : 0.9389783072471619 TRAIN  loss dict:  {'classification_loss': 0.9389783072471619}
2025-01-19 14:18:13,227 [INFO] Step[1300/2713]: training loss : 0.9514134633541107 TRAIN  loss dict:  {'classification_loss': 0.9514134633541107}
2025-01-19 14:18:24,730 [INFO] Step[1350/2713]: training loss : 0.9356019306182861 TRAIN  loss dict:  {'classification_loss': 0.9356019306182861}
2025-01-19 14:18:36,222 [INFO] Step[1400/2713]: training loss : 0.9372996342182159 TRAIN  loss dict:  {'classification_loss': 0.9372996342182159}
2025-01-19 14:18:47,736 [INFO] Step[1450/2713]: training loss : 0.9366981208324432 TRAIN  loss dict:  {'classification_loss': 0.9366981208324432}
2025-01-19 14:18:59,222 [INFO] Step[1500/2713]: training loss : 0.9349466001987458 TRAIN  loss dict:  {'classification_loss': 0.9349466001987458}
2025-01-19 14:19:10,716 [INFO] Step[1550/2713]: training loss : 0.9358704519271851 TRAIN  loss dict:  {'classification_loss': 0.9358704519271851}
2025-01-19 14:19:22,226 [INFO] Step[1600/2713]: training loss : 0.9366793084144592 TRAIN  loss dict:  {'classification_loss': 0.9366793084144592}
2025-01-19 14:19:33,709 [INFO] Step[1650/2713]: training loss : 0.9383409035205841 TRAIN  loss dict:  {'classification_loss': 0.9383409035205841}
2025-01-19 14:19:45,214 [INFO] Step[1700/2713]: training loss : 0.9350232899188995 TRAIN  loss dict:  {'classification_loss': 0.9350232899188995}
2025-01-19 14:19:56,728 [INFO] Step[1750/2713]: training loss : 0.9364765632152557 TRAIN  loss dict:  {'classification_loss': 0.9364765632152557}
2025-01-19 14:20:08,181 [INFO] Step[1800/2713]: training loss : 0.9360231935977936 TRAIN  loss dict:  {'classification_loss': 0.9360231935977936}
2025-01-19 14:20:19,712 [INFO] Step[1850/2713]: training loss : 0.9358818781375885 TRAIN  loss dict:  {'classification_loss': 0.9358818781375885}
2025-01-19 14:20:31,222 [INFO] Step[1900/2713]: training loss : 0.9368788087368012 TRAIN  loss dict:  {'classification_loss': 0.9368788087368012}
2025-01-19 14:20:42,717 [INFO] Step[1950/2713]: training loss : 0.9346456491947174 TRAIN  loss dict:  {'classification_loss': 0.9346456491947174}
2025-01-19 14:20:54,215 [INFO] Step[2000/2713]: training loss : 0.9377664661407471 TRAIN  loss dict:  {'classification_loss': 0.9377664661407471}
2025-01-19 14:21:05,657 [INFO] Step[2050/2713]: training loss : 0.9385470378398896 TRAIN  loss dict:  {'classification_loss': 0.9385470378398896}
2025-01-19 14:21:17,105 [INFO] Step[2100/2713]: training loss : 0.937079839706421 TRAIN  loss dict:  {'classification_loss': 0.937079839706421}
2025-01-19 14:21:28,602 [INFO] Step[2150/2713]: training loss : 0.9352975571155548 TRAIN  loss dict:  {'classification_loss': 0.9352975571155548}
2025-01-19 14:21:40,083 [INFO] Step[2200/2713]: training loss : 0.9357930374145508 TRAIN  loss dict:  {'classification_loss': 0.9357930374145508}
2025-01-19 14:21:51,581 [INFO] Step[2250/2713]: training loss : 0.935297623872757 TRAIN  loss dict:  {'classification_loss': 0.935297623872757}
2025-01-19 14:22:03,046 [INFO] Step[2300/2713]: training loss : 0.9343177890777588 TRAIN  loss dict:  {'classification_loss': 0.9343177890777588}
2025-01-19 14:22:14,545 [INFO] Step[2350/2713]: training loss : 0.9354875564575196 TRAIN  loss dict:  {'classification_loss': 0.9354875564575196}
2025-01-19 14:22:26,050 [INFO] Step[2400/2713]: training loss : 0.9382081770896912 TRAIN  loss dict:  {'classification_loss': 0.9382081770896912}
2025-01-19 14:22:37,540 [INFO] Step[2450/2713]: training loss : 0.9379659247398376 TRAIN  loss dict:  {'classification_loss': 0.9379659247398376}
2025-01-19 14:22:49,054 [INFO] Step[2500/2713]: training loss : 0.9362334823608398 TRAIN  loss dict:  {'classification_loss': 0.9362334823608398}
2025-01-19 14:23:00,547 [INFO] Step[2550/2713]: training loss : 0.9358878827095032 TRAIN  loss dict:  {'classification_loss': 0.9358878827095032}
2025-01-19 14:23:12,054 [INFO] Step[2600/2713]: training loss : 0.9394602513313294 TRAIN  loss dict:  {'classification_loss': 0.9394602513313294}
2025-01-19 14:23:23,523 [INFO] Step[2650/2713]: training loss : 0.936328387260437 TRAIN  loss dict:  {'classification_loss': 0.936328387260437}
2025-01-19 14:23:35,064 [INFO] Step[2700/2713]: training loss : 0.9401320326328277 TRAIN  loss dict:  {'classification_loss': 0.9401320326328277}
2025-01-19 14:24:41,198 [INFO] Label accuracies statistics:
2025-01-19 14:24:41,198 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 0.75, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 1.0, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 1.0, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 14:24:41,200 [INFO] [96] TRAIN  loss: 0.9371362529115574 acc: 0.9998771347831429
2025-01-19 14:24:41,200 [INFO] [96] TRAIN  loss dict: {'classification_loss': 0.9371362529115574}
2025-01-19 14:24:41,200 [INFO] [96] VALIDATION loss: 1.7400964845840197 VALIDATION acc: 0.8100313479623824
2025-01-19 14:24:41,200 [INFO] [96] VALIDATION loss dict: {'classification_loss': 1.7400964845840197}
2025-01-19 14:24:41,200 [INFO] 
2025-01-19 14:24:57,978 [INFO] Step[50/2713]: training loss : 0.9344084465503693 TRAIN  loss dict:  {'classification_loss': 0.9344084465503693}
2025-01-19 14:25:09,464 [INFO] Step[100/2713]: training loss : 0.9386412692070008 TRAIN  loss dict:  {'classification_loss': 0.9386412692070008}
2025-01-19 14:25:20,980 [INFO] Step[150/2713]: training loss : 0.9368411612510681 TRAIN  loss dict:  {'classification_loss': 0.9368411612510681}
2025-01-19 14:25:32,482 [INFO] Step[200/2713]: training loss : 0.9366334569454193 TRAIN  loss dict:  {'classification_loss': 0.9366334569454193}
2025-01-19 14:25:43,950 [INFO] Step[250/2713]: training loss : 0.9363518643379212 TRAIN  loss dict:  {'classification_loss': 0.9363518643379212}
2025-01-19 14:25:55,435 [INFO] Step[300/2713]: training loss : 0.935901095867157 TRAIN  loss dict:  {'classification_loss': 0.935901095867157}
2025-01-19 14:26:06,921 [INFO] Step[350/2713]: training loss : 0.9372049176692963 TRAIN  loss dict:  {'classification_loss': 0.9372049176692963}
2025-01-19 14:26:18,388 [INFO] Step[400/2713]: training loss : 0.9385870683193207 TRAIN  loss dict:  {'classification_loss': 0.9385870683193207}
2025-01-19 14:26:29,861 [INFO] Step[450/2713]: training loss : 0.9427469384670257 TRAIN  loss dict:  {'classification_loss': 0.9427469384670257}
2025-01-19 14:26:41,356 [INFO] Step[500/2713]: training loss : 0.9356432890892029 TRAIN  loss dict:  {'classification_loss': 0.9356432890892029}
2025-01-19 14:26:52,812 [INFO] Step[550/2713]: training loss : 0.9363021910190582 TRAIN  loss dict:  {'classification_loss': 0.9363021910190582}
2025-01-19 14:27:04,291 [INFO] Step[600/2713]: training loss : 0.9363424265384674 TRAIN  loss dict:  {'classification_loss': 0.9363424265384674}
2025-01-19 14:27:15,785 [INFO] Step[650/2713]: training loss : 0.9368197810649872 TRAIN  loss dict:  {'classification_loss': 0.9368197810649872}
2025-01-19 14:27:27,266 [INFO] Step[700/2713]: training loss : 0.938746120929718 TRAIN  loss dict:  {'classification_loss': 0.938746120929718}
2025-01-19 14:27:38,736 [INFO] Step[750/2713]: training loss : 0.9359393489360809 TRAIN  loss dict:  {'classification_loss': 0.9359393489360809}
2025-01-19 14:27:50,223 [INFO] Step[800/2713]: training loss : 0.9383996450901031 TRAIN  loss dict:  {'classification_loss': 0.9383996450901031}
2025-01-19 14:28:01,708 [INFO] Step[850/2713]: training loss : 0.9366866540908814 TRAIN  loss dict:  {'classification_loss': 0.9366866540908814}
2025-01-19 14:28:13,176 [INFO] Step[900/2713]: training loss : 0.9363781654834747 TRAIN  loss dict:  {'classification_loss': 0.9363781654834747}
2025-01-19 14:28:24,644 [INFO] Step[950/2713]: training loss : 0.9368116700649262 TRAIN  loss dict:  {'classification_loss': 0.9368116700649262}
2025-01-19 14:28:36,113 [INFO] Step[1000/2713]: training loss : 0.9365095973014832 TRAIN  loss dict:  {'classification_loss': 0.9365095973014832}
2025-01-19 14:28:47,614 [INFO] Step[1050/2713]: training loss : 0.9368326139450073 TRAIN  loss dict:  {'classification_loss': 0.9368326139450073}
2025-01-19 14:28:59,089 [INFO] Step[1100/2713]: training loss : 0.9415436828136444 TRAIN  loss dict:  {'classification_loss': 0.9415436828136444}
2025-01-19 14:29:10,564 [INFO] Step[1150/2713]: training loss : 0.9385298180580139 TRAIN  loss dict:  {'classification_loss': 0.9385298180580139}
2025-01-19 14:29:22,067 [INFO] Step[1200/2713]: training loss : 0.937049150466919 TRAIN  loss dict:  {'classification_loss': 0.937049150466919}
2025-01-19 14:29:33,536 [INFO] Step[1250/2713]: training loss : 0.9355310881137848 TRAIN  loss dict:  {'classification_loss': 0.9355310881137848}
2025-01-19 14:29:45,011 [INFO] Step[1300/2713]: training loss : 0.9363589990139007 TRAIN  loss dict:  {'classification_loss': 0.9363589990139007}
2025-01-19 14:29:56,488 [INFO] Step[1350/2713]: training loss : 0.9366803133487701 TRAIN  loss dict:  {'classification_loss': 0.9366803133487701}
2025-01-19 14:30:07,958 [INFO] Step[1400/2713]: training loss : 0.9378303861618043 TRAIN  loss dict:  {'classification_loss': 0.9378303861618043}
2025-01-19 14:30:19,467 [INFO] Step[1450/2713]: training loss : 0.9357015311717987 TRAIN  loss dict:  {'classification_loss': 0.9357015311717987}
2025-01-19 14:30:30,931 [INFO] Step[1500/2713]: training loss : 0.9373298990726471 TRAIN  loss dict:  {'classification_loss': 0.9373298990726471}
2025-01-19 14:30:42,415 [INFO] Step[1550/2713]: training loss : 0.9357482349872589 TRAIN  loss dict:  {'classification_loss': 0.9357482349872589}
2025-01-19 14:30:53,906 [INFO] Step[1600/2713]: training loss : 0.9363341510295868 TRAIN  loss dict:  {'classification_loss': 0.9363341510295868}
2025-01-19 14:31:05,374 [INFO] Step[1650/2713]: training loss : 0.9349675393104553 TRAIN  loss dict:  {'classification_loss': 0.9349675393104553}
2025-01-19 14:31:16,835 [INFO] Step[1700/2713]: training loss : 0.9358058404922486 TRAIN  loss dict:  {'classification_loss': 0.9358058404922486}
2025-01-19 14:31:28,313 [INFO] Step[1750/2713]: training loss : 0.9348651051521302 TRAIN  loss dict:  {'classification_loss': 0.9348651051521302}
2025-01-19 14:31:39,808 [INFO] Step[1800/2713]: training loss : 0.9356756424903869 TRAIN  loss dict:  {'classification_loss': 0.9356756424903869}
2025-01-19 14:31:51,305 [INFO] Step[1850/2713]: training loss : 0.9362388980388642 TRAIN  loss dict:  {'classification_loss': 0.9362388980388642}
2025-01-19 14:32:02,807 [INFO] Step[1900/2713]: training loss : 0.9368064701557159 TRAIN  loss dict:  {'classification_loss': 0.9368064701557159}
2025-01-19 14:32:14,290 [INFO] Step[1950/2713]: training loss : 0.9371274733543395 TRAIN  loss dict:  {'classification_loss': 0.9371274733543395}
2025-01-19 14:32:25,794 [INFO] Step[2000/2713]: training loss : 0.9357060062885284 TRAIN  loss dict:  {'classification_loss': 0.9357060062885284}
2025-01-19 14:32:37,278 [INFO] Step[2050/2713]: training loss : 0.9353523504734039 TRAIN  loss dict:  {'classification_loss': 0.9353523504734039}
2025-01-19 14:32:48,764 [INFO] Step[2100/2713]: training loss : 0.9363156890869141 TRAIN  loss dict:  {'classification_loss': 0.9363156890869141}
2025-01-19 14:33:00,281 [INFO] Step[2150/2713]: training loss : 0.949443975687027 TRAIN  loss dict:  {'classification_loss': 0.949443975687027}
2025-01-19 14:33:11,771 [INFO] Step[2200/2713]: training loss : 0.9369539511203766 TRAIN  loss dict:  {'classification_loss': 0.9369539511203766}
2025-01-19 14:33:23,289 [INFO] Step[2250/2713]: training loss : 0.9367333543300629 TRAIN  loss dict:  {'classification_loss': 0.9367333543300629}
2025-01-19 14:33:34,799 [INFO] Step[2300/2713]: training loss : 0.9381128811836242 TRAIN  loss dict:  {'classification_loss': 0.9381128811836242}
2025-01-19 14:33:46,280 [INFO] Step[2350/2713]: training loss : 0.9357398045063019 TRAIN  loss dict:  {'classification_loss': 0.9357398045063019}
2025-01-19 14:33:57,747 [INFO] Step[2400/2713]: training loss : 0.9348945820331573 TRAIN  loss dict:  {'classification_loss': 0.9348945820331573}
2025-01-19 14:34:09,306 [INFO] Step[2450/2713]: training loss : 0.936488332748413 TRAIN  loss dict:  {'classification_loss': 0.936488332748413}
2025-01-19 14:34:20,798 [INFO] Step[2500/2713]: training loss : 0.9375194323062896 TRAIN  loss dict:  {'classification_loss': 0.9375194323062896}
2025-01-19 14:34:32,301 [INFO] Step[2550/2713]: training loss : 0.9362655758857727 TRAIN  loss dict:  {'classification_loss': 0.9362655758857727}
2025-01-19 14:34:43,797 [INFO] Step[2600/2713]: training loss : 0.9368367516994476 TRAIN  loss dict:  {'classification_loss': 0.9368367516994476}
2025-01-19 14:34:55,321 [INFO] Step[2650/2713]: training loss : 0.936095472574234 TRAIN  loss dict:  {'classification_loss': 0.936095472574234}
2025-01-19 14:35:06,805 [INFO] Step[2700/2713]: training loss : 0.9355882406234741 TRAIN  loss dict:  {'classification_loss': 0.9355882406234741}
2025-01-19 14:36:13,390 [INFO] Label accuracies statistics:
2025-01-19 14:36:13,390 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 0.75, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 1.0, 215: 1.0, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.5, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 1.0, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 14:36:13,391 [INFO] [97] TRAIN  loss: 0.9369816299166751 acc: 1.0
2025-01-19 14:36:13,391 [INFO] [97] TRAIN  loss dict: {'classification_loss': 0.9369816299166751}
2025-01-19 14:36:13,392 [INFO] [97] VALIDATION loss: 1.7286484817365058 VALIDATION acc: 0.8043887147335423
2025-01-19 14:36:13,392 [INFO] [97] VALIDATION loss dict: {'classification_loss': 1.7286484817365058}
2025-01-19 14:36:13,392 [INFO] 
2025-01-19 14:36:30,143 [INFO] Step[50/2713]: training loss : 0.9402088654041291 TRAIN  loss dict:  {'classification_loss': 0.9402088654041291}
2025-01-19 14:36:41,652 [INFO] Step[100/2713]: training loss : 0.9342238426208496 TRAIN  loss dict:  {'classification_loss': 0.9342238426208496}
2025-01-19 14:36:53,189 [INFO] Step[150/2713]: training loss : 0.9348610126972199 TRAIN  loss dict:  {'classification_loss': 0.9348610126972199}
2025-01-19 14:37:04,674 [INFO] Step[200/2713]: training loss : 0.9346369183063508 TRAIN  loss dict:  {'classification_loss': 0.9346369183063508}
2025-01-19 14:37:16,150 [INFO] Step[250/2713]: training loss : 0.9377300703525543 TRAIN  loss dict:  {'classification_loss': 0.9377300703525543}
2025-01-19 14:37:27,660 [INFO] Step[300/2713]: training loss : 0.9352601480484009 TRAIN  loss dict:  {'classification_loss': 0.9352601480484009}
2025-01-19 14:37:39,184 [INFO] Step[350/2713]: training loss : 0.9386107158660889 TRAIN  loss dict:  {'classification_loss': 0.9386107158660889}
2025-01-19 14:37:50,716 [INFO] Step[400/2713]: training loss : 0.935864098072052 TRAIN  loss dict:  {'classification_loss': 0.935864098072052}
2025-01-19 14:38:02,213 [INFO] Step[450/2713]: training loss : 0.939553462266922 TRAIN  loss dict:  {'classification_loss': 0.939553462266922}
2025-01-19 14:38:13,701 [INFO] Step[500/2713]: training loss : 0.9378514647483825 TRAIN  loss dict:  {'classification_loss': 0.9378514647483825}
2025-01-19 14:38:25,222 [INFO] Step[550/2713]: training loss : 0.9349966168403625 TRAIN  loss dict:  {'classification_loss': 0.9349966168403625}
2025-01-19 14:38:36,752 [INFO] Step[600/2713]: training loss : 0.9375909209251404 TRAIN  loss dict:  {'classification_loss': 0.9375909209251404}
2025-01-19 14:38:48,251 [INFO] Step[650/2713]: training loss : 0.9365881848335266 TRAIN  loss dict:  {'classification_loss': 0.9365881848335266}
2025-01-19 14:38:59,791 [INFO] Step[700/2713]: training loss : 0.9362387681007385 TRAIN  loss dict:  {'classification_loss': 0.9362387681007385}
2025-01-19 14:39:11,290 [INFO] Step[750/2713]: training loss : 0.9457094824314117 TRAIN  loss dict:  {'classification_loss': 0.9457094824314117}
2025-01-19 14:39:22,778 [INFO] Step[800/2713]: training loss : 0.9365169787406922 TRAIN  loss dict:  {'classification_loss': 0.9365169787406922}
2025-01-19 14:39:34,294 [INFO] Step[850/2713]: training loss : 0.9383112549781799 TRAIN  loss dict:  {'classification_loss': 0.9383112549781799}
2025-01-19 14:39:45,803 [INFO] Step[900/2713]: training loss : 0.9382834911346436 TRAIN  loss dict:  {'classification_loss': 0.9382834911346436}
2025-01-19 14:39:57,305 [INFO] Step[950/2713]: training loss : 0.9358543539047242 TRAIN  loss dict:  {'classification_loss': 0.9358543539047242}
2025-01-19 14:40:08,819 [INFO] Step[1000/2713]: training loss : 0.9439117252826691 TRAIN  loss dict:  {'classification_loss': 0.9439117252826691}
2025-01-19 14:40:20,283 [INFO] Step[1050/2713]: training loss : 0.9362195634841919 TRAIN  loss dict:  {'classification_loss': 0.9362195634841919}
2025-01-19 14:40:31,770 [INFO] Step[1100/2713]: training loss : 0.9359605097770691 TRAIN  loss dict:  {'classification_loss': 0.9359605097770691}
2025-01-19 14:40:43,240 [INFO] Step[1150/2713]: training loss : 0.9365261006355285 TRAIN  loss dict:  {'classification_loss': 0.9365261006355285}
2025-01-19 14:40:54,732 [INFO] Step[1200/2713]: training loss : 0.9357081389427185 TRAIN  loss dict:  {'classification_loss': 0.9357081389427185}
2025-01-19 14:41:06,230 [INFO] Step[1250/2713]: training loss : 0.9360611772537232 TRAIN  loss dict:  {'classification_loss': 0.9360611772537232}
2025-01-19 14:41:17,718 [INFO] Step[1300/2713]: training loss : 0.9361807405948639 TRAIN  loss dict:  {'classification_loss': 0.9361807405948639}
2025-01-19 14:41:29,243 [INFO] Step[1350/2713]: training loss : 0.9376388013362884 TRAIN  loss dict:  {'classification_loss': 0.9376388013362884}
2025-01-19 14:41:40,739 [INFO] Step[1400/2713]: training loss : 0.9403698718547822 TRAIN  loss dict:  {'classification_loss': 0.9403698718547822}
2025-01-19 14:41:52,243 [INFO] Step[1450/2713]: training loss : 0.9349335038661957 TRAIN  loss dict:  {'classification_loss': 0.9349335038661957}
2025-01-19 14:42:03,759 [INFO] Step[1500/2713]: training loss : 0.9356903207302093 TRAIN  loss dict:  {'classification_loss': 0.9356903207302093}
2025-01-19 14:42:15,256 [INFO] Step[1550/2713]: training loss : 0.9395261776447296 TRAIN  loss dict:  {'classification_loss': 0.9395261776447296}
2025-01-19 14:42:26,730 [INFO] Step[1600/2713]: training loss : 0.9389803123474121 TRAIN  loss dict:  {'classification_loss': 0.9389803123474121}
2025-01-19 14:42:38,241 [INFO] Step[1650/2713]: training loss : 0.9369771015644074 TRAIN  loss dict:  {'classification_loss': 0.9369771015644074}
2025-01-19 14:42:49,739 [INFO] Step[1700/2713]: training loss : 0.9376080083847046 TRAIN  loss dict:  {'classification_loss': 0.9376080083847046}
2025-01-19 14:43:01,246 [INFO] Step[1750/2713]: training loss : 0.9348969602584839 TRAIN  loss dict:  {'classification_loss': 0.9348969602584839}
2025-01-19 14:43:12,747 [INFO] Step[1800/2713]: training loss : 0.9352891385555268 TRAIN  loss dict:  {'classification_loss': 0.9352891385555268}
2025-01-19 14:43:24,234 [INFO] Step[1850/2713]: training loss : 0.9359823441505433 TRAIN  loss dict:  {'classification_loss': 0.9359823441505433}
2025-01-19 14:43:35,762 [INFO] Step[1900/2713]: training loss : 0.9367306363582611 TRAIN  loss dict:  {'classification_loss': 0.9367306363582611}
2025-01-19 14:43:47,250 [INFO] Step[1950/2713]: training loss : 0.9341453409194946 TRAIN  loss dict:  {'classification_loss': 0.9341453409194946}
2025-01-19 14:43:58,729 [INFO] Step[2000/2713]: training loss : 0.9369335067272186 TRAIN  loss dict:  {'classification_loss': 0.9369335067272186}
2025-01-19 14:44:10,233 [INFO] Step[2050/2713]: training loss : 0.9390562331676483 TRAIN  loss dict:  {'classification_loss': 0.9390562331676483}
2025-01-19 14:44:21,713 [INFO] Step[2100/2713]: training loss : 0.9370040190219879 TRAIN  loss dict:  {'classification_loss': 0.9370040190219879}
2025-01-19 14:44:33,192 [INFO] Step[2150/2713]: training loss : 0.9340602052211762 TRAIN  loss dict:  {'classification_loss': 0.9340602052211762}
2025-01-19 14:44:44,702 [INFO] Step[2200/2713]: training loss : 0.9338652527332306 TRAIN  loss dict:  {'classification_loss': 0.9338652527332306}
2025-01-19 14:44:56,188 [INFO] Step[2250/2713]: training loss : 0.9372871828079223 TRAIN  loss dict:  {'classification_loss': 0.9372871828079223}
2025-01-19 14:45:07,734 [INFO] Step[2300/2713]: training loss : 0.9353865444660187 TRAIN  loss dict:  {'classification_loss': 0.9353865444660187}
2025-01-19 14:45:19,227 [INFO] Step[2350/2713]: training loss : 0.9361255609989166 TRAIN  loss dict:  {'classification_loss': 0.9361255609989166}
2025-01-19 14:45:30,710 [INFO] Step[2400/2713]: training loss : 0.9367407035827636 TRAIN  loss dict:  {'classification_loss': 0.9367407035827636}
2025-01-19 14:45:42,215 [INFO] Step[2450/2713]: training loss : 0.9355921685695648 TRAIN  loss dict:  {'classification_loss': 0.9355921685695648}
2025-01-19 14:45:53,755 [INFO] Step[2500/2713]: training loss : 0.9362831902503967 TRAIN  loss dict:  {'classification_loss': 0.9362831902503967}
2025-01-19 14:46:05,260 [INFO] Step[2550/2713]: training loss : 0.9357852470874787 TRAIN  loss dict:  {'classification_loss': 0.9357852470874787}
2025-01-19 14:46:16,781 [INFO] Step[2600/2713]: training loss : 0.9371383452415466 TRAIN  loss dict:  {'classification_loss': 0.9371383452415466}
2025-01-19 14:46:28,309 [INFO] Step[2650/2713]: training loss : 0.9343669950962067 TRAIN  loss dict:  {'classification_loss': 0.9343669950962067}
2025-01-19 14:46:39,804 [INFO] Step[2700/2713]: training loss : 0.9387695252895355 TRAIN  loss dict:  {'classification_loss': 0.9387695252895355}
2025-01-19 14:47:45,867 [INFO] Label accuracies statistics:
2025-01-19 14:47:45,867 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 1.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 14:47:45,869 [INFO] [98] TRAIN  loss: 0.9368906679998849 acc: 0.9998771347831429
2025-01-19 14:47:45,869 [INFO] [98] TRAIN  loss dict: {'classification_loss': 0.9368906679998849}
2025-01-19 14:47:45,869 [INFO] [98] VALIDATION loss: 1.7414191222952722 VALIDATION acc: 0.8100313479623824
2025-01-19 14:47:45,869 [INFO] [98] VALIDATION loss dict: {'classification_loss': 1.7414191222952722}
2025-01-19 14:47:45,869 [INFO] 
2025-01-19 14:48:02,342 [INFO] Step[50/2713]: training loss : 0.9344049966335297 TRAIN  loss dict:  {'classification_loss': 0.9344049966335297}
2025-01-19 14:48:13,783 [INFO] Step[100/2713]: training loss : 0.9370605456829071 TRAIN  loss dict:  {'classification_loss': 0.9370605456829071}
2025-01-19 14:48:25,272 [INFO] Step[150/2713]: training loss : 0.9381611430644989 TRAIN  loss dict:  {'classification_loss': 0.9381611430644989}
2025-01-19 14:48:36,734 [INFO] Step[200/2713]: training loss : 0.936283643245697 TRAIN  loss dict:  {'classification_loss': 0.936283643245697}
2025-01-19 14:48:48,185 [INFO] Step[250/2713]: training loss : 0.9352003967761994 TRAIN  loss dict:  {'classification_loss': 0.9352003967761994}
2025-01-19 14:48:59,690 [INFO] Step[300/2713]: training loss : 0.9348062694072723 TRAIN  loss dict:  {'classification_loss': 0.9348062694072723}
2025-01-19 14:49:11,184 [INFO] Step[350/2713]: training loss : 0.9367185711860657 TRAIN  loss dict:  {'classification_loss': 0.9367185711860657}
2025-01-19 14:49:22,660 [INFO] Step[400/2713]: training loss : 0.9390928602218628 TRAIN  loss dict:  {'classification_loss': 0.9390928602218628}
2025-01-19 14:49:34,123 [INFO] Step[450/2713]: training loss : 0.9358862733840942 TRAIN  loss dict:  {'classification_loss': 0.9358862733840942}
2025-01-19 14:49:45,560 [INFO] Step[500/2713]: training loss : 0.9368704056739807 TRAIN  loss dict:  {'classification_loss': 0.9368704056739807}
2025-01-19 14:49:57,024 [INFO] Step[550/2713]: training loss : 0.9359660649299621 TRAIN  loss dict:  {'classification_loss': 0.9359660649299621}
2025-01-19 14:50:08,507 [INFO] Step[600/2713]: training loss : 0.9350233769416809 TRAIN  loss dict:  {'classification_loss': 0.9350233769416809}
2025-01-19 14:50:19,965 [INFO] Step[650/2713]: training loss : 0.9340589797496796 TRAIN  loss dict:  {'classification_loss': 0.9340589797496796}
2025-01-19 14:50:31,460 [INFO] Step[700/2713]: training loss : 0.9351771306991578 TRAIN  loss dict:  {'classification_loss': 0.9351771306991578}
2025-01-19 14:50:42,915 [INFO] Step[750/2713]: training loss : 0.9353723418712616 TRAIN  loss dict:  {'classification_loss': 0.9353723418712616}
2025-01-19 14:50:54,396 [INFO] Step[800/2713]: training loss : 0.935790901184082 TRAIN  loss dict:  {'classification_loss': 0.935790901184082}
2025-01-19 14:51:05,882 [INFO] Step[850/2713]: training loss : 0.9354792535305023 TRAIN  loss dict:  {'classification_loss': 0.9354792535305023}
2025-01-19 14:51:17,346 [INFO] Step[900/2713]: training loss : 0.9361176216602325 TRAIN  loss dict:  {'classification_loss': 0.9361176216602325}
2025-01-19 14:51:28,870 [INFO] Step[950/2713]: training loss : 0.9365909385681153 TRAIN  loss dict:  {'classification_loss': 0.9365909385681153}
2025-01-19 14:51:40,327 [INFO] Step[1000/2713]: training loss : 0.9338806998729706 TRAIN  loss dict:  {'classification_loss': 0.9338806998729706}
2025-01-19 14:51:51,808 [INFO] Step[1050/2713]: training loss : 0.9372699844837189 TRAIN  loss dict:  {'classification_loss': 0.9372699844837189}
2025-01-19 14:52:03,289 [INFO] Step[1100/2713]: training loss : 0.9388859856128693 TRAIN  loss dict:  {'classification_loss': 0.9388859856128693}
2025-01-19 14:52:14,762 [INFO] Step[1150/2713]: training loss : 0.9362534177303314 TRAIN  loss dict:  {'classification_loss': 0.9362534177303314}
2025-01-19 14:52:26,262 [INFO] Step[1200/2713]: training loss : 0.9390248167514801 TRAIN  loss dict:  {'classification_loss': 0.9390248167514801}
2025-01-19 14:52:37,796 [INFO] Step[1250/2713]: training loss : 0.9337460052967071 TRAIN  loss dict:  {'classification_loss': 0.9337460052967071}
2025-01-19 14:52:49,306 [INFO] Step[1300/2713]: training loss : 0.9381145739555359 TRAIN  loss dict:  {'classification_loss': 0.9381145739555359}
2025-01-19 14:53:00,763 [INFO] Step[1350/2713]: training loss : 0.9402797973155975 TRAIN  loss dict:  {'classification_loss': 0.9402797973155975}
2025-01-19 14:53:12,255 [INFO] Step[1400/2713]: training loss : 0.9369143831729889 TRAIN  loss dict:  {'classification_loss': 0.9369143831729889}
2025-01-19 14:53:23,729 [INFO] Step[1450/2713]: training loss : 0.9443191802501678 TRAIN  loss dict:  {'classification_loss': 0.9443191802501678}
2025-01-19 14:53:35,191 [INFO] Step[1500/2713]: training loss : 0.9357222342491149 TRAIN  loss dict:  {'classification_loss': 0.9357222342491149}
2025-01-19 14:53:46,682 [INFO] Step[1550/2713]: training loss : 0.9358949875831604 TRAIN  loss dict:  {'classification_loss': 0.9358949875831604}
2025-01-19 14:53:58,131 [INFO] Step[1600/2713]: training loss : 0.9362888658046722 TRAIN  loss dict:  {'classification_loss': 0.9362888658046722}
2025-01-19 14:54:09,601 [INFO] Step[1650/2713]: training loss : 0.9342947137355805 TRAIN  loss dict:  {'classification_loss': 0.9342947137355805}
2025-01-19 14:54:21,064 [INFO] Step[1700/2713]: training loss : 0.9388604414463043 TRAIN  loss dict:  {'classification_loss': 0.9388604414463043}
2025-01-19 14:54:32,512 [INFO] Step[1750/2713]: training loss : 0.9345309686660767 TRAIN  loss dict:  {'classification_loss': 0.9345309686660767}
2025-01-19 14:54:43,999 [INFO] Step[1800/2713]: training loss : 0.9412462913990021 TRAIN  loss dict:  {'classification_loss': 0.9412462913990021}
2025-01-19 14:54:55,513 [INFO] Step[1850/2713]: training loss : 0.9373602867126465 TRAIN  loss dict:  {'classification_loss': 0.9373602867126465}
2025-01-19 14:55:07,004 [INFO] Step[1900/2713]: training loss : 0.9364083814620972 TRAIN  loss dict:  {'classification_loss': 0.9364083814620972}
2025-01-19 14:55:18,480 [INFO] Step[1950/2713]: training loss : 0.9364833271503449 TRAIN  loss dict:  {'classification_loss': 0.9364833271503449}
2025-01-19 14:55:29,938 [INFO] Step[2000/2713]: training loss : 0.9344494843482971 TRAIN  loss dict:  {'classification_loss': 0.9344494843482971}
2025-01-19 14:55:41,406 [INFO] Step[2050/2713]: training loss : 0.9370592248439789 TRAIN  loss dict:  {'classification_loss': 0.9370592248439789}
2025-01-19 14:55:52,862 [INFO] Step[2100/2713]: training loss : 0.9355939888954162 TRAIN  loss dict:  {'classification_loss': 0.9355939888954162}
2025-01-19 14:56:04,350 [INFO] Step[2150/2713]: training loss : 0.9363925075531006 TRAIN  loss dict:  {'classification_loss': 0.9363925075531006}
2025-01-19 14:56:15,851 [INFO] Step[2200/2713]: training loss : 0.9354575431346893 TRAIN  loss dict:  {'classification_loss': 0.9354575431346893}
2025-01-19 14:56:27,362 [INFO] Step[2250/2713]: training loss : 0.9366894078254699 TRAIN  loss dict:  {'classification_loss': 0.9366894078254699}
2025-01-19 14:56:38,838 [INFO] Step[2300/2713]: training loss : 0.9358924841880798 TRAIN  loss dict:  {'classification_loss': 0.9358924841880798}
2025-01-19 14:56:50,304 [INFO] Step[2350/2713]: training loss : 0.9333085656166077 TRAIN  loss dict:  {'classification_loss': 0.9333085656166077}
2025-01-19 14:57:01,819 [INFO] Step[2400/2713]: training loss : 0.9615068709850312 TRAIN  loss dict:  {'classification_loss': 0.9615068709850312}
2025-01-19 14:57:13,293 [INFO] Step[2450/2713]: training loss : 0.9356329786777496 TRAIN  loss dict:  {'classification_loss': 0.9356329786777496}
2025-01-19 14:57:24,806 [INFO] Step[2500/2713]: training loss : 0.9348419618606567 TRAIN  loss dict:  {'classification_loss': 0.9348419618606567}
2025-01-19 14:57:36,294 [INFO] Step[2550/2713]: training loss : 0.9356551778316498 TRAIN  loss dict:  {'classification_loss': 0.9356551778316498}
2025-01-19 14:57:47,774 [INFO] Step[2600/2713]: training loss : 0.9366208970546722 TRAIN  loss dict:  {'classification_loss': 0.9366208970546722}
2025-01-19 14:57:59,214 [INFO] Step[2650/2713]: training loss : 0.9355060768127441 TRAIN  loss dict:  {'classification_loss': 0.9355060768127441}
2025-01-19 14:58:10,716 [INFO] Step[2700/2713]: training loss : 0.9372550082206726 TRAIN  loss dict:  {'classification_loss': 0.9372550082206726}
2025-01-19 14:59:16,719 [INFO] Label accuracies statistics:
2025-01-19 14:59:16,719 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 14:59:16,721 [INFO] [99] TRAIN  loss: 0.9368745470855494 acc: 0.9997542695662858
2025-01-19 14:59:16,721 [INFO] [99] TRAIN  loss dict: {'classification_loss': 0.9368745470855494}
2025-01-19 14:59:16,721 [INFO] [99] VALIDATION loss: 1.716348592163925 VALIDATION acc: 0.812539184952978
2025-01-19 14:59:16,722 [INFO] [99] VALIDATION loss dict: {'classification_loss': 1.716348592163925}
2025-01-19 14:59:16,722 [INFO] 
2025-01-19 14:59:38,323 [INFO] Step[50/2713]: training loss : 0.9359507143497467 TRAIN  loss dict:  {'classification_loss': 0.9359507143497467}
2025-01-19 14:59:49,789 [INFO] Step[100/2713]: training loss : 0.9365990662574768 TRAIN  loss dict:  {'classification_loss': 0.9365990662574768}
2025-01-19 15:00:01,299 [INFO] Step[150/2713]: training loss : 0.9346389067173004 TRAIN  loss dict:  {'classification_loss': 0.9346389067173004}
2025-01-19 15:00:12,777 [INFO] Step[200/2713]: training loss : 0.9364890658855438 TRAIN  loss dict:  {'classification_loss': 0.9364890658855438}
2025-01-19 15:00:24,244 [INFO] Step[250/2713]: training loss : 0.9364792931079865 TRAIN  loss dict:  {'classification_loss': 0.9364792931079865}
2025-01-19 15:00:35,714 [INFO] Step[300/2713]: training loss : 0.9348336172103882 TRAIN  loss dict:  {'classification_loss': 0.9348336172103882}
2025-01-19 15:00:47,205 [INFO] Step[350/2713]: training loss : 0.934858832359314 TRAIN  loss dict:  {'classification_loss': 0.934858832359314}
2025-01-19 15:00:58,692 [INFO] Step[400/2713]: training loss : 0.9353714227676392 TRAIN  loss dict:  {'classification_loss': 0.9353714227676392}
2025-01-19 15:01:10,189 [INFO] Step[450/2713]: training loss : 0.936082570552826 TRAIN  loss dict:  {'classification_loss': 0.936082570552826}
2025-01-19 15:01:21,648 [INFO] Step[500/2713]: training loss : 0.9382387077808381 TRAIN  loss dict:  {'classification_loss': 0.9382387077808381}
2025-01-19 15:01:33,126 [INFO] Step[550/2713]: training loss : 0.9442816209793091 TRAIN  loss dict:  {'classification_loss': 0.9442816209793091}
2025-01-19 15:01:44,560 [INFO] Step[600/2713]: training loss : 0.9361347603797913 TRAIN  loss dict:  {'classification_loss': 0.9361347603797913}
2025-01-19 15:01:55,998 [INFO] Step[650/2713]: training loss : 0.9346384251117706 TRAIN  loss dict:  {'classification_loss': 0.9346384251117706}
2025-01-19 15:02:07,486 [INFO] Step[700/2713]: training loss : 0.935342345237732 TRAIN  loss dict:  {'classification_loss': 0.935342345237732}
2025-01-19 15:02:18,968 [INFO] Step[750/2713]: training loss : 0.9351329708099365 TRAIN  loss dict:  {'classification_loss': 0.9351329708099365}
2025-01-19 15:02:30,442 [INFO] Step[800/2713]: training loss : 0.9356786680221557 TRAIN  loss dict:  {'classification_loss': 0.9356786680221557}
2025-01-19 15:02:41,915 [INFO] Step[850/2713]: training loss : 0.9349779856204986 TRAIN  loss dict:  {'classification_loss': 0.9349779856204986}
2025-01-19 15:02:53,393 [INFO] Step[900/2713]: training loss : 0.9372387933731079 TRAIN  loss dict:  {'classification_loss': 0.9372387933731079}
2025-01-19 15:03:04,902 [INFO] Step[950/2713]: training loss : 0.9346071326732636 TRAIN  loss dict:  {'classification_loss': 0.9346071326732636}
2025-01-19 15:03:16,374 [INFO] Step[1000/2713]: training loss : 0.9356375372409821 TRAIN  loss dict:  {'classification_loss': 0.9356375372409821}
2025-01-19 15:03:27,889 [INFO] Step[1050/2713]: training loss : 0.935983475446701 TRAIN  loss dict:  {'classification_loss': 0.935983475446701}
2025-01-19 15:03:39,370 [INFO] Step[1100/2713]: training loss : 0.9356433570384979 TRAIN  loss dict:  {'classification_loss': 0.9356433570384979}
2025-01-19 15:03:50,830 [INFO] Step[1150/2713]: training loss : 0.9353446877002716 TRAIN  loss dict:  {'classification_loss': 0.9353446877002716}
2025-01-19 15:04:02,325 [INFO] Step[1200/2713]: training loss : 0.9363526105880737 TRAIN  loss dict:  {'classification_loss': 0.9363526105880737}
2025-01-19 15:04:13,811 [INFO] Step[1250/2713]: training loss : 0.9363595950603485 TRAIN  loss dict:  {'classification_loss': 0.9363595950603485}
2025-01-19 15:04:25,304 [INFO] Step[1300/2713]: training loss : 0.9351638317108154 TRAIN  loss dict:  {'classification_loss': 0.9351638317108154}
2025-01-19 15:04:36,764 [INFO] Step[1350/2713]: training loss : 0.9393012738227844 TRAIN  loss dict:  {'classification_loss': 0.9393012738227844}
2025-01-19 15:04:48,249 [INFO] Step[1400/2713]: training loss : 0.9373783814907074 TRAIN  loss dict:  {'classification_loss': 0.9373783814907074}
2025-01-19 15:04:59,717 [INFO] Step[1450/2713]: training loss : 0.936564815044403 TRAIN  loss dict:  {'classification_loss': 0.936564815044403}
2025-01-19 15:05:11,180 [INFO] Step[1500/2713]: training loss : 0.9389035797119141 TRAIN  loss dict:  {'classification_loss': 0.9389035797119141}
2025-01-19 15:05:22,697 [INFO] Step[1550/2713]: training loss : 0.9370354282855987 TRAIN  loss dict:  {'classification_loss': 0.9370354282855987}
2025-01-19 15:05:34,169 [INFO] Step[1600/2713]: training loss : 0.9355066251754761 TRAIN  loss dict:  {'classification_loss': 0.9355066251754761}
2025-01-19 15:05:45,677 [INFO] Step[1650/2713]: training loss : 0.9369439899921417 TRAIN  loss dict:  {'classification_loss': 0.9369439899921417}
2025-01-19 15:05:57,146 [INFO] Step[1700/2713]: training loss : 0.9366918110847473 TRAIN  loss dict:  {'classification_loss': 0.9366918110847473}
2025-01-19 15:06:08,634 [INFO] Step[1750/2713]: training loss : 0.9361258375644684 TRAIN  loss dict:  {'classification_loss': 0.9361258375644684}
2025-01-19 15:06:20,124 [INFO] Step[1800/2713]: training loss : 0.9358102643489837 TRAIN  loss dict:  {'classification_loss': 0.9358102643489837}
2025-01-19 15:06:31,598 [INFO] Step[1850/2713]: training loss : 0.9358894157409668 TRAIN  loss dict:  {'classification_loss': 0.9358894157409668}
2025-01-19 15:06:43,079 [INFO] Step[1900/2713]: training loss : 0.9366039180755615 TRAIN  loss dict:  {'classification_loss': 0.9366039180755615}
2025-01-19 15:06:54,567 [INFO] Step[1950/2713]: training loss : 0.9384244072437287 TRAIN  loss dict:  {'classification_loss': 0.9384244072437287}
2025-01-19 15:07:06,033 [INFO] Step[2000/2713]: training loss : 0.9358292329311371 TRAIN  loss dict:  {'classification_loss': 0.9358292329311371}
2025-01-19 15:07:17,522 [INFO] Step[2050/2713]: training loss : 0.9364764261245727 TRAIN  loss dict:  {'classification_loss': 0.9364764261245727}
2025-01-19 15:07:28,988 [INFO] Step[2100/2713]: training loss : 0.9350213706493378 TRAIN  loss dict:  {'classification_loss': 0.9350213706493378}
2025-01-19 15:07:40,491 [INFO] Step[2150/2713]: training loss : 0.9357464087009429 TRAIN  loss dict:  {'classification_loss': 0.9357464087009429}
2025-01-19 15:07:51,962 [INFO] Step[2200/2713]: training loss : 0.9366302645206451 TRAIN  loss dict:  {'classification_loss': 0.9366302645206451}
2025-01-19 15:08:03,458 [INFO] Step[2250/2713]: training loss : 0.936281304359436 TRAIN  loss dict:  {'classification_loss': 0.936281304359436}
2025-01-19 15:08:14,931 [INFO] Step[2300/2713]: training loss : 0.9358728408813477 TRAIN  loss dict:  {'classification_loss': 0.9358728408813477}
2025-01-19 15:08:26,405 [INFO] Step[2350/2713]: training loss : 0.9351295113563538 TRAIN  loss dict:  {'classification_loss': 0.9351295113563538}
2025-01-19 15:08:37,860 [INFO] Step[2400/2713]: training loss : 0.934019478559494 TRAIN  loss dict:  {'classification_loss': 0.934019478559494}
2025-01-19 15:08:49,313 [INFO] Step[2450/2713]: training loss : 0.9363117277622223 TRAIN  loss dict:  {'classification_loss': 0.9363117277622223}
2025-01-19 15:09:00,813 [INFO] Step[2500/2713]: training loss : 0.9344616734981537 TRAIN  loss dict:  {'classification_loss': 0.9344616734981537}
2025-01-19 15:09:12,304 [INFO] Step[2550/2713]: training loss : 0.9366272258758545 TRAIN  loss dict:  {'classification_loss': 0.9366272258758545}
2025-01-19 15:09:23,735 [INFO] Step[2600/2713]: training loss : 0.9379001927375793 TRAIN  loss dict:  {'classification_loss': 0.9379001927375793}
2025-01-19 15:09:35,254 [INFO] Step[2650/2713]: training loss : 0.9352443206310272 TRAIN  loss dict:  {'classification_loss': 0.9352443206310272}
2025-01-19 15:09:46,748 [INFO] Step[2700/2713]: training loss : 0.9360148191452027 TRAIN  loss dict:  {'classification_loss': 0.9360148191452027}
2025-01-19 15:10:52,713 [INFO] Label accuracies statistics:
2025-01-19 15:10:52,713 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.75, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 1.0, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 15:10:52,714 [INFO] [100] TRAIN  loss: 0.9362226265011966 acc: 1.0
2025-01-19 15:10:52,714 [INFO] [100] TRAIN  loss dict: {'classification_loss': 0.9362226265011966}
2025-01-19 15:10:52,715 [INFO] [100] VALIDATION loss: 1.7028373972813886 VALIDATION acc: 0.819435736677116
2025-01-19 15:10:52,715 [INFO] [100] VALIDATION loss dict: {'classification_loss': 1.7028373972813886}
2025-01-19 15:10:52,715 [INFO] 
2025-01-19 15:11:09,064 [INFO] Step[50/2713]: training loss : 0.9348271095752716 TRAIN  loss dict:  {'classification_loss': 0.9348271095752716}
2025-01-19 15:11:20,507 [INFO] Step[100/2713]: training loss : 0.9364235353469849 TRAIN  loss dict:  {'classification_loss': 0.9364235353469849}
2025-01-19 15:11:31,990 [INFO] Step[150/2713]: training loss : 0.93742351770401 TRAIN  loss dict:  {'classification_loss': 0.93742351770401}
2025-01-19 15:11:43,453 [INFO] Step[200/2713]: training loss : 0.9360321497917176 TRAIN  loss dict:  {'classification_loss': 0.9360321497917176}
2025-01-19 15:11:54,955 [INFO] Step[250/2713]: training loss : 0.9363087594509125 TRAIN  loss dict:  {'classification_loss': 0.9363087594509125}
2025-01-19 15:12:06,409 [INFO] Step[300/2713]: training loss : 0.9353996288776397 TRAIN  loss dict:  {'classification_loss': 0.9353996288776397}
2025-01-19 15:12:17,885 [INFO] Step[350/2713]: training loss : 0.9359941351413726 TRAIN  loss dict:  {'classification_loss': 0.9359941351413726}
2025-01-19 15:12:29,316 [INFO] Step[400/2713]: training loss : 0.9368963742256164 TRAIN  loss dict:  {'classification_loss': 0.9368963742256164}
2025-01-19 15:12:40,807 [INFO] Step[450/2713]: training loss : 0.9396462571620942 TRAIN  loss dict:  {'classification_loss': 0.9396462571620942}
2025-01-19 15:12:52,274 [INFO] Step[500/2713]: training loss : 0.935140061378479 TRAIN  loss dict:  {'classification_loss': 0.935140061378479}
2025-01-19 15:13:03,743 [INFO] Step[550/2713]: training loss : 0.9363533985614777 TRAIN  loss dict:  {'classification_loss': 0.9363533985614777}
2025-01-19 15:13:15,200 [INFO] Step[600/2713]: training loss : 0.9357678925991059 TRAIN  loss dict:  {'classification_loss': 0.9357678925991059}
2025-01-19 15:13:26,663 [INFO] Step[650/2713]: training loss : 0.9346496403217316 TRAIN  loss dict:  {'classification_loss': 0.9346496403217316}
2025-01-19 15:13:38,119 [INFO] Step[700/2713]: training loss : 0.9347083556652069 TRAIN  loss dict:  {'classification_loss': 0.9347083556652069}
2025-01-19 15:13:49,609 [INFO] Step[750/2713]: training loss : 0.9346616911888123 TRAIN  loss dict:  {'classification_loss': 0.9346616911888123}
2025-01-19 15:14:01,050 [INFO] Step[800/2713]: training loss : 0.9344291961193085 TRAIN  loss dict:  {'classification_loss': 0.9344291961193085}
2025-01-19 15:14:12,517 [INFO] Step[850/2713]: training loss : 0.9370943939685822 TRAIN  loss dict:  {'classification_loss': 0.9370943939685822}
2025-01-19 15:14:23,985 [INFO] Step[900/2713]: training loss : 0.946268275976181 TRAIN  loss dict:  {'classification_loss': 0.946268275976181}
2025-01-19 15:14:35,397 [INFO] Step[950/2713]: training loss : 0.9346627116203308 TRAIN  loss dict:  {'classification_loss': 0.9346627116203308}
2025-01-19 15:14:46,901 [INFO] Step[1000/2713]: training loss : 0.9361311542987824 TRAIN  loss dict:  {'classification_loss': 0.9361311542987824}
2025-01-19 15:14:58,357 [INFO] Step[1050/2713]: training loss : 0.9370927023887634 TRAIN  loss dict:  {'classification_loss': 0.9370927023887634}
2025-01-19 15:15:09,819 [INFO] Step[1100/2713]: training loss : 0.9358399164676666 TRAIN  loss dict:  {'classification_loss': 0.9358399164676666}
2025-01-19 15:15:21,281 [INFO] Step[1150/2713]: training loss : 0.94058846950531 TRAIN  loss dict:  {'classification_loss': 0.94058846950531}
2025-01-19 15:15:32,747 [INFO] Step[1200/2713]: training loss : 0.935178359746933 TRAIN  loss dict:  {'classification_loss': 0.935178359746933}
2025-01-19 15:15:44,200 [INFO] Step[1250/2713]: training loss : 0.9355255556106568 TRAIN  loss dict:  {'classification_loss': 0.9355255556106568}
2025-01-19 15:15:55,697 [INFO] Step[1300/2713]: training loss : 0.9366930305957795 TRAIN  loss dict:  {'classification_loss': 0.9366930305957795}
2025-01-19 15:16:07,146 [INFO] Step[1350/2713]: training loss : 0.9348964810371398 TRAIN  loss dict:  {'classification_loss': 0.9348964810371398}
2025-01-19 15:16:18,623 [INFO] Step[1400/2713]: training loss : 0.9352007496356964 TRAIN  loss dict:  {'classification_loss': 0.9352007496356964}
2025-01-19 15:16:30,090 [INFO] Step[1450/2713]: training loss : 0.9358168053627014 TRAIN  loss dict:  {'classification_loss': 0.9358168053627014}
2025-01-19 15:16:41,573 [INFO] Step[1500/2713]: training loss : 0.936942788362503 TRAIN  loss dict:  {'classification_loss': 0.936942788362503}
2025-01-19 15:16:53,056 [INFO] Step[1550/2713]: training loss : 0.939631358385086 TRAIN  loss dict:  {'classification_loss': 0.939631358385086}
2025-01-19 15:17:04,492 [INFO] Step[1600/2713]: training loss : 0.9349761688709259 TRAIN  loss dict:  {'classification_loss': 0.9349761688709259}
2025-01-19 15:17:15,966 [INFO] Step[1650/2713]: training loss : 0.9354157543182373 TRAIN  loss dict:  {'classification_loss': 0.9354157543182373}
2025-01-19 15:17:27,429 [INFO] Step[1700/2713]: training loss : 0.934731125831604 TRAIN  loss dict:  {'classification_loss': 0.934731125831604}
2025-01-19 15:17:38,896 [INFO] Step[1750/2713]: training loss : 0.9366059482097626 TRAIN  loss dict:  {'classification_loss': 0.9366059482097626}
2025-01-19 15:17:50,352 [INFO] Step[1800/2713]: training loss : 0.935055958032608 TRAIN  loss dict:  {'classification_loss': 0.935055958032608}
2025-01-19 15:18:01,828 [INFO] Step[1850/2713]: training loss : 0.9363125514984131 TRAIN  loss dict:  {'classification_loss': 0.9363125514984131}
2025-01-19 15:18:13,318 [INFO] Step[1900/2713]: training loss : 0.9332404255867004 TRAIN  loss dict:  {'classification_loss': 0.9332404255867004}
2025-01-19 15:18:24,792 [INFO] Step[1950/2713]: training loss : 0.9378152787685394 TRAIN  loss dict:  {'classification_loss': 0.9378152787685394}
2025-01-19 15:18:36,233 [INFO] Step[2000/2713]: training loss : 0.935711282491684 TRAIN  loss dict:  {'classification_loss': 0.935711282491684}
2025-01-19 15:18:47,720 [INFO] Step[2050/2713]: training loss : 0.9343752706050873 TRAIN  loss dict:  {'classification_loss': 0.9343752706050873}
2025-01-19 15:18:59,171 [INFO] Step[2100/2713]: training loss : 0.9347488296031952 TRAIN  loss dict:  {'classification_loss': 0.9347488296031952}
2025-01-19 15:19:10,648 [INFO] Step[2150/2713]: training loss : 0.934374966621399 TRAIN  loss dict:  {'classification_loss': 0.934374966621399}
2025-01-19 15:19:22,114 [INFO] Step[2200/2713]: training loss : 0.9351811254024506 TRAIN  loss dict:  {'classification_loss': 0.9351811254024506}
2025-01-19 15:19:33,595 [INFO] Step[2250/2713]: training loss : 0.9347177231311798 TRAIN  loss dict:  {'classification_loss': 0.9347177231311798}
2025-01-19 15:19:45,073 [INFO] Step[2300/2713]: training loss : 0.9359115529060363 TRAIN  loss dict:  {'classification_loss': 0.9359115529060363}
2025-01-19 15:19:56,561 [INFO] Step[2350/2713]: training loss : 0.9368880558013916 TRAIN  loss dict:  {'classification_loss': 0.9368880558013916}
2025-01-19 15:20:08,056 [INFO] Step[2400/2713]: training loss : 0.9354543316364289 TRAIN  loss dict:  {'classification_loss': 0.9354543316364289}
2025-01-19 15:20:19,544 [INFO] Step[2450/2713]: training loss : 0.9354946982860565 TRAIN  loss dict:  {'classification_loss': 0.9354946982860565}
2025-01-19 15:20:30,997 [INFO] Step[2500/2713]: training loss : 0.935933997631073 TRAIN  loss dict:  {'classification_loss': 0.935933997631073}
2025-01-19 15:20:42,448 [INFO] Step[2550/2713]: training loss : 0.9364756405353546 TRAIN  loss dict:  {'classification_loss': 0.9364756405353546}
2025-01-19 15:20:53,891 [INFO] Step[2600/2713]: training loss : 0.9378692638874054 TRAIN  loss dict:  {'classification_loss': 0.9378692638874054}
2025-01-19 15:21:05,395 [INFO] Step[2650/2713]: training loss : 0.9397899186611176 TRAIN  loss dict:  {'classification_loss': 0.9397899186611176}
2025-01-19 15:21:16,861 [INFO] Step[2700/2713]: training loss : 0.9333928918838501 TRAIN  loss dict:  {'classification_loss': 0.9333928918838501}
2025-01-19 15:22:23,037 [INFO] Label accuracies statistics:
2025-01-19 15:22:23,037 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.5, 206: 0.5, 207: 0.75, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.5, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 15:22:23,039 [INFO] [101] TRAIN  loss: 0.9361550873862919 acc: 1.0
2025-01-19 15:22:23,039 [INFO] [101] TRAIN  loss dict: {'classification_loss': 0.9361550873862919}
2025-01-19 15:22:23,039 [INFO] [101] VALIDATION loss: 1.7241119977674986 VALIDATION acc: 0.8150470219435737
2025-01-19 15:22:23,039 [INFO] [101] VALIDATION loss dict: {'classification_loss': 1.7241119977674986}
2025-01-19 15:22:23,039 [INFO] 
2025-01-19 15:22:39,821 [INFO] Step[50/2713]: training loss : 0.9348421776294709 TRAIN  loss dict:  {'classification_loss': 0.9348421776294709}
2025-01-19 15:22:51,340 [INFO] Step[100/2713]: training loss : 0.9354220378398895 TRAIN  loss dict:  {'classification_loss': 0.9354220378398895}
2025-01-19 15:23:02,863 [INFO] Step[150/2713]: training loss : 0.9349463009834289 TRAIN  loss dict:  {'classification_loss': 0.9349463009834289}
2025-01-19 15:23:14,348 [INFO] Step[200/2713]: training loss : 0.9353352510929107 TRAIN  loss dict:  {'classification_loss': 0.9353352510929107}
2025-01-19 15:23:25,849 [INFO] Step[250/2713]: training loss : 0.9371807765960694 TRAIN  loss dict:  {'classification_loss': 0.9371807765960694}
2025-01-19 15:23:37,374 [INFO] Step[300/2713]: training loss : 0.9350033748149872 TRAIN  loss dict:  {'classification_loss': 0.9350033748149872}
2025-01-19 15:23:48,908 [INFO] Step[350/2713]: training loss : 0.935318763256073 TRAIN  loss dict:  {'classification_loss': 0.935318763256073}
2025-01-19 15:24:00,421 [INFO] Step[400/2713]: training loss : 0.9381587815284729 TRAIN  loss dict:  {'classification_loss': 0.9381587815284729}
2025-01-19 15:24:11,895 [INFO] Step[450/2713]: training loss : 0.9355886363983155 TRAIN  loss dict:  {'classification_loss': 0.9355886363983155}
2025-01-19 15:24:23,387 [INFO] Step[500/2713]: training loss : 0.9340954637527465 TRAIN  loss dict:  {'classification_loss': 0.9340954637527465}
2025-01-19 15:24:34,902 [INFO] Step[550/2713]: training loss : 0.937006698846817 TRAIN  loss dict:  {'classification_loss': 0.937006698846817}
2025-01-19 15:24:46,434 [INFO] Step[600/2713]: training loss : 0.9349955928325653 TRAIN  loss dict:  {'classification_loss': 0.9349955928325653}
2025-01-19 15:24:57,939 [INFO] Step[650/2713]: training loss : 0.9344172871112824 TRAIN  loss dict:  {'classification_loss': 0.9344172871112824}
2025-01-19 15:25:09,453 [INFO] Step[700/2713]: training loss : 0.9354762172698975 TRAIN  loss dict:  {'classification_loss': 0.9354762172698975}
2025-01-19 15:25:20,964 [INFO] Step[750/2713]: training loss : 0.9345801639556884 TRAIN  loss dict:  {'classification_loss': 0.9345801639556884}
2025-01-19 15:25:32,427 [INFO] Step[800/2713]: training loss : 0.9361020100116729 TRAIN  loss dict:  {'classification_loss': 0.9361020100116729}
2025-01-19 15:25:43,946 [INFO] Step[850/2713]: training loss : 0.9358777749538422 TRAIN  loss dict:  {'classification_loss': 0.9358777749538422}
2025-01-19 15:25:55,496 [INFO] Step[900/2713]: training loss : 0.9351651763916016 TRAIN  loss dict:  {'classification_loss': 0.9351651763916016}
2025-01-19 15:26:07,019 [INFO] Step[950/2713]: training loss : 0.9354139077663421 TRAIN  loss dict:  {'classification_loss': 0.9354139077663421}
2025-01-19 15:26:18,535 [INFO] Step[1000/2713]: training loss : 0.9339726042747497 TRAIN  loss dict:  {'classification_loss': 0.9339726042747497}
2025-01-19 15:26:30,029 [INFO] Step[1050/2713]: training loss : 0.9350642716884613 TRAIN  loss dict:  {'classification_loss': 0.9350642716884613}
2025-01-19 15:26:41,523 [INFO] Step[1100/2713]: training loss : 0.935022383928299 TRAIN  loss dict:  {'classification_loss': 0.935022383928299}
2025-01-19 15:26:52,999 [INFO] Step[1150/2713]: training loss : 0.9343477344512939 TRAIN  loss dict:  {'classification_loss': 0.9343477344512939}
2025-01-19 15:27:04,536 [INFO] Step[1200/2713]: training loss : 0.9349734568595887 TRAIN  loss dict:  {'classification_loss': 0.9349734568595887}
2025-01-19 15:27:16,049 [INFO] Step[1250/2713]: training loss : 0.9361206221580506 TRAIN  loss dict:  {'classification_loss': 0.9361206221580506}
2025-01-19 15:27:27,512 [INFO] Step[1300/2713]: training loss : 0.9347637104988098 TRAIN  loss dict:  {'classification_loss': 0.9347637104988098}
2025-01-19 15:27:39,020 [INFO] Step[1350/2713]: training loss : 0.9347434937953949 TRAIN  loss dict:  {'classification_loss': 0.9347434937953949}
2025-01-19 15:27:50,512 [INFO] Step[1400/2713]: training loss : 0.9347397649288177 TRAIN  loss dict:  {'classification_loss': 0.9347397649288177}
2025-01-19 15:28:02,012 [INFO] Step[1450/2713]: training loss : 0.9370667552947998 TRAIN  loss dict:  {'classification_loss': 0.9370667552947998}
2025-01-19 15:28:13,526 [INFO] Step[1500/2713]: training loss : 0.9358069026470184 TRAIN  loss dict:  {'classification_loss': 0.9358069026470184}
2025-01-19 15:28:25,048 [INFO] Step[1550/2713]: training loss : 0.9351297688484191 TRAIN  loss dict:  {'classification_loss': 0.9351297688484191}
2025-01-19 15:28:36,509 [INFO] Step[1600/2713]: training loss : 0.9354434943199158 TRAIN  loss dict:  {'classification_loss': 0.9354434943199158}
2025-01-19 15:28:47,996 [INFO] Step[1650/2713]: training loss : 0.934550906419754 TRAIN  loss dict:  {'classification_loss': 0.934550906419754}
2025-01-19 15:28:59,494 [INFO] Step[1700/2713]: training loss : 0.9352340519428253 TRAIN  loss dict:  {'classification_loss': 0.9352340519428253}
2025-01-19 15:29:11,009 [INFO] Step[1750/2713]: training loss : 0.9346392273902893 TRAIN  loss dict:  {'classification_loss': 0.9346392273902893}
2025-01-19 15:29:22,498 [INFO] Step[1800/2713]: training loss : 0.9365265798568726 TRAIN  loss dict:  {'classification_loss': 0.9365265798568726}
2025-01-19 15:29:34,016 [INFO] Step[1850/2713]: training loss : 0.9375006330013275 TRAIN  loss dict:  {'classification_loss': 0.9375006330013275}
2025-01-19 15:29:45,495 [INFO] Step[1900/2713]: training loss : 0.9387909150123597 TRAIN  loss dict:  {'classification_loss': 0.9387909150123597}
2025-01-19 15:29:57,010 [INFO] Step[1950/2713]: training loss : 0.9352764093875885 TRAIN  loss dict:  {'classification_loss': 0.9352764093875885}
2025-01-19 15:30:08,521 [INFO] Step[2000/2713]: training loss : 0.934568510055542 TRAIN  loss dict:  {'classification_loss': 0.934568510055542}
2025-01-19 15:30:20,030 [INFO] Step[2050/2713]: training loss : 0.934497241973877 TRAIN  loss dict:  {'classification_loss': 0.934497241973877}
2025-01-19 15:30:31,438 [INFO] Step[2100/2713]: training loss : 0.9347879469394684 TRAIN  loss dict:  {'classification_loss': 0.9347879469394684}
2025-01-19 15:30:42,971 [INFO] Step[2150/2713]: training loss : 0.9361146271228791 TRAIN  loss dict:  {'classification_loss': 0.9361146271228791}
2025-01-19 15:30:54,440 [INFO] Step[2200/2713]: training loss : 0.933659462928772 TRAIN  loss dict:  {'classification_loss': 0.933659462928772}
2025-01-19 15:31:05,944 [INFO] Step[2250/2713]: training loss : 0.9374947774410248 TRAIN  loss dict:  {'classification_loss': 0.9374947774410248}
2025-01-19 15:31:17,394 [INFO] Step[2300/2713]: training loss : 0.9365114760398865 TRAIN  loss dict:  {'classification_loss': 0.9365114760398865}
2025-01-19 15:31:28,895 [INFO] Step[2350/2713]: training loss : 0.9363133251667023 TRAIN  loss dict:  {'classification_loss': 0.9363133251667023}
2025-01-19 15:31:40,388 [INFO] Step[2400/2713]: training loss : 0.934390834569931 TRAIN  loss dict:  {'classification_loss': 0.934390834569931}
2025-01-19 15:31:51,870 [INFO] Step[2450/2713]: training loss : 0.935213930606842 TRAIN  loss dict:  {'classification_loss': 0.935213930606842}
2025-01-19 15:32:03,350 [INFO] Step[2500/2713]: training loss : 0.9355038487911225 TRAIN  loss dict:  {'classification_loss': 0.9355038487911225}
2025-01-19 15:32:14,822 [INFO] Step[2550/2713]: training loss : 0.9373956072330475 TRAIN  loss dict:  {'classification_loss': 0.9373956072330475}
2025-01-19 15:32:26,325 [INFO] Step[2600/2713]: training loss : 0.9350742626190186 TRAIN  loss dict:  {'classification_loss': 0.9350742626190186}
2025-01-19 15:32:37,811 [INFO] Step[2650/2713]: training loss : 0.9350904881954193 TRAIN  loss dict:  {'classification_loss': 0.9350904881954193}
2025-01-19 15:32:49,292 [INFO] Step[2700/2713]: training loss : 0.9352060425281524 TRAIN  loss dict:  {'classification_loss': 0.9352060425281524}
2025-01-19 15:33:55,292 [INFO] Label accuracies statistics:
2025-01-19 15:33:55,292 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.5, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-19 15:33:55,294 [INFO] [102] TRAIN  loss: 0.9354823426351396 acc: 1.0
2025-01-19 15:33:55,294 [INFO] [102] TRAIN  loss dict: {'classification_loss': 0.9354823426351396}
2025-01-19 15:33:55,294 [INFO] [102] VALIDATION loss: 1.741485136680137 VALIDATION acc: 0.8056426332288401
2025-01-19 15:33:55,294 [INFO] [102] VALIDATION loss dict: {'classification_loss': 1.741485136680137}
2025-01-19 15:33:55,294 [INFO] 
2025-01-19 15:34:11,589 [INFO] Step[50/2713]: training loss : 0.9365483450889588 TRAIN  loss dict:  {'classification_loss': 0.9365483450889588}
2025-01-19 15:34:23,108 [INFO] Step[100/2713]: training loss : 0.934290052652359 TRAIN  loss dict:  {'classification_loss': 0.934290052652359}
2025-01-19 15:34:34,619 [INFO] Step[150/2713]: training loss : 0.9356529498100281 TRAIN  loss dict:  {'classification_loss': 0.9356529498100281}
2025-01-19 15:34:46,063 [INFO] Step[200/2713]: training loss : 0.9348313927650451 TRAIN  loss dict:  {'classification_loss': 0.9348313927650451}
2025-01-19 15:34:57,583 [INFO] Step[250/2713]: training loss : 0.9339345443248749 TRAIN  loss dict:  {'classification_loss': 0.9339345443248749}
2025-01-19 15:35:09,078 [INFO] Step[300/2713]: training loss : 0.9380379366874695 TRAIN  loss dict:  {'classification_loss': 0.9380379366874695}
2025-01-19 15:35:20,584 [INFO] Step[350/2713]: training loss : 0.9345427191257477 TRAIN  loss dict:  {'classification_loss': 0.9345427191257477}
2025-01-19 15:35:32,103 [INFO] Step[400/2713]: training loss : 0.9338508319854736 TRAIN  loss dict:  {'classification_loss': 0.9338508319854736}
2025-01-19 15:35:43,625 [INFO] Step[450/2713]: training loss : 0.934944964647293 TRAIN  loss dict:  {'classification_loss': 0.934944964647293}
2025-01-19 15:35:55,130 [INFO] Step[500/2713]: training loss : 0.9337039065361022 TRAIN  loss dict:  {'classification_loss': 0.9337039065361022}
2025-01-19 15:36:06,628 [INFO] Step[550/2713]: training loss : 0.9353739845752717 TRAIN  loss dict:  {'classification_loss': 0.9353739845752717}
2025-01-19 15:36:18,119 [INFO] Step[600/2713]: training loss : 0.9354531872272491 TRAIN  loss dict:  {'classification_loss': 0.9354531872272491}
2025-01-19 15:36:29,618 [INFO] Step[650/2713]: training loss : 0.934559918642044 TRAIN  loss dict:  {'classification_loss': 0.934559918642044}
2025-01-19 15:36:41,092 [INFO] Step[700/2713]: training loss : 0.9341896545886993 TRAIN  loss dict:  {'classification_loss': 0.9341896545886993}
2025-01-19 15:36:52,580 [INFO] Step[750/2713]: training loss : 0.9347512018680573 TRAIN  loss dict:  {'classification_loss': 0.9347512018680573}
2025-01-19 15:37:04,070 [INFO] Step[800/2713]: training loss : 0.9356160342693329 TRAIN  loss dict:  {'classification_loss': 0.9356160342693329}
2025-01-19 15:37:15,564 [INFO] Step[850/2713]: training loss : 0.9355999958515168 TRAIN  loss dict:  {'classification_loss': 0.9355999958515168}
2025-01-19 15:37:27,045 [INFO] Step[900/2713]: training loss : 0.9343377757072449 TRAIN  loss dict:  {'classification_loss': 0.9343377757072449}
2025-01-19 15:37:38,587 [INFO] Step[950/2713]: training loss : 0.9337124729156494 TRAIN  loss dict:  {'classification_loss': 0.9337124729156494}
2025-01-19 15:37:50,065 [INFO] Step[1000/2713]: training loss : 0.934362655878067 TRAIN  loss dict:  {'classification_loss': 0.934362655878067}
2025-01-19 15:38:01,554 [INFO] Step[1050/2713]: training loss : 0.9350141191482544 TRAIN  loss dict:  {'classification_loss': 0.9350141191482544}
2025-01-19 15:38:13,067 [INFO] Step[1100/2713]: training loss : 0.935320086479187 TRAIN  loss dict:  {'classification_loss': 0.935320086479187}
2025-01-19 15:38:24,584 [INFO] Step[1150/2713]: training loss : 0.9334601700305939 TRAIN  loss dict:  {'classification_loss': 0.9334601700305939}
2025-01-19 15:38:36,058 [INFO] Step[1200/2713]: training loss : 0.9348075914382935 TRAIN  loss dict:  {'classification_loss': 0.9348075914382935}
2025-01-19 15:38:47,552 [INFO] Step[1250/2713]: training loss : 0.9400620687007905 TRAIN  loss dict:  {'classification_loss': 0.9400620687007905}
2025-01-19 15:38:59,016 [INFO] Step[1300/2713]: training loss : 0.9393758594989776 TRAIN  loss dict:  {'classification_loss': 0.9393758594989776}
2025-01-19 15:39:10,522 [INFO] Step[1350/2713]: training loss : 0.9356872475147248 TRAIN  loss dict:  {'classification_loss': 0.9356872475147248}
2025-01-19 15:39:22,005 [INFO] Step[1400/2713]: training loss : 0.9340899777412415 TRAIN  loss dict:  {'classification_loss': 0.9340899777412415}
2025-01-19 15:39:33,526 [INFO] Step[1450/2713]: training loss : 0.9355165779590606 TRAIN  loss dict:  {'classification_loss': 0.9355165779590606}
2025-01-19 15:39:45,023 [INFO] Step[1500/2713]: training loss : 0.9330440640449524 TRAIN  loss dict:  {'classification_loss': 0.9330440640449524}
2025-01-19 15:39:56,519 [INFO] Step[1550/2713]: training loss : 0.9380648851394653 TRAIN  loss dict:  {'classification_loss': 0.9380648851394653}
2025-01-19 15:40:08,014 [INFO] Step[1600/2713]: training loss : 0.9354901671409607 TRAIN  loss dict:  {'classification_loss': 0.9354901671409607}
2025-01-19 15:40:19,483 [INFO] Step[1650/2713]: training loss : 0.9352924132347107 TRAIN  loss dict:  {'classification_loss': 0.9352924132347107}
2025-01-19 15:40:30,977 [INFO] Step[1700/2713]: training loss : 0.9351554691791535 TRAIN  loss dict:  {'classification_loss': 0.9351554691791535}
2025-01-19 15:40:42,480 [INFO] Step[1750/2713]: training loss : 0.9354582500457763 TRAIN  loss dict:  {'classification_loss': 0.9354582500457763}
2025-01-19 15:40:53,976 [INFO] Step[1800/2713]: training loss : 0.9343534779548645 TRAIN  loss dict:  {'classification_loss': 0.9343534779548645}
2025-01-19 15:41:05,466 [INFO] Step[1850/2713]: training loss : 0.9354330050945282 TRAIN  loss dict:  {'classification_loss': 0.9354330050945282}
2025-01-19 15:41:16,969 [INFO] Step[1900/2713]: training loss : 0.9375792849063873 TRAIN  loss dict:  {'classification_loss': 0.9375792849063873}
2025-01-19 15:41:28,458 [INFO] Step[1950/2713]: training loss : 0.9346150541305542 TRAIN  loss dict:  {'classification_loss': 0.9346150541305542}
2025-01-19 15:41:39,978 [INFO] Step[2000/2713]: training loss : 0.9384052371978759 TRAIN  loss dict:  {'classification_loss': 0.9384052371978759}
2025-01-19 15:41:51,539 [INFO] Step[2050/2713]: training loss : 0.9340205514430999 TRAIN  loss dict:  {'classification_loss': 0.9340205514430999}
2025-01-19 15:42:03,026 [INFO] Step[2100/2713]: training loss : 0.9337521982192993 TRAIN  loss dict:  {'classification_loss': 0.9337521982192993}
2025-01-19 15:42:14,503 [INFO] Step[2150/2713]: training loss : 0.9437502551078797 TRAIN  loss dict:  {'classification_loss': 0.9437502551078797}
2025-01-19 15:42:26,014 [INFO] Step[2200/2713]: training loss : 0.9349211835861206 TRAIN  loss dict:  {'classification_loss': 0.9349211835861206}
2025-01-19 15:42:37,461 [INFO] Step[2250/2713]: training loss : 0.9342340123653412 TRAIN  loss dict:  {'classification_loss': 0.9342340123653412}
2025-01-19 15:42:48,974 [INFO] Step[2300/2713]: training loss : 0.9337339580059052 TRAIN  loss dict:  {'classification_loss': 0.9337339580059052}
2025-01-19 15:43:00,529 [INFO] Step[2350/2713]: training loss : 0.934637143611908 TRAIN  loss dict:  {'classification_loss': 0.934637143611908}
2025-01-19 15:43:12,026 [INFO] Step[2400/2713]: training loss : 0.9337344813346863 TRAIN  loss dict:  {'classification_loss': 0.9337344813346863}
2025-01-19 15:43:23,529 [INFO] Step[2450/2713]: training loss : 0.934175797700882 TRAIN  loss dict:  {'classification_loss': 0.934175797700882}
2025-01-19 15:43:35,006 [INFO] Step[2500/2713]: training loss : 0.935540611743927 TRAIN  loss dict:  {'classification_loss': 0.935540611743927}
2025-01-19 15:43:46,505 [INFO] Step[2550/2713]: training loss : 0.9366154205799103 TRAIN  loss dict:  {'classification_loss': 0.9366154205799103}
2025-01-19 15:43:57,964 [INFO] Step[2600/2713]: training loss : 0.9354240083694458 TRAIN  loss dict:  {'classification_loss': 0.9354240083694458}
2025-01-19 15:44:09,427 [INFO] Step[2650/2713]: training loss : 0.9344347250461579 TRAIN  loss dict:  {'classification_loss': 0.9344347250461579}
2025-01-19 15:44:20,891 [INFO] Step[2700/2713]: training loss : 0.9339292430877686 TRAIN  loss dict:  {'classification_loss': 0.9339292430877686}
2025-01-19 15:45:26,905 [INFO] Label accuracies statistics:
2025-01-19 15:45:26,905 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 1.0, 259: 0.75, 260: 0.25, 261: 1.0, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 1.0, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 15:45:26,907 [INFO] [103] TRAIN  loss: 0.9353416241977297 acc: 1.0
2025-01-19 15:45:26,907 [INFO] [103] TRAIN  loss dict: {'classification_loss': 0.9353416241977297}
2025-01-19 15:45:26,907 [INFO] [103] VALIDATION loss: 1.7226945564038771 VALIDATION acc: 0.8119122257053292
2025-01-19 15:45:26,907 [INFO] [103] VALIDATION loss dict: {'classification_loss': 1.7226945564038771}
2025-01-19 15:45:26,907 [INFO] 
2025-01-19 15:45:43,641 [INFO] Step[50/2713]: training loss : 0.9339797818660736 TRAIN  loss dict:  {'classification_loss': 0.9339797818660736}
2025-01-19 15:45:55,141 [INFO] Step[100/2713]: training loss : 0.9347883534431457 TRAIN  loss dict:  {'classification_loss': 0.9347883534431457}
2025-01-19 15:46:06,648 [INFO] Step[150/2713]: training loss : 0.934761950969696 TRAIN  loss dict:  {'classification_loss': 0.934761950969696}
2025-01-19 15:46:18,171 [INFO] Step[200/2713]: training loss : 0.9366295158863067 TRAIN  loss dict:  {'classification_loss': 0.9366295158863067}
2025-01-19 15:46:29,672 [INFO] Step[250/2713]: training loss : 0.9362415051460267 TRAIN  loss dict:  {'classification_loss': 0.9362415051460267}
2025-01-19 15:46:41,161 [INFO] Step[300/2713]: training loss : 0.9366786980628967 TRAIN  loss dict:  {'classification_loss': 0.9366786980628967}
2025-01-19 15:46:52,705 [INFO] Step[350/2713]: training loss : 0.9365151023864746 TRAIN  loss dict:  {'classification_loss': 0.9365151023864746}
2025-01-19 15:47:04,173 [INFO] Step[400/2713]: training loss : 0.9336054122447968 TRAIN  loss dict:  {'classification_loss': 0.9336054122447968}
2025-01-19 15:47:15,642 [INFO] Step[450/2713]: training loss : 0.9370249998569489 TRAIN  loss dict:  {'classification_loss': 0.9370249998569489}
2025-01-19 15:47:27,141 [INFO] Step[500/2713]: training loss : 0.9358124876022339 TRAIN  loss dict:  {'classification_loss': 0.9358124876022339}
2025-01-19 15:47:38,624 [INFO] Step[550/2713]: training loss : 0.942751623392105 TRAIN  loss dict:  {'classification_loss': 0.942751623392105}
2025-01-19 15:47:50,139 [INFO] Step[600/2713]: training loss : 0.9354970264434814 TRAIN  loss dict:  {'classification_loss': 0.9354970264434814}
2025-01-19 15:48:01,649 [INFO] Step[650/2713]: training loss : 0.935884712934494 TRAIN  loss dict:  {'classification_loss': 0.935884712934494}
2025-01-19 15:48:13,159 [INFO] Step[700/2713]: training loss : 0.9378916001319886 TRAIN  loss dict:  {'classification_loss': 0.9378916001319886}
2025-01-19 15:48:24,626 [INFO] Step[750/2713]: training loss : 0.93627228140831 TRAIN  loss dict:  {'classification_loss': 0.93627228140831}
2025-01-19 15:48:36,107 [INFO] Step[800/2713]: training loss : 0.937245979309082 TRAIN  loss dict:  {'classification_loss': 0.937245979309082}
2025-01-19 15:48:47,621 [INFO] Step[850/2713]: training loss : 0.9423456311225891 TRAIN  loss dict:  {'classification_loss': 0.9423456311225891}
2025-01-19 15:48:59,140 [INFO] Step[900/2713]: training loss : 0.945149050951004 TRAIN  loss dict:  {'classification_loss': 0.945149050951004}
2025-01-19 15:49:10,645 [INFO] Step[950/2713]: training loss : 0.9386935448646545 TRAIN  loss dict:  {'classification_loss': 0.9386935448646545}
2025-01-19 15:49:22,169 [INFO] Step[1000/2713]: training loss : 0.9366375267505646 TRAIN  loss dict:  {'classification_loss': 0.9366375267505646}
2025-01-19 15:49:33,659 [INFO] Step[1050/2713]: training loss : 0.9357948410511017 TRAIN  loss dict:  {'classification_loss': 0.9357948410511017}
2025-01-19 15:49:45,103 [INFO] Step[1100/2713]: training loss : 0.9349050700664521 TRAIN  loss dict:  {'classification_loss': 0.9349050700664521}
2025-01-19 15:49:56,618 [INFO] Step[1150/2713]: training loss : 0.9345227003097534 TRAIN  loss dict:  {'classification_loss': 0.9345227003097534}
2025-01-19 15:50:08,114 [INFO] Step[1200/2713]: training loss : 0.9344132423400879 TRAIN  loss dict:  {'classification_loss': 0.9344132423400879}
2025-01-19 15:50:19,590 [INFO] Step[1250/2713]: training loss : 0.9343396317958832 TRAIN  loss dict:  {'classification_loss': 0.9343396317958832}
2025-01-19 15:50:31,066 [INFO] Step[1300/2713]: training loss : 0.934243152141571 TRAIN  loss dict:  {'classification_loss': 0.934243152141571}
2025-01-19 15:50:42,594 [INFO] Step[1350/2713]: training loss : 0.9381837975978852 TRAIN  loss dict:  {'classification_loss': 0.9381837975978852}
2025-01-19 15:50:54,081 [INFO] Step[1400/2713]: training loss : 0.9357642722129822 TRAIN  loss dict:  {'classification_loss': 0.9357642722129822}
2025-01-19 15:51:05,584 [INFO] Step[1450/2713]: training loss : 0.9341889190673828 TRAIN  loss dict:  {'classification_loss': 0.9341889190673828}
2025-01-19 15:51:17,071 [INFO] Step[1500/2713]: training loss : 0.9338561499118805 TRAIN  loss dict:  {'classification_loss': 0.9338561499118805}
2025-01-19 15:51:28,592 [INFO] Step[1550/2713]: training loss : 0.9350554728507996 TRAIN  loss dict:  {'classification_loss': 0.9350554728507996}
2025-01-19 15:51:40,112 [INFO] Step[1600/2713]: training loss : 0.9344247627258301 TRAIN  loss dict:  {'classification_loss': 0.9344247627258301}
2025-01-19 15:51:51,588 [INFO] Step[1650/2713]: training loss : 0.935070207118988 TRAIN  loss dict:  {'classification_loss': 0.935070207118988}
2025-01-19 15:52:03,044 [INFO] Step[1700/2713]: training loss : 0.9356346273422241 TRAIN  loss dict:  {'classification_loss': 0.9356346273422241}
2025-01-19 15:52:14,547 [INFO] Step[1750/2713]: training loss : 0.9349656832218171 TRAIN  loss dict:  {'classification_loss': 0.9349656832218171}
2025-01-19 15:52:26,061 [INFO] Step[1800/2713]: training loss : 0.93402104139328 TRAIN  loss dict:  {'classification_loss': 0.93402104139328}
2025-01-19 15:52:37,587 [INFO] Step[1850/2713]: training loss : 0.9345192205905914 TRAIN  loss dict:  {'classification_loss': 0.9345192205905914}
2025-01-19 15:52:49,062 [INFO] Step[1900/2713]: training loss : 0.9386891174316406 TRAIN  loss dict:  {'classification_loss': 0.9386891174316406}
2025-01-19 15:53:00,538 [INFO] Step[1950/2713]: training loss : 0.935300452709198 TRAIN  loss dict:  {'classification_loss': 0.935300452709198}
2025-01-19 15:53:12,021 [INFO] Step[2000/2713]: training loss : 0.9346632421016693 TRAIN  loss dict:  {'classification_loss': 0.9346632421016693}
2025-01-19 15:53:23,508 [INFO] Step[2050/2713]: training loss : 0.9367247354984284 TRAIN  loss dict:  {'classification_loss': 0.9367247354984284}
2025-01-19 15:53:35,020 [INFO] Step[2100/2713]: training loss : 0.9348293948173523 TRAIN  loss dict:  {'classification_loss': 0.9348293948173523}
2025-01-19 15:53:46,515 [INFO] Step[2150/2713]: training loss : 0.9357860600948333 TRAIN  loss dict:  {'classification_loss': 0.9357860600948333}
2025-01-19 15:53:58,030 [INFO] Step[2200/2713]: training loss : 0.9362469696998597 TRAIN  loss dict:  {'classification_loss': 0.9362469696998597}
2025-01-19 15:54:09,542 [INFO] Step[2250/2713]: training loss : 0.9345455956459046 TRAIN  loss dict:  {'classification_loss': 0.9345455956459046}
2025-01-19 15:54:21,063 [INFO] Step[2300/2713]: training loss : 0.935308598279953 TRAIN  loss dict:  {'classification_loss': 0.935308598279953}
2025-01-19 15:54:32,557 [INFO] Step[2350/2713]: training loss : 0.9340264558792114 TRAIN  loss dict:  {'classification_loss': 0.9340264558792114}
2025-01-19 15:54:44,057 [INFO] Step[2400/2713]: training loss : 0.934345828294754 TRAIN  loss dict:  {'classification_loss': 0.934345828294754}
2025-01-19 15:54:55,593 [INFO] Step[2450/2713]: training loss : 0.9347906482219696 TRAIN  loss dict:  {'classification_loss': 0.9347906482219696}
2025-01-19 15:55:07,099 [INFO] Step[2500/2713]: training loss : 0.9344979631900787 TRAIN  loss dict:  {'classification_loss': 0.9344979631900787}
2025-01-19 15:55:18,596 [INFO] Step[2550/2713]: training loss : 0.9418533015251159 TRAIN  loss dict:  {'classification_loss': 0.9418533015251159}
2025-01-19 15:55:30,076 [INFO] Step[2600/2713]: training loss : 0.9437361061573029 TRAIN  loss dict:  {'classification_loss': 0.9437361061573029}
2025-01-19 15:55:41,557 [INFO] Step[2650/2713]: training loss : 0.9341139101982117 TRAIN  loss dict:  {'classification_loss': 0.9341139101982117}
2025-01-19 15:55:53,071 [INFO] Step[2700/2713]: training loss : 0.9335791146755219 TRAIN  loss dict:  {'classification_loss': 0.9335791146755219}
2025-01-19 15:56:58,996 [INFO] Label accuracies statistics:
2025-01-19 15:56:58,996 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 1.0, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-19 15:56:58,998 [INFO] [104] TRAIN  loss: 0.9361253779091784 acc: 0.9997542695662858
2025-01-19 15:56:58,998 [INFO] [104] TRAIN  loss dict: {'classification_loss': 0.9361253779091784}
2025-01-19 15:56:58,998 [INFO] [104] VALIDATION loss: 1.760385355779103 VALIDATION acc: 0.8037617554858935
2025-01-19 15:56:58,998 [INFO] [104] VALIDATION loss dict: {'classification_loss': 1.760385355779103}
2025-01-19 15:56:58,998 [INFO] 
2025-01-19 15:57:15,652 [INFO] Step[50/2713]: training loss : 0.9342687618732453 TRAIN  loss dict:  {'classification_loss': 0.9342687618732453}
2025-01-19 15:57:27,076 [INFO] Step[100/2713]: training loss : 0.9354032826423645 TRAIN  loss dict:  {'classification_loss': 0.9354032826423645}
2025-01-19 15:57:38,558 [INFO] Step[150/2713]: training loss : 0.9349323213100433 TRAIN  loss dict:  {'classification_loss': 0.9349323213100433}
2025-01-19 15:57:50,013 [INFO] Step[200/2713]: training loss : 0.9347759020328522 TRAIN  loss dict:  {'classification_loss': 0.9347759020328522}
2025-01-19 15:58:01,516 [INFO] Step[250/2713]: training loss : 0.933992109298706 TRAIN  loss dict:  {'classification_loss': 0.933992109298706}
2025-01-19 15:58:12,972 [INFO] Step[300/2713]: training loss : 0.934739978313446 TRAIN  loss dict:  {'classification_loss': 0.934739978313446}
2025-01-19 15:58:24,469 [INFO] Step[350/2713]: training loss : 0.9345867145061493 TRAIN  loss dict:  {'classification_loss': 0.9345867145061493}
2025-01-19 15:58:35,956 [INFO] Step[400/2713]: training loss : 0.9337503790855408 TRAIN  loss dict:  {'classification_loss': 0.9337503790855408}
2025-01-19 15:58:47,434 [INFO] Step[450/2713]: training loss : 0.9342817890644074 TRAIN  loss dict:  {'classification_loss': 0.9342817890644074}
2025-01-19 15:58:58,914 [INFO] Step[500/2713]: training loss : 0.9356594860553742 TRAIN  loss dict:  {'classification_loss': 0.9356594860553742}
2025-01-19 15:59:10,392 [INFO] Step[550/2713]: training loss : 0.9332411718368531 TRAIN  loss dict:  {'classification_loss': 0.9332411718368531}
2025-01-19 15:59:21,854 [INFO] Step[600/2713]: training loss : 0.935592737197876 TRAIN  loss dict:  {'classification_loss': 0.935592737197876}
2025-01-19 15:59:33,362 [INFO] Step[650/2713]: training loss : 0.933806654214859 TRAIN  loss dict:  {'classification_loss': 0.933806654214859}
2025-01-19 15:59:44,821 [INFO] Step[700/2713]: training loss : 0.9332380187511444 TRAIN  loss dict:  {'classification_loss': 0.9332380187511444}
2025-01-19 15:59:56,338 [INFO] Step[750/2713]: training loss : 0.9341086816787719 TRAIN  loss dict:  {'classification_loss': 0.9341086816787719}
2025-01-19 16:00:07,838 [INFO] Step[800/2713]: training loss : 0.9354051172733306 TRAIN  loss dict:  {'classification_loss': 0.9354051172733306}
2025-01-19 16:00:19,264 [INFO] Step[850/2713]: training loss : 0.9336762809753418 TRAIN  loss dict:  {'classification_loss': 0.9336762809753418}
2025-01-19 16:00:30,734 [INFO] Step[900/2713]: training loss : 0.9351289653778077 TRAIN  loss dict:  {'classification_loss': 0.9351289653778077}
2025-01-19 16:00:42,223 [INFO] Step[950/2713]: training loss : 0.9339296472072601 TRAIN  loss dict:  {'classification_loss': 0.9339296472072601}
2025-01-19 16:00:53,700 [INFO] Step[1000/2713]: training loss : 0.934967075586319 TRAIN  loss dict:  {'classification_loss': 0.934967075586319}
2025-01-19 16:01:05,141 [INFO] Step[1050/2713]: training loss : 0.937391664981842 TRAIN  loss dict:  {'classification_loss': 0.937391664981842}
2025-01-19 16:01:16,615 [INFO] Step[1100/2713]: training loss : 0.9422043859958649 TRAIN  loss dict:  {'classification_loss': 0.9422043859958649}
2025-01-19 16:01:28,132 [INFO] Step[1150/2713]: training loss : 0.9361508858203887 TRAIN  loss dict:  {'classification_loss': 0.9361508858203887}
2025-01-19 16:01:39,593 [INFO] Step[1200/2713]: training loss : 0.9374934947490692 TRAIN  loss dict:  {'classification_loss': 0.9374934947490692}
2025-01-19 16:01:51,088 [INFO] Step[1250/2713]: training loss : 0.9378324890136719 TRAIN  loss dict:  {'classification_loss': 0.9378324890136719}
2025-01-19 16:02:02,584 [INFO] Step[1300/2713]: training loss : 0.9365197622776031 TRAIN  loss dict:  {'classification_loss': 0.9365197622776031}
2025-01-19 16:02:14,042 [INFO] Step[1350/2713]: training loss : 0.9355424499511719 TRAIN  loss dict:  {'classification_loss': 0.9355424499511719}
2025-01-19 16:02:25,532 [INFO] Step[1400/2713]: training loss : 0.9342793786525726 TRAIN  loss dict:  {'classification_loss': 0.9342793786525726}
2025-01-19 16:02:37,025 [INFO] Step[1450/2713]: training loss : 0.9356621515750885 TRAIN  loss dict:  {'classification_loss': 0.9356621515750885}
2025-01-19 16:02:48,504 [INFO] Step[1500/2713]: training loss : 0.9360870718955994 TRAIN  loss dict:  {'classification_loss': 0.9360870718955994}
2025-01-19 16:02:59,983 [INFO] Step[1550/2713]: training loss : 0.9351389777660369 TRAIN  loss dict:  {'classification_loss': 0.9351389777660369}
2025-01-19 16:03:11,449 [INFO] Step[1600/2713]: training loss : 0.934677973985672 TRAIN  loss dict:  {'classification_loss': 0.934677973985672}
2025-01-19 16:03:22,913 [INFO] Step[1650/2713]: training loss : 0.9363586962223053 TRAIN  loss dict:  {'classification_loss': 0.9363586962223053}
2025-01-19 16:03:34,428 [INFO] Step[1700/2713]: training loss : 0.9351263296604156 TRAIN  loss dict:  {'classification_loss': 0.9351263296604156}
2025-01-19 16:03:45,893 [INFO] Step[1750/2713]: training loss : 0.9351911222934723 TRAIN  loss dict:  {'classification_loss': 0.9351911222934723}
2025-01-19 16:03:57,367 [INFO] Step[1800/2713]: training loss : 0.9355688095092773 TRAIN  loss dict:  {'classification_loss': 0.9355688095092773}
2025-01-19 16:04:08,860 [INFO] Step[1850/2713]: training loss : 0.9343193447589875 TRAIN  loss dict:  {'classification_loss': 0.9343193447589875}
2025-01-19 16:04:20,361 [INFO] Step[1900/2713]: training loss : 0.9352592837810516 TRAIN  loss dict:  {'classification_loss': 0.9352592837810516}
2025-01-19 16:04:31,851 [INFO] Step[1950/2713]: training loss : 0.9360456097126008 TRAIN  loss dict:  {'classification_loss': 0.9360456097126008}
2025-01-19 16:04:43,354 [INFO] Step[2000/2713]: training loss : 0.9345610046386719 TRAIN  loss dict:  {'classification_loss': 0.9345610046386719}
2025-01-19 16:04:54,822 [INFO] Step[2050/2713]: training loss : 0.9336866998672485 TRAIN  loss dict:  {'classification_loss': 0.9336866998672485}
2025-01-19 16:05:06,311 [INFO] Step[2100/2713]: training loss : 0.9341572034358978 TRAIN  loss dict:  {'classification_loss': 0.9341572034358978}
2025-01-19 16:05:17,786 [INFO] Step[2150/2713]: training loss : 0.9359080374240876 TRAIN  loss dict:  {'classification_loss': 0.9359080374240876}
2025-01-19 16:05:29,260 [INFO] Step[2200/2713]: training loss : 0.9343365454673767 TRAIN  loss dict:  {'classification_loss': 0.9343365454673767}
2025-01-19 16:05:40,748 [INFO] Step[2250/2713]: training loss : 0.9356356728076934 TRAIN  loss dict:  {'classification_loss': 0.9356356728076934}
2025-01-19 16:05:52,224 [INFO] Step[2300/2713]: training loss : 0.9338184857368469 TRAIN  loss dict:  {'classification_loss': 0.9338184857368469}
2025-01-19 16:06:03,698 [INFO] Step[2350/2713]: training loss : 0.9335361289978027 TRAIN  loss dict:  {'classification_loss': 0.9335361289978027}
2025-01-19 16:06:15,200 [INFO] Step[2400/2713]: training loss : 0.9341808736324311 TRAIN  loss dict:  {'classification_loss': 0.9341808736324311}
2025-01-19 16:06:26,691 [INFO] Step[2450/2713]: training loss : 0.9359519422054291 TRAIN  loss dict:  {'classification_loss': 0.9359519422054291}
2025-01-19 16:06:38,141 [INFO] Step[2500/2713]: training loss : 0.9353717064857483 TRAIN  loss dict:  {'classification_loss': 0.9353717064857483}
2025-01-19 16:06:49,624 [INFO] Step[2550/2713]: training loss : 0.9359430646896363 TRAIN  loss dict:  {'classification_loss': 0.9359430646896363}
2025-01-19 16:07:01,114 [INFO] Step[2600/2713]: training loss : 0.9353840506076813 TRAIN  loss dict:  {'classification_loss': 0.9353840506076813}
2025-01-19 16:07:12,548 [INFO] Step[2650/2713]: training loss : 0.9343391370773315 TRAIN  loss dict:  {'classification_loss': 0.9343391370773315}
2025-01-19 16:07:24,029 [INFO] Step[2700/2713]: training loss : 0.9345451772212983 TRAIN  loss dict:  {'classification_loss': 0.9345451772212983}
2025-01-19 16:08:30,049 [INFO] Label accuracies statistics:
2025-01-19 16:08:30,049 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 1.0, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 16:08:30,050 [INFO] [105] TRAIN  loss: 0.9351536855678299 acc: 1.0
2025-01-19 16:08:30,050 [INFO] [105] TRAIN  loss dict: {'classification_loss': 0.9351536855678299}
2025-01-19 16:08:30,051 [INFO] [105] VALIDATION loss: 1.715746600040816 VALIDATION acc: 0.8181818181818182
2025-01-19 16:08:30,051 [INFO] [105] VALIDATION loss dict: {'classification_loss': 1.715746600040816}
2025-01-19 16:08:30,051 [INFO] 
2025-01-19 16:08:46,398 [INFO] Step[50/2713]: training loss : 0.935847544670105 TRAIN  loss dict:  {'classification_loss': 0.935847544670105}
2025-01-19 16:08:57,910 [INFO] Step[100/2713]: training loss : 0.9348136985301971 TRAIN  loss dict:  {'classification_loss': 0.9348136985301971}
2025-01-19 16:09:09,379 [INFO] Step[150/2713]: training loss : 0.9350891172885895 TRAIN  loss dict:  {'classification_loss': 0.9350891172885895}
2025-01-19 16:09:20,889 [INFO] Step[200/2713]: training loss : 0.9340614664554596 TRAIN  loss dict:  {'classification_loss': 0.9340614664554596}
2025-01-19 16:09:32,407 [INFO] Step[250/2713]: training loss : 0.9372260892391204 TRAIN  loss dict:  {'classification_loss': 0.9372260892391204}
2025-01-19 16:09:43,913 [INFO] Step[300/2713]: training loss : 0.9331718742847442 TRAIN  loss dict:  {'classification_loss': 0.9331718742847442}
2025-01-19 16:09:55,457 [INFO] Step[350/2713]: training loss : 0.9355913305282593 TRAIN  loss dict:  {'classification_loss': 0.9355913305282593}
2025-01-19 16:10:06,935 [INFO] Step[400/2713]: training loss : 0.934768042564392 TRAIN  loss dict:  {'classification_loss': 0.934768042564392}
2025-01-19 16:10:18,419 [INFO] Step[450/2713]: training loss : 0.9343629133701324 TRAIN  loss dict:  {'classification_loss': 0.9343629133701324}
2025-01-19 16:10:29,928 [INFO] Step[500/2713]: training loss : 0.9366732239723206 TRAIN  loss dict:  {'classification_loss': 0.9366732239723206}
2025-01-19 16:10:41,458 [INFO] Step[550/2713]: training loss : 0.9384502923488617 TRAIN  loss dict:  {'classification_loss': 0.9384502923488617}
2025-01-19 16:10:53,006 [INFO] Step[600/2713]: training loss : 0.9328318011760711 TRAIN  loss dict:  {'classification_loss': 0.9328318011760711}
2025-01-19 16:11:04,503 [INFO] Step[650/2713]: training loss : 0.9339422571659088 TRAIN  loss dict:  {'classification_loss': 0.9339422571659088}
2025-01-19 16:11:15,987 [INFO] Step[700/2713]: training loss : 0.9353480577468872 TRAIN  loss dict:  {'classification_loss': 0.9353480577468872}
2025-01-19 16:11:27,523 [INFO] Step[750/2713]: training loss : 0.9337188231945038 TRAIN  loss dict:  {'classification_loss': 0.9337188231945038}
2025-01-19 16:11:39,061 [INFO] Step[800/2713]: training loss : 0.9340018129348755 TRAIN  loss dict:  {'classification_loss': 0.9340018129348755}
2025-01-19 16:11:50,571 [INFO] Step[850/2713]: training loss : 0.9348632442951202 TRAIN  loss dict:  {'classification_loss': 0.9348632442951202}
2025-01-19 16:12:02,058 [INFO] Step[900/2713]: training loss : 0.9345246589183808 TRAIN  loss dict:  {'classification_loss': 0.9345246589183808}
2025-01-19 16:12:13,611 [INFO] Step[950/2713]: training loss : 0.9351744508743286 TRAIN  loss dict:  {'classification_loss': 0.9351744508743286}
2025-01-19 16:12:25,124 [INFO] Step[1000/2713]: training loss : 0.9335323107242585 TRAIN  loss dict:  {'classification_loss': 0.9335323107242585}
2025-01-19 16:12:36,647 [INFO] Step[1050/2713]: training loss : 0.9385824882984162 TRAIN  loss dict:  {'classification_loss': 0.9385824882984162}
2025-01-19 16:12:48,160 [INFO] Step[1100/2713]: training loss : 0.9342826962471008 TRAIN  loss dict:  {'classification_loss': 0.9342826962471008}
2025-01-19 16:12:59,643 [INFO] Step[1150/2713]: training loss : 0.9346895825862884 TRAIN  loss dict:  {'classification_loss': 0.9346895825862884}
2025-01-19 16:13:11,144 [INFO] Step[1200/2713]: training loss : 0.9337907826900482 TRAIN  loss dict:  {'classification_loss': 0.9337907826900482}
2025-01-19 16:13:22,626 [INFO] Step[1250/2713]: training loss : 0.9349367105960846 TRAIN  loss dict:  {'classification_loss': 0.9349367105960846}
2025-01-19 16:13:34,140 [INFO] Step[1300/2713]: training loss : 0.9363657581806183 TRAIN  loss dict:  {'classification_loss': 0.9363657581806183}
2025-01-19 16:13:45,654 [INFO] Step[1350/2713]: training loss : 0.9349948251247406 TRAIN  loss dict:  {'classification_loss': 0.9349948251247406}
2025-01-19 16:13:57,165 [INFO] Step[1400/2713]: training loss : 0.9356336081027985 TRAIN  loss dict:  {'classification_loss': 0.9356336081027985}
2025-01-19 16:14:08,695 [INFO] Step[1450/2713]: training loss : 0.9349770414829254 TRAIN  loss dict:  {'classification_loss': 0.9349770414829254}
2025-01-19 16:14:20,216 [INFO] Step[1500/2713]: training loss : 0.9371255362033843 TRAIN  loss dict:  {'classification_loss': 0.9371255362033843}
2025-01-19 16:14:31,671 [INFO] Step[1550/2713]: training loss : 0.9354906189441681 TRAIN  loss dict:  {'classification_loss': 0.9354906189441681}
2025-01-19 16:14:43,106 [INFO] Step[1600/2713]: training loss : 0.9355806720256805 TRAIN  loss dict:  {'classification_loss': 0.9355806720256805}
2025-01-19 16:14:54,634 [INFO] Step[1650/2713]: training loss : 0.9343630957603455 TRAIN  loss dict:  {'classification_loss': 0.9343630957603455}
2025-01-19 16:15:06,139 [INFO] Step[1700/2713]: training loss : 0.9374682438373566 TRAIN  loss dict:  {'classification_loss': 0.9374682438373566}
2025-01-19 16:15:17,644 [INFO] Step[1750/2713]: training loss : 0.935608080625534 TRAIN  loss dict:  {'classification_loss': 0.935608080625534}
2025-01-19 16:15:29,161 [INFO] Step[1800/2713]: training loss : 0.935153433084488 TRAIN  loss dict:  {'classification_loss': 0.935153433084488}
2025-01-19 16:15:40,669 [INFO] Step[1850/2713]: training loss : 0.935373820066452 TRAIN  loss dict:  {'classification_loss': 0.935373820066452}
2025-01-19 16:15:52,162 [INFO] Step[1900/2713]: training loss : 0.9372687661647796 TRAIN  loss dict:  {'classification_loss': 0.9372687661647796}
2025-01-19 16:16:03,662 [INFO] Step[1950/2713]: training loss : 0.9355027663707733 TRAIN  loss dict:  {'classification_loss': 0.9355027663707733}
2025-01-19 16:16:15,177 [INFO] Step[2000/2713]: training loss : 0.9349967122077942 TRAIN  loss dict:  {'classification_loss': 0.9349967122077942}
2025-01-19 16:16:26,672 [INFO] Step[2050/2713]: training loss : 0.9355207526683808 TRAIN  loss dict:  {'classification_loss': 0.9355207526683808}
2025-01-19 16:16:38,161 [INFO] Step[2100/2713]: training loss : 0.9337681531906128 TRAIN  loss dict:  {'classification_loss': 0.9337681531906128}
2025-01-19 16:16:49,621 [INFO] Step[2150/2713]: training loss : 0.934601777791977 TRAIN  loss dict:  {'classification_loss': 0.934601777791977}
2025-01-19 16:17:01,087 [INFO] Step[2200/2713]: training loss : 0.9346232461929321 TRAIN  loss dict:  {'classification_loss': 0.9346232461929321}
2025-01-19 16:17:12,553 [INFO] Step[2250/2713]: training loss : 0.9347939538955689 TRAIN  loss dict:  {'classification_loss': 0.9347939538955689}
2025-01-19 16:17:24,011 [INFO] Step[2300/2713]: training loss : 0.9338909447193146 TRAIN  loss dict:  {'classification_loss': 0.9338909447193146}
2025-01-19 16:17:35,521 [INFO] Step[2350/2713]: training loss : 0.9369881403446197 TRAIN  loss dict:  {'classification_loss': 0.9369881403446197}
2025-01-19 16:17:46,998 [INFO] Step[2400/2713]: training loss : 0.9348783850669861 TRAIN  loss dict:  {'classification_loss': 0.9348783850669861}
2025-01-19 16:17:58,479 [INFO] Step[2450/2713]: training loss : 0.935703227519989 TRAIN  loss dict:  {'classification_loss': 0.935703227519989}
2025-01-19 16:18:09,983 [INFO] Step[2500/2713]: training loss : 0.936877521276474 TRAIN  loss dict:  {'classification_loss': 0.936877521276474}
2025-01-19 16:18:21,447 [INFO] Step[2550/2713]: training loss : 0.9354735338687896 TRAIN  loss dict:  {'classification_loss': 0.9354735338687896}
2025-01-19 16:18:32,967 [INFO] Step[2600/2713]: training loss : 0.9354409205913544 TRAIN  loss dict:  {'classification_loss': 0.9354409205913544}
2025-01-19 16:18:44,465 [INFO] Step[2650/2713]: training loss : 0.933789929151535 TRAIN  loss dict:  {'classification_loss': 0.933789929151535}
2025-01-19 16:18:55,881 [INFO] Step[2700/2713]: training loss : 0.9342955791950226 TRAIN  loss dict:  {'classification_loss': 0.9342955791950226}
2025-01-19 16:20:01,415 [INFO] Label accuracies statistics:
2025-01-19 16:20:01,416 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 1.0, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.5, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-19 16:20:01,417 [INFO] [106] TRAIN  loss: 0.9352233572020286 acc: 1.0
2025-01-19 16:20:01,417 [INFO] [106] TRAIN  loss dict: {'classification_loss': 0.9352233572020286}
2025-01-19 16:20:01,418 [INFO] [106] VALIDATION loss: 1.7487530107784988 VALIDATION acc: 0.8100313479623824
2025-01-19 16:20:01,418 [INFO] [106] VALIDATION loss dict: {'classification_loss': 1.7487530107784988}
2025-01-19 16:20:01,418 [INFO] 
2025-01-19 16:20:01,418 [INFO] 

***Stop training***


2025-01-19 16:20:01,418 [INFO] 
Testing checkpointed models starting...

2025-01-19 16:20:01,418 [INFO] 
Evaluating checkpoint with best validation loss...

2025-01-19 16:20:57,680 [INFO] Label accuracies statistics:
2025-01-19 16:20:57,681 [INFO] {0: 0.0, 1: 0.75, 2: 1.0, 3: 0.75, 4: 0.5, 5: 0.5, 6: 1.0, 7: 0.75, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.75, 12: 1.0, 13: 0.75, 14: 0.75, 15: 1.0, 16: 1.0, 17: 0.3333333333333333, 18: 0.75, 19: 1.0, 20: 0.75, 21: 1.0, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 1.0, 28: 1.0, 29: 0.75, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 1.0, 45: 1.0, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 1.0, 52: 1.0, 53: 1.0, 54: 0.75, 55: 0.75, 56: 0.5, 57: 0.75, 58: 0.75, 59: 1.0, 60: 0.5, 61: 0.75, 62: 1.0, 63: 1.0, 64: 0.75, 65: 1.0, 66: 1.0, 67: 0.75, 68: 1.0, 69: 1.0, 70: 0.75, 71: 1.0, 72: 0.75, 73: 1.0, 74: 1.0, 75: 0.75, 76: 0.75, 77: 1.0, 78: 1.0, 79: 1.0, 80: 0.75, 81: 1.0, 82: 0.5, 83: 0.75, 84: 1.0, 85: 0.75, 86: 0.75, 87: 1.0, 88: 1.0, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.75, 95: 0.75, 96: 0.5, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 0.75, 104: 0.75, 105: 0.75, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 0.5, 116: 1.0, 117: 1.0, 118: 0.5, 119: 0.75, 120: 1.0, 121: 1.0, 122: 0.75, 123: 0.75, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.5, 130: 0.5, 131: 0.75, 132: 0.5, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 1.0, 152: 0.75, 153: 1.0, 154: 1.0, 155: 0.75, 156: 0.75, 157: 0.75, 158: 0.75, 159: 1.0, 160: 1.0, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 0.75, 166: 0.5, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.25, 172: 1.0, 173: 1.0, 174: 0.75, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.75, 180: 1.0, 181: 1.0, 182: 0.75, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 0.75, 188: 0.75, 189: 0.25, 190: 0.75, 191: 0.75, 192: 0.75, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.25, 199: 0.3333333333333333, 200: 0.75, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.6666666666666666, 205: 1.0, 206: 0.75, 207: 1.0, 208: 1.0, 209: 0.5, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.5, 214: 1.0, 215: 1.0, 216: 0.75, 217: 0.5, 218: 0.75, 219: 1.0, 220: 1.0, 221: 1.0, 222: 1.0, 223: 1.0, 224: 0.75, 225: 1.0, 226: 0.75, 227: 1.0, 228: 0.75, 229: 0.5, 230: 0.25, 231: 0.75, 232: 1.0, 233: 1.0, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 0.6666666666666666, 245: 1.0, 246: 1.0, 247: 0.5, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 1.0, 269: 0.75, 270: 1.0, 271: 1.0, 272: 1.0, 273: 1.0, 274: 0.75, 275: 1.0, 276: 0.5, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 0.75, 284: 1.0, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.75, 296: 1.0, 297: 0.75, 298: 0.75, 299: 0.5, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.75, 305: 1.0, 306: 1.0, 307: 1.0, 308: 0.5, 309: 1.0, 310: 1.0, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.6666666666666666, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 1.0, 331: 1.0, 332: 0.75, 333: 0.75, 334: 1.0, 335: 0.75, 336: 1.0, 337: 0.5, 338: 1.0, 339: 1.0, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.75, 351: 1.0, 352: 0.5, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 0.75, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 1.0, 363: 1.0, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.5, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.5, 376: 0.6666666666666666, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 0.75, 398: 1.0, 399: 1.0}

2025-01-19 16:20:57,683 [INFO] 
Testing accuracy (Best Loss Checkpoint): 0.8373417721518988
2025-01-19 16:20:57,683 [INFO] 
Evaluating checkpoint with best validation accuracy...

2025-01-19 16:21:53,525 [INFO] Label accuracies statistics:
2025-01-19 16:21:53,525 [INFO] {0: 0.0, 1: 0.75, 2: 1.0, 3: 0.75, 4: 0.5, 5: 0.75, 6: 1.0, 7: 0.75, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 1.0, 13: 0.75, 14: 0.5, 15: 1.0, 16: 1.0, 17: 0.3333333333333333, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 1.0, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.5, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 1.0, 45: 1.0, 46: 0.75, 47: 0.75, 48: 1.0, 49: 0.5, 50: 1.0, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 1.0, 56: 0.5, 57: 0.75, 58: 0.75, 59: 1.0, 60: 0.75, 61: 1.0, 62: 1.0, 63: 0.75, 64: 0.5, 65: 0.75, 66: 1.0, 67: 0.75, 68: 1.0, 69: 1.0, 70: 0.5, 71: 1.0, 72: 0.75, 73: 1.0, 74: 0.75, 75: 0.75, 76: 0.75, 77: 1.0, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.5, 87: 1.0, 88: 1.0, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 0.75, 94: 1.0, 95: 0.75, 96: 0.25, 97: 0.75, 98: 1.0, 99: 0.75, 100: 1.0, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 0.75, 106: 1.0, 107: 0.75, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 0.5, 113: 0.25, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 0.5, 119: 0.75, 120: 1.0, 121: 1.0, 122: 0.75, 123: 0.5, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.5, 130: 0.75, 131: 0.75, 132: 0.5, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.5, 158: 0.75, 159: 1.0, 160: 1.0, 161: 0.5, 162: 1.0, 163: 0.75, 164: 0.75, 165: 0.75, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.5, 170: 1.0, 171: 0.25, 172: 1.0, 173: 0.75, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.5, 180: 0.75, 181: 1.0, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 0.75, 188: 0.75, 189: 0.5, 190: 0.75, 191: 1.0, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.3333333333333333, 200: 0.75, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.6666666666666666, 205: 1.0, 206: 0.75, 207: 1.0, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.5, 214: 1.0, 215: 1.0, 216: 0.75, 217: 0.5, 218: 1.0, 219: 1.0, 220: 1.0, 221: 1.0, 222: 1.0, 223: 1.0, 224: 0.75, 225: 1.0, 226: 0.75, 227: 1.0, 228: 0.5, 229: 0.5, 230: 0.5, 231: 0.75, 232: 0.75, 233: 1.0, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 0.25, 248: 1.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 1.0, 269: 0.75, 270: 1.0, 271: 1.0, 272: 1.0, 273: 1.0, 274: 0.75, 275: 1.0, 276: 0.5, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 1.0, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.6666666666666666, 295: 0.5, 296: 1.0, 297: 1.0, 298: 0.75, 299: 0.25, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 1.0, 308: 0.5, 309: 0.75, 310: 1.0, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 1.0, 324: 1.0, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.25, 329: 0.75, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 0.75, 336: 0.75, 337: 0.5, 338: 0.75, 339: 1.0, 340: 0.75, 341: 0.75, 342: 1.0, 343: 0.75, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 1.0, 352: 0.5, 353: 0.25, 354: 0.25, 355: 1.0, 356: 0.75, 357: 0.75, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 1.0, 363: 1.0, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.5, 376: 0.6666666666666666, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.75, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-19 16:21:53,526 [INFO] 
Testing accuracy (Best Acc Checkpoint): 0.8234177215189873
