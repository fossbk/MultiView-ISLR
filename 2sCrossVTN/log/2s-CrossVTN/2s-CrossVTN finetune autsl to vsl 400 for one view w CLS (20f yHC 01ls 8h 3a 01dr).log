2025-01-14 11:24:53,560 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 3a 01dr)...


2025-01-14 11:25:34,603 [INFO] Step[50/2713]: training loss : 6.216899528503418 TRAIN  loss dict:  {'classification_loss': 6.216899528503418}
2025-01-14 11:25:59,950 [INFO] Step[100/2713]: training loss : 6.031339263916015 TRAIN  loss dict:  {'classification_loss': 6.031339263916015}
2025-01-14 11:26:21,195 [INFO] Step[150/2713]: training loss : 5.976235246658325 TRAIN  loss dict:  {'classification_loss': 5.976235246658325}
2025-01-14 11:26:45,689 [INFO] Step[200/2713]: training loss : 5.735142364501953 TRAIN  loss dict:  {'classification_loss': 5.735142364501953}
2025-01-14 11:27:08,859 [INFO] Step[250/2713]: training loss : 5.54272762298584 TRAIN  loss dict:  {'classification_loss': 5.54272762298584}
2025-01-14 11:27:22,770 [INFO] Step[300/2713]: training loss : 5.526943302154541 TRAIN  loss dict:  {'classification_loss': 5.526943302154541}
2025-01-14 11:27:36,293 [INFO] Step[350/2713]: training loss : 5.305605916976929 TRAIN  loss dict:  {'classification_loss': 5.305605916976929}
2025-01-14 11:27:49,540 [INFO] Step[400/2713]: training loss : 5.056546416282654 TRAIN  loss dict:  {'classification_loss': 5.056546416282654}
2025-01-14 11:28:03,560 [INFO] Step[450/2713]: training loss : 4.8779977464675905 TRAIN  loss dict:  {'classification_loss': 4.8779977464675905}
2025-01-14 11:28:17,122 [INFO] Step[500/2713]: training loss : 4.545716156959534 TRAIN  loss dict:  {'classification_loss': 4.545716156959534}
2025-01-14 11:28:31,043 [INFO] Step[550/2713]: training loss : 4.322440228462219 TRAIN  loss dict:  {'classification_loss': 4.322440228462219}
2025-01-14 11:28:44,580 [INFO] Step[600/2713]: training loss : 3.916147665977478 TRAIN  loss dict:  {'classification_loss': 3.916147665977478}
2025-01-14 11:28:57,935 [INFO] Step[650/2713]: training loss : 3.8466113376617432 TRAIN  loss dict:  {'classification_loss': 3.8466113376617432}
2025-01-14 11:29:11,507 [INFO] Step[700/2713]: training loss : 3.593343849182129 TRAIN  loss dict:  {'classification_loss': 3.593343849182129}
2025-01-14 11:29:25,615 [INFO] Step[750/2713]: training loss : 3.3756435918807983 TRAIN  loss dict:  {'classification_loss': 3.3756435918807983}
2025-01-14 11:29:39,335 [INFO] Step[800/2713]: training loss : 2.9392163610458373 TRAIN  loss dict:  {'classification_loss': 2.9392163610458373}
2025-01-14 11:29:52,920 [INFO] Step[850/2713]: training loss : 2.93078711271286 TRAIN  loss dict:  {'classification_loss': 2.93078711271286}
2025-01-14 11:30:06,777 [INFO] Step[900/2713]: training loss : 2.894596481323242 TRAIN  loss dict:  {'classification_loss': 2.894596481323242}
2025-01-14 11:30:20,739 [INFO] Step[950/2713]: training loss : 2.5947666907310487 TRAIN  loss dict:  {'classification_loss': 2.5947666907310487}
2025-01-14 11:30:34,433 [INFO] Step[1000/2713]: training loss : 2.4031321692466734 TRAIN  loss dict:  {'classification_loss': 2.4031321692466734}
2025-01-14 11:30:48,106 [INFO] Step[1050/2713]: training loss : 2.446452066898346 TRAIN  loss dict:  {'classification_loss': 2.446452066898346}
2025-01-14 11:31:02,041 [INFO] Step[1100/2713]: training loss : 2.4608837366104126 TRAIN  loss dict:  {'classification_loss': 2.4608837366104126}
2025-01-14 11:31:15,287 [INFO] Step[1150/2713]: training loss : 2.425991003513336 TRAIN  loss dict:  {'classification_loss': 2.425991003513336}
2025-01-14 11:31:28,789 [INFO] Step[1200/2713]: training loss : 2.455755717754364 TRAIN  loss dict:  {'classification_loss': 2.455755717754364}
2025-01-14 11:31:42,496 [INFO] Step[1250/2713]: training loss : 2.612075345516205 TRAIN  loss dict:  {'classification_loss': 2.612075345516205}
2025-01-14 11:31:56,457 [INFO] Step[1300/2713]: training loss : 2.2618232107162477 TRAIN  loss dict:  {'classification_loss': 2.2618232107162477}
2025-01-14 11:32:10,416 [INFO] Step[1350/2713]: training loss : 2.1082694363594054 TRAIN  loss dict:  {'classification_loss': 2.1082694363594054}
2025-01-14 11:32:24,235 [INFO] Step[1400/2713]: training loss : 2.042629187107086 TRAIN  loss dict:  {'classification_loss': 2.042629187107086}
2025-01-14 11:32:37,950 [INFO] Step[1450/2713]: training loss : 2.238151252269745 TRAIN  loss dict:  {'classification_loss': 2.238151252269745}
2025-01-14 11:32:51,522 [INFO] Step[1500/2713]: training loss : 2.1062575364112854 TRAIN  loss dict:  {'classification_loss': 2.1062575364112854}
2025-01-14 11:33:05,035 [INFO] Step[1550/2713]: training loss : 2.2503639698028564 TRAIN  loss dict:  {'classification_loss': 2.2503639698028564}
2025-01-14 11:33:18,940 [INFO] Step[1600/2713]: training loss : 2.061607961654663 TRAIN  loss dict:  {'classification_loss': 2.061607961654663}
2025-01-14 11:33:32,612 [INFO] Step[1650/2713]: training loss : 2.164014117717743 TRAIN  loss dict:  {'classification_loss': 2.164014117717743}
2025-01-14 11:33:46,128 [INFO] Step[1700/2713]: training loss : 2.1122859072685243 TRAIN  loss dict:  {'classification_loss': 2.1122859072685243}
2025-01-14 11:33:59,594 [INFO] Step[1750/2713]: training loss : 1.8490597939491271 TRAIN  loss dict:  {'classification_loss': 1.8490597939491271}
2025-01-14 11:34:13,306 [INFO] Step[1800/2713]: training loss : 1.9572151565551759 TRAIN  loss dict:  {'classification_loss': 1.9572151565551759}
2025-01-14 11:34:26,705 [INFO] Step[1850/2713]: training loss : 1.9193115854263305 TRAIN  loss dict:  {'classification_loss': 1.9193115854263305}
2025-01-14 11:34:39,955 [INFO] Step[1900/2713]: training loss : 1.9082103300094604 TRAIN  loss dict:  {'classification_loss': 1.9082103300094604}
2025-01-14 11:34:53,916 [INFO] Step[1950/2713]: training loss : 1.8391035676002503 TRAIN  loss dict:  {'classification_loss': 1.8391035676002503}
2025-01-14 11:35:07,361 [INFO] Step[2000/2713]: training loss : 2.010314109325409 TRAIN  loss dict:  {'classification_loss': 2.010314109325409}
2025-01-14 11:35:21,257 [INFO] Step[2050/2713]: training loss : 2.163946964740753 TRAIN  loss dict:  {'classification_loss': 2.163946964740753}
2025-01-14 11:35:35,012 [INFO] Step[2100/2713]: training loss : 1.9674083971977234 TRAIN  loss dict:  {'classification_loss': 1.9674083971977234}
2025-01-14 11:35:48,692 [INFO] Step[2150/2713]: training loss : 1.9707113099098206 TRAIN  loss dict:  {'classification_loss': 1.9707113099098206}
2025-01-14 11:36:02,225 [INFO] Step[2200/2713]: training loss : 1.7835821986198426 TRAIN  loss dict:  {'classification_loss': 1.7835821986198426}
2025-01-14 11:36:16,023 [INFO] Step[2250/2713]: training loss : 1.8911616396903992 TRAIN  loss dict:  {'classification_loss': 1.8911616396903992}
2025-01-14 11:36:29,691 [INFO] Step[2300/2713]: training loss : 2.251528878211975 TRAIN  loss dict:  {'classification_loss': 2.251528878211975}
2025-01-14 11:36:42,910 [INFO] Step[2350/2713]: training loss : 1.7869350743293761 TRAIN  loss dict:  {'classification_loss': 1.7869350743293761}
2025-01-14 11:36:59,121 [INFO] Step[2400/2713]: training loss : 1.8521082878112793 TRAIN  loss dict:  {'classification_loss': 1.8521082878112793}
2025-01-14 11:37:14,150 [INFO] Step[2450/2713]: training loss : 1.9717749619483949 TRAIN  loss dict:  {'classification_loss': 1.9717749619483949}
2025-01-14 11:37:27,626 [INFO] Step[2500/2713]: training loss : 1.7588268446922302 TRAIN  loss dict:  {'classification_loss': 1.7588268446922302}
2025-01-14 11:37:40,971 [INFO] Step[2550/2713]: training loss : 1.765119411945343 TRAIN  loss dict:  {'classification_loss': 1.765119411945343}
2025-01-14 11:37:54,476 [INFO] Step[2600/2713]: training loss : 1.7539674854278564 TRAIN  loss dict:  {'classification_loss': 1.7539674854278564}
2025-01-14 11:38:08,193 [INFO] Step[2650/2713]: training loss : 1.9348993873596192 TRAIN  loss dict:  {'classification_loss': 1.9348993873596192}
2025-01-14 11:38:21,955 [INFO] Step[2700/2713]: training loss : 1.7848055267333984 TRAIN  loss dict:  {'classification_loss': 1.7848055267333984}
2025-01-14 11:39:39,909 [INFO] Label accuracies statistics:
2025-01-14 11:39:39,909 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.25, 9: 0.5, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 0.25, 27: 0.5, 28: 1.0, 29: 1.0, 30: 0.25, 31: 1.0, 32: 0.75, 33: 0.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 0.75, 38: 0.75, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.5, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.5, 74: 0.75, 75: 1.0, 76: 0.25, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.0, 83: 0.5, 84: 0.5, 85: 0.75, 86: 0.75, 87: 1.0, 88: 1.0, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 0.75, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.5, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 0.5, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 0.5, 132: 0.5, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 0.75, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.3333333333333333, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 0.75, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.5, 178: 0.25, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 0.75, 188: 0.5, 189: 1.0, 190: 0.75, 191: 0.25, 192: 0.5, 193: 1.0, 194: 1.0, 195: 0.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.25, 200: 0.5, 201: 0.25, 202: 0.0, 203: 0.5, 204: 0.75, 205: 0.5, 206: 0.25, 207: 0.75, 208: 0.75, 209: 0.5, 210: 0.5, 211: 0.0, 212: 0.0, 213: 0.0, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.5, 226: 0.5, 227: 0.75, 228: 0.5, 229: 0.0, 230: 1.0, 231: 0.75, 232: 0.0, 233: 0.5, 234: 1.0, 235: 0.25, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.0, 240: 0.75, 241: 0.75, 242: 0.0, 243: 0.25, 244: 0.5, 245: 1.0, 246: 1.0, 247: 0.25, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.25, 257: 0.5, 258: 0.25, 259: 0.75, 260: 0.0, 261: 0.5, 262: 0.75, 263: 0.25, 264: 0.5, 265: 0.5, 266: 0.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 0.5, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.25, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.25, 286: 0.0, 287: 0.5, 288: 0.75, 289: 0.25, 290: 0.0, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.25, 297: 0.0, 298: 0.75, 299: 0.0, 300: 0.75, 301: 0.75, 302: 0.0, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.25, 315: 0.5, 316: 0.75, 317: 0.25, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.5, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.0, 334: 0.75, 335: 0.0, 336: 1.0, 337: 0.25, 338: 0.5, 339: 0.75, 340: 0.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.25, 350: 0.0, 351: 0.75, 352: 0.25, 353: 0.25, 354: 0.25, 355: 0.0, 356: 0.75, 357: 0.75, 358: 0.5, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.5, 363: 0.75, 364: 0.25, 365: 0.5, 366: 0.75, 367: 0.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.0, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.25, 376: 0.25, 377: 0.75, 378: 1.0, 379: 0.75, 380: 0.75, 381: 0.5, 382: 0.75, 383: 0.5, 384: 0.75, 385: 1.0, 386: 0.25, 387: 0.25, 388: 0.75, 389: 0.25, 390: 0.25, 391: 0.75, 392: 0.5, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 11:39:41,310 [INFO] [1] TRAIN  loss: 2.9111621846055966 acc: 0.5467502150141295
2025-01-14 11:39:41,310 [INFO] [1] TRAIN  loss dict: {'classification_loss': 2.9111621846055966}
2025-01-14 11:39:41,310 [INFO] [1] VALIDATION loss: 2.238477387374505 VALIDATION acc: 0.6576802507836991
2025-01-14 11:39:41,310 [INFO] [1] VALIDATION loss dict: {'classification_loss': 2.238477387374505}
2025-01-14 11:39:41,310 [INFO] 
2025-01-14 11:40:00,743 [INFO] Step[50/2713]: training loss : 1.5323633933067322 TRAIN  loss dict:  {'classification_loss': 1.5323633933067322}
2025-01-14 11:40:14,490 [INFO] Step[100/2713]: training loss : 1.5490395283699037 TRAIN  loss dict:  {'classification_loss': 1.5490395283699037}
2025-01-14 11:40:28,062 [INFO] Step[150/2713]: training loss : 1.4233556294441223 TRAIN  loss dict:  {'classification_loss': 1.4233556294441223}
2025-01-14 11:40:41,830 [INFO] Step[200/2713]: training loss : 1.5918485069274901 TRAIN  loss dict:  {'classification_loss': 1.5918485069274901}
2025-01-14 11:40:55,068 [INFO] Step[250/2713]: training loss : 1.4945881462097168 TRAIN  loss dict:  {'classification_loss': 1.4945881462097168}
2025-01-14 11:41:08,315 [INFO] Step[300/2713]: training loss : 1.6527081966400146 TRAIN  loss dict:  {'classification_loss': 1.6527081966400146}
2025-01-14 11:41:22,438 [INFO] Step[350/2713]: training loss : 1.605717658996582 TRAIN  loss dict:  {'classification_loss': 1.605717658996582}
2025-01-14 11:41:36,697 [INFO] Step[400/2713]: training loss : 1.5246611547470093 TRAIN  loss dict:  {'classification_loss': 1.5246611547470093}
2025-01-14 11:41:50,400 [INFO] Step[450/2713]: training loss : 1.638246114253998 TRAIN  loss dict:  {'classification_loss': 1.638246114253998}
2025-01-14 11:42:04,285 [INFO] Step[500/2713]: training loss : 1.747379927635193 TRAIN  loss dict:  {'classification_loss': 1.747379927635193}
2025-01-14 11:42:17,685 [INFO] Step[550/2713]: training loss : 1.514946506023407 TRAIN  loss dict:  {'classification_loss': 1.514946506023407}
2025-01-14 11:42:31,493 [INFO] Step[600/2713]: training loss : 1.6597705030441283 TRAIN  loss dict:  {'classification_loss': 1.6597705030441283}
2025-01-14 11:42:45,420 [INFO] Step[650/2713]: training loss : 1.6055382215976715 TRAIN  loss dict:  {'classification_loss': 1.6055382215976715}
2025-01-14 11:42:59,119 [INFO] Step[700/2713]: training loss : 1.6810662055015564 TRAIN  loss dict:  {'classification_loss': 1.6810662055015564}
2025-01-14 11:43:12,854 [INFO] Step[750/2713]: training loss : 1.6600237274169922 TRAIN  loss dict:  {'classification_loss': 1.6600237274169922}
2025-01-14 11:43:26,893 [INFO] Step[800/2713]: training loss : 1.6007289958000184 TRAIN  loss dict:  {'classification_loss': 1.6007289958000184}
2025-01-14 11:43:40,610 [INFO] Step[850/2713]: training loss : 1.5196853637695313 TRAIN  loss dict:  {'classification_loss': 1.5196853637695313}
2025-01-14 11:43:54,875 [INFO] Step[900/2713]: training loss : 1.6109059977531432 TRAIN  loss dict:  {'classification_loss': 1.6109059977531432}
2025-01-14 11:44:08,384 [INFO] Step[950/2713]: training loss : 1.8038405799865722 TRAIN  loss dict:  {'classification_loss': 1.8038405799865722}
2025-01-14 11:44:21,987 [INFO] Step[1000/2713]: training loss : 1.584606375694275 TRAIN  loss dict:  {'classification_loss': 1.584606375694275}
2025-01-14 11:44:35,559 [INFO] Step[1050/2713]: training loss : 1.6126907229423524 TRAIN  loss dict:  {'classification_loss': 1.6126907229423524}
2025-01-14 11:44:49,426 [INFO] Step[1100/2713]: training loss : 1.6835932326316834 TRAIN  loss dict:  {'classification_loss': 1.6835932326316834}
2025-01-14 11:45:02,783 [INFO] Step[1150/2713]: training loss : 1.7191573095321655 TRAIN  loss dict:  {'classification_loss': 1.7191573095321655}
2025-01-14 11:45:16,029 [INFO] Step[1200/2713]: training loss : 1.755103998184204 TRAIN  loss dict:  {'classification_loss': 1.755103998184204}
2025-01-14 11:45:29,322 [INFO] Step[1250/2713]: training loss : 1.6686381363868714 TRAIN  loss dict:  {'classification_loss': 1.6686381363868714}
2025-01-14 11:45:43,432 [INFO] Step[1300/2713]: training loss : 1.6010519409179687 TRAIN  loss dict:  {'classification_loss': 1.6010519409179687}
2025-01-14 11:45:57,439 [INFO] Step[1350/2713]: training loss : 1.5212245321273803 TRAIN  loss dict:  {'classification_loss': 1.5212245321273803}
2025-01-14 11:46:11,198 [INFO] Step[1400/2713]: training loss : 1.5298616528511046 TRAIN  loss dict:  {'classification_loss': 1.5298616528511046}
2025-01-14 11:46:24,694 [INFO] Step[1450/2713]: training loss : 1.4695601916313172 TRAIN  loss dict:  {'classification_loss': 1.4695601916313172}
2025-01-14 11:46:38,644 [INFO] Step[1500/2713]: training loss : 1.5767745399475097 TRAIN  loss dict:  {'classification_loss': 1.5767745399475097}
2025-01-14 11:46:53,890 [INFO] Step[1550/2713]: training loss : 1.6472245812416078 TRAIN  loss dict:  {'classification_loss': 1.6472245812416078}
2025-01-14 11:47:08,792 [INFO] Step[1600/2713]: training loss : 1.622727575302124 TRAIN  loss dict:  {'classification_loss': 1.622727575302124}
2025-01-14 11:47:22,076 [INFO] Step[1650/2713]: training loss : 1.606872432231903 TRAIN  loss dict:  {'classification_loss': 1.606872432231903}
2025-01-14 11:47:35,844 [INFO] Step[1700/2713]: training loss : 1.58156498670578 TRAIN  loss dict:  {'classification_loss': 1.58156498670578}
2025-01-14 11:47:49,239 [INFO] Step[1750/2713]: training loss : 1.6268162870407104 TRAIN  loss dict:  {'classification_loss': 1.6268162870407104}
2025-01-14 11:48:02,806 [INFO] Step[1800/2713]: training loss : 1.5753944838047027 TRAIN  loss dict:  {'classification_loss': 1.5753944838047027}
2025-01-14 11:48:16,095 [INFO] Step[1850/2713]: training loss : 1.6589147448539734 TRAIN  loss dict:  {'classification_loss': 1.6589147448539734}
2025-01-14 11:48:29,381 [INFO] Step[1900/2713]: training loss : 1.669745466709137 TRAIN  loss dict:  {'classification_loss': 1.669745466709137}
2025-01-14 11:48:43,036 [INFO] Step[1950/2713]: training loss : 1.7155742502212525 TRAIN  loss dict:  {'classification_loss': 1.7155742502212525}
2025-01-14 11:48:56,823 [INFO] Step[2000/2713]: training loss : 1.6387559819221496 TRAIN  loss dict:  {'classification_loss': 1.6387559819221496}
2025-01-14 11:49:10,750 [INFO] Step[2050/2713]: training loss : 1.6798771834373474 TRAIN  loss dict:  {'classification_loss': 1.6798771834373474}
2025-01-14 11:49:24,710 [INFO] Step[2100/2713]: training loss : 1.6564435720443726 TRAIN  loss dict:  {'classification_loss': 1.6564435720443726}
2025-01-14 11:49:38,669 [INFO] Step[2150/2713]: training loss : 1.6298681807518005 TRAIN  loss dict:  {'classification_loss': 1.6298681807518005}
2025-01-14 11:49:51,922 [INFO] Step[2200/2713]: training loss : 1.6138235425949097 TRAIN  loss dict:  {'classification_loss': 1.6138235425949097}
2025-01-14 11:50:05,613 [INFO] Step[2250/2713]: training loss : 1.4503780889511109 TRAIN  loss dict:  {'classification_loss': 1.4503780889511109}
2025-01-14 11:50:19,714 [INFO] Step[2300/2713]: training loss : 1.6970926165580749 TRAIN  loss dict:  {'classification_loss': 1.6970926165580749}
2025-01-14 11:50:33,360 [INFO] Step[2350/2713]: training loss : 1.5685220503807067 TRAIN  loss dict:  {'classification_loss': 1.5685220503807067}
2025-01-14 11:50:47,183 [INFO] Step[2400/2713]: training loss : 1.564079167842865 TRAIN  loss dict:  {'classification_loss': 1.564079167842865}
2025-01-14 11:51:01,296 [INFO] Step[2450/2713]: training loss : 1.4811241674423217 TRAIN  loss dict:  {'classification_loss': 1.4811241674423217}
2025-01-14 11:51:15,424 [INFO] Step[2500/2713]: training loss : 1.608231852054596 TRAIN  loss dict:  {'classification_loss': 1.608231852054596}
2025-01-14 11:51:29,258 [INFO] Step[2550/2713]: training loss : 1.5260233402252197 TRAIN  loss dict:  {'classification_loss': 1.5260233402252197}
2025-01-14 11:51:42,514 [INFO] Step[2600/2713]: training loss : 1.554794855117798 TRAIN  loss dict:  {'classification_loss': 1.554794855117798}
2025-01-14 11:51:55,842 [INFO] Step[2650/2713]: training loss : 1.6011889863014221 TRAIN  loss dict:  {'classification_loss': 1.6011889863014221}
2025-01-14 11:52:09,437 [INFO] Step[2700/2713]: training loss : 1.439407641887665 TRAIN  loss dict:  {'classification_loss': 1.439407641887665}
2025-01-14 11:53:25,861 [INFO] Label accuracies statistics:
2025-01-14 11:53:25,862 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.5, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.25, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.75, 65: 0.75, 66: 0.75, 67: 0.25, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.0, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.0, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 0.75, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 0.75, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.5, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.0, 143: 0.75, 144: 0.75, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.5, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.5, 165: 0.75, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.6666666666666666, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 0.75, 188: 0.75, 189: 1.0, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.5, 200: 0.75, 201: 0.0, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.5, 206: 0.25, 207: 0.5, 208: 0.75, 209: 0.5, 210: 0.75, 211: 0.75, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.5, 220: 0.5, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.0, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.0, 240: 1.0, 241: 0.25, 242: 0.0, 243: 0.25, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.5, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.25, 275: 0.75, 276: 0.5, 277: 0.5, 278: 0.25, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 1.0, 303: 0.5, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 0.75, 309: 0.5, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.0, 317: 0.75, 318: 0.5, 319: 1.0, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.5, 332: 1.0, 333: 0.25, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.25, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.5, 347: 1.0, 348: 1.0, 349: 0.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.25, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 0.75, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 1.0, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-14 11:53:28,407 [INFO] [2] TRAIN  loss: 1.6028722391065267 acc: 0.8165622312323382
2025-01-14 11:53:28,407 [INFO] [2] TRAIN  loss dict: {'classification_loss': 1.6028722391065267}
2025-01-14 11:53:28,407 [INFO] [2] VALIDATION loss: 2.042543257089486 VALIDATION acc: 0.735423197492163
2025-01-14 11:53:28,407 [INFO] [2] VALIDATION loss dict: {'classification_loss': 2.042543257089486}
2025-01-14 11:53:28,407 [INFO] 
2025-01-14 11:53:47,430 [INFO] Step[50/2713]: training loss : 1.3516555547714233 TRAIN  loss dict:  {'classification_loss': 1.3516555547714233}
2025-01-14 11:54:00,753 [INFO] Step[100/2713]: training loss : 1.3823376631736755 TRAIN  loss dict:  {'classification_loss': 1.3823376631736755}
2025-01-14 11:54:14,408 [INFO] Step[150/2713]: training loss : 1.4840063309669496 TRAIN  loss dict:  {'classification_loss': 1.4840063309669496}
2025-01-14 11:54:28,700 [INFO] Step[200/2713]: training loss : 1.339592182636261 TRAIN  loss dict:  {'classification_loss': 1.339592182636261}
2025-01-14 11:54:42,630 [INFO] Step[250/2713]: training loss : 1.3890196752548218 TRAIN  loss dict:  {'classification_loss': 1.3890196752548218}
2025-01-14 11:54:56,351 [INFO] Step[300/2713]: training loss : 1.3388273739814758 TRAIN  loss dict:  {'classification_loss': 1.3388273739814758}
2025-01-14 11:55:10,648 [INFO] Step[350/2713]: training loss : 1.408847918510437 TRAIN  loss dict:  {'classification_loss': 1.408847918510437}
2025-01-14 11:55:24,602 [INFO] Step[400/2713]: training loss : 1.4037293910980224 TRAIN  loss dict:  {'classification_loss': 1.4037293910980224}
2025-01-14 11:55:38,873 [INFO] Step[450/2713]: training loss : 1.4061699867248536 TRAIN  loss dict:  {'classification_loss': 1.4061699867248536}
2025-01-14 11:55:52,218 [INFO] Step[500/2713]: training loss : 1.5125134181976319 TRAIN  loss dict:  {'classification_loss': 1.5125134181976319}
2025-01-14 11:56:05,982 [INFO] Step[550/2713]: training loss : 1.4187065529823304 TRAIN  loss dict:  {'classification_loss': 1.4187065529823304}
2025-01-14 11:56:19,606 [INFO] Step[600/2713]: training loss : 1.3566612648963927 TRAIN  loss dict:  {'classification_loss': 1.3566612648963927}
2025-01-14 11:56:33,335 [INFO] Step[650/2713]: training loss : 1.4054530358314514 TRAIN  loss dict:  {'classification_loss': 1.4054530358314514}
2025-01-14 11:56:47,570 [INFO] Step[700/2713]: training loss : 1.438073194026947 TRAIN  loss dict:  {'classification_loss': 1.438073194026947}
2025-01-14 11:57:01,368 [INFO] Step[750/2713]: training loss : 1.344715292453766 TRAIN  loss dict:  {'classification_loss': 1.344715292453766}
2025-01-14 11:57:14,983 [INFO] Step[800/2713]: training loss : 1.400365607738495 TRAIN  loss dict:  {'classification_loss': 1.400365607738495}
2025-01-14 11:57:28,990 [INFO] Step[850/2713]: training loss : 1.4144403982162475 TRAIN  loss dict:  {'classification_loss': 1.4144403982162475}
2025-01-14 11:57:42,630 [INFO] Step[900/2713]: training loss : 1.5526570582389831 TRAIN  loss dict:  {'classification_loss': 1.5526570582389831}
2025-01-14 11:57:56,413 [INFO] Step[950/2713]: training loss : 1.345877742767334 TRAIN  loss dict:  {'classification_loss': 1.345877742767334}
2025-01-14 11:58:10,142 [INFO] Step[1000/2713]: training loss : 1.3846757102012635 TRAIN  loss dict:  {'classification_loss': 1.3846757102012635}
2025-01-14 11:58:24,053 [INFO] Step[1050/2713]: training loss : 1.5078602075576781 TRAIN  loss dict:  {'classification_loss': 1.5078602075576781}
2025-01-14 11:58:37,941 [INFO] Step[1100/2713]: training loss : 1.6243165981769563 TRAIN  loss dict:  {'classification_loss': 1.6243165981769563}
2025-01-14 11:58:51,792 [INFO] Step[1150/2713]: training loss : 1.4017996764183045 TRAIN  loss dict:  {'classification_loss': 1.4017996764183045}
2025-01-14 11:59:05,307 [INFO] Step[1200/2713]: training loss : 1.397676203250885 TRAIN  loss dict:  {'classification_loss': 1.397676203250885}
2025-01-14 11:59:18,901 [INFO] Step[1250/2713]: training loss : 1.392968146800995 TRAIN  loss dict:  {'classification_loss': 1.392968146800995}
2025-01-14 11:59:32,756 [INFO] Step[1300/2713]: training loss : 1.3744480395317078 TRAIN  loss dict:  {'classification_loss': 1.3744480395317078}
2025-01-14 11:59:47,006 [INFO] Step[1350/2713]: training loss : 1.5258681130409242 TRAIN  loss dict:  {'classification_loss': 1.5258681130409242}
2025-01-14 12:00:00,958 [INFO] Step[1400/2713]: training loss : 1.4140915226936341 TRAIN  loss dict:  {'classification_loss': 1.4140915226936341}
2025-01-14 12:00:14,615 [INFO] Step[1450/2713]: training loss : 1.4415632152557374 TRAIN  loss dict:  {'classification_loss': 1.4415632152557374}
2025-01-14 12:00:28,169 [INFO] Step[1500/2713]: training loss : 1.457965772151947 TRAIN  loss dict:  {'classification_loss': 1.457965772151947}
2025-01-14 12:00:41,424 [INFO] Step[1550/2713]: training loss : 1.323533982038498 TRAIN  loss dict:  {'classification_loss': 1.323533982038498}
2025-01-14 12:00:54,849 [INFO] Step[1600/2713]: training loss : 1.4684157752990723 TRAIN  loss dict:  {'classification_loss': 1.4684157752990723}
2025-01-14 12:01:08,778 [INFO] Step[1650/2713]: training loss : 1.4176333045959473 TRAIN  loss dict:  {'classification_loss': 1.4176333045959473}
2025-01-14 12:01:22,566 [INFO] Step[1700/2713]: training loss : 1.4434473538398742 TRAIN  loss dict:  {'classification_loss': 1.4434473538398742}
2025-01-14 12:01:36,174 [INFO] Step[1750/2713]: training loss : 1.4337555480003357 TRAIN  loss dict:  {'classification_loss': 1.4337555480003357}
2025-01-14 12:01:49,871 [INFO] Step[1800/2713]: training loss : 1.481565625667572 TRAIN  loss dict:  {'classification_loss': 1.481565625667572}
2025-01-14 12:02:04,163 [INFO] Step[1850/2713]: training loss : 1.492145857810974 TRAIN  loss dict:  {'classification_loss': 1.492145857810974}
2025-01-14 12:02:17,483 [INFO] Step[1900/2713]: training loss : 1.600492638349533 TRAIN  loss dict:  {'classification_loss': 1.600492638349533}
2025-01-14 12:02:31,118 [INFO] Step[1950/2713]: training loss : 1.4870692932605742 TRAIN  loss dict:  {'classification_loss': 1.4870692932605742}
2025-01-14 12:02:44,360 [INFO] Step[2000/2713]: training loss : 1.405800895690918 TRAIN  loss dict:  {'classification_loss': 1.405800895690918}
2025-01-14 12:02:57,623 [INFO] Step[2050/2713]: training loss : 1.4495114421844482 TRAIN  loss dict:  {'classification_loss': 1.4495114421844482}
2025-01-14 12:03:10,870 [INFO] Step[2100/2713]: training loss : 1.4914565837383271 TRAIN  loss dict:  {'classification_loss': 1.4914565837383271}
2025-01-14 12:03:25,167 [INFO] Step[2150/2713]: training loss : 1.4711116123199464 TRAIN  loss dict:  {'classification_loss': 1.4711116123199464}
2025-01-14 12:03:39,132 [INFO] Step[2200/2713]: training loss : 1.4188339769840241 TRAIN  loss dict:  {'classification_loss': 1.4188339769840241}
2025-01-14 12:03:52,885 [INFO] Step[2250/2713]: training loss : 1.4371688103675841 TRAIN  loss dict:  {'classification_loss': 1.4371688103675841}
2025-01-14 12:04:06,868 [INFO] Step[2300/2713]: training loss : 1.5875747752189637 TRAIN  loss dict:  {'classification_loss': 1.5875747752189637}
2025-01-14 12:04:20,355 [INFO] Step[2350/2713]: training loss : 1.5637256455421449 TRAIN  loss dict:  {'classification_loss': 1.5637256455421449}
2025-01-14 12:04:33,921 [INFO] Step[2400/2713]: training loss : 1.4527600073814393 TRAIN  loss dict:  {'classification_loss': 1.4527600073814393}
2025-01-14 12:04:47,182 [INFO] Step[2450/2713]: training loss : 1.4106270503997802 TRAIN  loss dict:  {'classification_loss': 1.4106270503997802}
2025-01-14 12:05:01,040 [INFO] Step[2500/2713]: training loss : 1.5474673533439636 TRAIN  loss dict:  {'classification_loss': 1.5474673533439636}
2025-01-14 12:05:14,631 [INFO] Step[2550/2713]: training loss : 1.4298570513725282 TRAIN  loss dict:  {'classification_loss': 1.4298570513725282}
2025-01-14 12:05:28,205 [INFO] Step[2600/2713]: training loss : 1.531512153148651 TRAIN  loss dict:  {'classification_loss': 1.531512153148651}
2025-01-14 12:05:41,780 [INFO] Step[2650/2713]: training loss : 1.447042577266693 TRAIN  loss dict:  {'classification_loss': 1.447042577266693}
2025-01-14 12:05:55,044 [INFO] Step[2700/2713]: training loss : 1.5576546716690063 TRAIN  loss dict:  {'classification_loss': 1.5576546716690063}
2025-01-14 12:07:11,351 [INFO] Label accuracies statistics:
2025-01-14 12:07:11,351 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.25, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.5, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.5, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.5, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 0.5, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 0.75, 132: 0.5, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 0.75, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.5, 177: 0.5, 178: 0.5, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.0, 190: 0.5, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.25, 201: 0.25, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.75, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 0.5, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.25, 225: 0.5, 226: 0.5, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.25, 234: 0.5, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 0.75, 255: 1.0, 256: 0.25, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.25, 274: 1.0, 275: 0.75, 276: 0.5, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.5, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.5, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.5, 319: 0.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.75, 329: 0.5, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.25, 342: 1.0, 343: 1.0, 344: 0.25, 345: 0.5, 346: 0.5, 347: 0.75, 348: 0.75, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.25, 357: 0.5, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.5, 365: 0.5, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.25, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.25, 377: 0.75, 378: 0.5, 379: 0.5, 380: 1.0, 381: 0.5, 382: 0.75, 383: 0.25, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.25, 394: 0.25, 395: 0.75, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 12:07:12,482 [INFO] [3] TRAIN  loss: 1.4432655502587195 acc: 0.862022361469468
2025-01-14 12:07:12,482 [INFO] [3] TRAIN  loss dict: {'classification_loss': 1.4432655502587195}
2025-01-14 12:07:12,482 [INFO] [3] VALIDATION loss: 2.0150821762425557 VALIDATION acc: 0.7260188087774294
2025-01-14 12:07:12,482 [INFO] [3] VALIDATION loss dict: {'classification_loss': 2.0150821762425557}
2025-01-14 12:07:12,483 [INFO] 
2025-01-14 12:07:31,386 [INFO] Step[50/2713]: training loss : 1.2491775465011596 TRAIN  loss dict:  {'classification_loss': 1.2491775465011596}
2025-01-14 12:07:44,965 [INFO] Step[100/2713]: training loss : 1.369441463947296 TRAIN  loss dict:  {'classification_loss': 1.369441463947296}
2025-01-14 12:07:58,267 [INFO] Step[150/2713]: training loss : 1.3620376086235046 TRAIN  loss dict:  {'classification_loss': 1.3620376086235046}
2025-01-14 12:08:11,751 [INFO] Step[200/2713]: training loss : 1.3553982198238372 TRAIN  loss dict:  {'classification_loss': 1.3553982198238372}
2025-01-14 12:08:25,688 [INFO] Step[250/2713]: training loss : 1.3008647799491881 TRAIN  loss dict:  {'classification_loss': 1.3008647799491881}
2025-01-14 12:08:39,679 [INFO] Step[300/2713]: training loss : 1.2677569925785064 TRAIN  loss dict:  {'classification_loss': 1.2677569925785064}
2025-01-14 12:08:53,418 [INFO] Step[350/2713]: training loss : 1.313811447620392 TRAIN  loss dict:  {'classification_loss': 1.313811447620392}
2025-01-14 12:09:06,682 [INFO] Step[400/2713]: training loss : 1.2983509075641633 TRAIN  loss dict:  {'classification_loss': 1.2983509075641633}
2025-01-14 12:09:20,166 [INFO] Step[450/2713]: training loss : 1.2426150166988372 TRAIN  loss dict:  {'classification_loss': 1.2426150166988372}
2025-01-14 12:09:34,159 [INFO] Step[500/2713]: training loss : 1.2656510591506958 TRAIN  loss dict:  {'classification_loss': 1.2656510591506958}
2025-01-14 12:09:47,738 [INFO] Step[550/2713]: training loss : 1.2813982665538788 TRAIN  loss dict:  {'classification_loss': 1.2813982665538788}
2025-01-14 12:10:01,476 [INFO] Step[600/2713]: training loss : 1.2127095365524292 TRAIN  loss dict:  {'classification_loss': 1.2127095365524292}
2025-01-14 12:10:15,013 [INFO] Step[650/2713]: training loss : 1.3285200119018554 TRAIN  loss dict:  {'classification_loss': 1.3285200119018554}
2025-01-14 12:10:28,513 [INFO] Step[700/2713]: training loss : 1.2633967447280883 TRAIN  loss dict:  {'classification_loss': 1.2633967447280883}
2025-01-14 12:10:42,345 [INFO] Step[750/2713]: training loss : 1.3627549147605895 TRAIN  loss dict:  {'classification_loss': 1.3627549147605895}
2025-01-14 12:10:56,569 [INFO] Step[800/2713]: training loss : 1.446906497478485 TRAIN  loss dict:  {'classification_loss': 1.446906497478485}
2025-01-14 12:11:10,316 [INFO] Step[850/2713]: training loss : 1.3547356033325195 TRAIN  loss dict:  {'classification_loss': 1.3547356033325195}
2025-01-14 12:11:23,901 [INFO] Step[900/2713]: training loss : 1.261121678352356 TRAIN  loss dict:  {'classification_loss': 1.261121678352356}
2025-01-14 12:11:37,929 [INFO] Step[950/2713]: training loss : 1.3315217614173889 TRAIN  loss dict:  {'classification_loss': 1.3315217614173889}
2025-01-14 12:11:51,284 [INFO] Step[1000/2713]: training loss : 1.37251034617424 TRAIN  loss dict:  {'classification_loss': 1.37251034617424}
2025-01-14 12:12:05,475 [INFO] Step[1050/2713]: training loss : 1.390555374622345 TRAIN  loss dict:  {'classification_loss': 1.390555374622345}
2025-01-14 12:12:19,150 [INFO] Step[1100/2713]: training loss : 1.482835807800293 TRAIN  loss dict:  {'classification_loss': 1.482835807800293}
2025-01-14 12:12:32,874 [INFO] Step[1150/2713]: training loss : 1.2304711854457855 TRAIN  loss dict:  {'classification_loss': 1.2304711854457855}
2025-01-14 12:12:46,880 [INFO] Step[1200/2713]: training loss : 1.2836669790744781 TRAIN  loss dict:  {'classification_loss': 1.2836669790744781}
2025-01-14 12:13:00,215 [INFO] Step[1250/2713]: training loss : 1.313720715045929 TRAIN  loss dict:  {'classification_loss': 1.313720715045929}
2025-01-14 12:13:13,984 [INFO] Step[1300/2713]: training loss : 1.459316349029541 TRAIN  loss dict:  {'classification_loss': 1.459316349029541}
2025-01-14 12:13:27,547 [INFO] Step[1350/2713]: training loss : 1.375057168006897 TRAIN  loss dict:  {'classification_loss': 1.375057168006897}
2025-01-14 12:13:41,615 [INFO] Step[1400/2713]: training loss : 1.3724905419349671 TRAIN  loss dict:  {'classification_loss': 1.3724905419349671}
2025-01-14 12:13:55,207 [INFO] Step[1450/2713]: training loss : 1.337043409347534 TRAIN  loss dict:  {'classification_loss': 1.337043409347534}
2025-01-14 12:14:09,537 [INFO] Step[1500/2713]: training loss : 1.3583546936511994 TRAIN  loss dict:  {'classification_loss': 1.3583546936511994}
2025-01-14 12:14:23,668 [INFO] Step[1550/2713]: training loss : 1.3738107752799988 TRAIN  loss dict:  {'classification_loss': 1.3738107752799988}
2025-01-14 12:14:37,918 [INFO] Step[1600/2713]: training loss : 1.418984956741333 TRAIN  loss dict:  {'classification_loss': 1.418984956741333}
2025-01-14 12:14:51,835 [INFO] Step[1650/2713]: training loss : 1.263365582227707 TRAIN  loss dict:  {'classification_loss': 1.263365582227707}
2025-01-14 12:15:05,291 [INFO] Step[1700/2713]: training loss : 1.3667358422279359 TRAIN  loss dict:  {'classification_loss': 1.3667358422279359}
2025-01-14 12:15:18,612 [INFO] Step[1750/2713]: training loss : 1.3424875903129578 TRAIN  loss dict:  {'classification_loss': 1.3424875903129578}
2025-01-14 12:15:31,917 [INFO] Step[1800/2713]: training loss : 1.3053901064395905 TRAIN  loss dict:  {'classification_loss': 1.3053901064395905}
2025-01-14 12:15:45,568 [INFO] Step[1850/2713]: training loss : 1.288043613433838 TRAIN  loss dict:  {'classification_loss': 1.288043613433838}
2025-01-14 12:15:58,844 [INFO] Step[1900/2713]: training loss : 1.3697373616695403 TRAIN  loss dict:  {'classification_loss': 1.3697373616695403}
2025-01-14 12:16:12,731 [INFO] Step[1950/2713]: training loss : 1.396268262863159 TRAIN  loss dict:  {'classification_loss': 1.396268262863159}
2025-01-14 12:16:26,000 [INFO] Step[2000/2713]: training loss : 1.2843636345863343 TRAIN  loss dict:  {'classification_loss': 1.2843636345863343}
2025-01-14 12:16:39,861 [INFO] Step[2050/2713]: training loss : 1.485787444114685 TRAIN  loss dict:  {'classification_loss': 1.485787444114685}
2025-01-14 12:16:53,569 [INFO] Step[2100/2713]: training loss : 1.3755553889274597 TRAIN  loss dict:  {'classification_loss': 1.3755553889274597}
2025-01-14 12:17:07,246 [INFO] Step[2150/2713]: training loss : 1.3104267883300782 TRAIN  loss dict:  {'classification_loss': 1.3104267883300782}
2025-01-14 12:17:20,798 [INFO] Step[2200/2713]: training loss : 1.3708873617649078 TRAIN  loss dict:  {'classification_loss': 1.3708873617649078}
2025-01-14 12:17:34,864 [INFO] Step[2250/2713]: training loss : 1.3453727328777314 TRAIN  loss dict:  {'classification_loss': 1.3453727328777314}
2025-01-14 12:17:48,573 [INFO] Step[2300/2713]: training loss : 1.400982482433319 TRAIN  loss dict:  {'classification_loss': 1.400982482433319}
2025-01-14 12:18:02,700 [INFO] Step[2350/2713]: training loss : 1.2878404819965363 TRAIN  loss dict:  {'classification_loss': 1.2878404819965363}
2025-01-14 12:18:16,707 [INFO] Step[2400/2713]: training loss : 1.4631467473506927 TRAIN  loss dict:  {'classification_loss': 1.4631467473506927}
2025-01-14 12:18:30,501 [INFO] Step[2450/2713]: training loss : 1.4843574953079224 TRAIN  loss dict:  {'classification_loss': 1.4843574953079224}
2025-01-14 12:18:44,531 [INFO] Step[2500/2713]: training loss : 1.5406063055992127 TRAIN  loss dict:  {'classification_loss': 1.5406063055992127}
2025-01-14 12:18:58,351 [INFO] Step[2550/2713]: training loss : 1.3791284203529357 TRAIN  loss dict:  {'classification_loss': 1.3791284203529357}
2025-01-14 12:19:11,647 [INFO] Step[2600/2713]: training loss : 1.3359955835342407 TRAIN  loss dict:  {'classification_loss': 1.3359955835342407}
2025-01-14 12:19:25,872 [INFO] Step[2650/2713]: training loss : 1.2695041179656983 TRAIN  loss dict:  {'classification_loss': 1.2695041179656983}
2025-01-14 12:19:39,489 [INFO] Step[2700/2713]: training loss : 1.3566097927093506 TRAIN  loss dict:  {'classification_loss': 1.3566097927093506}
2025-01-14 12:20:55,964 [INFO] Label accuracies statistics:
2025-01-14 12:20:55,964 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 0.75, 25: 0.75, 26: 0.25, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 0.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 1.0, 61: 0.5, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 1.0, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.0, 87: 0.5, 88: 0.5, 89: 1.0, 90: 1.0, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.25, 108: 0.75, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 0.75, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.25, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.5, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.3333333333333333, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.5, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.5, 170: 1.0, 171: 0.5, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.6666666666666666, 180: 0.5, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 0.75, 197: 1.0, 198: 0.25, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 1.0, 204: 0.5, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.0, 214: 0.5, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.25, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.0, 230: 1.0, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.0, 238: 1.0, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 1.0, 252: 0.5, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.0, 259: 0.5, 260: 0.25, 261: 1.0, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.75, 269: 0.5, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.25, 274: 0.25, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.25, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.25, 291: 0.75, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 0.5, 307: 1.0, 308: 0.75, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.25, 319: 0.75, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 0.75, 325: 1.0, 326: 0.75, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.25, 337: 0.25, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.25, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.25, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 12:20:58,409 [INFO] [4] TRAIN  loss: 1.3448302102537117 acc: 0.8975304091411721
2025-01-14 12:20:58,409 [INFO] [4] TRAIN  loss dict: {'classification_loss': 1.3448302102537117}
2025-01-14 12:20:58,409 [INFO] [4] VALIDATION loss: 1.9847466972537506 VALIDATION acc: 0.7391849529780564
2025-01-14 12:20:58,409 [INFO] [4] VALIDATION loss dict: {'classification_loss': 1.9847466972537506}
2025-01-14 12:20:58,409 [INFO] 
2025-01-14 12:21:17,222 [INFO] Step[50/2713]: training loss : 1.2542228829860687 TRAIN  loss dict:  {'classification_loss': 1.2542228829860687}
2025-01-14 12:21:30,514 [INFO] Step[100/2713]: training loss : 1.236817409992218 TRAIN  loss dict:  {'classification_loss': 1.236817409992218}
2025-01-14 12:21:44,424 [INFO] Step[150/2713]: training loss : 1.3123419213294982 TRAIN  loss dict:  {'classification_loss': 1.3123419213294982}
2025-01-14 12:21:57,778 [INFO] Step[200/2713]: training loss : 1.263615550994873 TRAIN  loss dict:  {'classification_loss': 1.263615550994873}
2025-01-14 12:22:11,335 [INFO] Step[250/2713]: training loss : 1.3653273153305054 TRAIN  loss dict:  {'classification_loss': 1.3653273153305054}
2025-01-14 12:22:24,544 [INFO] Step[300/2713]: training loss : 1.165773183107376 TRAIN  loss dict:  {'classification_loss': 1.165773183107376}
2025-01-14 12:22:38,574 [INFO] Step[350/2713]: training loss : 1.3564568114280702 TRAIN  loss dict:  {'classification_loss': 1.3564568114280702}
2025-01-14 12:22:52,015 [INFO] Step[400/2713]: training loss : 1.255480808019638 TRAIN  loss dict:  {'classification_loss': 1.255480808019638}
2025-01-14 12:23:05,267 [INFO] Step[450/2713]: training loss : 1.2532927322387695 TRAIN  loss dict:  {'classification_loss': 1.2532927322387695}
2025-01-14 12:23:18,999 [INFO] Step[500/2713]: training loss : 1.2837990880012513 TRAIN  loss dict:  {'classification_loss': 1.2837990880012513}
2025-01-14 12:23:32,806 [INFO] Step[550/2713]: training loss : 1.347680733203888 TRAIN  loss dict:  {'classification_loss': 1.347680733203888}
2025-01-14 12:23:46,489 [INFO] Step[600/2713]: training loss : 1.3012244606018066 TRAIN  loss dict:  {'classification_loss': 1.3012244606018066}
2025-01-14 12:24:00,014 [INFO] Step[650/2713]: training loss : 1.3101552057266235 TRAIN  loss dict:  {'classification_loss': 1.3101552057266235}
2025-01-14 12:24:13,700 [INFO] Step[700/2713]: training loss : 1.252411687374115 TRAIN  loss dict:  {'classification_loss': 1.252411687374115}
2025-01-14 12:24:27,433 [INFO] Step[750/2713]: training loss : 1.249657073020935 TRAIN  loss dict:  {'classification_loss': 1.249657073020935}
2025-01-14 12:24:41,622 [INFO] Step[800/2713]: training loss : 1.2541627955436707 TRAIN  loss dict:  {'classification_loss': 1.2541627955436707}
2025-01-14 12:24:57,956 [INFO] Step[850/2713]: training loss : 1.3463985657691955 TRAIN  loss dict:  {'classification_loss': 1.3463985657691955}
2025-01-14 12:25:11,803 [INFO] Step[900/2713]: training loss : 1.3417632794380188 TRAIN  loss dict:  {'classification_loss': 1.3417632794380188}
2025-01-14 12:25:25,090 [INFO] Step[950/2713]: training loss : 1.2929196763038635 TRAIN  loss dict:  {'classification_loss': 1.2929196763038635}
2025-01-14 12:25:39,191 [INFO] Step[1000/2713]: training loss : 1.3085767889022828 TRAIN  loss dict:  {'classification_loss': 1.3085767889022828}
2025-01-14 12:25:53,116 [INFO] Step[1050/2713]: training loss : 1.2261269772052765 TRAIN  loss dict:  {'classification_loss': 1.2261269772052765}
2025-01-14 12:26:06,608 [INFO] Step[1100/2713]: training loss : 1.2633719587326049 TRAIN  loss dict:  {'classification_loss': 1.2633719587326049}
2025-01-14 12:26:20,627 [INFO] Step[1150/2713]: training loss : 1.2271128380298615 TRAIN  loss dict:  {'classification_loss': 1.2271128380298615}
2025-01-14 12:26:34,650 [INFO] Step[1200/2713]: training loss : 1.271546471118927 TRAIN  loss dict:  {'classification_loss': 1.271546471118927}
2025-01-14 12:26:48,194 [INFO] Step[1250/2713]: training loss : 1.3430585312843322 TRAIN  loss dict:  {'classification_loss': 1.3430585312843322}
2025-01-14 12:27:01,471 [INFO] Step[1300/2713]: training loss : 1.161648461818695 TRAIN  loss dict:  {'classification_loss': 1.161648461818695}
2025-01-14 12:27:15,508 [INFO] Step[1350/2713]: training loss : 1.268428816795349 TRAIN  loss dict:  {'classification_loss': 1.268428816795349}
2025-01-14 12:27:28,988 [INFO] Step[1400/2713]: training loss : 1.2251272439956664 TRAIN  loss dict:  {'classification_loss': 1.2251272439956664}
2025-01-14 12:27:42,265 [INFO] Step[1450/2713]: training loss : 1.3461680614948273 TRAIN  loss dict:  {'classification_loss': 1.3461680614948273}
2025-01-14 12:27:55,502 [INFO] Step[1500/2713]: training loss : 1.3055253839492797 TRAIN  loss dict:  {'classification_loss': 1.3055253839492797}
2025-01-14 12:28:09,301 [INFO] Step[1550/2713]: training loss : 1.2747215127944946 TRAIN  loss dict:  {'classification_loss': 1.2747215127944946}
2025-01-14 12:28:22,760 [INFO] Step[1600/2713]: training loss : 1.2419080317020417 TRAIN  loss dict:  {'classification_loss': 1.2419080317020417}
2025-01-14 12:28:36,817 [INFO] Step[1650/2713]: training loss : 1.2333066773414612 TRAIN  loss dict:  {'classification_loss': 1.2333066773414612}
2025-01-14 12:28:50,359 [INFO] Step[1700/2713]: training loss : 1.3146633148193358 TRAIN  loss dict:  {'classification_loss': 1.3146633148193358}
2025-01-14 12:29:04,410 [INFO] Step[1750/2713]: training loss : 1.2624413967132568 TRAIN  loss dict:  {'classification_loss': 1.2624413967132568}
2025-01-14 12:29:18,457 [INFO] Step[1800/2713]: training loss : 1.2576301181316376 TRAIN  loss dict:  {'classification_loss': 1.2576301181316376}
2025-01-14 12:29:31,747 [INFO] Step[1850/2713]: training loss : 1.2083321058750152 TRAIN  loss dict:  {'classification_loss': 1.2083321058750152}
2025-01-14 12:29:45,400 [INFO] Step[1900/2713]: training loss : 1.3782046151161194 TRAIN  loss dict:  {'classification_loss': 1.3782046151161194}
2025-01-14 12:29:59,437 [INFO] Step[1950/2713]: training loss : 1.3169368672370911 TRAIN  loss dict:  {'classification_loss': 1.3169368672370911}
2025-01-14 12:30:12,685 [INFO] Step[2000/2713]: training loss : 1.257246037721634 TRAIN  loss dict:  {'classification_loss': 1.257246037721634}
2025-01-14 12:30:26,249 [INFO] Step[2050/2713]: training loss : 1.2740573179721832 TRAIN  loss dict:  {'classification_loss': 1.2740573179721832}
2025-01-14 12:30:40,406 [INFO] Step[2100/2713]: training loss : 1.2549512779712677 TRAIN  loss dict:  {'classification_loss': 1.2549512779712677}
2025-01-14 12:30:53,851 [INFO] Step[2150/2713]: training loss : 1.3328268659114837 TRAIN  loss dict:  {'classification_loss': 1.3328268659114837}
2025-01-14 12:31:07,657 [INFO] Step[2200/2713]: training loss : 1.3085779023170472 TRAIN  loss dict:  {'classification_loss': 1.3085779023170472}
2025-01-14 12:31:21,928 [INFO] Step[2250/2713]: training loss : 1.2751011645793915 TRAIN  loss dict:  {'classification_loss': 1.2751011645793915}
2025-01-14 12:31:35,767 [INFO] Step[2300/2713]: training loss : 1.434893833398819 TRAIN  loss dict:  {'classification_loss': 1.434893833398819}
2025-01-14 12:31:49,421 [INFO] Step[2350/2713]: training loss : 1.3073421669006349 TRAIN  loss dict:  {'classification_loss': 1.3073421669006349}
2025-01-14 12:32:02,983 [INFO] Step[2400/2713]: training loss : 1.343272750377655 TRAIN  loss dict:  {'classification_loss': 1.343272750377655}
2025-01-14 12:32:17,134 [INFO] Step[2450/2713]: training loss : 1.3667919194698335 TRAIN  loss dict:  {'classification_loss': 1.3667919194698335}
2025-01-14 12:32:30,640 [INFO] Step[2500/2713]: training loss : 1.2283787500858308 TRAIN  loss dict:  {'classification_loss': 1.2283787500858308}
2025-01-14 12:32:44,277 [INFO] Step[2550/2713]: training loss : 1.2847411382198333 TRAIN  loss dict:  {'classification_loss': 1.2847411382198333}
2025-01-14 12:32:58,272 [INFO] Step[2600/2713]: training loss : 1.3196176743507386 TRAIN  loss dict:  {'classification_loss': 1.3196176743507386}
2025-01-14 12:33:11,844 [INFO] Step[2650/2713]: training loss : 1.2745743441581725 TRAIN  loss dict:  {'classification_loss': 1.2745743441581725}
2025-01-14 12:33:25,193 [INFO] Step[2700/2713]: training loss : 1.3196598923206329 TRAIN  loss dict:  {'classification_loss': 1.3196598923206329}
2025-01-14 12:34:42,157 [INFO] Label accuracies statistics:
2025-01-14 12:34:42,157 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.5, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.5, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.5, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.25, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.5, 65: 0.75, 66: 0.5, 67: 1.0, 68: 0.25, 69: 1.0, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.0, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.5, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.5, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 0.5, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.5, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 1.0, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.0, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.25, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.5, 234: 0.5, 235: 0.5, 236: 1.0, 237: 0.5, 238: 0.75, 239: 0.5, 240: 0.5, 241: 1.0, 242: 0.0, 243: 1.0, 244: 0.75, 245: 1.0, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.25, 276: 0.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.25, 286: 0.75, 287: 0.5, 288: 0.75, 289: 0.75, 290: 0.0, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.5, 295: 1.0, 296: 0.25, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.5, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.5, 316: 0.25, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.5, 321: 0.5, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.5, 326: 1.0, 327: 0.75, 328: 1.0, 329: 0.75, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.5, 338: 0.5, 339: 0.5, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.25, 353: 0.5, 354: 0.25, 355: 0.5, 356: 0.25, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.25, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.5, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-14 12:34:42,159 [INFO] [5] TRAIN  loss: 1.2863314782962385 acc: 0.9164516525371668
2025-01-14 12:34:42,159 [INFO] [5] TRAIN  loss dict: {'classification_loss': 1.2863314782962385}
2025-01-14 12:34:42,159 [INFO] [5] VALIDATION loss: 2.0195123557757615 VALIDATION acc: 0.7253918495297805
2025-01-14 12:34:42,159 [INFO] [5] VALIDATION loss dict: {'classification_loss': 2.0195123557757615}
2025-01-14 12:34:42,159 [INFO] 
2025-01-14 12:35:01,074 [INFO] Step[50/2713]: training loss : 1.2227728879451751 TRAIN  loss dict:  {'classification_loss': 1.2227728879451751}
2025-01-14 12:35:14,737 [INFO] Step[100/2713]: training loss : 1.2703428864479065 TRAIN  loss dict:  {'classification_loss': 1.2703428864479065}
2025-01-14 12:35:28,930 [INFO] Step[150/2713]: training loss : 1.2668222320079803 TRAIN  loss dict:  {'classification_loss': 1.2668222320079803}
2025-01-14 12:35:42,805 [INFO] Step[200/2713]: training loss : 1.198740303516388 TRAIN  loss dict:  {'classification_loss': 1.198740303516388}
2025-01-14 12:35:56,813 [INFO] Step[250/2713]: training loss : 1.2713448452949523 TRAIN  loss dict:  {'classification_loss': 1.2713448452949523}
2025-01-14 12:36:10,168 [INFO] Step[300/2713]: training loss : 1.1410608029365539 TRAIN  loss dict:  {'classification_loss': 1.1410608029365539}
2025-01-14 12:36:23,877 [INFO] Step[350/2713]: training loss : 1.3010063946247101 TRAIN  loss dict:  {'classification_loss': 1.3010063946247101}
2025-01-14 12:36:37,487 [INFO] Step[400/2713]: training loss : 1.2830183362960816 TRAIN  loss dict:  {'classification_loss': 1.2830183362960816}
2025-01-14 12:36:51,148 [INFO] Step[450/2713]: training loss : 1.2433179926872253 TRAIN  loss dict:  {'classification_loss': 1.2433179926872253}
2025-01-14 12:37:04,365 [INFO] Step[500/2713]: training loss : 1.3324408960342407 TRAIN  loss dict:  {'classification_loss': 1.3324408960342407}
2025-01-14 12:37:18,289 [INFO] Step[550/2713]: training loss : 1.3287756323814393 TRAIN  loss dict:  {'classification_loss': 1.3287756323814393}
2025-01-14 12:37:32,377 [INFO] Step[600/2713]: training loss : 1.1416328012943269 TRAIN  loss dict:  {'classification_loss': 1.1416328012943269}
2025-01-14 12:37:45,672 [INFO] Step[650/2713]: training loss : 1.259477390050888 TRAIN  loss dict:  {'classification_loss': 1.259477390050888}
2025-01-14 12:37:58,915 [INFO] Step[700/2713]: training loss : 1.1721964383125305 TRAIN  loss dict:  {'classification_loss': 1.1721964383125305}
2025-01-14 12:38:12,517 [INFO] Step[750/2713]: training loss : 1.1451412236690521 TRAIN  loss dict:  {'classification_loss': 1.1451412236690521}
2025-01-14 12:38:26,131 [INFO] Step[800/2713]: training loss : 1.231185417175293 TRAIN  loss dict:  {'classification_loss': 1.231185417175293}
2025-01-14 12:38:39,431 [INFO] Step[850/2713]: training loss : 1.3148501586914063 TRAIN  loss dict:  {'classification_loss': 1.3148501586914063}
2025-01-14 12:38:53,045 [INFO] Step[900/2713]: training loss : 1.3068244564533233 TRAIN  loss dict:  {'classification_loss': 1.3068244564533233}
2025-01-14 12:39:06,284 [INFO] Step[950/2713]: training loss : 1.362898693084717 TRAIN  loss dict:  {'classification_loss': 1.362898693084717}
2025-01-14 12:39:20,299 [INFO] Step[1000/2713]: training loss : 1.2668726968765258 TRAIN  loss dict:  {'classification_loss': 1.2668726968765258}
2025-01-14 12:39:34,044 [INFO] Step[1050/2713]: training loss : 1.237078251838684 TRAIN  loss dict:  {'classification_loss': 1.237078251838684}
2025-01-14 12:39:47,757 [INFO] Step[1100/2713]: training loss : 1.1927099275588988 TRAIN  loss dict:  {'classification_loss': 1.1927099275588988}
2025-01-14 12:40:01,754 [INFO] Step[1150/2713]: training loss : 1.1897548258304596 TRAIN  loss dict:  {'classification_loss': 1.1897548258304596}
2025-01-14 12:40:14,924 [INFO] Step[1200/2713]: training loss : 1.2491952431201936 TRAIN  loss dict:  {'classification_loss': 1.2491952431201936}
2025-01-14 12:40:28,335 [INFO] Step[1250/2713]: training loss : 1.244674813747406 TRAIN  loss dict:  {'classification_loss': 1.244674813747406}
2025-01-14 12:40:42,008 [INFO] Step[1300/2713]: training loss : 1.255980190038681 TRAIN  loss dict:  {'classification_loss': 1.255980190038681}
2025-01-14 12:40:56,117 [INFO] Step[1350/2713]: training loss : 1.3145382344722747 TRAIN  loss dict:  {'classification_loss': 1.3145382344722747}
2025-01-14 12:41:09,329 [INFO] Step[1400/2713]: training loss : 1.116048185825348 TRAIN  loss dict:  {'classification_loss': 1.116048185825348}
2025-01-14 12:41:22,948 [INFO] Step[1450/2713]: training loss : 1.382277978658676 TRAIN  loss dict:  {'classification_loss': 1.382277978658676}
2025-01-14 12:41:36,781 [INFO] Step[1500/2713]: training loss : 1.260995125770569 TRAIN  loss dict:  {'classification_loss': 1.260995125770569}
2025-01-14 12:41:50,517 [INFO] Step[1550/2713]: training loss : 1.2760103833675385 TRAIN  loss dict:  {'classification_loss': 1.2760103833675385}
2025-01-14 12:42:04,047 [INFO] Step[1600/2713]: training loss : 1.267461268901825 TRAIN  loss dict:  {'classification_loss': 1.267461268901825}
2025-01-14 12:42:17,546 [INFO] Step[1650/2713]: training loss : 1.2882348489761353 TRAIN  loss dict:  {'classification_loss': 1.2882348489761353}
2025-01-14 12:42:31,135 [INFO] Step[1700/2713]: training loss : 1.2683513057231903 TRAIN  loss dict:  {'classification_loss': 1.2683513057231903}
2025-01-14 12:42:45,219 [INFO] Step[1750/2713]: training loss : 1.3175638175010682 TRAIN  loss dict:  {'classification_loss': 1.3175638175010682}
2025-01-14 12:42:58,806 [INFO] Step[1800/2713]: training loss : 1.332262955904007 TRAIN  loss dict:  {'classification_loss': 1.332262955904007}
2025-01-14 12:43:13,081 [INFO] Step[1850/2713]: training loss : 1.187497112751007 TRAIN  loss dict:  {'classification_loss': 1.187497112751007}
2025-01-14 12:43:26,858 [INFO] Step[1900/2713]: training loss : 1.2177083945274354 TRAIN  loss dict:  {'classification_loss': 1.2177083945274354}
2025-01-14 12:43:40,353 [INFO] Step[1950/2713]: training loss : 1.2445030367374421 TRAIN  loss dict:  {'classification_loss': 1.2445030367374421}
2025-01-14 12:43:53,987 [INFO] Step[2000/2713]: training loss : 1.247593448162079 TRAIN  loss dict:  {'classification_loss': 1.247593448162079}
2025-01-14 12:44:07,385 [INFO] Step[2050/2713]: training loss : 1.2548991346359253 TRAIN  loss dict:  {'classification_loss': 1.2548991346359253}
2025-01-14 12:44:20,983 [INFO] Step[2100/2713]: training loss : 1.2537954688072204 TRAIN  loss dict:  {'classification_loss': 1.2537954688072204}
2025-01-14 12:44:34,680 [INFO] Step[2150/2713]: training loss : 1.3271367609500886 TRAIN  loss dict:  {'classification_loss': 1.3271367609500886}
2025-01-14 12:44:48,324 [INFO] Step[2200/2713]: training loss : 1.2394482851028443 TRAIN  loss dict:  {'classification_loss': 1.2394482851028443}
2025-01-14 12:45:02,305 [INFO] Step[2250/2713]: training loss : 1.3070325720310212 TRAIN  loss dict:  {'classification_loss': 1.3070325720310212}
2025-01-14 12:45:15,900 [INFO] Step[2300/2713]: training loss : 1.2486816680431365 TRAIN  loss dict:  {'classification_loss': 1.2486816680431365}
2025-01-14 12:45:29,501 [INFO] Step[2350/2713]: training loss : 1.2346918761730195 TRAIN  loss dict:  {'classification_loss': 1.2346918761730195}
2025-01-14 12:45:43,080 [INFO] Step[2400/2713]: training loss : 1.2684604597091675 TRAIN  loss dict:  {'classification_loss': 1.2684604597091675}
2025-01-14 12:45:56,648 [INFO] Step[2450/2713]: training loss : 1.3412041461467743 TRAIN  loss dict:  {'classification_loss': 1.3412041461467743}
2025-01-14 12:46:10,524 [INFO] Step[2500/2713]: training loss : 1.3746373307704927 TRAIN  loss dict:  {'classification_loss': 1.3746373307704927}
2025-01-14 12:46:23,978 [INFO] Step[2550/2713]: training loss : 1.1860158300399781 TRAIN  loss dict:  {'classification_loss': 1.1860158300399781}
2025-01-14 12:46:37,370 [INFO] Step[2600/2713]: training loss : 1.2680219995975495 TRAIN  loss dict:  {'classification_loss': 1.2680219995975495}
2025-01-14 12:46:50,847 [INFO] Step[2650/2713]: training loss : 1.2707482886314392 TRAIN  loss dict:  {'classification_loss': 1.2707482886314392}
2025-01-14 12:47:04,305 [INFO] Step[2700/2713]: training loss : 1.2681548666954041 TRAIN  loss dict:  {'classification_loss': 1.2681548666954041}
2025-01-14 12:48:21,812 [INFO] Label accuracies statistics:
2025-01-14 12:48:21,812 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.25, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.25, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.75, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.25, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.75, 64: 0.5, 65: 0.75, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 0.75, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.0, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.25, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 1.0, 99: 0.8, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 1.0, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 0.75, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.5, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.25, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.0, 176: 0.75, 177: 0.75, 178: 0.5, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 1.0, 190: 0.25, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.0, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.0, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.5, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.25, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.25, 274: 0.75, 275: 0.75, 276: 0.5, 277: 0.5, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.25, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 0.75, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.5, 319: 0.75, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.25, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.25, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.25, 365: 0.5, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.25, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 12:48:21,814 [INFO] [6] TRAIN  loss: 1.2595821700762262 acc: 0.9240692959823074
2025-01-14 12:48:21,814 [INFO] [6] TRAIN  loss dict: {'classification_loss': 1.2595821700762262}
2025-01-14 12:48:21,815 [INFO] [6] VALIDATION loss: 1.9933307682885264 VALIDATION acc: 0.7385579937304075
2025-01-14 12:48:21,815 [INFO] [6] VALIDATION loss dict: {'classification_loss': 1.9933307682885264}
2025-01-14 12:48:21,815 [INFO] 
2025-01-14 12:48:40,863 [INFO] Step[50/2713]: training loss : 1.1892448604106902 TRAIN  loss dict:  {'classification_loss': 1.1892448604106902}
2025-01-14 12:48:54,418 [INFO] Step[100/2713]: training loss : 1.141563149690628 TRAIN  loss dict:  {'classification_loss': 1.141563149690628}
2025-01-14 12:49:08,211 [INFO] Step[150/2713]: training loss : 1.1609393870830536 TRAIN  loss dict:  {'classification_loss': 1.1609393870830536}
2025-01-14 12:49:21,405 [INFO] Step[200/2713]: training loss : 1.3153443455696106 TRAIN  loss dict:  {'classification_loss': 1.3153443455696106}
2025-01-14 12:49:34,659 [INFO] Step[250/2713]: training loss : 1.2339490020275117 TRAIN  loss dict:  {'classification_loss': 1.2339490020275117}
2025-01-14 12:49:48,360 [INFO] Step[300/2713]: training loss : 1.2090871167182922 TRAIN  loss dict:  {'classification_loss': 1.2090871167182922}
2025-01-14 12:50:01,896 [INFO] Step[350/2713]: training loss : 1.3375035154819488 TRAIN  loss dict:  {'classification_loss': 1.3375035154819488}
2025-01-14 12:50:15,477 [INFO] Step[400/2713]: training loss : 1.1421508514881134 TRAIN  loss dict:  {'classification_loss': 1.1421508514881134}
2025-01-14 12:50:29,434 [INFO] Step[450/2713]: training loss : 1.1708163166046142 TRAIN  loss dict:  {'classification_loss': 1.1708163166046142}
2025-01-14 12:50:43,313 [INFO] Step[500/2713]: training loss : 1.2192644596099853 TRAIN  loss dict:  {'classification_loss': 1.2192644596099853}
2025-01-14 12:50:57,022 [INFO] Step[550/2713]: training loss : 1.2461871767044068 TRAIN  loss dict:  {'classification_loss': 1.2461871767044068}
2025-01-14 12:51:10,567 [INFO] Step[600/2713]: training loss : 1.1506592118740082 TRAIN  loss dict:  {'classification_loss': 1.1506592118740082}
2025-01-14 12:51:24,173 [INFO] Step[650/2713]: training loss : 1.226905562877655 TRAIN  loss dict:  {'classification_loss': 1.226905562877655}
2025-01-14 12:51:37,988 [INFO] Step[700/2713]: training loss : 1.2061045920848847 TRAIN  loss dict:  {'classification_loss': 1.2061045920848847}
2025-01-14 12:51:52,262 [INFO] Step[750/2713]: training loss : 1.2885729372501373 TRAIN  loss dict:  {'classification_loss': 1.2885729372501373}
2025-01-14 12:52:06,504 [INFO] Step[800/2713]: training loss : 1.129493968486786 TRAIN  loss dict:  {'classification_loss': 1.129493968486786}
2025-01-14 12:52:20,105 [INFO] Step[850/2713]: training loss : 1.2902163255214691 TRAIN  loss dict:  {'classification_loss': 1.2902163255214691}
2025-01-14 12:52:33,890 [INFO] Step[900/2713]: training loss : 1.2638257491588591 TRAIN  loss dict:  {'classification_loss': 1.2638257491588591}
2025-01-14 12:52:47,478 [INFO] Step[950/2713]: training loss : 1.160553652048111 TRAIN  loss dict:  {'classification_loss': 1.160553652048111}
2025-01-14 12:53:00,894 [INFO] Step[1000/2713]: training loss : 1.2151959979534148 TRAIN  loss dict:  {'classification_loss': 1.2151959979534148}
2025-01-14 12:53:14,476 [INFO] Step[1050/2713]: training loss : 1.1617234671115875 TRAIN  loss dict:  {'classification_loss': 1.1617234671115875}
2025-01-14 12:53:28,488 [INFO] Step[1100/2713]: training loss : 1.2757509577274322 TRAIN  loss dict:  {'classification_loss': 1.2757509577274322}
2025-01-14 12:53:42,202 [INFO] Step[1150/2713]: training loss : 1.1994235146045684 TRAIN  loss dict:  {'classification_loss': 1.1994235146045684}
2025-01-14 12:53:55,863 [INFO] Step[1200/2713]: training loss : 1.187356148958206 TRAIN  loss dict:  {'classification_loss': 1.187356148958206}
2025-01-14 12:54:09,517 [INFO] Step[1250/2713]: training loss : 1.2055083894729615 TRAIN  loss dict:  {'classification_loss': 1.2055083894729615}
2025-01-14 12:54:22,782 [INFO] Step[1300/2713]: training loss : 1.2273635792732238 TRAIN  loss dict:  {'classification_loss': 1.2273635792732238}
2025-01-14 12:54:36,798 [INFO] Step[1350/2713]: training loss : 1.1597064328193665 TRAIN  loss dict:  {'classification_loss': 1.1597064328193665}
2025-01-14 12:54:50,823 [INFO] Step[1400/2713]: training loss : 1.1839368653297424 TRAIN  loss dict:  {'classification_loss': 1.1839368653297424}
2025-01-14 12:55:05,073 [INFO] Step[1450/2713]: training loss : 1.2580541145801545 TRAIN  loss dict:  {'classification_loss': 1.2580541145801545}
2025-01-14 12:55:18,902 [INFO] Step[1500/2713]: training loss : 1.2375009191036224 TRAIN  loss dict:  {'classification_loss': 1.2375009191036224}
2025-01-14 12:55:33,000 [INFO] Step[1550/2713]: training loss : 1.1764450192451477 TRAIN  loss dict:  {'classification_loss': 1.1764450192451477}
2025-01-14 12:55:46,968 [INFO] Step[1600/2713]: training loss : 1.2086259412765503 TRAIN  loss dict:  {'classification_loss': 1.2086259412765503}
2025-01-14 12:56:00,685 [INFO] Step[1650/2713]: training loss : 1.2839017117023468 TRAIN  loss dict:  {'classification_loss': 1.2839017117023468}
2025-01-14 12:56:13,952 [INFO] Step[1700/2713]: training loss : 1.1913309121131896 TRAIN  loss dict:  {'classification_loss': 1.1913309121131896}
2025-01-14 12:56:28,021 [INFO] Step[1750/2713]: training loss : 1.224685890674591 TRAIN  loss dict:  {'classification_loss': 1.224685890674591}
2025-01-14 12:56:41,858 [INFO] Step[1800/2713]: training loss : 1.2551405775547027 TRAIN  loss dict:  {'classification_loss': 1.2551405775547027}
2025-01-14 12:56:56,121 [INFO] Step[1850/2713]: training loss : 1.1877629196643829 TRAIN  loss dict:  {'classification_loss': 1.1877629196643829}
2025-01-14 12:57:12,515 [INFO] Step[1900/2713]: training loss : 1.2438720655441284 TRAIN  loss dict:  {'classification_loss': 1.2438720655441284}
2025-01-14 12:57:28,170 [INFO] Step[1950/2713]: training loss : 1.262783180475235 TRAIN  loss dict:  {'classification_loss': 1.262783180475235}
2025-01-14 12:57:41,742 [INFO] Step[2000/2713]: training loss : 1.2124701499938966 TRAIN  loss dict:  {'classification_loss': 1.2124701499938966}
2025-01-14 12:57:55,264 [INFO] Step[2050/2713]: training loss : 1.17592041015625 TRAIN  loss dict:  {'classification_loss': 1.17592041015625}
2025-01-14 12:58:09,220 [INFO] Step[2100/2713]: training loss : 1.1772083640098572 TRAIN  loss dict:  {'classification_loss': 1.1772083640098572}
2025-01-14 12:58:22,534 [INFO] Step[2150/2713]: training loss : 1.2637722301483154 TRAIN  loss dict:  {'classification_loss': 1.2637722301483154}
2025-01-14 12:58:36,721 [INFO] Step[2200/2713]: training loss : 1.2437574589252471 TRAIN  loss dict:  {'classification_loss': 1.2437574589252471}
2025-01-14 12:58:50,328 [INFO] Step[2250/2713]: training loss : 1.2434072852134705 TRAIN  loss dict:  {'classification_loss': 1.2434072852134705}
2025-01-14 12:59:04,357 [INFO] Step[2300/2713]: training loss : 1.1937191319465636 TRAIN  loss dict:  {'classification_loss': 1.1937191319465636}
2025-01-14 12:59:18,309 [INFO] Step[2350/2713]: training loss : 1.2725856018066406 TRAIN  loss dict:  {'classification_loss': 1.2725856018066406}
2025-01-14 12:59:31,553 [INFO] Step[2400/2713]: training loss : 1.1871756327152252 TRAIN  loss dict:  {'classification_loss': 1.1871756327152252}
2025-01-14 12:59:45,018 [INFO] Step[2450/2713]: training loss : 1.182619220018387 TRAIN  loss dict:  {'classification_loss': 1.182619220018387}
2025-01-14 12:59:58,536 [INFO] Step[2500/2713]: training loss : 1.1521956634521484 TRAIN  loss dict:  {'classification_loss': 1.1521956634521484}
2025-01-14 13:00:12,604 [INFO] Step[2550/2713]: training loss : 1.328403103351593 TRAIN  loss dict:  {'classification_loss': 1.328403103351593}
2025-01-14 13:00:26,464 [INFO] Step[2600/2713]: training loss : 1.2136293351650238 TRAIN  loss dict:  {'classification_loss': 1.2136293351650238}
2025-01-14 13:00:40,100 [INFO] Step[2650/2713]: training loss : 1.2209991979599 TRAIN  loss dict:  {'classification_loss': 1.2209991979599}
2025-01-14 13:00:54,403 [INFO] Step[2700/2713]: training loss : 1.224001680612564 TRAIN  loss dict:  {'classification_loss': 1.224001680612564}
2025-01-14 13:02:11,341 [INFO] Label accuracies statistics:
2025-01-14 13:02:11,342 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.25, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.0, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.5, 120: 0.5, 121: 0.75, 122: 1.0, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.5, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.5, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 0.75, 242: 0.25, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.3333333333333333, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.5, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.25, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.25, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.5, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 0.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.5, 302: 1.0, 303: 0.5, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 0.75, 309: 0.5, 310: 1.0, 311: 0.75, 312: 1.0, 313: 0.5, 314: 1.0, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.5, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.75, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.5, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.0, 350: 0.25, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 0.5, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.5, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.75, 377: 0.5, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.25, 382: 0.75, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 13:02:13,700 [INFO] [7] TRAIN  loss: 1.2168351204422536 acc: 0.9389359872220174
2025-01-14 13:02:13,700 [INFO] [7] TRAIN  loss dict: {'classification_loss': 1.2168351204422536}
2025-01-14 13:02:13,700 [INFO] [7] VALIDATION loss: 1.9260254826088596 VALIDATION acc: 0.7510971786833855
2025-01-14 13:02:13,700 [INFO] [7] VALIDATION loss dict: {'classification_loss': 1.9260254826088596}
2025-01-14 13:02:13,701 [INFO] 
2025-01-14 13:02:31,812 [INFO] Step[50/2713]: training loss : 1.1640333664417266 TRAIN  loss dict:  {'classification_loss': 1.1640333664417266}
2025-01-14 13:02:46,054 [INFO] Step[100/2713]: training loss : 1.1650185430049895 TRAIN  loss dict:  {'classification_loss': 1.1650185430049895}
2025-01-14 13:02:59,649 [INFO] Step[150/2713]: training loss : 1.2102385139465333 TRAIN  loss dict:  {'classification_loss': 1.2102385139465333}
2025-01-14 13:03:12,937 [INFO] Step[200/2713]: training loss : 1.1655150961875915 TRAIN  loss dict:  {'classification_loss': 1.1655150961875915}
2025-01-14 13:03:26,233 [INFO] Step[250/2713]: training loss : 1.181865358352661 TRAIN  loss dict:  {'classification_loss': 1.181865358352661}
2025-01-14 13:03:40,164 [INFO] Step[300/2713]: training loss : 1.1871811878681182 TRAIN  loss dict:  {'classification_loss': 1.1871811878681182}
2025-01-14 13:03:54,111 [INFO] Step[350/2713]: training loss : 1.2119354701042175 TRAIN  loss dict:  {'classification_loss': 1.2119354701042175}
2025-01-14 13:04:07,627 [INFO] Step[400/2713]: training loss : 1.3010188138484955 TRAIN  loss dict:  {'classification_loss': 1.3010188138484955}
2025-01-14 13:04:21,191 [INFO] Step[450/2713]: training loss : 1.1115844202041627 TRAIN  loss dict:  {'classification_loss': 1.1115844202041627}
2025-01-14 13:04:34,442 [INFO] Step[500/2713]: training loss : 1.2748240053653717 TRAIN  loss dict:  {'classification_loss': 1.2748240053653717}
2025-01-14 13:04:48,121 [INFO] Step[550/2713]: training loss : 1.212880754470825 TRAIN  loss dict:  {'classification_loss': 1.212880754470825}
2025-01-14 13:05:02,538 [INFO] Step[600/2713]: training loss : 1.2174649131298065 TRAIN  loss dict:  {'classification_loss': 1.2174649131298065}
2025-01-14 13:05:16,485 [INFO] Step[650/2713]: training loss : 1.1228780364990234 TRAIN  loss dict:  {'classification_loss': 1.1228780364990234}
2025-01-14 13:05:30,111 [INFO] Step[700/2713]: training loss : 1.1884708523750305 TRAIN  loss dict:  {'classification_loss': 1.1884708523750305}
2025-01-14 13:05:43,657 [INFO] Step[750/2713]: training loss : 1.1984791076183319 TRAIN  loss dict:  {'classification_loss': 1.1984791076183319}
2025-01-14 13:05:57,327 [INFO] Step[800/2713]: training loss : 1.2295402324199676 TRAIN  loss dict:  {'classification_loss': 1.2295402324199676}
2025-01-14 13:06:10,671 [INFO] Step[850/2713]: training loss : 1.169318288564682 TRAIN  loss dict:  {'classification_loss': 1.169318288564682}
2025-01-14 13:06:24,514 [INFO] Step[900/2713]: training loss : 1.2734703755378722 TRAIN  loss dict:  {'classification_loss': 1.2734703755378722}
2025-01-14 13:06:38,157 [INFO] Step[950/2713]: training loss : 1.171880033016205 TRAIN  loss dict:  {'classification_loss': 1.171880033016205}
2025-01-14 13:06:52,464 [INFO] Step[1000/2713]: training loss : 1.1896118748188018 TRAIN  loss dict:  {'classification_loss': 1.1896118748188018}
2025-01-14 13:07:06,214 [INFO] Step[1050/2713]: training loss : 1.212992845773697 TRAIN  loss dict:  {'classification_loss': 1.212992845773697}
2025-01-14 13:07:20,409 [INFO] Step[1100/2713]: training loss : 1.2198347783088683 TRAIN  loss dict:  {'classification_loss': 1.2198347783088683}
2025-01-14 13:07:33,756 [INFO] Step[1150/2713]: training loss : 1.1898946535587311 TRAIN  loss dict:  {'classification_loss': 1.1898946535587311}
2025-01-14 13:07:48,045 [INFO] Step[1200/2713]: training loss : 1.1240206348896027 TRAIN  loss dict:  {'classification_loss': 1.1240206348896027}
2025-01-14 13:08:01,680 [INFO] Step[1250/2713]: training loss : 1.1829469215869903 TRAIN  loss dict:  {'classification_loss': 1.1829469215869903}
2025-01-14 13:08:15,272 [INFO] Step[1300/2713]: training loss : 1.2492000877857208 TRAIN  loss dict:  {'classification_loss': 1.2492000877857208}
2025-01-14 13:08:28,613 [INFO] Step[1350/2713]: training loss : 1.1146697342395782 TRAIN  loss dict:  {'classification_loss': 1.1146697342395782}
2025-01-14 13:08:42,470 [INFO] Step[1400/2713]: training loss : 1.1327204632759094 TRAIN  loss dict:  {'classification_loss': 1.1327204632759094}
2025-01-14 13:08:56,540 [INFO] Step[1450/2713]: training loss : 1.2105423665046693 TRAIN  loss dict:  {'classification_loss': 1.2105423665046693}
2025-01-14 13:09:12,656 [INFO] Step[1500/2713]: training loss : 1.1674727272987366 TRAIN  loss dict:  {'classification_loss': 1.1674727272987366}
2025-01-14 13:09:27,891 [INFO] Step[1550/2713]: training loss : 1.1665894877910614 TRAIN  loss dict:  {'classification_loss': 1.1665894877910614}
2025-01-14 13:09:41,201 [INFO] Step[1600/2713]: training loss : 1.200209629535675 TRAIN  loss dict:  {'classification_loss': 1.200209629535675}
2025-01-14 13:09:55,061 [INFO] Step[1650/2713]: training loss : 1.1937717711925506 TRAIN  loss dict:  {'classification_loss': 1.1937717711925506}
2025-01-14 13:10:08,598 [INFO] Step[1700/2713]: training loss : 1.1350623643398285 TRAIN  loss dict:  {'classification_loss': 1.1350623643398285}
2025-01-14 13:10:22,643 [INFO] Step[1750/2713]: training loss : 1.1885842180252075 TRAIN  loss dict:  {'classification_loss': 1.1885842180252075}
2025-01-14 13:10:36,176 [INFO] Step[1800/2713]: training loss : 1.1915278482437133 TRAIN  loss dict:  {'classification_loss': 1.1915278482437133}
2025-01-14 13:10:50,024 [INFO] Step[1850/2713]: training loss : 1.2370418810844421 TRAIN  loss dict:  {'classification_loss': 1.2370418810844421}
2025-01-14 13:11:03,629 [INFO] Step[1900/2713]: training loss : 1.2100110030174256 TRAIN  loss dict:  {'classification_loss': 1.2100110030174256}
2025-01-14 13:11:17,664 [INFO] Step[1950/2713]: training loss : 1.2409903728961944 TRAIN  loss dict:  {'classification_loss': 1.2409903728961944}
2025-01-14 13:11:31,494 [INFO] Step[2000/2713]: training loss : 1.187764562368393 TRAIN  loss dict:  {'classification_loss': 1.187764562368393}
2025-01-14 13:11:44,800 [INFO] Step[2050/2713]: training loss : 1.2392837798595429 TRAIN  loss dict:  {'classification_loss': 1.2392837798595429}
2025-01-14 13:11:58,353 [INFO] Step[2100/2713]: training loss : 1.2826170170307158 TRAIN  loss dict:  {'classification_loss': 1.2826170170307158}
2025-01-14 13:12:12,271 [INFO] Step[2150/2713]: training loss : 1.2353395164012908 TRAIN  loss dict:  {'classification_loss': 1.2353395164012908}
2025-01-14 13:12:26,023 [INFO] Step[2200/2713]: training loss : 1.2987014734745026 TRAIN  loss dict:  {'classification_loss': 1.2987014734745026}
2025-01-14 13:12:39,648 [INFO] Step[2250/2713]: training loss : 1.1836391878128052 TRAIN  loss dict:  {'classification_loss': 1.1836391878128052}
2025-01-14 13:12:53,207 [INFO] Step[2300/2713]: training loss : 1.1552393317222596 TRAIN  loss dict:  {'classification_loss': 1.1552393317222596}
2025-01-14 13:13:06,879 [INFO] Step[2350/2713]: training loss : 1.2213636088371276 TRAIN  loss dict:  {'classification_loss': 1.2213636088371276}
2025-01-14 13:13:20,937 [INFO] Step[2400/2713]: training loss : 1.1858920300006865 TRAIN  loss dict:  {'classification_loss': 1.1858920300006865}
2025-01-14 13:13:34,976 [INFO] Step[2450/2713]: training loss : 1.1387396848201752 TRAIN  loss dict:  {'classification_loss': 1.1387396848201752}
2025-01-14 13:13:48,974 [INFO] Step[2500/2713]: training loss : 1.2107427310943604 TRAIN  loss dict:  {'classification_loss': 1.2107427310943604}
2025-01-14 13:14:02,772 [INFO] Step[2550/2713]: training loss : 1.136656472682953 TRAIN  loss dict:  {'classification_loss': 1.136656472682953}
2025-01-14 13:14:16,895 [INFO] Step[2600/2713]: training loss : 1.264552608728409 TRAIN  loss dict:  {'classification_loss': 1.264552608728409}
2025-01-14 13:14:30,203 [INFO] Step[2650/2713]: training loss : 1.244272894859314 TRAIN  loss dict:  {'classification_loss': 1.244272894859314}
2025-01-14 13:14:43,890 [INFO] Step[2700/2713]: training loss : 1.194330621957779 TRAIN  loss dict:  {'classification_loss': 1.194330621957779}
2025-01-14 13:16:00,283 [INFO] Label accuracies statistics:
2025-01-14 13:16:00,283 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 1.0, 21: 0.5, 22: 0.75, 23: 1.0, 24: 0.5, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.25, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 1.0, 57: 0.5, 58: 0.5, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.5, 131: 1.0, 132: 1.0, 133: 0.5, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.25, 143: 1.0, 144: 0.75, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.3333333333333333, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 0.75, 179: 0.0, 180: 0.25, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.5, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 1.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.0, 207: 0.5, 208: 0.5, 209: 0.5, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.25, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.25, 236: 0.75, 237: 0.25, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.5, 254: 0.75, 255: 1.0, 256: 0.5, 257: 0.5, 258: 0.75, 259: 0.25, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.25, 268: 0.25, 269: 0.75, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.25, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.5, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 0.75, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.0, 355: 0.75, 356: 0.75, 357: 0.5, 358: 0.75, 359: 1.0, 360: 0.5, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.5, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.25, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.0, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.5, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 13:16:00,285 [INFO] [8] TRAIN  loss: 1.1977880948415067 acc: 0.9433591350288734
2025-01-14 13:16:00,285 [INFO] [8] TRAIN  loss dict: {'classification_loss': 1.1977880948415067}
2025-01-14 13:16:00,285 [INFO] [8] VALIDATION loss: 1.9810970327011626 VALIDATION acc: 0.7373040752351098
2025-01-14 13:16:00,285 [INFO] [8] VALIDATION loss dict: {'classification_loss': 1.9810970327011626}
2025-01-14 13:16:00,285 [INFO] 
2025-01-14 13:16:19,323 [INFO] Step[50/2713]: training loss : 1.232753849029541 TRAIN  loss dict:  {'classification_loss': 1.232753849029541}
2025-01-14 13:16:32,708 [INFO] Step[100/2713]: training loss : 1.1572879481315612 TRAIN  loss dict:  {'classification_loss': 1.1572879481315612}
2025-01-14 13:16:46,696 [INFO] Step[150/2713]: training loss : 1.103807339668274 TRAIN  loss dict:  {'classification_loss': 1.103807339668274}
2025-01-14 13:17:00,237 [INFO] Step[200/2713]: training loss : 1.2292119574546814 TRAIN  loss dict:  {'classification_loss': 1.2292119574546814}
2025-01-14 13:17:13,904 [INFO] Step[250/2713]: training loss : 1.2118005621433259 TRAIN  loss dict:  {'classification_loss': 1.2118005621433259}
2025-01-14 13:17:27,413 [INFO] Step[300/2713]: training loss : 1.1873335468769073 TRAIN  loss dict:  {'classification_loss': 1.1873335468769073}
2025-01-14 13:17:41,398 [INFO] Step[350/2713]: training loss : 1.23222092628479 TRAIN  loss dict:  {'classification_loss': 1.23222092628479}
2025-01-14 13:17:55,627 [INFO] Step[400/2713]: training loss : 1.1927060890197754 TRAIN  loss dict:  {'classification_loss': 1.1927060890197754}
2025-01-14 13:18:09,314 [INFO] Step[450/2713]: training loss : 1.205281183719635 TRAIN  loss dict:  {'classification_loss': 1.205281183719635}
2025-01-14 13:18:23,465 [INFO] Step[500/2713]: training loss : 1.1455247437953948 TRAIN  loss dict:  {'classification_loss': 1.1455247437953948}
2025-01-14 13:18:37,103 [INFO] Step[550/2713]: training loss : 1.2060951066017152 TRAIN  loss dict:  {'classification_loss': 1.2060951066017152}
2025-01-14 13:18:52,922 [INFO] Step[600/2713]: training loss : 1.1723244714736938 TRAIN  loss dict:  {'classification_loss': 1.1723244714736938}
2025-01-14 13:19:08,397 [INFO] Step[650/2713]: training loss : 1.2364520287513734 TRAIN  loss dict:  {'classification_loss': 1.2364520287513734}
2025-01-14 13:19:21,977 [INFO] Step[700/2713]: training loss : 1.1484745919704438 TRAIN  loss dict:  {'classification_loss': 1.1484745919704438}
2025-01-14 13:19:35,970 [INFO] Step[750/2713]: training loss : 1.1911716890335082 TRAIN  loss dict:  {'classification_loss': 1.1911716890335082}
2025-01-14 13:19:49,662 [INFO] Step[800/2713]: training loss : 1.1226692771911622 TRAIN  loss dict:  {'classification_loss': 1.1226692771911622}
2025-01-14 13:20:03,142 [INFO] Step[850/2713]: training loss : 1.2046712601184846 TRAIN  loss dict:  {'classification_loss': 1.2046712601184846}
2025-01-14 13:20:16,911 [INFO] Step[900/2713]: training loss : 1.1912735676765442 TRAIN  loss dict:  {'classification_loss': 1.1912735676765442}
2025-01-14 13:20:30,104 [INFO] Step[950/2713]: training loss : 1.2408707022666932 TRAIN  loss dict:  {'classification_loss': 1.2408707022666932}
2025-01-14 13:20:43,866 [INFO] Step[1000/2713]: training loss : 1.1625521981716156 TRAIN  loss dict:  {'classification_loss': 1.1625521981716156}
2025-01-14 13:20:57,567 [INFO] Step[1050/2713]: training loss : 1.1448737561702729 TRAIN  loss dict:  {'classification_loss': 1.1448737561702729}
2025-01-14 13:21:11,471 [INFO] Step[1100/2713]: training loss : 1.1658684492111206 TRAIN  loss dict:  {'classification_loss': 1.1658684492111206}
2025-01-14 13:21:25,298 [INFO] Step[1150/2713]: training loss : 1.1624344575405121 TRAIN  loss dict:  {'classification_loss': 1.1624344575405121}
2025-01-14 13:21:38,524 [INFO] Step[1200/2713]: training loss : 1.148383013010025 TRAIN  loss dict:  {'classification_loss': 1.148383013010025}
2025-01-14 13:21:52,328 [INFO] Step[1250/2713]: training loss : 1.1556922209262848 TRAIN  loss dict:  {'classification_loss': 1.1556922209262848}
2025-01-14 13:22:06,147 [INFO] Step[1300/2713]: training loss : 1.1103427660465242 TRAIN  loss dict:  {'classification_loss': 1.1103427660465242}
2025-01-14 13:22:19,641 [INFO] Step[1350/2713]: training loss : 1.2507451260089875 TRAIN  loss dict:  {'classification_loss': 1.2507451260089875}
2025-01-14 13:22:33,439 [INFO] Step[1400/2713]: training loss : 1.1361481821537018 TRAIN  loss dict:  {'classification_loss': 1.1361481821537018}
2025-01-14 13:22:46,734 [INFO] Step[1450/2713]: training loss : 1.183782331943512 TRAIN  loss dict:  {'classification_loss': 1.183782331943512}
2025-01-14 13:22:59,940 [INFO] Step[1500/2713]: training loss : 1.224346910715103 TRAIN  loss dict:  {'classification_loss': 1.224346910715103}
2025-01-14 13:23:13,146 [INFO] Step[1550/2713]: training loss : 1.1595561814308166 TRAIN  loss dict:  {'classification_loss': 1.1595561814308166}
2025-01-14 13:23:26,374 [INFO] Step[1600/2713]: training loss : 1.161782113313675 TRAIN  loss dict:  {'classification_loss': 1.161782113313675}
2025-01-14 13:23:39,926 [INFO] Step[1650/2713]: training loss : 1.2422864472866058 TRAIN  loss dict:  {'classification_loss': 1.2422864472866058}
2025-01-14 13:23:53,318 [INFO] Step[1700/2713]: training loss : 1.2048716616630555 TRAIN  loss dict:  {'classification_loss': 1.2048716616630555}
2025-01-14 13:24:07,270 [INFO] Step[1750/2713]: training loss : 1.1948006665706634 TRAIN  loss dict:  {'classification_loss': 1.1948006665706634}
2025-01-14 13:24:21,007 [INFO] Step[1800/2713]: training loss : 1.1590399360656738 TRAIN  loss dict:  {'classification_loss': 1.1590399360656738}
2025-01-14 13:24:34,822 [INFO] Step[1850/2713]: training loss : 1.165755113363266 TRAIN  loss dict:  {'classification_loss': 1.165755113363266}
2025-01-14 13:24:48,799 [INFO] Step[1900/2713]: training loss : 1.1094305658340453 TRAIN  loss dict:  {'classification_loss': 1.1094305658340453}
2025-01-14 13:25:02,285 [INFO] Step[1950/2713]: training loss : 1.1752488601207733 TRAIN  loss dict:  {'classification_loss': 1.1752488601207733}
2025-01-14 13:25:16,140 [INFO] Step[2000/2713]: training loss : 1.2053717041015626 TRAIN  loss dict:  {'classification_loss': 1.2053717041015626}
2025-01-14 13:25:29,564 [INFO] Step[2050/2713]: training loss : 1.1552562379837037 TRAIN  loss dict:  {'classification_loss': 1.1552562379837037}
2025-01-14 13:25:43,618 [INFO] Step[2100/2713]: training loss : 1.1882241547107697 TRAIN  loss dict:  {'classification_loss': 1.1882241547107697}
2025-01-14 13:25:56,917 [INFO] Step[2150/2713]: training loss : 1.2621537172794342 TRAIN  loss dict:  {'classification_loss': 1.2621537172794342}
2025-01-14 13:26:11,152 [INFO] Step[2200/2713]: training loss : 1.1637737464904785 TRAIN  loss dict:  {'classification_loss': 1.1637737464904785}
2025-01-14 13:26:24,940 [INFO] Step[2250/2713]: training loss : 1.227510826587677 TRAIN  loss dict:  {'classification_loss': 1.227510826587677}
2025-01-14 13:26:38,138 [INFO] Step[2300/2713]: training loss : 1.1775779139995575 TRAIN  loss dict:  {'classification_loss': 1.1775779139995575}
2025-01-14 13:26:51,698 [INFO] Step[2350/2713]: training loss : 1.1208660423755645 TRAIN  loss dict:  {'classification_loss': 1.1208660423755645}
2025-01-14 13:27:05,396 [INFO] Step[2400/2713]: training loss : 1.1610150301456452 TRAIN  loss dict:  {'classification_loss': 1.1610150301456452}
2025-01-14 13:27:18,802 [INFO] Step[2450/2713]: training loss : 1.1898512065410614 TRAIN  loss dict:  {'classification_loss': 1.1898512065410614}
2025-01-14 13:27:32,934 [INFO] Step[2500/2713]: training loss : 1.1268177938461303 TRAIN  loss dict:  {'classification_loss': 1.1268177938461303}
2025-01-14 13:27:46,868 [INFO] Step[2550/2713]: training loss : 1.1399517238140107 TRAIN  loss dict:  {'classification_loss': 1.1399517238140107}
2025-01-14 13:28:00,766 [INFO] Step[2600/2713]: training loss : 1.1976117205619812 TRAIN  loss dict:  {'classification_loss': 1.1976117205619812}
2025-01-14 13:28:14,621 [INFO] Step[2650/2713]: training loss : 1.1365933656692504 TRAIN  loss dict:  {'classification_loss': 1.1365933656692504}
2025-01-14 13:28:28,355 [INFO] Step[2700/2713]: training loss : 1.174204750061035 TRAIN  loss dict:  {'classification_loss': 1.174204750061035}
2025-01-14 13:29:44,881 [INFO] Label accuracies statistics:
2025-01-14 13:29:44,881 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.25, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.5, 19: 0.5, 20: 1.0, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.5, 57: 0.75, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 1.0, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.5, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.5, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.5, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.25, 214: 0.75, 215: 1.0, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.5, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.0, 238: 1.0, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 1.0, 260: 0.75, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.5, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.0, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.5, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 0.75, 358: 1.0, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.5, 364: 0.5, 365: 0.75, 366: 0.75, 367: 0.5, 368: 0.75, 369: 0.75, 370: 0.75, 371: 0.0, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.25, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.0, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 13:29:47,226 [INFO] [9] TRAIN  loss: 1.1788654645078964 acc: 0.9470450915345866
2025-01-14 13:29:47,226 [INFO] [9] TRAIN  loss dict: {'classification_loss': 1.1788654645078964}
2025-01-14 13:29:47,226 [INFO] [9] VALIDATION loss: 1.904580073921304 VALIDATION acc: 0.7636363636363637
2025-01-14 13:29:47,226 [INFO] [9] VALIDATION loss dict: {'classification_loss': 1.904580073921304}
2025-01-14 13:29:47,226 [INFO] 
2025-01-14 13:30:05,511 [INFO] Step[50/2713]: training loss : 1.1925252127647399 TRAIN  loss dict:  {'classification_loss': 1.1925252127647399}
2025-01-14 13:30:19,293 [INFO] Step[100/2713]: training loss : 1.164877771139145 TRAIN  loss dict:  {'classification_loss': 1.164877771139145}
2025-01-14 13:30:32,782 [INFO] Step[150/2713]: training loss : 1.1270565867424012 TRAIN  loss dict:  {'classification_loss': 1.1270565867424012}
2025-01-14 13:30:46,626 [INFO] Step[200/2713]: training loss : 1.1877183413505554 TRAIN  loss dict:  {'classification_loss': 1.1877183413505554}
2025-01-14 13:31:00,033 [INFO] Step[250/2713]: training loss : 1.1238961219787598 TRAIN  loss dict:  {'classification_loss': 1.1238961219787598}
2025-01-14 13:31:14,021 [INFO] Step[300/2713]: training loss : 1.2085738575458527 TRAIN  loss dict:  {'classification_loss': 1.2085738575458527}
2025-01-14 13:31:27,690 [INFO] Step[350/2713]: training loss : 1.1596859252452851 TRAIN  loss dict:  {'classification_loss': 1.1596859252452851}
2025-01-14 13:31:41,974 [INFO] Step[400/2713]: training loss : 1.1747491800785064 TRAIN  loss dict:  {'classification_loss': 1.1747491800785064}
2025-01-14 13:31:55,878 [INFO] Step[450/2713]: training loss : 1.182590333223343 TRAIN  loss dict:  {'classification_loss': 1.182590333223343}
2025-01-14 13:32:09,477 [INFO] Step[500/2713]: training loss : 1.1789703702926635 TRAIN  loss dict:  {'classification_loss': 1.1789703702926635}
2025-01-14 13:32:23,343 [INFO] Step[550/2713]: training loss : 1.1905893719196319 TRAIN  loss dict:  {'classification_loss': 1.1905893719196319}
2025-01-14 13:32:37,126 [INFO] Step[600/2713]: training loss : 1.1466858768463135 TRAIN  loss dict:  {'classification_loss': 1.1466858768463135}
2025-01-14 13:32:50,957 [INFO] Step[650/2713]: training loss : 1.1410795116424561 TRAIN  loss dict:  {'classification_loss': 1.1410795116424561}
2025-01-14 13:33:04,511 [INFO] Step[700/2713]: training loss : 1.120259462594986 TRAIN  loss dict:  {'classification_loss': 1.120259462594986}
2025-01-14 13:33:18,186 [INFO] Step[750/2713]: training loss : 1.1775150799751282 TRAIN  loss dict:  {'classification_loss': 1.1775150799751282}
2025-01-14 13:33:31,776 [INFO] Step[800/2713]: training loss : 1.1400343513488769 TRAIN  loss dict:  {'classification_loss': 1.1400343513488769}
2025-01-14 13:33:45,692 [INFO] Step[850/2713]: training loss : 1.0987768793106079 TRAIN  loss dict:  {'classification_loss': 1.0987768793106079}
2025-01-14 13:33:59,683 [INFO] Step[900/2713]: training loss : 1.2185369634628296 TRAIN  loss dict:  {'classification_loss': 1.2185369634628296}
2025-01-14 13:34:13,455 [INFO] Step[950/2713]: training loss : 1.1508133816719055 TRAIN  loss dict:  {'classification_loss': 1.1508133816719055}
2025-01-14 13:34:27,141 [INFO] Step[1000/2713]: training loss : 1.1344664072990418 TRAIN  loss dict:  {'classification_loss': 1.1344664072990418}
2025-01-14 13:34:40,596 [INFO] Step[1050/2713]: training loss : 1.0929635334014893 TRAIN  loss dict:  {'classification_loss': 1.0929635334014893}
2025-01-14 13:34:54,118 [INFO] Step[1100/2713]: training loss : 1.1766819143295288 TRAIN  loss dict:  {'classification_loss': 1.1766819143295288}
2025-01-14 13:35:07,934 [INFO] Step[1150/2713]: training loss : 1.1372222137451171 TRAIN  loss dict:  {'classification_loss': 1.1372222137451171}
2025-01-14 13:35:21,948 [INFO] Step[1200/2713]: training loss : 1.142365756034851 TRAIN  loss dict:  {'classification_loss': 1.142365756034851}
2025-01-14 13:35:35,514 [INFO] Step[1250/2713]: training loss : 1.1183761262893677 TRAIN  loss dict:  {'classification_loss': 1.1183761262893677}
2025-01-14 13:35:49,573 [INFO] Step[1300/2713]: training loss : 1.173262006044388 TRAIN  loss dict:  {'classification_loss': 1.173262006044388}
2025-01-14 13:36:03,815 [INFO] Step[1350/2713]: training loss : 1.18684352517128 TRAIN  loss dict:  {'classification_loss': 1.18684352517128}
2025-01-14 13:36:17,439 [INFO] Step[1400/2713]: training loss : 1.1653436195850373 TRAIN  loss dict:  {'classification_loss': 1.1653436195850373}
2025-01-14 13:36:30,987 [INFO] Step[1450/2713]: training loss : 1.1088680481910707 TRAIN  loss dict:  {'classification_loss': 1.1088680481910707}
2025-01-14 13:36:45,132 [INFO] Step[1500/2713]: training loss : 1.1128614926338196 TRAIN  loss dict:  {'classification_loss': 1.1128614926338196}
2025-01-14 13:36:59,253 [INFO] Step[1550/2713]: training loss : 1.1656861197948456 TRAIN  loss dict:  {'classification_loss': 1.1656861197948456}
2025-01-14 13:37:12,474 [INFO] Step[1600/2713]: training loss : 1.1450253212451935 TRAIN  loss dict:  {'classification_loss': 1.1450253212451935}
2025-01-14 13:37:25,711 [INFO] Step[1650/2713]: training loss : 1.0970486044883727 TRAIN  loss dict:  {'classification_loss': 1.0970486044883727}
2025-01-14 13:37:39,147 [INFO] Step[1700/2713]: training loss : 1.1692783963680267 TRAIN  loss dict:  {'classification_loss': 1.1692783963680267}
2025-01-14 13:37:52,850 [INFO] Step[1750/2713]: training loss : 1.1240733504295348 TRAIN  loss dict:  {'classification_loss': 1.1240733504295348}
2025-01-14 13:38:06,346 [INFO] Step[1800/2713]: training loss : 1.1125997924804687 TRAIN  loss dict:  {'classification_loss': 1.1125997924804687}
2025-01-14 13:38:20,204 [INFO] Step[1850/2713]: training loss : 1.1910370802879333 TRAIN  loss dict:  {'classification_loss': 1.1910370802879333}
2025-01-14 13:38:33,410 [INFO] Step[1900/2713]: training loss : 1.133320767879486 TRAIN  loss dict:  {'classification_loss': 1.133320767879486}
2025-01-14 13:38:46,924 [INFO] Step[1950/2713]: training loss : 1.1231521368026733 TRAIN  loss dict:  {'classification_loss': 1.1231521368026733}
2025-01-14 13:39:00,205 [INFO] Step[2000/2713]: training loss : 1.0783510386943818 TRAIN  loss dict:  {'classification_loss': 1.0783510386943818}
2025-01-14 13:39:14,223 [INFO] Step[2050/2713]: training loss : 1.1804604566097259 TRAIN  loss dict:  {'classification_loss': 1.1804604566097259}
2025-01-14 13:39:28,027 [INFO] Step[2100/2713]: training loss : 1.1143386304378509 TRAIN  loss dict:  {'classification_loss': 1.1143386304378509}
2025-01-14 13:39:41,671 [INFO] Step[2150/2713]: training loss : 1.217207762002945 TRAIN  loss dict:  {'classification_loss': 1.217207762002945}
2025-01-14 13:39:55,900 [INFO] Step[2200/2713]: training loss : 1.2313623106479645 TRAIN  loss dict:  {'classification_loss': 1.2313623106479645}
2025-01-14 13:40:09,260 [INFO] Step[2250/2713]: training loss : 1.1998904252052307 TRAIN  loss dict:  {'classification_loss': 1.1998904252052307}
2025-01-14 13:40:22,835 [INFO] Step[2300/2713]: training loss : 1.1344747698307038 TRAIN  loss dict:  {'classification_loss': 1.1344747698307038}
2025-01-14 13:40:36,824 [INFO] Step[2350/2713]: training loss : 1.0841220152378082 TRAIN  loss dict:  {'classification_loss': 1.0841220152378082}
2025-01-14 13:40:50,159 [INFO] Step[2400/2713]: training loss : 1.1477358734607697 TRAIN  loss dict:  {'classification_loss': 1.1477358734607697}
2025-01-14 13:41:03,688 [INFO] Step[2450/2713]: training loss : 1.1114917969703675 TRAIN  loss dict:  {'classification_loss': 1.1114917969703675}
2025-01-14 13:41:17,789 [INFO] Step[2500/2713]: training loss : 1.1827385449409484 TRAIN  loss dict:  {'classification_loss': 1.1827385449409484}
2025-01-14 13:41:31,979 [INFO] Step[2550/2713]: training loss : 1.2525399827957153 TRAIN  loss dict:  {'classification_loss': 1.2525399827957153}
2025-01-14 13:41:45,833 [INFO] Step[2600/2713]: training loss : 1.1959260869026185 TRAIN  loss dict:  {'classification_loss': 1.1959260869026185}
2025-01-14 13:41:59,785 [INFO] Step[2650/2713]: training loss : 1.1700773811340333 TRAIN  loss dict:  {'classification_loss': 1.1700773811340333}
2025-01-14 13:42:13,944 [INFO] Step[2700/2713]: training loss : 1.1850389015674592 TRAIN  loss dict:  {'classification_loss': 1.1850389015674592}
2025-01-14 13:43:33,175 [INFO] Label accuracies statistics:
2025-01-14 13:43:33,175 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.25, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.5, 58: 1.0, 59: 0.5, 60: 1.0, 61: 0.5, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.5, 71: 0.75, 72: 0.75, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.0, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 0.75, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 1.0, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.25, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.5, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 0.75, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.5, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.5, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 1.0, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.5, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.75, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.5, 234: 0.5, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 0.75, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.3333333333333333, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.25, 257: 1.0, 258: 0.25, 259: 1.0, 260: 0.75, 261: 1.0, 262: 0.75, 263: 0.25, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.5, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.25, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.5, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 0.75, 330: 0.5, 331: 0.75, 332: 0.75, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.25, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.25, 357: 0.75, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.0, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 0.5, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.75, 394: 0.0, 395: 1.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 13:43:33,177 [INFO] [10] TRAIN  loss: 1.1549495983246811 acc: 0.9549084654134414
2025-01-14 13:43:33,177 [INFO] [10] TRAIN  loss dict: {'classification_loss': 1.1549495983246811}
2025-01-14 13:43:33,177 [INFO] [10] VALIDATION loss: 1.971156257099675 VALIDATION acc: 0.7536050156739812
2025-01-14 13:43:33,177 [INFO] [10] VALIDATION loss dict: {'classification_loss': 1.971156257099675}
2025-01-14 13:43:33,177 [INFO] 
2025-01-14 13:43:51,623 [INFO] Step[50/2713]: training loss : 1.099137704372406 TRAIN  loss dict:  {'classification_loss': 1.099137704372406}
2025-01-14 13:44:05,442 [INFO] Step[100/2713]: training loss : 1.1075305831432343 TRAIN  loss dict:  {'classification_loss': 1.1075305831432343}
2025-01-14 13:44:19,660 [INFO] Step[150/2713]: training loss : 1.0595181810855865 TRAIN  loss dict:  {'classification_loss': 1.0595181810855865}
2025-01-14 13:44:33,745 [INFO] Step[200/2713]: training loss : 1.0924384689331055 TRAIN  loss dict:  {'classification_loss': 1.0924384689331055}
2025-01-14 13:44:47,399 [INFO] Step[250/2713]: training loss : 1.1211660826206207 TRAIN  loss dict:  {'classification_loss': 1.1211660826206207}
2025-01-14 13:45:02,604 [INFO] Step[300/2713]: training loss : 1.0949746358394623 TRAIN  loss dict:  {'classification_loss': 1.0949746358394623}
2025-01-14 13:45:18,003 [INFO] Step[350/2713]: training loss : 1.0793634521961213 TRAIN  loss dict:  {'classification_loss': 1.0793634521961213}
2025-01-14 13:45:31,510 [INFO] Step[400/2713]: training loss : 1.1260722982883453 TRAIN  loss dict:  {'classification_loss': 1.1260722982883453}
2025-01-14 13:45:44,908 [INFO] Step[450/2713]: training loss : 1.1319055736064911 TRAIN  loss dict:  {'classification_loss': 1.1319055736064911}
2025-01-14 13:45:58,790 [INFO] Step[500/2713]: training loss : 1.0989075338840484 TRAIN  loss dict:  {'classification_loss': 1.0989075338840484}
2025-01-14 13:46:12,547 [INFO] Step[550/2713]: training loss : 1.1557316851615906 TRAIN  loss dict:  {'classification_loss': 1.1557316851615906}
2025-01-14 13:46:26,047 [INFO] Step[600/2713]: training loss : 1.0863493847846986 TRAIN  loss dict:  {'classification_loss': 1.0863493847846986}
2025-01-14 13:46:39,625 [INFO] Step[650/2713]: training loss : 1.0776019597053528 TRAIN  loss dict:  {'classification_loss': 1.0776019597053528}
2025-01-14 13:46:52,779 [INFO] Step[700/2713]: training loss : 1.0887032389640807 TRAIN  loss dict:  {'classification_loss': 1.0887032389640807}
2025-01-14 13:47:06,776 [INFO] Step[750/2713]: training loss : 1.1122265553474426 TRAIN  loss dict:  {'classification_loss': 1.1122265553474426}
2025-01-14 13:47:20,362 [INFO] Step[800/2713]: training loss : 1.1726331281661988 TRAIN  loss dict:  {'classification_loss': 1.1726331281661988}
2025-01-14 13:47:34,015 [INFO] Step[850/2713]: training loss : 1.1158206343650818 TRAIN  loss dict:  {'classification_loss': 1.1158206343650818}
2025-01-14 13:47:48,200 [INFO] Step[900/2713]: training loss : 1.1148230946063995 TRAIN  loss dict:  {'classification_loss': 1.1148230946063995}
2025-01-14 13:48:02,431 [INFO] Step[950/2713]: training loss : 1.0764999759197236 TRAIN  loss dict:  {'classification_loss': 1.0764999759197236}
2025-01-14 13:48:15,936 [INFO] Step[1000/2713]: training loss : 1.0898789978027343 TRAIN  loss dict:  {'classification_loss': 1.0898789978027343}
2025-01-14 13:48:29,839 [INFO] Step[1050/2713]: training loss : 1.1109191393852234 TRAIN  loss dict:  {'classification_loss': 1.1109191393852234}
2025-01-14 13:48:43,425 [INFO] Step[1100/2713]: training loss : 1.0886447262763976 TRAIN  loss dict:  {'classification_loss': 1.0886447262763976}
2025-01-14 13:48:56,875 [INFO] Step[1150/2713]: training loss : 1.1494549405574799 TRAIN  loss dict:  {'classification_loss': 1.1494549405574799}
2025-01-14 13:49:10,229 [INFO] Step[1200/2713]: training loss : 1.1167524063587189 TRAIN  loss dict:  {'classification_loss': 1.1167524063587189}
2025-01-14 13:49:23,843 [INFO] Step[1250/2713]: training loss : 1.0862960612773895 TRAIN  loss dict:  {'classification_loss': 1.0862960612773895}
2025-01-14 13:49:37,510 [INFO] Step[1300/2713]: training loss : 1.0912897598743438 TRAIN  loss dict:  {'classification_loss': 1.0912897598743438}
2025-01-14 13:49:50,899 [INFO] Step[1350/2713]: training loss : 1.092573529481888 TRAIN  loss dict:  {'classification_loss': 1.092573529481888}
2025-01-14 13:50:04,525 [INFO] Step[1400/2713]: training loss : 1.0984785199165343 TRAIN  loss dict:  {'classification_loss': 1.0984785199165343}
2025-01-14 13:50:18,150 [INFO] Step[1450/2713]: training loss : 1.06077791929245 TRAIN  loss dict:  {'classification_loss': 1.06077791929245}
2025-01-14 13:50:31,989 [INFO] Step[1500/2713]: training loss : 1.1046193897724152 TRAIN  loss dict:  {'classification_loss': 1.1046193897724152}
2025-01-14 13:50:45,776 [INFO] Step[1550/2713]: training loss : 1.1066344964504242 TRAIN  loss dict:  {'classification_loss': 1.1066344964504242}
2025-01-14 13:50:58,989 [INFO] Step[1600/2713]: training loss : 1.1134286236763 TRAIN  loss dict:  {'classification_loss': 1.1134286236763}
2025-01-14 13:51:12,745 [INFO] Step[1650/2713]: training loss : 1.0711081206798554 TRAIN  loss dict:  {'classification_loss': 1.0711081206798554}
2025-01-14 13:51:26,368 [INFO] Step[1700/2713]: training loss : 1.0897498965263366 TRAIN  loss dict:  {'classification_loss': 1.0897498965263366}
2025-01-14 13:51:39,608 [INFO] Step[1750/2713]: training loss : 1.103911772966385 TRAIN  loss dict:  {'classification_loss': 1.103911772966385}
2025-01-14 13:51:53,616 [INFO] Step[1800/2713]: training loss : 1.1148358857631684 TRAIN  loss dict:  {'classification_loss': 1.1148358857631684}
2025-01-14 13:52:07,827 [INFO] Step[1850/2713]: training loss : 1.0524598753452301 TRAIN  loss dict:  {'classification_loss': 1.0524598753452301}
2025-01-14 13:52:21,471 [INFO] Step[1900/2713]: training loss : 1.140605436563492 TRAIN  loss dict:  {'classification_loss': 1.140605436563492}
2025-01-14 13:52:37,637 [INFO] Step[1950/2713]: training loss : 1.0883247768878936 TRAIN  loss dict:  {'classification_loss': 1.0883247768878936}
2025-01-14 13:52:53,526 [INFO] Step[2000/2713]: training loss : 1.1184579348564148 TRAIN  loss dict:  {'classification_loss': 1.1184579348564148}
2025-01-14 13:53:06,978 [INFO] Step[2050/2713]: training loss : 1.0773922717571258 TRAIN  loss dict:  {'classification_loss': 1.0773922717571258}
2025-01-14 13:53:20,870 [INFO] Step[2100/2713]: training loss : 1.0958605706691742 TRAIN  loss dict:  {'classification_loss': 1.0958605706691742}
2025-01-14 13:53:34,689 [INFO] Step[2150/2713]: training loss : 1.043725665807724 TRAIN  loss dict:  {'classification_loss': 1.043725665807724}
2025-01-14 13:53:48,259 [INFO] Step[2200/2713]: training loss : 1.1412907540798187 TRAIN  loss dict:  {'classification_loss': 1.1412907540798187}
2025-01-14 13:54:01,525 [INFO] Step[2250/2713]: training loss : 1.076839154958725 TRAIN  loss dict:  {'classification_loss': 1.076839154958725}
2025-01-14 13:54:15,118 [INFO] Step[2300/2713]: training loss : 1.1526254510879517 TRAIN  loss dict:  {'classification_loss': 1.1526254510879517}
2025-01-14 13:54:28,906 [INFO] Step[2350/2713]: training loss : 1.1195279562473297 TRAIN  loss dict:  {'classification_loss': 1.1195279562473297}
2025-01-14 13:54:42,311 [INFO] Step[2400/2713]: training loss : 1.0539955413341522 TRAIN  loss dict:  {'classification_loss': 1.0539955413341522}
2025-01-14 13:54:56,124 [INFO] Step[2450/2713]: training loss : 1.0698594641685486 TRAIN  loss dict:  {'classification_loss': 1.0698594641685486}
2025-01-14 13:55:09,944 [INFO] Step[2500/2713]: training loss : 1.0766326582431793 TRAIN  loss dict:  {'classification_loss': 1.0766326582431793}
2025-01-14 13:55:23,285 [INFO] Step[2550/2713]: training loss : 1.089465501308441 TRAIN  loss dict:  {'classification_loss': 1.089465501308441}
2025-01-14 13:55:37,288 [INFO] Step[2600/2713]: training loss : 1.114751627445221 TRAIN  loss dict:  {'classification_loss': 1.114751627445221}
2025-01-14 13:55:51,127 [INFO] Step[2650/2713]: training loss : 1.0541869521141052 TRAIN  loss dict:  {'classification_loss': 1.0541869521141052}
2025-01-14 13:56:04,525 [INFO] Step[2700/2713]: training loss : 1.0915574741363525 TRAIN  loss dict:  {'classification_loss': 1.0915574741363525}
2025-01-14 13:57:20,839 [INFO] Label accuracies statistics:
2025-01-14 13:57:20,839 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.5, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 1.0, 57: 0.5, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.25, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.3333333333333333, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.5, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.25, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 1.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.25, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.25, 225: 0.75, 226: 0.5, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.0, 240: 0.75, 241: 1.0, 242: 1.0, 243: 0.25, 244: 1.0, 245: 0.5, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.75, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.25, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.0, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.25, 334: 0.5, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 1.0, 342: 0.5, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.25, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.75, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.25, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.5, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.0, 394: 0.5, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 13:57:23,279 [INFO] [11] TRAIN  loss: 1.0998056510520342 acc: 0.9695294262194373
2025-01-14 13:57:23,279 [INFO] [11] TRAIN  loss dict: {'classification_loss': 1.0998056510520342}
2025-01-14 13:57:23,280 [INFO] [11] VALIDATION loss: 1.871737608448007 VALIDATION acc: 0.7705329153605016
2025-01-14 13:57:23,280 [INFO] [11] VALIDATION loss dict: {'classification_loss': 1.871737608448007}
2025-01-14 13:57:23,280 [INFO] 
2025-01-14 13:57:43,289 [INFO] Step[50/2713]: training loss : 1.0505183112621308 TRAIN  loss dict:  {'classification_loss': 1.0505183112621308}
2025-01-14 13:57:59,849 [INFO] Step[100/2713]: training loss : 1.0467318284511566 TRAIN  loss dict:  {'classification_loss': 1.0467318284511566}
2025-01-14 13:58:13,242 [INFO] Step[150/2713]: training loss : 1.0808556652069092 TRAIN  loss dict:  {'classification_loss': 1.0808556652069092}
2025-01-14 13:58:26,509 [INFO] Step[200/2713]: training loss : 1.048946079015732 TRAIN  loss dict:  {'classification_loss': 1.048946079015732}
2025-01-14 13:58:40,785 [INFO] Step[250/2713]: training loss : 1.0660200643539428 TRAIN  loss dict:  {'classification_loss': 1.0660200643539428}
2025-01-14 13:58:54,639 [INFO] Step[300/2713]: training loss : 1.095174194574356 TRAIN  loss dict:  {'classification_loss': 1.095174194574356}
2025-01-14 13:59:08,810 [INFO] Step[350/2713]: training loss : 1.1161317050457 TRAIN  loss dict:  {'classification_loss': 1.1161317050457}
2025-01-14 13:59:23,066 [INFO] Step[400/2713]: training loss : 1.098159589767456 TRAIN  loss dict:  {'classification_loss': 1.098159589767456}
2025-01-14 13:59:36,871 [INFO] Step[450/2713]: training loss : 1.129402527809143 TRAIN  loss dict:  {'classification_loss': 1.129402527809143}
2025-01-14 13:59:50,059 [INFO] Step[500/2713]: training loss : 1.0501683402061461 TRAIN  loss dict:  {'classification_loss': 1.0501683402061461}
2025-01-14 14:00:03,500 [INFO] Step[550/2713]: training loss : 1.1339259374141692 TRAIN  loss dict:  {'classification_loss': 1.1339259374141692}
2025-01-14 14:00:17,266 [INFO] Step[600/2713]: training loss : 1.0886764967441558 TRAIN  loss dict:  {'classification_loss': 1.0886764967441558}
2025-01-14 14:00:30,894 [INFO] Step[650/2713]: training loss : 1.0695053493976594 TRAIN  loss dict:  {'classification_loss': 1.0695053493976594}
2025-01-14 14:00:45,087 [INFO] Step[700/2713]: training loss : 1.0716033411026 TRAIN  loss dict:  {'classification_loss': 1.0716033411026}
2025-01-14 14:00:59,015 [INFO] Step[750/2713]: training loss : 1.0361043691635132 TRAIN  loss dict:  {'classification_loss': 1.0361043691635132}
2025-01-14 14:01:12,398 [INFO] Step[800/2713]: training loss : 1.0926931166648866 TRAIN  loss dict:  {'classification_loss': 1.0926931166648866}
2025-01-14 14:01:26,079 [INFO] Step[850/2713]: training loss : 1.0960687899589538 TRAIN  loss dict:  {'classification_loss': 1.0960687899589538}
2025-01-14 14:01:39,749 [INFO] Step[900/2713]: training loss : 1.0732166612148284 TRAIN  loss dict:  {'classification_loss': 1.0732166612148284}
2025-01-14 14:01:53,416 [INFO] Step[950/2713]: training loss : 1.0377778422832489 TRAIN  loss dict:  {'classification_loss': 1.0377778422832489}
2025-01-14 14:02:07,089 [INFO] Step[1000/2713]: training loss : 1.0373148798942566 TRAIN  loss dict:  {'classification_loss': 1.0373148798942566}
2025-01-14 14:02:20,383 [INFO] Step[1050/2713]: training loss : 1.0471880066394805 TRAIN  loss dict:  {'classification_loss': 1.0471880066394805}
2025-01-14 14:02:33,742 [INFO] Step[1100/2713]: training loss : 1.0801095724105836 TRAIN  loss dict:  {'classification_loss': 1.0801095724105836}
2025-01-14 14:02:48,023 [INFO] Step[1150/2713]: training loss : 1.0716421103477478 TRAIN  loss dict:  {'classification_loss': 1.0716421103477478}
2025-01-14 14:03:01,567 [INFO] Step[1200/2713]: training loss : 1.1769914793968201 TRAIN  loss dict:  {'classification_loss': 1.1769914793968201}
2025-01-14 14:03:15,260 [INFO] Step[1250/2713]: training loss : 1.070784355401993 TRAIN  loss dict:  {'classification_loss': 1.070784355401993}
2025-01-14 14:03:29,183 [INFO] Step[1300/2713]: training loss : 1.0978205907344818 TRAIN  loss dict:  {'classification_loss': 1.0978205907344818}
2025-01-14 14:03:42,429 [INFO] Step[1350/2713]: training loss : 1.0653860926628114 TRAIN  loss dict:  {'classification_loss': 1.0653860926628114}
2025-01-14 14:03:56,030 [INFO] Step[1400/2713]: training loss : 1.1410660970211028 TRAIN  loss dict:  {'classification_loss': 1.1410660970211028}
2025-01-14 14:04:09,783 [INFO] Step[1450/2713]: training loss : 1.100146814584732 TRAIN  loss dict:  {'classification_loss': 1.100146814584732}
2025-01-14 14:04:23,354 [INFO] Step[1500/2713]: training loss : 1.0556759595870973 TRAIN  loss dict:  {'classification_loss': 1.0556759595870973}
2025-01-14 14:04:36,958 [INFO] Step[1550/2713]: training loss : 1.1101204180717468 TRAIN  loss dict:  {'classification_loss': 1.1101204180717468}
2025-01-14 14:04:50,930 [INFO] Step[1600/2713]: training loss : 1.0761741781234742 TRAIN  loss dict:  {'classification_loss': 1.0761741781234742}
2025-01-14 14:05:04,654 [INFO] Step[1650/2713]: training loss : 1.0584764575958252 TRAIN  loss dict:  {'classification_loss': 1.0584764575958252}
2025-01-14 14:05:18,296 [INFO] Step[1700/2713]: training loss : 1.0723462414741516 TRAIN  loss dict:  {'classification_loss': 1.0723462414741516}
2025-01-14 14:05:31,572 [INFO] Step[1750/2713]: training loss : 1.1044578075408935 TRAIN  loss dict:  {'classification_loss': 1.1044578075408935}
2025-01-14 14:05:44,790 [INFO] Step[1800/2713]: training loss : 1.0911632168293 TRAIN  loss dict:  {'classification_loss': 1.0911632168293}
2025-01-14 14:05:58,625 [INFO] Step[1850/2713]: training loss : 1.1091706037521363 TRAIN  loss dict:  {'classification_loss': 1.1091706037521363}
2025-01-14 14:06:12,162 [INFO] Step[1900/2713]: training loss : 1.0520661413669585 TRAIN  loss dict:  {'classification_loss': 1.0520661413669585}
2025-01-14 14:06:25,898 [INFO] Step[1950/2713]: training loss : 1.084617134332657 TRAIN  loss dict:  {'classification_loss': 1.084617134332657}
2025-01-14 14:06:39,584 [INFO] Step[2000/2713]: training loss : 1.0835583245754241 TRAIN  loss dict:  {'classification_loss': 1.0835583245754241}
2025-01-14 14:06:53,244 [INFO] Step[2050/2713]: training loss : 1.0731964576244355 TRAIN  loss dict:  {'classification_loss': 1.0731964576244355}
2025-01-14 14:07:06,864 [INFO] Step[2100/2713]: training loss : 1.0710815238952636 TRAIN  loss dict:  {'classification_loss': 1.0710815238952636}
2025-01-14 14:07:20,329 [INFO] Step[2150/2713]: training loss : 1.0664979147911071 TRAIN  loss dict:  {'classification_loss': 1.0664979147911071}
2025-01-14 14:07:33,676 [INFO] Step[2200/2713]: training loss : 1.073111834526062 TRAIN  loss dict:  {'classification_loss': 1.073111834526062}
2025-01-14 14:07:47,757 [INFO] Step[2250/2713]: training loss : 1.0389158821105957 TRAIN  loss dict:  {'classification_loss': 1.0389158821105957}
2025-01-14 14:08:00,969 [INFO] Step[2300/2713]: training loss : 1.019044075012207 TRAIN  loss dict:  {'classification_loss': 1.019044075012207}
2025-01-14 14:08:14,641 [INFO] Step[2350/2713]: training loss : 1.1049632012844086 TRAIN  loss dict:  {'classification_loss': 1.1049632012844086}
2025-01-14 14:08:28,617 [INFO] Step[2400/2713]: training loss : 1.0351175892353057 TRAIN  loss dict:  {'classification_loss': 1.0351175892353057}
2025-01-14 14:08:42,182 [INFO] Step[2450/2713]: training loss : 1.0684930241107942 TRAIN  loss dict:  {'classification_loss': 1.0684930241107942}
2025-01-14 14:08:58,284 [INFO] Step[2500/2713]: training loss : 1.0663358664512634 TRAIN  loss dict:  {'classification_loss': 1.0663358664512634}
2025-01-14 14:09:13,314 [INFO] Step[2550/2713]: training loss : 1.0737000358104707 TRAIN  loss dict:  {'classification_loss': 1.0737000358104707}
2025-01-14 14:09:26,838 [INFO] Step[2600/2713]: training loss : 1.0421383810043334 TRAIN  loss dict:  {'classification_loss': 1.0421383810043334}
2025-01-14 14:09:40,366 [INFO] Step[2650/2713]: training loss : 1.1045474231243133 TRAIN  loss dict:  {'classification_loss': 1.1045474231243133}
2025-01-14 14:09:54,219 [INFO] Step[2700/2713]: training loss : 1.1112014651298523 TRAIN  loss dict:  {'classification_loss': 1.1112014651298523}
2025-01-14 14:11:10,409 [INFO] Label accuracies statistics:
2025-01-14 14:11:10,409 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.25, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.5, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 0.75, 179: 0.6666666666666666, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.25, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 0.75, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 1.0, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 0.75, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.25, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.25, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.5, 291: 0.5, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 1.0, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.5, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.5, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.25, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 14:11:12,853 [INFO] [12] TRAIN  loss: 1.077881001696743 acc: 0.977147069664578
2025-01-14 14:11:12,853 [INFO] [12] TRAIN  loss dict: {'classification_loss': 1.077881001696743}
2025-01-14 14:11:12,853 [INFO] [12] VALIDATION loss: 1.8197670775024515 VALIDATION acc: 0.7799373040752351
2025-01-14 14:11:12,853 [INFO] [12] VALIDATION loss dict: {'classification_loss': 1.8197670775024515}
2025-01-14 14:11:12,853 [INFO] 
2025-01-14 14:11:31,181 [INFO] Step[50/2713]: training loss : 1.0379145383834838 TRAIN  loss dict:  {'classification_loss': 1.0379145383834838}
2025-01-14 14:11:46,422 [INFO] Step[100/2713]: training loss : 1.0532987570762635 TRAIN  loss dict:  {'classification_loss': 1.0532987570762635}
2025-01-14 14:12:00,728 [INFO] Step[150/2713]: training loss : 1.0310021030902863 TRAIN  loss dict:  {'classification_loss': 1.0310021030902863}
2025-01-14 14:12:14,211 [INFO] Step[200/2713]: training loss : 1.1068076407909393 TRAIN  loss dict:  {'classification_loss': 1.1068076407909393}
2025-01-14 14:12:27,544 [INFO] Step[250/2713]: training loss : 1.0523753190040588 TRAIN  loss dict:  {'classification_loss': 1.0523753190040588}
2025-01-14 14:12:41,705 [INFO] Step[300/2713]: training loss : 1.0478287982940673 TRAIN  loss dict:  {'classification_loss': 1.0478287982940673}
2025-01-14 14:12:55,023 [INFO] Step[350/2713]: training loss : 1.048523770570755 TRAIN  loss dict:  {'classification_loss': 1.048523770570755}
2025-01-14 14:13:08,299 [INFO] Step[400/2713]: training loss : 1.110935364961624 TRAIN  loss dict:  {'classification_loss': 1.110935364961624}
2025-01-14 14:13:22,049 [INFO] Step[450/2713]: training loss : 1.0436553847789765 TRAIN  loss dict:  {'classification_loss': 1.0436553847789765}
2025-01-14 14:13:35,819 [INFO] Step[500/2713]: training loss : 1.0378229486942292 TRAIN  loss dict:  {'classification_loss': 1.0378229486942292}
2025-01-14 14:13:50,086 [INFO] Step[550/2713]: training loss : 1.0134345376491547 TRAIN  loss dict:  {'classification_loss': 1.0134345376491547}
2025-01-14 14:14:03,621 [INFO] Step[600/2713]: training loss : 1.0650215995311738 TRAIN  loss dict:  {'classification_loss': 1.0650215995311738}
2025-01-14 14:14:17,150 [INFO] Step[650/2713]: training loss : 1.0418870568275451 TRAIN  loss dict:  {'classification_loss': 1.0418870568275451}
2025-01-14 14:14:30,807 [INFO] Step[700/2713]: training loss : 1.0415617203712464 TRAIN  loss dict:  {'classification_loss': 1.0415617203712464}
2025-01-14 14:14:44,661 [INFO] Step[750/2713]: training loss : 1.1006730139255523 TRAIN  loss dict:  {'classification_loss': 1.1006730139255523}
2025-01-14 14:14:57,982 [INFO] Step[800/2713]: training loss : 1.0720150339603425 TRAIN  loss dict:  {'classification_loss': 1.0720150339603425}
2025-01-14 14:15:12,220 [INFO] Step[850/2713]: training loss : 1.0244068586826325 TRAIN  loss dict:  {'classification_loss': 1.0244068586826325}
2025-01-14 14:15:25,494 [INFO] Step[900/2713]: training loss : 1.0335678994655608 TRAIN  loss dict:  {'classification_loss': 1.0335678994655608}
2025-01-14 14:15:39,114 [INFO] Step[950/2713]: training loss : 1.1248925256729125 TRAIN  loss dict:  {'classification_loss': 1.1248925256729125}
2025-01-14 14:15:52,697 [INFO] Step[1000/2713]: training loss : 1.0140616023540496 TRAIN  loss dict:  {'classification_loss': 1.0140616023540496}
2025-01-14 14:16:06,180 [INFO] Step[1050/2713]: training loss : 1.0392196691036224 TRAIN  loss dict:  {'classification_loss': 1.0392196691036224}
2025-01-14 14:16:19,502 [INFO] Step[1100/2713]: training loss : 1.0758403074741363 TRAIN  loss dict:  {'classification_loss': 1.0758403074741363}
2025-01-14 14:16:33,409 [INFO] Step[1150/2713]: training loss : 1.0564842855930328 TRAIN  loss dict:  {'classification_loss': 1.0564842855930328}
2025-01-14 14:16:47,073 [INFO] Step[1200/2713]: training loss : 1.065236690044403 TRAIN  loss dict:  {'classification_loss': 1.065236690044403}
2025-01-14 14:17:00,454 [INFO] Step[1250/2713]: training loss : 1.0691439604759216 TRAIN  loss dict:  {'classification_loss': 1.0691439604759216}
2025-01-14 14:17:13,886 [INFO] Step[1300/2713]: training loss : 1.0882240891456605 TRAIN  loss dict:  {'classification_loss': 1.0882240891456605}
2025-01-14 14:17:27,191 [INFO] Step[1350/2713]: training loss : 1.0229921972751617 TRAIN  loss dict:  {'classification_loss': 1.0229921972751617}
2025-01-14 14:17:40,821 [INFO] Step[1400/2713]: training loss : 1.086053957939148 TRAIN  loss dict:  {'classification_loss': 1.086053957939148}
2025-01-14 14:17:54,976 [INFO] Step[1450/2713]: training loss : 1.0990329587459564 TRAIN  loss dict:  {'classification_loss': 1.0990329587459564}
2025-01-14 14:18:08,593 [INFO] Step[1500/2713]: training loss : 1.0878901839256288 TRAIN  loss dict:  {'classification_loss': 1.0878901839256288}
2025-01-14 14:18:22,053 [INFO] Step[1550/2713]: training loss : 1.0775117444992066 TRAIN  loss dict:  {'classification_loss': 1.0775117444992066}
2025-01-14 14:18:36,178 [INFO] Step[1600/2713]: training loss : 1.0896896839141845 TRAIN  loss dict:  {'classification_loss': 1.0896896839141845}
2025-01-14 14:18:49,690 [INFO] Step[1650/2713]: training loss : 1.0411494994163513 TRAIN  loss dict:  {'classification_loss': 1.0411494994163513}
2025-01-14 14:19:03,354 [INFO] Step[1700/2713]: training loss : 1.0842200577259065 TRAIN  loss dict:  {'classification_loss': 1.0842200577259065}
2025-01-14 14:19:16,944 [INFO] Step[1750/2713]: training loss : 1.0763226985931396 TRAIN  loss dict:  {'classification_loss': 1.0763226985931396}
2025-01-14 14:19:30,213 [INFO] Step[1800/2713]: training loss : 1.0927236247062684 TRAIN  loss dict:  {'classification_loss': 1.0927236247062684}
2025-01-14 14:19:44,110 [INFO] Step[1850/2713]: training loss : 1.075560177564621 TRAIN  loss dict:  {'classification_loss': 1.075560177564621}
2025-01-14 14:19:57,930 [INFO] Step[1900/2713]: training loss : 1.0529748845100402 TRAIN  loss dict:  {'classification_loss': 1.0529748845100402}
2025-01-14 14:20:14,273 [INFO] Step[1950/2713]: training loss : 1.0484661602973937 TRAIN  loss dict:  {'classification_loss': 1.0484661602973937}
2025-01-14 14:20:28,784 [INFO] Step[2000/2713]: training loss : 1.0537236773967742 TRAIN  loss dict:  {'classification_loss': 1.0537236773967742}
2025-01-14 14:20:42,894 [INFO] Step[2050/2713]: training loss : 1.089798814058304 TRAIN  loss dict:  {'classification_loss': 1.089798814058304}
2025-01-14 14:20:57,095 [INFO] Step[2100/2713]: training loss : 1.0449841964244841 TRAIN  loss dict:  {'classification_loss': 1.0449841964244841}
2025-01-14 14:21:10,726 [INFO] Step[2150/2713]: training loss : 1.0526122903823854 TRAIN  loss dict:  {'classification_loss': 1.0526122903823854}
2025-01-14 14:21:24,482 [INFO] Step[2200/2713]: training loss : 1.085121294260025 TRAIN  loss dict:  {'classification_loss': 1.085121294260025}
2025-01-14 14:21:38,178 [INFO] Step[2250/2713]: training loss : 1.1046814703941346 TRAIN  loss dict:  {'classification_loss': 1.1046814703941346}
2025-01-14 14:21:52,184 [INFO] Step[2300/2713]: training loss : 1.0506279706954955 TRAIN  loss dict:  {'classification_loss': 1.0506279706954955}
2025-01-14 14:22:05,405 [INFO] Step[2350/2713]: training loss : 1.0295830380916595 TRAIN  loss dict:  {'classification_loss': 1.0295830380916595}
2025-01-14 14:22:18,977 [INFO] Step[2400/2713]: training loss : 1.0592530286312103 TRAIN  loss dict:  {'classification_loss': 1.0592530286312103}
2025-01-14 14:22:32,580 [INFO] Step[2450/2713]: training loss : 1.1241459572315216 TRAIN  loss dict:  {'classification_loss': 1.1241459572315216}
2025-01-14 14:22:46,207 [INFO] Step[2500/2713]: training loss : 1.0614524233341216 TRAIN  loss dict:  {'classification_loss': 1.0614524233341216}
2025-01-14 14:23:00,371 [INFO] Step[2550/2713]: training loss : 1.0499073481559753 TRAIN  loss dict:  {'classification_loss': 1.0499073481559753}
2025-01-14 14:23:14,584 [INFO] Step[2600/2713]: training loss : 1.06004825592041 TRAIN  loss dict:  {'classification_loss': 1.06004825592041}
2025-01-14 14:23:28,332 [INFO] Step[2650/2713]: training loss : 1.0771164870262147 TRAIN  loss dict:  {'classification_loss': 1.0771164870262147}
2025-01-14 14:23:42,547 [INFO] Step[2700/2713]: training loss : 1.0526759445667266 TRAIN  loss dict:  {'classification_loss': 1.0526759445667266}
2025-01-14 14:24:59,295 [INFO] Label accuracies statistics:
2025-01-14 14:24:59,296 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 1.0, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.5, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.5, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.5, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.5, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 1.0, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.75, 269: 0.5, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 1.0, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.25, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.5, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.5, 319: 1.0, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.5, 331: 0.75, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.0, 394: 0.75, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 14:25:00,450 [INFO] [13] TRAIN  loss: 1.0637190084900312 acc: 0.9787443174837204
2025-01-14 14:25:00,450 [INFO] [13] TRAIN  loss dict: {'classification_loss': 1.0637190084900312}
2025-01-14 14:25:00,451 [INFO] [13] VALIDATION loss: 1.8188047144646036 VALIDATION acc: 0.7799373040752351
2025-01-14 14:25:00,451 [INFO] [13] VALIDATION loss dict: {'classification_loss': 1.8188047144646036}
2025-01-14 14:25:00,451 [INFO] 
2025-01-14 14:25:19,436 [INFO] Step[50/2713]: training loss : 1.0367125391960144 TRAIN  loss dict:  {'classification_loss': 1.0367125391960144}
2025-01-14 14:25:32,680 [INFO] Step[100/2713]: training loss : 1.0282024133205414 TRAIN  loss dict:  {'classification_loss': 1.0282024133205414}
2025-01-14 14:25:46,024 [INFO] Step[150/2713]: training loss : 1.047404602766037 TRAIN  loss dict:  {'classification_loss': 1.047404602766037}
2025-01-14 14:26:00,090 [INFO] Step[200/2713]: training loss : 1.0577998399734496 TRAIN  loss dict:  {'classification_loss': 1.0577998399734496}
2025-01-14 14:26:13,524 [INFO] Step[250/2713]: training loss : 1.0884015905857085 TRAIN  loss dict:  {'classification_loss': 1.0884015905857085}
2025-01-14 14:26:26,722 [INFO] Step[300/2713]: training loss : 1.0341029620170594 TRAIN  loss dict:  {'classification_loss': 1.0341029620170594}
2025-01-14 14:26:40,455 [INFO] Step[350/2713]: training loss : 1.0808706164360047 TRAIN  loss dict:  {'classification_loss': 1.0808706164360047}
2025-01-14 14:26:53,658 [INFO] Step[400/2713]: training loss : 1.0782930624485016 TRAIN  loss dict:  {'classification_loss': 1.0782930624485016}
2025-01-14 14:27:06,894 [INFO] Step[450/2713]: training loss : 1.0808986699581147 TRAIN  loss dict:  {'classification_loss': 1.0808986699581147}
2025-01-14 14:27:20,354 [INFO] Step[500/2713]: training loss : 1.0734518146514893 TRAIN  loss dict:  {'classification_loss': 1.0734518146514893}
2025-01-14 14:27:34,336 [INFO] Step[550/2713]: training loss : 1.0836232161521913 TRAIN  loss dict:  {'classification_loss': 1.0836232161521913}
2025-01-14 14:27:48,503 [INFO] Step[600/2713]: training loss : 1.0774017143249512 TRAIN  loss dict:  {'classification_loss': 1.0774017143249512}
2025-01-14 14:28:02,751 [INFO] Step[650/2713]: training loss : 1.036268515586853 TRAIN  loss dict:  {'classification_loss': 1.036268515586853}
2025-01-14 14:28:16,164 [INFO] Step[700/2713]: training loss : 1.079350653886795 TRAIN  loss dict:  {'classification_loss': 1.079350653886795}
2025-01-14 14:28:29,674 [INFO] Step[750/2713]: training loss : 1.0530924999713898 TRAIN  loss dict:  {'classification_loss': 1.0530924999713898}
2025-01-14 14:28:42,939 [INFO] Step[800/2713]: training loss : 1.0369332933425903 TRAIN  loss dict:  {'classification_loss': 1.0369332933425903}
2025-01-14 14:28:56,801 [INFO] Step[850/2713]: training loss : 1.0849123251438142 TRAIN  loss dict:  {'classification_loss': 1.0849123251438142}
2025-01-14 14:29:10,622 [INFO] Step[900/2713]: training loss : 1.044377051591873 TRAIN  loss dict:  {'classification_loss': 1.044377051591873}
2025-01-14 14:29:24,161 [INFO] Step[950/2713]: training loss : 1.0714705419540405 TRAIN  loss dict:  {'classification_loss': 1.0714705419540405}
2025-01-14 14:29:38,320 [INFO] Step[1000/2713]: training loss : 1.0534607827663423 TRAIN  loss dict:  {'classification_loss': 1.0534607827663423}
2025-01-14 14:29:52,345 [INFO] Step[1050/2713]: training loss : 1.0488365614414215 TRAIN  loss dict:  {'classification_loss': 1.0488365614414215}
2025-01-14 14:30:06,060 [INFO] Step[1100/2713]: training loss : 1.0479100930690766 TRAIN  loss dict:  {'classification_loss': 1.0479100930690766}
2025-01-14 14:30:20,001 [INFO] Step[1150/2713]: training loss : 1.0918833816051483 TRAIN  loss dict:  {'classification_loss': 1.0918833816051483}
2025-01-14 14:30:33,211 [INFO] Step[1200/2713]: training loss : 1.0607758808135985 TRAIN  loss dict:  {'classification_loss': 1.0607758808135985}
2025-01-14 14:30:47,305 [INFO] Step[1250/2713]: training loss : 1.056337797641754 TRAIN  loss dict:  {'classification_loss': 1.056337797641754}
2025-01-14 14:31:01,273 [INFO] Step[1300/2713]: training loss : 1.0446719753742217 TRAIN  loss dict:  {'classification_loss': 1.0446719753742217}
2025-01-14 14:31:15,273 [INFO] Step[1350/2713]: training loss : 1.0514743053913116 TRAIN  loss dict:  {'classification_loss': 1.0514743053913116}
2025-01-14 14:31:29,102 [INFO] Step[1400/2713]: training loss : 1.0581611168384553 TRAIN  loss dict:  {'classification_loss': 1.0581611168384553}
2025-01-14 14:31:42,951 [INFO] Step[1450/2713]: training loss : 1.0366702389717102 TRAIN  loss dict:  {'classification_loss': 1.0366702389717102}
2025-01-14 14:31:56,198 [INFO] Step[1500/2713]: training loss : 1.055766555070877 TRAIN  loss dict:  {'classification_loss': 1.055766555070877}
2025-01-14 14:32:09,410 [INFO] Step[1550/2713]: training loss : 1.051646339893341 TRAIN  loss dict:  {'classification_loss': 1.051646339893341}
2025-01-14 14:32:23,090 [INFO] Step[1600/2713]: training loss : 1.0905296051502227 TRAIN  loss dict:  {'classification_loss': 1.0905296051502227}
2025-01-14 14:32:36,903 [INFO] Step[1650/2713]: training loss : 1.0278421425819397 TRAIN  loss dict:  {'classification_loss': 1.0278421425819397}
2025-01-14 14:32:50,742 [INFO] Step[1700/2713]: training loss : 1.0605740642547608 TRAIN  loss dict:  {'classification_loss': 1.0605740642547608}
2025-01-14 14:33:04,975 [INFO] Step[1750/2713]: training loss : 1.110586256980896 TRAIN  loss dict:  {'classification_loss': 1.110586256980896}
2025-01-14 14:33:19,216 [INFO] Step[1800/2713]: training loss : 1.0304108691215514 TRAIN  loss dict:  {'classification_loss': 1.0304108691215514}
2025-01-14 14:33:34,066 [INFO] Step[1850/2713]: training loss : 1.0214872658252716 TRAIN  loss dict:  {'classification_loss': 1.0214872658252716}
2025-01-14 14:33:49,524 [INFO] Step[1900/2713]: training loss : 1.0531765258312225 TRAIN  loss dict:  {'classification_loss': 1.0531765258312225}
2025-01-14 14:34:02,848 [INFO] Step[1950/2713]: training loss : 1.062322714328766 TRAIN  loss dict:  {'classification_loss': 1.062322714328766}
2025-01-14 14:34:16,320 [INFO] Step[2000/2713]: training loss : 1.099261589050293 TRAIN  loss dict:  {'classification_loss': 1.099261589050293}
2025-01-14 14:34:30,248 [INFO] Step[2050/2713]: training loss : 1.1133466422557832 TRAIN  loss dict:  {'classification_loss': 1.1133466422557832}
2025-01-14 14:34:44,179 [INFO] Step[2100/2713]: training loss : 1.0330362594127656 TRAIN  loss dict:  {'classification_loss': 1.0330362594127656}
2025-01-14 14:34:58,371 [INFO] Step[2150/2713]: training loss : 1.1088605034351349 TRAIN  loss dict:  {'classification_loss': 1.1088605034351349}
2025-01-14 14:35:12,391 [INFO] Step[2200/2713]: training loss : 1.0401913702487946 TRAIN  loss dict:  {'classification_loss': 1.0401913702487946}
2025-01-14 14:35:25,617 [INFO] Step[2250/2713]: training loss : 1.0766821920871734 TRAIN  loss dict:  {'classification_loss': 1.0766821920871734}
2025-01-14 14:35:39,011 [INFO] Step[2300/2713]: training loss : 1.0672014844417572 TRAIN  loss dict:  {'classification_loss': 1.0672014844417572}
2025-01-14 14:35:52,399 [INFO] Step[2350/2713]: training loss : 1.0686733865737914 TRAIN  loss dict:  {'classification_loss': 1.0686733865737914}
2025-01-14 14:36:06,270 [INFO] Step[2400/2713]: training loss : 1.0434605491161346 TRAIN  loss dict:  {'classification_loss': 1.0434605491161346}
2025-01-14 14:36:20,394 [INFO] Step[2450/2713]: training loss : 1.0331066715717316 TRAIN  loss dict:  {'classification_loss': 1.0331066715717316}
2025-01-14 14:36:34,142 [INFO] Step[2500/2713]: training loss : 1.0323222649097443 TRAIN  loss dict:  {'classification_loss': 1.0323222649097443}
2025-01-14 14:36:47,913 [INFO] Step[2550/2713]: training loss : 1.0128923010826112 TRAIN  loss dict:  {'classification_loss': 1.0128923010826112}
2025-01-14 14:37:01,316 [INFO] Step[2600/2713]: training loss : 1.0461434149742126 TRAIN  loss dict:  {'classification_loss': 1.0461434149742126}
2025-01-14 14:37:14,844 [INFO] Step[2650/2713]: training loss : 1.0567075312137604 TRAIN  loss dict:  {'classification_loss': 1.0567075312137604}
2025-01-14 14:37:28,616 [INFO] Step[2700/2713]: training loss : 1.003792836666107 TRAIN  loss dict:  {'classification_loss': 1.003792836666107}
2025-01-14 14:38:44,912 [INFO] Label accuracies statistics:
2025-01-14 14:38:44,912 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.5, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 1.0, 57: 0.5, 58: 0.25, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 0.5, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.0, 183: 0.75, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.25, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 0.5, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 0.75, 258: 0.75, 259: 1.0, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.5, 274: 0.25, 275: 0.75, 276: 0.25, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.25, 290: 0.75, 291: 0.5, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 0.25, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 1.0, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 0.75, 358: 1.0, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.0, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 14:38:44,914 [INFO] [14] TRAIN  loss: 1.0577563827344512 acc: 0.9812016218208626
2025-01-14 14:38:44,914 [INFO] [14] TRAIN  loss dict: {'classification_loss': 1.0577563827344512}
2025-01-14 14:38:44,914 [INFO] [14] VALIDATION loss: 1.872801558751809 VALIDATION acc: 0.7673981191222571
2025-01-14 14:38:44,914 [INFO] [14] VALIDATION loss dict: {'classification_loss': 1.872801558751809}
2025-01-14 14:38:44,914 [INFO] 
2025-01-14 14:39:05,155 [INFO] Step[50/2713]: training loss : 1.0488924825191497 TRAIN  loss dict:  {'classification_loss': 1.0488924825191497}
2025-01-14 14:39:18,609 [INFO] Step[100/2713]: training loss : 1.0350200378894805 TRAIN  loss dict:  {'classification_loss': 1.0350200378894805}
2025-01-14 14:39:32,287 [INFO] Step[150/2713]: training loss : 1.0222426748275757 TRAIN  loss dict:  {'classification_loss': 1.0222426748275757}
2025-01-14 14:39:46,539 [INFO] Step[200/2713]: training loss : 1.0052366006374358 TRAIN  loss dict:  {'classification_loss': 1.0052366006374358}
2025-01-14 14:40:00,296 [INFO] Step[250/2713]: training loss : 1.0462238347530366 TRAIN  loss dict:  {'classification_loss': 1.0462238347530366}
2025-01-14 14:40:13,524 [INFO] Step[300/2713]: training loss : 1.0848741364479064 TRAIN  loss dict:  {'classification_loss': 1.0848741364479064}
2025-01-14 14:40:27,498 [INFO] Step[350/2713]: training loss : 1.0522485709190368 TRAIN  loss dict:  {'classification_loss': 1.0522485709190368}
2025-01-14 14:40:41,528 [INFO] Step[400/2713]: training loss : 1.0855847525596618 TRAIN  loss dict:  {'classification_loss': 1.0855847525596618}
2025-01-14 14:40:54,799 [INFO] Step[450/2713]: training loss : 1.0136493611335755 TRAIN  loss dict:  {'classification_loss': 1.0136493611335755}
2025-01-14 14:41:08,656 [INFO] Step[500/2713]: training loss : 1.0550649094581603 TRAIN  loss dict:  {'classification_loss': 1.0550649094581603}
2025-01-14 14:41:22,985 [INFO] Step[550/2713]: training loss : 1.0432878017425538 TRAIN  loss dict:  {'classification_loss': 1.0432878017425538}
2025-01-14 14:41:36,993 [INFO] Step[600/2713]: training loss : 1.0495597589015961 TRAIN  loss dict:  {'classification_loss': 1.0495597589015961}
2025-01-14 14:41:50,284 [INFO] Step[650/2713]: training loss : 1.0639169347286224 TRAIN  loss dict:  {'classification_loss': 1.0639169347286224}
2025-01-14 14:42:03,810 [INFO] Step[700/2713]: training loss : 1.050574119091034 TRAIN  loss dict:  {'classification_loss': 1.050574119091034}
2025-01-14 14:42:17,555 [INFO] Step[750/2713]: training loss : 1.0377963709831237 TRAIN  loss dict:  {'classification_loss': 1.0377963709831237}
2025-01-14 14:42:31,084 [INFO] Step[800/2713]: training loss : 1.0113156402111054 TRAIN  loss dict:  {'classification_loss': 1.0113156402111054}
2025-01-14 14:42:44,646 [INFO] Step[850/2713]: training loss : 1.0362936413288117 TRAIN  loss dict:  {'classification_loss': 1.0362936413288117}
2025-01-14 14:42:58,617 [INFO] Step[900/2713]: training loss : 1.0082067918777466 TRAIN  loss dict:  {'classification_loss': 1.0082067918777466}
2025-01-14 14:43:12,195 [INFO] Step[950/2713]: training loss : 1.0657107281684874 TRAIN  loss dict:  {'classification_loss': 1.0657107281684874}
2025-01-14 14:43:25,904 [INFO] Step[1000/2713]: training loss : 1.0357606673240662 TRAIN  loss dict:  {'classification_loss': 1.0357606673240662}
2025-01-14 14:43:39,702 [INFO] Step[1050/2713]: training loss : 1.0502760791778565 TRAIN  loss dict:  {'classification_loss': 1.0502760791778565}
2025-01-14 14:43:53,943 [INFO] Step[1100/2713]: training loss : 1.0289268565177918 TRAIN  loss dict:  {'classification_loss': 1.0289268565177918}
2025-01-14 14:44:07,779 [INFO] Step[1150/2713]: training loss : 1.0633326864242554 TRAIN  loss dict:  {'classification_loss': 1.0633326864242554}
2025-01-14 14:44:21,015 [INFO] Step[1200/2713]: training loss : 1.0496551239490508 TRAIN  loss dict:  {'classification_loss': 1.0496551239490508}
2025-01-14 14:44:34,276 [INFO] Step[1250/2713]: training loss : 1.075293823480606 TRAIN  loss dict:  {'classification_loss': 1.075293823480606}
2025-01-14 14:44:47,968 [INFO] Step[1300/2713]: training loss : 1.0288454902172088 TRAIN  loss dict:  {'classification_loss': 1.0288454902172088}
2025-01-14 14:45:02,102 [INFO] Step[1350/2713]: training loss : 1.0627263391017914 TRAIN  loss dict:  {'classification_loss': 1.0627263391017914}
2025-01-14 14:45:16,159 [INFO] Step[1400/2713]: training loss : 1.0754536938667298 TRAIN  loss dict:  {'classification_loss': 1.0754536938667298}
2025-01-14 14:45:29,981 [INFO] Step[1450/2713]: training loss : 1.0728380286693573 TRAIN  loss dict:  {'classification_loss': 1.0728380286693573}
2025-01-14 14:45:43,790 [INFO] Step[1500/2713]: training loss : 1.0163814854621886 TRAIN  loss dict:  {'classification_loss': 1.0163814854621886}
2025-01-14 14:45:57,492 [INFO] Step[1550/2713]: training loss : 1.0825854229927063 TRAIN  loss dict:  {'classification_loss': 1.0825854229927063}
2025-01-14 14:46:11,485 [INFO] Step[1600/2713]: training loss : 1.029408061504364 TRAIN  loss dict:  {'classification_loss': 1.029408061504364}
2025-01-14 14:46:25,209 [INFO] Step[1650/2713]: training loss : 1.0617945551872254 TRAIN  loss dict:  {'classification_loss': 1.0617945551872254}
2025-01-14 14:46:38,926 [INFO] Step[1700/2713]: training loss : 1.0559296345710754 TRAIN  loss dict:  {'classification_loss': 1.0559296345710754}
2025-01-14 14:46:52,870 [INFO] Step[1750/2713]: training loss : 1.0692469227313994 TRAIN  loss dict:  {'classification_loss': 1.0692469227313994}
2025-01-14 14:47:06,817 [INFO] Step[1800/2713]: training loss : 1.0436452150344848 TRAIN  loss dict:  {'classification_loss': 1.0436452150344848}
2025-01-14 14:47:20,456 [INFO] Step[1850/2713]: training loss : 1.019640576839447 TRAIN  loss dict:  {'classification_loss': 1.019640576839447}
2025-01-14 14:47:34,451 [INFO] Step[1900/2713]: training loss : 1.0754240012168885 TRAIN  loss dict:  {'classification_loss': 1.0754240012168885}
2025-01-14 14:47:48,465 [INFO] Step[1950/2713]: training loss : 1.0244805467128755 TRAIN  loss dict:  {'classification_loss': 1.0244805467128755}
2025-01-14 14:48:02,678 [INFO] Step[2000/2713]: training loss : 1.0675436329841614 TRAIN  loss dict:  {'classification_loss': 1.0675436329841614}
2025-01-14 14:48:17,888 [INFO] Step[2050/2713]: training loss : 1.0630027747154236 TRAIN  loss dict:  {'classification_loss': 1.0630027747154236}
2025-01-14 14:48:34,668 [INFO] Step[2100/2713]: training loss : 1.0962396907806395 TRAIN  loss dict:  {'classification_loss': 1.0962396907806395}
2025-01-14 14:48:48,567 [INFO] Step[2150/2713]: training loss : 1.069147754907608 TRAIN  loss dict:  {'classification_loss': 1.069147754907608}
2025-01-14 14:49:02,585 [INFO] Step[2200/2713]: training loss : 1.0241139698028565 TRAIN  loss dict:  {'classification_loss': 1.0241139698028565}
2025-01-14 14:49:16,570 [INFO] Step[2250/2713]: training loss : 1.0143389892578125 TRAIN  loss dict:  {'classification_loss': 1.0143389892578125}
2025-01-14 14:49:30,347 [INFO] Step[2300/2713]: training loss : 1.0666893327236175 TRAIN  loss dict:  {'classification_loss': 1.0666893327236175}
2025-01-14 14:49:44,107 [INFO] Step[2350/2713]: training loss : 1.037222321033478 TRAIN  loss dict:  {'classification_loss': 1.037222321033478}
2025-01-14 14:49:57,770 [INFO] Step[2400/2713]: training loss : 1.0856460297107697 TRAIN  loss dict:  {'classification_loss': 1.0856460297107697}
2025-01-14 14:50:11,712 [INFO] Step[2450/2713]: training loss : 1.1012583577632904 TRAIN  loss dict:  {'classification_loss': 1.1012583577632904}
2025-01-14 14:50:25,682 [INFO] Step[2500/2713]: training loss : 1.0396952748298645 TRAIN  loss dict:  {'classification_loss': 1.0396952748298645}
2025-01-14 14:50:39,555 [INFO] Step[2550/2713]: training loss : 1.0276863861083985 TRAIN  loss dict:  {'classification_loss': 1.0276863861083985}
2025-01-14 14:50:53,282 [INFO] Step[2600/2713]: training loss : 1.0331805229187012 TRAIN  loss dict:  {'classification_loss': 1.0331805229187012}
2025-01-14 14:51:07,171 [INFO] Step[2650/2713]: training loss : 1.0429541778564453 TRAIN  loss dict:  {'classification_loss': 1.0429541778564453}
2025-01-14 14:51:20,863 [INFO] Step[2700/2713]: training loss : 1.0486453771591187 TRAIN  loss dict:  {'classification_loss': 1.0486453771591187}
2025-01-14 14:52:37,644 [INFO] Label accuracies statistics:
2025-01-14 14:52:37,644 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.25, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.25, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.75, 69: 1.0, 70: 0.5, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.75, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.5, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.25, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 0.75, 255: 0.5, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.5, 277: 1.0, 278: 1.0, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.25, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.5, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.5, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 1.0, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.25, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 0.75, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 14:52:37,646 [INFO] [15] TRAIN  loss: 1.049091714227327 acc: 0.9837817913748618
2025-01-14 14:52:37,646 [INFO] [15] TRAIN  loss dict: {'classification_loss': 1.049091714227327}
2025-01-14 14:52:37,647 [INFO] [15] VALIDATION loss: 1.8559440978263553 VALIDATION acc: 0.7673981191222571
2025-01-14 14:52:37,647 [INFO] [15] VALIDATION loss dict: {'classification_loss': 1.8559440978263553}
2025-01-14 14:52:37,647 [INFO] 
2025-01-14 14:52:56,772 [INFO] Step[50/2713]: training loss : 1.017224850654602 TRAIN  loss dict:  {'classification_loss': 1.017224850654602}
2025-01-14 14:53:09,990 [INFO] Step[100/2713]: training loss : 1.0350280046463012 TRAIN  loss dict:  {'classification_loss': 1.0350280046463012}
2025-01-14 14:53:23,893 [INFO] Step[150/2713]: training loss : 1.0468636500835418 TRAIN  loss dict:  {'classification_loss': 1.0468636500835418}
2025-01-14 14:53:37,559 [INFO] Step[200/2713]: training loss : 1.052158280611038 TRAIN  loss dict:  {'classification_loss': 1.052158280611038}
2025-01-14 14:53:51,509 [INFO] Step[250/2713]: training loss : 1.0215488851070404 TRAIN  loss dict:  {'classification_loss': 1.0215488851070404}
2025-01-14 14:54:05,408 [INFO] Step[300/2713]: training loss : 1.0286648964881897 TRAIN  loss dict:  {'classification_loss': 1.0286648964881897}
2025-01-14 14:54:18,994 [INFO] Step[350/2713]: training loss : 1.0458555495738984 TRAIN  loss dict:  {'classification_loss': 1.0458555495738984}
2025-01-14 14:54:32,464 [INFO] Step[400/2713]: training loss : 1.0723253464698792 TRAIN  loss dict:  {'classification_loss': 1.0723253464698792}
2025-01-14 14:54:45,723 [INFO] Step[450/2713]: training loss : 1.0504859709739685 TRAIN  loss dict:  {'classification_loss': 1.0504859709739685}
2025-01-14 14:54:59,726 [INFO] Step[500/2713]: training loss : 1.012656660079956 TRAIN  loss dict:  {'classification_loss': 1.012656660079956}
2025-01-14 14:55:13,665 [INFO] Step[550/2713]: training loss : 1.0314265990257263 TRAIN  loss dict:  {'classification_loss': 1.0314265990257263}
2025-01-14 14:55:26,945 [INFO] Step[600/2713]: training loss : 1.0457058978080749 TRAIN  loss dict:  {'classification_loss': 1.0457058978080749}
2025-01-14 14:55:41,111 [INFO] Step[650/2713]: training loss : 1.025583839416504 TRAIN  loss dict:  {'classification_loss': 1.025583839416504}
2025-01-14 14:55:54,968 [INFO] Step[700/2713]: training loss : 1.054539715051651 TRAIN  loss dict:  {'classification_loss': 1.054539715051651}
2025-01-14 14:56:08,974 [INFO] Step[750/2713]: training loss : 1.0127804851531983 TRAIN  loss dict:  {'classification_loss': 1.0127804851531983}
2025-01-14 14:56:22,364 [INFO] Step[800/2713]: training loss : 1.023822581768036 TRAIN  loss dict:  {'classification_loss': 1.023822581768036}
2025-01-14 14:56:36,122 [INFO] Step[850/2713]: training loss : 1.025867154598236 TRAIN  loss dict:  {'classification_loss': 1.025867154598236}
2025-01-14 14:56:50,129 [INFO] Step[900/2713]: training loss : 1.03640038728714 TRAIN  loss dict:  {'classification_loss': 1.03640038728714}
2025-01-14 14:57:04,265 [INFO] Step[950/2713]: training loss : 1.024311512708664 TRAIN  loss dict:  {'classification_loss': 1.024311512708664}
2025-01-14 14:57:17,560 [INFO] Step[1000/2713]: training loss : 1.0280606925487519 TRAIN  loss dict:  {'classification_loss': 1.0280606925487519}
2025-01-14 14:57:31,173 [INFO] Step[1050/2713]: training loss : 1.032189108133316 TRAIN  loss dict:  {'classification_loss': 1.032189108133316}
2025-01-14 14:57:46,180 [INFO] Step[1100/2713]: training loss : 1.077831324338913 TRAIN  loss dict:  {'classification_loss': 1.077831324338913}
2025-01-14 14:58:03,347 [INFO] Step[1150/2713]: training loss : 1.0352802312374114 TRAIN  loss dict:  {'classification_loss': 1.0352802312374114}
2025-01-14 14:58:18,113 [INFO] Step[1200/2713]: training loss : 1.0285741627216338 TRAIN  loss dict:  {'classification_loss': 1.0285741627216338}
2025-01-14 14:58:31,878 [INFO] Step[1250/2713]: training loss : 1.0128107798099517 TRAIN  loss dict:  {'classification_loss': 1.0128107798099517}
2025-01-14 14:58:45,917 [INFO] Step[1300/2713]: training loss : 1.0347586023807525 TRAIN  loss dict:  {'classification_loss': 1.0347586023807525}
2025-01-14 14:58:59,997 [INFO] Step[1350/2713]: training loss : 1.0281326627731324 TRAIN  loss dict:  {'classification_loss': 1.0281326627731324}
2025-01-14 14:59:14,128 [INFO] Step[1400/2713]: training loss : 1.044761095046997 TRAIN  loss dict:  {'classification_loss': 1.044761095046997}
2025-01-14 14:59:27,630 [INFO] Step[1450/2713]: training loss : 1.0456784212589263 TRAIN  loss dict:  {'classification_loss': 1.0456784212589263}
2025-01-14 14:59:41,180 [INFO] Step[1500/2713]: training loss : 1.048221549987793 TRAIN  loss dict:  {'classification_loss': 1.048221549987793}
2025-01-14 14:59:55,167 [INFO] Step[1550/2713]: training loss : 1.032644087076187 TRAIN  loss dict:  {'classification_loss': 1.032644087076187}
2025-01-14 15:00:09,198 [INFO] Step[1600/2713]: training loss : 1.0558285343647003 TRAIN  loss dict:  {'classification_loss': 1.0558285343647003}
2025-01-14 15:00:23,142 [INFO] Step[1650/2713]: training loss : 1.0226264142990111 TRAIN  loss dict:  {'classification_loss': 1.0226264142990111}
2025-01-14 15:00:37,149 [INFO] Step[1700/2713]: training loss : 1.0225535786151887 TRAIN  loss dict:  {'classification_loss': 1.0225535786151887}
2025-01-14 15:00:50,664 [INFO] Step[1750/2713]: training loss : 1.0296300721168519 TRAIN  loss dict:  {'classification_loss': 1.0296300721168519}
2025-01-14 15:01:04,038 [INFO] Step[1800/2713]: training loss : 1.0440519154071808 TRAIN  loss dict:  {'classification_loss': 1.0440519154071808}
2025-01-14 15:01:17,852 [INFO] Step[1850/2713]: training loss : 1.067405390739441 TRAIN  loss dict:  {'classification_loss': 1.067405390739441}
2025-01-14 15:01:31,952 [INFO] Step[1900/2713]: training loss : 1.0327612447738648 TRAIN  loss dict:  {'classification_loss': 1.0327612447738648}
2025-01-14 15:01:45,309 [INFO] Step[1950/2713]: training loss : 1.0256575238704682 TRAIN  loss dict:  {'classification_loss': 1.0256575238704682}
2025-01-14 15:01:58,603 [INFO] Step[2000/2713]: training loss : 1.0773205411434175 TRAIN  loss dict:  {'classification_loss': 1.0773205411434175}
2025-01-14 15:02:11,925 [INFO] Step[2050/2713]: training loss : 1.1224033796787263 TRAIN  loss dict:  {'classification_loss': 1.1224033796787263}
2025-01-14 15:02:25,279 [INFO] Step[2100/2713]: training loss : 1.0605657410621643 TRAIN  loss dict:  {'classification_loss': 1.0605657410621643}
2025-01-14 15:02:39,443 [INFO] Step[2150/2713]: training loss : 1.0496404504776 TRAIN  loss dict:  {'classification_loss': 1.0496404504776}
2025-01-14 15:02:53,335 [INFO] Step[2200/2713]: training loss : 1.0326244616508484 TRAIN  loss dict:  {'classification_loss': 1.0326244616508484}
2025-01-14 15:03:07,158 [INFO] Step[2250/2713]: training loss : 1.0881690764427185 TRAIN  loss dict:  {'classification_loss': 1.0881690764427185}
2025-01-14 15:03:21,237 [INFO] Step[2300/2713]: training loss : 1.0492761468887328 TRAIN  loss dict:  {'classification_loss': 1.0492761468887328}
2025-01-14 15:03:35,370 [INFO] Step[2350/2713]: training loss : 1.0444470822811127 TRAIN  loss dict:  {'classification_loss': 1.0444470822811127}
2025-01-14 15:03:48,946 [INFO] Step[2400/2713]: training loss : 1.018893438577652 TRAIN  loss dict:  {'classification_loss': 1.018893438577652}
2025-01-14 15:04:02,326 [INFO] Step[2450/2713]: training loss : 1.0105055940151215 TRAIN  loss dict:  {'classification_loss': 1.0105055940151215}
2025-01-14 15:04:15,632 [INFO] Step[2500/2713]: training loss : 1.0450474941730499 TRAIN  loss dict:  {'classification_loss': 1.0450474941730499}
2025-01-14 15:04:29,347 [INFO] Step[2550/2713]: training loss : 1.0802036774158479 TRAIN  loss dict:  {'classification_loss': 1.0802036774158479}
2025-01-14 15:04:42,835 [INFO] Step[2600/2713]: training loss : 1.0441329741477967 TRAIN  loss dict:  {'classification_loss': 1.0441329741477967}
2025-01-14 15:04:56,494 [INFO] Step[2650/2713]: training loss : 1.0427037405967712 TRAIN  loss dict:  {'classification_loss': 1.0427037405967712}
2025-01-14 15:05:09,951 [INFO] Step[2700/2713]: training loss : 1.0836475718021392 TRAIN  loss dict:  {'classification_loss': 1.0836475718021392}
2025-01-14 15:06:26,477 [INFO] Label accuracies statistics:
2025-01-14 15:06:26,478 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.5, 117: 0.5, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.5, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.3333333333333333, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.5, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.25, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.5, 241: 1.0, 242: 0.25, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.5, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.25, 261: 0.0, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 0.75, 267: 0.75, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.25, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.25, 388: 0.5, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.5, 395: 1.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 15:06:26,479 [INFO] [16] TRAIN  loss: 1.0421537437695574 acc: 0.9832903305074333
2025-01-14 15:06:26,479 [INFO] [16] TRAIN  loss dict: {'classification_loss': 1.0421537437695574}
2025-01-14 15:06:26,479 [INFO] [16] VALIDATION loss: 1.864147084099906 VALIDATION acc: 0.7730407523510971
2025-01-14 15:06:26,479 [INFO] [16] VALIDATION loss dict: {'classification_loss': 1.864147084099906}
2025-01-14 15:06:26,480 [INFO] 
2025-01-14 15:07:00,090 [INFO] Step[50/2713]: training loss : 1.0135688877105713 TRAIN  loss dict:  {'classification_loss': 1.0135688877105713}
2025-01-14 15:07:13,795 [INFO] Step[100/2713]: training loss : 1.0385151398181915 TRAIN  loss dict:  {'classification_loss': 1.0385151398181915}
2025-01-14 15:07:27,303 [INFO] Step[150/2713]: training loss : 1.031096317768097 TRAIN  loss dict:  {'classification_loss': 1.031096317768097}
2025-01-14 15:07:42,476 [INFO] Step[200/2713]: training loss : 1.0436836743354798 TRAIN  loss dict:  {'classification_loss': 1.0436836743354798}
2025-01-14 15:07:57,085 [INFO] Step[250/2713]: training loss : 1.0436915826797486 TRAIN  loss dict:  {'classification_loss': 1.0436915826797486}
2025-01-14 15:08:11,077 [INFO] Step[300/2713]: training loss : 1.0491842138767242 TRAIN  loss dict:  {'classification_loss': 1.0491842138767242}
2025-01-14 15:08:27,796 [INFO] Step[350/2713]: training loss : 1.012369500398636 TRAIN  loss dict:  {'classification_loss': 1.012369500398636}
2025-01-14 15:08:41,344 [INFO] Step[400/2713]: training loss : 1.0268567502498627 TRAIN  loss dict:  {'classification_loss': 1.0268567502498627}
2025-01-14 15:08:55,097 [INFO] Step[450/2713]: training loss : 1.0620505428314209 TRAIN  loss dict:  {'classification_loss': 1.0620505428314209}
2025-01-14 15:09:08,791 [INFO] Step[500/2713]: training loss : 1.0742548298835755 TRAIN  loss dict:  {'classification_loss': 1.0742548298835755}
2025-01-14 15:09:22,992 [INFO] Step[550/2713]: training loss : 1.008600025177002 TRAIN  loss dict:  {'classification_loss': 1.008600025177002}
2025-01-14 15:09:36,357 [INFO] Step[600/2713]: training loss : 1.0507757294178008 TRAIN  loss dict:  {'classification_loss': 1.0507757294178008}
2025-01-14 15:09:50,261 [INFO] Step[650/2713]: training loss : 1.0100088834762573 TRAIN  loss dict:  {'classification_loss': 1.0100088834762573}
2025-01-14 15:10:04,074 [INFO] Step[700/2713]: training loss : 1.0174250280857087 TRAIN  loss dict:  {'classification_loss': 1.0174250280857087}
2025-01-14 15:10:17,770 [INFO] Step[750/2713]: training loss : 1.0361519944667816 TRAIN  loss dict:  {'classification_loss': 1.0361519944667816}
2025-01-14 15:10:31,125 [INFO] Step[800/2713]: training loss : 1.0231417977809907 TRAIN  loss dict:  {'classification_loss': 1.0231417977809907}
2025-01-14 15:10:45,085 [INFO] Step[850/2713]: training loss : 1.0501361751556397 TRAIN  loss dict:  {'classification_loss': 1.0501361751556397}
2025-01-14 15:10:59,307 [INFO] Step[900/2713]: training loss : 1.0313679146766663 TRAIN  loss dict:  {'classification_loss': 1.0313679146766663}
2025-01-14 15:11:13,101 [INFO] Step[950/2713]: training loss : 1.0265254497528076 TRAIN  loss dict:  {'classification_loss': 1.0265254497528076}
2025-01-14 15:11:27,003 [INFO] Step[1000/2713]: training loss : 1.0488632893562317 TRAIN  loss dict:  {'classification_loss': 1.0488632893562317}
2025-01-14 15:11:40,356 [INFO] Step[1050/2713]: training loss : 1.0137750399112702 TRAIN  loss dict:  {'classification_loss': 1.0137750399112702}
2025-01-14 15:11:54,370 [INFO] Step[1100/2713]: training loss : 1.0496589732170105 TRAIN  loss dict:  {'classification_loss': 1.0496589732170105}
2025-01-14 15:12:08,629 [INFO] Step[1150/2713]: training loss : 1.0227726924419402 TRAIN  loss dict:  {'classification_loss': 1.0227726924419402}
2025-01-14 15:12:22,297 [INFO] Step[1200/2713]: training loss : 1.0327822279930114 TRAIN  loss dict:  {'classification_loss': 1.0327822279930114}
2025-01-14 15:12:35,971 [INFO] Step[1250/2713]: training loss : 1.020529692173004 TRAIN  loss dict:  {'classification_loss': 1.020529692173004}
2025-01-14 15:12:49,620 [INFO] Step[1300/2713]: training loss : 1.043047184944153 TRAIN  loss dict:  {'classification_loss': 1.043047184944153}
2025-01-14 15:13:03,621 [INFO] Step[1350/2713]: training loss : 1.0986557114124298 TRAIN  loss dict:  {'classification_loss': 1.0986557114124298}
2025-01-14 15:13:16,963 [INFO] Step[1400/2713]: training loss : 1.0887041342258454 TRAIN  loss dict:  {'classification_loss': 1.0887041342258454}
2025-01-14 15:13:30,679 [INFO] Step[1450/2713]: training loss : 1.0234193360805512 TRAIN  loss dict:  {'classification_loss': 1.0234193360805512}
2025-01-14 15:13:44,892 [INFO] Step[1500/2713]: training loss : 1.0433576333522796 TRAIN  loss dict:  {'classification_loss': 1.0433576333522796}
2025-01-14 15:13:59,087 [INFO] Step[1550/2713]: training loss : 1.0270859003067017 TRAIN  loss dict:  {'classification_loss': 1.0270859003067017}
2025-01-14 15:14:12,705 [INFO] Step[1600/2713]: training loss : 1.0390831816196442 TRAIN  loss dict:  {'classification_loss': 1.0390831816196442}
2025-01-14 15:14:26,307 [INFO] Step[1650/2713]: training loss : 0.9993646645545959 TRAIN  loss dict:  {'classification_loss': 0.9993646645545959}
2025-01-14 15:14:39,823 [INFO] Step[1700/2713]: training loss : 1.0187015199661256 TRAIN  loss dict:  {'classification_loss': 1.0187015199661256}
2025-01-14 15:14:53,366 [INFO] Step[1750/2713]: training loss : 1.0717215788364411 TRAIN  loss dict:  {'classification_loss': 1.0717215788364411}
2025-01-14 15:15:07,195 [INFO] Step[1800/2713]: training loss : 1.0356544697284698 TRAIN  loss dict:  {'classification_loss': 1.0356544697284698}
2025-01-14 15:15:23,679 [INFO] Step[1850/2713]: training loss : 1.0427063453197478 TRAIN  loss dict:  {'classification_loss': 1.0427063453197478}
2025-01-14 15:15:39,382 [INFO] Step[1900/2713]: training loss : 1.0472598600387573 TRAIN  loss dict:  {'classification_loss': 1.0472598600387573}
2025-01-14 15:15:53,482 [INFO] Step[1950/2713]: training loss : 1.0400585746765136 TRAIN  loss dict:  {'classification_loss': 1.0400585746765136}
2025-01-14 15:16:07,111 [INFO] Step[2000/2713]: training loss : 1.0325256848335267 TRAIN  loss dict:  {'classification_loss': 1.0325256848335267}
2025-01-14 15:16:21,084 [INFO] Step[2050/2713]: training loss : 1.0532587611675261 TRAIN  loss dict:  {'classification_loss': 1.0532587611675261}
2025-01-14 15:16:34,654 [INFO] Step[2100/2713]: training loss : 1.0432282495498657 TRAIN  loss dict:  {'classification_loss': 1.0432282495498657}
2025-01-14 15:16:48,393 [INFO] Step[2150/2713]: training loss : 1.007279554605484 TRAIN  loss dict:  {'classification_loss': 1.007279554605484}
2025-01-14 15:17:01,851 [INFO] Step[2200/2713]: training loss : 1.0701554596424103 TRAIN  loss dict:  {'classification_loss': 1.0701554596424103}
2025-01-14 15:17:15,559 [INFO] Step[2250/2713]: training loss : 1.022628663778305 TRAIN  loss dict:  {'classification_loss': 1.022628663778305}
2025-01-14 15:17:28,587 [INFO] Step[2300/2713]: training loss : 1.0361255955696107 TRAIN  loss dict:  {'classification_loss': 1.0361255955696107}
2025-01-14 15:17:42,300 [INFO] Step[2350/2713]: training loss : 1.0033934795856476 TRAIN  loss dict:  {'classification_loss': 1.0033934795856476}
2025-01-14 15:17:56,039 [INFO] Step[2400/2713]: training loss : 1.0381058979034423 TRAIN  loss dict:  {'classification_loss': 1.0381058979034423}
2025-01-14 15:18:09,835 [INFO] Step[2450/2713]: training loss : 1.0259467113018035 TRAIN  loss dict:  {'classification_loss': 1.0259467113018035}
2025-01-14 15:18:23,364 [INFO] Step[2500/2713]: training loss : 1.0227614891529084 TRAIN  loss dict:  {'classification_loss': 1.0227614891529084}
2025-01-14 15:18:37,095 [INFO] Step[2550/2713]: training loss : 1.0329251217842101 TRAIN  loss dict:  {'classification_loss': 1.0329251217842101}
2025-01-14 15:18:50,647 [INFO] Step[2600/2713]: training loss : 1.0534945595264436 TRAIN  loss dict:  {'classification_loss': 1.0534945595264436}
2025-01-14 15:19:03,742 [INFO] Step[2650/2713]: training loss : 1.052120281457901 TRAIN  loss dict:  {'classification_loss': 1.052120281457901}
2025-01-14 15:19:17,614 [INFO] Step[2700/2713]: training loss : 1.0839118683338165 TRAIN  loss dict:  {'classification_loss': 1.0839118683338165}
2025-01-14 15:20:34,905 [INFO] Label accuracies statistics:
2025-01-14 15:20:34,905 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.5, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 0.75, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 0.75, 140: 0.75, 141: 0.5, 142: 0.75, 143: 1.0, 144: 0.75, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.25, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.3333333333333333, 180: 0.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.75, 240: 0.75, 241: 0.75, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.25, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.25, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 15:20:34,907 [INFO] [17] TRAIN  loss: 1.0377188607682348 acc: 0.9834131957242904
2025-01-14 15:20:34,907 [INFO] [17] TRAIN  loss dict: {'classification_loss': 1.0377188607682348}
2025-01-14 15:20:34,907 [INFO] [17] VALIDATION loss: 1.873318033446943 VALIDATION acc: 0.7699059561128526
2025-01-14 15:20:34,907 [INFO] [17] VALIDATION loss dict: {'classification_loss': 1.873318033446943}
2025-01-14 15:20:34,907 [INFO] 
2025-01-14 15:20:54,097 [INFO] Step[50/2713]: training loss : 1.0030204629898072 TRAIN  loss dict:  {'classification_loss': 1.0030204629898072}
2025-01-14 15:21:07,963 [INFO] Step[100/2713]: training loss : 1.013415938615799 TRAIN  loss dict:  {'classification_loss': 1.013415938615799}
2025-01-14 15:21:21,938 [INFO] Step[150/2713]: training loss : 1.0267396020889281 TRAIN  loss dict:  {'classification_loss': 1.0267396020889281}
2025-01-14 15:21:35,558 [INFO] Step[200/2713]: training loss : 1.0027994680404664 TRAIN  loss dict:  {'classification_loss': 1.0027994680404664}
2025-01-14 15:21:48,796 [INFO] Step[250/2713]: training loss : 1.0176258015632629 TRAIN  loss dict:  {'classification_loss': 1.0176258015632629}
2025-01-14 15:22:02,006 [INFO] Step[300/2713]: training loss : 1.0139867043495179 TRAIN  loss dict:  {'classification_loss': 1.0139867043495179}
2025-01-14 15:22:15,764 [INFO] Step[350/2713]: training loss : 1.0262661910057067 TRAIN  loss dict:  {'classification_loss': 1.0262661910057067}
2025-01-14 15:22:29,953 [INFO] Step[400/2713]: training loss : 1.0090611064434052 TRAIN  loss dict:  {'classification_loss': 1.0090611064434052}
2025-01-14 15:22:43,897 [INFO] Step[450/2713]: training loss : 1.0111392426490784 TRAIN  loss dict:  {'classification_loss': 1.0111392426490784}
2025-01-14 15:22:57,917 [INFO] Step[500/2713]: training loss : 1.047230634689331 TRAIN  loss dict:  {'classification_loss': 1.047230634689331}
2025-01-14 15:23:11,995 [INFO] Step[550/2713]: training loss : 1.0530566716194152 TRAIN  loss dict:  {'classification_loss': 1.0530566716194152}
2025-01-14 15:23:25,973 [INFO] Step[600/2713]: training loss : 1.0216250646114349 TRAIN  loss dict:  {'classification_loss': 1.0216250646114349}
2025-01-14 15:23:39,911 [INFO] Step[650/2713]: training loss : 0.9855724728107452 TRAIN  loss dict:  {'classification_loss': 0.9855724728107452}
2025-01-14 15:23:53,273 [INFO] Step[700/2713]: training loss : 0.9904840469360352 TRAIN  loss dict:  {'classification_loss': 0.9904840469360352}
2025-01-14 15:24:06,508 [INFO] Step[750/2713]: training loss : 1.0245591604709625 TRAIN  loss dict:  {'classification_loss': 1.0245591604709625}
2025-01-14 15:24:20,248 [INFO] Step[800/2713]: training loss : 1.067590435743332 TRAIN  loss dict:  {'classification_loss': 1.067590435743332}
2025-01-14 15:24:33,952 [INFO] Step[850/2713]: training loss : 1.0289708483219147 TRAIN  loss dict:  {'classification_loss': 1.0289708483219147}
2025-01-14 15:24:47,449 [INFO] Step[900/2713]: training loss : 1.0224504911899566 TRAIN  loss dict:  {'classification_loss': 1.0224504911899566}
2025-01-14 15:25:03,557 [INFO] Step[950/2713]: training loss : 1.0306346774101258 TRAIN  loss dict:  {'classification_loss': 1.0306346774101258}
2025-01-14 15:25:17,790 [INFO] Step[1000/2713]: training loss : 1.0194564712047578 TRAIN  loss dict:  {'classification_loss': 1.0194564712047578}
2025-01-14 15:25:31,655 [INFO] Step[1050/2713]: training loss : 0.9909094941616058 TRAIN  loss dict:  {'classification_loss': 0.9909094941616058}
2025-01-14 15:25:45,158 [INFO] Step[1100/2713]: training loss : 1.0213958835601806 TRAIN  loss dict:  {'classification_loss': 1.0213958835601806}
2025-01-14 15:25:58,835 [INFO] Step[1150/2713]: training loss : 1.0326148962974548 TRAIN  loss dict:  {'classification_loss': 1.0326148962974548}
2025-01-14 15:26:12,167 [INFO] Step[1200/2713]: training loss : 1.004712324142456 TRAIN  loss dict:  {'classification_loss': 1.004712324142456}
2025-01-14 15:26:25,880 [INFO] Step[1250/2713]: training loss : 1.0069753813743592 TRAIN  loss dict:  {'classification_loss': 1.0069753813743592}
2025-01-14 15:26:39,124 [INFO] Step[1300/2713]: training loss : 1.013902133703232 TRAIN  loss dict:  {'classification_loss': 1.013902133703232}
2025-01-14 15:26:52,400 [INFO] Step[1350/2713]: training loss : 1.0369895350933076 TRAIN  loss dict:  {'classification_loss': 1.0369895350933076}
2025-01-14 15:27:05,973 [INFO] Step[1400/2713]: training loss : 1.0322022819519043 TRAIN  loss dict:  {'classification_loss': 1.0322022819519043}
2025-01-14 15:27:19,481 [INFO] Step[1450/2713]: training loss : 1.0515803062915803 TRAIN  loss dict:  {'classification_loss': 1.0515803062915803}
2025-01-14 15:27:33,038 [INFO] Step[1500/2713]: training loss : 0.9918437421321868 TRAIN  loss dict:  {'classification_loss': 0.9918437421321868}
2025-01-14 15:27:46,288 [INFO] Step[1550/2713]: training loss : 1.0571101129055023 TRAIN  loss dict:  {'classification_loss': 1.0571101129055023}
2025-01-14 15:28:00,140 [INFO] Step[1600/2713]: training loss : 1.0418752861022949 TRAIN  loss dict:  {'classification_loss': 1.0418752861022949}
2025-01-14 15:28:13,341 [INFO] Step[1650/2713]: training loss : 1.0320929932594298 TRAIN  loss dict:  {'classification_loss': 1.0320929932594298}
2025-01-14 15:28:27,174 [INFO] Step[1700/2713]: training loss : 1.027733027935028 TRAIN  loss dict:  {'classification_loss': 1.027733027935028}
2025-01-14 15:28:41,416 [INFO] Step[1750/2713]: training loss : 1.040811333656311 TRAIN  loss dict:  {'classification_loss': 1.040811333656311}
2025-01-14 15:28:54,968 [INFO] Step[1800/2713]: training loss : 1.0384107446670532 TRAIN  loss dict:  {'classification_loss': 1.0384107446670532}
2025-01-14 15:29:09,141 [INFO] Step[1850/2713]: training loss : 1.0441830229759217 TRAIN  loss dict:  {'classification_loss': 1.0441830229759217}
2025-01-14 15:29:22,483 [INFO] Step[1900/2713]: training loss : 1.0566011655330658 TRAIN  loss dict:  {'classification_loss': 1.0566011655330658}
2025-01-14 15:29:36,538 [INFO] Step[1950/2713]: training loss : 1.0443913006782533 TRAIN  loss dict:  {'classification_loss': 1.0443913006782533}
2025-01-14 15:29:50,781 [INFO] Step[2000/2713]: training loss : 1.0356317901611327 TRAIN  loss dict:  {'classification_loss': 1.0356317901611327}
2025-01-14 15:30:04,714 [INFO] Step[2050/2713]: training loss : 1.0135498237609863 TRAIN  loss dict:  {'classification_loss': 1.0135498237609863}
2025-01-14 15:30:18,350 [INFO] Step[2100/2713]: training loss : 1.014779578447342 TRAIN  loss dict:  {'classification_loss': 1.014779578447342}
2025-01-14 15:30:32,104 [INFO] Step[2150/2713]: training loss : 1.0256005823612213 TRAIN  loss dict:  {'classification_loss': 1.0256005823612213}
2025-01-14 15:30:46,064 [INFO] Step[2200/2713]: training loss : 1.0528322529792786 TRAIN  loss dict:  {'classification_loss': 1.0528322529792786}
2025-01-14 15:31:00,036 [INFO] Step[2250/2713]: training loss : 1.049440279006958 TRAIN  loss dict:  {'classification_loss': 1.049440279006958}
2025-01-14 15:31:13,934 [INFO] Step[2300/2713]: training loss : 1.0229426836967468 TRAIN  loss dict:  {'classification_loss': 1.0229426836967468}
2025-01-14 15:31:27,921 [INFO] Step[2350/2713]: training loss : 1.0343838846683502 TRAIN  loss dict:  {'classification_loss': 1.0343838846683502}
2025-01-14 15:31:41,628 [INFO] Step[2400/2713]: training loss : 1.0111967957019805 TRAIN  loss dict:  {'classification_loss': 1.0111967957019805}
2025-01-14 15:31:55,236 [INFO] Step[2450/2713]: training loss : 1.0121608936786652 TRAIN  loss dict:  {'classification_loss': 1.0121608936786652}
2025-01-14 15:32:09,259 [INFO] Step[2500/2713]: training loss : 1.0216099524497986 TRAIN  loss dict:  {'classification_loss': 1.0216099524497986}
2025-01-14 15:32:22,839 [INFO] Step[2550/2713]: training loss : 1.065968234539032 TRAIN  loss dict:  {'classification_loss': 1.065968234539032}
2025-01-14 15:32:36,682 [INFO] Step[2600/2713]: training loss : 1.005472527742386 TRAIN  loss dict:  {'classification_loss': 1.005472527742386}
2025-01-14 15:32:50,247 [INFO] Step[2650/2713]: training loss : 1.0335668873786927 TRAIN  loss dict:  {'classification_loss': 1.0335668873786927}
2025-01-14 15:33:03,975 [INFO] Step[2700/2713]: training loss : 1.0487123453617095 TRAIN  loss dict:  {'classification_loss': 1.0487123453617095}
2025-01-14 15:34:20,547 [INFO] Label accuracies statistics:
2025-01-14 15:34:20,547 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 1.0, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.5, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.5, 117: 0.5, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.5, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.25, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.5, 240: 0.75, 241: 0.75, 242: 0.75, 243: 0.5, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 1.0, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 0.75, 266: 0.75, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.5, 290: 0.0, 291: 1.0, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 0.75, 343: 1.0, 344: 1.0, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.25, 394: 0.75, 395: 0.5, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 15:34:20,549 [INFO] [18] TRAIN  loss: 1.0265775530530745 acc: 0.9864848261457182
2025-01-14 15:34:20,549 [INFO] [18] TRAIN  loss dict: {'classification_loss': 1.0265775530530745}
2025-01-14 15:34:20,549 [INFO] [18] VALIDATION loss: 1.9186980291864926 VALIDATION acc: 0.7642633228840126
2025-01-14 15:34:20,549 [INFO] [18] VALIDATION loss dict: {'classification_loss': 1.9186980291864926}
2025-01-14 15:34:20,549 [INFO] 
2025-01-14 15:34:39,819 [INFO] Step[50/2713]: training loss : 1.0118812906742096 TRAIN  loss dict:  {'classification_loss': 1.0118812906742096}
2025-01-14 15:34:53,761 [INFO] Step[100/2713]: training loss : 1.052162528038025 TRAIN  loss dict:  {'classification_loss': 1.052162528038025}
2025-01-14 15:35:07,591 [INFO] Step[150/2713]: training loss : 1.0181352412700653 TRAIN  loss dict:  {'classification_loss': 1.0181352412700653}
2025-01-14 15:35:21,459 [INFO] Step[200/2713]: training loss : 0.9838190531730652 TRAIN  loss dict:  {'classification_loss': 0.9838190531730652}
2025-01-14 15:35:34,738 [INFO] Step[250/2713]: training loss : 1.0076990962028503 TRAIN  loss dict:  {'classification_loss': 1.0076990962028503}
2025-01-14 15:35:48,685 [INFO] Step[300/2713]: training loss : 1.001020656824112 TRAIN  loss dict:  {'classification_loss': 1.001020656824112}
2025-01-14 15:36:02,678 [INFO] Step[350/2713]: training loss : 1.0013167119026185 TRAIN  loss dict:  {'classification_loss': 1.0013167119026185}
2025-01-14 15:36:16,748 [INFO] Step[400/2713]: training loss : 1.079998117685318 TRAIN  loss dict:  {'classification_loss': 1.079998117685318}
2025-01-14 15:36:30,766 [INFO] Step[450/2713]: training loss : 0.9935383236408234 TRAIN  loss dict:  {'classification_loss': 0.9935383236408234}
2025-01-14 15:36:44,870 [INFO] Step[500/2713]: training loss : 1.0087156283855438 TRAIN  loss dict:  {'classification_loss': 1.0087156283855438}
2025-01-14 15:36:58,128 [INFO] Step[550/2713]: training loss : 0.9983430624008178 TRAIN  loss dict:  {'classification_loss': 0.9983430624008178}
2025-01-14 15:37:11,639 [INFO] Step[600/2713]: training loss : 1.0152830827236174 TRAIN  loss dict:  {'classification_loss': 1.0152830827236174}
2025-01-14 15:37:25,507 [INFO] Step[650/2713]: training loss : 1.0313625478744506 TRAIN  loss dict:  {'classification_loss': 1.0313625478744506}
2025-01-14 15:37:38,803 [INFO] Step[700/2713]: training loss : 1.0181247043609618 TRAIN  loss dict:  {'classification_loss': 1.0181247043609618}
2025-01-14 15:37:52,709 [INFO] Step[750/2713]: training loss : 1.0357346367835998 TRAIN  loss dict:  {'classification_loss': 1.0357346367835998}
2025-01-14 15:38:05,961 [INFO] Step[800/2713]: training loss : 1.009886373281479 TRAIN  loss dict:  {'classification_loss': 1.009886373281479}
2025-01-14 15:38:19,193 [INFO] Step[850/2713]: training loss : 1.000609622001648 TRAIN  loss dict:  {'classification_loss': 1.000609622001648}
2025-01-14 15:38:33,274 [INFO] Step[900/2713]: training loss : 1.0761684155464173 TRAIN  loss dict:  {'classification_loss': 1.0761684155464173}
2025-01-14 15:38:46,712 [INFO] Step[950/2713]: training loss : 1.0780825662612914 TRAIN  loss dict:  {'classification_loss': 1.0780825662612914}
2025-01-14 15:39:00,535 [INFO] Step[1000/2713]: training loss : 1.083602659702301 TRAIN  loss dict:  {'classification_loss': 1.083602659702301}
2025-01-14 15:39:14,459 [INFO] Step[1050/2713]: training loss : 1.0341958916187286 TRAIN  loss dict:  {'classification_loss': 1.0341958916187286}
2025-01-14 15:39:28,421 [INFO] Step[1100/2713]: training loss : 1.0098912107944489 TRAIN  loss dict:  {'classification_loss': 1.0098912107944489}
2025-01-14 15:39:42,269 [INFO] Step[1150/2713]: training loss : 1.0319350135326386 TRAIN  loss dict:  {'classification_loss': 1.0319350135326386}
2025-01-14 15:39:56,155 [INFO] Step[1200/2713]: training loss : 1.0515769708156586 TRAIN  loss dict:  {'classification_loss': 1.0515769708156586}
2025-01-14 15:40:10,395 [INFO] Step[1250/2713]: training loss : 1.0166381335258483 TRAIN  loss dict:  {'classification_loss': 1.0166381335258483}
2025-01-14 15:40:23,872 [INFO] Step[1300/2713]: training loss : 1.011059240102768 TRAIN  loss dict:  {'classification_loss': 1.011059240102768}
2025-01-14 15:40:37,540 [INFO] Step[1350/2713]: training loss : 0.99602658867836 TRAIN  loss dict:  {'classification_loss': 0.99602658867836}
2025-01-14 15:40:51,032 [INFO] Step[1400/2713]: training loss : 0.9939319920539856 TRAIN  loss dict:  {'classification_loss': 0.9939319920539856}
2025-01-14 15:41:04,542 [INFO] Step[1450/2713]: training loss : 1.0210713124275208 TRAIN  loss dict:  {'classification_loss': 1.0210713124275208}
2025-01-14 15:41:18,732 [INFO] Step[1500/2713]: training loss : 1.0101545941829682 TRAIN  loss dict:  {'classification_loss': 1.0101545941829682}
2025-01-14 15:41:32,239 [INFO] Step[1550/2713]: training loss : 1.004979784488678 TRAIN  loss dict:  {'classification_loss': 1.004979784488678}
2025-01-14 15:41:46,165 [INFO] Step[1600/2713]: training loss : 1.0166201531887054 TRAIN  loss dict:  {'classification_loss': 1.0166201531887054}
2025-01-14 15:42:00,224 [INFO] Step[1650/2713]: training loss : 1.03749342918396 TRAIN  loss dict:  {'classification_loss': 1.03749342918396}
2025-01-14 15:42:14,286 [INFO] Step[1700/2713]: training loss : 1.0235953187942506 TRAIN  loss dict:  {'classification_loss': 1.0235953187942506}
2025-01-14 15:42:28,191 [INFO] Step[1750/2713]: training loss : 0.9953854846954345 TRAIN  loss dict:  {'classification_loss': 0.9953854846954345}
2025-01-14 15:42:41,934 [INFO] Step[1800/2713]: training loss : 1.0576128613948823 TRAIN  loss dict:  {'classification_loss': 1.0576128613948823}
2025-01-14 15:42:55,680 [INFO] Step[1850/2713]: training loss : 1.0633380913734436 TRAIN  loss dict:  {'classification_loss': 1.0633380913734436}
2025-01-14 15:43:09,251 [INFO] Step[1900/2713]: training loss : 1.014718647003174 TRAIN  loss dict:  {'classification_loss': 1.014718647003174}
2025-01-14 15:43:23,159 [INFO] Step[1950/2713]: training loss : 1.0574380767345428 TRAIN  loss dict:  {'classification_loss': 1.0574380767345428}
2025-01-14 15:43:36,429 [INFO] Step[2000/2713]: training loss : 1.0161563384532928 TRAIN  loss dict:  {'classification_loss': 1.0161563384532928}
2025-01-14 15:43:49,684 [INFO] Step[2050/2713]: training loss : 1.0259537518024444 TRAIN  loss dict:  {'classification_loss': 1.0259537518024444}
2025-01-14 15:44:02,916 [INFO] Step[2100/2713]: training loss : 1.0508519542217254 TRAIN  loss dict:  {'classification_loss': 1.0508519542217254}
2025-01-14 15:44:16,655 [INFO] Step[2150/2713]: training loss : 1.01377507686615 TRAIN  loss dict:  {'classification_loss': 1.01377507686615}
2025-01-14 15:44:30,601 [INFO] Step[2200/2713]: training loss : 1.0141090095043181 TRAIN  loss dict:  {'classification_loss': 1.0141090095043181}
2025-01-14 15:44:44,836 [INFO] Step[2250/2713]: training loss : 1.0154237389564513 TRAIN  loss dict:  {'classification_loss': 1.0154237389564513}
2025-01-14 15:44:58,822 [INFO] Step[2300/2713]: training loss : 1.0445314133167267 TRAIN  loss dict:  {'classification_loss': 1.0445314133167267}
2025-01-14 15:45:12,773 [INFO] Step[2350/2713]: training loss : 1.031376737356186 TRAIN  loss dict:  {'classification_loss': 1.031376737356186}
2025-01-14 15:45:26,523 [INFO] Step[2400/2713]: training loss : 1.089650752544403 TRAIN  loss dict:  {'classification_loss': 1.089650752544403}
2025-01-14 15:45:40,044 [INFO] Step[2450/2713]: training loss : 1.0458878922462462 TRAIN  loss dict:  {'classification_loss': 1.0458878922462462}
2025-01-14 15:45:53,914 [INFO] Step[2500/2713]: training loss : 0.9925834262371063 TRAIN  loss dict:  {'classification_loss': 0.9925834262371063}
2025-01-14 15:46:07,608 [INFO] Step[2550/2713]: training loss : 1.033898445367813 TRAIN  loss dict:  {'classification_loss': 1.033898445367813}
2025-01-14 15:46:21,120 [INFO] Step[2600/2713]: training loss : 1.021731197834015 TRAIN  loss dict:  {'classification_loss': 1.021731197834015}
2025-01-14 15:46:34,800 [INFO] Step[2650/2713]: training loss : 1.0415730679035187 TRAIN  loss dict:  {'classification_loss': 1.0415730679035187}
2025-01-14 15:46:48,319 [INFO] Step[2700/2713]: training loss : 1.0090321898460388 TRAIN  loss dict:  {'classification_loss': 1.0090321898460388}
2025-01-14 15:48:04,954 [INFO] Label accuracies statistics:
2025-01-14 15:48:04,954 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.5, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.25, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.25, 238: 1.0, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.5, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 0.75, 256: 0.5, 257: 1.0, 258: 1.0, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.25, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 1.0, 316: 0.25, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 1.0, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.25, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.25, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 15:48:07,314 [INFO] [19] TRAIN  loss: 1.0260718059065244 acc: 0.9851333087602899
2025-01-14 15:48:07,314 [INFO] [19] TRAIN  loss dict: {'classification_loss': 1.0260718059065244}
2025-01-14 15:48:07,315 [INFO] [19] VALIDATION loss: 1.813685903871866 VALIDATION acc: 0.7849529780564264
2025-01-14 15:48:07,315 [INFO] [19] VALIDATION loss dict: {'classification_loss': 1.813685903871866}
2025-01-14 15:48:07,315 [INFO] 
2025-01-14 15:48:25,511 [INFO] Step[50/2713]: training loss : 1.0116649961471558 TRAIN  loss dict:  {'classification_loss': 1.0116649961471558}
2025-01-14 15:48:38,752 [INFO] Step[100/2713]: training loss : 0.9936489593982697 TRAIN  loss dict:  {'classification_loss': 0.9936489593982697}
2025-01-14 15:48:52,426 [INFO] Step[150/2713]: training loss : 1.0288148522377014 TRAIN  loss dict:  {'classification_loss': 1.0288148522377014}
2025-01-14 15:49:06,144 [INFO] Step[200/2713]: training loss : 0.9931780707836151 TRAIN  loss dict:  {'classification_loss': 0.9931780707836151}
2025-01-14 15:49:20,047 [INFO] Step[250/2713]: training loss : 1.0281661331653595 TRAIN  loss dict:  {'classification_loss': 1.0281661331653595}
2025-01-14 15:49:33,397 [INFO] Step[300/2713]: training loss : 0.9975379633903504 TRAIN  loss dict:  {'classification_loss': 0.9975379633903504}
2025-01-14 15:49:46,885 [INFO] Step[350/2713]: training loss : 1.0423214113712311 TRAIN  loss dict:  {'classification_loss': 1.0423214113712311}
2025-01-14 15:50:00,470 [INFO] Step[400/2713]: training loss : 1.013816361427307 TRAIN  loss dict:  {'classification_loss': 1.013816361427307}
2025-01-14 15:50:14,067 [INFO] Step[450/2713]: training loss : 1.0066086840629578 TRAIN  loss dict:  {'classification_loss': 1.0066086840629578}
2025-01-14 15:50:27,738 [INFO] Step[500/2713]: training loss : 0.9927936744689941 TRAIN  loss dict:  {'classification_loss': 0.9927936744689941}
2025-01-14 15:50:41,051 [INFO] Step[550/2713]: training loss : 1.0165654289722443 TRAIN  loss dict:  {'classification_loss': 1.0165654289722443}
2025-01-14 15:50:54,819 [INFO] Step[600/2713]: training loss : 1.027477080821991 TRAIN  loss dict:  {'classification_loss': 1.027477080821991}
2025-01-14 15:51:08,382 [INFO] Step[650/2713]: training loss : 1.0238935887813567 TRAIN  loss dict:  {'classification_loss': 1.0238935887813567}
2025-01-14 15:51:22,587 [INFO] Step[700/2713]: training loss : 1.0386595797538758 TRAIN  loss dict:  {'classification_loss': 1.0386595797538758}
2025-01-14 15:51:36,481 [INFO] Step[750/2713]: training loss : 0.9898192286491394 TRAIN  loss dict:  {'classification_loss': 0.9898192286491394}
2025-01-14 15:51:50,543 [INFO] Step[800/2713]: training loss : 1.010839523077011 TRAIN  loss dict:  {'classification_loss': 1.010839523077011}
2025-01-14 15:52:04,195 [INFO] Step[850/2713]: training loss : 1.0091973876953124 TRAIN  loss dict:  {'classification_loss': 1.0091973876953124}
2025-01-14 15:52:18,109 [INFO] Step[900/2713]: training loss : 1.009772036075592 TRAIN  loss dict:  {'classification_loss': 1.009772036075592}
2025-01-14 15:52:31,455 [INFO] Step[950/2713]: training loss : 0.9890074384212494 TRAIN  loss dict:  {'classification_loss': 0.9890074384212494}
2025-01-14 15:52:44,928 [INFO] Step[1000/2713]: training loss : 0.9905913579463959 TRAIN  loss dict:  {'classification_loss': 0.9905913579463959}
2025-01-14 15:52:58,282 [INFO] Step[1050/2713]: training loss : 0.9977006840705872 TRAIN  loss dict:  {'classification_loss': 0.9977006840705872}
2025-01-14 15:53:11,858 [INFO] Step[1100/2713]: training loss : 0.9953424632549286 TRAIN  loss dict:  {'classification_loss': 0.9953424632549286}
2025-01-14 15:53:25,522 [INFO] Step[1150/2713]: training loss : 1.0178397762775422 TRAIN  loss dict:  {'classification_loss': 1.0178397762775422}
2025-01-14 15:53:39,075 [INFO] Step[1200/2713]: training loss : 1.029855329990387 TRAIN  loss dict:  {'classification_loss': 1.029855329990387}
2025-01-14 15:53:52,964 [INFO] Step[1250/2713]: training loss : 0.9893204164505005 TRAIN  loss dict:  {'classification_loss': 0.9893204164505005}
2025-01-14 15:54:06,594 [INFO] Step[1300/2713]: training loss : 1.0163928914070128 TRAIN  loss dict:  {'classification_loss': 1.0163928914070128}
2025-01-14 15:54:20,204 [INFO] Step[1350/2713]: training loss : 1.0010767352581025 TRAIN  loss dict:  {'classification_loss': 1.0010767352581025}
2025-01-14 15:54:33,764 [INFO] Step[1400/2713]: training loss : 1.0078090989589692 TRAIN  loss dict:  {'classification_loss': 1.0078090989589692}
2025-01-14 15:54:47,235 [INFO] Step[1450/2713]: training loss : 1.027459316253662 TRAIN  loss dict:  {'classification_loss': 1.027459316253662}
2025-01-14 15:55:00,839 [INFO] Step[1500/2713]: training loss : 1.0164652359485626 TRAIN  loss dict:  {'classification_loss': 1.0164652359485626}
2025-01-14 15:55:14,476 [INFO] Step[1550/2713]: training loss : 1.0695664298534393 TRAIN  loss dict:  {'classification_loss': 1.0695664298534393}
2025-01-14 15:55:28,070 [INFO] Step[1600/2713]: training loss : 0.9933649468421936 TRAIN  loss dict:  {'classification_loss': 0.9933649468421936}
2025-01-14 15:55:41,935 [INFO] Step[1650/2713]: training loss : 1.036299694776535 TRAIN  loss dict:  {'classification_loss': 1.036299694776535}
2025-01-14 15:55:56,073 [INFO] Step[1700/2713]: training loss : 1.0054064738750457 TRAIN  loss dict:  {'classification_loss': 1.0054064738750457}
2025-01-14 15:56:09,521 [INFO] Step[1750/2713]: training loss : 1.0012735450267791 TRAIN  loss dict:  {'classification_loss': 1.0012735450267791}
2025-01-14 15:56:23,191 [INFO] Step[1800/2713]: training loss : 1.0282606291770935 TRAIN  loss dict:  {'classification_loss': 1.0282606291770935}
2025-01-14 15:56:36,679 [INFO] Step[1850/2713]: training loss : 1.0107094240188599 TRAIN  loss dict:  {'classification_loss': 1.0107094240188599}
2025-01-14 15:56:50,292 [INFO] Step[1900/2713]: training loss : 1.010343976020813 TRAIN  loss dict:  {'classification_loss': 1.010343976020813}
2025-01-14 15:57:03,537 [INFO] Step[1950/2713]: training loss : 1.004145245552063 TRAIN  loss dict:  {'classification_loss': 1.004145245552063}
2025-01-14 15:57:17,083 [INFO] Step[2000/2713]: training loss : 1.0076517021656037 TRAIN  loss dict:  {'classification_loss': 1.0076517021656037}
2025-01-14 15:57:31,799 [INFO] Step[2050/2713]: training loss : 0.9872023475170135 TRAIN  loss dict:  {'classification_loss': 0.9872023475170135}
2025-01-14 15:57:46,907 [INFO] Step[2100/2713]: training loss : 1.0498806357383728 TRAIN  loss dict:  {'classification_loss': 1.0498806357383728}
2025-01-14 15:58:00,550 [INFO] Step[2150/2713]: training loss : 1.0114056468009949 TRAIN  loss dict:  {'classification_loss': 1.0114056468009949}
2025-01-14 15:58:13,969 [INFO] Step[2200/2713]: training loss : 1.0077957725524902 TRAIN  loss dict:  {'classification_loss': 1.0077957725524902}
2025-01-14 15:58:27,782 [INFO] Step[2250/2713]: training loss : 1.0148271572589875 TRAIN  loss dict:  {'classification_loss': 1.0148271572589875}
2025-01-14 15:58:41,431 [INFO] Step[2300/2713]: training loss : 1.0225848853588104 TRAIN  loss dict:  {'classification_loss': 1.0225848853588104}
2025-01-14 15:58:55,051 [INFO] Step[2350/2713]: training loss : 1.0001559126377106 TRAIN  loss dict:  {'classification_loss': 1.0001559126377106}
2025-01-14 15:59:08,721 [INFO] Step[2400/2713]: training loss : 1.0349624967575073 TRAIN  loss dict:  {'classification_loss': 1.0349624967575073}
2025-01-14 15:59:22,573 [INFO] Step[2450/2713]: training loss : 0.9922540628910065 TRAIN  loss dict:  {'classification_loss': 0.9922540628910065}
2025-01-14 15:59:35,768 [INFO] Step[2500/2713]: training loss : 1.1115379667282104 TRAIN  loss dict:  {'classification_loss': 1.1115379667282104}
2025-01-14 15:59:49,308 [INFO] Step[2550/2713]: training loss : 1.0087110376358033 TRAIN  loss dict:  {'classification_loss': 1.0087110376358033}
2025-01-14 16:00:03,242 [INFO] Step[2600/2713]: training loss : 1.0493854081630707 TRAIN  loss dict:  {'classification_loss': 1.0493854081630707}
2025-01-14 16:00:16,956 [INFO] Step[2650/2713]: training loss : 1.0596073806285857 TRAIN  loss dict:  {'classification_loss': 1.0596073806285857}
2025-01-14 16:00:30,154 [INFO] Step[2700/2713]: training loss : 1.0206517195701599 TRAIN  loss dict:  {'classification_loss': 1.0206517195701599}
2025-01-14 16:01:46,641 [INFO] Label accuracies statistics:
2025-01-14 16:01:46,641 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 0.75, 62: 1.0, 63: 0.0, 64: 1.0, 65: 1.0, 66: 0.0, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 0.75, 132: 0.5, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.5, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 0.75, 155: 0.75, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.0, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.5, 205: 0.75, 206: 0.25, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.5, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 0.5, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.0, 291: 0.75, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 1.0, 335: 0.75, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 16:01:46,643 [INFO] [20] TRAIN  loss: 1.0158685817298438 acc: 0.9891878609165745
2025-01-14 16:01:46,643 [INFO] [20] TRAIN  loss dict: {'classification_loss': 1.0158685817298438}
2025-01-14 16:01:46,643 [INFO] [20] VALIDATION loss: 1.9293239974885954 VALIDATION acc: 0.7617554858934169
2025-01-14 16:01:46,643 [INFO] [20] VALIDATION loss dict: {'classification_loss': 1.9293239974885954}
2025-01-14 16:01:46,643 [INFO] 
2025-01-14 16:02:05,411 [INFO] Step[50/2713]: training loss : 1.0180336892604829 TRAIN  loss dict:  {'classification_loss': 1.0180336892604829}
2025-01-14 16:02:18,693 [INFO] Step[100/2713]: training loss : 0.992458951473236 TRAIN  loss dict:  {'classification_loss': 0.992458951473236}
2025-01-14 16:02:32,555 [INFO] Step[150/2713]: training loss : 0.9904039931297303 TRAIN  loss dict:  {'classification_loss': 0.9904039931297303}
2025-01-14 16:02:45,830 [INFO] Step[200/2713]: training loss : 0.9945175230503083 TRAIN  loss dict:  {'classification_loss': 0.9945175230503083}
2025-01-14 16:02:59,848 [INFO] Step[250/2713]: training loss : 1.0160306537151336 TRAIN  loss dict:  {'classification_loss': 1.0160306537151336}
2025-01-14 16:03:14,145 [INFO] Step[300/2713]: training loss : 1.0001087367534638 TRAIN  loss dict:  {'classification_loss': 1.0001087367534638}
2025-01-14 16:03:27,662 [INFO] Step[350/2713]: training loss : 0.9851083481311798 TRAIN  loss dict:  {'classification_loss': 0.9851083481311798}
2025-01-14 16:03:41,387 [INFO] Step[400/2713]: training loss : 0.9788274884223938 TRAIN  loss dict:  {'classification_loss': 0.9788274884223938}
2025-01-14 16:03:55,201 [INFO] Step[450/2713]: training loss : 1.0008303284645081 TRAIN  loss dict:  {'classification_loss': 1.0008303284645081}
2025-01-14 16:04:11,984 [INFO] Step[500/2713]: training loss : 1.0023668444156646 TRAIN  loss dict:  {'classification_loss': 1.0023668444156646}
2025-01-14 16:04:26,873 [INFO] Step[550/2713]: training loss : 1.0186234176158906 TRAIN  loss dict:  {'classification_loss': 1.0186234176158906}
2025-01-14 16:04:40,162 [INFO] Step[600/2713]: training loss : 0.9972124087810517 TRAIN  loss dict:  {'classification_loss': 0.9972124087810517}
2025-01-14 16:04:53,881 [INFO] Step[650/2713]: training loss : 0.9953482294082642 TRAIN  loss dict:  {'classification_loss': 0.9953482294082642}
2025-01-14 16:05:07,513 [INFO] Step[700/2713]: training loss : 1.032185423374176 TRAIN  loss dict:  {'classification_loss': 1.032185423374176}
2025-01-14 16:05:21,764 [INFO] Step[750/2713]: training loss : 1.0124019980430603 TRAIN  loss dict:  {'classification_loss': 1.0124019980430603}
2025-01-14 16:05:35,983 [INFO] Step[800/2713]: training loss : 0.9928934907913208 TRAIN  loss dict:  {'classification_loss': 0.9928934907913208}
2025-01-14 16:05:49,916 [INFO] Step[850/2713]: training loss : 0.9852662014961243 TRAIN  loss dict:  {'classification_loss': 0.9852662014961243}
2025-01-14 16:06:03,586 [INFO] Step[900/2713]: training loss : 1.0266472756862641 TRAIN  loss dict:  {'classification_loss': 1.0266472756862641}
2025-01-14 16:06:17,342 [INFO] Step[950/2713]: training loss : 1.0138805508613586 TRAIN  loss dict:  {'classification_loss': 1.0138805508613586}
2025-01-14 16:06:31,377 [INFO] Step[1000/2713]: training loss : 1.0027894699573516 TRAIN  loss dict:  {'classification_loss': 1.0027894699573516}
2025-01-14 16:06:45,367 [INFO] Step[1050/2713]: training loss : 0.9855923783779145 TRAIN  loss dict:  {'classification_loss': 0.9855923783779145}
2025-01-14 16:06:58,640 [INFO] Step[1100/2713]: training loss : 0.98977912068367 TRAIN  loss dict:  {'classification_loss': 0.98977912068367}
2025-01-14 16:07:12,570 [INFO] Step[1150/2713]: training loss : 0.992632862329483 TRAIN  loss dict:  {'classification_loss': 0.992632862329483}
2025-01-14 16:07:26,741 [INFO] Step[1200/2713]: training loss : 0.9970799374580384 TRAIN  loss dict:  {'classification_loss': 0.9970799374580384}
2025-01-14 16:07:40,575 [INFO] Step[1250/2713]: training loss : 1.0132604217529297 TRAIN  loss dict:  {'classification_loss': 1.0132604217529297}
2025-01-14 16:07:54,828 [INFO] Step[1300/2713]: training loss : 1.0183118081092835 TRAIN  loss dict:  {'classification_loss': 1.0183118081092835}
2025-01-14 16:08:08,607 [INFO] Step[1350/2713]: training loss : 0.9888325929641724 TRAIN  loss dict:  {'classification_loss': 0.9888325929641724}
2025-01-14 16:08:21,859 [INFO] Step[1400/2713]: training loss : 1.0002179217338563 TRAIN  loss dict:  {'classification_loss': 1.0002179217338563}
2025-01-14 16:08:35,600 [INFO] Step[1450/2713]: training loss : 1.0091401660442352 TRAIN  loss dict:  {'classification_loss': 1.0091401660442352}
2025-01-14 16:08:49,471 [INFO] Step[1500/2713]: training loss : 1.0172631227970124 TRAIN  loss dict:  {'classification_loss': 1.0172631227970124}
2025-01-14 16:09:03,486 [INFO] Step[1550/2713]: training loss : 0.9919719052314758 TRAIN  loss dict:  {'classification_loss': 0.9919719052314758}
2025-01-14 16:09:17,768 [INFO] Step[1600/2713]: training loss : 1.0298073756694794 TRAIN  loss dict:  {'classification_loss': 1.0298073756694794}
2025-01-14 16:09:31,310 [INFO] Step[1650/2713]: training loss : 0.984188562631607 TRAIN  loss dict:  {'classification_loss': 0.984188562631607}
2025-01-14 16:09:45,210 [INFO] Step[1700/2713]: training loss : 1.0210020983219146 TRAIN  loss dict:  {'classification_loss': 1.0210020983219146}
2025-01-14 16:09:59,101 [INFO] Step[1750/2713]: training loss : 1.024441294670105 TRAIN  loss dict:  {'classification_loss': 1.024441294670105}
2025-01-14 16:10:12,915 [INFO] Step[1800/2713]: training loss : 1.0241285836696625 TRAIN  loss dict:  {'classification_loss': 1.0241285836696625}
2025-01-14 16:10:26,908 [INFO] Step[1850/2713]: training loss : 0.9936697459220887 TRAIN  loss dict:  {'classification_loss': 0.9936697459220887}
2025-01-14 16:10:40,366 [INFO] Step[1900/2713]: training loss : 0.9906463599205018 TRAIN  loss dict:  {'classification_loss': 0.9906463599205018}
2025-01-14 16:10:54,391 [INFO] Step[1950/2713]: training loss : 0.9987858498096466 TRAIN  loss dict:  {'classification_loss': 0.9987858498096466}
2025-01-14 16:11:08,019 [INFO] Step[2000/2713]: training loss : 0.9972101140022278 TRAIN  loss dict:  {'classification_loss': 0.9972101140022278}
2025-01-14 16:11:21,794 [INFO] Step[2050/2713]: training loss : 0.9857262206077576 TRAIN  loss dict:  {'classification_loss': 0.9857262206077576}
2025-01-14 16:11:35,734 [INFO] Step[2100/2713]: training loss : 1.0583324384689332 TRAIN  loss dict:  {'classification_loss': 1.0583324384689332}
2025-01-14 16:11:49,813 [INFO] Step[2150/2713]: training loss : 0.9901046001911163 TRAIN  loss dict:  {'classification_loss': 0.9901046001911163}
2025-01-14 16:12:03,131 [INFO] Step[2200/2713]: training loss : 1.0432143437862396 TRAIN  loss dict:  {'classification_loss': 1.0432143437862396}
2025-01-14 16:12:17,220 [INFO] Step[2250/2713]: training loss : 1.0082324981689452 TRAIN  loss dict:  {'classification_loss': 1.0082324981689452}
2025-01-14 16:12:30,495 [INFO] Step[2300/2713]: training loss : 1.0278295540809632 TRAIN  loss dict:  {'classification_loss': 1.0278295540809632}
2025-01-14 16:12:44,340 [INFO] Step[2350/2713]: training loss : 0.9876857435703278 TRAIN  loss dict:  {'classification_loss': 0.9876857435703278}
2025-01-14 16:12:58,000 [INFO] Step[2400/2713]: training loss : 1.0179398894309997 TRAIN  loss dict:  {'classification_loss': 1.0179398894309997}
2025-01-14 16:13:11,769 [INFO] Step[2450/2713]: training loss : 1.0046749246120452 TRAIN  loss dict:  {'classification_loss': 1.0046749246120452}
2025-01-14 16:13:25,431 [INFO] Step[2500/2713]: training loss : 0.9813946545124054 TRAIN  loss dict:  {'classification_loss': 0.9813946545124054}
2025-01-14 16:13:38,724 [INFO] Step[2550/2713]: training loss : 0.9743724071979523 TRAIN  loss dict:  {'classification_loss': 0.9743724071979523}
2025-01-14 16:13:52,010 [INFO] Step[2600/2713]: training loss : 1.0224114108085631 TRAIN  loss dict:  {'classification_loss': 1.0224114108085631}
2025-01-14 16:14:05,798 [INFO] Step[2650/2713]: training loss : 0.9714771986007691 TRAIN  loss dict:  {'classification_loss': 0.9714771986007691}
2025-01-14 16:14:19,804 [INFO] Step[2700/2713]: training loss : 1.0109204363822937 TRAIN  loss dict:  {'classification_loss': 1.0109204363822937}
2025-01-14 16:15:35,724 [INFO] Label accuracies statistics:
2025-01-14 16:15:35,725 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.25, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.25, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.5, 71: 0.75, 72: 0.75, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.3333333333333333, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.0, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.25, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.5, 210: 0.75, 211: 0.25, 212: 0.5, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.25, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 1.0, 339: 0.75, 340: 0.75, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 0.75, 392: 1.0, 393: 0.25, 394: 0.5, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 16:15:35,726 [INFO] [21] TRAIN  loss: 1.0038297310817563 acc: 0.990907973952574
2025-01-14 16:15:35,727 [INFO] [21] TRAIN  loss dict: {'classification_loss': 1.0038297310817563}
2025-01-14 16:15:35,727 [INFO] [21] VALIDATION loss: 1.832880088597312 VALIDATION acc: 0.7818181818181819
2025-01-14 16:15:35,727 [INFO] [21] VALIDATION loss dict: {'classification_loss': 1.832880088597312}
2025-01-14 16:15:35,727 [INFO] 
2025-01-14 16:15:54,762 [INFO] Step[50/2713]: training loss : 1.0039070773124694 TRAIN  loss dict:  {'classification_loss': 1.0039070773124694}
2025-01-14 16:16:08,358 [INFO] Step[100/2713]: training loss : 0.9846496725082398 TRAIN  loss dict:  {'classification_loss': 0.9846496725082398}
2025-01-14 16:16:22,333 [INFO] Step[150/2713]: training loss : 0.991080527305603 TRAIN  loss dict:  {'classification_loss': 0.991080527305603}
2025-01-14 16:16:35,591 [INFO] Step[200/2713]: training loss : 0.9837082898616791 TRAIN  loss dict:  {'classification_loss': 0.9837082898616791}
2025-01-14 16:16:49,213 [INFO] Step[250/2713]: training loss : 0.994429144859314 TRAIN  loss dict:  {'classification_loss': 0.994429144859314}
2025-01-14 16:17:02,863 [INFO] Step[300/2713]: training loss : 0.9884284830093384 TRAIN  loss dict:  {'classification_loss': 0.9884284830093384}
2025-01-14 16:17:16,432 [INFO] Step[350/2713]: training loss : 0.981884822845459 TRAIN  loss dict:  {'classification_loss': 0.981884822845459}
2025-01-14 16:17:29,715 [INFO] Step[400/2713]: training loss : 1.0020501518249512 TRAIN  loss dict:  {'classification_loss': 1.0020501518249512}
2025-01-14 16:17:43,021 [INFO] Step[450/2713]: training loss : 0.9751103901863098 TRAIN  loss dict:  {'classification_loss': 0.9751103901863098}
2025-01-14 16:17:56,294 [INFO] Step[500/2713]: training loss : 0.9936190378665924 TRAIN  loss dict:  {'classification_loss': 0.9936190378665924}
2025-01-14 16:18:09,796 [INFO] Step[550/2713]: training loss : 0.9986583173274994 TRAIN  loss dict:  {'classification_loss': 0.9986583173274994}
2025-01-14 16:18:23,688 [INFO] Step[600/2713]: training loss : 0.9871164846420288 TRAIN  loss dict:  {'classification_loss': 0.9871164846420288}
2025-01-14 16:18:37,847 [INFO] Step[650/2713]: training loss : 0.9728262889385223 TRAIN  loss dict:  {'classification_loss': 0.9728262889385223}
2025-01-14 16:18:51,682 [INFO] Step[700/2713]: training loss : 0.9709348702430725 TRAIN  loss dict:  {'classification_loss': 0.9709348702430725}
2025-01-14 16:19:05,349 [INFO] Step[750/2713]: training loss : 1.0190390527248383 TRAIN  loss dict:  {'classification_loss': 1.0190390527248383}
2025-01-14 16:19:18,554 [INFO] Step[800/2713]: training loss : 1.0025778090953827 TRAIN  loss dict:  {'classification_loss': 1.0025778090953827}
2025-01-14 16:19:32,137 [INFO] Step[850/2713]: training loss : 0.9943078470230102 TRAIN  loss dict:  {'classification_loss': 0.9943078470230102}
2025-01-14 16:19:45,729 [INFO] Step[900/2713]: training loss : 0.9941178250312805 TRAIN  loss dict:  {'classification_loss': 0.9941178250312805}
2025-01-14 16:19:59,029 [INFO] Step[950/2713]: training loss : 0.973795006275177 TRAIN  loss dict:  {'classification_loss': 0.973795006275177}
2025-01-14 16:20:12,782 [INFO] Step[1000/2713]: training loss : 0.9916928517818451 TRAIN  loss dict:  {'classification_loss': 0.9916928517818451}
2025-01-14 16:20:26,948 [INFO] Step[1050/2713]: training loss : 0.998090797662735 TRAIN  loss dict:  {'classification_loss': 0.998090797662735}
2025-01-14 16:20:40,420 [INFO] Step[1100/2713]: training loss : 1.0031193149089814 TRAIN  loss dict:  {'classification_loss': 1.0031193149089814}
2025-01-14 16:20:53,991 [INFO] Step[1150/2713]: training loss : 0.9762739896774292 TRAIN  loss dict:  {'classification_loss': 0.9762739896774292}
2025-01-14 16:21:07,287 [INFO] Step[1200/2713]: training loss : 1.0162681448459625 TRAIN  loss dict:  {'classification_loss': 1.0162681448459625}
2025-01-14 16:21:21,016 [INFO] Step[1250/2713]: training loss : 0.9873381638526917 TRAIN  loss dict:  {'classification_loss': 0.9873381638526917}
2025-01-14 16:21:34,831 [INFO] Step[1300/2713]: training loss : 0.9839741909503936 TRAIN  loss dict:  {'classification_loss': 0.9839741909503936}
2025-01-14 16:21:48,519 [INFO] Step[1350/2713]: training loss : 1.0394899046421051 TRAIN  loss dict:  {'classification_loss': 1.0394899046421051}
2025-01-14 16:22:02,167 [INFO] Step[1400/2713]: training loss : 0.9927257788181305 TRAIN  loss dict:  {'classification_loss': 0.9927257788181305}
2025-01-14 16:22:15,448 [INFO] Step[1450/2713]: training loss : 0.9916770315170288 TRAIN  loss dict:  {'classification_loss': 0.9916770315170288}
2025-01-14 16:22:29,026 [INFO] Step[1500/2713]: training loss : 1.0133341372013092 TRAIN  loss dict:  {'classification_loss': 1.0133341372013092}
2025-01-14 16:22:43,279 [INFO] Step[1550/2713]: training loss : 0.977480046749115 TRAIN  loss dict:  {'classification_loss': 0.977480046749115}
2025-01-14 16:22:56,571 [INFO] Step[1600/2713]: training loss : 0.9833437263965606 TRAIN  loss dict:  {'classification_loss': 0.9833437263965606}
2025-01-14 16:23:10,521 [INFO] Step[1650/2713]: training loss : 0.9908905065059662 TRAIN  loss dict:  {'classification_loss': 0.9908905065059662}
2025-01-14 16:23:24,367 [INFO] Step[1700/2713]: training loss : 0.9970548808574676 TRAIN  loss dict:  {'classification_loss': 0.9970548808574676}
2025-01-14 16:23:38,530 [INFO] Step[1750/2713]: training loss : 1.019545704126358 TRAIN  loss dict:  {'classification_loss': 1.019545704126358}
2025-01-14 16:23:52,257 [INFO] Step[1800/2713]: training loss : 0.9706629383563995 TRAIN  loss dict:  {'classification_loss': 0.9706629383563995}
2025-01-14 16:24:05,532 [INFO] Step[1850/2713]: training loss : 0.9722356104850769 TRAIN  loss dict:  {'classification_loss': 0.9722356104850769}
2025-01-14 16:24:19,579 [INFO] Step[1900/2713]: training loss : 0.9841705584526061 TRAIN  loss dict:  {'classification_loss': 0.9841705584526061}
2025-01-14 16:24:33,315 [INFO] Step[1950/2713]: training loss : 0.9888814532756806 TRAIN  loss dict:  {'classification_loss': 0.9888814532756806}
2025-01-14 16:24:47,186 [INFO] Step[2000/2713]: training loss : 0.9986933183670044 TRAIN  loss dict:  {'classification_loss': 0.9986933183670044}
2025-01-14 16:25:01,379 [INFO] Step[2050/2713]: training loss : 0.9729498445987701 TRAIN  loss dict:  {'classification_loss': 0.9729498445987701}
2025-01-14 16:25:14,895 [INFO] Step[2100/2713]: training loss : 0.9737482357025147 TRAIN  loss dict:  {'classification_loss': 0.9737482357025147}
2025-01-14 16:25:28,915 [INFO] Step[2150/2713]: training loss : 0.9742308664321899 TRAIN  loss dict:  {'classification_loss': 0.9742308664321899}
2025-01-14 16:25:42,614 [INFO] Step[2200/2713]: training loss : 1.00929936170578 TRAIN  loss dict:  {'classification_loss': 1.00929936170578}
2025-01-14 16:25:56,074 [INFO] Step[2250/2713]: training loss : 0.979967429637909 TRAIN  loss dict:  {'classification_loss': 0.979967429637909}
2025-01-14 16:26:10,230 [INFO] Step[2300/2713]: training loss : 1.0242465674877166 TRAIN  loss dict:  {'classification_loss': 1.0242465674877166}
2025-01-14 16:26:24,046 [INFO] Step[2350/2713]: training loss : 0.9761798501014709 TRAIN  loss dict:  {'classification_loss': 0.9761798501014709}
2025-01-14 16:26:38,266 [INFO] Step[2400/2713]: training loss : 0.980640218257904 TRAIN  loss dict:  {'classification_loss': 0.980640218257904}
2025-01-14 16:26:52,312 [INFO] Step[2450/2713]: training loss : 0.9951565945148468 TRAIN  loss dict:  {'classification_loss': 0.9951565945148468}
2025-01-14 16:27:06,075 [INFO] Step[2500/2713]: training loss : 0.9714392113685608 TRAIN  loss dict:  {'classification_loss': 0.9714392113685608}
2025-01-14 16:27:20,189 [INFO] Step[2550/2713]: training loss : 1.0262709558010101 TRAIN  loss dict:  {'classification_loss': 1.0262709558010101}
2025-01-14 16:27:34,311 [INFO] Step[2600/2713]: training loss : 1.0119416677951814 TRAIN  loss dict:  {'classification_loss': 1.0119416677951814}
2025-01-14 16:27:47,603 [INFO] Step[2650/2713]: training loss : 1.0142430639266968 TRAIN  loss dict:  {'classification_loss': 1.0142430639266968}
2025-01-14 16:28:01,144 [INFO] Step[2700/2713]: training loss : 0.9972512435913086 TRAIN  loss dict:  {'classification_loss': 0.9972512435913086}
2025-01-14 16:29:17,103 [INFO] Label accuracies statistics:
2025-01-14 16:29:17,103 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.5, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.75, 69: 1.0, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.5, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.5, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.25, 234: 0.25, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.5, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.5, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 0.75, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.25, 395: 0.75, 396: 1.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-14 16:29:17,105 [INFO] [22] TRAIN  loss: 0.9926261484425225 acc: 0.9923823565548593
2025-01-14 16:29:17,105 [INFO] [22] TRAIN  loss dict: {'classification_loss': 0.9926261484425225}
2025-01-14 16:29:17,105 [INFO] [22] VALIDATION loss: 1.85745609848571 VALIDATION acc: 0.7749216300940439
2025-01-14 16:29:17,105 [INFO] [22] VALIDATION loss dict: {'classification_loss': 1.85745609848571}
2025-01-14 16:29:17,105 [INFO] 
2025-01-14 16:29:35,840 [INFO] Step[50/2713]: training loss : 0.9984693646430969 TRAIN  loss dict:  {'classification_loss': 0.9984693646430969}
2025-01-14 16:29:49,480 [INFO] Step[100/2713]: training loss : 0.9768294334411621 TRAIN  loss dict:  {'classification_loss': 0.9768294334411621}
2025-01-14 16:30:03,500 [INFO] Step[150/2713]: training loss : 1.0348426854610444 TRAIN  loss dict:  {'classification_loss': 1.0348426854610444}
2025-01-14 16:30:17,838 [INFO] Step[200/2713]: training loss : 1.0042002403736114 TRAIN  loss dict:  {'classification_loss': 1.0042002403736114}
2025-01-14 16:30:31,758 [INFO] Step[250/2713]: training loss : 0.9784766244888305 TRAIN  loss dict:  {'classification_loss': 0.9784766244888305}
2025-01-14 16:30:45,008 [INFO] Step[300/2713]: training loss : 0.9757749259471893 TRAIN  loss dict:  {'classification_loss': 0.9757749259471893}
2025-01-14 16:30:58,713 [INFO] Step[350/2713]: training loss : 0.9867731201648712 TRAIN  loss dict:  {'classification_loss': 0.9867731201648712}
2025-01-14 16:31:12,660 [INFO] Step[400/2713]: training loss : 0.9900790691375733 TRAIN  loss dict:  {'classification_loss': 0.9900790691375733}
2025-01-14 16:31:26,300 [INFO] Step[450/2713]: training loss : 0.9763130617141723 TRAIN  loss dict:  {'classification_loss': 0.9763130617141723}
2025-01-14 16:31:39,941 [INFO] Step[500/2713]: training loss : 0.9617668604850769 TRAIN  loss dict:  {'classification_loss': 0.9617668604850769}
2025-01-14 16:31:53,585 [INFO] Step[550/2713]: training loss : 1.0029385709762573 TRAIN  loss dict:  {'classification_loss': 1.0029385709762573}
2025-01-14 16:32:07,442 [INFO] Step[600/2713]: training loss : 0.9688813889026642 TRAIN  loss dict:  {'classification_loss': 0.9688813889026642}
2025-01-14 16:32:23,892 [INFO] Step[650/2713]: training loss : 1.0019087493419647 TRAIN  loss dict:  {'classification_loss': 1.0019087493419647}
2025-01-14 16:32:38,721 [INFO] Step[700/2713]: training loss : 1.0058144652843475 TRAIN  loss dict:  {'classification_loss': 1.0058144652843475}
2025-01-14 16:32:52,121 [INFO] Step[750/2713]: training loss : 0.9642942869663238 TRAIN  loss dict:  {'classification_loss': 0.9642942869663238}
2025-01-14 16:33:05,909 [INFO] Step[800/2713]: training loss : 0.991876220703125 TRAIN  loss dict:  {'classification_loss': 0.991876220703125}
2025-01-14 16:33:19,201 [INFO] Step[850/2713]: training loss : 0.9953504931926728 TRAIN  loss dict:  {'classification_loss': 0.9953504931926728}
2025-01-14 16:33:32,467 [INFO] Step[900/2713]: training loss : 0.9786690282821655 TRAIN  loss dict:  {'classification_loss': 0.9786690282821655}
2025-01-14 16:33:46,089 [INFO] Step[950/2713]: training loss : 0.9704766702651978 TRAIN  loss dict:  {'classification_loss': 0.9704766702651978}
2025-01-14 16:34:00,391 [INFO] Step[1000/2713]: training loss : 0.9713973581790925 TRAIN  loss dict:  {'classification_loss': 0.9713973581790925}
2025-01-14 16:34:14,088 [INFO] Step[1050/2713]: training loss : 1.0024745345115662 TRAIN  loss dict:  {'classification_loss': 1.0024745345115662}
2025-01-14 16:34:27,729 [INFO] Step[1100/2713]: training loss : 1.0219245862960815 TRAIN  loss dict:  {'classification_loss': 1.0219245862960815}
2025-01-14 16:34:41,050 [INFO] Step[1150/2713]: training loss : 1.0072608280181885 TRAIN  loss dict:  {'classification_loss': 1.0072608280181885}
2025-01-14 16:34:54,762 [INFO] Step[1200/2713]: training loss : 1.0225050616264344 TRAIN  loss dict:  {'classification_loss': 1.0225050616264344}
2025-01-14 16:35:08,051 [INFO] Step[1250/2713]: training loss : 0.9847734081745148 TRAIN  loss dict:  {'classification_loss': 0.9847734081745148}
2025-01-14 16:35:21,390 [INFO] Step[1300/2713]: training loss : 0.9790551662445068 TRAIN  loss dict:  {'classification_loss': 0.9790551662445068}
2025-01-14 16:35:35,603 [INFO] Step[1350/2713]: training loss : 0.9670767164230347 TRAIN  loss dict:  {'classification_loss': 0.9670767164230347}
2025-01-14 16:35:48,881 [INFO] Step[1400/2713]: training loss : 0.9900322306156158 TRAIN  loss dict:  {'classification_loss': 0.9900322306156158}
2025-01-14 16:36:03,064 [INFO] Step[1450/2713]: training loss : 0.9723601174354554 TRAIN  loss dict:  {'classification_loss': 0.9723601174354554}
2025-01-14 16:36:18,880 [INFO] Step[1500/2713]: training loss : 0.9967154371738434 TRAIN  loss dict:  {'classification_loss': 0.9967154371738434}
2025-01-14 16:36:33,044 [INFO] Step[1550/2713]: training loss : 0.9758067202568054 TRAIN  loss dict:  {'classification_loss': 0.9758067202568054}
2025-01-14 16:36:46,655 [INFO] Step[1600/2713]: training loss : 0.9933739054203033 TRAIN  loss dict:  {'classification_loss': 0.9933739054203033}
2025-01-14 16:37:00,518 [INFO] Step[1650/2713]: training loss : 0.972479282617569 TRAIN  loss dict:  {'classification_loss': 0.972479282617569}
2025-01-14 16:37:14,081 [INFO] Step[1700/2713]: training loss : 1.0009396970272064 TRAIN  loss dict:  {'classification_loss': 1.0009396970272064}
2025-01-14 16:37:28,291 [INFO] Step[1750/2713]: training loss : 0.9736731600761414 TRAIN  loss dict:  {'classification_loss': 0.9736731600761414}
2025-01-14 16:37:42,180 [INFO] Step[1800/2713]: training loss : 0.9884703195095063 TRAIN  loss dict:  {'classification_loss': 0.9884703195095063}
2025-01-14 16:37:56,451 [INFO] Step[1850/2713]: training loss : 0.967792751789093 TRAIN  loss dict:  {'classification_loss': 0.967792751789093}
2025-01-14 16:38:10,728 [INFO] Step[1900/2713]: training loss : 0.9892468857765198 TRAIN  loss dict:  {'classification_loss': 0.9892468857765198}
2025-01-14 16:38:24,519 [INFO] Step[1950/2713]: training loss : 1.017635840177536 TRAIN  loss dict:  {'classification_loss': 1.017635840177536}
2025-01-14 16:38:38,494 [INFO] Step[2000/2713]: training loss : 0.9632629454135895 TRAIN  loss dict:  {'classification_loss': 0.9632629454135895}
2025-01-14 16:38:53,866 [INFO] Step[2050/2713]: training loss : 0.9760214209556579 TRAIN  loss dict:  {'classification_loss': 0.9760214209556579}
2025-01-14 16:39:09,471 [INFO] Step[2100/2713]: training loss : 0.9768241035938263 TRAIN  loss dict:  {'classification_loss': 0.9768241035938263}
2025-01-14 16:39:23,249 [INFO] Step[2150/2713]: training loss : 0.9738847744464875 TRAIN  loss dict:  {'classification_loss': 0.9738847744464875}
2025-01-14 16:39:36,551 [INFO] Step[2200/2713]: training loss : 1.009403657913208 TRAIN  loss dict:  {'classification_loss': 1.009403657913208}
2025-01-14 16:39:50,416 [INFO] Step[2250/2713]: training loss : 0.9792296695709228 TRAIN  loss dict:  {'classification_loss': 0.9792296695709228}
2025-01-14 16:40:04,571 [INFO] Step[2300/2713]: training loss : 1.0177669167518615 TRAIN  loss dict:  {'classification_loss': 1.0177669167518615}
2025-01-14 16:40:18,455 [INFO] Step[2350/2713]: training loss : 0.9783788895606995 TRAIN  loss dict:  {'classification_loss': 0.9783788895606995}
2025-01-14 16:40:32,726 [INFO] Step[2400/2713]: training loss : 0.9816861891746521 TRAIN  loss dict:  {'classification_loss': 0.9816861891746521}
2025-01-14 16:40:46,264 [INFO] Step[2450/2713]: training loss : 0.9835739731788635 TRAIN  loss dict:  {'classification_loss': 0.9835739731788635}
2025-01-14 16:41:00,088 [INFO] Step[2500/2713]: training loss : 1.0092396926879883 TRAIN  loss dict:  {'classification_loss': 1.0092396926879883}
2025-01-14 16:41:14,218 [INFO] Step[2550/2713]: training loss : 0.9761170709133148 TRAIN  loss dict:  {'classification_loss': 0.9761170709133148}
2025-01-14 16:41:27,996 [INFO] Step[2600/2713]: training loss : 1.0010797095298767 TRAIN  loss dict:  {'classification_loss': 1.0010797095298767}
2025-01-14 16:41:41,845 [INFO] Step[2650/2713]: training loss : 1.0029930329322816 TRAIN  loss dict:  {'classification_loss': 1.0029930329322816}
2025-01-14 16:41:55,834 [INFO] Step[2700/2713]: training loss : 1.0037609541416168 TRAIN  loss dict:  {'classification_loss': 1.0037609541416168}
2025-01-14 16:43:11,695 [INFO] Label accuracies statistics:
2025-01-14 16:43:11,695 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.5, 57: 0.5, 58: 1.0, 59: 1.0, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.75, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.5, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 0.5, 251: 1.0, 252: 1.0, 253: 0.5, 254: 0.75, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.75, 291: 0.75, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-14 16:43:14,184 [INFO] [23] TRAIN  loss: 0.9887223152318125 acc: 0.9938567391571446
2025-01-14 16:43:14,184 [INFO] [23] TRAIN  loss dict: {'classification_loss': 0.9887223152318125}
2025-01-14 16:43:14,184 [INFO] [23] VALIDATION loss: 1.8015217786668836 VALIDATION acc: 0.7924764890282132
2025-01-14 16:43:14,184 [INFO] [23] VALIDATION loss dict: {'classification_loss': 1.8015217786668836}
2025-01-14 16:43:14,184 [INFO] 
2025-01-14 16:43:32,150 [INFO] Step[50/2713]: training loss : 0.9747814130783081 TRAIN  loss dict:  {'classification_loss': 0.9747814130783081}
2025-01-14 16:43:45,615 [INFO] Step[100/2713]: training loss : 1.0040121161937714 TRAIN  loss dict:  {'classification_loss': 1.0040121161937714}
2025-01-14 16:43:59,266 [INFO] Step[150/2713]: training loss : 0.9809410440921783 TRAIN  loss dict:  {'classification_loss': 0.9809410440921783}
2025-01-14 16:44:13,266 [INFO] Step[200/2713]: training loss : 0.9840100514888763 TRAIN  loss dict:  {'classification_loss': 0.9840100514888763}
2025-01-14 16:44:26,597 [INFO] Step[250/2713]: training loss : 0.9740253686904907 TRAIN  loss dict:  {'classification_loss': 0.9740253686904907}
2025-01-14 16:44:40,096 [INFO] Step[300/2713]: training loss : 0.9922544276714325 TRAIN  loss dict:  {'classification_loss': 0.9922544276714325}
2025-01-14 16:44:53,390 [INFO] Step[350/2713]: training loss : 0.9642036092281342 TRAIN  loss dict:  {'classification_loss': 0.9642036092281342}
2025-01-14 16:45:06,855 [INFO] Step[400/2713]: training loss : 0.9681388568878174 TRAIN  loss dict:  {'classification_loss': 0.9681388568878174}
2025-01-14 16:45:20,413 [INFO] Step[450/2713]: training loss : 0.9906315410137176 TRAIN  loss dict:  {'classification_loss': 0.9906315410137176}
2025-01-14 16:45:34,120 [INFO] Step[500/2713]: training loss : 0.9725365364551544 TRAIN  loss dict:  {'classification_loss': 0.9725365364551544}
2025-01-14 16:45:47,962 [INFO] Step[550/2713]: training loss : 0.9720348381996154 TRAIN  loss dict:  {'classification_loss': 0.9720348381996154}
2025-01-14 16:46:01,388 [INFO] Step[600/2713]: training loss : 0.9765918660163879 TRAIN  loss dict:  {'classification_loss': 0.9765918660163879}
2025-01-14 16:46:14,952 [INFO] Step[650/2713]: training loss : 0.9845001316070556 TRAIN  loss dict:  {'classification_loss': 0.9845001316070556}
2025-01-14 16:46:28,398 [INFO] Step[700/2713]: training loss : 0.9755822014808655 TRAIN  loss dict:  {'classification_loss': 0.9755822014808655}
2025-01-14 16:46:42,357 [INFO] Step[750/2713]: training loss : 0.9800995254516601 TRAIN  loss dict:  {'classification_loss': 0.9800995254516601}
2025-01-14 16:46:55,870 [INFO] Step[800/2713]: training loss : 1.0170305383205414 TRAIN  loss dict:  {'classification_loss': 1.0170305383205414}
2025-01-14 16:47:09,717 [INFO] Step[850/2713]: training loss : 0.9722637498378753 TRAIN  loss dict:  {'classification_loss': 0.9722637498378753}
2025-01-14 16:47:23,611 [INFO] Step[900/2713]: training loss : 0.9912620389461517 TRAIN  loss dict:  {'classification_loss': 0.9912620389461517}
2025-01-14 16:47:37,272 [INFO] Step[950/2713]: training loss : 0.9799054157733917 TRAIN  loss dict:  {'classification_loss': 0.9799054157733917}
2025-01-14 16:47:51,500 [INFO] Step[1000/2713]: training loss : 0.9664866960048676 TRAIN  loss dict:  {'classification_loss': 0.9664866960048676}
2025-01-14 16:48:05,824 [INFO] Step[1050/2713]: training loss : 0.9882450771331787 TRAIN  loss dict:  {'classification_loss': 0.9882450771331787}
2025-01-14 16:48:20,011 [INFO] Step[1100/2713]: training loss : 0.9814731538295746 TRAIN  loss dict:  {'classification_loss': 0.9814731538295746}
2025-01-14 16:48:33,295 [INFO] Step[1150/2713]: training loss : 1.0099303233623504 TRAIN  loss dict:  {'classification_loss': 1.0099303233623504}
2025-01-14 16:48:46,781 [INFO] Step[1200/2713]: training loss : 0.9688426530361176 TRAIN  loss dict:  {'classification_loss': 0.9688426530361176}
2025-01-14 16:49:00,567 [INFO] Step[1250/2713]: training loss : 0.9713400757312775 TRAIN  loss dict:  {'classification_loss': 0.9713400757312775}
2025-01-14 16:49:14,556 [INFO] Step[1300/2713]: training loss : 1.0075542151927948 TRAIN  loss dict:  {'classification_loss': 1.0075542151927948}
2025-01-14 16:49:28,191 [INFO] Step[1350/2713]: training loss : 0.9793164229393005 TRAIN  loss dict:  {'classification_loss': 0.9793164229393005}
2025-01-14 16:49:41,604 [INFO] Step[1400/2713]: training loss : 0.9801021420955658 TRAIN  loss dict:  {'classification_loss': 0.9801021420955658}
2025-01-14 16:49:55,285 [INFO] Step[1450/2713]: training loss : 1.0009713113307952 TRAIN  loss dict:  {'classification_loss': 1.0009713113307952}
2025-01-14 16:50:09,536 [INFO] Step[1500/2713]: training loss : 0.9703836631774903 TRAIN  loss dict:  {'classification_loss': 0.9703836631774903}
2025-01-14 16:50:23,535 [INFO] Step[1550/2713]: training loss : 0.9873650109767914 TRAIN  loss dict:  {'classification_loss': 0.9873650109767914}
2025-01-14 16:50:37,526 [INFO] Step[1600/2713]: training loss : 1.0107430350780486 TRAIN  loss dict:  {'classification_loss': 1.0107430350780486}
2025-01-14 16:50:51,518 [INFO] Step[1650/2713]: training loss : 0.971268697977066 TRAIN  loss dict:  {'classification_loss': 0.971268697977066}
2025-01-14 16:51:05,189 [INFO] Step[1700/2713]: training loss : 0.9811475694179534 TRAIN  loss dict:  {'classification_loss': 0.9811475694179534}
2025-01-14 16:51:19,243 [INFO] Step[1750/2713]: training loss : 0.97819366812706 TRAIN  loss dict:  {'classification_loss': 0.97819366812706}
2025-01-14 16:51:33,206 [INFO] Step[1800/2713]: training loss : 0.9742842531204223 TRAIN  loss dict:  {'classification_loss': 0.9742842531204223}
2025-01-14 16:51:47,299 [INFO] Step[1850/2713]: training loss : 1.0163644897937774 TRAIN  loss dict:  {'classification_loss': 1.0163644897937774}
2025-01-14 16:52:00,746 [INFO] Step[1900/2713]: training loss : 1.0042665922641754 TRAIN  loss dict:  {'classification_loss': 1.0042665922641754}
2025-01-14 16:52:14,006 [INFO] Step[1950/2713]: training loss : 0.9722551596164704 TRAIN  loss dict:  {'classification_loss': 0.9722551596164704}
2025-01-14 16:52:27,267 [INFO] Step[2000/2713]: training loss : 1.006532951593399 TRAIN  loss dict:  {'classification_loss': 1.006532951593399}
2025-01-14 16:52:41,200 [INFO] Step[2050/2713]: training loss : 0.9728991234302521 TRAIN  loss dict:  {'classification_loss': 0.9728991234302521}
2025-01-14 16:52:54,517 [INFO] Step[2100/2713]: training loss : 0.9845075881481171 TRAIN  loss dict:  {'classification_loss': 0.9845075881481171}
2025-01-14 16:53:08,588 [INFO] Step[2150/2713]: training loss : 0.9822909724712372 TRAIN  loss dict:  {'classification_loss': 0.9822909724712372}
2025-01-14 16:53:22,283 [INFO] Step[2200/2713]: training loss : 0.9799244558811188 TRAIN  loss dict:  {'classification_loss': 0.9799244558811188}
2025-01-14 16:53:36,086 [INFO] Step[2250/2713]: training loss : 0.9816225028038025 TRAIN  loss dict:  {'classification_loss': 0.9816225028038025}
2025-01-14 16:53:50,179 [INFO] Step[2300/2713]: training loss : 0.9681819152832031 TRAIN  loss dict:  {'classification_loss': 0.9681819152832031}
2025-01-14 16:54:04,170 [INFO] Step[2350/2713]: training loss : 0.9761789238452911 TRAIN  loss dict:  {'classification_loss': 0.9761789238452911}
2025-01-14 16:54:18,129 [INFO] Step[2400/2713]: training loss : 1.008846904039383 TRAIN  loss dict:  {'classification_loss': 1.008846904039383}
2025-01-14 16:54:31,637 [INFO] Step[2450/2713]: training loss : 0.9692603266239166 TRAIN  loss dict:  {'classification_loss': 0.9692603266239166}
2025-01-14 16:54:45,545 [INFO] Step[2500/2713]: training loss : 0.9964341282844543 TRAIN  loss dict:  {'classification_loss': 0.9964341282844543}
2025-01-14 16:54:59,771 [INFO] Step[2550/2713]: training loss : 0.9858153557777405 TRAIN  loss dict:  {'classification_loss': 0.9858153557777405}
2025-01-14 16:55:13,485 [INFO] Step[2600/2713]: training loss : 0.9904445695877075 TRAIN  loss dict:  {'classification_loss': 0.9904445695877075}
2025-01-14 16:55:27,312 [INFO] Step[2650/2713]: training loss : 0.99114914894104 TRAIN  loss dict:  {'classification_loss': 0.99114914894104}
2025-01-14 16:55:41,280 [INFO] Step[2700/2713]: training loss : 0.9808509683609009 TRAIN  loss dict:  {'classification_loss': 0.9808509683609009}
2025-01-14 16:56:57,318 [INFO] Label accuracies statistics:
2025-01-14 16:56:57,318 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.5, 69: 1.0, 70: 0.5, 71: 0.75, 72: 0.5, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.5, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.25, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.75, 225: 1.0, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 0.75, 242: 0.0, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 1.0, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.5, 331: 1.0, 332: 0.75, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.5, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.5, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 16:56:57,320 [INFO] [24] TRAIN  loss: 0.9842540191145256 acc: 0.9942253348077159
2025-01-14 16:56:57,320 [INFO] [24] TRAIN  loss dict: {'classification_loss': 0.9842540191145256}
2025-01-14 16:56:57,320 [INFO] [24] VALIDATION loss: 1.8179299488551635 VALIDATION acc: 0.7924764890282132
2025-01-14 16:56:57,320 [INFO] [24] VALIDATION loss dict: {'classification_loss': 1.8179299488551635}
2025-01-14 16:56:57,321 [INFO] 
2025-01-14 16:57:15,519 [INFO] Step[50/2713]: training loss : 1.0022853195667267 TRAIN  loss dict:  {'classification_loss': 1.0022853195667267}
2025-01-14 16:57:28,882 [INFO] Step[100/2713]: training loss : 1.0141609811782837 TRAIN  loss dict:  {'classification_loss': 1.0141609811782837}
2025-01-14 16:57:42,167 [INFO] Step[150/2713]: training loss : 0.9706958460807801 TRAIN  loss dict:  {'classification_loss': 0.9706958460807801}
2025-01-14 16:57:55,694 [INFO] Step[200/2713]: training loss : 0.9697382223606109 TRAIN  loss dict:  {'classification_loss': 0.9697382223606109}
2025-01-14 16:58:09,550 [INFO] Step[250/2713]: training loss : 0.9642002689838409 TRAIN  loss dict:  {'classification_loss': 0.9642002689838409}
2025-01-14 16:58:23,334 [INFO] Step[300/2713]: training loss : 0.9735237276554107 TRAIN  loss dict:  {'classification_loss': 0.9735237276554107}
2025-01-14 16:58:37,089 [INFO] Step[350/2713]: training loss : 0.976837604045868 TRAIN  loss dict:  {'classification_loss': 0.976837604045868}
2025-01-14 16:58:50,999 [INFO] Step[400/2713]: training loss : 0.9765569067001343 TRAIN  loss dict:  {'classification_loss': 0.9765569067001343}
2025-01-14 16:59:04,610 [INFO] Step[450/2713]: training loss : 0.9870359718799591 TRAIN  loss dict:  {'classification_loss': 0.9870359718799591}
2025-01-14 16:59:18,417 [INFO] Step[500/2713]: training loss : 0.9802554321289062 TRAIN  loss dict:  {'classification_loss': 0.9802554321289062}
2025-01-14 16:59:32,442 [INFO] Step[550/2713]: training loss : 0.9841083669662476 TRAIN  loss dict:  {'classification_loss': 0.9841083669662476}
2025-01-14 16:59:45,801 [INFO] Step[600/2713]: training loss : 0.973161119222641 TRAIN  loss dict:  {'classification_loss': 0.973161119222641}
2025-01-14 16:59:59,931 [INFO] Step[650/2713]: training loss : 0.9716672563552856 TRAIN  loss dict:  {'classification_loss': 0.9716672563552856}
2025-01-14 17:00:13,318 [INFO] Step[700/2713]: training loss : 0.9677227187156677 TRAIN  loss dict:  {'classification_loss': 0.9677227187156677}
2025-01-14 17:00:26,697 [INFO] Step[750/2713]: training loss : 0.9863458752632142 TRAIN  loss dict:  {'classification_loss': 0.9863458752632142}
2025-01-14 17:00:40,193 [INFO] Step[800/2713]: training loss : 0.9689898943901062 TRAIN  loss dict:  {'classification_loss': 0.9689898943901062}
2025-01-14 17:00:54,199 [INFO] Step[850/2713]: training loss : 0.9766369676589965 TRAIN  loss dict:  {'classification_loss': 0.9766369676589965}
2025-01-14 17:01:07,723 [INFO] Step[900/2713]: training loss : 0.9980930006504058 TRAIN  loss dict:  {'classification_loss': 0.9980930006504058}
2025-01-14 17:01:21,343 [INFO] Step[950/2713]: training loss : 0.9922813749313355 TRAIN  loss dict:  {'classification_loss': 0.9922813749313355}
2025-01-14 17:01:34,929 [INFO] Step[1000/2713]: training loss : 0.9826308965682984 TRAIN  loss dict:  {'classification_loss': 0.9826308965682984}
2025-01-14 17:01:48,899 [INFO] Step[1050/2713]: training loss : 0.9626665902137757 TRAIN  loss dict:  {'classification_loss': 0.9626665902137757}
2025-01-14 17:02:02,916 [INFO] Step[1100/2713]: training loss : 0.9684008646011353 TRAIN  loss dict:  {'classification_loss': 0.9684008646011353}
2025-01-14 17:02:16,758 [INFO] Step[1150/2713]: training loss : 0.9881201338768005 TRAIN  loss dict:  {'classification_loss': 0.9881201338768005}
2025-01-14 17:02:30,384 [INFO] Step[1200/2713]: training loss : 0.973768310546875 TRAIN  loss dict:  {'classification_loss': 0.973768310546875}
2025-01-14 17:02:43,579 [INFO] Step[1250/2713]: training loss : 1.0112947630882263 TRAIN  loss dict:  {'classification_loss': 1.0112947630882263}
2025-01-14 17:02:57,167 [INFO] Step[1300/2713]: training loss : 0.9902386856079102 TRAIN  loss dict:  {'classification_loss': 0.9902386856079102}
2025-01-14 17:03:11,112 [INFO] Step[1350/2713]: training loss : 0.9739153051376342 TRAIN  loss dict:  {'classification_loss': 0.9739153051376342}
2025-01-14 17:03:24,474 [INFO] Step[1400/2713]: training loss : 1.0148045110702515 TRAIN  loss dict:  {'classification_loss': 1.0148045110702515}
2025-01-14 17:03:38,288 [INFO] Step[1450/2713]: training loss : 0.9856137537956238 TRAIN  loss dict:  {'classification_loss': 0.9856137537956238}
2025-01-14 17:03:52,020 [INFO] Step[1500/2713]: training loss : 1.0180262303352356 TRAIN  loss dict:  {'classification_loss': 1.0180262303352356}
2025-01-14 17:04:05,576 [INFO] Step[1550/2713]: training loss : 0.982167261838913 TRAIN  loss dict:  {'classification_loss': 0.982167261838913}
2025-01-14 17:04:19,166 [INFO] Step[1600/2713]: training loss : 0.9842995548248291 TRAIN  loss dict:  {'classification_loss': 0.9842995548248291}
2025-01-14 17:04:33,380 [INFO] Step[1650/2713]: training loss : 0.9757470393180847 TRAIN  loss dict:  {'classification_loss': 0.9757470393180847}
2025-01-14 17:04:48,134 [INFO] Step[1700/2713]: training loss : 0.9944602489471436 TRAIN  loss dict:  {'classification_loss': 0.9944602489471436}
2025-01-14 17:05:03,229 [INFO] Step[1750/2713]: training loss : 0.9837496793270111 TRAIN  loss dict:  {'classification_loss': 0.9837496793270111}
2025-01-14 17:05:17,962 [INFO] Step[1800/2713]: training loss : 0.9928045344352722 TRAIN  loss dict:  {'classification_loss': 0.9928045344352722}
2025-01-14 17:05:33,582 [INFO] Step[1850/2713]: training loss : 0.9667885863780975 TRAIN  loss dict:  {'classification_loss': 0.9667885863780975}
2025-01-14 17:05:47,722 [INFO] Step[1900/2713]: training loss : 0.9773975610733032 TRAIN  loss dict:  {'classification_loss': 0.9773975610733032}
2025-01-14 17:06:01,421 [INFO] Step[1950/2713]: training loss : 0.975070571899414 TRAIN  loss dict:  {'classification_loss': 0.975070571899414}
2025-01-14 17:06:15,532 [INFO] Step[2000/2713]: training loss : 0.9723702037334442 TRAIN  loss dict:  {'classification_loss': 0.9723702037334442}
2025-01-14 17:06:29,437 [INFO] Step[2050/2713]: training loss : 0.9793695521354675 TRAIN  loss dict:  {'classification_loss': 0.9793695521354675}
2025-01-14 17:06:43,035 [INFO] Step[2100/2713]: training loss : 0.9755841159820556 TRAIN  loss dict:  {'classification_loss': 0.9755841159820556}
2025-01-14 17:06:56,795 [INFO] Step[2150/2713]: training loss : 0.9666578269004822 TRAIN  loss dict:  {'classification_loss': 0.9666578269004822}
2025-01-14 17:07:09,987 [INFO] Step[2200/2713]: training loss : 1.003083381652832 TRAIN  loss dict:  {'classification_loss': 1.003083381652832}
2025-01-14 17:07:24,031 [INFO] Step[2250/2713]: training loss : 0.9645124685764312 TRAIN  loss dict:  {'classification_loss': 0.9645124685764312}
2025-01-14 17:07:37,221 [INFO] Step[2300/2713]: training loss : 0.9911261832714081 TRAIN  loss dict:  {'classification_loss': 0.9911261832714081}
2025-01-14 17:07:50,395 [INFO] Step[2350/2713]: training loss : 0.9832234025001526 TRAIN  loss dict:  {'classification_loss': 0.9832234025001526}
2025-01-14 17:08:04,019 [INFO] Step[2400/2713]: training loss : 0.9642410242557525 TRAIN  loss dict:  {'classification_loss': 0.9642410242557525}
2025-01-14 17:08:17,921 [INFO] Step[2450/2713]: training loss : 0.9697181236743927 TRAIN  loss dict:  {'classification_loss': 0.9697181236743927}
2025-01-14 17:08:31,152 [INFO] Step[2500/2713]: training loss : 0.9794066441059113 TRAIN  loss dict:  {'classification_loss': 0.9794066441059113}
2025-01-14 17:08:44,778 [INFO] Step[2550/2713]: training loss : 0.9678341329097748 TRAIN  loss dict:  {'classification_loss': 0.9678341329097748}
2025-01-14 17:08:58,633 [INFO] Step[2600/2713]: training loss : 0.9812871432304382 TRAIN  loss dict:  {'classification_loss': 0.9812871432304382}
2025-01-14 17:09:12,423 [INFO] Step[2650/2713]: training loss : 0.9662093377113342 TRAIN  loss dict:  {'classification_loss': 0.9662093377113342}
2025-01-14 17:09:26,051 [INFO] Step[2700/2713]: training loss : 0.9661899363994598 TRAIN  loss dict:  {'classification_loss': 0.9661899363994598}
2025-01-14 17:10:42,569 [INFO] Label accuracies statistics:
2025-01-14 17:10:42,569 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.5, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.25, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.5, 244: 1.0, 245: 0.5, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.5, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 17:10:42,571 [INFO] [25] TRAIN  loss: 0.9810065249213863 acc: 0.994348200024573
2025-01-14 17:10:42,571 [INFO] [25] TRAIN  loss dict: {'classification_loss': 0.9810065249213863}
2025-01-14 17:10:42,571 [INFO] [25] VALIDATION loss: 1.8399484410769957 VALIDATION acc: 0.780564263322884
2025-01-14 17:10:42,571 [INFO] [25] VALIDATION loss dict: {'classification_loss': 1.8399484410769957}
2025-01-14 17:10:42,571 [INFO] 
2025-01-14 17:11:00,759 [INFO] Step[50/2713]: training loss : 0.9660570514202118 TRAIN  loss dict:  {'classification_loss': 0.9660570514202118}
2025-01-14 17:11:14,188 [INFO] Step[100/2713]: training loss : 0.9808371722698211 TRAIN  loss dict:  {'classification_loss': 0.9808371722698211}
2025-01-14 17:11:27,499 [INFO] Step[150/2713]: training loss : 0.9771135342121124 TRAIN  loss dict:  {'classification_loss': 0.9771135342121124}
2025-01-14 17:11:41,068 [INFO] Step[200/2713]: training loss : 0.9776137661933899 TRAIN  loss dict:  {'classification_loss': 0.9776137661933899}
2025-01-14 17:11:55,071 [INFO] Step[250/2713]: training loss : 0.9736474180221557 TRAIN  loss dict:  {'classification_loss': 0.9736474180221557}
2025-01-14 17:12:08,911 [INFO] Step[300/2713]: training loss : 0.9712749969959259 TRAIN  loss dict:  {'classification_loss': 0.9712749969959259}
2025-01-14 17:12:22,588 [INFO] Step[350/2713]: training loss : 0.9814509522914886 TRAIN  loss dict:  {'classification_loss': 0.9814509522914886}
2025-01-14 17:12:36,492 [INFO] Step[400/2713]: training loss : 0.9645717048645019 TRAIN  loss dict:  {'classification_loss': 0.9645717048645019}
2025-01-14 17:12:49,868 [INFO] Step[450/2713]: training loss : 0.9768783390522003 TRAIN  loss dict:  {'classification_loss': 0.9768783390522003}
2025-01-14 17:13:04,040 [INFO] Step[500/2713]: training loss : 0.9981630051136017 TRAIN  loss dict:  {'classification_loss': 0.9981630051136017}
2025-01-14 17:13:17,370 [INFO] Step[550/2713]: training loss : 0.9646006631851196 TRAIN  loss dict:  {'classification_loss': 0.9646006631851196}
2025-01-14 17:13:31,336 [INFO] Step[600/2713]: training loss : 0.9778800594806671 TRAIN  loss dict:  {'classification_loss': 0.9778800594806671}
2025-01-14 17:13:44,922 [INFO] Step[650/2713]: training loss : 0.9849269354343414 TRAIN  loss dict:  {'classification_loss': 0.9849269354343414}
2025-01-14 17:13:58,543 [INFO] Step[700/2713]: training loss : 0.9749662482738495 TRAIN  loss dict:  {'classification_loss': 0.9749662482738495}
2025-01-14 17:14:12,055 [INFO] Step[750/2713]: training loss : 0.9636690545082093 TRAIN  loss dict:  {'classification_loss': 0.9636690545082093}
2025-01-14 17:14:25,436 [INFO] Step[800/2713]: training loss : 0.9835153985023498 TRAIN  loss dict:  {'classification_loss': 0.9835153985023498}
2025-01-14 17:14:39,242 [INFO] Step[850/2713]: training loss : 0.9679021179676056 TRAIN  loss dict:  {'classification_loss': 0.9679021179676056}
2025-01-14 17:14:52,936 [INFO] Step[900/2713]: training loss : 0.9740068924427032 TRAIN  loss dict:  {'classification_loss': 0.9740068924427032}
2025-01-14 17:15:06,568 [INFO] Step[950/2713]: training loss : 0.9709061920642853 TRAIN  loss dict:  {'classification_loss': 0.9709061920642853}
2025-01-14 17:15:20,345 [INFO] Step[1000/2713]: training loss : 0.9986731624603271 TRAIN  loss dict:  {'classification_loss': 0.9986731624603271}
2025-01-14 17:15:34,048 [INFO] Step[1050/2713]: training loss : 1.0030088603496552 TRAIN  loss dict:  {'classification_loss': 1.0030088603496552}
2025-01-14 17:15:47,791 [INFO] Step[1100/2713]: training loss : 0.9795741879940033 TRAIN  loss dict:  {'classification_loss': 0.9795741879940033}
2025-01-14 17:16:02,256 [INFO] Step[1150/2713]: training loss : 0.9834578633308411 TRAIN  loss dict:  {'classification_loss': 0.9834578633308411}
2025-01-14 17:16:16,397 [INFO] Step[1200/2713]: training loss : 0.9609938442707062 TRAIN  loss dict:  {'classification_loss': 0.9609938442707062}
2025-01-14 17:16:29,927 [INFO] Step[1250/2713]: training loss : 0.980677627325058 TRAIN  loss dict:  {'classification_loss': 0.980677627325058}
2025-01-14 17:16:44,094 [INFO] Step[1300/2713]: training loss : 0.9966793096065522 TRAIN  loss dict:  {'classification_loss': 0.9966793096065522}
2025-01-14 17:16:58,233 [INFO] Step[1350/2713]: training loss : 0.9730392789840698 TRAIN  loss dict:  {'classification_loss': 0.9730392789840698}
2025-01-14 17:17:11,736 [INFO] Step[1400/2713]: training loss : 1.0347061157226562 TRAIN  loss dict:  {'classification_loss': 1.0347061157226562}
2025-01-14 17:17:24,885 [INFO] Step[1450/2713]: training loss : 0.9958803403377533 TRAIN  loss dict:  {'classification_loss': 0.9958803403377533}
2025-01-14 17:17:38,449 [INFO] Step[1500/2713]: training loss : 0.9638978326320649 TRAIN  loss dict:  {'classification_loss': 0.9638978326320649}
2025-01-14 17:17:52,083 [INFO] Step[1550/2713]: training loss : 0.9975174975395202 TRAIN  loss dict:  {'classification_loss': 0.9975174975395202}
2025-01-14 17:18:05,853 [INFO] Step[1600/2713]: training loss : 1.014534410238266 TRAIN  loss dict:  {'classification_loss': 1.014534410238266}
2025-01-14 17:18:19,323 [INFO] Step[1650/2713]: training loss : 1.0234990572929383 TRAIN  loss dict:  {'classification_loss': 1.0234990572929383}
2025-01-14 17:18:33,156 [INFO] Step[1700/2713]: training loss : 0.9881804132461548 TRAIN  loss dict:  {'classification_loss': 0.9881804132461548}
2025-01-14 17:18:46,667 [INFO] Step[1750/2713]: training loss : 0.9815975379943848 TRAIN  loss dict:  {'classification_loss': 0.9815975379943848}
2025-01-14 17:19:00,255 [INFO] Step[1800/2713]: training loss : 0.9856544423103333 TRAIN  loss dict:  {'classification_loss': 0.9856544423103333}
2025-01-14 17:19:13,802 [INFO] Step[1850/2713]: training loss : 0.9651220428943634 TRAIN  loss dict:  {'classification_loss': 0.9651220428943634}
2025-01-14 17:19:27,473 [INFO] Step[1900/2713]: training loss : 0.9704162812232972 TRAIN  loss dict:  {'classification_loss': 0.9704162812232972}
2025-01-14 17:19:41,008 [INFO] Step[1950/2713]: training loss : 0.9864311814308167 TRAIN  loss dict:  {'classification_loss': 0.9864311814308167}
2025-01-14 17:19:54,883 [INFO] Step[2000/2713]: training loss : 0.9708144783973693 TRAIN  loss dict:  {'classification_loss': 0.9708144783973693}
2025-01-14 17:20:08,280 [INFO] Step[2050/2713]: training loss : 0.9767951190471649 TRAIN  loss dict:  {'classification_loss': 0.9767951190471649}
2025-01-14 17:20:22,058 [INFO] Step[2100/2713]: training loss : 0.9683826196193696 TRAIN  loss dict:  {'classification_loss': 0.9683826196193696}
2025-01-14 17:20:35,534 [INFO] Step[2150/2713]: training loss : 0.9940086853504181 TRAIN  loss dict:  {'classification_loss': 0.9940086853504181}
2025-01-14 17:20:48,862 [INFO] Step[2200/2713]: training loss : 0.9817287015914917 TRAIN  loss dict:  {'classification_loss': 0.9817287015914917}
2025-01-14 17:21:02,456 [INFO] Step[2250/2713]: training loss : 0.9809143602848053 TRAIN  loss dict:  {'classification_loss': 0.9809143602848053}
2025-01-14 17:21:16,340 [INFO] Step[2300/2713]: training loss : 0.9755675089359284 TRAIN  loss dict:  {'classification_loss': 0.9755675089359284}
2025-01-14 17:21:30,448 [INFO] Step[2350/2713]: training loss : 0.9768229067325592 TRAIN  loss dict:  {'classification_loss': 0.9768229067325592}
2025-01-14 17:21:44,257 [INFO] Step[2400/2713]: training loss : 0.9717270827293396 TRAIN  loss dict:  {'classification_loss': 0.9717270827293396}
2025-01-14 17:21:58,036 [INFO] Step[2450/2713]: training loss : 0.9687166249752045 TRAIN  loss dict:  {'classification_loss': 0.9687166249752045}
2025-01-14 17:22:11,450 [INFO] Step[2500/2713]: training loss : 0.9886296188831329 TRAIN  loss dict:  {'classification_loss': 0.9886296188831329}
2025-01-14 17:22:24,605 [INFO] Step[2550/2713]: training loss : 1.0362969315052033 TRAIN  loss dict:  {'classification_loss': 1.0362969315052033}
2025-01-14 17:22:37,742 [INFO] Step[2600/2713]: training loss : 0.9623412454128265 TRAIN  loss dict:  {'classification_loss': 0.9623412454128265}
2025-01-14 17:22:51,109 [INFO] Step[2650/2713]: training loss : 0.9841680634021759 TRAIN  loss dict:  {'classification_loss': 0.9841680634021759}
2025-01-14 17:23:04,997 [INFO] Step[2700/2713]: training loss : 0.999413652420044 TRAIN  loss dict:  {'classification_loss': 0.999413652420044}
2025-01-14 17:24:23,038 [INFO] Label accuracies statistics:
2025-01-14 17:24:23,039 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.0, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.25, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.5, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 1.0, 240: 0.5, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.5, 277: 0.75, 278: 1.0, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 1.0, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.25, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.5, 355: 1.0, 356: 0.75, 357: 0.5, 358: 1.0, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 17:24:23,040 [INFO] [26] TRAIN  loss: 0.9824167294252679 acc: 0.9944710652414301
2025-01-14 17:24:23,040 [INFO] [26] TRAIN  loss dict: {'classification_loss': 0.9824167294252679}
2025-01-14 17:24:23,041 [INFO] [26] VALIDATION loss: 1.827523478439876 VALIDATION acc: 0.7912225705329153
2025-01-14 17:24:23,041 [INFO] [26] VALIDATION loss dict: {'classification_loss': 1.827523478439876}
2025-01-14 17:24:23,041 [INFO] 
2025-01-14 17:24:41,283 [INFO] Step[50/2713]: training loss : 0.9641138339042663 TRAIN  loss dict:  {'classification_loss': 0.9641138339042663}
2025-01-14 17:24:54,392 [INFO] Step[100/2713]: training loss : 0.979472621679306 TRAIN  loss dict:  {'classification_loss': 0.979472621679306}
2025-01-14 17:25:07,906 [INFO] Step[150/2713]: training loss : 0.9715759789943695 TRAIN  loss dict:  {'classification_loss': 0.9715759789943695}
2025-01-14 17:25:21,441 [INFO] Step[200/2713]: training loss : 0.9812892436981201 TRAIN  loss dict:  {'classification_loss': 0.9812892436981201}
2025-01-14 17:25:34,595 [INFO] Step[250/2713]: training loss : 0.987921199798584 TRAIN  loss dict:  {'classification_loss': 0.987921199798584}
2025-01-14 17:25:47,731 [INFO] Step[300/2713]: training loss : 0.9884653329849243 TRAIN  loss dict:  {'classification_loss': 0.9884653329849243}
2025-01-14 17:26:00,910 [INFO] Step[350/2713]: training loss : 0.9882917368412018 TRAIN  loss dict:  {'classification_loss': 0.9882917368412018}
2025-01-14 17:26:14,924 [INFO] Step[400/2713]: training loss : 0.9783310544490814 TRAIN  loss dict:  {'classification_loss': 0.9783310544490814}
2025-01-14 17:26:28,312 [INFO] Step[450/2713]: training loss : 0.9806593418121338 TRAIN  loss dict:  {'classification_loss': 0.9806593418121338}
2025-01-14 17:26:41,594 [INFO] Step[500/2713]: training loss : 0.9835929131507873 TRAIN  loss dict:  {'classification_loss': 0.9835929131507873}
2025-01-14 17:26:55,189 [INFO] Step[550/2713]: training loss : 0.9751168823242188 TRAIN  loss dict:  {'classification_loss': 0.9751168823242188}
2025-01-14 17:27:08,719 [INFO] Step[600/2713]: training loss : 0.9661189198493958 TRAIN  loss dict:  {'classification_loss': 0.9661189198493958}
2025-01-14 17:27:22,256 [INFO] Step[650/2713]: training loss : 0.9916467142105102 TRAIN  loss dict:  {'classification_loss': 0.9916467142105102}
2025-01-14 17:27:35,996 [INFO] Step[700/2713]: training loss : 0.9908768773078919 TRAIN  loss dict:  {'classification_loss': 0.9908768773078919}
2025-01-14 17:27:50,203 [INFO] Step[750/2713]: training loss : 0.9802391135692596 TRAIN  loss dict:  {'classification_loss': 0.9802391135692596}
2025-01-14 17:28:03,835 [INFO] Step[800/2713]: training loss : 0.9843648219108582 TRAIN  loss dict:  {'classification_loss': 0.9843648219108582}
2025-01-14 17:28:17,301 [INFO] Step[850/2713]: training loss : 0.9764965426921844 TRAIN  loss dict:  {'classification_loss': 0.9764965426921844}
2025-01-14 17:28:30,887 [INFO] Step[900/2713]: training loss : 0.9802324712276459 TRAIN  loss dict:  {'classification_loss': 0.9802324712276459}
2025-01-14 17:28:44,443 [INFO] Step[950/2713]: training loss : 1.0028939068317413 TRAIN  loss dict:  {'classification_loss': 1.0028939068317413}
2025-01-14 17:28:57,560 [INFO] Step[1000/2713]: training loss : 0.9648261272907257 TRAIN  loss dict:  {'classification_loss': 0.9648261272907257}
2025-01-14 17:29:11,277 [INFO] Step[1050/2713]: training loss : 0.9923295414447785 TRAIN  loss dict:  {'classification_loss': 0.9923295414447785}
2025-01-14 17:29:25,325 [INFO] Step[1100/2713]: training loss : 0.9665184843540192 TRAIN  loss dict:  {'classification_loss': 0.9665184843540192}
2025-01-14 17:29:39,416 [INFO] Step[1150/2713]: training loss : 0.9625398170948029 TRAIN  loss dict:  {'classification_loss': 0.9625398170948029}
2025-01-14 17:29:52,813 [INFO] Step[1200/2713]: training loss : 1.008843401670456 TRAIN  loss dict:  {'classification_loss': 1.008843401670456}
2025-01-14 17:30:06,380 [INFO] Step[1250/2713]: training loss : 0.9779761970043183 TRAIN  loss dict:  {'classification_loss': 0.9779761970043183}
2025-01-14 17:30:19,970 [INFO] Step[1300/2713]: training loss : 0.959812911748886 TRAIN  loss dict:  {'classification_loss': 0.959812911748886}
2025-01-14 17:30:33,496 [INFO] Step[1350/2713]: training loss : 0.9729351317882537 TRAIN  loss dict:  {'classification_loss': 0.9729351317882537}
2025-01-14 17:30:46,613 [INFO] Step[1400/2713]: training loss : 0.9656116735935211 TRAIN  loss dict:  {'classification_loss': 0.9656116735935211}
2025-01-14 17:31:00,205 [INFO] Step[1450/2713]: training loss : 0.9629297637939453 TRAIN  loss dict:  {'classification_loss': 0.9629297637939453}
2025-01-14 17:31:13,767 [INFO] Step[1500/2713]: training loss : 0.9824086391925811 TRAIN  loss dict:  {'classification_loss': 0.9824086391925811}
2025-01-14 17:31:27,910 [INFO] Step[1550/2713]: training loss : 0.9989929676055909 TRAIN  loss dict:  {'classification_loss': 0.9989929676055909}
2025-01-14 17:31:41,221 [INFO] Step[1600/2713]: training loss : 0.9688940465450286 TRAIN  loss dict:  {'classification_loss': 0.9688940465450286}
2025-01-14 17:31:54,380 [INFO] Step[1650/2713]: training loss : 0.992473019361496 TRAIN  loss dict:  {'classification_loss': 0.992473019361496}
2025-01-14 17:32:07,676 [INFO] Step[1700/2713]: training loss : 0.9696058368682862 TRAIN  loss dict:  {'classification_loss': 0.9696058368682862}
2025-01-14 17:32:20,921 [INFO] Step[1750/2713]: training loss : 0.9961873126029969 TRAIN  loss dict:  {'classification_loss': 0.9961873126029969}
2025-01-14 17:32:34,266 [INFO] Step[1800/2713]: training loss : 0.9807985186576843 TRAIN  loss dict:  {'classification_loss': 0.9807985186576843}
2025-01-14 17:32:47,545 [INFO] Step[1850/2713]: training loss : 0.9694210958480834 TRAIN  loss dict:  {'classification_loss': 0.9694210958480834}
2025-01-14 17:33:01,088 [INFO] Step[1900/2713]: training loss : 1.009010328054428 TRAIN  loss dict:  {'classification_loss': 1.009010328054428}
2025-01-14 17:33:15,015 [INFO] Step[1950/2713]: training loss : 0.9655404889583588 TRAIN  loss dict:  {'classification_loss': 0.9655404889583588}
2025-01-14 17:33:28,333 [INFO] Step[2000/2713]: training loss : 0.986231302022934 TRAIN  loss dict:  {'classification_loss': 0.986231302022934}
2025-01-14 17:33:42,090 [INFO] Step[2050/2713]: training loss : 0.9780332219600677 TRAIN  loss dict:  {'classification_loss': 0.9780332219600677}
2025-01-14 17:33:55,871 [INFO] Step[2100/2713]: training loss : 0.9742934679985047 TRAIN  loss dict:  {'classification_loss': 0.9742934679985047}
2025-01-14 17:34:09,268 [INFO] Step[2150/2713]: training loss : 0.9883823418617248 TRAIN  loss dict:  {'classification_loss': 0.9883823418617248}
2025-01-14 17:34:23,087 [INFO] Step[2200/2713]: training loss : 0.9824761223793029 TRAIN  loss dict:  {'classification_loss': 0.9824761223793029}
2025-01-14 17:34:36,588 [INFO] Step[2250/2713]: training loss : 1.0001371502876282 TRAIN  loss dict:  {'classification_loss': 1.0001371502876282}
2025-01-14 17:34:50,347 [INFO] Step[2300/2713]: training loss : 1.0066196477413178 TRAIN  loss dict:  {'classification_loss': 1.0066196477413178}
2025-01-14 17:35:04,126 [INFO] Step[2350/2713]: training loss : 0.9680870234966278 TRAIN  loss dict:  {'classification_loss': 0.9680870234966278}
2025-01-14 17:35:17,683 [INFO] Step[2400/2713]: training loss : 0.9965280222892762 TRAIN  loss dict:  {'classification_loss': 0.9965280222892762}
2025-01-14 17:35:31,263 [INFO] Step[2450/2713]: training loss : 0.9943932855129242 TRAIN  loss dict:  {'classification_loss': 0.9943932855129242}
2025-01-14 17:35:44,973 [INFO] Step[2500/2713]: training loss : 1.0031004738807678 TRAIN  loss dict:  {'classification_loss': 1.0031004738807678}
2025-01-14 17:35:58,268 [INFO] Step[2550/2713]: training loss : 0.9924699532985687 TRAIN  loss dict:  {'classification_loss': 0.9924699532985687}
2025-01-14 17:36:11,731 [INFO] Step[2600/2713]: training loss : 0.9806788539886475 TRAIN  loss dict:  {'classification_loss': 0.9806788539886475}
2025-01-14 17:36:25,148 [INFO] Step[2650/2713]: training loss : 0.9746713650226593 TRAIN  loss dict:  {'classification_loss': 0.9746713650226593}
2025-01-14 17:36:38,843 [INFO] Step[2700/2713]: training loss : 0.9707610487937928 TRAIN  loss dict:  {'classification_loss': 0.9707610487937928}
2025-01-14 17:37:58,908 [INFO] Label accuracies statistics:
2025-01-14 17:37:58,909 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.5, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.25, 108: 0.75, 109: 0.5, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 0.75, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 0.75, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 0.75, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.25, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 0.5, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 17:37:58,911 [INFO] [27] TRAIN  loss: 0.9818340598152785 acc: 0.9933652782897162
2025-01-14 17:37:58,911 [INFO] [27] TRAIN  loss dict: {'classification_loss': 0.9818340598152785}
2025-01-14 17:37:58,911 [INFO] [27] VALIDATION loss: 1.8944531167136098 VALIDATION acc: 0.773667711598746
2025-01-14 17:37:58,911 [INFO] [27] VALIDATION loss dict: {'classification_loss': 1.8944531167136098}
2025-01-14 17:37:58,911 [INFO] 
2025-01-14 17:38:18,344 [INFO] Step[50/2713]: training loss : 0.9759958004951477 TRAIN  loss dict:  {'classification_loss': 0.9759958004951477}
2025-01-14 17:38:32,137 [INFO] Step[100/2713]: training loss : 0.9585962975025177 TRAIN  loss dict:  {'classification_loss': 0.9585962975025177}
2025-01-14 17:38:45,908 [INFO] Step[150/2713]: training loss : 0.9597712910175323 TRAIN  loss dict:  {'classification_loss': 0.9597712910175323}
2025-01-14 17:38:59,255 [INFO] Step[200/2713]: training loss : 0.9822066402435303 TRAIN  loss dict:  {'classification_loss': 0.9822066402435303}
2025-01-14 17:39:12,990 [INFO] Step[250/2713]: training loss : 0.977629075050354 TRAIN  loss dict:  {'classification_loss': 0.977629075050354}
2025-01-14 17:39:26,796 [INFO] Step[300/2713]: training loss : 0.9653699421882629 TRAIN  loss dict:  {'classification_loss': 0.9653699421882629}
2025-01-14 17:39:40,565 [INFO] Step[350/2713]: training loss : 0.9673161542415619 TRAIN  loss dict:  {'classification_loss': 0.9673161542415619}
2025-01-14 17:39:54,132 [INFO] Step[400/2713]: training loss : 0.9686491465568543 TRAIN  loss dict:  {'classification_loss': 0.9686491465568543}
2025-01-14 17:40:07,265 [INFO] Step[450/2713]: training loss : 0.9641619539260864 TRAIN  loss dict:  {'classification_loss': 0.9641619539260864}
2025-01-14 17:40:21,185 [INFO] Step[500/2713]: training loss : 0.9609086775779724 TRAIN  loss dict:  {'classification_loss': 0.9609086775779724}
2025-01-14 17:40:34,386 [INFO] Step[550/2713]: training loss : 0.9725378966331482 TRAIN  loss dict:  {'classification_loss': 0.9725378966331482}
2025-01-14 17:40:48,305 [INFO] Step[600/2713]: training loss : 0.9764506387710571 TRAIN  loss dict:  {'classification_loss': 0.9764506387710571}
2025-01-14 17:41:01,797 [INFO] Step[650/2713]: training loss : 0.9683546662330628 TRAIN  loss dict:  {'classification_loss': 0.9683546662330628}
2025-01-14 17:41:14,970 [INFO] Step[700/2713]: training loss : 0.9944788908958435 TRAIN  loss dict:  {'classification_loss': 0.9944788908958435}
2025-01-14 17:41:29,042 [INFO] Step[750/2713]: training loss : 0.9607645094394683 TRAIN  loss dict:  {'classification_loss': 0.9607645094394683}
2025-01-14 17:41:42,560 [INFO] Step[800/2713]: training loss : 0.9678727149963379 TRAIN  loss dict:  {'classification_loss': 0.9678727149963379}
2025-01-14 17:41:56,713 [INFO] Step[850/2713]: training loss : 0.9884000051021576 TRAIN  loss dict:  {'classification_loss': 0.9884000051021576}
2025-01-14 17:42:10,071 [INFO] Step[900/2713]: training loss : 1.0080964696407317 TRAIN  loss dict:  {'classification_loss': 1.0080964696407317}
2025-01-14 17:42:23,551 [INFO] Step[950/2713]: training loss : 0.9611933743953704 TRAIN  loss dict:  {'classification_loss': 0.9611933743953704}
2025-01-14 17:42:37,420 [INFO] Step[1000/2713]: training loss : 0.9541457176208497 TRAIN  loss dict:  {'classification_loss': 0.9541457176208497}
2025-01-14 17:42:51,580 [INFO] Step[1050/2713]: training loss : 0.9851908969879151 TRAIN  loss dict:  {'classification_loss': 0.9851908969879151}
2025-01-14 17:43:05,689 [INFO] Step[1100/2713]: training loss : 0.9741602087020874 TRAIN  loss dict:  {'classification_loss': 0.9741602087020874}
2025-01-14 17:43:19,455 [INFO] Step[1150/2713]: training loss : 1.0282173824310303 TRAIN  loss dict:  {'classification_loss': 1.0282173824310303}
2025-01-14 17:43:33,398 [INFO] Step[1200/2713]: training loss : 1.0131116533279418 TRAIN  loss dict:  {'classification_loss': 1.0131116533279418}
2025-01-14 17:43:47,782 [INFO] Step[1250/2713]: training loss : 0.9727370262145996 TRAIN  loss dict:  {'classification_loss': 0.9727370262145996}
2025-01-14 17:44:01,679 [INFO] Step[1300/2713]: training loss : 1.000317223072052 TRAIN  loss dict:  {'classification_loss': 1.000317223072052}
2025-01-14 17:44:15,401 [INFO] Step[1350/2713]: training loss : 0.9621614241600036 TRAIN  loss dict:  {'classification_loss': 0.9621614241600036}
2025-01-14 17:44:29,293 [INFO] Step[1400/2713]: training loss : 0.9890231323242188 TRAIN  loss dict:  {'classification_loss': 0.9890231323242188}
2025-01-14 17:44:42,596 [INFO] Step[1450/2713]: training loss : 0.9860484671592712 TRAIN  loss dict:  {'classification_loss': 0.9860484671592712}
2025-01-14 17:44:56,492 [INFO] Step[1500/2713]: training loss : 0.9616166365146637 TRAIN  loss dict:  {'classification_loss': 0.9616166365146637}
2025-01-14 17:45:10,104 [INFO] Step[1550/2713]: training loss : 0.9627222490310668 TRAIN  loss dict:  {'classification_loss': 0.9627222490310668}
2025-01-14 17:45:23,468 [INFO] Step[1600/2713]: training loss : 0.9960595035552978 TRAIN  loss dict:  {'classification_loss': 0.9960595035552978}
2025-01-14 17:45:36,742 [INFO] Step[1650/2713]: training loss : 0.9648807537555695 TRAIN  loss dict:  {'classification_loss': 0.9648807537555695}
2025-01-14 17:45:50,202 [INFO] Step[1700/2713]: training loss : 0.9634542083740234 TRAIN  loss dict:  {'classification_loss': 0.9634542083740234}
2025-01-14 17:46:03,678 [INFO] Step[1750/2713]: training loss : 0.9706990003585816 TRAIN  loss dict:  {'classification_loss': 0.9706990003585816}
2025-01-14 17:46:17,450 [INFO] Step[1800/2713]: training loss : 0.9742328321933746 TRAIN  loss dict:  {'classification_loss': 0.9742328321933746}
2025-01-14 17:46:30,952 [INFO] Step[1850/2713]: training loss : 0.9566876018047332 TRAIN  loss dict:  {'classification_loss': 0.9566876018047332}
2025-01-14 17:46:44,441 [INFO] Step[1900/2713]: training loss : 0.9816537714004516 TRAIN  loss dict:  {'classification_loss': 0.9816537714004516}
2025-01-14 17:46:58,016 [INFO] Step[1950/2713]: training loss : 0.959427559375763 TRAIN  loss dict:  {'classification_loss': 0.959427559375763}
2025-01-14 17:47:11,207 [INFO] Step[2000/2713]: training loss : 0.9947748494148254 TRAIN  loss dict:  {'classification_loss': 0.9947748494148254}
2025-01-14 17:47:24,912 [INFO] Step[2050/2713]: training loss : 0.9868349194526672 TRAIN  loss dict:  {'classification_loss': 0.9868349194526672}
2025-01-14 17:47:38,362 [INFO] Step[2100/2713]: training loss : 0.9705182409286499 TRAIN  loss dict:  {'classification_loss': 0.9705182409286499}
2025-01-14 17:47:51,940 [INFO] Step[2150/2713]: training loss : 0.9764304709434509 TRAIN  loss dict:  {'classification_loss': 0.9764304709434509}
2025-01-14 17:48:05,925 [INFO] Step[2200/2713]: training loss : 0.9771263468265533 TRAIN  loss dict:  {'classification_loss': 0.9771263468265533}
2025-01-14 17:48:19,446 [INFO] Step[2250/2713]: training loss : 0.9870947742462158 TRAIN  loss dict:  {'classification_loss': 0.9870947742462158}
2025-01-14 17:48:33,160 [INFO] Step[2300/2713]: training loss : 1.0121221220493317 TRAIN  loss dict:  {'classification_loss': 1.0121221220493317}
2025-01-14 17:48:47,033 [INFO] Step[2350/2713]: training loss : 0.9757716929912568 TRAIN  loss dict:  {'classification_loss': 0.9757716929912568}
2025-01-14 17:49:01,092 [INFO] Step[2400/2713]: training loss : 0.9819312763214111 TRAIN  loss dict:  {'classification_loss': 0.9819312763214111}
2025-01-14 17:49:14,693 [INFO] Step[2450/2713]: training loss : 0.9647392630577087 TRAIN  loss dict:  {'classification_loss': 0.9647392630577087}
2025-01-14 17:49:28,455 [INFO] Step[2500/2713]: training loss : 0.9840734219551086 TRAIN  loss dict:  {'classification_loss': 0.9840734219551086}
2025-01-14 17:49:42,599 [INFO] Step[2550/2713]: training loss : 0.9933866429328918 TRAIN  loss dict:  {'classification_loss': 0.9933866429328918}
2025-01-14 17:49:56,440 [INFO] Step[2600/2713]: training loss : 0.9662809097766876 TRAIN  loss dict:  {'classification_loss': 0.9662809097766876}
2025-01-14 17:50:13,060 [INFO] Step[2650/2713]: training loss : 0.9594747817516327 TRAIN  loss dict:  {'classification_loss': 0.9594747817516327}
2025-01-14 17:50:26,731 [INFO] Step[2700/2713]: training loss : 0.9745881724357605 TRAIN  loss dict:  {'classification_loss': 0.9745881724357605}
2025-01-14 17:51:44,376 [INFO] Label accuracies statistics:
2025-01-14 17:51:44,376 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.0, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 0.5, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 1.0, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.25, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.25, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 0.75, 358: 1.0, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-14 17:51:44,378 [INFO] [28] TRAIN  loss: 0.9765479057598853 acc: 0.9948396608920015
2025-01-14 17:51:44,378 [INFO] [28] TRAIN  loss dict: {'classification_loss': 0.9765479057598853}
2025-01-14 17:51:44,379 [INFO] [28] VALIDATION loss: 1.8736233618250466 VALIDATION acc: 0.7793103448275862
2025-01-14 17:51:44,379 [INFO] [28] VALIDATION loss dict: {'classification_loss': 1.8736233618250466}
2025-01-14 17:51:44,379 [INFO] 
2025-01-14 17:52:03,513 [INFO] Step[50/2713]: training loss : 0.9717476093769073 TRAIN  loss dict:  {'classification_loss': 0.9717476093769073}
2025-01-14 17:52:17,110 [INFO] Step[100/2713]: training loss : 0.9747775638103485 TRAIN  loss dict:  {'classification_loss': 0.9747775638103485}
2025-01-14 17:52:31,271 [INFO] Step[150/2713]: training loss : 0.9774190986156464 TRAIN  loss dict:  {'classification_loss': 0.9774190986156464}
2025-01-14 17:52:45,224 [INFO] Step[200/2713]: training loss : 0.9663966345787048 TRAIN  loss dict:  {'classification_loss': 0.9663966345787048}
2025-01-14 17:52:58,850 [INFO] Step[250/2713]: training loss : 0.9639013624191284 TRAIN  loss dict:  {'classification_loss': 0.9639013624191284}
2025-01-14 17:53:12,956 [INFO] Step[300/2713]: training loss : 0.9756376338005066 TRAIN  loss dict:  {'classification_loss': 0.9756376338005066}
2025-01-14 17:53:26,945 [INFO] Step[350/2713]: training loss : 0.9588455462455749 TRAIN  loss dict:  {'classification_loss': 0.9588455462455749}
2025-01-14 17:53:40,836 [INFO] Step[400/2713]: training loss : 0.9583524775505066 TRAIN  loss dict:  {'classification_loss': 0.9583524775505066}
2025-01-14 17:53:54,953 [INFO] Step[450/2713]: training loss : 0.9924753081798553 TRAIN  loss dict:  {'classification_loss': 0.9924753081798553}
2025-01-14 17:54:09,190 [INFO] Step[500/2713]: training loss : 1.0252472019195558 TRAIN  loss dict:  {'classification_loss': 1.0252472019195558}
2025-01-14 17:54:22,540 [INFO] Step[550/2713]: training loss : 1.0070141863822937 TRAIN  loss dict:  {'classification_loss': 1.0070141863822937}
2025-01-14 17:54:36,405 [INFO] Step[600/2713]: training loss : 0.9613750672340393 TRAIN  loss dict:  {'classification_loss': 0.9613750672340393}
2025-01-14 17:54:49,901 [INFO] Step[650/2713]: training loss : 0.9913212943077088 TRAIN  loss dict:  {'classification_loss': 0.9913212943077088}
2025-01-14 17:55:03,147 [INFO] Step[700/2713]: training loss : 0.9642161130905151 TRAIN  loss dict:  {'classification_loss': 0.9642161130905151}
2025-01-14 17:55:16,716 [INFO] Step[750/2713]: training loss : 0.9663860428333283 TRAIN  loss dict:  {'classification_loss': 0.9663860428333283}
2025-01-14 17:55:29,907 [INFO] Step[800/2713]: training loss : 1.0167859625816345 TRAIN  loss dict:  {'classification_loss': 1.0167859625816345}
2025-01-14 17:55:43,158 [INFO] Step[850/2713]: training loss : 0.9679880559444427 TRAIN  loss dict:  {'classification_loss': 0.9679880559444427}
2025-01-14 17:55:56,593 [INFO] Step[900/2713]: training loss : 0.958105422258377 TRAIN  loss dict:  {'classification_loss': 0.958105422258377}
2025-01-14 17:56:10,627 [INFO] Step[950/2713]: training loss : 0.9646618723869324 TRAIN  loss dict:  {'classification_loss': 0.9646618723869324}
2025-01-14 17:56:24,429 [INFO] Step[1000/2713]: training loss : 0.9574572253227234 TRAIN  loss dict:  {'classification_loss': 0.9574572253227234}
2025-01-14 17:56:37,698 [INFO] Step[1050/2713]: training loss : 1.021789778470993 TRAIN  loss dict:  {'classification_loss': 1.021789778470993}
2025-01-14 17:56:50,948 [INFO] Step[1100/2713]: training loss : 0.9854076969623565 TRAIN  loss dict:  {'classification_loss': 0.9854076969623565}
2025-01-14 17:57:04,749 [INFO] Step[1150/2713]: training loss : 0.9723314940929413 TRAIN  loss dict:  {'classification_loss': 0.9723314940929413}
2025-01-14 17:57:18,232 [INFO] Step[1200/2713]: training loss : 0.9684823000431061 TRAIN  loss dict:  {'classification_loss': 0.9684823000431061}
2025-01-14 17:57:31,932 [INFO] Step[1250/2713]: training loss : 0.9687776839733124 TRAIN  loss dict:  {'classification_loss': 0.9687776839733124}
2025-01-14 17:57:45,397 [INFO] Step[1300/2713]: training loss : 0.966401937007904 TRAIN  loss dict:  {'classification_loss': 0.966401937007904}
2025-01-14 17:57:58,808 [INFO] Step[1350/2713]: training loss : 0.9707570993900299 TRAIN  loss dict:  {'classification_loss': 0.9707570993900299}
2025-01-14 17:58:12,000 [INFO] Step[1400/2713]: training loss : 0.963920590877533 TRAIN  loss dict:  {'classification_loss': 0.963920590877533}
2025-01-14 17:58:25,606 [INFO] Step[1450/2713]: training loss : 0.9917502748966217 TRAIN  loss dict:  {'classification_loss': 0.9917502748966217}
2025-01-14 17:58:39,404 [INFO] Step[1500/2713]: training loss : 0.9679855489730835 TRAIN  loss dict:  {'classification_loss': 0.9679855489730835}
2025-01-14 17:58:53,304 [INFO] Step[1550/2713]: training loss : 0.9586517369747162 TRAIN  loss dict:  {'classification_loss': 0.9586517369747162}
2025-01-14 17:59:06,815 [INFO] Step[1600/2713]: training loss : 0.9850422608852386 TRAIN  loss dict:  {'classification_loss': 0.9850422608852386}
2025-01-14 17:59:20,376 [INFO] Step[1650/2713]: training loss : 0.9744172871112824 TRAIN  loss dict:  {'classification_loss': 0.9744172871112824}
2025-01-14 17:59:34,330 [INFO] Step[1700/2713]: training loss : 0.9740608954429626 TRAIN  loss dict:  {'classification_loss': 0.9740608954429626}
2025-01-14 17:59:48,215 [INFO] Step[1750/2713]: training loss : 0.9664391088485718 TRAIN  loss dict:  {'classification_loss': 0.9664391088485718}
2025-01-14 18:00:02,300 [INFO] Step[1800/2713]: training loss : 0.972390114068985 TRAIN  loss dict:  {'classification_loss': 0.972390114068985}
2025-01-14 18:00:16,094 [INFO] Step[1850/2713]: training loss : 0.9706305193901063 TRAIN  loss dict:  {'classification_loss': 0.9706305193901063}
2025-01-14 18:00:29,427 [INFO] Step[1900/2713]: training loss : 1.0005112707614898 TRAIN  loss dict:  {'classification_loss': 1.0005112707614898}
2025-01-14 18:00:43,412 [INFO] Step[1950/2713]: training loss : 0.9695461499691009 TRAIN  loss dict:  {'classification_loss': 0.9695461499691009}
2025-01-14 18:00:57,155 [INFO] Step[2000/2713]: training loss : 0.9585627698898316 TRAIN  loss dict:  {'classification_loss': 0.9585627698898316}
2025-01-14 18:01:10,813 [INFO] Step[2050/2713]: training loss : 0.9986725187301636 TRAIN  loss dict:  {'classification_loss': 0.9986725187301636}
2025-01-14 18:01:24,633 [INFO] Step[2100/2713]: training loss : 0.9654181480407715 TRAIN  loss dict:  {'classification_loss': 0.9654181480407715}
2025-01-14 18:01:38,674 [INFO] Step[2150/2713]: training loss : 0.9659466314315795 TRAIN  loss dict:  {'classification_loss': 0.9659466314315795}
2025-01-14 18:01:52,118 [INFO] Step[2200/2713]: training loss : 1.0002927494049072 TRAIN  loss dict:  {'classification_loss': 1.0002927494049072}
2025-01-14 18:02:06,329 [INFO] Step[2250/2713]: training loss : 0.991830528974533 TRAIN  loss dict:  {'classification_loss': 0.991830528974533}
2025-01-14 18:02:20,799 [INFO] Step[2300/2713]: training loss : 0.9866791236400604 TRAIN  loss dict:  {'classification_loss': 0.9866791236400604}
2025-01-14 18:02:34,451 [INFO] Step[2350/2713]: training loss : 0.9598635375499726 TRAIN  loss dict:  {'classification_loss': 0.9598635375499726}
2025-01-14 18:02:48,113 [INFO] Step[2400/2713]: training loss : 1.0056778943538667 TRAIN  loss dict:  {'classification_loss': 1.0056778943538667}
2025-01-14 18:03:01,757 [INFO] Step[2450/2713]: training loss : 0.9633997285366058 TRAIN  loss dict:  {'classification_loss': 0.9633997285366058}
2025-01-14 18:03:14,995 [INFO] Step[2500/2713]: training loss : 0.9657213723659516 TRAIN  loss dict:  {'classification_loss': 0.9657213723659516}
2025-01-14 18:03:28,247 [INFO] Step[2550/2713]: training loss : 0.9685981750488282 TRAIN  loss dict:  {'classification_loss': 0.9685981750488282}
2025-01-14 18:03:41,493 [INFO] Step[2600/2713]: training loss : 1.0113974177837373 TRAIN  loss dict:  {'classification_loss': 1.0113974177837373}
2025-01-14 18:03:55,087 [INFO] Step[2650/2713]: training loss : 0.9772465193271637 TRAIN  loss dict:  {'classification_loss': 0.9772465193271637}
2025-01-14 18:04:08,575 [INFO] Step[2700/2713]: training loss : 0.9517264008522034 TRAIN  loss dict:  {'classification_loss': 0.9517264008522034}
2025-01-14 18:05:26,879 [INFO] Label accuracies statistics:
2025-01-14 18:05:26,879 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.5, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 0.75, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 0.75, 256: 0.5, 257: 0.75, 258: 0.25, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.5, 269: 0.75, 270: 0.75, 271: 0.75, 272: 0.5, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.5, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.5, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.25, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 18:05:26,881 [INFO] [29] TRAIN  loss: 0.9765495891901473 acc: 0.9947167956751444
2025-01-14 18:05:26,881 [INFO] [29] TRAIN  loss dict: {'classification_loss': 0.9765495891901473}
2025-01-14 18:05:26,882 [INFO] [29] VALIDATION loss: 1.873894123990733 VALIDATION acc: 0.774294670846395
2025-01-14 18:05:26,882 [INFO] [29] VALIDATION loss dict: {'classification_loss': 1.873894123990733}
2025-01-14 18:05:26,882 [INFO] 
2025-01-14 18:05:45,908 [INFO] Step[50/2713]: training loss : 0.9658448243141174 TRAIN  loss dict:  {'classification_loss': 0.9658448243141174}
2025-01-14 18:05:59,737 [INFO] Step[100/2713]: training loss : 0.9643522083759308 TRAIN  loss dict:  {'classification_loss': 0.9643522083759308}
2025-01-14 18:06:13,282 [INFO] Step[150/2713]: training loss : 0.9661725807189941 TRAIN  loss dict:  {'classification_loss': 0.9661725807189941}
2025-01-14 18:06:27,160 [INFO] Step[200/2713]: training loss : 0.9650780785083771 TRAIN  loss dict:  {'classification_loss': 0.9650780785083771}
2025-01-14 18:06:40,839 [INFO] Step[250/2713]: training loss : 0.9597691333293915 TRAIN  loss dict:  {'classification_loss': 0.9597691333293915}
2025-01-14 18:06:54,359 [INFO] Step[300/2713]: training loss : 0.9657466959953308 TRAIN  loss dict:  {'classification_loss': 0.9657466959953308}
2025-01-14 18:07:07,627 [INFO] Step[350/2713]: training loss : 0.9579261493682861 TRAIN  loss dict:  {'classification_loss': 0.9579261493682861}
2025-01-14 18:07:20,950 [INFO] Step[400/2713]: training loss : 0.9648879933357238 TRAIN  loss dict:  {'classification_loss': 0.9648879933357238}
2025-01-14 18:07:34,859 [INFO] Step[450/2713]: training loss : 0.9542449247837067 TRAIN  loss dict:  {'classification_loss': 0.9542449247837067}
2025-01-14 18:07:48,207 [INFO] Step[500/2713]: training loss : 1.0347408294677733 TRAIN  loss dict:  {'classification_loss': 1.0347408294677733}
2025-01-14 18:08:02,395 [INFO] Step[550/2713]: training loss : 0.9683932554721832 TRAIN  loss dict:  {'classification_loss': 0.9683932554721832}
2025-01-14 18:08:15,731 [INFO] Step[600/2713]: training loss : 0.9558116626739502 TRAIN  loss dict:  {'classification_loss': 0.9558116626739502}
2025-01-14 18:08:29,436 [INFO] Step[650/2713]: training loss : 0.9994021368026733 TRAIN  loss dict:  {'classification_loss': 0.9994021368026733}
2025-01-14 18:08:43,240 [INFO] Step[700/2713]: training loss : 0.9679149806499481 TRAIN  loss dict:  {'classification_loss': 0.9679149806499481}
2025-01-14 18:08:56,720 [INFO] Step[750/2713]: training loss : 0.9723968410491943 TRAIN  loss dict:  {'classification_loss': 0.9723968410491943}
2025-01-14 18:09:10,629 [INFO] Step[800/2713]: training loss : 1.0255192291736603 TRAIN  loss dict:  {'classification_loss': 1.0255192291736603}
2025-01-14 18:09:24,137 [INFO] Step[850/2713]: training loss : 0.9650917434692383 TRAIN  loss dict:  {'classification_loss': 0.9650917434692383}
2025-01-14 18:09:38,305 [INFO] Step[900/2713]: training loss : 0.957108439207077 TRAIN  loss dict:  {'classification_loss': 0.957108439207077}
2025-01-14 18:09:52,113 [INFO] Step[950/2713]: training loss : 0.9616443133354187 TRAIN  loss dict:  {'classification_loss': 0.9616443133354187}
2025-01-14 18:10:06,237 [INFO] Step[1000/2713]: training loss : 0.9882052540779114 TRAIN  loss dict:  {'classification_loss': 0.9882052540779114}
2025-01-14 18:10:20,085 [INFO] Step[1050/2713]: training loss : 0.9638171064853668 TRAIN  loss dict:  {'classification_loss': 0.9638171064853668}
2025-01-14 18:10:34,213 [INFO] Step[1100/2713]: training loss : 0.9644406664371491 TRAIN  loss dict:  {'classification_loss': 0.9644406664371491}
2025-01-14 18:10:48,224 [INFO] Step[1150/2713]: training loss : 0.9894248235225678 TRAIN  loss dict:  {'classification_loss': 0.9894248235225678}
2025-01-14 18:11:02,339 [INFO] Step[1200/2713]: training loss : 0.967414253950119 TRAIN  loss dict:  {'classification_loss': 0.967414253950119}
2025-01-14 18:11:16,550 [INFO] Step[1250/2713]: training loss : 0.9547288048267365 TRAIN  loss dict:  {'classification_loss': 0.9547288048267365}
2025-01-14 18:11:30,423 [INFO] Step[1300/2713]: training loss : 0.9573295521736145 TRAIN  loss dict:  {'classification_loss': 0.9573295521736145}
2025-01-14 18:11:44,374 [INFO] Step[1350/2713]: training loss : 0.996523209810257 TRAIN  loss dict:  {'classification_loss': 0.996523209810257}
2025-01-14 18:11:58,186 [INFO] Step[1400/2713]: training loss : 0.9783772420883179 TRAIN  loss dict:  {'classification_loss': 0.9783772420883179}
2025-01-14 18:12:12,063 [INFO] Step[1450/2713]: training loss : 0.9780970728397369 TRAIN  loss dict:  {'classification_loss': 0.9780970728397369}
2025-01-14 18:12:25,638 [INFO] Step[1500/2713]: training loss : 0.9647247576713562 TRAIN  loss dict:  {'classification_loss': 0.9647247576713562}
2025-01-14 18:12:39,152 [INFO] Step[1550/2713]: training loss : 0.9810518586635589 TRAIN  loss dict:  {'classification_loss': 0.9810518586635589}
2025-01-14 18:12:53,091 [INFO] Step[1600/2713]: training loss : 0.9810570180416107 TRAIN  loss dict:  {'classification_loss': 0.9810570180416107}
2025-01-14 18:13:06,689 [INFO] Step[1650/2713]: training loss : 0.9585999763011932 TRAIN  loss dict:  {'classification_loss': 0.9585999763011932}
2025-01-14 18:13:20,663 [INFO] Step[1700/2713]: training loss : 0.9787962663173676 TRAIN  loss dict:  {'classification_loss': 0.9787962663173676}
2025-01-14 18:13:34,613 [INFO] Step[1750/2713]: training loss : 0.99444744348526 TRAIN  loss dict:  {'classification_loss': 0.99444744348526}
2025-01-14 18:13:48,562 [INFO] Step[1800/2713]: training loss : 0.976572095155716 TRAIN  loss dict:  {'classification_loss': 0.976572095155716}
2025-01-14 18:14:02,388 [INFO] Step[1850/2713]: training loss : 0.9698450887203216 TRAIN  loss dict:  {'classification_loss': 0.9698450887203216}
2025-01-14 18:14:16,218 [INFO] Step[1900/2713]: training loss : 0.9795938766002655 TRAIN  loss dict:  {'classification_loss': 0.9795938766002655}
2025-01-14 18:14:30,308 [INFO] Step[1950/2713]: training loss : 0.9742993497848511 TRAIN  loss dict:  {'classification_loss': 0.9742993497848511}
2025-01-14 18:14:43,521 [INFO] Step[2000/2713]: training loss : 0.9655579006671906 TRAIN  loss dict:  {'classification_loss': 0.9655579006671906}
2025-01-14 18:14:57,187 [INFO] Step[2050/2713]: training loss : 0.9684624326229095 TRAIN  loss dict:  {'classification_loss': 0.9684624326229095}
2025-01-14 18:15:11,406 [INFO] Step[2100/2713]: training loss : 0.9843637335300446 TRAIN  loss dict:  {'classification_loss': 0.9843637335300446}
2025-01-14 18:15:24,619 [INFO] Step[2150/2713]: training loss : 0.9632209312915802 TRAIN  loss dict:  {'classification_loss': 0.9632209312915802}
2025-01-14 18:15:38,226 [INFO] Step[2200/2713]: training loss : 0.9877165758609772 TRAIN  loss dict:  {'classification_loss': 0.9877165758609772}
2025-01-14 18:15:51,856 [INFO] Step[2250/2713]: training loss : 0.953018673658371 TRAIN  loss dict:  {'classification_loss': 0.953018673658371}
2025-01-14 18:16:05,109 [INFO] Step[2300/2713]: training loss : 0.9534761846065521 TRAIN  loss dict:  {'classification_loss': 0.9534761846065521}
2025-01-14 18:16:18,661 [INFO] Step[2350/2713]: training loss : 0.9795689833164215 TRAIN  loss dict:  {'classification_loss': 0.9795689833164215}
2025-01-14 18:16:32,492 [INFO] Step[2400/2713]: training loss : 0.9911591625213623 TRAIN  loss dict:  {'classification_loss': 0.9911591625213623}
2025-01-14 18:16:45,880 [INFO] Step[2450/2713]: training loss : 0.9916328167915345 TRAIN  loss dict:  {'classification_loss': 0.9916328167915345}
2025-01-14 18:16:59,456 [INFO] Step[2500/2713]: training loss : 0.996168714761734 TRAIN  loss dict:  {'classification_loss': 0.996168714761734}
2025-01-14 18:17:13,134 [INFO] Step[2550/2713]: training loss : 0.9690642726421356 TRAIN  loss dict:  {'classification_loss': 0.9690642726421356}
2025-01-14 18:17:27,140 [INFO] Step[2600/2713]: training loss : 0.9886387348175049 TRAIN  loss dict:  {'classification_loss': 0.9886387348175049}
2025-01-14 18:17:41,401 [INFO] Step[2650/2713]: training loss : 0.9887876498699188 TRAIN  loss dict:  {'classification_loss': 0.9887876498699188}
2025-01-14 18:17:54,932 [INFO] Step[2700/2713]: training loss : 0.9700965058803558 TRAIN  loss dict:  {'classification_loss': 0.9700965058803558}
2025-01-14 18:19:11,069 [INFO] Label accuracies statistics:
2025-01-14 18:19:11,070 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 0.75, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 1.0, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 0.75, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 0.75, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 1.0, 279: 0.75, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.5, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.25, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 1.0, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 18:19:11,072 [INFO] [30] TRAIN  loss: 0.974411004876194 acc: 0.9949625261088586
2025-01-14 18:19:11,072 [INFO] [30] TRAIN  loss dict: {'classification_loss': 0.974411004876194}
2025-01-14 18:19:11,072 [INFO] [30] VALIDATION loss: 1.886537368136241 VALIDATION acc: 0.7786833855799373
2025-01-14 18:19:11,072 [INFO] [30] VALIDATION loss dict: {'classification_loss': 1.886537368136241}
2025-01-14 18:19:11,072 [INFO] 
2025-01-14 18:19:34,749 [INFO] Step[50/2713]: training loss : 0.9878864133358002 TRAIN  loss dict:  {'classification_loss': 0.9878864133358002}
2025-01-14 18:19:48,566 [INFO] Step[100/2713]: training loss : 0.9592024517059327 TRAIN  loss dict:  {'classification_loss': 0.9592024517059327}
2025-01-14 18:20:02,352 [INFO] Step[150/2713]: training loss : 0.9552357065677642 TRAIN  loss dict:  {'classification_loss': 0.9552357065677642}
2025-01-14 18:20:16,126 [INFO] Step[200/2713]: training loss : 0.9780904138088227 TRAIN  loss dict:  {'classification_loss': 0.9780904138088227}
2025-01-14 18:20:29,930 [INFO] Step[250/2713]: training loss : 0.9617747724056244 TRAIN  loss dict:  {'classification_loss': 0.9617747724056244}
2025-01-14 18:20:43,557 [INFO] Step[300/2713]: training loss : 0.9986720979213715 TRAIN  loss dict:  {'classification_loss': 0.9986720979213715}
2025-01-14 18:20:57,752 [INFO] Step[350/2713]: training loss : 0.9732737600803375 TRAIN  loss dict:  {'classification_loss': 0.9732737600803375}
2025-01-14 18:21:11,704 [INFO] Step[400/2713]: training loss : 1.0103013336658477 TRAIN  loss dict:  {'classification_loss': 1.0103013336658477}
2025-01-14 18:21:25,067 [INFO] Step[450/2713]: training loss : 0.9769841802120208 TRAIN  loss dict:  {'classification_loss': 0.9769841802120208}
2025-01-14 18:21:39,261 [INFO] Step[500/2713]: training loss : 0.9566195142269135 TRAIN  loss dict:  {'classification_loss': 0.9566195142269135}
2025-01-14 18:21:53,161 [INFO] Step[550/2713]: training loss : 0.9573132264614105 TRAIN  loss dict:  {'classification_loss': 0.9573132264614105}
2025-01-14 18:22:07,083 [INFO] Step[600/2713]: training loss : 0.995128003358841 TRAIN  loss dict:  {'classification_loss': 0.995128003358841}
2025-01-14 18:22:23,269 [INFO] Step[650/2713]: training loss : 1.0087039947509766 TRAIN  loss dict:  {'classification_loss': 1.0087039947509766}
2025-01-14 18:22:38,700 [INFO] Step[700/2713]: training loss : 0.9663742840290069 TRAIN  loss dict:  {'classification_loss': 0.9663742840290069}
2025-01-14 18:22:55,173 [INFO] Step[750/2713]: training loss : 0.9506611061096192 TRAIN  loss dict:  {'classification_loss': 0.9506611061096192}
2025-01-14 18:23:09,418 [INFO] Step[800/2713]: training loss : 0.9816840696334839 TRAIN  loss dict:  {'classification_loss': 0.9816840696334839}
2025-01-14 18:23:22,937 [INFO] Step[850/2713]: training loss : 0.9563093197345733 TRAIN  loss dict:  {'classification_loss': 0.9563093197345733}
2025-01-14 18:23:36,122 [INFO] Step[900/2713]: training loss : 0.9558224403858184 TRAIN  loss dict:  {'classification_loss': 0.9558224403858184}
2025-01-14 18:23:49,387 [INFO] Step[950/2713]: training loss : 0.95576016664505 TRAIN  loss dict:  {'classification_loss': 0.95576016664505}
2025-01-14 18:24:03,004 [INFO] Step[1000/2713]: training loss : 0.9700252175331116 TRAIN  loss dict:  {'classification_loss': 0.9700252175331116}
2025-01-14 18:24:16,540 [INFO] Step[1050/2713]: training loss : 0.9658349967002868 TRAIN  loss dict:  {'classification_loss': 0.9658349967002868}
2025-01-14 18:24:29,746 [INFO] Step[1100/2713]: training loss : 0.9540890777111053 TRAIN  loss dict:  {'classification_loss': 0.9540890777111053}
2025-01-14 18:24:43,659 [INFO] Step[1150/2713]: training loss : 0.9518142032623291 TRAIN  loss dict:  {'classification_loss': 0.9518142032623291}
2025-01-14 18:24:57,146 [INFO] Step[1200/2713]: training loss : 0.9627751266956329 TRAIN  loss dict:  {'classification_loss': 0.9627751266956329}
2025-01-14 18:25:10,477 [INFO] Step[1250/2713]: training loss : 0.9530183291435241 TRAIN  loss dict:  {'classification_loss': 0.9530183291435241}
2025-01-14 18:25:23,941 [INFO] Step[1300/2713]: training loss : 0.9527671253681183 TRAIN  loss dict:  {'classification_loss': 0.9527671253681183}
2025-01-14 18:25:37,517 [INFO] Step[1350/2713]: training loss : 0.9636489105224609 TRAIN  loss dict:  {'classification_loss': 0.9636489105224609}
2025-01-14 18:25:50,894 [INFO] Step[1400/2713]: training loss : 0.9901726496219635 TRAIN  loss dict:  {'classification_loss': 0.9901726496219635}
2025-01-14 18:26:04,723 [INFO] Step[1450/2713]: training loss : 0.9551465046405793 TRAIN  loss dict:  {'classification_loss': 0.9551465046405793}
2025-01-14 18:26:18,840 [INFO] Step[1500/2713]: training loss : 0.9568581128120422 TRAIN  loss dict:  {'classification_loss': 0.9568581128120422}
2025-01-14 18:26:32,976 [INFO] Step[1550/2713]: training loss : 0.9548421704769134 TRAIN  loss dict:  {'classification_loss': 0.9548421704769134}
2025-01-14 18:26:46,491 [INFO] Step[1600/2713]: training loss : 0.9849866557121277 TRAIN  loss dict:  {'classification_loss': 0.9849866557121277}
2025-01-14 18:27:00,185 [INFO] Step[1650/2713]: training loss : 0.9667238390445709 TRAIN  loss dict:  {'classification_loss': 0.9667238390445709}
2025-01-14 18:27:13,975 [INFO] Step[1700/2713]: training loss : 0.959068523645401 TRAIN  loss dict:  {'classification_loss': 0.959068523645401}
2025-01-14 18:27:27,778 [INFO] Step[1750/2713]: training loss : 0.9577680397033691 TRAIN  loss dict:  {'classification_loss': 0.9577680397033691}
2025-01-14 18:27:41,199 [INFO] Step[1800/2713]: training loss : 0.9699589169025421 TRAIN  loss dict:  {'classification_loss': 0.9699589169025421}
2025-01-14 18:27:55,068 [INFO] Step[1850/2713]: training loss : 0.9550977993011475 TRAIN  loss dict:  {'classification_loss': 0.9550977993011475}
2025-01-14 18:28:09,295 [INFO] Step[1900/2713]: training loss : 0.9577832162380219 TRAIN  loss dict:  {'classification_loss': 0.9577832162380219}
2025-01-14 18:28:23,127 [INFO] Step[1950/2713]: training loss : 0.9583308064937591 TRAIN  loss dict:  {'classification_loss': 0.9583308064937591}
2025-01-14 18:28:37,260 [INFO] Step[2000/2713]: training loss : 0.9621841108798981 TRAIN  loss dict:  {'classification_loss': 0.9621841108798981}
2025-01-14 18:28:51,040 [INFO] Step[2050/2713]: training loss : 0.9939473509788513 TRAIN  loss dict:  {'classification_loss': 0.9939473509788513}
2025-01-14 18:29:04,855 [INFO] Step[2100/2713]: training loss : 0.9632636880874634 TRAIN  loss dict:  {'classification_loss': 0.9632636880874634}
2025-01-14 18:29:18,234 [INFO] Step[2150/2713]: training loss : 0.9862177789211273 TRAIN  loss dict:  {'classification_loss': 0.9862177789211273}
2025-01-14 18:29:31,708 [INFO] Step[2200/2713]: training loss : 0.9689354813098907 TRAIN  loss dict:  {'classification_loss': 0.9689354813098907}
2025-01-14 18:29:45,388 [INFO] Step[2250/2713]: training loss : 0.9523167622089386 TRAIN  loss dict:  {'classification_loss': 0.9523167622089386}
2025-01-14 18:29:59,031 [INFO] Step[2300/2713]: training loss : 0.9673560702800751 TRAIN  loss dict:  {'classification_loss': 0.9673560702800751}
2025-01-14 18:30:12,377 [INFO] Step[2350/2713]: training loss : 0.9554751074314117 TRAIN  loss dict:  {'classification_loss': 0.9554751074314117}
2025-01-14 18:30:25,824 [INFO] Step[2400/2713]: training loss : 0.9939477682113648 TRAIN  loss dict:  {'classification_loss': 0.9939477682113648}
2025-01-14 18:30:39,051 [INFO] Step[2450/2713]: training loss : 0.9508326852321625 TRAIN  loss dict:  {'classification_loss': 0.9508326852321625}
2025-01-14 18:30:52,721 [INFO] Step[2500/2713]: training loss : 0.95496049284935 TRAIN  loss dict:  {'classification_loss': 0.95496049284935}
2025-01-14 18:31:08,588 [INFO] Step[2550/2713]: training loss : 0.965174252986908 TRAIN  loss dict:  {'classification_loss': 0.965174252986908}
2025-01-14 18:31:23,992 [INFO] Step[2600/2713]: training loss : 0.955490061044693 TRAIN  loss dict:  {'classification_loss': 0.955490061044693}
2025-01-14 18:31:37,249 [INFO] Step[2650/2713]: training loss : 0.9551711392402649 TRAIN  loss dict:  {'classification_loss': 0.9551711392402649}
2025-01-14 18:31:50,867 [INFO] Step[2700/2713]: training loss : 0.9504173409938812 TRAIN  loss dict:  {'classification_loss': 0.9504173409938812}
2025-01-14 18:33:06,705 [INFO] Label accuracies statistics:
2025-01-14 18:33:06,705 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.5, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 1.0, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.5, 276: 1.0, 277: 0.5, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.5, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.25, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.5, 355: 1.0, 356: 0.5, 357: 0.75, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 18:33:07,881 [INFO] [31] TRAIN  loss: 0.9666202437864473 acc: 0.99533112175943
2025-01-14 18:33:07,881 [INFO] [31] TRAIN  loss dict: {'classification_loss': 0.9666202437864473}
2025-01-14 18:33:07,881 [INFO] [31] VALIDATION loss: 1.8004356961053116 VALIDATION acc: 0.7887147335423198
2025-01-14 18:33:07,881 [INFO] [31] VALIDATION loss dict: {'classification_loss': 1.8004356961053116}
2025-01-14 18:33:07,881 [INFO] 
2025-01-14 18:33:26,696 [INFO] Step[50/2713]: training loss : 0.9935310757160187 TRAIN  loss dict:  {'classification_loss': 0.9935310757160187}
2025-01-14 18:33:40,123 [INFO] Step[100/2713]: training loss : 0.9580993866920471 TRAIN  loss dict:  {'classification_loss': 0.9580993866920471}
2025-01-14 18:33:53,866 [INFO] Step[150/2713]: training loss : 0.9561519634723663 TRAIN  loss dict:  {'classification_loss': 0.9561519634723663}
2025-01-14 18:34:08,029 [INFO] Step[200/2713]: training loss : 0.9614428019523621 TRAIN  loss dict:  {'classification_loss': 0.9614428019523621}
2025-01-14 18:34:21,332 [INFO] Step[250/2713]: training loss : 0.9589481019973755 TRAIN  loss dict:  {'classification_loss': 0.9589481019973755}
2025-01-14 18:34:35,185 [INFO] Step[300/2713]: training loss : 0.953356157541275 TRAIN  loss dict:  {'classification_loss': 0.953356157541275}
2025-01-14 18:34:48,431 [INFO] Step[350/2713]: training loss : 0.9554255366325378 TRAIN  loss dict:  {'classification_loss': 0.9554255366325378}
2025-01-14 18:35:01,685 [INFO] Step[400/2713]: training loss : 0.9482591950893402 TRAIN  loss dict:  {'classification_loss': 0.9482591950893402}
2025-01-14 18:35:15,735 [INFO] Step[450/2713]: training loss : 0.9599278724193573 TRAIN  loss dict:  {'classification_loss': 0.9599278724193573}
2025-01-14 18:35:29,689 [INFO] Step[500/2713]: training loss : 0.9722214663028717 TRAIN  loss dict:  {'classification_loss': 0.9722214663028717}
2025-01-14 18:35:42,993 [INFO] Step[550/2713]: training loss : 0.9493332922458648 TRAIN  loss dict:  {'classification_loss': 0.9493332922458648}
2025-01-14 18:35:56,821 [INFO] Step[600/2713]: training loss : 0.9553150928020477 TRAIN  loss dict:  {'classification_loss': 0.9553150928020477}
2025-01-14 18:36:10,768 [INFO] Step[650/2713]: training loss : 0.9561062395572663 TRAIN  loss dict:  {'classification_loss': 0.9561062395572663}
2025-01-14 18:36:27,378 [INFO] Step[700/2713]: training loss : 0.9511804389953613 TRAIN  loss dict:  {'classification_loss': 0.9511804389953613}
2025-01-14 18:36:40,956 [INFO] Step[750/2713]: training loss : 0.948161655664444 TRAIN  loss dict:  {'classification_loss': 0.948161655664444}
2025-01-14 18:36:54,543 [INFO] Step[800/2713]: training loss : 0.9514834260940552 TRAIN  loss dict:  {'classification_loss': 0.9514834260940552}
2025-01-14 18:37:08,433 [INFO] Step[850/2713]: training loss : 0.9506275808811188 TRAIN  loss dict:  {'classification_loss': 0.9506275808811188}
2025-01-14 18:37:22,329 [INFO] Step[900/2713]: training loss : 0.9561212241649628 TRAIN  loss dict:  {'classification_loss': 0.9561212241649628}
2025-01-14 18:37:36,020 [INFO] Step[950/2713]: training loss : 0.9464318406581879 TRAIN  loss dict:  {'classification_loss': 0.9464318406581879}
2025-01-14 18:37:49,625 [INFO] Step[1000/2713]: training loss : 0.9500199091434479 TRAIN  loss dict:  {'classification_loss': 0.9500199091434479}
2025-01-14 18:38:02,900 [INFO] Step[1050/2713]: training loss : 0.9727199220657349 TRAIN  loss dict:  {'classification_loss': 0.9727199220657349}
2025-01-14 18:38:16,684 [INFO] Step[1100/2713]: training loss : 0.9515915060043335 TRAIN  loss dict:  {'classification_loss': 0.9515915060043335}
2025-01-14 18:38:30,706 [INFO] Step[1150/2713]: training loss : 0.9565326845645905 TRAIN  loss dict:  {'classification_loss': 0.9565326845645905}
2025-01-14 18:38:44,830 [INFO] Step[1200/2713]: training loss : 0.9530977153778076 TRAIN  loss dict:  {'classification_loss': 0.9530977153778076}
2025-01-14 18:38:58,190 [INFO] Step[1250/2713]: training loss : 0.9679938888549805 TRAIN  loss dict:  {'classification_loss': 0.9679938888549805}
2025-01-14 18:39:11,765 [INFO] Step[1300/2713]: training loss : 0.9635218298435211 TRAIN  loss dict:  {'classification_loss': 0.9635218298435211}
2025-01-14 18:39:24,988 [INFO] Step[1350/2713]: training loss : 0.9621305620670318 TRAIN  loss dict:  {'classification_loss': 0.9621305620670318}
2025-01-14 18:39:38,740 [INFO] Step[1400/2713]: training loss : 0.9668004047870636 TRAIN  loss dict:  {'classification_loss': 0.9668004047870636}
2025-01-14 18:39:51,920 [INFO] Step[1450/2713]: training loss : 0.9472521138191223 TRAIN  loss dict:  {'classification_loss': 0.9472521138191223}
2025-01-14 18:40:05,141 [INFO] Step[1500/2713]: training loss : 0.9511598908901214 TRAIN  loss dict:  {'classification_loss': 0.9511598908901214}
2025-01-14 18:40:19,116 [INFO] Step[1550/2713]: training loss : 0.9521411538124085 TRAIN  loss dict:  {'classification_loss': 0.9521411538124085}
2025-01-14 18:40:32,896 [INFO] Step[1600/2713]: training loss : 0.9547953844070435 TRAIN  loss dict:  {'classification_loss': 0.9547953844070435}
2025-01-14 18:40:46,462 [INFO] Step[1650/2713]: training loss : 0.9706720781326293 TRAIN  loss dict:  {'classification_loss': 0.9706720781326293}
2025-01-14 18:41:00,290 [INFO] Step[1700/2713]: training loss : 0.9457242035865784 TRAIN  loss dict:  {'classification_loss': 0.9457242035865784}
2025-01-14 18:41:14,539 [INFO] Step[1750/2713]: training loss : 0.9579128396511077 TRAIN  loss dict:  {'classification_loss': 0.9579128396511077}
2025-01-14 18:41:28,192 [INFO] Step[1800/2713]: training loss : 0.9545634973049164 TRAIN  loss dict:  {'classification_loss': 0.9545634973049164}
2025-01-14 18:41:41,900 [INFO] Step[1850/2713]: training loss : 0.9850803995132447 TRAIN  loss dict:  {'classification_loss': 0.9850803995132447}
2025-01-14 18:41:55,723 [INFO] Step[1900/2713]: training loss : 0.9574441814422607 TRAIN  loss dict:  {'classification_loss': 0.9574441814422607}
2025-01-14 18:42:08,929 [INFO] Step[1950/2713]: training loss : 0.9857347810268402 TRAIN  loss dict:  {'classification_loss': 0.9857347810268402}
2025-01-14 18:42:23,113 [INFO] Step[2000/2713]: training loss : 0.9792601716518402 TRAIN  loss dict:  {'classification_loss': 0.9792601716518402}
2025-01-14 18:42:37,170 [INFO] Step[2050/2713]: training loss : 0.9689597797393799 TRAIN  loss dict:  {'classification_loss': 0.9689597797393799}
2025-01-14 18:42:50,349 [INFO] Step[2100/2713]: training loss : 0.9536841261386871 TRAIN  loss dict:  {'classification_loss': 0.9536841261386871}
2025-01-14 18:43:04,111 [INFO] Step[2150/2713]: training loss : 0.9724224174022674 TRAIN  loss dict:  {'classification_loss': 0.9724224174022674}
2025-01-14 18:43:17,353 [INFO] Step[2200/2713]: training loss : 0.9630323314666748 TRAIN  loss dict:  {'classification_loss': 0.9630323314666748}
2025-01-14 18:43:31,124 [INFO] Step[2250/2713]: training loss : 0.9695928955078125 TRAIN  loss dict:  {'classification_loss': 0.9695928955078125}
2025-01-14 18:43:44,711 [INFO] Step[2300/2713]: training loss : 0.9607239639759064 TRAIN  loss dict:  {'classification_loss': 0.9607239639759064}
2025-01-14 18:43:58,526 [INFO] Step[2350/2713]: training loss : 0.9612665736675262 TRAIN  loss dict:  {'classification_loss': 0.9612665736675262}
2025-01-14 18:44:11,860 [INFO] Step[2400/2713]: training loss : 0.9649479711055755 TRAIN  loss dict:  {'classification_loss': 0.9649479711055755}
2025-01-14 18:44:25,677 [INFO] Step[2450/2713]: training loss : 0.9540344214439392 TRAIN  loss dict:  {'classification_loss': 0.9540344214439392}
2025-01-14 18:44:39,612 [INFO] Step[2500/2713]: training loss : 0.9656477081775665 TRAIN  loss dict:  {'classification_loss': 0.9656477081775665}
2025-01-14 18:44:53,845 [INFO] Step[2550/2713]: training loss : 0.9483908891677857 TRAIN  loss dict:  {'classification_loss': 0.9483908891677857}
2025-01-14 18:45:07,592 [INFO] Step[2600/2713]: training loss : 0.9497880184650421 TRAIN  loss dict:  {'classification_loss': 0.9497880184650421}
2025-01-14 18:45:21,630 [INFO] Step[2650/2713]: training loss : 0.9609959638118744 TRAIN  loss dict:  {'classification_loss': 0.9609959638118744}
2025-01-14 18:45:34,937 [INFO] Step[2700/2713]: training loss : 0.9543122375011444 TRAIN  loss dict:  {'classification_loss': 0.9543122375011444}
2025-01-14 18:46:51,599 [INFO] Label accuracies statistics:
2025-01-14 18:46:51,599 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.5, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.25, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.5, 290: 0.75, 291: 1.0, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 0.75, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 1.0, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 18:46:51,601 [INFO] [32] TRAIN  loss: 0.9594843675850327 acc: 0.9971741000122866
2025-01-14 18:46:51,601 [INFO] [32] TRAIN  loss dict: {'classification_loss': 0.9594843675850327}
2025-01-14 18:46:51,601 [INFO] [32] VALIDATION loss: 1.8740778664000948 VALIDATION acc: 0.7824451410658307
2025-01-14 18:46:51,601 [INFO] [32] VALIDATION loss dict: {'classification_loss': 1.8740778664000948}
2025-01-14 18:46:51,601 [INFO] 
2025-01-14 18:47:11,592 [INFO] Step[50/2713]: training loss : 0.954974056482315 TRAIN  loss dict:  {'classification_loss': 0.954974056482315}
2025-01-14 18:47:25,172 [INFO] Step[100/2713]: training loss : 0.9529831850528717 TRAIN  loss dict:  {'classification_loss': 0.9529831850528717}
2025-01-14 18:47:39,027 [INFO] Step[150/2713]: training loss : 0.9561530733108521 TRAIN  loss dict:  {'classification_loss': 0.9561530733108521}
2025-01-14 18:47:52,644 [INFO] Step[200/2713]: training loss : 0.9916860699653626 TRAIN  loss dict:  {'classification_loss': 0.9916860699653626}
2025-01-14 18:48:06,497 [INFO] Step[250/2713]: training loss : 0.9674875092506409 TRAIN  loss dict:  {'classification_loss': 0.9674875092506409}
2025-01-14 18:48:20,743 [INFO] Step[300/2713]: training loss : 0.9502933263778687 TRAIN  loss dict:  {'classification_loss': 0.9502933263778687}
2025-01-14 18:48:34,123 [INFO] Step[350/2713]: training loss : 0.9572635221481324 TRAIN  loss dict:  {'classification_loss': 0.9572635221481324}
2025-01-14 18:48:47,836 [INFO] Step[400/2713]: training loss : 0.9515910160541534 TRAIN  loss dict:  {'classification_loss': 0.9515910160541534}
2025-01-14 18:49:01,774 [INFO] Step[450/2713]: training loss : 0.9473369741439819 TRAIN  loss dict:  {'classification_loss': 0.9473369741439819}
2025-01-14 18:49:15,574 [INFO] Step[500/2713]: training loss : 0.9513591241836548 TRAIN  loss dict:  {'classification_loss': 0.9513591241836548}
2025-01-14 18:49:29,411 [INFO] Step[550/2713]: training loss : 0.945674147605896 TRAIN  loss dict:  {'classification_loss': 0.945674147605896}
2025-01-14 18:49:43,583 [INFO] Step[600/2713]: training loss : 0.9529860091209411 TRAIN  loss dict:  {'classification_loss': 0.9529860091209411}
2025-01-14 18:49:57,551 [INFO] Step[650/2713]: training loss : 0.9503791499137878 TRAIN  loss dict:  {'classification_loss': 0.9503791499137878}
2025-01-14 18:50:11,390 [INFO] Step[700/2713]: training loss : 0.950690039396286 TRAIN  loss dict:  {'classification_loss': 0.950690039396286}
2025-01-14 18:50:25,368 [INFO] Step[750/2713]: training loss : 0.9655527627468109 TRAIN  loss dict:  {'classification_loss': 0.9655527627468109}
2025-01-14 18:50:39,073 [INFO] Step[800/2713]: training loss : 0.9915562427043915 TRAIN  loss dict:  {'classification_loss': 0.9915562427043915}
2025-01-14 18:50:52,570 [INFO] Step[850/2713]: training loss : 0.9627736830711364 TRAIN  loss dict:  {'classification_loss': 0.9627736830711364}
2025-01-14 18:51:05,767 [INFO] Step[900/2713]: training loss : 0.963283166885376 TRAIN  loss dict:  {'classification_loss': 0.963283166885376}
2025-01-14 18:51:19,447 [INFO] Step[950/2713]: training loss : 0.9535101914405822 TRAIN  loss dict:  {'classification_loss': 0.9535101914405822}
2025-01-14 18:51:33,655 [INFO] Step[1000/2713]: training loss : 0.9490410017967225 TRAIN  loss dict:  {'classification_loss': 0.9490410017967225}
2025-01-14 18:51:47,381 [INFO] Step[1050/2713]: training loss : 1.0179480576515199 TRAIN  loss dict:  {'classification_loss': 1.0179480576515199}
2025-01-14 18:52:01,027 [INFO] Step[1100/2713]: training loss : 0.9858293020725251 TRAIN  loss dict:  {'classification_loss': 0.9858293020725251}
2025-01-14 18:52:15,099 [INFO] Step[1150/2713]: training loss : 0.9464825439453125 TRAIN  loss dict:  {'classification_loss': 0.9464825439453125}
2025-01-14 18:52:28,718 [INFO] Step[1200/2713]: training loss : 0.9493747282028199 TRAIN  loss dict:  {'classification_loss': 0.9493747282028199}
2025-01-14 18:52:42,327 [INFO] Step[1250/2713]: training loss : 0.9586007297039032 TRAIN  loss dict:  {'classification_loss': 0.9586007297039032}
2025-01-14 18:52:57,223 [INFO] Step[1300/2713]: training loss : 0.9520236766338348 TRAIN  loss dict:  {'classification_loss': 0.9520236766338348}
2025-01-14 18:53:12,930 [INFO] Step[1350/2713]: training loss : 0.9515423810482025 TRAIN  loss dict:  {'classification_loss': 0.9515423810482025}
2025-01-14 18:53:26,626 [INFO] Step[1400/2713]: training loss : 0.9630354952812195 TRAIN  loss dict:  {'classification_loss': 0.9630354952812195}
2025-01-14 18:53:39,883 [INFO] Step[1450/2713]: training loss : 0.9700282907485962 TRAIN  loss dict:  {'classification_loss': 0.9700282907485962}
2025-01-14 18:53:53,651 [INFO] Step[1500/2713]: training loss : 0.9450744295120239 TRAIN  loss dict:  {'classification_loss': 0.9450744295120239}
2025-01-14 18:54:07,629 [INFO] Step[1550/2713]: training loss : 0.9520610105991364 TRAIN  loss dict:  {'classification_loss': 0.9520610105991364}
2025-01-14 18:54:21,528 [INFO] Step[1600/2713]: training loss : 0.9586453723907471 TRAIN  loss dict:  {'classification_loss': 0.9586453723907471}
2025-01-14 18:54:35,611 [INFO] Step[1650/2713]: training loss : 0.9650612425804138 TRAIN  loss dict:  {'classification_loss': 0.9650612425804138}
2025-01-14 18:54:49,514 [INFO] Step[1700/2713]: training loss : 0.9627012729644775 TRAIN  loss dict:  {'classification_loss': 0.9627012729644775}
2025-01-14 18:55:03,466 [INFO] Step[1750/2713]: training loss : 0.9501523661613465 TRAIN  loss dict:  {'classification_loss': 0.9501523661613465}
2025-01-14 18:55:17,500 [INFO] Step[1800/2713]: training loss : 0.9512428891658783 TRAIN  loss dict:  {'classification_loss': 0.9512428891658783}
2025-01-14 18:55:31,353 [INFO] Step[1850/2713]: training loss : 0.9608156049251556 TRAIN  loss dict:  {'classification_loss': 0.9608156049251556}
2025-01-14 18:55:48,141 [INFO] Step[1900/2713]: training loss : 0.9542858850955963 TRAIN  loss dict:  {'classification_loss': 0.9542858850955963}
2025-01-14 18:56:02,681 [INFO] Step[1950/2713]: training loss : 0.9695266473293305 TRAIN  loss dict:  {'classification_loss': 0.9695266473293305}
2025-01-14 18:56:16,230 [INFO] Step[2000/2713]: training loss : 0.9502726888656616 TRAIN  loss dict:  {'classification_loss': 0.9502726888656616}
2025-01-14 18:56:29,832 [INFO] Step[2050/2713]: training loss : 0.9591827416419982 TRAIN  loss dict:  {'classification_loss': 0.9591827416419982}
2025-01-14 18:56:45,676 [INFO] Step[2100/2713]: training loss : 0.9704985344409942 TRAIN  loss dict:  {'classification_loss': 0.9704985344409942}
2025-01-14 18:56:59,721 [INFO] Step[2150/2713]: training loss : 0.9585279822349548 TRAIN  loss dict:  {'classification_loss': 0.9585279822349548}
2025-01-14 18:57:13,330 [INFO] Step[2200/2713]: training loss : 0.9543465065956116 TRAIN  loss dict:  {'classification_loss': 0.9543465065956116}
2025-01-14 18:57:26,586 [INFO] Step[2250/2713]: training loss : 0.9521667456626892 TRAIN  loss dict:  {'classification_loss': 0.9521667456626892}
2025-01-14 18:57:40,159 [INFO] Step[2300/2713]: training loss : 0.9533344280719757 TRAIN  loss dict:  {'classification_loss': 0.9533344280719757}
2025-01-14 18:57:55,957 [INFO] Step[2350/2713]: training loss : 0.9554126751422882 TRAIN  loss dict:  {'classification_loss': 0.9554126751422882}
2025-01-14 18:58:11,503 [INFO] Step[2400/2713]: training loss : 0.9515593373775482 TRAIN  loss dict:  {'classification_loss': 0.9515593373775482}
2025-01-14 18:58:24,805 [INFO] Step[2450/2713]: training loss : 0.9550545108318329 TRAIN  loss dict:  {'classification_loss': 0.9550545108318329}
2025-01-14 18:58:38,456 [INFO] Step[2500/2713]: training loss : 0.9567023169994354 TRAIN  loss dict:  {'classification_loss': 0.9567023169994354}
2025-01-14 18:58:52,350 [INFO] Step[2550/2713]: training loss : 0.9568991398811341 TRAIN  loss dict:  {'classification_loss': 0.9568991398811341}
2025-01-14 18:59:05,580 [INFO] Step[2600/2713]: training loss : 0.9631985139846801 TRAIN  loss dict:  {'classification_loss': 0.9631985139846801}
2025-01-14 18:59:18,858 [INFO] Step[2650/2713]: training loss : 0.9644378554821015 TRAIN  loss dict:  {'classification_loss': 0.9644378554821015}
2025-01-14 18:59:32,105 [INFO] Step[2700/2713]: training loss : 0.9565089058876037 TRAIN  loss dict:  {'classification_loss': 0.9565089058876037}
2025-01-14 19:00:50,685 [INFO] Label accuracies statistics:
2025-01-14 19:00:50,685 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.0, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.5, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.75, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.5, 116: 0.5, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.25, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.75, 191: 0.75, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.5, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.5, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.25, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.5, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.25, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 19:00:50,687 [INFO] [33] TRAIN  loss: 0.9590212972323217 acc: 0.9964369087111439
2025-01-14 19:00:50,687 [INFO] [33] TRAIN  loss dict: {'classification_loss': 0.9590212972323217}
2025-01-14 19:00:50,687 [INFO] [33] VALIDATION loss: 1.8385998541699315 VALIDATION acc: 0.7880877742946708
2025-01-14 19:00:50,688 [INFO] [33] VALIDATION loss dict: {'classification_loss': 1.8385998541699315}
2025-01-14 19:00:50,688 [INFO] 
2025-01-14 19:01:09,944 [INFO] Step[50/2713]: training loss : 0.9544314730167389 TRAIN  loss dict:  {'classification_loss': 0.9544314730167389}
2025-01-14 19:01:23,183 [INFO] Step[100/2713]: training loss : 0.9736598658561707 TRAIN  loss dict:  {'classification_loss': 0.9736598658561707}
2025-01-14 19:01:36,837 [INFO] Step[150/2713]: training loss : 0.9484556877613067 TRAIN  loss dict:  {'classification_loss': 0.9484556877613067}
2025-01-14 19:01:50,091 [INFO] Step[200/2713]: training loss : 0.9528231525421142 TRAIN  loss dict:  {'classification_loss': 0.9528231525421142}
2025-01-14 19:02:03,885 [INFO] Step[250/2713]: training loss : 0.9523660004138946 TRAIN  loss dict:  {'classification_loss': 0.9523660004138946}
2025-01-14 19:02:17,719 [INFO] Step[300/2713]: training loss : 0.9488103866577149 TRAIN  loss dict:  {'classification_loss': 0.9488103866577149}
2025-01-14 19:02:31,306 [INFO] Step[350/2713]: training loss : 0.955670976638794 TRAIN  loss dict:  {'classification_loss': 0.955670976638794}
2025-01-14 19:02:44,937 [INFO] Step[400/2713]: training loss : 0.9461195588111877 TRAIN  loss dict:  {'classification_loss': 0.9461195588111877}
2025-01-14 19:02:58,776 [INFO] Step[450/2713]: training loss : 0.9566809284687042 TRAIN  loss dict:  {'classification_loss': 0.9566809284687042}
2025-01-14 19:03:12,436 [INFO] Step[500/2713]: training loss : 0.9555926167964935 TRAIN  loss dict:  {'classification_loss': 0.9555926167964935}
2025-01-14 19:03:26,020 [INFO] Step[550/2713]: training loss : 0.9490296852588653 TRAIN  loss dict:  {'classification_loss': 0.9490296852588653}
2025-01-14 19:03:39,927 [INFO] Step[600/2713]: training loss : 0.9537272524833679 TRAIN  loss dict:  {'classification_loss': 0.9537272524833679}
2025-01-14 19:03:53,821 [INFO] Step[650/2713]: training loss : 0.9440181934833527 TRAIN  loss dict:  {'classification_loss': 0.9440181934833527}
2025-01-14 19:04:07,422 [INFO] Step[700/2713]: training loss : 0.9462049996852875 TRAIN  loss dict:  {'classification_loss': 0.9462049996852875}
2025-01-14 19:04:21,654 [INFO] Step[750/2713]: training loss : 0.9653779995441437 TRAIN  loss dict:  {'classification_loss': 0.9653779995441437}
2025-01-14 19:04:35,509 [INFO] Step[800/2713]: training loss : 0.9458813154697419 TRAIN  loss dict:  {'classification_loss': 0.9458813154697419}
2025-01-14 19:04:49,657 [INFO] Step[850/2713]: training loss : 0.9772569561004638 TRAIN  loss dict:  {'classification_loss': 0.9772569561004638}
2025-01-14 19:05:02,909 [INFO] Step[900/2713]: training loss : 0.9657847857475281 TRAIN  loss dict:  {'classification_loss': 0.9657847857475281}
2025-01-14 19:05:16,713 [INFO] Step[950/2713]: training loss : 0.9668459963798522 TRAIN  loss dict:  {'classification_loss': 0.9668459963798522}
2025-01-14 19:05:30,539 [INFO] Step[1000/2713]: training loss : 0.9723022532463074 TRAIN  loss dict:  {'classification_loss': 0.9723022532463074}
2025-01-14 19:05:44,818 [INFO] Step[1050/2713]: training loss : 0.9855141079425812 TRAIN  loss dict:  {'classification_loss': 0.9855141079425812}
2025-01-14 19:05:59,035 [INFO] Step[1100/2713]: training loss : 0.9421820199489593 TRAIN  loss dict:  {'classification_loss': 0.9421820199489593}
2025-01-14 19:06:12,471 [INFO] Step[1150/2713]: training loss : 0.9455977952480317 TRAIN  loss dict:  {'classification_loss': 0.9455977952480317}
2025-01-14 19:06:26,119 [INFO] Step[1200/2713]: training loss : 0.9694937312602997 TRAIN  loss dict:  {'classification_loss': 0.9694937312602997}
2025-01-14 19:06:39,703 [INFO] Step[1250/2713]: training loss : 0.9570057189464569 TRAIN  loss dict:  {'classification_loss': 0.9570057189464569}
2025-01-14 19:06:52,863 [INFO] Step[1300/2713]: training loss : 0.9543921840190888 TRAIN  loss dict:  {'classification_loss': 0.9543921840190888}
2025-01-14 19:07:06,447 [INFO] Step[1350/2713]: training loss : 0.952748430967331 TRAIN  loss dict:  {'classification_loss': 0.952748430967331}
2025-01-14 19:07:19,691 [INFO] Step[1400/2713]: training loss : 0.9663106942176819 TRAIN  loss dict:  {'classification_loss': 0.9663106942176819}
2025-01-14 19:07:33,483 [INFO] Step[1450/2713]: training loss : 0.9486171197891236 TRAIN  loss dict:  {'classification_loss': 0.9486171197891236}
2025-01-14 19:07:46,737 [INFO] Step[1500/2713]: training loss : 0.9695099449157715 TRAIN  loss dict:  {'classification_loss': 0.9695099449157715}
2025-01-14 19:08:00,380 [INFO] Step[1550/2713]: training loss : 0.9535221624374389 TRAIN  loss dict:  {'classification_loss': 0.9535221624374389}
2025-01-14 19:08:13,895 [INFO] Step[1600/2713]: training loss : 0.951117033958435 TRAIN  loss dict:  {'classification_loss': 0.951117033958435}
2025-01-14 19:08:27,504 [INFO] Step[1650/2713]: training loss : 0.964593596458435 TRAIN  loss dict:  {'classification_loss': 0.964593596458435}
2025-01-14 19:08:41,503 [INFO] Step[1700/2713]: training loss : 0.953484833240509 TRAIN  loss dict:  {'classification_loss': 0.953484833240509}
2025-01-14 19:08:55,367 [INFO] Step[1750/2713]: training loss : 0.9471675968170166 TRAIN  loss dict:  {'classification_loss': 0.9471675968170166}
2025-01-14 19:09:08,687 [INFO] Step[1800/2713]: training loss : 0.9461916399002075 TRAIN  loss dict:  {'classification_loss': 0.9461916399002075}
2025-01-14 19:09:22,635 [INFO] Step[1850/2713]: training loss : 0.9842922496795654 TRAIN  loss dict:  {'classification_loss': 0.9842922496795654}
2025-01-14 19:09:36,319 [INFO] Step[1900/2713]: training loss : 0.9486693418025971 TRAIN  loss dict:  {'classification_loss': 0.9486693418025971}
2025-01-14 19:09:50,308 [INFO] Step[1950/2713]: training loss : 0.9673182404041291 TRAIN  loss dict:  {'classification_loss': 0.9673182404041291}
2025-01-14 19:10:03,812 [INFO] Step[2000/2713]: training loss : 0.9974115133285523 TRAIN  loss dict:  {'classification_loss': 0.9974115133285523}
2025-01-14 19:10:17,373 [INFO] Step[2050/2713]: training loss : 0.973984397649765 TRAIN  loss dict:  {'classification_loss': 0.973984397649765}
2025-01-14 19:10:30,985 [INFO] Step[2100/2713]: training loss : 0.9544541442394257 TRAIN  loss dict:  {'classification_loss': 0.9544541442394257}
2025-01-14 19:10:44,681 [INFO] Step[2150/2713]: training loss : 0.9784946942329407 TRAIN  loss dict:  {'classification_loss': 0.9784946942329407}
2025-01-14 19:10:58,373 [INFO] Step[2200/2713]: training loss : 0.9569107234477997 TRAIN  loss dict:  {'classification_loss': 0.9569107234477997}
2025-01-14 19:11:11,752 [INFO] Step[2250/2713]: training loss : 0.9501048171520233 TRAIN  loss dict:  {'classification_loss': 0.9501048171520233}
2025-01-14 19:11:25,974 [INFO] Step[2300/2713]: training loss : 0.9585901975631714 TRAIN  loss dict:  {'classification_loss': 0.9585901975631714}
2025-01-14 19:11:39,552 [INFO] Step[2350/2713]: training loss : 0.9595660924911499 TRAIN  loss dict:  {'classification_loss': 0.9595660924911499}
2025-01-14 19:11:53,595 [INFO] Step[2400/2713]: training loss : 0.9510511600971222 TRAIN  loss dict:  {'classification_loss': 0.9510511600971222}
2025-01-14 19:12:07,131 [INFO] Step[2450/2713]: training loss : 0.9511255669593811 TRAIN  loss dict:  {'classification_loss': 0.9511255669593811}
2025-01-14 19:12:21,036 [INFO] Step[2500/2713]: training loss : 0.9441374695301056 TRAIN  loss dict:  {'classification_loss': 0.9441374695301056}
2025-01-14 19:12:34,750 [INFO] Step[2550/2713]: training loss : 0.9656553447246552 TRAIN  loss dict:  {'classification_loss': 0.9656553447246552}
2025-01-14 19:12:48,722 [INFO] Step[2600/2713]: training loss : 0.9464011347293854 TRAIN  loss dict:  {'classification_loss': 0.9464011347293854}
2025-01-14 19:13:02,061 [INFO] Step[2650/2713]: training loss : 0.9507038748264313 TRAIN  loss dict:  {'classification_loss': 0.9507038748264313}
2025-01-14 19:13:15,278 [INFO] Step[2700/2713]: training loss : 0.9450608384609223 TRAIN  loss dict:  {'classification_loss': 0.9450608384609223}
2025-01-14 19:14:32,230 [INFO] Label accuracies statistics:
2025-01-14 19:14:32,230 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 1.0, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.0, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 1.0, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 1.0, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 19:14:32,231 [INFO] [34] TRAIN  loss: 0.9578286601395963 acc: 0.9964369087111439
2025-01-14 19:14:32,231 [INFO] [34] TRAIN  loss dict: {'classification_loss': 0.9578286601395963}
2025-01-14 19:14:32,232 [INFO] [34] VALIDATION loss: 1.8179202578345637 VALIDATION acc: 0.7924764890282132
2025-01-14 19:14:32,232 [INFO] [34] VALIDATION loss dict: {'classification_loss': 1.8179202578345637}
2025-01-14 19:14:32,232 [INFO] 
2025-01-14 19:14:50,896 [INFO] Step[50/2713]: training loss : 0.9462641334533691 TRAIN  loss dict:  {'classification_loss': 0.9462641334533691}
2025-01-14 19:15:04,144 [INFO] Step[100/2713]: training loss : 0.966959388256073 TRAIN  loss dict:  {'classification_loss': 0.966959388256073}
2025-01-14 19:15:18,011 [INFO] Step[150/2713]: training loss : 0.9439555323123932 TRAIN  loss dict:  {'classification_loss': 0.9439555323123932}
2025-01-14 19:15:31,375 [INFO] Step[200/2713]: training loss : 0.9514316046237945 TRAIN  loss dict:  {'classification_loss': 0.9514316046237945}
2025-01-14 19:15:44,957 [INFO] Step[250/2713]: training loss : 0.9516845858097076 TRAIN  loss dict:  {'classification_loss': 0.9516845858097076}
2025-01-14 19:15:58,247 [INFO] Step[300/2713]: training loss : 0.984936329126358 TRAIN  loss dict:  {'classification_loss': 0.984936329126358}
2025-01-14 19:16:11,867 [INFO] Step[350/2713]: training loss : 0.9571068334579468 TRAIN  loss dict:  {'classification_loss': 0.9571068334579468}
2025-01-14 19:16:25,735 [INFO] Step[400/2713]: training loss : 0.9505666160583496 TRAIN  loss dict:  {'classification_loss': 0.9505666160583496}
2025-01-14 19:16:39,446 [INFO] Step[450/2713]: training loss : 0.9500511395931244 TRAIN  loss dict:  {'classification_loss': 0.9500511395931244}
2025-01-14 19:16:53,418 [INFO] Step[500/2713]: training loss : 0.9533513987064361 TRAIN  loss dict:  {'classification_loss': 0.9533513987064361}
2025-01-14 19:17:07,250 [INFO] Step[550/2713]: training loss : 0.9456626272201538 TRAIN  loss dict:  {'classification_loss': 0.9456626272201538}
2025-01-14 19:17:20,436 [INFO] Step[600/2713]: training loss : 0.94869668841362 TRAIN  loss dict:  {'classification_loss': 0.94869668841362}
2025-01-14 19:17:33,646 [INFO] Step[650/2713]: training loss : 0.9833944201469421 TRAIN  loss dict:  {'classification_loss': 0.9833944201469421}
2025-01-14 19:17:47,121 [INFO] Step[700/2713]: training loss : 0.9508021223545075 TRAIN  loss dict:  {'classification_loss': 0.9508021223545075}
2025-01-14 19:18:00,950 [INFO] Step[750/2713]: training loss : 0.9544521081447601 TRAIN  loss dict:  {'classification_loss': 0.9544521081447601}
2025-01-14 19:18:14,572 [INFO] Step[800/2713]: training loss : 0.9516926169395447 TRAIN  loss dict:  {'classification_loss': 0.9516926169395447}
2025-01-14 19:18:28,099 [INFO] Step[850/2713]: training loss : 0.9570981502532959 TRAIN  loss dict:  {'classification_loss': 0.9570981502532959}
2025-01-14 19:18:42,306 [INFO] Step[900/2713]: training loss : 0.9638313734531403 TRAIN  loss dict:  {'classification_loss': 0.9638313734531403}
2025-01-14 19:18:55,982 [INFO] Step[950/2713]: training loss : 0.9585595667362213 TRAIN  loss dict:  {'classification_loss': 0.9585595667362213}
2025-01-14 19:19:09,587 [INFO] Step[1000/2713]: training loss : 0.9473969995975494 TRAIN  loss dict:  {'classification_loss': 0.9473969995975494}
2025-01-14 19:19:22,817 [INFO] Step[1050/2713]: training loss : 0.9470126175880432 TRAIN  loss dict:  {'classification_loss': 0.9470126175880432}
2025-01-14 19:19:36,196 [INFO] Step[1100/2713]: training loss : 0.9542523634433746 TRAIN  loss dict:  {'classification_loss': 0.9542523634433746}
2025-01-14 19:19:50,102 [INFO] Step[1150/2713]: training loss : 0.9487599945068359 TRAIN  loss dict:  {'classification_loss': 0.9487599945068359}
2025-01-14 19:20:03,362 [INFO] Step[1200/2713]: training loss : 0.9530595922470093 TRAIN  loss dict:  {'classification_loss': 0.9530595922470093}
2025-01-14 19:20:16,866 [INFO] Step[1250/2713]: training loss : 0.9528995311260223 TRAIN  loss dict:  {'classification_loss': 0.9528995311260223}
2025-01-14 19:20:30,418 [INFO] Step[1300/2713]: training loss : 0.947771074771881 TRAIN  loss dict:  {'classification_loss': 0.947771074771881}
2025-01-14 19:20:43,637 [INFO] Step[1350/2713]: training loss : 0.9638122510910034 TRAIN  loss dict:  {'classification_loss': 0.9638122510910034}
2025-01-14 19:20:57,504 [INFO] Step[1400/2713]: training loss : 0.9893368065357209 TRAIN  loss dict:  {'classification_loss': 0.9893368065357209}
2025-01-14 19:21:11,579 [INFO] Step[1450/2713]: training loss : 0.9463508343696594 TRAIN  loss dict:  {'classification_loss': 0.9463508343696594}
2025-01-14 19:21:25,267 [INFO] Step[1500/2713]: training loss : 0.9471791005134582 TRAIN  loss dict:  {'classification_loss': 0.9471791005134582}
2025-01-14 19:21:39,430 [INFO] Step[1550/2713]: training loss : 0.966100481748581 TRAIN  loss dict:  {'classification_loss': 0.966100481748581}
2025-01-14 19:21:53,692 [INFO] Step[1600/2713]: training loss : 0.9868641126155854 TRAIN  loss dict:  {'classification_loss': 0.9868641126155854}
2025-01-14 19:22:07,565 [INFO] Step[1650/2713]: training loss : 0.9485127365589142 TRAIN  loss dict:  {'classification_loss': 0.9485127365589142}
2025-01-14 19:22:21,258 [INFO] Step[1700/2713]: training loss : 0.9464159488677979 TRAIN  loss dict:  {'classification_loss': 0.9464159488677979}
2025-01-14 19:22:34,472 [INFO] Step[1750/2713]: training loss : 0.9555784702301026 TRAIN  loss dict:  {'classification_loss': 0.9555784702301026}
2025-01-14 19:22:47,672 [INFO] Step[1800/2713]: training loss : 0.949028388261795 TRAIN  loss dict:  {'classification_loss': 0.949028388261795}
2025-01-14 19:23:01,144 [INFO] Step[1850/2713]: training loss : 0.9619625103473664 TRAIN  loss dict:  {'classification_loss': 0.9619625103473664}
2025-01-14 19:23:14,708 [INFO] Step[1900/2713]: training loss : 0.9554075264930725 TRAIN  loss dict:  {'classification_loss': 0.9554075264930725}
2025-01-14 19:23:28,319 [INFO] Step[1950/2713]: training loss : 0.952337896823883 TRAIN  loss dict:  {'classification_loss': 0.952337896823883}
2025-01-14 19:23:42,124 [INFO] Step[2000/2713]: training loss : 0.9570203471183777 TRAIN  loss dict:  {'classification_loss': 0.9570203471183777}
2025-01-14 19:23:56,121 [INFO] Step[2050/2713]: training loss : 0.9457130718231201 TRAIN  loss dict:  {'classification_loss': 0.9457130718231201}
2025-01-14 19:24:09,328 [INFO] Step[2100/2713]: training loss : 0.9704338836669922 TRAIN  loss dict:  {'classification_loss': 0.9704338836669922}
2025-01-14 19:24:23,388 [INFO] Step[2150/2713]: training loss : 0.9464628958702087 TRAIN  loss dict:  {'classification_loss': 0.9464628958702087}
2025-01-14 19:24:37,365 [INFO] Step[2200/2713]: training loss : 0.9817135381698608 TRAIN  loss dict:  {'classification_loss': 0.9817135381698608}
2025-01-14 19:24:50,755 [INFO] Step[2250/2713]: training loss : 0.968036652803421 TRAIN  loss dict:  {'classification_loss': 0.968036652803421}
2025-01-14 19:25:04,899 [INFO] Step[2300/2713]: training loss : 0.9495588159561157 TRAIN  loss dict:  {'classification_loss': 0.9495588159561157}
2025-01-14 19:25:18,176 [INFO] Step[2350/2713]: training loss : 0.9491512882709503 TRAIN  loss dict:  {'classification_loss': 0.9491512882709503}
2025-01-14 19:25:31,981 [INFO] Step[2400/2713]: training loss : 0.9577032804489136 TRAIN  loss dict:  {'classification_loss': 0.9577032804489136}
2025-01-14 19:25:46,112 [INFO] Step[2450/2713]: training loss : 0.9430995154380798 TRAIN  loss dict:  {'classification_loss': 0.9430995154380798}
2025-01-14 19:25:59,851 [INFO] Step[2500/2713]: training loss : 0.9494381272792816 TRAIN  loss dict:  {'classification_loss': 0.9494381272792816}
2025-01-14 19:26:13,418 [INFO] Step[2550/2713]: training loss : 0.962636079788208 TRAIN  loss dict:  {'classification_loss': 0.962636079788208}
2025-01-14 19:26:27,370 [INFO] Step[2600/2713]: training loss : 0.9494815135002136 TRAIN  loss dict:  {'classification_loss': 0.9494815135002136}
2025-01-14 19:26:41,342 [INFO] Step[2650/2713]: training loss : 0.9604074788093567 TRAIN  loss dict:  {'classification_loss': 0.9604074788093567}
2025-01-14 19:26:55,233 [INFO] Step[2700/2713]: training loss : 0.9470277309417725 TRAIN  loss dict:  {'classification_loss': 0.9470277309417725}
2025-01-14 19:28:11,940 [INFO] Label accuracies statistics:
2025-01-14 19:28:11,940 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.25, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.0, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.5, 187: 1.0, 188: 0.25, 189: 0.5, 190: 0.5, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 1.0, 334: 1.0, 335: 1.0, 336: 0.5, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 19:28:14,245 [INFO] [35] TRAIN  loss: 0.9560456973487849 acc: 0.9975426956628578
2025-01-14 19:28:14,245 [INFO] [35] TRAIN  loss dict: {'classification_loss': 0.9560456973487849}
2025-01-14 19:28:14,245 [INFO] [35] VALIDATION loss: 1.777241130296449 VALIDATION acc: 0.8012539184952978
2025-01-14 19:28:14,245 [INFO] [35] VALIDATION loss dict: {'classification_loss': 1.777241130296449}
2025-01-14 19:28:14,245 [INFO] 
2025-01-14 19:28:32,925 [INFO] Step[50/2713]: training loss : 0.9649089479446411 TRAIN  loss dict:  {'classification_loss': 0.9649089479446411}
2025-01-14 19:28:46,489 [INFO] Step[100/2713]: training loss : 0.9456706786155701 TRAIN  loss dict:  {'classification_loss': 0.9456706786155701}
2025-01-14 19:28:59,722 [INFO] Step[150/2713]: training loss : 0.9559651494026185 TRAIN  loss dict:  {'classification_loss': 0.9559651494026185}
2025-01-14 19:29:13,412 [INFO] Step[200/2713]: training loss : 0.9510977745056153 TRAIN  loss dict:  {'classification_loss': 0.9510977745056153}
2025-01-14 19:29:27,673 [INFO] Step[250/2713]: training loss : 0.9544956922531128 TRAIN  loss dict:  {'classification_loss': 0.9544956922531128}
2025-01-14 19:29:41,495 [INFO] Step[300/2713]: training loss : 0.954349330663681 TRAIN  loss dict:  {'classification_loss': 0.954349330663681}
2025-01-14 19:29:55,202 [INFO] Step[350/2713]: training loss : 0.9467025494575501 TRAIN  loss dict:  {'classification_loss': 0.9467025494575501}
2025-01-14 19:30:09,086 [INFO] Step[400/2713]: training loss : 0.9540625584125518 TRAIN  loss dict:  {'classification_loss': 0.9540625584125518}
2025-01-14 19:30:22,391 [INFO] Step[450/2713]: training loss : 0.9599612998962402 TRAIN  loss dict:  {'classification_loss': 0.9599612998962402}
2025-01-14 19:30:35,910 [INFO] Step[500/2713]: training loss : 0.9460415375232697 TRAIN  loss dict:  {'classification_loss': 0.9460415375232697}
2025-01-14 19:30:49,804 [INFO] Step[550/2713]: training loss : 0.9581243360042572 TRAIN  loss dict:  {'classification_loss': 0.9581243360042572}
2025-01-14 19:31:03,635 [INFO] Step[600/2713]: training loss : 0.9469085443019867 TRAIN  loss dict:  {'classification_loss': 0.9469085443019867}
2025-01-14 19:31:17,424 [INFO] Step[650/2713]: training loss : 0.9931615269184113 TRAIN  loss dict:  {'classification_loss': 0.9931615269184113}
2025-01-14 19:31:30,749 [INFO] Step[700/2713]: training loss : 0.962350800037384 TRAIN  loss dict:  {'classification_loss': 0.962350800037384}
2025-01-14 19:31:44,680 [INFO] Step[750/2713]: training loss : 0.9604028999805451 TRAIN  loss dict:  {'classification_loss': 0.9604028999805451}
2025-01-14 19:31:58,371 [INFO] Step[800/2713]: training loss : 0.949960230588913 TRAIN  loss dict:  {'classification_loss': 0.949960230588913}
2025-01-14 19:32:12,474 [INFO] Step[850/2713]: training loss : 0.9560486984252929 TRAIN  loss dict:  {'classification_loss': 0.9560486984252929}
2025-01-14 19:32:26,377 [INFO] Step[900/2713]: training loss : 0.957058619260788 TRAIN  loss dict:  {'classification_loss': 0.957058619260788}
2025-01-14 19:32:39,679 [INFO] Step[950/2713]: training loss : 0.9475732719898224 TRAIN  loss dict:  {'classification_loss': 0.9475732719898224}
2025-01-14 19:32:53,243 [INFO] Step[1000/2713]: training loss : 0.947116197347641 TRAIN  loss dict:  {'classification_loss': 0.947116197347641}
2025-01-14 19:33:09,306 [INFO] Step[1050/2713]: training loss : 0.9506948518753052 TRAIN  loss dict:  {'classification_loss': 0.9506948518753052}
2025-01-14 19:33:24,526 [INFO] Step[1100/2713]: training loss : 0.9951711618900299 TRAIN  loss dict:  {'classification_loss': 0.9951711618900299}
2025-01-14 19:33:38,479 [INFO] Step[1150/2713]: training loss : 0.9510933482646942 TRAIN  loss dict:  {'classification_loss': 0.9510933482646942}
2025-01-14 19:33:52,665 [INFO] Step[1200/2713]: training loss : 0.9555295920372009 TRAIN  loss dict:  {'classification_loss': 0.9555295920372009}
2025-01-14 19:34:05,864 [INFO] Step[1250/2713]: training loss : 0.9479552662372589 TRAIN  loss dict:  {'classification_loss': 0.9479552662372589}
2025-01-14 19:34:19,432 [INFO] Step[1300/2713]: training loss : 0.9610597205162048 TRAIN  loss dict:  {'classification_loss': 0.9610597205162048}
2025-01-14 19:34:33,687 [INFO] Step[1350/2713]: training loss : 0.9662995660305023 TRAIN  loss dict:  {'classification_loss': 0.9662995660305023}
2025-01-14 19:34:47,137 [INFO] Step[1400/2713]: training loss : 0.9564182198047638 TRAIN  loss dict:  {'classification_loss': 0.9564182198047638}
2025-01-14 19:35:00,773 [INFO] Step[1450/2713]: training loss : 0.9555165505409241 TRAIN  loss dict:  {'classification_loss': 0.9555165505409241}
2025-01-14 19:35:14,019 [INFO] Step[1500/2713]: training loss : 0.9636944687366485 TRAIN  loss dict:  {'classification_loss': 0.9636944687366485}
2025-01-14 19:35:27,258 [INFO] Step[1550/2713]: training loss : 0.9510910713672638 TRAIN  loss dict:  {'classification_loss': 0.9510910713672638}
2025-01-14 19:35:41,334 [INFO] Step[1600/2713]: training loss : 0.9709772443771363 TRAIN  loss dict:  {'classification_loss': 0.9709772443771363}
2025-01-14 19:35:55,366 [INFO] Step[1650/2713]: training loss : 0.9712992978096008 TRAIN  loss dict:  {'classification_loss': 0.9712992978096008}
2025-01-14 19:36:09,205 [INFO] Step[1700/2713]: training loss : 0.944679092168808 TRAIN  loss dict:  {'classification_loss': 0.944679092168808}
2025-01-14 19:36:23,361 [INFO] Step[1750/2713]: training loss : 0.9529309380054474 TRAIN  loss dict:  {'classification_loss': 0.9529309380054474}
2025-01-14 19:36:37,233 [INFO] Step[1800/2713]: training loss : 0.9694051957130432 TRAIN  loss dict:  {'classification_loss': 0.9694051957130432}
2025-01-14 19:36:51,139 [INFO] Step[1850/2713]: training loss : 0.9785571718215942 TRAIN  loss dict:  {'classification_loss': 0.9785571718215942}
2025-01-14 19:37:04,888 [INFO] Step[1900/2713]: training loss : 0.9523754930496215 TRAIN  loss dict:  {'classification_loss': 0.9523754930496215}
2025-01-14 19:37:20,127 [INFO] Step[1950/2713]: training loss : 0.944510509967804 TRAIN  loss dict:  {'classification_loss': 0.944510509967804}
2025-01-14 19:37:35,640 [INFO] Step[2000/2713]: training loss : 0.9610634768009185 TRAIN  loss dict:  {'classification_loss': 0.9610634768009185}
2025-01-14 19:37:49,689 [INFO] Step[2050/2713]: training loss : 0.956471529006958 TRAIN  loss dict:  {'classification_loss': 0.956471529006958}
2025-01-14 19:38:03,345 [INFO] Step[2100/2713]: training loss : 0.9584184813499451 TRAIN  loss dict:  {'classification_loss': 0.9584184813499451}
2025-01-14 19:38:17,211 [INFO] Step[2150/2713]: training loss : 0.9602565515041351 TRAIN  loss dict:  {'classification_loss': 0.9602565515041351}
2025-01-14 19:38:30,888 [INFO] Step[2200/2713]: training loss : 0.9438766193389893 TRAIN  loss dict:  {'classification_loss': 0.9438766193389893}
2025-01-14 19:38:44,139 [INFO] Step[2250/2713]: training loss : 0.9546526622772217 TRAIN  loss dict:  {'classification_loss': 0.9546526622772217}
2025-01-14 19:38:57,998 [INFO] Step[2300/2713]: training loss : 0.9517405688762665 TRAIN  loss dict:  {'classification_loss': 0.9517405688762665}
2025-01-14 19:39:11,786 [INFO] Step[2350/2713]: training loss : 0.9463372325897217 TRAIN  loss dict:  {'classification_loss': 0.9463372325897217}
2025-01-14 19:39:25,072 [INFO] Step[2400/2713]: training loss : 0.9429040467739105 TRAIN  loss dict:  {'classification_loss': 0.9429040467739105}
2025-01-14 19:39:39,027 [INFO] Step[2450/2713]: training loss : 0.9857290041446686 TRAIN  loss dict:  {'classification_loss': 0.9857290041446686}
2025-01-14 19:39:53,264 [INFO] Step[2500/2713]: training loss : 0.9432120048999786 TRAIN  loss dict:  {'classification_loss': 0.9432120048999786}
2025-01-14 19:40:07,508 [INFO] Step[2550/2713]: training loss : 0.9459894502162933 TRAIN  loss dict:  {'classification_loss': 0.9459894502162933}
2025-01-14 19:40:21,438 [INFO] Step[2600/2713]: training loss : 0.9677366697788239 TRAIN  loss dict:  {'classification_loss': 0.9677366697788239}
2025-01-14 19:40:34,798 [INFO] Step[2650/2713]: training loss : 0.9590601098537445 TRAIN  loss dict:  {'classification_loss': 0.9590601098537445}
2025-01-14 19:40:48,384 [INFO] Step[2700/2713]: training loss : 0.967342711687088 TRAIN  loss dict:  {'classification_loss': 0.967342711687088}
2025-01-14 19:42:04,264 [INFO] Label accuracies statistics:
2025-01-14 19:42:04,265 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.5, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.5, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.25, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 0.75, 193: 0.75, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.25, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.0, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 0.5, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 19:42:04,267 [INFO] [36] TRAIN  loss: 0.9573195713101565 acc: 0.9966826391448581
2025-01-14 19:42:04,267 [INFO] [36] TRAIN  loss dict: {'classification_loss': 0.9573195713101565}
2025-01-14 19:42:04,267 [INFO] [36] VALIDATION loss: 1.8395003593505774 VALIDATION acc: 0.7843260188087774
2025-01-14 19:42:04,267 [INFO] [36] VALIDATION loss dict: {'classification_loss': 1.8395003593505774}
2025-01-14 19:42:04,267 [INFO] 
2025-01-14 19:42:23,056 [INFO] Step[50/2713]: training loss : 0.9483157169818878 TRAIN  loss dict:  {'classification_loss': 0.9483157169818878}
2025-01-14 19:42:36,646 [INFO] Step[100/2713]: training loss : 0.9515805745124817 TRAIN  loss dict:  {'classification_loss': 0.9515805745124817}
2025-01-14 19:42:50,753 [INFO] Step[150/2713]: training loss : 0.9524650287628174 TRAIN  loss dict:  {'classification_loss': 0.9524650287628174}
2025-01-14 19:43:04,025 [INFO] Step[200/2713]: training loss : 0.9485299634933472 TRAIN  loss dict:  {'classification_loss': 0.9485299634933472}
2025-01-14 19:43:17,871 [INFO] Step[250/2713]: training loss : 0.9461196148395539 TRAIN  loss dict:  {'classification_loss': 0.9461196148395539}
2025-01-14 19:43:32,091 [INFO] Step[300/2713]: training loss : 0.9576616156101226 TRAIN  loss dict:  {'classification_loss': 0.9576616156101226}
2025-01-14 19:43:45,836 [INFO] Step[350/2713]: training loss : 0.9863900125026703 TRAIN  loss dict:  {'classification_loss': 0.9863900125026703}
2025-01-14 19:43:59,398 [INFO] Step[400/2713]: training loss : 0.9696870732307434 TRAIN  loss dict:  {'classification_loss': 0.9696870732307434}
2025-01-14 19:44:12,762 [INFO] Step[450/2713]: training loss : 0.9494745397567749 TRAIN  loss dict:  {'classification_loss': 0.9494745397567749}
2025-01-14 19:44:26,242 [INFO] Step[500/2713]: training loss : 0.9506147587299347 TRAIN  loss dict:  {'classification_loss': 0.9506147587299347}
2025-01-14 19:44:39,637 [INFO] Step[550/2713]: training loss : 0.9510646617412567 TRAIN  loss dict:  {'classification_loss': 0.9510646617412567}
2025-01-14 19:44:53,575 [INFO] Step[600/2713]: training loss : 0.9504164755344391 TRAIN  loss dict:  {'classification_loss': 0.9504164755344391}
2025-01-14 19:45:07,576 [INFO] Step[650/2713]: training loss : 0.9485985231399536 TRAIN  loss dict:  {'classification_loss': 0.9485985231399536}
2025-01-14 19:45:21,258 [INFO] Step[700/2713]: training loss : 0.9434904491901398 TRAIN  loss dict:  {'classification_loss': 0.9434904491901398}
2025-01-14 19:45:34,520 [INFO] Step[750/2713]: training loss : 0.9531855976581574 TRAIN  loss dict:  {'classification_loss': 0.9531855976581574}
2025-01-14 19:45:48,193 [INFO] Step[800/2713]: training loss : 0.9462192213535309 TRAIN  loss dict:  {'classification_loss': 0.9462192213535309}
2025-01-14 19:46:01,723 [INFO] Step[850/2713]: training loss : 0.9484472000598907 TRAIN  loss dict:  {'classification_loss': 0.9484472000598907}
2025-01-14 19:46:15,697 [INFO] Step[900/2713]: training loss : 0.9581650733947754 TRAIN  loss dict:  {'classification_loss': 0.9581650733947754}
2025-01-14 19:46:29,575 [INFO] Step[950/2713]: training loss : 0.9448129260540008 TRAIN  loss dict:  {'classification_loss': 0.9448129260540008}
2025-01-14 19:46:43,858 [INFO] Step[1000/2713]: training loss : 0.9920769357681274 TRAIN  loss dict:  {'classification_loss': 0.9920769357681274}
2025-01-14 19:46:57,552 [INFO] Step[1050/2713]: training loss : 0.9692732512950897 TRAIN  loss dict:  {'classification_loss': 0.9692732512950897}
2025-01-14 19:47:11,244 [INFO] Step[1100/2713]: training loss : 0.9639991569519043 TRAIN  loss dict:  {'classification_loss': 0.9639991569519043}
2025-01-14 19:47:25,331 [INFO] Step[1150/2713]: training loss : 0.9474249148368835 TRAIN  loss dict:  {'classification_loss': 0.9474249148368835}
2025-01-14 19:47:38,967 [INFO] Step[1200/2713]: training loss : 0.9590379655361175 TRAIN  loss dict:  {'classification_loss': 0.9590379655361175}
2025-01-14 19:47:52,971 [INFO] Step[1250/2713]: training loss : 0.9487253785133362 TRAIN  loss dict:  {'classification_loss': 0.9487253785133362}
2025-01-14 19:48:06,182 [INFO] Step[1300/2713]: training loss : 0.9459723722934723 TRAIN  loss dict:  {'classification_loss': 0.9459723722934723}
2025-01-14 19:48:19,409 [INFO] Step[1350/2713]: training loss : 0.9861027574539185 TRAIN  loss dict:  {'classification_loss': 0.9861027574539185}
2025-01-14 19:48:32,642 [INFO] Step[1400/2713]: training loss : 0.9513809180259705 TRAIN  loss dict:  {'classification_loss': 0.9513809180259705}
2025-01-14 19:48:45,971 [INFO] Step[1450/2713]: training loss : 0.9520597171783447 TRAIN  loss dict:  {'classification_loss': 0.9520597171783447}
2025-01-14 19:48:59,587 [INFO] Step[1500/2713]: training loss : 0.9653788578510284 TRAIN  loss dict:  {'classification_loss': 0.9653788578510284}
2025-01-14 19:49:13,158 [INFO] Step[1550/2713]: training loss : 0.9744770741462707 TRAIN  loss dict:  {'classification_loss': 0.9744770741462707}
2025-01-14 19:49:26,447 [INFO] Step[1600/2713]: training loss : 0.9617158651351929 TRAIN  loss dict:  {'classification_loss': 0.9617158651351929}
2025-01-14 19:49:39,722 [INFO] Step[1650/2713]: training loss : 0.9567187249660491 TRAIN  loss dict:  {'classification_loss': 0.9567187249660491}
2025-01-14 19:49:53,435 [INFO] Step[1700/2713]: training loss : 0.946070020198822 TRAIN  loss dict:  {'classification_loss': 0.946070020198822}
2025-01-14 19:50:07,453 [INFO] Step[1750/2713]: training loss : 0.9614063763618469 TRAIN  loss dict:  {'classification_loss': 0.9614063763618469}
2025-01-14 19:50:21,094 [INFO] Step[1800/2713]: training loss : 0.9713105320930481 TRAIN  loss dict:  {'classification_loss': 0.9713105320930481}
2025-01-14 19:50:34,947 [INFO] Step[1850/2713]: training loss : 0.9582187497615814 TRAIN  loss dict:  {'classification_loss': 0.9582187497615814}
2025-01-14 19:50:50,412 [INFO] Step[1900/2713]: training loss : 0.9536066925525666 TRAIN  loss dict:  {'classification_loss': 0.9536066925525666}
2025-01-14 19:51:06,377 [INFO] Step[1950/2713]: training loss : 0.9480990850925446 TRAIN  loss dict:  {'classification_loss': 0.9480990850925446}
2025-01-14 19:51:19,682 [INFO] Step[2000/2713]: training loss : 0.9547290921211242 TRAIN  loss dict:  {'classification_loss': 0.9547290921211242}
2025-01-14 19:51:33,332 [INFO] Step[2050/2713]: training loss : 0.9449408984184265 TRAIN  loss dict:  {'classification_loss': 0.9449408984184265}
2025-01-14 19:51:47,158 [INFO] Step[2100/2713]: training loss : 0.9561074757575989 TRAIN  loss dict:  {'classification_loss': 0.9561074757575989}
2025-01-14 19:52:00,818 [INFO] Step[2150/2713]: training loss : 0.9548203110694885 TRAIN  loss dict:  {'classification_loss': 0.9548203110694885}
2025-01-14 19:52:14,855 [INFO] Step[2200/2713]: training loss : 0.9841746318340302 TRAIN  loss dict:  {'classification_loss': 0.9841746318340302}
2025-01-14 19:52:28,551 [INFO] Step[2250/2713]: training loss : 0.9557529127597809 TRAIN  loss dict:  {'classification_loss': 0.9557529127597809}
2025-01-14 19:52:42,177 [INFO] Step[2300/2713]: training loss : 0.9494082450866699 TRAIN  loss dict:  {'classification_loss': 0.9494082450866699}
2025-01-14 19:52:55,978 [INFO] Step[2350/2713]: training loss : 0.941216299533844 TRAIN  loss dict:  {'classification_loss': 0.941216299533844}
2025-01-14 19:53:09,815 [INFO] Step[2400/2713]: training loss : 0.9514254033565521 TRAIN  loss dict:  {'classification_loss': 0.9514254033565521}
2025-01-14 19:53:25,921 [INFO] Step[2450/2713]: training loss : 0.9524878907203674 TRAIN  loss dict:  {'classification_loss': 0.9524878907203674}
2025-01-14 19:53:41,056 [INFO] Step[2500/2713]: training loss : 0.9466547405719757 TRAIN  loss dict:  {'classification_loss': 0.9466547405719757}
2025-01-14 19:53:54,310 [INFO] Step[2550/2713]: training loss : 0.9535053694248199 TRAIN  loss dict:  {'classification_loss': 0.9535053694248199}
2025-01-14 19:54:07,544 [INFO] Step[2600/2713]: training loss : 0.94484614610672 TRAIN  loss dict:  {'classification_loss': 0.94484614610672}
2025-01-14 19:54:20,807 [INFO] Step[2650/2713]: training loss : 0.9645637285709381 TRAIN  loss dict:  {'classification_loss': 0.9645637285709381}
2025-01-14 19:54:34,338 [INFO] Step[2700/2713]: training loss : 0.9510017764568329 TRAIN  loss dict:  {'classification_loss': 0.9510017764568329}
2025-01-14 19:55:49,930 [INFO] Label accuracies statistics:
2025-01-14 19:55:49,930 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.5, 396: 0.75, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-14 19:55:51,079 [INFO] [37] TRAIN  loss: 0.9560920374005912 acc: 0.9972969652291437
2025-01-14 19:55:51,079 [INFO] [37] TRAIN  loss dict: {'classification_loss': 0.9560920374005912}
2025-01-14 19:55:51,079 [INFO] [37] VALIDATION loss: 1.8088168701283018 VALIDATION acc: 0.8037617554858935
2025-01-14 19:55:51,080 [INFO] [37] VALIDATION loss dict: {'classification_loss': 1.8088168701283018}
2025-01-14 19:55:51,080 [INFO] 
2025-01-14 19:56:09,871 [INFO] Step[50/2713]: training loss : 0.9631460678577423 TRAIN  loss dict:  {'classification_loss': 0.9631460678577423}
2025-01-14 19:56:23,062 [INFO] Step[100/2713]: training loss : 0.9457838356494903 TRAIN  loss dict:  {'classification_loss': 0.9457838356494903}
2025-01-14 19:56:36,324 [INFO] Step[150/2713]: training loss : 0.9569663107395172 TRAIN  loss dict:  {'classification_loss': 0.9569663107395172}
2025-01-14 19:56:50,338 [INFO] Step[200/2713]: training loss : 0.9500257980823517 TRAIN  loss dict:  {'classification_loss': 0.9500257980823517}
2025-01-14 19:57:03,945 [INFO] Step[250/2713]: training loss : 0.9452348732948304 TRAIN  loss dict:  {'classification_loss': 0.9452348732948304}
2025-01-14 19:57:17,161 [INFO] Step[300/2713]: training loss : 0.9463007664680481 TRAIN  loss dict:  {'classification_loss': 0.9463007664680481}
2025-01-14 19:57:30,946 [INFO] Step[350/2713]: training loss : 0.9552833449840545 TRAIN  loss dict:  {'classification_loss': 0.9552833449840545}
2025-01-14 19:57:44,491 [INFO] Step[400/2713]: training loss : 0.9818390691280365 TRAIN  loss dict:  {'classification_loss': 0.9818390691280365}
2025-01-14 19:57:57,723 [INFO] Step[450/2713]: training loss : 0.9470163357257843 TRAIN  loss dict:  {'classification_loss': 0.9470163357257843}
2025-01-14 19:58:11,717 [INFO] Step[500/2713]: training loss : 0.9439874243736267 TRAIN  loss dict:  {'classification_loss': 0.9439874243736267}
2025-01-14 19:58:25,068 [INFO] Step[550/2713]: training loss : 0.9991286814212799 TRAIN  loss dict:  {'classification_loss': 0.9991286814212799}
2025-01-14 19:58:38,561 [INFO] Step[600/2713]: training loss : 0.9540359914302826 TRAIN  loss dict:  {'classification_loss': 0.9540359914302826}
2025-01-14 19:58:52,131 [INFO] Step[650/2713]: training loss : 0.9449110364913941 TRAIN  loss dict:  {'classification_loss': 0.9449110364913941}
2025-01-14 19:59:05,769 [INFO] Step[700/2713]: training loss : 0.9503189408779145 TRAIN  loss dict:  {'classification_loss': 0.9503189408779145}
2025-01-14 19:59:19,215 [INFO] Step[750/2713]: training loss : 0.9713627612590789 TRAIN  loss dict:  {'classification_loss': 0.9713627612590789}
2025-01-14 19:59:33,249 [INFO] Step[800/2713]: training loss : 0.950943044424057 TRAIN  loss dict:  {'classification_loss': 0.950943044424057}
2025-01-14 19:59:46,713 [INFO] Step[850/2713]: training loss : 0.9422267925739288 TRAIN  loss dict:  {'classification_loss': 0.9422267925739288}
2025-01-14 20:00:00,512 [INFO] Step[900/2713]: training loss : 0.9689145958423615 TRAIN  loss dict:  {'classification_loss': 0.9689145958423615}
2025-01-14 20:00:14,566 [INFO] Step[950/2713]: training loss : 0.9458183944225311 TRAIN  loss dict:  {'classification_loss': 0.9458183944225311}
2025-01-14 20:00:28,010 [INFO] Step[1000/2713]: training loss : 0.9931125843524933 TRAIN  loss dict:  {'classification_loss': 0.9931125843524933}
2025-01-14 20:00:41,779 [INFO] Step[1050/2713]: training loss : 0.9447563409805297 TRAIN  loss dict:  {'classification_loss': 0.9447563409805297}
2025-01-14 20:00:54,989 [INFO] Step[1100/2713]: training loss : 0.950647304058075 TRAIN  loss dict:  {'classification_loss': 0.950647304058075}
2025-01-14 20:01:08,814 [INFO] Step[1150/2713]: training loss : 0.965461609363556 TRAIN  loss dict:  {'classification_loss': 0.965461609363556}
2025-01-14 20:01:22,441 [INFO] Step[1200/2713]: training loss : 0.9686882412433624 TRAIN  loss dict:  {'classification_loss': 0.9686882412433624}
2025-01-14 20:01:36,360 [INFO] Step[1250/2713]: training loss : 1.0162277793884278 TRAIN  loss dict:  {'classification_loss': 1.0162277793884278}
2025-01-14 20:01:50,250 [INFO] Step[1300/2713]: training loss : 0.9531280291080475 TRAIN  loss dict:  {'classification_loss': 0.9531280291080475}
2025-01-14 20:02:04,083 [INFO] Step[1350/2713]: training loss : 0.9487694132328034 TRAIN  loss dict:  {'classification_loss': 0.9487694132328034}
2025-01-14 20:02:17,558 [INFO] Step[1400/2713]: training loss : 0.9671344816684723 TRAIN  loss dict:  {'classification_loss': 0.9671344816684723}
2025-01-14 20:02:31,367 [INFO] Step[1450/2713]: training loss : 0.9443195724487304 TRAIN  loss dict:  {'classification_loss': 0.9443195724487304}
2025-01-14 20:02:44,594 [INFO] Step[1500/2713]: training loss : 0.9919863820075989 TRAIN  loss dict:  {'classification_loss': 0.9919863820075989}
2025-01-14 20:02:57,854 [INFO] Step[1550/2713]: training loss : 0.9570780205726623 TRAIN  loss dict:  {'classification_loss': 0.9570780205726623}
2025-01-14 20:03:11,455 [INFO] Step[1600/2713]: training loss : 0.9652962446212768 TRAIN  loss dict:  {'classification_loss': 0.9652962446212768}
2025-01-14 20:03:26,536 [INFO] Step[1650/2713]: training loss : 0.9560868632793427 TRAIN  loss dict:  {'classification_loss': 0.9560868632793427}
2025-01-14 20:03:41,407 [INFO] Step[1700/2713]: training loss : 0.9510047507286071 TRAIN  loss dict:  {'classification_loss': 0.9510047507286071}
2025-01-14 20:03:54,749 [INFO] Step[1750/2713]: training loss : 0.9891263329982758 TRAIN  loss dict:  {'classification_loss': 0.9891263329982758}
2025-01-14 20:04:08,470 [INFO] Step[1800/2713]: training loss : 0.9439218783378601 TRAIN  loss dict:  {'classification_loss': 0.9439218783378601}
2025-01-14 20:04:21,941 [INFO] Step[1850/2713]: training loss : 0.9530149006843567 TRAIN  loss dict:  {'classification_loss': 0.9530149006843567}
2025-01-14 20:04:35,540 [INFO] Step[1900/2713]: training loss : 0.9976823997497558 TRAIN  loss dict:  {'classification_loss': 0.9976823997497558}
2025-01-14 20:04:49,345 [INFO] Step[1950/2713]: training loss : 0.9678519082069397 TRAIN  loss dict:  {'classification_loss': 0.9678519082069397}
2025-01-14 20:05:03,671 [INFO] Step[2000/2713]: training loss : 0.9471166133880615 TRAIN  loss dict:  {'classification_loss': 0.9471166133880615}
2025-01-14 20:05:17,524 [INFO] Step[2050/2713]: training loss : 0.9442012798786164 TRAIN  loss dict:  {'classification_loss': 0.9442012798786164}
2025-01-14 20:05:30,822 [INFO] Step[2100/2713]: training loss : 0.9658574914932251 TRAIN  loss dict:  {'classification_loss': 0.9658574914932251}
2025-01-14 20:05:44,519 [INFO] Step[2150/2713]: training loss : 0.9440802383422852 TRAIN  loss dict:  {'classification_loss': 0.9440802383422852}
2025-01-14 20:05:58,249 [INFO] Step[2200/2713]: training loss : 0.9705286753177643 TRAIN  loss dict:  {'classification_loss': 0.9705286753177643}
2025-01-14 20:06:11,561 [INFO] Step[2250/2713]: training loss : 0.9486318564414978 TRAIN  loss dict:  {'classification_loss': 0.9486318564414978}
2025-01-14 20:06:25,472 [INFO] Step[2300/2713]: training loss : 0.9452863478660584 TRAIN  loss dict:  {'classification_loss': 0.9452863478660584}
2025-01-14 20:06:39,291 [INFO] Step[2350/2713]: training loss : 0.9693215441703796 TRAIN  loss dict:  {'classification_loss': 0.9693215441703796}
2025-01-14 20:06:52,900 [INFO] Step[2400/2713]: training loss : 0.9652516889572144 TRAIN  loss dict:  {'classification_loss': 0.9652516889572144}
2025-01-14 20:07:06,388 [INFO] Step[2450/2713]: training loss : 0.9515378665924072 TRAIN  loss dict:  {'classification_loss': 0.9515378665924072}
2025-01-14 20:07:20,051 [INFO] Step[2500/2713]: training loss : 0.9667364013195038 TRAIN  loss dict:  {'classification_loss': 0.9667364013195038}
2025-01-14 20:07:33,515 [INFO] Step[2550/2713]: training loss : 0.9509880387783051 TRAIN  loss dict:  {'classification_loss': 0.9509880387783051}
2025-01-14 20:07:47,830 [INFO] Step[2600/2713]: training loss : 0.9478249526023865 TRAIN  loss dict:  {'classification_loss': 0.9478249526023865}
2025-01-14 20:08:02,123 [INFO] Step[2650/2713]: training loss : 0.965922679901123 TRAIN  loss dict:  {'classification_loss': 0.965922679901123}
2025-01-14 20:08:15,874 [INFO] Step[2700/2713]: training loss : 0.9442513370513916 TRAIN  loss dict:  {'classification_loss': 0.9442513370513916}
2025-01-14 20:09:31,657 [INFO] Label accuracies statistics:
2025-01-14 20:09:31,657 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.5, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.25, 190: 0.5, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 1.0, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 20:09:31,658 [INFO] [38] TRAIN  loss: 0.9595502619149278 acc: 0.9956997174100012
2025-01-14 20:09:31,658 [INFO] [38] TRAIN  loss dict: {'classification_loss': 0.9595502619149278}
2025-01-14 20:09:31,659 [INFO] [38] VALIDATION loss: 1.8112810148780507 VALIDATION acc: 0.7818181818181819
2025-01-14 20:09:31,659 [INFO] [38] VALIDATION loss dict: {'classification_loss': 1.8112810148780507}
2025-01-14 20:09:31,659 [INFO] 
2025-01-14 20:09:50,735 [INFO] Step[50/2713]: training loss : 0.9673053193092346 TRAIN  loss dict:  {'classification_loss': 0.9673053193092346}
2025-01-14 20:10:04,308 [INFO] Step[100/2713]: training loss : 0.9622122418880462 TRAIN  loss dict:  {'classification_loss': 0.9622122418880462}
2025-01-14 20:10:17,976 [INFO] Step[150/2713]: training loss : 0.9560003459453583 TRAIN  loss dict:  {'classification_loss': 0.9560003459453583}
2025-01-14 20:10:31,859 [INFO] Step[200/2713]: training loss : 0.9827941989898682 TRAIN  loss dict:  {'classification_loss': 0.9827941989898682}
2025-01-14 20:10:45,267 [INFO] Step[250/2713]: training loss : 0.9535220849514008 TRAIN  loss dict:  {'classification_loss': 0.9535220849514008}
2025-01-14 20:10:58,782 [INFO] Step[300/2713]: training loss : 0.945700410604477 TRAIN  loss dict:  {'classification_loss': 0.945700410604477}
2025-01-14 20:11:12,083 [INFO] Step[350/2713]: training loss : 0.9420302748680115 TRAIN  loss dict:  {'classification_loss': 0.9420302748680115}
2025-01-14 20:11:26,040 [INFO] Step[400/2713]: training loss : 0.9514219975471496 TRAIN  loss dict:  {'classification_loss': 0.9514219975471496}
2025-01-14 20:11:41,502 [INFO] Step[450/2713]: training loss : 0.9441587483882904 TRAIN  loss dict:  {'classification_loss': 0.9441587483882904}
2025-01-14 20:11:57,515 [INFO] Step[500/2713]: training loss : 0.9666584324836731 TRAIN  loss dict:  {'classification_loss': 0.9666584324836731}
2025-01-14 20:12:11,719 [INFO] Step[550/2713]: training loss : 0.9546099543571472 TRAIN  loss dict:  {'classification_loss': 0.9546099543571472}
2025-01-14 20:12:25,716 [INFO] Step[600/2713]: training loss : 0.9427177214622497 TRAIN  loss dict:  {'classification_loss': 0.9427177214622497}
2025-01-14 20:12:39,521 [INFO] Step[650/2713]: training loss : 0.9433173632621765 TRAIN  loss dict:  {'classification_loss': 0.9433173632621765}
2025-01-14 20:12:53,196 [INFO] Step[700/2713]: training loss : 0.9417421984672546 TRAIN  loss dict:  {'classification_loss': 0.9417421984672546}
2025-01-14 20:13:07,136 [INFO] Step[750/2713]: training loss : 0.9454054725170136 TRAIN  loss dict:  {'classification_loss': 0.9454054725170136}
2025-01-14 20:13:21,369 [INFO] Step[800/2713]: training loss : 0.9450664854049683 TRAIN  loss dict:  {'classification_loss': 0.9450664854049683}
2025-01-14 20:13:35,309 [INFO] Step[850/2713]: training loss : 0.984357841014862 TRAIN  loss dict:  {'classification_loss': 0.984357841014862}
2025-01-14 20:13:49,180 [INFO] Step[900/2713]: training loss : 0.9543523788452148 TRAIN  loss dict:  {'classification_loss': 0.9543523788452148}
2025-01-14 20:14:03,002 [INFO] Step[950/2713]: training loss : 0.9469887757301331 TRAIN  loss dict:  {'classification_loss': 0.9469887757301331}
2025-01-14 20:14:16,483 [INFO] Step[1000/2713]: training loss : 0.943314710855484 TRAIN  loss dict:  {'classification_loss': 0.943314710855484}
2025-01-14 20:14:30,377 [INFO] Step[1050/2713]: training loss : 0.9476851630210876 TRAIN  loss dict:  {'classification_loss': 0.9476851630210876}
2025-01-14 20:14:44,115 [INFO] Step[1100/2713]: training loss : 0.9468489766120911 TRAIN  loss dict:  {'classification_loss': 0.9468489766120911}
2025-01-14 20:14:57,652 [INFO] Step[1150/2713]: training loss : 0.9440013527870178 TRAIN  loss dict:  {'classification_loss': 0.9440013527870178}
2025-01-14 20:15:11,811 [INFO] Step[1200/2713]: training loss : 0.9434993624687195 TRAIN  loss dict:  {'classification_loss': 0.9434993624687195}
2025-01-14 20:15:25,087 [INFO] Step[1250/2713]: training loss : 0.9428479242324829 TRAIN  loss dict:  {'classification_loss': 0.9428479242324829}
2025-01-14 20:15:38,508 [INFO] Step[1300/2713]: training loss : 0.9433316242694855 TRAIN  loss dict:  {'classification_loss': 0.9433316242694855}
2025-01-14 20:15:52,242 [INFO] Step[1350/2713]: training loss : 0.9460938024520874 TRAIN  loss dict:  {'classification_loss': 0.9460938024520874}
2025-01-14 20:16:06,388 [INFO] Step[1400/2713]: training loss : 0.9464555966854096 TRAIN  loss dict:  {'classification_loss': 0.9464555966854096}
2025-01-14 20:16:19,931 [INFO] Step[1450/2713]: training loss : 0.9458744895458221 TRAIN  loss dict:  {'classification_loss': 0.9458744895458221}
2025-01-14 20:16:33,795 [INFO] Step[1500/2713]: training loss : 0.9486851525306702 TRAIN  loss dict:  {'classification_loss': 0.9486851525306702}
2025-01-14 20:16:47,280 [INFO] Step[1550/2713]: training loss : 0.9439032089710235 TRAIN  loss dict:  {'classification_loss': 0.9439032089710235}
2025-01-14 20:17:01,018 [INFO] Step[1600/2713]: training loss : 0.9422649788856506 TRAIN  loss dict:  {'classification_loss': 0.9422649788856506}
2025-01-14 20:17:14,245 [INFO] Step[1650/2713]: training loss : 0.9427034687995911 TRAIN  loss dict:  {'classification_loss': 0.9427034687995911}
2025-01-14 20:17:27,873 [INFO] Step[1700/2713]: training loss : 0.9488128674030304 TRAIN  loss dict:  {'classification_loss': 0.9488128674030304}
2025-01-14 20:17:41,794 [INFO] Step[1750/2713]: training loss : 0.9412077760696411 TRAIN  loss dict:  {'classification_loss': 0.9412077760696411}
2025-01-14 20:17:55,094 [INFO] Step[1800/2713]: training loss : 0.9485872614383698 TRAIN  loss dict:  {'classification_loss': 0.9485872614383698}
2025-01-14 20:18:09,080 [INFO] Step[1850/2713]: training loss : 0.9403450274467469 TRAIN  loss dict:  {'classification_loss': 0.9403450274467469}
2025-01-14 20:18:22,904 [INFO] Step[1900/2713]: training loss : 0.9613774478435516 TRAIN  loss dict:  {'classification_loss': 0.9613774478435516}
2025-01-14 20:18:36,612 [INFO] Step[1950/2713]: training loss : 0.942628984451294 TRAIN  loss dict:  {'classification_loss': 0.942628984451294}
2025-01-14 20:18:50,463 [INFO] Step[2000/2713]: training loss : 0.953548356294632 TRAIN  loss dict:  {'classification_loss': 0.953548356294632}
2025-01-14 20:19:04,616 [INFO] Step[2050/2713]: training loss : 0.9631344568729401 TRAIN  loss dict:  {'classification_loss': 0.9631344568729401}
2025-01-14 20:19:18,616 [INFO] Step[2100/2713]: training loss : 0.9605180025100708 TRAIN  loss dict:  {'classification_loss': 0.9605180025100708}
2025-01-14 20:19:32,613 [INFO] Step[2150/2713]: training loss : 0.9450349330902099 TRAIN  loss dict:  {'classification_loss': 0.9450349330902099}
2025-01-14 20:19:46,321 [INFO] Step[2200/2713]: training loss : 0.996394876241684 TRAIN  loss dict:  {'classification_loss': 0.996394876241684}
2025-01-14 20:20:00,161 [INFO] Step[2250/2713]: training loss : 0.9589582002162933 TRAIN  loss dict:  {'classification_loss': 0.9589582002162933}
2025-01-14 20:20:13,673 [INFO] Step[2300/2713]: training loss : 0.941508766412735 TRAIN  loss dict:  {'classification_loss': 0.941508766412735}
2025-01-14 20:20:27,240 [INFO] Step[2350/2713]: training loss : 0.9482138776779174 TRAIN  loss dict:  {'classification_loss': 0.9482138776779174}
2025-01-14 20:20:40,475 [INFO] Step[2400/2713]: training loss : 0.9432868230342865 TRAIN  loss dict:  {'classification_loss': 0.9432868230342865}
2025-01-14 20:20:53,695 [INFO] Step[2450/2713]: training loss : 0.9669206130504608 TRAIN  loss dict:  {'classification_loss': 0.9669206130504608}
2025-01-14 20:21:07,303 [INFO] Step[2500/2713]: training loss : 0.9504397821426391 TRAIN  loss dict:  {'classification_loss': 0.9504397821426391}
2025-01-14 20:21:20,825 [INFO] Step[2550/2713]: training loss : 0.9874272096157074 TRAIN  loss dict:  {'classification_loss': 0.9874272096157074}
2025-01-14 20:21:34,527 [INFO] Step[2600/2713]: training loss : 0.9561060178279877 TRAIN  loss dict:  {'classification_loss': 0.9561060178279877}
2025-01-14 20:21:48,201 [INFO] Step[2650/2713]: training loss : 0.9401852285861969 TRAIN  loss dict:  {'classification_loss': 0.9401852285861969}
2025-01-14 20:22:02,489 [INFO] Step[2700/2713]: training loss : 0.9483195328712464 TRAIN  loss dict:  {'classification_loss': 0.9483195328712464}
2025-01-14 20:23:18,424 [INFO] Label accuracies statistics:
2025-01-14 20:23:18,424 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.25, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 0.6, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 0.5, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 1.0, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.25, 275: 0.5, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-14 20:23:18,426 [INFO] [39] TRAIN  loss: 0.9518014020116576 acc: 0.9975426956628578
2025-01-14 20:23:18,426 [INFO] [39] TRAIN  loss dict: {'classification_loss': 0.9518014020116576}
2025-01-14 20:23:18,426 [INFO] [39] VALIDATION loss: 1.8103197275247789 VALIDATION acc: 0.7931034482758621
2025-01-14 20:23:18,426 [INFO] [39] VALIDATION loss dict: {'classification_loss': 1.8103197275247789}
2025-01-14 20:23:18,426 [INFO] 
2025-01-14 20:23:38,079 [INFO] Step[50/2713]: training loss : 0.9420906281471253 TRAIN  loss dict:  {'classification_loss': 0.9420906281471253}
2025-01-14 20:23:52,187 [INFO] Step[100/2713]: training loss : 0.9425064432621002 TRAIN  loss dict:  {'classification_loss': 0.9425064432621002}
2025-01-14 20:24:05,944 [INFO] Step[150/2713]: training loss : 0.9756706726551055 TRAIN  loss dict:  {'classification_loss': 0.9756706726551055}
2025-01-14 20:24:19,848 [INFO] Step[200/2713]: training loss : 0.9524005818367004 TRAIN  loss dict:  {'classification_loss': 0.9524005818367004}
2025-01-14 20:24:33,572 [INFO] Step[250/2713]: training loss : 0.9477342867851257 TRAIN  loss dict:  {'classification_loss': 0.9477342867851257}
2025-01-14 20:24:47,445 [INFO] Step[300/2713]: training loss : 0.955186288356781 TRAIN  loss dict:  {'classification_loss': 0.955186288356781}
2025-01-14 20:25:01,427 [INFO] Step[350/2713]: training loss : 0.9605386579036712 TRAIN  loss dict:  {'classification_loss': 0.9605386579036712}
2025-01-14 20:25:14,931 [INFO] Step[400/2713]: training loss : 0.961974309682846 TRAIN  loss dict:  {'classification_loss': 0.961974309682846}
2025-01-14 20:25:28,429 [INFO] Step[450/2713]: training loss : 0.9450165498256683 TRAIN  loss dict:  {'classification_loss': 0.9450165498256683}
2025-01-14 20:25:42,125 [INFO] Step[500/2713]: training loss : 0.9412162005901337 TRAIN  loss dict:  {'classification_loss': 0.9412162005901337}
2025-01-14 20:25:56,098 [INFO] Step[550/2713]: training loss : 0.96788498878479 TRAIN  loss dict:  {'classification_loss': 0.96788498878479}
2025-01-14 20:26:09,575 [INFO] Step[600/2713]: training loss : 0.9463788604736328 TRAIN  loss dict:  {'classification_loss': 0.9463788604736328}
2025-01-14 20:26:23,384 [INFO] Step[650/2713]: training loss : 0.9840615653991699 TRAIN  loss dict:  {'classification_loss': 0.9840615653991699}
2025-01-14 20:26:37,189 [INFO] Step[700/2713]: training loss : 0.9571844828128815 TRAIN  loss dict:  {'classification_loss': 0.9571844828128815}
2025-01-14 20:26:50,971 [INFO] Step[750/2713]: training loss : 0.9859619998931884 TRAIN  loss dict:  {'classification_loss': 0.9859619998931884}
2025-01-14 20:27:04,208 [INFO] Step[800/2713]: training loss : 0.9458980560302734 TRAIN  loss dict:  {'classification_loss': 0.9458980560302734}
2025-01-14 20:27:17,999 [INFO] Step[850/2713]: training loss : 0.9473774850368499 TRAIN  loss dict:  {'classification_loss': 0.9473774850368499}
2025-01-14 20:27:32,027 [INFO] Step[900/2713]: training loss : 0.9575868618488311 TRAIN  loss dict:  {'classification_loss': 0.9575868618488311}
2025-01-14 20:27:45,918 [INFO] Step[950/2713]: training loss : 0.9517233777046203 TRAIN  loss dict:  {'classification_loss': 0.9517233777046203}
2025-01-14 20:27:59,553 [INFO] Step[1000/2713]: training loss : 0.9452502918243408 TRAIN  loss dict:  {'classification_loss': 0.9452502918243408}
2025-01-14 20:28:13,033 [INFO] Step[1050/2713]: training loss : 0.9505870544910431 TRAIN  loss dict:  {'classification_loss': 0.9505870544910431}
2025-01-14 20:28:26,469 [INFO] Step[1100/2713]: training loss : 0.9472419571876526 TRAIN  loss dict:  {'classification_loss': 0.9472419571876526}
2025-01-14 20:28:39,876 [INFO] Step[1150/2713]: training loss : 0.9472235691547394 TRAIN  loss dict:  {'classification_loss': 0.9472235691547394}
2025-01-14 20:28:53,441 [INFO] Step[1200/2713]: training loss : 0.9464640653133393 TRAIN  loss dict:  {'classification_loss': 0.9464640653133393}
2025-01-14 20:29:07,038 [INFO] Step[1250/2713]: training loss : 0.9430041837692261 TRAIN  loss dict:  {'classification_loss': 0.9430041837692261}
2025-01-14 20:29:21,146 [INFO] Step[1300/2713]: training loss : 0.9409062540531159 TRAIN  loss dict:  {'classification_loss': 0.9409062540531159}
2025-01-14 20:29:34,807 [INFO] Step[1350/2713]: training loss : 0.9455269956588745 TRAIN  loss dict:  {'classification_loss': 0.9455269956588745}
2025-01-14 20:29:48,023 [INFO] Step[1400/2713]: training loss : 0.971616905927658 TRAIN  loss dict:  {'classification_loss': 0.971616905927658}
2025-01-14 20:30:01,581 [INFO] Step[1450/2713]: training loss : 0.9560052192211151 TRAIN  loss dict:  {'classification_loss': 0.9560052192211151}
2025-01-14 20:30:15,638 [INFO] Step[1500/2713]: training loss : 0.9459536445140838 TRAIN  loss dict:  {'classification_loss': 0.9459536445140838}
2025-01-14 20:30:29,983 [INFO] Step[1550/2713]: training loss : 0.9440164935588836 TRAIN  loss dict:  {'classification_loss': 0.9440164935588836}
2025-01-14 20:30:44,189 [INFO] Step[1600/2713]: training loss : 0.9460757446289062 TRAIN  loss dict:  {'classification_loss': 0.9460757446289062}
2025-01-14 20:30:58,460 [INFO] Step[1650/2713]: training loss : 0.9520257818698883 TRAIN  loss dict:  {'classification_loss': 0.9520257818698883}
2025-01-14 20:31:12,076 [INFO] Step[1700/2713]: training loss : 0.9456982791423798 TRAIN  loss dict:  {'classification_loss': 0.9456982791423798}
2025-01-14 20:31:25,795 [INFO] Step[1750/2713]: training loss : 0.9407886338233947 TRAIN  loss dict:  {'classification_loss': 0.9407886338233947}
2025-01-14 20:31:39,744 [INFO] Step[1800/2713]: training loss : 0.9473218250274659 TRAIN  loss dict:  {'classification_loss': 0.9473218250274659}
2025-01-14 20:31:53,337 [INFO] Step[1850/2713]: training loss : 0.9474097859859466 TRAIN  loss dict:  {'classification_loss': 0.9474097859859466}
2025-01-14 20:32:06,903 [INFO] Step[1900/2713]: training loss : 0.9723628997802735 TRAIN  loss dict:  {'classification_loss': 0.9723628997802735}
2025-01-14 20:32:21,211 [INFO] Step[1950/2713]: training loss : 0.9481816136837006 TRAIN  loss dict:  {'classification_loss': 0.9481816136837006}
2025-01-14 20:32:34,914 [INFO] Step[2000/2713]: training loss : 0.9624634575843811 TRAIN  loss dict:  {'classification_loss': 0.9624634575843811}
2025-01-14 20:32:48,939 [INFO] Step[2050/2713]: training loss : 0.941165372133255 TRAIN  loss dict:  {'classification_loss': 0.941165372133255}
2025-01-14 20:33:02,494 [INFO] Step[2100/2713]: training loss : 0.9425127136707306 TRAIN  loss dict:  {'classification_loss': 0.9425127136707306}
2025-01-14 20:33:15,938 [INFO] Step[2150/2713]: training loss : 0.9451888179779053 TRAIN  loss dict:  {'classification_loss': 0.9451888179779053}
2025-01-14 20:33:29,526 [INFO] Step[2200/2713]: training loss : 0.9802302181720733 TRAIN  loss dict:  {'classification_loss': 0.9802302181720733}
2025-01-14 20:33:43,364 [INFO] Step[2250/2713]: training loss : 0.9398563694953919 TRAIN  loss dict:  {'classification_loss': 0.9398563694953919}
2025-01-14 20:33:57,198 [INFO] Step[2300/2713]: training loss : 0.9670200347900391 TRAIN  loss dict:  {'classification_loss': 0.9670200347900391}
2025-01-14 20:34:11,237 [INFO] Step[2350/2713]: training loss : 0.9416121852397918 TRAIN  loss dict:  {'classification_loss': 0.9416121852397918}
2025-01-14 20:34:25,325 [INFO] Step[2400/2713]: training loss : 0.9439065313339233 TRAIN  loss dict:  {'classification_loss': 0.9439065313339233}
2025-01-14 20:34:38,855 [INFO] Step[2450/2713]: training loss : 0.9434656178951264 TRAIN  loss dict:  {'classification_loss': 0.9434656178951264}
2025-01-14 20:34:52,394 [INFO] Step[2500/2713]: training loss : 0.9459567797183991 TRAIN  loss dict:  {'classification_loss': 0.9459567797183991}
2025-01-14 20:35:06,008 [INFO] Step[2550/2713]: training loss : 0.9433286798000335 TRAIN  loss dict:  {'classification_loss': 0.9433286798000335}
2025-01-14 20:35:19,419 [INFO] Step[2600/2713]: training loss : 0.9450809621810913 TRAIN  loss dict:  {'classification_loss': 0.9450809621810913}
2025-01-14 20:35:33,445 [INFO] Step[2650/2713]: training loss : 0.9527603411674499 TRAIN  loss dict:  {'classification_loss': 0.9527603411674499}
2025-01-14 20:35:46,687 [INFO] Step[2700/2713]: training loss : 0.9593921327590942 TRAIN  loss dict:  {'classification_loss': 0.9593921327590942}
2025-01-14 20:37:02,931 [INFO] Label accuracies statistics:
2025-01-14 20:37:02,931 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.5, 59: 1.0, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.0, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 1.0, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.0, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.5, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.25, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 1.0, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 20:37:02,934 [INFO] [40] TRAIN  loss: 0.9518960236272731 acc: 0.9968055043617152
2025-01-14 20:37:02,934 [INFO] [40] TRAIN  loss dict: {'classification_loss': 0.9518960236272731}
2025-01-14 20:37:02,934 [INFO] [40] VALIDATION loss: 1.8079289523954678 VALIDATION acc: 0.7943573667711599
2025-01-14 20:37:02,934 [INFO] [40] VALIDATION loss dict: {'classification_loss': 1.8079289523954678}
2025-01-14 20:37:02,934 [INFO] 
2025-01-14 20:37:21,659 [INFO] Step[50/2713]: training loss : 0.9531311142444611 TRAIN  loss dict:  {'classification_loss': 0.9531311142444611}
2025-01-14 20:37:35,102 [INFO] Step[100/2713]: training loss : 0.9411124384403229 TRAIN  loss dict:  {'classification_loss': 0.9411124384403229}
2025-01-14 20:37:49,029 [INFO] Step[150/2713]: training loss : 0.9467290794849396 TRAIN  loss dict:  {'classification_loss': 0.9467290794849396}
2025-01-14 20:38:02,238 [INFO] Step[200/2713]: training loss : 0.9446203541755677 TRAIN  loss dict:  {'classification_loss': 0.9446203541755677}
2025-01-14 20:38:16,428 [INFO] Step[250/2713]: training loss : 0.9430202794075012 TRAIN  loss dict:  {'classification_loss': 0.9430202794075012}
2025-01-14 20:38:29,820 [INFO] Step[300/2713]: training loss : 0.9737948417663574 TRAIN  loss dict:  {'classification_loss': 0.9737948417663574}
2025-01-14 20:38:43,145 [INFO] Step[350/2713]: training loss : 0.9427219927310944 TRAIN  loss dict:  {'classification_loss': 0.9427219927310944}
2025-01-14 20:38:56,979 [INFO] Step[400/2713]: training loss : 0.9417951965332031 TRAIN  loss dict:  {'classification_loss': 0.9417951965332031}
2025-01-14 20:39:10,468 [INFO] Step[450/2713]: training loss : 0.9624825191497802 TRAIN  loss dict:  {'classification_loss': 0.9624825191497802}
2025-01-14 20:39:23,806 [INFO] Step[500/2713]: training loss : 0.9437259423732758 TRAIN  loss dict:  {'classification_loss': 0.9437259423732758}
2025-01-14 20:39:38,106 [INFO] Step[550/2713]: training loss : 0.9435028958320618 TRAIN  loss dict:  {'classification_loss': 0.9435028958320618}
2025-01-14 20:39:52,302 [INFO] Step[600/2713]: training loss : 0.9433969724178314 TRAIN  loss dict:  {'classification_loss': 0.9433969724178314}
2025-01-14 20:40:05,806 [INFO] Step[650/2713]: training loss : 0.9396921730041504 TRAIN  loss dict:  {'classification_loss': 0.9396921730041504}
2025-01-14 20:40:19,036 [INFO] Step[700/2713]: training loss : 0.9414985692501068 TRAIN  loss dict:  {'classification_loss': 0.9414985692501068}
2025-01-14 20:40:32,754 [INFO] Step[750/2713]: training loss : 0.9419551050662994 TRAIN  loss dict:  {'classification_loss': 0.9419551050662994}
2025-01-14 20:40:46,051 [INFO] Step[800/2713]: training loss : 0.9389551055431365 TRAIN  loss dict:  {'classification_loss': 0.9389551055431365}
2025-01-14 20:41:00,001 [INFO] Step[850/2713]: training loss : 0.9397270798683166 TRAIN  loss dict:  {'classification_loss': 0.9397270798683166}
2025-01-14 20:41:13,911 [INFO] Step[900/2713]: training loss : 0.9656140100955963 TRAIN  loss dict:  {'classification_loss': 0.9656140100955963}
2025-01-14 20:41:27,960 [INFO] Step[950/2713]: training loss : 0.9631679105758667 TRAIN  loss dict:  {'classification_loss': 0.9631679105758667}
2025-01-14 20:41:41,323 [INFO] Step[1000/2713]: training loss : 0.947974078655243 TRAIN  loss dict:  {'classification_loss': 0.947974078655243}
2025-01-14 20:41:54,868 [INFO] Step[1050/2713]: training loss : 0.9403465008735656 TRAIN  loss dict:  {'classification_loss': 0.9403465008735656}
2025-01-14 20:42:09,153 [INFO] Step[1100/2713]: training loss : 0.9387248730659485 TRAIN  loss dict:  {'classification_loss': 0.9387248730659485}
2025-01-14 20:42:23,395 [INFO] Step[1150/2713]: training loss : 0.9784063482284546 TRAIN  loss dict:  {'classification_loss': 0.9784063482284546}
2025-01-14 20:42:37,116 [INFO] Step[1200/2713]: training loss : 0.9436719202995301 TRAIN  loss dict:  {'classification_loss': 0.9436719202995301}
2025-01-14 20:42:50,610 [INFO] Step[1250/2713]: training loss : 0.9685041904449463 TRAIN  loss dict:  {'classification_loss': 0.9685041904449463}
2025-01-14 20:43:03,876 [INFO] Step[1300/2713]: training loss : 0.9373771595954895 TRAIN  loss dict:  {'classification_loss': 0.9373771595954895}
2025-01-14 20:43:17,179 [INFO] Step[1350/2713]: training loss : 0.9481899440288544 TRAIN  loss dict:  {'classification_loss': 0.9481899440288544}
2025-01-14 20:43:30,492 [INFO] Step[1400/2713]: training loss : 0.9383031678199768 TRAIN  loss dict:  {'classification_loss': 0.9383031678199768}
2025-01-14 20:43:44,728 [INFO] Step[1450/2713]: training loss : 0.9441069662570953 TRAIN  loss dict:  {'classification_loss': 0.9441069662570953}
2025-01-14 20:44:00,749 [INFO] Step[1500/2713]: training loss : 0.9394073903560638 TRAIN  loss dict:  {'classification_loss': 0.9394073903560638}
2025-01-14 20:44:16,851 [INFO] Step[1550/2713]: training loss : 0.9400447583198548 TRAIN  loss dict:  {'classification_loss': 0.9400447583198548}
2025-01-14 20:44:30,139 [INFO] Step[1600/2713]: training loss : 0.9382150268554688 TRAIN  loss dict:  {'classification_loss': 0.9382150268554688}
2025-01-14 20:44:44,074 [INFO] Step[1650/2713]: training loss : 0.9405880725383758 TRAIN  loss dict:  {'classification_loss': 0.9405880725383758}
2025-01-14 20:44:57,336 [INFO] Step[1700/2713]: training loss : 0.9424189424514771 TRAIN  loss dict:  {'classification_loss': 0.9424189424514771}
2025-01-14 20:45:10,797 [INFO] Step[1750/2713]: training loss : 0.9475373876094818 TRAIN  loss dict:  {'classification_loss': 0.9475373876094818}
2025-01-14 20:45:25,049 [INFO] Step[1800/2713]: training loss : 0.9425115609169006 TRAIN  loss dict:  {'classification_loss': 0.9425115609169006}
2025-01-14 20:45:39,204 [INFO] Step[1850/2713]: training loss : 0.9453184950351715 TRAIN  loss dict:  {'classification_loss': 0.9453184950351715}
2025-01-14 20:45:52,933 [INFO] Step[1900/2713]: training loss : 0.9420193922519684 TRAIN  loss dict:  {'classification_loss': 0.9420193922519684}
2025-01-14 20:46:07,193 [INFO] Step[1950/2713]: training loss : 0.9410211229324341 TRAIN  loss dict:  {'classification_loss': 0.9410211229324341}
2025-01-14 20:46:21,423 [INFO] Step[2000/2713]: training loss : 0.945505577325821 TRAIN  loss dict:  {'classification_loss': 0.945505577325821}
2025-01-14 20:46:34,847 [INFO] Step[2050/2713]: training loss : 0.9406500232219696 TRAIN  loss dict:  {'classification_loss': 0.9406500232219696}
2025-01-14 20:46:48,067 [INFO] Step[2100/2713]: training loss : 0.9377534699440002 TRAIN  loss dict:  {'classification_loss': 0.9377534699440002}
2025-01-14 20:47:01,335 [INFO] Step[2150/2713]: training loss : 0.9389122974872589 TRAIN  loss dict:  {'classification_loss': 0.9389122974872589}
2025-01-14 20:47:14,594 [INFO] Step[2200/2713]: training loss : 0.9492662525177002 TRAIN  loss dict:  {'classification_loss': 0.9492662525177002}
2025-01-14 20:47:27,875 [INFO] Step[2250/2713]: training loss : 0.9394784867763519 TRAIN  loss dict:  {'classification_loss': 0.9394784867763519}
2025-01-14 20:47:41,128 [INFO] Step[2300/2713]: training loss : 0.9406581425666809 TRAIN  loss dict:  {'classification_loss': 0.9406581425666809}
2025-01-14 20:47:54,405 [INFO] Step[2350/2713]: training loss : 0.9426606714725494 TRAIN  loss dict:  {'classification_loss': 0.9426606714725494}
2025-01-14 20:48:07,995 [INFO] Step[2400/2713]: training loss : 0.9408402729034424 TRAIN  loss dict:  {'classification_loss': 0.9408402729034424}
2025-01-14 20:48:21,711 [INFO] Step[2450/2713]: training loss : 0.9567487144470215 TRAIN  loss dict:  {'classification_loss': 0.9567487144470215}
2025-01-14 20:48:35,234 [INFO] Step[2500/2713]: training loss : 0.9448260283470153 TRAIN  loss dict:  {'classification_loss': 0.9448260283470153}
2025-01-14 20:48:49,046 [INFO] Step[2550/2713]: training loss : 0.9455255949497223 TRAIN  loss dict:  {'classification_loss': 0.9455255949497223}
2025-01-14 20:49:02,634 [INFO] Step[2600/2713]: training loss : 0.9392991757392883 TRAIN  loss dict:  {'classification_loss': 0.9392991757392883}
2025-01-14 20:49:15,918 [INFO] Step[2650/2713]: training loss : 0.9423532867431641 TRAIN  loss dict:  {'classification_loss': 0.9423532867431641}
2025-01-14 20:49:29,340 [INFO] Step[2700/2713]: training loss : 0.9397440135478974 TRAIN  loss dict:  {'classification_loss': 0.9397440135478974}
2025-01-14 20:50:56,064 [INFO] Label accuracies statistics:
2025-01-14 20:50:56,065 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 1.0, 203: 0.0, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 1.0, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.5, 390: 1.0, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 20:50:56,067 [INFO] [41] TRAIN  loss: 0.94550169017365 acc: 0.9984027521808576
2025-01-14 20:50:56,067 [INFO] [41] TRAIN  loss dict: {'classification_loss': 0.94550169017365}
2025-01-14 20:50:56,067 [INFO] [41] VALIDATION loss: 1.8167699939550315 VALIDATION acc: 0.7962382445141066
2025-01-14 20:50:56,067 [INFO] [41] VALIDATION loss dict: {'classification_loss': 1.8167699939550315}
2025-01-14 20:50:56,067 [INFO] 
2025-01-14 20:51:14,129 [INFO] Step[50/2713]: training loss : 0.9386380362510681 TRAIN  loss dict:  {'classification_loss': 0.9386380362510681}
2025-01-14 20:51:27,721 [INFO] Step[100/2713]: training loss : 0.9419699048995972 TRAIN  loss dict:  {'classification_loss': 0.9419699048995972}
2025-01-14 20:51:41,327 [INFO] Step[150/2713]: training loss : 0.9419084763526917 TRAIN  loss dict:  {'classification_loss': 0.9419084763526917}
2025-01-14 20:51:54,929 [INFO] Step[200/2713]: training loss : 0.9436816668510437 TRAIN  loss dict:  {'classification_loss': 0.9436816668510437}
2025-01-14 20:52:08,618 [INFO] Step[250/2713]: training loss : 0.9418561708927154 TRAIN  loss dict:  {'classification_loss': 0.9418561708927154}
2025-01-14 20:52:22,873 [INFO] Step[300/2713]: training loss : 0.9379462218284607 TRAIN  loss dict:  {'classification_loss': 0.9379462218284607}
2025-01-14 20:52:36,149 [INFO] Step[350/2713]: training loss : 0.9391260039806366 TRAIN  loss dict:  {'classification_loss': 0.9391260039806366}
2025-01-14 20:52:49,794 [INFO] Step[400/2713]: training loss : 0.9663766741752624 TRAIN  loss dict:  {'classification_loss': 0.9663766741752624}
2025-01-14 20:53:03,372 [INFO] Step[450/2713]: training loss : 0.9653258872032165 TRAIN  loss dict:  {'classification_loss': 0.9653258872032165}
2025-01-14 20:53:16,624 [INFO] Step[500/2713]: training loss : 0.9387209117412567 TRAIN  loss dict:  {'classification_loss': 0.9387209117412567}
2025-01-14 20:53:29,899 [INFO] Step[550/2713]: training loss : 0.9400762116909027 TRAIN  loss dict:  {'classification_loss': 0.9400762116909027}
2025-01-14 20:53:43,486 [INFO] Step[600/2713]: training loss : 0.9388352477550507 TRAIN  loss dict:  {'classification_loss': 0.9388352477550507}
2025-01-14 20:53:57,407 [INFO] Step[650/2713]: training loss : 0.9392345917224884 TRAIN  loss dict:  {'classification_loss': 0.9392345917224884}
2025-01-14 20:54:10,669 [INFO] Step[700/2713]: training loss : 0.9481719136238098 TRAIN  loss dict:  {'classification_loss': 0.9481719136238098}
2025-01-14 20:54:24,241 [INFO] Step[750/2713]: training loss : 0.9356690382957459 TRAIN  loss dict:  {'classification_loss': 0.9356690382957459}
2025-01-14 20:54:37,946 [INFO] Step[800/2713]: training loss : 0.9377272307872773 TRAIN  loss dict:  {'classification_loss': 0.9377272307872773}
2025-01-14 20:54:51,356 [INFO] Step[850/2713]: training loss : 0.9419492244720459 TRAIN  loss dict:  {'classification_loss': 0.9419492244720459}
2025-01-14 20:55:05,492 [INFO] Step[900/2713]: training loss : 0.951249315738678 TRAIN  loss dict:  {'classification_loss': 0.951249315738678}
2025-01-14 20:55:19,049 [INFO] Step[950/2713]: training loss : 0.9443461656570434 TRAIN  loss dict:  {'classification_loss': 0.9443461656570434}
2025-01-14 20:55:32,944 [INFO] Step[1000/2713]: training loss : 0.9418105137348175 TRAIN  loss dict:  {'classification_loss': 0.9418105137348175}
2025-01-14 20:55:46,292 [INFO] Step[1050/2713]: training loss : 0.9425227892398834 TRAIN  loss dict:  {'classification_loss': 0.9425227892398834}
2025-01-14 20:55:59,717 [INFO] Step[1100/2713]: training loss : 0.952956827878952 TRAIN  loss dict:  {'classification_loss': 0.952956827878952}
2025-01-14 20:56:13,326 [INFO] Step[1150/2713]: training loss : 0.9409830951690674 TRAIN  loss dict:  {'classification_loss': 0.9409830951690674}
2025-01-14 20:56:27,323 [INFO] Step[1200/2713]: training loss : 0.9476022219657898 TRAIN  loss dict:  {'classification_loss': 0.9476022219657898}
2025-01-14 20:56:40,923 [INFO] Step[1250/2713]: training loss : 0.9407923412322998 TRAIN  loss dict:  {'classification_loss': 0.9407923412322998}
2025-01-14 20:56:54,601 [INFO] Step[1300/2713]: training loss : 0.9393597638607025 TRAIN  loss dict:  {'classification_loss': 0.9393597638607025}
2025-01-14 20:57:07,855 [INFO] Step[1350/2713]: training loss : 0.9385022532939911 TRAIN  loss dict:  {'classification_loss': 0.9385022532939911}
2025-01-14 20:57:21,326 [INFO] Step[1400/2713]: training loss : 0.9542736041545868 TRAIN  loss dict:  {'classification_loss': 0.9542736041545868}
2025-01-14 20:57:35,506 [INFO] Step[1450/2713]: training loss : 0.9384899306297302 TRAIN  loss dict:  {'classification_loss': 0.9384899306297302}
2025-01-14 20:57:49,140 [INFO] Step[1500/2713]: training loss : 0.9404578542709351 TRAIN  loss dict:  {'classification_loss': 0.9404578542709351}
2025-01-14 20:58:02,936 [INFO] Step[1550/2713]: training loss : 0.9604756200313568 TRAIN  loss dict:  {'classification_loss': 0.9604756200313568}
2025-01-14 20:58:16,662 [INFO] Step[1600/2713]: training loss : 0.9442883443832397 TRAIN  loss dict:  {'classification_loss': 0.9442883443832397}
2025-01-14 20:58:29,958 [INFO] Step[1650/2713]: training loss : 0.9376289319992065 TRAIN  loss dict:  {'classification_loss': 0.9376289319992065}
2025-01-14 20:58:43,678 [INFO] Step[1700/2713]: training loss : 0.9428673660755158 TRAIN  loss dict:  {'classification_loss': 0.9428673660755158}
2025-01-14 20:58:57,323 [INFO] Step[1750/2713]: training loss : 0.9473114478588104 TRAIN  loss dict:  {'classification_loss': 0.9473114478588104}
2025-01-14 20:59:10,999 [INFO] Step[1800/2713]: training loss : 0.9595995342731476 TRAIN  loss dict:  {'classification_loss': 0.9595995342731476}
2025-01-14 20:59:24,525 [INFO] Step[1850/2713]: training loss : 0.942904988527298 TRAIN  loss dict:  {'classification_loss': 0.942904988527298}
2025-01-14 20:59:38,172 [INFO] Step[1900/2713]: training loss : 0.9578910553455353 TRAIN  loss dict:  {'classification_loss': 0.9578910553455353}
2025-01-14 20:59:52,421 [INFO] Step[1950/2713]: training loss : 0.9790751016139985 TRAIN  loss dict:  {'classification_loss': 0.9790751016139985}
2025-01-14 21:00:06,552 [INFO] Step[2000/2713]: training loss : 0.9422355997562408 TRAIN  loss dict:  {'classification_loss': 0.9422355997562408}
2025-01-14 21:00:20,057 [INFO] Step[2050/2713]: training loss : 0.9415279924869537 TRAIN  loss dict:  {'classification_loss': 0.9415279924869537}
2025-01-14 21:00:33,312 [INFO] Step[2100/2713]: training loss : 0.9438266456127167 TRAIN  loss dict:  {'classification_loss': 0.9438266456127167}
2025-01-14 21:00:47,283 [INFO] Step[2150/2713]: training loss : 0.9460824823379517 TRAIN  loss dict:  {'classification_loss': 0.9460824823379517}
2025-01-14 21:01:00,844 [INFO] Step[2200/2713]: training loss : 0.9457532942295075 TRAIN  loss dict:  {'classification_loss': 0.9457532942295075}
2025-01-14 21:01:14,346 [INFO] Step[2250/2713]: training loss : 0.9381593835353851 TRAIN  loss dict:  {'classification_loss': 0.9381593835353851}
2025-01-14 21:01:27,742 [INFO] Step[2300/2713]: training loss : 0.9392429792881012 TRAIN  loss dict:  {'classification_loss': 0.9392429792881012}
2025-01-14 21:01:41,685 [INFO] Step[2350/2713]: training loss : 0.9413109683990478 TRAIN  loss dict:  {'classification_loss': 0.9413109683990478}
2025-01-14 21:01:54,957 [INFO] Step[2400/2713]: training loss : 0.9778659272193909 TRAIN  loss dict:  {'classification_loss': 0.9778659272193909}
2025-01-14 21:02:08,431 [INFO] Step[2450/2713]: training loss : 0.9794868314266205 TRAIN  loss dict:  {'classification_loss': 0.9794868314266205}
2025-01-14 21:02:21,755 [INFO] Step[2500/2713]: training loss : 0.9408793127536774 TRAIN  loss dict:  {'classification_loss': 0.9408793127536774}
2025-01-14 21:02:35,861 [INFO] Step[2550/2713]: training loss : 0.9380255925655365 TRAIN  loss dict:  {'classification_loss': 0.9380255925655365}
2025-01-14 21:02:49,615 [INFO] Step[2600/2713]: training loss : 0.9827379381656647 TRAIN  loss dict:  {'classification_loss': 0.9827379381656647}
2025-01-14 21:03:03,772 [INFO] Step[2650/2713]: training loss : 0.9388561570644378 TRAIN  loss dict:  {'classification_loss': 0.9388561570644378}
2025-01-14 21:03:17,574 [INFO] Step[2700/2713]: training loss : 0.9448475766181946 TRAIN  loss dict:  {'classification_loss': 0.9448475766181946}
2025-01-14 21:04:33,546 [INFO] Label accuracies statistics:
2025-01-14 21:04:33,546 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.25, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.25, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.5, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-14 21:04:33,548 [INFO] [42] TRAIN  loss: 0.9467047045822298 acc: 0.9971741000122866
2025-01-14 21:04:33,548 [INFO] [42] TRAIN  loss dict: {'classification_loss': 0.9467047045822298}
2025-01-14 21:04:33,548 [INFO] [42] VALIDATION loss: 1.8363113686778492 VALIDATION acc: 0.7843260188087774
2025-01-14 21:04:33,548 [INFO] [42] VALIDATION loss dict: {'classification_loss': 1.8363113686778492}
2025-01-14 21:04:33,548 [INFO] 
2025-01-14 21:04:51,556 [INFO] Step[50/2713]: training loss : 0.9355346512794495 TRAIN  loss dict:  {'classification_loss': 0.9355346512794495}
2025-01-14 21:05:05,487 [INFO] Step[100/2713]: training loss : 0.9386731803417205 TRAIN  loss dict:  {'classification_loss': 0.9386731803417205}
2025-01-14 21:05:19,752 [INFO] Step[150/2713]: training loss : 0.9544312214851379 TRAIN  loss dict:  {'classification_loss': 0.9544312214851379}
2025-01-14 21:05:33,566 [INFO] Step[200/2713]: training loss : 0.9446233773231506 TRAIN  loss dict:  {'classification_loss': 0.9446233773231506}
2025-01-14 21:05:47,210 [INFO] Step[250/2713]: training loss : 0.9383929789066314 TRAIN  loss dict:  {'classification_loss': 0.9383929789066314}
2025-01-14 21:06:00,895 [INFO] Step[300/2713]: training loss : 0.9381337666511536 TRAIN  loss dict:  {'classification_loss': 0.9381337666511536}
2025-01-14 21:06:14,428 [INFO] Step[350/2713]: training loss : 0.9372325778007508 TRAIN  loss dict:  {'classification_loss': 0.9372325778007508}
2025-01-14 21:06:28,390 [INFO] Step[400/2713]: training loss : 0.9483406960964202 TRAIN  loss dict:  {'classification_loss': 0.9483406960964202}
2025-01-14 21:06:42,342 [INFO] Step[450/2713]: training loss : 0.9396375894546509 TRAIN  loss dict:  {'classification_loss': 0.9396375894546509}
2025-01-14 21:06:56,195 [INFO] Step[500/2713]: training loss : 0.9372113311290741 TRAIN  loss dict:  {'classification_loss': 0.9372113311290741}
2025-01-14 21:07:09,674 [INFO] Step[550/2713]: training loss : 0.9600487637519837 TRAIN  loss dict:  {'classification_loss': 0.9600487637519837}
2025-01-14 21:07:23,970 [INFO] Step[600/2713]: training loss : 0.9446902072429657 TRAIN  loss dict:  {'classification_loss': 0.9446902072429657}
2025-01-14 21:07:37,948 [INFO] Step[650/2713]: training loss : 0.9406419289112091 TRAIN  loss dict:  {'classification_loss': 0.9406419289112091}
2025-01-14 21:07:51,454 [INFO] Step[700/2713]: training loss : 0.9559755098819732 TRAIN  loss dict:  {'classification_loss': 0.9559755098819732}
2025-01-14 21:08:05,189 [INFO] Step[750/2713]: training loss : 0.9384393560886383 TRAIN  loss dict:  {'classification_loss': 0.9384393560886383}
2025-01-14 21:08:18,821 [INFO] Step[800/2713]: training loss : 0.9387462663650513 TRAIN  loss dict:  {'classification_loss': 0.9387462663650513}
2025-01-14 21:08:32,716 [INFO] Step[850/2713]: training loss : 0.9413257515430451 TRAIN  loss dict:  {'classification_loss': 0.9413257515430451}
2025-01-14 21:08:46,357 [INFO] Step[900/2713]: training loss : 0.9366346430778504 TRAIN  loss dict:  {'classification_loss': 0.9366346430778504}
2025-01-14 21:09:00,237 [INFO] Step[950/2713]: training loss : 0.9389334142208099 TRAIN  loss dict:  {'classification_loss': 0.9389334142208099}
2025-01-14 21:09:14,249 [INFO] Step[1000/2713]: training loss : 0.9399053597450256 TRAIN  loss dict:  {'classification_loss': 0.9399053597450256}
2025-01-14 21:09:28,356 [INFO] Step[1050/2713]: training loss : 0.9460523676872253 TRAIN  loss dict:  {'classification_loss': 0.9460523676872253}
2025-01-14 21:09:42,280 [INFO] Step[1100/2713]: training loss : 0.9386575269699097 TRAIN  loss dict:  {'classification_loss': 0.9386575269699097}
2025-01-14 21:09:55,692 [INFO] Step[1150/2713]: training loss : 0.9484654605388642 TRAIN  loss dict:  {'classification_loss': 0.9484654605388642}
2025-01-14 21:10:09,775 [INFO] Step[1200/2713]: training loss : 0.9385644638538361 TRAIN  loss dict:  {'classification_loss': 0.9385644638538361}
2025-01-14 21:10:23,054 [INFO] Step[1250/2713]: training loss : 0.9376410555839538 TRAIN  loss dict:  {'classification_loss': 0.9376410555839538}
2025-01-14 21:10:36,591 [INFO] Step[1300/2713]: training loss : 0.9500053000450134 TRAIN  loss dict:  {'classification_loss': 0.9500053000450134}
2025-01-14 21:10:50,212 [INFO] Step[1350/2713]: training loss : 0.9424809825420379 TRAIN  loss dict:  {'classification_loss': 0.9424809825420379}
2025-01-14 21:11:03,478 [INFO] Step[1400/2713]: training loss : 0.9433143723011017 TRAIN  loss dict:  {'classification_loss': 0.9433143723011017}
2025-01-14 21:11:16,768 [INFO] Step[1450/2713]: training loss : 0.9376605343818665 TRAIN  loss dict:  {'classification_loss': 0.9376605343818665}
2025-01-14 21:11:30,045 [INFO] Step[1500/2713]: training loss : 0.9396011054515838 TRAIN  loss dict:  {'classification_loss': 0.9396011054515838}
2025-01-14 21:11:43,923 [INFO] Step[1550/2713]: training loss : 0.9464599967002869 TRAIN  loss dict:  {'classification_loss': 0.9464599967002869}
2025-01-14 21:11:57,630 [INFO] Step[1600/2713]: training loss : 0.9396228003501892 TRAIN  loss dict:  {'classification_loss': 0.9396228003501892}
2025-01-14 21:12:11,676 [INFO] Step[1650/2713]: training loss : 0.9403322446346283 TRAIN  loss dict:  {'classification_loss': 0.9403322446346283}
2025-01-14 21:12:25,875 [INFO] Step[1700/2713]: training loss : 0.9466087806224823 TRAIN  loss dict:  {'classification_loss': 0.9466087806224823}
2025-01-14 21:12:40,530 [INFO] Step[1750/2713]: training loss : 0.943859543800354 TRAIN  loss dict:  {'classification_loss': 0.943859543800354}
2025-01-14 21:12:56,170 [INFO] Step[1800/2713]: training loss : 0.9384874379634858 TRAIN  loss dict:  {'classification_loss': 0.9384874379634858}
2025-01-14 21:13:09,449 [INFO] Step[1850/2713]: training loss : 0.9506588816642761 TRAIN  loss dict:  {'classification_loss': 0.9506588816642761}
2025-01-14 21:13:23,191 [INFO] Step[1900/2713]: training loss : 0.9368388819694519 TRAIN  loss dict:  {'classification_loss': 0.9368388819694519}
2025-01-14 21:13:36,496 [INFO] Step[1950/2713]: training loss : 0.9422926092147828 TRAIN  loss dict:  {'classification_loss': 0.9422926092147828}
2025-01-14 21:13:50,596 [INFO] Step[2000/2713]: training loss : 0.9361702954769134 TRAIN  loss dict:  {'classification_loss': 0.9361702954769134}
2025-01-14 21:14:04,644 [INFO] Step[2050/2713]: training loss : 0.9385141038894653 TRAIN  loss dict:  {'classification_loss': 0.9385141038894653}
2025-01-14 21:14:18,413 [INFO] Step[2100/2713]: training loss : 0.9383477151393891 TRAIN  loss dict:  {'classification_loss': 0.9383477151393891}
2025-01-14 21:14:31,739 [INFO] Step[2150/2713]: training loss : 0.9677148628234863 TRAIN  loss dict:  {'classification_loss': 0.9677148628234863}
2025-01-14 21:14:45,061 [INFO] Step[2200/2713]: training loss : 0.9391161799430847 TRAIN  loss dict:  {'classification_loss': 0.9391161799430847}
2025-01-14 21:14:59,061 [INFO] Step[2250/2713]: training loss : 0.9403908705711365 TRAIN  loss dict:  {'classification_loss': 0.9403908705711365}
2025-01-14 21:15:13,020 [INFO] Step[2300/2713]: training loss : 0.94001296043396 TRAIN  loss dict:  {'classification_loss': 0.94001296043396}
2025-01-14 21:15:26,871 [INFO] Step[2350/2713]: training loss : 0.9412871873378754 TRAIN  loss dict:  {'classification_loss': 0.9412871873378754}
2025-01-14 21:15:40,468 [INFO] Step[2400/2713]: training loss : 0.9422861135005951 TRAIN  loss dict:  {'classification_loss': 0.9422861135005951}
2025-01-14 21:15:54,333 [INFO] Step[2450/2713]: training loss : 0.9398056507110596 TRAIN  loss dict:  {'classification_loss': 0.9398056507110596}
2025-01-14 21:16:07,900 [INFO] Step[2500/2713]: training loss : 0.9426569652557373 TRAIN  loss dict:  {'classification_loss': 0.9426569652557373}
2025-01-14 21:16:21,239 [INFO] Step[2550/2713]: training loss : 0.9590351259708405 TRAIN  loss dict:  {'classification_loss': 0.9590351259708405}
2025-01-14 21:16:35,182 [INFO] Step[2600/2713]: training loss : 0.9401102459430695 TRAIN  loss dict:  {'classification_loss': 0.9401102459430695}
2025-01-14 21:16:48,490 [INFO] Step[2650/2713]: training loss : 0.9538407862186432 TRAIN  loss dict:  {'classification_loss': 0.9538407862186432}
2025-01-14 21:17:02,329 [INFO] Step[2700/2713]: training loss : 0.9416206681728363 TRAIN  loss dict:  {'classification_loss': 0.9416206681728363}
2025-01-14 21:18:18,632 [INFO] Label accuracies statistics:
2025-01-14 21:18:18,632 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.25, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.0, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.25, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.5, 187: 1.0, 188: 0.5, 189: 0.25, 190: 0.5, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.75, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.5, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.0, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.5, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 21:18:18,634 [INFO] [43] TRAIN  loss: 0.942903395622716 acc: 0.9984027521808576
2025-01-14 21:18:18,634 [INFO] [43] TRAIN  loss dict: {'classification_loss': 0.942903395622716}
2025-01-14 21:18:18,634 [INFO] [43] VALIDATION loss: 1.8039087075039857 VALIDATION acc: 0.8006269592476489
2025-01-14 21:18:18,634 [INFO] [43] VALIDATION loss dict: {'classification_loss': 1.8039087075039857}
2025-01-14 21:18:18,634 [INFO] 
2025-01-14 21:18:37,414 [INFO] Step[50/2713]: training loss : 0.9467513763904571 TRAIN  loss dict:  {'classification_loss': 0.9467513763904571}
2025-01-14 21:18:50,960 [INFO] Step[100/2713]: training loss : 0.9523456978797913 TRAIN  loss dict:  {'classification_loss': 0.9523456978797913}
2025-01-14 21:19:04,582 [INFO] Step[150/2713]: training loss : 0.9406905496120452 TRAIN  loss dict:  {'classification_loss': 0.9406905496120452}
2025-01-14 21:19:18,043 [INFO] Step[200/2713]: training loss : 0.9386455380916595 TRAIN  loss dict:  {'classification_loss': 0.9386455380916595}
2025-01-14 21:19:31,790 [INFO] Step[250/2713]: training loss : 0.940540441274643 TRAIN  loss dict:  {'classification_loss': 0.940540441274643}
2025-01-14 21:19:45,656 [INFO] Step[300/2713]: training loss : 0.9475037586688996 TRAIN  loss dict:  {'classification_loss': 0.9475037586688996}
2025-01-14 21:19:59,305 [INFO] Step[350/2713]: training loss : 0.9406640303134918 TRAIN  loss dict:  {'classification_loss': 0.9406640303134918}
2025-01-14 21:20:13,517 [INFO] Step[400/2713]: training loss : 0.9376265513896942 TRAIN  loss dict:  {'classification_loss': 0.9376265513896942}
2025-01-14 21:20:27,393 [INFO] Step[450/2713]: training loss : 0.9426806616783142 TRAIN  loss dict:  {'classification_loss': 0.9426806616783142}
2025-01-14 21:20:41,363 [INFO] Step[500/2713]: training loss : 0.9409801137447357 TRAIN  loss dict:  {'classification_loss': 0.9409801137447357}
2025-01-14 21:20:55,192 [INFO] Step[550/2713]: training loss : 0.9398610532283783 TRAIN  loss dict:  {'classification_loss': 0.9398610532283783}
2025-01-14 21:21:08,760 [INFO] Step[600/2713]: training loss : 0.941894977092743 TRAIN  loss dict:  {'classification_loss': 0.941894977092743}
2025-01-14 21:21:22,364 [INFO] Step[650/2713]: training loss : 0.9430850386619568 TRAIN  loss dict:  {'classification_loss': 0.9430850386619568}
2025-01-14 21:21:35,943 [INFO] Step[700/2713]: training loss : 0.9554086816310883 TRAIN  loss dict:  {'classification_loss': 0.9554086816310883}
2025-01-14 21:21:49,481 [INFO] Step[750/2713]: training loss : 0.9362182712554932 TRAIN  loss dict:  {'classification_loss': 0.9362182712554932}
2025-01-14 21:22:02,960 [INFO] Step[800/2713]: training loss : 0.9386011505126953 TRAIN  loss dict:  {'classification_loss': 0.9386011505126953}
2025-01-14 21:22:19,197 [INFO] Step[850/2713]: training loss : 0.937244325876236 TRAIN  loss dict:  {'classification_loss': 0.937244325876236}
2025-01-14 21:22:33,394 [INFO] Step[900/2713]: training loss : 0.9494268679618836 TRAIN  loss dict:  {'classification_loss': 0.9494268679618836}
2025-01-14 21:22:47,470 [INFO] Step[950/2713]: training loss : 0.9454782140254975 TRAIN  loss dict:  {'classification_loss': 0.9454782140254975}
2025-01-14 21:23:01,454 [INFO] Step[1000/2713]: training loss : 0.9378612649440765 TRAIN  loss dict:  {'classification_loss': 0.9378612649440765}
2025-01-14 21:23:15,027 [INFO] Step[1050/2713]: training loss : 0.9376487958431244 TRAIN  loss dict:  {'classification_loss': 0.9376487958431244}
2025-01-14 21:23:28,310 [INFO] Step[1100/2713]: training loss : 0.952044757604599 TRAIN  loss dict:  {'classification_loss': 0.952044757604599}
2025-01-14 21:23:41,762 [INFO] Step[1150/2713]: training loss : 0.9409247529506684 TRAIN  loss dict:  {'classification_loss': 0.9409247529506684}
2025-01-14 21:23:58,310 [INFO] Step[1200/2713]: training loss : 0.9383021342754364 TRAIN  loss dict:  {'classification_loss': 0.9383021342754364}
2025-01-14 21:24:13,371 [INFO] Step[1250/2713]: training loss : 0.9366668188571929 TRAIN  loss dict:  {'classification_loss': 0.9366668188571929}
2025-01-14 21:24:27,148 [INFO] Step[1300/2713]: training loss : 0.9607981204986572 TRAIN  loss dict:  {'classification_loss': 0.9607981204986572}
2025-01-14 21:24:41,310 [INFO] Step[1350/2713]: training loss : 0.937685569524765 TRAIN  loss dict:  {'classification_loss': 0.937685569524765}
2025-01-14 21:24:54,852 [INFO] Step[1400/2713]: training loss : 0.9408058428764343 TRAIN  loss dict:  {'classification_loss': 0.9408058428764343}
2025-01-14 21:25:08,592 [INFO] Step[1450/2713]: training loss : 0.9423897552490235 TRAIN  loss dict:  {'classification_loss': 0.9423897552490235}
2025-01-14 21:25:22,032 [INFO] Step[1500/2713]: training loss : 0.9374580156803131 TRAIN  loss dict:  {'classification_loss': 0.9374580156803131}
2025-01-14 21:25:35,240 [INFO] Step[1550/2713]: training loss : 0.9375178074836731 TRAIN  loss dict:  {'classification_loss': 0.9375178074836731}
2025-01-14 21:25:49,013 [INFO] Step[1600/2713]: training loss : 0.9463738024234771 TRAIN  loss dict:  {'classification_loss': 0.9463738024234771}
2025-01-14 21:26:02,839 [INFO] Step[1650/2713]: training loss : 0.9424712598323822 TRAIN  loss dict:  {'classification_loss': 0.9424712598323822}
2025-01-14 21:26:17,045 [INFO] Step[1700/2713]: training loss : 0.9357433211803436 TRAIN  loss dict:  {'classification_loss': 0.9357433211803436}
2025-01-14 21:26:30,313 [INFO] Step[1750/2713]: training loss : 0.9543111824989319 TRAIN  loss dict:  {'classification_loss': 0.9543111824989319}
2025-01-14 21:26:44,478 [INFO] Step[1800/2713]: training loss : 0.9395421206951141 TRAIN  loss dict:  {'classification_loss': 0.9395421206951141}
2025-01-14 21:26:57,821 [INFO] Step[1850/2713]: training loss : 0.9415861570835113 TRAIN  loss dict:  {'classification_loss': 0.9415861570835113}
2025-01-14 21:27:11,534 [INFO] Step[1900/2713]: training loss : 0.9389311873912811 TRAIN  loss dict:  {'classification_loss': 0.9389311873912811}
2025-01-14 21:27:24,806 [INFO] Step[1950/2713]: training loss : 0.9602317559719086 TRAIN  loss dict:  {'classification_loss': 0.9602317559719086}
2025-01-14 21:27:38,269 [INFO] Step[2000/2713]: training loss : 0.9381650042533874 TRAIN  loss dict:  {'classification_loss': 0.9381650042533874}
2025-01-14 21:27:52,489 [INFO] Step[2050/2713]: training loss : 0.9473772859573364 TRAIN  loss dict:  {'classification_loss': 0.9473772859573364}
2025-01-14 21:28:06,208 [INFO] Step[2100/2713]: training loss : 0.9516289174556732 TRAIN  loss dict:  {'classification_loss': 0.9516289174556732}
2025-01-14 21:28:19,637 [INFO] Step[2150/2713]: training loss : 0.9548478174209595 TRAIN  loss dict:  {'classification_loss': 0.9548478174209595}
2025-01-14 21:28:33,764 [INFO] Step[2200/2713]: training loss : 0.9430300486087799 TRAIN  loss dict:  {'classification_loss': 0.9430300486087799}
2025-01-14 21:28:46,950 [INFO] Step[2250/2713]: training loss : 0.9415851640701294 TRAIN  loss dict:  {'classification_loss': 0.9415851640701294}
2025-01-14 21:29:00,657 [INFO] Step[2300/2713]: training loss : 0.9502075445652008 TRAIN  loss dict:  {'classification_loss': 0.9502075445652008}
2025-01-14 21:29:14,724 [INFO] Step[2350/2713]: training loss : 0.9388782691955566 TRAIN  loss dict:  {'classification_loss': 0.9388782691955566}
2025-01-14 21:29:27,978 [INFO] Step[2400/2713]: training loss : 0.9421847665309906 TRAIN  loss dict:  {'classification_loss': 0.9421847665309906}
2025-01-14 21:29:41,513 [INFO] Step[2450/2713]: training loss : 0.9404801487922668 TRAIN  loss dict:  {'classification_loss': 0.9404801487922668}
2025-01-14 21:29:55,096 [INFO] Step[2500/2713]: training loss : 0.964271719455719 TRAIN  loss dict:  {'classification_loss': 0.964271719455719}
2025-01-14 21:30:09,021 [INFO] Step[2550/2713]: training loss : 0.9389922833442688 TRAIN  loss dict:  {'classification_loss': 0.9389922833442688}
2025-01-14 21:30:22,843 [INFO] Step[2600/2713]: training loss : 0.942557908296585 TRAIN  loss dict:  {'classification_loss': 0.942557908296585}
2025-01-14 21:30:36,781 [INFO] Step[2650/2713]: training loss : 0.9561482667922974 TRAIN  loss dict:  {'classification_loss': 0.9561482667922974}
2025-01-14 21:30:50,454 [INFO] Step[2700/2713]: training loss : 0.9388956069946289 TRAIN  loss dict:  {'classification_loss': 0.9388956069946289}
2025-01-14 21:32:06,526 [INFO] Label accuracies statistics:
2025-01-14 21:32:06,526 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.5, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.3333333333333333, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 1.0, 260: 0.5, 261: 1.0, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 1.0, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 1.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-14 21:32:09,208 [INFO] [44] TRAIN  loss: 0.9437573261028983 acc: 0.9981570217471434
2025-01-14 21:32:09,209 [INFO] [44] TRAIN  loss dict: {'classification_loss': 0.9437573261028983}
2025-01-14 21:32:09,209 [INFO] [44] VALIDATION loss: 1.7502660787195192 VALIDATION acc: 0.8100313479623824
2025-01-14 21:32:09,209 [INFO] [44] VALIDATION loss dict: {'classification_loss': 1.7502660787195192}
2025-01-14 21:32:09,209 [INFO] 
2025-01-14 21:32:27,297 [INFO] Step[50/2713]: training loss : 0.9390167903900146 TRAIN  loss dict:  {'classification_loss': 0.9390167903900146}
2025-01-14 21:32:40,681 [INFO] Step[100/2713]: training loss : 0.9395412886142731 TRAIN  loss dict:  {'classification_loss': 0.9395412886142731}
2025-01-14 21:32:54,230 [INFO] Step[150/2713]: training loss : 0.9391662240028381 TRAIN  loss dict:  {'classification_loss': 0.9391662240028381}
2025-01-14 21:33:08,302 [INFO] Step[200/2713]: training loss : 0.943707765340805 TRAIN  loss dict:  {'classification_loss': 0.943707765340805}
2025-01-14 21:33:21,828 [INFO] Step[250/2713]: training loss : 0.9355562019348145 TRAIN  loss dict:  {'classification_loss': 0.9355562019348145}
2025-01-14 21:33:35,983 [INFO] Step[300/2713]: training loss : 0.9374957799911499 TRAIN  loss dict:  {'classification_loss': 0.9374957799911499}
2025-01-14 21:33:49,902 [INFO] Step[350/2713]: training loss : 0.9481672632694245 TRAIN  loss dict:  {'classification_loss': 0.9481672632694245}
2025-01-14 21:34:04,002 [INFO] Step[400/2713]: training loss : 0.9380438590049743 TRAIN  loss dict:  {'classification_loss': 0.9380438590049743}
2025-01-14 21:34:17,637 [INFO] Step[450/2713]: training loss : 0.9434069740772247 TRAIN  loss dict:  {'classification_loss': 0.9434069740772247}
2025-01-14 21:34:31,093 [INFO] Step[500/2713]: training loss : 0.9376392376422882 TRAIN  loss dict:  {'classification_loss': 0.9376392376422882}
2025-01-14 21:34:44,540 [INFO] Step[550/2713]: training loss : 0.9458640122413635 TRAIN  loss dict:  {'classification_loss': 0.9458640122413635}
2025-01-14 21:34:58,252 [INFO] Step[600/2713]: training loss : 0.9357662522792816 TRAIN  loss dict:  {'classification_loss': 0.9357662522792816}
2025-01-14 21:35:12,235 [INFO] Step[650/2713]: training loss : 0.94030482172966 TRAIN  loss dict:  {'classification_loss': 0.94030482172966}
2025-01-14 21:35:25,681 [INFO] Step[700/2713]: training loss : 0.968873153924942 TRAIN  loss dict:  {'classification_loss': 0.968873153924942}
2025-01-14 21:35:38,966 [INFO] Step[750/2713]: training loss : 0.9601949095726013 TRAIN  loss dict:  {'classification_loss': 0.9601949095726013}
2025-01-14 21:35:52,530 [INFO] Step[800/2713]: training loss : 0.9395994210243225 TRAIN  loss dict:  {'classification_loss': 0.9395994210243225}
2025-01-14 21:36:06,307 [INFO] Step[850/2713]: training loss : 0.9343306171894074 TRAIN  loss dict:  {'classification_loss': 0.9343306171894074}
2025-01-14 21:36:20,135 [INFO] Step[900/2713]: training loss : 0.9368053901195527 TRAIN  loss dict:  {'classification_loss': 0.9368053901195527}
2025-01-14 21:36:33,832 [INFO] Step[950/2713]: training loss : 0.9421357870101928 TRAIN  loss dict:  {'classification_loss': 0.9421357870101928}
2025-01-14 21:36:47,227 [INFO] Step[1000/2713]: training loss : 0.9385163688659668 TRAIN  loss dict:  {'classification_loss': 0.9385163688659668}
2025-01-14 21:37:00,739 [INFO] Step[1050/2713]: training loss : 0.9581061661243438 TRAIN  loss dict:  {'classification_loss': 0.9581061661243438}
2025-01-14 21:37:14,577 [INFO] Step[1100/2713]: training loss : 0.9381691372394562 TRAIN  loss dict:  {'classification_loss': 0.9381691372394562}
2025-01-14 21:37:28,443 [INFO] Step[1150/2713]: training loss : 0.9372318959236146 TRAIN  loss dict:  {'classification_loss': 0.9372318959236146}
2025-01-14 21:37:42,142 [INFO] Step[1200/2713]: training loss : 0.9745456445217132 TRAIN  loss dict:  {'classification_loss': 0.9745456445217132}
2025-01-14 21:37:55,775 [INFO] Step[1250/2713]: training loss : 0.9371802353858948 TRAIN  loss dict:  {'classification_loss': 0.9371802353858948}
2025-01-14 21:38:09,795 [INFO] Step[1300/2713]: training loss : 0.9409136199951171 TRAIN  loss dict:  {'classification_loss': 0.9409136199951171}
2025-01-14 21:38:23,355 [INFO] Step[1350/2713]: training loss : 0.9466030502319336 TRAIN  loss dict:  {'classification_loss': 0.9466030502319336}
2025-01-14 21:38:37,476 [INFO] Step[1400/2713]: training loss : 0.9407379746437072 TRAIN  loss dict:  {'classification_loss': 0.9407379746437072}
2025-01-14 21:38:50,874 [INFO] Step[1450/2713]: training loss : 0.9856528353691101 TRAIN  loss dict:  {'classification_loss': 0.9856528353691101}
2025-01-14 21:39:04,751 [INFO] Step[1500/2713]: training loss : 0.9382058048248291 TRAIN  loss dict:  {'classification_loss': 0.9382058048248291}
2025-01-14 21:39:18,302 [INFO] Step[1550/2713]: training loss : 0.949817214012146 TRAIN  loss dict:  {'classification_loss': 0.949817214012146}
2025-01-14 21:39:31,874 [INFO] Step[1600/2713]: training loss : 0.9427321481704712 TRAIN  loss dict:  {'classification_loss': 0.9427321481704712}
2025-01-14 21:39:45,073 [INFO] Step[1650/2713]: training loss : 0.9627021396160126 TRAIN  loss dict:  {'classification_loss': 0.9627021396160126}
2025-01-14 21:39:58,575 [INFO] Step[1700/2713]: training loss : 0.9391398966312409 TRAIN  loss dict:  {'classification_loss': 0.9391398966312409}
2025-01-14 21:40:12,759 [INFO] Step[1750/2713]: training loss : 0.9391388607025146 TRAIN  loss dict:  {'classification_loss': 0.9391388607025146}
2025-01-14 21:40:26,996 [INFO] Step[1800/2713]: training loss : 0.9363308250904083 TRAIN  loss dict:  {'classification_loss': 0.9363308250904083}
2025-01-14 21:40:40,229 [INFO] Step[1850/2713]: training loss : 0.9382255589962005 TRAIN  loss dict:  {'classification_loss': 0.9382255589962005}
2025-01-14 21:40:54,002 [INFO] Step[1900/2713]: training loss : 0.93929203748703 TRAIN  loss dict:  {'classification_loss': 0.93929203748703}
2025-01-14 21:41:07,657 [INFO] Step[1950/2713]: training loss : 0.9489703297615051 TRAIN  loss dict:  {'classification_loss': 0.9489703297615051}
2025-01-14 21:41:20,862 [INFO] Step[2000/2713]: training loss : 0.938490915298462 TRAIN  loss dict:  {'classification_loss': 0.938490915298462}
2025-01-14 21:41:34,407 [INFO] Step[2050/2713]: training loss : 0.9429269361495972 TRAIN  loss dict:  {'classification_loss': 0.9429269361495972}
2025-01-14 21:41:47,674 [INFO] Step[2100/2713]: training loss : 0.9427637374401092 TRAIN  loss dict:  {'classification_loss': 0.9427637374401092}
2025-01-14 21:42:01,560 [INFO] Step[2150/2713]: training loss : 0.9382343137264252 TRAIN  loss dict:  {'classification_loss': 0.9382343137264252}
2025-01-14 21:42:15,563 [INFO] Step[2200/2713]: training loss : 0.9425042235851288 TRAIN  loss dict:  {'classification_loss': 0.9425042235851288}
2025-01-14 21:42:28,769 [INFO] Step[2250/2713]: training loss : 0.9498064970970154 TRAIN  loss dict:  {'classification_loss': 0.9498064970970154}
2025-01-14 21:42:42,326 [INFO] Step[2300/2713]: training loss : 0.9416687440872192 TRAIN  loss dict:  {'classification_loss': 0.9416687440872192}
2025-01-14 21:42:57,030 [INFO] Step[2350/2713]: training loss : 0.9374006390571594 TRAIN  loss dict:  {'classification_loss': 0.9374006390571594}
2025-01-14 21:43:12,710 [INFO] Step[2400/2713]: training loss : 0.9452095353603363 TRAIN  loss dict:  {'classification_loss': 0.9452095353603363}
2025-01-14 21:43:26,190 [INFO] Step[2450/2713]: training loss : 0.9379606103897095 TRAIN  loss dict:  {'classification_loss': 0.9379606103897095}
2025-01-14 21:43:39,794 [INFO] Step[2500/2713]: training loss : 0.9500362753868103 TRAIN  loss dict:  {'classification_loss': 0.9500362753868103}
2025-01-14 21:43:53,797 [INFO] Step[2550/2713]: training loss : 0.9363582682609558 TRAIN  loss dict:  {'classification_loss': 0.9363582682609558}
2025-01-14 21:44:07,465 [INFO] Step[2600/2713]: training loss : 0.9356946563720703 TRAIN  loss dict:  {'classification_loss': 0.9356946563720703}
2025-01-14 21:44:21,280 [INFO] Step[2650/2713]: training loss : 0.9348727405071259 TRAIN  loss dict:  {'classification_loss': 0.9348727405071259}
2025-01-14 21:44:35,193 [INFO] Step[2700/2713]: training loss : 0.942702797651291 TRAIN  loss dict:  {'classification_loss': 0.942702797651291}
2025-01-14 21:45:51,260 [INFO] Label accuracies statistics:
2025-01-14 21:45:51,260 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 0.75, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 1.0, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.5, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.75, 291: 1.0, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.5, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 0.75, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 21:45:51,262 [INFO] [45] TRAIN  loss: 0.9436964850077715 acc: 0.9981570217471434
2025-01-14 21:45:51,262 [INFO] [45] TRAIN  loss dict: {'classification_loss': 0.9436964850077715}
2025-01-14 21:45:51,262 [INFO] [45] VALIDATION loss: 1.809734078726374 VALIDATION acc: 0.7880877742946708
2025-01-14 21:45:51,262 [INFO] [45] VALIDATION loss dict: {'classification_loss': 1.809734078726374}
2025-01-14 21:45:51,262 [INFO] 
2025-01-14 21:46:10,122 [INFO] Step[50/2713]: training loss : 0.9377955770492554 TRAIN  loss dict:  {'classification_loss': 0.9377955770492554}
2025-01-14 21:46:23,924 [INFO] Step[100/2713]: training loss : 0.9376287174224853 TRAIN  loss dict:  {'classification_loss': 0.9376287174224853}
2025-01-14 21:46:37,765 [INFO] Step[150/2713]: training loss : 0.9394108963012695 TRAIN  loss dict:  {'classification_loss': 0.9394108963012695}
2025-01-14 21:46:51,598 [INFO] Step[200/2713]: training loss : 0.9369491362571716 TRAIN  loss dict:  {'classification_loss': 0.9369491362571716}
2025-01-14 21:47:04,975 [INFO] Step[250/2713]: training loss : 0.9378052306175232 TRAIN  loss dict:  {'classification_loss': 0.9378052306175232}
2025-01-14 21:47:19,218 [INFO] Step[300/2713]: training loss : 0.9375370359420776 TRAIN  loss dict:  {'classification_loss': 0.9375370359420776}
2025-01-14 21:47:32,761 [INFO] Step[350/2713]: training loss : 0.9373890888690949 TRAIN  loss dict:  {'classification_loss': 0.9373890888690949}
2025-01-14 21:47:46,639 [INFO] Step[400/2713]: training loss : 0.9384070754051208 TRAIN  loss dict:  {'classification_loss': 0.9384070754051208}
2025-01-14 21:48:00,085 [INFO] Step[450/2713]: training loss : 0.9393486976623535 TRAIN  loss dict:  {'classification_loss': 0.9393486976623535}
2025-01-14 21:48:13,772 [INFO] Step[500/2713]: training loss : 0.939667866230011 TRAIN  loss dict:  {'classification_loss': 0.939667866230011}
2025-01-14 21:48:27,223 [INFO] Step[550/2713]: training loss : 0.9372331476211548 TRAIN  loss dict:  {'classification_loss': 0.9372331476211548}
2025-01-14 21:48:40,929 [INFO] Step[600/2713]: training loss : 0.9460931527614593 TRAIN  loss dict:  {'classification_loss': 0.9460931527614593}
2025-01-14 21:48:54,629 [INFO] Step[650/2713]: training loss : 0.9566097700595856 TRAIN  loss dict:  {'classification_loss': 0.9566097700595856}
2025-01-14 21:49:08,318 [INFO] Step[700/2713]: training loss : 0.9389553105831147 TRAIN  loss dict:  {'classification_loss': 0.9389553105831147}
2025-01-14 21:49:22,501 [INFO] Step[750/2713]: training loss : 0.9375763392448425 TRAIN  loss dict:  {'classification_loss': 0.9375763392448425}
2025-01-14 21:49:35,841 [INFO] Step[800/2713]: training loss : 0.9448661041259766 TRAIN  loss dict:  {'classification_loss': 0.9448661041259766}
2025-01-14 21:49:49,931 [INFO] Step[850/2713]: training loss : 0.970291451215744 TRAIN  loss dict:  {'classification_loss': 0.970291451215744}
2025-01-14 21:50:03,752 [INFO] Step[900/2713]: training loss : 0.9378549659252167 TRAIN  loss dict:  {'classification_loss': 0.9378549659252167}
2025-01-14 21:50:17,942 [INFO] Step[950/2713]: training loss : 0.9698480355739594 TRAIN  loss dict:  {'classification_loss': 0.9698480355739594}
2025-01-14 21:50:31,543 [INFO] Step[1000/2713]: training loss : 0.9405594420433044 TRAIN  loss dict:  {'classification_loss': 0.9405594420433044}
2025-01-14 21:50:45,407 [INFO] Step[1050/2713]: training loss : 0.9464960718154907 TRAIN  loss dict:  {'classification_loss': 0.9464960718154907}
2025-01-14 21:50:59,277 [INFO] Step[1100/2713]: training loss : 0.9379938840866089 TRAIN  loss dict:  {'classification_loss': 0.9379938840866089}
2025-01-14 21:51:13,465 [INFO] Step[1150/2713]: training loss : 0.9460051953792572 TRAIN  loss dict:  {'classification_loss': 0.9460051953792572}
2025-01-14 21:51:27,397 [INFO] Step[1200/2713]: training loss : 0.984295381307602 TRAIN  loss dict:  {'classification_loss': 0.984295381307602}
2025-01-14 21:51:41,175 [INFO] Step[1250/2713]: training loss : 0.9365168154239655 TRAIN  loss dict:  {'classification_loss': 0.9365168154239655}
2025-01-14 21:51:55,357 [INFO] Step[1300/2713]: training loss : 0.9418614292144776 TRAIN  loss dict:  {'classification_loss': 0.9418614292144776}
2025-01-14 21:52:11,644 [INFO] Step[1350/2713]: training loss : 0.9385469090938569 TRAIN  loss dict:  {'classification_loss': 0.9385469090938569}
2025-01-14 21:52:26,421 [INFO] Step[1400/2713]: training loss : 0.9592051029205322 TRAIN  loss dict:  {'classification_loss': 0.9592051029205322}
2025-01-14 21:52:39,607 [INFO] Step[1450/2713]: training loss : 0.938101407289505 TRAIN  loss dict:  {'classification_loss': 0.938101407289505}
2025-01-14 21:52:52,923 [INFO] Step[1500/2713]: training loss : 0.964903382062912 TRAIN  loss dict:  {'classification_loss': 0.964903382062912}
2025-01-14 21:53:06,361 [INFO] Step[1550/2713]: training loss : 0.9389584422111511 TRAIN  loss dict:  {'classification_loss': 0.9389584422111511}
2025-01-14 21:53:20,025 [INFO] Step[1600/2713]: training loss : 0.9626432085037231 TRAIN  loss dict:  {'classification_loss': 0.9626432085037231}
2025-01-14 21:53:34,083 [INFO] Step[1650/2713]: training loss : 0.9381348717212677 TRAIN  loss dict:  {'classification_loss': 0.9381348717212677}
2025-01-14 21:53:47,652 [INFO] Step[1700/2713]: training loss : 0.9408166718482971 TRAIN  loss dict:  {'classification_loss': 0.9408166718482971}
2025-01-14 21:54:00,863 [INFO] Step[1750/2713]: training loss : 0.9588169622421264 TRAIN  loss dict:  {'classification_loss': 0.9588169622421264}
2025-01-14 21:54:15,005 [INFO] Step[1800/2713]: training loss : 0.9365491402149201 TRAIN  loss dict:  {'classification_loss': 0.9365491402149201}
2025-01-14 21:54:28,489 [INFO] Step[1850/2713]: training loss : 0.9381006896495819 TRAIN  loss dict:  {'classification_loss': 0.9381006896495819}
2025-01-14 21:54:42,453 [INFO] Step[1900/2713]: training loss : 0.9408224892616271 TRAIN  loss dict:  {'classification_loss': 0.9408224892616271}
2025-01-14 21:54:55,947 [INFO] Step[1950/2713]: training loss : 0.9604500341415405 TRAIN  loss dict:  {'classification_loss': 0.9604500341415405}
2025-01-14 21:55:09,595 [INFO] Step[2000/2713]: training loss : 0.9392461693286895 TRAIN  loss dict:  {'classification_loss': 0.9392461693286895}
2025-01-14 21:55:23,271 [INFO] Step[2050/2713]: training loss : 0.9685088419914245 TRAIN  loss dict:  {'classification_loss': 0.9685088419914245}
2025-01-14 21:55:36,911 [INFO] Step[2100/2713]: training loss : 0.9552655291557312 TRAIN  loss dict:  {'classification_loss': 0.9552655291557312}
2025-01-14 21:55:50,266 [INFO] Step[2150/2713]: training loss : 0.9410796344280243 TRAIN  loss dict:  {'classification_loss': 0.9410796344280243}
2025-01-14 21:56:04,075 [INFO] Step[2200/2713]: training loss : 0.9357683086395263 TRAIN  loss dict:  {'classification_loss': 0.9357683086395263}
2025-01-14 21:56:18,283 [INFO] Step[2250/2713]: training loss : 0.9413105940818787 TRAIN  loss dict:  {'classification_loss': 0.9413105940818787}
2025-01-14 21:56:31,887 [INFO] Step[2300/2713]: training loss : 0.9377233457565307 TRAIN  loss dict:  {'classification_loss': 0.9377233457565307}
2025-01-14 21:56:45,578 [INFO] Step[2350/2713]: training loss : 0.9405450928211212 TRAIN  loss dict:  {'classification_loss': 0.9405450928211212}
2025-01-14 21:56:59,101 [INFO] Step[2400/2713]: training loss : 0.9649541532993317 TRAIN  loss dict:  {'classification_loss': 0.9649541532993317}
2025-01-14 21:57:12,558 [INFO] Step[2450/2713]: training loss : 0.9378896927833558 TRAIN  loss dict:  {'classification_loss': 0.9378896927833558}
2025-01-14 21:57:26,501 [INFO] Step[2500/2713]: training loss : 0.9385433220863342 TRAIN  loss dict:  {'classification_loss': 0.9385433220863342}
2025-01-14 21:57:39,714 [INFO] Step[2550/2713]: training loss : 0.9366005909442902 TRAIN  loss dict:  {'classification_loss': 0.9366005909442902}
2025-01-14 21:57:53,452 [INFO] Step[2600/2713]: training loss : 0.9473501813411712 TRAIN  loss dict:  {'classification_loss': 0.9473501813411712}
2025-01-14 21:58:07,264 [INFO] Step[2650/2713]: training loss : 0.9582144093513488 TRAIN  loss dict:  {'classification_loss': 0.9582144093513488}
2025-01-14 21:58:21,185 [INFO] Step[2700/2713]: training loss : 0.9379851174354553 TRAIN  loss dict:  {'classification_loss': 0.9379851174354553}
2025-01-14 21:59:37,602 [INFO] Label accuracies statistics:
2025-01-14 21:59:37,602 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 0.25, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.75, 207: 1.0, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 1.0, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 1.0, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 1.0, 385: 0.5, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 21:59:40,040 [INFO] [46] TRAIN  loss: 0.9453716455374175 acc: 0.9976655608797149
2025-01-14 21:59:40,041 [INFO] [46] TRAIN  loss dict: {'classification_loss': 0.9453716455374175}
2025-01-14 21:59:40,041 [INFO] [46] VALIDATION loss: 1.748652853799942 VALIDATION acc: 0.8112852664576803
2025-01-14 21:59:40,041 [INFO] [46] VALIDATION loss dict: {'classification_loss': 1.748652853799942}
2025-01-14 21:59:40,041 [INFO] 
2025-01-14 21:59:58,860 [INFO] Step[50/2713]: training loss : 0.942730907201767 TRAIN  loss dict:  {'classification_loss': 0.942730907201767}
2025-01-14 22:00:12,359 [INFO] Step[100/2713]: training loss : 0.9433902752399445 TRAIN  loss dict:  {'classification_loss': 0.9433902752399445}
2025-01-14 22:00:26,453 [INFO] Step[150/2713]: training loss : 0.9394538223743438 TRAIN  loss dict:  {'classification_loss': 0.9394538223743438}
2025-01-14 22:00:40,287 [INFO] Step[200/2713]: training loss : 0.9381041061878205 TRAIN  loss dict:  {'classification_loss': 0.9381041061878205}
2025-01-14 22:00:53,716 [INFO] Step[250/2713]: training loss : 0.9366458821296691 TRAIN  loss dict:  {'classification_loss': 0.9366458821296691}
2025-01-14 22:01:07,420 [INFO] Step[300/2713]: training loss : 0.9370688712596893 TRAIN  loss dict:  {'classification_loss': 0.9370688712596893}
2025-01-14 22:01:20,863 [INFO] Step[350/2713]: training loss : 0.9389422690868378 TRAIN  loss dict:  {'classification_loss': 0.9389422690868378}
2025-01-14 22:01:34,463 [INFO] Step[400/2713]: training loss : 0.9367889261245728 TRAIN  loss dict:  {'classification_loss': 0.9367889261245728}
2025-01-14 22:01:47,710 [INFO] Step[450/2713]: training loss : 0.9537054991722107 TRAIN  loss dict:  {'classification_loss': 0.9537054991722107}
2025-01-14 22:02:01,646 [INFO] Step[500/2713]: training loss : 0.9469346964359283 TRAIN  loss dict:  {'classification_loss': 0.9469346964359283}
2025-01-14 22:02:15,842 [INFO] Step[550/2713]: training loss : 0.9698356676101685 TRAIN  loss dict:  {'classification_loss': 0.9698356676101685}
2025-01-14 22:02:29,426 [INFO] Step[600/2713]: training loss : 0.9682079768180847 TRAIN  loss dict:  {'classification_loss': 0.9682079768180847}
2025-01-14 22:02:42,840 [INFO] Step[650/2713]: training loss : 0.9384381639957428 TRAIN  loss dict:  {'classification_loss': 0.9384381639957428}
2025-01-14 22:02:56,050 [INFO] Step[700/2713]: training loss : 0.9465648865699768 TRAIN  loss dict:  {'classification_loss': 0.9465648865699768}
2025-01-14 22:03:09,293 [INFO] Step[750/2713]: training loss : 0.9503730881214142 TRAIN  loss dict:  {'classification_loss': 0.9503730881214142}
2025-01-14 22:03:22,461 [INFO] Step[800/2713]: training loss : 0.9360472524166107 TRAIN  loss dict:  {'classification_loss': 0.9360472524166107}
2025-01-14 22:03:35,699 [INFO] Step[850/2713]: training loss : 0.9673293864727021 TRAIN  loss dict:  {'classification_loss': 0.9673293864727021}
2025-01-14 22:03:49,273 [INFO] Step[900/2713]: training loss : 0.9366135311126709 TRAIN  loss dict:  {'classification_loss': 0.9366135311126709}
2025-01-14 22:04:03,149 [INFO] Step[950/2713]: training loss : 0.9487916159629822 TRAIN  loss dict:  {'classification_loss': 0.9487916159629822}
2025-01-14 22:04:16,630 [INFO] Step[1000/2713]: training loss : 0.9434133231639862 TRAIN  loss dict:  {'classification_loss': 0.9434133231639862}
2025-01-14 22:04:29,829 [INFO] Step[1050/2713]: training loss : 0.9383685004711151 TRAIN  loss dict:  {'classification_loss': 0.9383685004711151}
2025-01-14 22:04:43,414 [INFO] Step[1100/2713]: training loss : 0.9459212374687195 TRAIN  loss dict:  {'classification_loss': 0.9459212374687195}
2025-01-14 22:04:56,645 [INFO] Step[1150/2713]: training loss : 0.9376782798767089 TRAIN  loss dict:  {'classification_loss': 0.9376782798767089}
2025-01-14 22:05:10,259 [INFO] Step[1200/2713]: training loss : 0.9372983288764953 TRAIN  loss dict:  {'classification_loss': 0.9372983288764953}
2025-01-14 22:05:23,682 [INFO] Step[1250/2713]: training loss : 0.9453462111949921 TRAIN  loss dict:  {'classification_loss': 0.9453462111949921}
2025-01-14 22:05:37,668 [INFO] Step[1300/2713]: training loss : 0.943934588432312 TRAIN  loss dict:  {'classification_loss': 0.943934588432312}
2025-01-14 22:05:51,275 [INFO] Step[1350/2713]: training loss : 0.9378726541996002 TRAIN  loss dict:  {'classification_loss': 0.9378726541996002}
2025-01-14 22:06:04,987 [INFO] Step[1400/2713]: training loss : 0.9391212344169617 TRAIN  loss dict:  {'classification_loss': 0.9391212344169617}
2025-01-14 22:06:18,778 [INFO] Step[1450/2713]: training loss : 0.937354896068573 TRAIN  loss dict:  {'classification_loss': 0.937354896068573}
2025-01-14 22:06:32,611 [INFO] Step[1500/2713]: training loss : 0.9441934287548065 TRAIN  loss dict:  {'classification_loss': 0.9441934287548065}
2025-01-14 22:06:46,522 [INFO] Step[1550/2713]: training loss : 0.9582159090042114 TRAIN  loss dict:  {'classification_loss': 0.9582159090042114}
2025-01-14 22:07:00,248 [INFO] Step[1600/2713]: training loss : 0.948083827495575 TRAIN  loss dict:  {'classification_loss': 0.948083827495575}
2025-01-14 22:07:14,313 [INFO] Step[1650/2713]: training loss : 0.9375461840629578 TRAIN  loss dict:  {'classification_loss': 0.9375461840629578}
2025-01-14 22:07:28,194 [INFO] Step[1700/2713]: training loss : 0.9356800377368927 TRAIN  loss dict:  {'classification_loss': 0.9356800377368927}
2025-01-14 22:07:41,801 [INFO] Step[1750/2713]: training loss : 0.9407595193386078 TRAIN  loss dict:  {'classification_loss': 0.9407595193386078}
2025-01-14 22:07:55,483 [INFO] Step[1800/2713]: training loss : 0.9398241746425628 TRAIN  loss dict:  {'classification_loss': 0.9398241746425628}
2025-01-14 22:08:08,963 [INFO] Step[1850/2713]: training loss : 0.9382459819316864 TRAIN  loss dict:  {'classification_loss': 0.9382459819316864}
2025-01-14 22:08:22,182 [INFO] Step[1900/2713]: training loss : 0.9705836796760559 TRAIN  loss dict:  {'classification_loss': 0.9705836796760559}
2025-01-14 22:08:36,081 [INFO] Step[1950/2713]: training loss : 0.9408805346488953 TRAIN  loss dict:  {'classification_loss': 0.9408805346488953}
2025-01-14 22:08:49,899 [INFO] Step[2000/2713]: training loss : 0.9382011425495148 TRAIN  loss dict:  {'classification_loss': 0.9382011425495148}
2025-01-14 22:09:03,093 [INFO] Step[2050/2713]: training loss : 0.9447405230998993 TRAIN  loss dict:  {'classification_loss': 0.9447405230998993}
2025-01-14 22:09:16,329 [INFO] Step[2100/2713]: training loss : 0.9368345057964325 TRAIN  loss dict:  {'classification_loss': 0.9368345057964325}
2025-01-14 22:09:29,546 [INFO] Step[2150/2713]: training loss : 0.9957855319976807 TRAIN  loss dict:  {'classification_loss': 0.9957855319976807}
2025-01-14 22:09:43,129 [INFO] Step[2200/2713]: training loss : 0.9403594553470611 TRAIN  loss dict:  {'classification_loss': 0.9403594553470611}
2025-01-14 22:09:56,342 [INFO] Step[2250/2713]: training loss : 0.9663942503929138 TRAIN  loss dict:  {'classification_loss': 0.9663942503929138}
2025-01-14 22:10:09,562 [INFO] Step[2300/2713]: training loss : 0.9605525255203247 TRAIN  loss dict:  {'classification_loss': 0.9605525255203247}
2025-01-14 22:10:23,234 [INFO] Step[2350/2713]: training loss : 0.9417462384700775 TRAIN  loss dict:  {'classification_loss': 0.9417462384700775}
2025-01-14 22:10:37,035 [INFO] Step[2400/2713]: training loss : 0.9349149072170257 TRAIN  loss dict:  {'classification_loss': 0.9349149072170257}
2025-01-14 22:10:50,765 [INFO] Step[2450/2713]: training loss : 0.9385349249839783 TRAIN  loss dict:  {'classification_loss': 0.9385349249839783}
2025-01-14 22:11:04,201 [INFO] Step[2500/2713]: training loss : 0.9379095101356506 TRAIN  loss dict:  {'classification_loss': 0.9379095101356506}
2025-01-14 22:11:17,796 [INFO] Step[2550/2713]: training loss : 0.9392203545570373 TRAIN  loss dict:  {'classification_loss': 0.9392203545570373}
2025-01-14 22:11:31,194 [INFO] Step[2600/2713]: training loss : 0.9617159020900726 TRAIN  loss dict:  {'classification_loss': 0.9617159020900726}
2025-01-14 22:11:44,726 [INFO] Step[2650/2713]: training loss : 0.9376181137561798 TRAIN  loss dict:  {'classification_loss': 0.9376181137561798}
2025-01-14 22:11:58,372 [INFO] Step[2700/2713]: training loss : 0.9376285123825073 TRAIN  loss dict:  {'classification_loss': 0.9376285123825073}
2025-01-14 22:13:14,510 [INFO] Label accuracies statistics:
2025-01-14 22:13:14,510 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.0, 204: 0.5, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 22:13:14,512 [INFO] [47] TRAIN  loss: 0.9453236982253834 acc: 0.997788426096572
2025-01-14 22:13:14,512 [INFO] [47] TRAIN  loss dict: {'classification_loss': 0.9453236982253834}
2025-01-14 22:13:14,512 [INFO] [47] VALIDATION loss: 1.7979527542689688 VALIDATION acc: 0.7981191222570533
2025-01-14 22:13:14,512 [INFO] [47] VALIDATION loss dict: {'classification_loss': 1.7979527542689688}
2025-01-14 22:13:14,512 [INFO] 
2025-01-14 22:13:33,411 [INFO] Step[50/2713]: training loss : 0.9444462919235229 TRAIN  loss dict:  {'classification_loss': 0.9444462919235229}
2025-01-14 22:13:47,446 [INFO] Step[100/2713]: training loss : 0.9423167741298676 TRAIN  loss dict:  {'classification_loss': 0.9423167741298676}
2025-01-14 22:14:00,765 [INFO] Step[150/2713]: training loss : 0.9818828582763672 TRAIN  loss dict:  {'classification_loss': 0.9818828582763672}
2025-01-14 22:14:14,981 [INFO] Step[200/2713]: training loss : 0.9690891075134277 TRAIN  loss dict:  {'classification_loss': 0.9690891075134277}
2025-01-14 22:14:28,438 [INFO] Step[250/2713]: training loss : 0.9392514646053314 TRAIN  loss dict:  {'classification_loss': 0.9392514646053314}
2025-01-14 22:14:41,743 [INFO] Step[300/2713]: training loss : 0.9390757060050965 TRAIN  loss dict:  {'classification_loss': 0.9390757060050965}
2025-01-14 22:14:57,324 [INFO] Step[350/2713]: training loss : 0.936337571144104 TRAIN  loss dict:  {'classification_loss': 0.936337571144104}
2025-01-14 22:15:13,331 [INFO] Step[400/2713]: training loss : 0.938939073085785 TRAIN  loss dict:  {'classification_loss': 0.938939073085785}
2025-01-14 22:15:27,285 [INFO] Step[450/2713]: training loss : 0.956195752620697 TRAIN  loss dict:  {'classification_loss': 0.956195752620697}
2025-01-14 22:15:41,171 [INFO] Step[500/2713]: training loss : 0.9570606338977814 TRAIN  loss dict:  {'classification_loss': 0.9570606338977814}
2025-01-14 22:15:54,491 [INFO] Step[550/2713]: training loss : 0.9412443041801453 TRAIN  loss dict:  {'classification_loss': 0.9412443041801453}
2025-01-14 22:16:08,066 [INFO] Step[600/2713]: training loss : 0.9397307801246643 TRAIN  loss dict:  {'classification_loss': 0.9397307801246643}
2025-01-14 22:16:22,416 [INFO] Step[650/2713]: training loss : 0.942875429391861 TRAIN  loss dict:  {'classification_loss': 0.942875429391861}
2025-01-14 22:16:36,272 [INFO] Step[700/2713]: training loss : 0.9376177418231965 TRAIN  loss dict:  {'classification_loss': 0.9376177418231965}
2025-01-14 22:16:49,973 [INFO] Step[750/2713]: training loss : 0.9361966407299042 TRAIN  loss dict:  {'classification_loss': 0.9361966407299042}
2025-01-14 22:17:03,658 [INFO] Step[800/2713]: training loss : 0.9398765110969544 TRAIN  loss dict:  {'classification_loss': 0.9398765110969544}
2025-01-14 22:17:17,932 [INFO] Step[850/2713]: training loss : 0.9465844404697418 TRAIN  loss dict:  {'classification_loss': 0.9465844404697418}
2025-01-14 22:17:31,519 [INFO] Step[900/2713]: training loss : 0.9358114802837372 TRAIN  loss dict:  {'classification_loss': 0.9358114802837372}
2025-01-14 22:17:45,156 [INFO] Step[950/2713]: training loss : 0.9866509008407592 TRAIN  loss dict:  {'classification_loss': 0.9866509008407592}
2025-01-14 22:17:58,434 [INFO] Step[1000/2713]: training loss : 0.940924916267395 TRAIN  loss dict:  {'classification_loss': 0.940924916267395}
2025-01-14 22:18:11,737 [INFO] Step[1050/2713]: training loss : 0.9404837477207184 TRAIN  loss dict:  {'classification_loss': 0.9404837477207184}
2025-01-14 22:18:25,496 [INFO] Step[1100/2713]: training loss : 0.9365411865711212 TRAIN  loss dict:  {'classification_loss': 0.9365411865711212}
2025-01-14 22:18:39,357 [INFO] Step[1150/2713]: training loss : 0.9367090845108033 TRAIN  loss dict:  {'classification_loss': 0.9367090845108033}
2025-01-14 22:18:53,016 [INFO] Step[1200/2713]: training loss : 0.9358836400508881 TRAIN  loss dict:  {'classification_loss': 0.9358836400508881}
2025-01-14 22:19:07,074 [INFO] Step[1250/2713]: training loss : 0.9413211131095887 TRAIN  loss dict:  {'classification_loss': 0.9413211131095887}
2025-01-14 22:19:20,698 [INFO] Step[1300/2713]: training loss : 0.9419007134437561 TRAIN  loss dict:  {'classification_loss': 0.9419007134437561}
2025-01-14 22:19:34,524 [INFO] Step[1350/2713]: training loss : 0.9392630851268768 TRAIN  loss dict:  {'classification_loss': 0.9392630851268768}
2025-01-14 22:19:48,278 [INFO] Step[1400/2713]: training loss : 0.9651032400131225 TRAIN  loss dict:  {'classification_loss': 0.9651032400131225}
2025-01-14 22:20:02,429 [INFO] Step[1450/2713]: training loss : 0.9396589624881745 TRAIN  loss dict:  {'classification_loss': 0.9396589624881745}
2025-01-14 22:20:16,368 [INFO] Step[1500/2713]: training loss : 0.9397999596595764 TRAIN  loss dict:  {'classification_loss': 0.9397999596595764}
2025-01-14 22:20:30,462 [INFO] Step[1550/2713]: training loss : 0.9380113339424133 TRAIN  loss dict:  {'classification_loss': 0.9380113339424133}
2025-01-14 22:20:43,746 [INFO] Step[1600/2713]: training loss : 0.9349016237258911 TRAIN  loss dict:  {'classification_loss': 0.9349016237258911}
2025-01-14 22:20:57,749 [INFO] Step[1650/2713]: training loss : 0.9373269546031952 TRAIN  loss dict:  {'classification_loss': 0.9373269546031952}
2025-01-14 22:21:11,279 [INFO] Step[1700/2713]: training loss : 0.9403845465183258 TRAIN  loss dict:  {'classification_loss': 0.9403845465183258}
2025-01-14 22:21:25,529 [INFO] Step[1750/2713]: training loss : 0.9353549075126648 TRAIN  loss dict:  {'classification_loss': 0.9353549075126648}
2025-01-14 22:21:42,512 [INFO] Step[1800/2713]: training loss : 0.9660169970989227 TRAIN  loss dict:  {'classification_loss': 0.9660169970989227}
2025-01-14 22:21:56,377 [INFO] Step[1850/2713]: training loss : 0.976043711900711 TRAIN  loss dict:  {'classification_loss': 0.976043711900711}
2025-01-14 22:22:10,051 [INFO] Step[1900/2713]: training loss : 0.9737863516807557 TRAIN  loss dict:  {'classification_loss': 0.9737863516807557}
2025-01-14 22:22:23,352 [INFO] Step[1950/2713]: training loss : 0.9376636755466461 TRAIN  loss dict:  {'classification_loss': 0.9376636755466461}
2025-01-14 22:22:37,069 [INFO] Step[2000/2713]: training loss : 0.9413809430599213 TRAIN  loss dict:  {'classification_loss': 0.9413809430599213}
2025-01-14 22:22:50,556 [INFO] Step[2050/2713]: training loss : 0.9363873863220215 TRAIN  loss dict:  {'classification_loss': 0.9363873863220215}
2025-01-14 22:23:04,255 [INFO] Step[2100/2713]: training loss : 0.9438624465465546 TRAIN  loss dict:  {'classification_loss': 0.9438624465465546}
2025-01-14 22:23:18,142 [INFO] Step[2150/2713]: training loss : 0.9444673717021942 TRAIN  loss dict:  {'classification_loss': 0.9444673717021942}
2025-01-14 22:23:32,411 [INFO] Step[2200/2713]: training loss : 0.958270001411438 TRAIN  loss dict:  {'classification_loss': 0.958270001411438}
2025-01-14 22:23:46,147 [INFO] Step[2250/2713]: training loss : 0.9392654299736023 TRAIN  loss dict:  {'classification_loss': 0.9392654299736023}
2025-01-14 22:23:59,833 [INFO] Step[2300/2713]: training loss : 0.9418087482452393 TRAIN  loss dict:  {'classification_loss': 0.9418087482452393}
2025-01-14 22:24:13,907 [INFO] Step[2350/2713]: training loss : 0.9412041509151459 TRAIN  loss dict:  {'classification_loss': 0.9412041509151459}
2025-01-14 22:24:27,780 [INFO] Step[2400/2713]: training loss : 0.9400356984138489 TRAIN  loss dict:  {'classification_loss': 0.9400356984138489}
2025-01-14 22:24:41,366 [INFO] Step[2450/2713]: training loss : 0.9392614495754242 TRAIN  loss dict:  {'classification_loss': 0.9392614495754242}
2025-01-14 22:24:55,216 [INFO] Step[2500/2713]: training loss : 0.9397596120834351 TRAIN  loss dict:  {'classification_loss': 0.9397596120834351}
2025-01-14 22:25:09,016 [INFO] Step[2550/2713]: training loss : 0.9411359250545501 TRAIN  loss dict:  {'classification_loss': 0.9411359250545501}
2025-01-14 22:25:23,018 [INFO] Step[2600/2713]: training loss : 0.9380325961112976 TRAIN  loss dict:  {'classification_loss': 0.9380325961112976}
2025-01-14 22:25:36,401 [INFO] Step[2650/2713]: training loss : 0.945660148859024 TRAIN  loss dict:  {'classification_loss': 0.945660148859024}
2025-01-14 22:25:49,930 [INFO] Step[2700/2713]: training loss : 0.9367453002929688 TRAIN  loss dict:  {'classification_loss': 0.9367453002929688}
2025-01-14 22:27:06,949 [INFO] Label accuracies statistics:
2025-01-14 22:27:06,949 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.0, 114: 1.0, 115: 0.75, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.5, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.5, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 0.75, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.5, 290: 1.0, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 22:27:06,951 [INFO] [48] TRAIN  loss: 0.9450562966296122 acc: 0.9981570217471434
2025-01-14 22:27:06,951 [INFO] [48] TRAIN  loss dict: {'classification_loss': 0.9450562966296122}
2025-01-14 22:27:06,952 [INFO] [48] VALIDATION loss: 1.7848051053688938 VALIDATION acc: 0.8012539184952978
2025-01-14 22:27:06,952 [INFO] [48] VALIDATION loss dict: {'classification_loss': 1.7848051053688938}
2025-01-14 22:27:06,952 [INFO] 
2025-01-14 22:27:25,639 [INFO] Step[50/2713]: training loss : 0.9365679430961609 TRAIN  loss dict:  {'classification_loss': 0.9365679430961609}
2025-01-14 22:27:39,067 [INFO] Step[100/2713]: training loss : 0.937176262140274 TRAIN  loss dict:  {'classification_loss': 0.937176262140274}
2025-01-14 22:27:52,495 [INFO] Step[150/2713]: training loss : 0.9372943663597106 TRAIN  loss dict:  {'classification_loss': 0.9372943663597106}
2025-01-14 22:28:06,012 [INFO] Step[200/2713]: training loss : 0.9469907224178314 TRAIN  loss dict:  {'classification_loss': 0.9469907224178314}
2025-01-14 22:28:20,186 [INFO] Step[250/2713]: training loss : 0.9409508550167084 TRAIN  loss dict:  {'classification_loss': 0.9409508550167084}
2025-01-14 22:28:33,880 [INFO] Step[300/2713]: training loss : 0.9383998703956604 TRAIN  loss dict:  {'classification_loss': 0.9383998703956604}
2025-01-14 22:28:47,441 [INFO] Step[350/2713]: training loss : 0.9447048759460449 TRAIN  loss dict:  {'classification_loss': 0.9447048759460449}
2025-01-14 22:29:00,781 [INFO] Step[400/2713]: training loss : 0.9353650856018066 TRAIN  loss dict:  {'classification_loss': 0.9353650856018066}
2025-01-14 22:29:14,041 [INFO] Step[450/2713]: training loss : 0.9353870415687561 TRAIN  loss dict:  {'classification_loss': 0.9353870415687561}
2025-01-14 22:29:27,192 [INFO] Step[500/2713]: training loss : 0.9340063774585724 TRAIN  loss dict:  {'classification_loss': 0.9340063774585724}
2025-01-14 22:29:40,629 [INFO] Step[550/2713]: training loss : 0.9375923788547516 TRAIN  loss dict:  {'classification_loss': 0.9375923788547516}
2025-01-14 22:29:54,202 [INFO] Step[600/2713]: training loss : 0.9421311843395234 TRAIN  loss dict:  {'classification_loss': 0.9421311843395234}
2025-01-14 22:30:08,124 [INFO] Step[650/2713]: training loss : 0.9760434889793396 TRAIN  loss dict:  {'classification_loss': 0.9760434889793396}
2025-01-14 22:30:21,783 [INFO] Step[700/2713]: training loss : 0.954925285577774 TRAIN  loss dict:  {'classification_loss': 0.954925285577774}
2025-01-14 22:30:35,213 [INFO] Step[750/2713]: training loss : 0.9374692642688751 TRAIN  loss dict:  {'classification_loss': 0.9374692642688751}
2025-01-14 22:30:48,967 [INFO] Step[800/2713]: training loss : 0.9349109613895417 TRAIN  loss dict:  {'classification_loss': 0.9349109613895417}
2025-01-14 22:31:02,190 [INFO] Step[850/2713]: training loss : 0.9348109543323517 TRAIN  loss dict:  {'classification_loss': 0.9348109543323517}
2025-01-14 22:31:16,057 [INFO] Step[900/2713]: training loss : 0.9372902870178222 TRAIN  loss dict:  {'classification_loss': 0.9372902870178222}
2025-01-14 22:31:30,154 [INFO] Step[950/2713]: training loss : 0.9387149930000305 TRAIN  loss dict:  {'classification_loss': 0.9387149930000305}
2025-01-14 22:31:44,073 [INFO] Step[1000/2713]: training loss : 0.9351500284671783 TRAIN  loss dict:  {'classification_loss': 0.9351500284671783}
2025-01-14 22:31:57,680 [INFO] Step[1050/2713]: training loss : 0.9352211153507233 TRAIN  loss dict:  {'classification_loss': 0.9352211153507233}
2025-01-14 22:32:10,852 [INFO] Step[1100/2713]: training loss : 0.9371555507183075 TRAIN  loss dict:  {'classification_loss': 0.9371555507183075}
2025-01-14 22:32:25,016 [INFO] Step[1150/2713]: training loss : 0.9365087628364563 TRAIN  loss dict:  {'classification_loss': 0.9365087628364563}
2025-01-14 22:32:39,010 [INFO] Step[1200/2713]: training loss : 0.9392296779155731 TRAIN  loss dict:  {'classification_loss': 0.9392296779155731}
2025-01-14 22:32:53,079 [INFO] Step[1250/2713]: training loss : 0.9377632570266724 TRAIN  loss dict:  {'classification_loss': 0.9377632570266724}
2025-01-14 22:33:06,734 [INFO] Step[1300/2713]: training loss : 0.9389861512184143 TRAIN  loss dict:  {'classification_loss': 0.9389861512184143}
2025-01-14 22:33:20,256 [INFO] Step[1350/2713]: training loss : 0.9379340755939484 TRAIN  loss dict:  {'classification_loss': 0.9379340755939484}
2025-01-14 22:33:33,752 [INFO] Step[1400/2713]: training loss : 0.9384912478923798 TRAIN  loss dict:  {'classification_loss': 0.9384912478923798}
2025-01-14 22:33:47,344 [INFO] Step[1450/2713]: training loss : 0.9430867290496826 TRAIN  loss dict:  {'classification_loss': 0.9430867290496826}
2025-01-14 22:34:00,891 [INFO] Step[1500/2713]: training loss : 0.9350585532188416 TRAIN  loss dict:  {'classification_loss': 0.9350585532188416}
2025-01-14 22:34:14,869 [INFO] Step[1550/2713]: training loss : 0.9379865050315856 TRAIN  loss dict:  {'classification_loss': 0.9379865050315856}
2025-01-14 22:34:28,839 [INFO] Step[1600/2713]: training loss : 0.9414905726909637 TRAIN  loss dict:  {'classification_loss': 0.9414905726909637}
2025-01-14 22:34:42,194 [INFO] Step[1650/2713]: training loss : 0.9354386627674103 TRAIN  loss dict:  {'classification_loss': 0.9354386627674103}
2025-01-14 22:34:56,049 [INFO] Step[1700/2713]: training loss : 0.9396855735778809 TRAIN  loss dict:  {'classification_loss': 0.9396855735778809}
2025-01-14 22:35:10,258 [INFO] Step[1750/2713]: training loss : 0.9363387656211853 TRAIN  loss dict:  {'classification_loss': 0.9363387656211853}
2025-01-14 22:35:24,502 [INFO] Step[1800/2713]: training loss : 0.9385463643074036 TRAIN  loss dict:  {'classification_loss': 0.9385463643074036}
2025-01-14 22:35:38,001 [INFO] Step[1850/2713]: training loss : 0.9357779479026794 TRAIN  loss dict:  {'classification_loss': 0.9357779479026794}
2025-01-14 22:35:51,196 [INFO] Step[1900/2713]: training loss : 0.9347632277011871 TRAIN  loss dict:  {'classification_loss': 0.9347632277011871}
2025-01-14 22:36:04,722 [INFO] Step[1950/2713]: training loss : 0.9358661007881165 TRAIN  loss dict:  {'classification_loss': 0.9358661007881165}
2025-01-14 22:36:18,231 [INFO] Step[2000/2713]: training loss : 0.9386002409458161 TRAIN  loss dict:  {'classification_loss': 0.9386002409458161}
2025-01-14 22:36:32,077 [INFO] Step[2050/2713]: training loss : 0.937410227060318 TRAIN  loss dict:  {'classification_loss': 0.937410227060318}
2025-01-14 22:36:45,527 [INFO] Step[2100/2713]: training loss : 0.9724871385097503 TRAIN  loss dict:  {'classification_loss': 0.9724871385097503}
2025-01-14 22:36:58,905 [INFO] Step[2150/2713]: training loss : 0.9414035785198211 TRAIN  loss dict:  {'classification_loss': 0.9414035785198211}
2025-01-14 22:37:12,685 [INFO] Step[2200/2713]: training loss : 0.9427506685256958 TRAIN  loss dict:  {'classification_loss': 0.9427506685256958}
2025-01-14 22:37:26,913 [INFO] Step[2250/2713]: training loss : 0.9372292387485505 TRAIN  loss dict:  {'classification_loss': 0.9372292387485505}
2025-01-14 22:37:40,814 [INFO] Step[2300/2713]: training loss : 0.9415393114089966 TRAIN  loss dict:  {'classification_loss': 0.9415393114089966}
2025-01-14 22:37:54,180 [INFO] Step[2350/2713]: training loss : 0.9369235157966613 TRAIN  loss dict:  {'classification_loss': 0.9369235157966613}
2025-01-14 22:38:07,676 [INFO] Step[2400/2713]: training loss : 0.9357797300815582 TRAIN  loss dict:  {'classification_loss': 0.9357797300815582}
2025-01-14 22:38:21,221 [INFO] Step[2450/2713]: training loss : 0.9665185809135437 TRAIN  loss dict:  {'classification_loss': 0.9665185809135437}
2025-01-14 22:38:34,687 [INFO] Step[2500/2713]: training loss : 0.9426948237419128 TRAIN  loss dict:  {'classification_loss': 0.9426948237419128}
2025-01-14 22:38:48,476 [INFO] Step[2550/2713]: training loss : 0.9352849721908569 TRAIN  loss dict:  {'classification_loss': 0.9352849721908569}
2025-01-14 22:39:01,952 [INFO] Step[2600/2713]: training loss : 0.9413104152679443 TRAIN  loss dict:  {'classification_loss': 0.9413104152679443}
2025-01-14 22:39:15,662 [INFO] Step[2650/2713]: training loss : 0.9372698593139649 TRAIN  loss dict:  {'classification_loss': 0.9372698593139649}
2025-01-14 22:39:28,911 [INFO] Step[2700/2713]: training loss : 0.9346053111553192 TRAIN  loss dict:  {'classification_loss': 0.9346053111553192}
2025-01-14 22:40:45,790 [INFO] Label accuracies statistics:
2025-01-14 22:40:45,790 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.75, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 1.0, 279: 1.0, 280: 0.75, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 22:40:45,792 [INFO] [49] TRAIN  loss: 0.940214424193938 acc: 0.9991399434820002
2025-01-14 22:40:45,792 [INFO] [49] TRAIN  loss dict: {'classification_loss': 0.940214424193938}
2025-01-14 22:40:45,792 [INFO] [49] VALIDATION loss: 1.7853777429887228 VALIDATION acc: 0.8068965517241379
2025-01-14 22:40:45,792 [INFO] [49] VALIDATION loss dict: {'classification_loss': 1.7853777429887228}
2025-01-14 22:40:45,792 [INFO] 
2025-01-14 22:41:06,062 [INFO] Step[50/2713]: training loss : 0.9348963153362274 TRAIN  loss dict:  {'classification_loss': 0.9348963153362274}
2025-01-14 22:41:19,613 [INFO] Step[100/2713]: training loss : 0.9360000550746918 TRAIN  loss dict:  {'classification_loss': 0.9360000550746918}
2025-01-14 22:41:33,610 [INFO] Step[150/2713]: training loss : 0.9355126917362213 TRAIN  loss dict:  {'classification_loss': 0.9355126917362213}
2025-01-14 22:41:47,054 [INFO] Step[200/2713]: training loss : 0.9451454877853394 TRAIN  loss dict:  {'classification_loss': 0.9451454877853394}
2025-01-14 22:42:00,250 [INFO] Step[250/2713]: training loss : 0.9403107976913452 TRAIN  loss dict:  {'classification_loss': 0.9403107976913452}
2025-01-14 22:42:13,836 [INFO] Step[300/2713]: training loss : 0.9373160374164581 TRAIN  loss dict:  {'classification_loss': 0.9373160374164581}
2025-01-14 22:42:27,226 [INFO] Step[350/2713]: training loss : 0.938633154630661 TRAIN  loss dict:  {'classification_loss': 0.938633154630661}
2025-01-14 22:42:40,436 [INFO] Step[400/2713]: training loss : 0.9354836201667786 TRAIN  loss dict:  {'classification_loss': 0.9354836201667786}
2025-01-14 22:42:54,386 [INFO] Step[450/2713]: training loss : 0.9398370969295502 TRAIN  loss dict:  {'classification_loss': 0.9398370969295502}
2025-01-14 22:43:08,215 [INFO] Step[500/2713]: training loss : 0.9753335213661194 TRAIN  loss dict:  {'classification_loss': 0.9753335213661194}
2025-01-14 22:43:22,335 [INFO] Step[550/2713]: training loss : 0.9456364345550538 TRAIN  loss dict:  {'classification_loss': 0.9456364345550538}
2025-01-14 22:43:36,156 [INFO] Step[600/2713]: training loss : 0.9373304581642151 TRAIN  loss dict:  {'classification_loss': 0.9373304581642151}
2025-01-14 22:43:49,707 [INFO] Step[650/2713]: training loss : 0.9374455046653748 TRAIN  loss dict:  {'classification_loss': 0.9374455046653748}
2025-01-14 22:44:03,323 [INFO] Step[700/2713]: training loss : 0.940866516828537 TRAIN  loss dict:  {'classification_loss': 0.940866516828537}
2025-01-14 22:44:17,492 [INFO] Step[750/2713]: training loss : 0.9361708402633667 TRAIN  loss dict:  {'classification_loss': 0.9361708402633667}
2025-01-14 22:44:30,721 [INFO] Step[800/2713]: training loss : 0.9371773171424865 TRAIN  loss dict:  {'classification_loss': 0.9371773171424865}
2025-01-14 22:44:44,282 [INFO] Step[850/2713]: training loss : 0.936112813949585 TRAIN  loss dict:  {'classification_loss': 0.936112813949585}
2025-01-14 22:44:58,168 [INFO] Step[900/2713]: training loss : 0.9365271174907684 TRAIN  loss dict:  {'classification_loss': 0.9365271174907684}
2025-01-14 22:45:11,737 [INFO] Step[950/2713]: training loss : 0.9527142262458801 TRAIN  loss dict:  {'classification_loss': 0.9527142262458801}
2025-01-14 22:45:25,761 [INFO] Step[1000/2713]: training loss : 0.9348310041427612 TRAIN  loss dict:  {'classification_loss': 0.9348310041427612}
2025-01-14 22:45:42,150 [INFO] Step[1050/2713]: training loss : 0.9378704977035522 TRAIN  loss dict:  {'classification_loss': 0.9378704977035522}
2025-01-14 22:45:55,690 [INFO] Step[1100/2713]: training loss : 0.9370974099636078 TRAIN  loss dict:  {'classification_loss': 0.9370974099636078}
2025-01-14 22:46:09,383 [INFO] Step[1150/2713]: training loss : 0.9354303097724914 TRAIN  loss dict:  {'classification_loss': 0.9354303097724914}
2025-01-14 22:46:24,441 [INFO] Step[1200/2713]: training loss : 0.9353512001037597 TRAIN  loss dict:  {'classification_loss': 0.9353512001037597}
2025-01-14 22:46:39,939 [INFO] Step[1250/2713]: training loss : 0.936154500246048 TRAIN  loss dict:  {'classification_loss': 0.936154500246048}
2025-01-14 22:46:53,800 [INFO] Step[1300/2713]: training loss : 0.9361231744289398 TRAIN  loss dict:  {'classification_loss': 0.9361231744289398}
2025-01-14 22:47:07,696 [INFO] Step[1350/2713]: training loss : 0.9382829809188843 TRAIN  loss dict:  {'classification_loss': 0.9382829809188843}
2025-01-14 22:47:21,368 [INFO] Step[1400/2713]: training loss : 0.9525256705284119 TRAIN  loss dict:  {'classification_loss': 0.9525256705284119}
2025-01-14 22:47:35,093 [INFO] Step[1450/2713]: training loss : 0.9354216551780701 TRAIN  loss dict:  {'classification_loss': 0.9354216551780701}
2025-01-14 22:47:48,272 [INFO] Step[1500/2713]: training loss : 0.9556813204288482 TRAIN  loss dict:  {'classification_loss': 0.9556813204288482}
2025-01-14 22:48:01,808 [INFO] Step[1550/2713]: training loss : 0.9368756091594697 TRAIN  loss dict:  {'classification_loss': 0.9368756091594697}
2025-01-14 22:48:15,702 [INFO] Step[1600/2713]: training loss : 0.9375112998485565 TRAIN  loss dict:  {'classification_loss': 0.9375112998485565}
2025-01-14 22:48:29,857 [INFO] Step[1650/2713]: training loss : 0.9384029245376587 TRAIN  loss dict:  {'classification_loss': 0.9384029245376587}
2025-01-14 22:48:43,118 [INFO] Step[1700/2713]: training loss : 0.9622623562812805 TRAIN  loss dict:  {'classification_loss': 0.9622623562812805}
2025-01-14 22:48:57,166 [INFO] Step[1750/2713]: training loss : 0.9356554591655731 TRAIN  loss dict:  {'classification_loss': 0.9356554591655731}
2025-01-14 22:49:10,808 [INFO] Step[1800/2713]: training loss : 0.9548398840427399 TRAIN  loss dict:  {'classification_loss': 0.9548398840427399}
2025-01-14 22:49:24,716 [INFO] Step[1850/2713]: training loss : 0.9348037314414978 TRAIN  loss dict:  {'classification_loss': 0.9348037314414978}
2025-01-14 22:49:37,989 [INFO] Step[1900/2713]: training loss : 0.9369056260585785 TRAIN  loss dict:  {'classification_loss': 0.9369056260585785}
2025-01-14 22:49:51,868 [INFO] Step[1950/2713]: training loss : 0.9371709406375885 TRAIN  loss dict:  {'classification_loss': 0.9371709406375885}
2025-01-14 22:50:05,569 [INFO] Step[2000/2713]: training loss : 0.935521913766861 TRAIN  loss dict:  {'classification_loss': 0.935521913766861}
2025-01-14 22:50:19,230 [INFO] Step[2050/2713]: training loss : 0.9376053702831268 TRAIN  loss dict:  {'classification_loss': 0.9376053702831268}
2025-01-14 22:50:33,016 [INFO] Step[2100/2713]: training loss : 0.9365456461906433 TRAIN  loss dict:  {'classification_loss': 0.9365456461906433}
2025-01-14 22:50:47,149 [INFO] Step[2150/2713]: training loss : 0.9367983281612396 TRAIN  loss dict:  {'classification_loss': 0.9367983281612396}
2025-01-14 22:51:00,719 [INFO] Step[2200/2713]: training loss : 0.9404324281215668 TRAIN  loss dict:  {'classification_loss': 0.9404324281215668}
2025-01-14 22:51:14,585 [INFO] Step[2250/2713]: training loss : 0.9451209259033203 TRAIN  loss dict:  {'classification_loss': 0.9451209259033203}
2025-01-14 22:51:28,521 [INFO] Step[2300/2713]: training loss : 0.9399660742282867 TRAIN  loss dict:  {'classification_loss': 0.9399660742282867}
2025-01-14 22:51:42,358 [INFO] Step[2350/2713]: training loss : 0.9387569892406463 TRAIN  loss dict:  {'classification_loss': 0.9387569892406463}
2025-01-14 22:51:56,147 [INFO] Step[2400/2713]: training loss : 0.936306129693985 TRAIN  loss dict:  {'classification_loss': 0.936306129693985}
2025-01-14 22:52:09,841 [INFO] Step[2450/2713]: training loss : 0.9388266611099243 TRAIN  loss dict:  {'classification_loss': 0.9388266611099243}
2025-01-14 22:52:23,304 [INFO] Step[2500/2713]: training loss : 0.9561799108982086 TRAIN  loss dict:  {'classification_loss': 0.9561799108982086}
2025-01-14 22:52:36,959 [INFO] Step[2550/2713]: training loss : 0.9363325333595276 TRAIN  loss dict:  {'classification_loss': 0.9363325333595276}
2025-01-14 22:52:50,141 [INFO] Step[2600/2713]: training loss : 0.9355414950847626 TRAIN  loss dict:  {'classification_loss': 0.9355414950847626}
2025-01-14 22:53:03,370 [INFO] Step[2650/2713]: training loss : 0.9476630771160126 TRAIN  loss dict:  {'classification_loss': 0.9476630771160126}
2025-01-14 22:53:16,550 [INFO] Step[2700/2713]: training loss : 0.9363645398616791 TRAIN  loss dict:  {'classification_loss': 0.9363645398616791}
2025-01-14 22:54:33,577 [INFO] Label accuracies statistics:
2025-01-14 22:54:33,577 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.25, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.5, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.75, 115: 0.5, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.5, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.5, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.5, 203: 0.25, 204: 0.75, 205: 1.0, 206: 1.0, 207: 1.0, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 22:54:33,580 [INFO] [50] TRAIN  loss: 0.9404635328539965 acc: 0.9984027521808576
2025-01-14 22:54:33,580 [INFO] [50] TRAIN  loss dict: {'classification_loss': 0.9404635328539965}
2025-01-14 22:54:33,580 [INFO] [50] VALIDATION loss: 1.830927149469691 VALIDATION acc: 0.7949843260188088
2025-01-14 22:54:33,580 [INFO] [50] VALIDATION loss dict: {'classification_loss': 1.830927149469691}
2025-01-14 22:54:33,580 [INFO] 
2025-01-14 22:54:52,327 [INFO] Step[50/2713]: training loss : 0.9351827502250671 TRAIN  loss dict:  {'classification_loss': 0.9351827502250671}
2025-01-14 22:55:05,803 [INFO] Step[100/2713]: training loss : 0.9358196902275085 TRAIN  loss dict:  {'classification_loss': 0.9358196902275085}
2025-01-14 22:55:20,418 [INFO] Step[150/2713]: training loss : 0.9336638581752777 TRAIN  loss dict:  {'classification_loss': 0.9336638581752777}
2025-01-14 22:55:35,715 [INFO] Step[200/2713]: training loss : 0.936144073009491 TRAIN  loss dict:  {'classification_loss': 0.936144073009491}
2025-01-14 22:55:49,752 [INFO] Step[250/2713]: training loss : 0.9346862626075745 TRAIN  loss dict:  {'classification_loss': 0.9346862626075745}
2025-01-14 22:56:03,394 [INFO] Step[300/2713]: training loss : 0.9525609171390533 TRAIN  loss dict:  {'classification_loss': 0.9525609171390533}
2025-01-14 22:56:17,203 [INFO] Step[350/2713]: training loss : 0.9354590177536011 TRAIN  loss dict:  {'classification_loss': 0.9354590177536011}
2025-01-14 22:56:31,198 [INFO] Step[400/2713]: training loss : 0.9347797858715058 TRAIN  loss dict:  {'classification_loss': 0.9347797858715058}
2025-01-14 22:56:46,649 [INFO] Step[450/2713]: training loss : 0.9550799584388733 TRAIN  loss dict:  {'classification_loss': 0.9550799584388733}
2025-01-14 22:57:00,911 [INFO] Step[500/2713]: training loss : 0.9341146409511566 TRAIN  loss dict:  {'classification_loss': 0.9341146409511566}
2025-01-14 22:57:14,117 [INFO] Step[550/2713]: training loss : 0.9375934922695159 TRAIN  loss dict:  {'classification_loss': 0.9375934922695159}
2025-01-14 22:57:27,479 [INFO] Step[600/2713]: training loss : 0.9337930500507354 TRAIN  loss dict:  {'classification_loss': 0.9337930500507354}
2025-01-14 22:57:40,797 [INFO] Step[650/2713]: training loss : 0.9330606925487518 TRAIN  loss dict:  {'classification_loss': 0.9330606925487518}
2025-01-14 22:57:54,441 [INFO] Step[700/2713]: training loss : 0.9670485949516296 TRAIN  loss dict:  {'classification_loss': 0.9670485949516296}
2025-01-14 22:58:07,783 [INFO] Step[750/2713]: training loss : 0.9361705625057221 TRAIN  loss dict:  {'classification_loss': 0.9361705625057221}
2025-01-14 22:58:21,563 [INFO] Step[800/2713]: training loss : 0.936122168302536 TRAIN  loss dict:  {'classification_loss': 0.936122168302536}
2025-01-14 22:58:35,196 [INFO] Step[850/2713]: training loss : 0.9358065032958984 TRAIN  loss dict:  {'classification_loss': 0.9358065032958984}
2025-01-14 22:58:49,370 [INFO] Step[900/2713]: training loss : 0.9346910727024078 TRAIN  loss dict:  {'classification_loss': 0.9346910727024078}
2025-01-14 22:59:03,237 [INFO] Step[950/2713]: training loss : 0.9542776608467102 TRAIN  loss dict:  {'classification_loss': 0.9542776608467102}
2025-01-14 22:59:16,953 [INFO] Step[1000/2713]: training loss : 0.933635516166687 TRAIN  loss dict:  {'classification_loss': 0.933635516166687}
2025-01-14 22:59:30,755 [INFO] Step[1050/2713]: training loss : 0.9392060220241547 TRAIN  loss dict:  {'classification_loss': 0.9392060220241547}
2025-01-14 22:59:44,271 [INFO] Step[1100/2713]: training loss : 0.9363943016529084 TRAIN  loss dict:  {'classification_loss': 0.9363943016529084}
2025-01-14 22:59:57,477 [INFO] Step[1150/2713]: training loss : 0.9555591607093811 TRAIN  loss dict:  {'classification_loss': 0.9555591607093811}
2025-01-14 23:00:10,966 [INFO] Step[1200/2713]: training loss : 0.9367611122131347 TRAIN  loss dict:  {'classification_loss': 0.9367611122131347}
2025-01-14 23:00:25,187 [INFO] Step[1250/2713]: training loss : 0.9336801147460938 TRAIN  loss dict:  {'classification_loss': 0.9336801147460938}
2025-01-14 23:00:38,914 [INFO] Step[1300/2713]: training loss : 0.9345434880256653 TRAIN  loss dict:  {'classification_loss': 0.9345434880256653}
2025-01-14 23:00:52,644 [INFO] Step[1350/2713]: training loss : 0.9350635182857513 TRAIN  loss dict:  {'classification_loss': 0.9350635182857513}
2025-01-14 23:01:06,379 [INFO] Step[1400/2713]: training loss : 0.9353415799140931 TRAIN  loss dict:  {'classification_loss': 0.9353415799140931}
2025-01-14 23:01:19,584 [INFO] Step[1450/2713]: training loss : 0.9393286800384522 TRAIN  loss dict:  {'classification_loss': 0.9393286800384522}
2025-01-14 23:01:32,992 [INFO] Step[1500/2713]: training loss : 0.940049238204956 TRAIN  loss dict:  {'classification_loss': 0.940049238204956}
2025-01-14 23:01:46,838 [INFO] Step[1550/2713]: training loss : 0.9343152248859405 TRAIN  loss dict:  {'classification_loss': 0.9343152248859405}
2025-01-14 23:02:00,069 [INFO] Step[1600/2713]: training loss : 0.933260190486908 TRAIN  loss dict:  {'classification_loss': 0.933260190486908}
2025-01-14 23:02:13,700 [INFO] Step[1650/2713]: training loss : 0.940421199798584 TRAIN  loss dict:  {'classification_loss': 0.940421199798584}
2025-01-14 23:02:27,648 [INFO] Step[1700/2713]: training loss : 0.9350356674194336 TRAIN  loss dict:  {'classification_loss': 0.9350356674194336}
2025-01-14 23:02:41,318 [INFO] Step[1750/2713]: training loss : 0.949846956729889 TRAIN  loss dict:  {'classification_loss': 0.949846956729889}
2025-01-14 23:02:55,204 [INFO] Step[1800/2713]: training loss : 0.9354366755485535 TRAIN  loss dict:  {'classification_loss': 0.9354366755485535}
2025-01-14 23:03:08,833 [INFO] Step[1850/2713]: training loss : 0.9346022832393647 TRAIN  loss dict:  {'classification_loss': 0.9346022832393647}
2025-01-14 23:03:22,026 [INFO] Step[1900/2713]: training loss : 0.9341719746589661 TRAIN  loss dict:  {'classification_loss': 0.9341719746589661}
2025-01-14 23:03:35,727 [INFO] Step[1950/2713]: training loss : 0.9366261219978332 TRAIN  loss dict:  {'classification_loss': 0.9366261219978332}
2025-01-14 23:03:49,345 [INFO] Step[2000/2713]: training loss : 0.9346052873134613 TRAIN  loss dict:  {'classification_loss': 0.9346052873134613}
2025-01-14 23:04:02,809 [INFO] Step[2050/2713]: training loss : 0.9340697598457336 TRAIN  loss dict:  {'classification_loss': 0.9340697598457336}
2025-01-14 23:04:16,186 [INFO] Step[2100/2713]: training loss : 0.9338555884361267 TRAIN  loss dict:  {'classification_loss': 0.9338555884361267}
2025-01-14 23:04:30,038 [INFO] Step[2150/2713]: training loss : 0.9336184597015381 TRAIN  loss dict:  {'classification_loss': 0.9336184597015381}
2025-01-14 23:04:44,067 [INFO] Step[2200/2713]: training loss : 0.9349876356124878 TRAIN  loss dict:  {'classification_loss': 0.9349876356124878}
2025-01-14 23:04:57,317 [INFO] Step[2250/2713]: training loss : 0.9772032248973846 TRAIN  loss dict:  {'classification_loss': 0.9772032248973846}
2025-01-14 23:05:10,551 [INFO] Step[2300/2713]: training loss : 0.9350852203369141 TRAIN  loss dict:  {'classification_loss': 0.9350852203369141}
2025-01-14 23:05:24,165 [INFO] Step[2350/2713]: training loss : 0.9346868455410003 TRAIN  loss dict:  {'classification_loss': 0.9346868455410003}
2025-01-14 23:05:38,164 [INFO] Step[2400/2713]: training loss : 0.9356602895259857 TRAIN  loss dict:  {'classification_loss': 0.9356602895259857}
2025-01-14 23:05:51,793 [INFO] Step[2450/2713]: training loss : 0.9346052920818329 TRAIN  loss dict:  {'classification_loss': 0.9346052920818329}
2025-01-14 23:06:05,837 [INFO] Step[2500/2713]: training loss : 0.9521852302551269 TRAIN  loss dict:  {'classification_loss': 0.9521852302551269}
2025-01-14 23:06:19,819 [INFO] Step[2550/2713]: training loss : 0.9348211455345153 TRAIN  loss dict:  {'classification_loss': 0.9348211455345153}
2025-01-14 23:06:34,056 [INFO] Step[2600/2713]: training loss : 0.9355721580982208 TRAIN  loss dict:  {'classification_loss': 0.9355721580982208}
2025-01-14 23:06:47,606 [INFO] Step[2650/2713]: training loss : 0.9347222220897674 TRAIN  loss dict:  {'classification_loss': 0.9347222220897674}
2025-01-14 23:07:01,651 [INFO] Step[2700/2713]: training loss : 0.9543842577934265 TRAIN  loss dict:  {'classification_loss': 0.9543842577934265}
2025-01-14 23:08:20,135 [INFO] Label accuracies statistics:
2025-01-14 23:08:20,135 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 1.0, 260: 0.25, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 1.0, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 1.0, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 1.0, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-14 23:09:10,721 [INFO] [51] TRAIN  loss: 0.9390478247740239 acc: 0.9985256173977147
2025-01-14 23:09:10,721 [INFO] [51] TRAIN  loss dict: {'classification_loss': 0.9390478247740239}
2025-01-14 23:09:10,721 [INFO] [51] VALIDATION loss: 1.742189913091803 VALIDATION acc: 0.8144200626959248
2025-01-14 23:09:10,721 [INFO] [51] VALIDATION loss dict: {'classification_loss': 1.742189913091803}
2025-01-14 23:09:10,721 [INFO] 
2025-01-14 23:09:32,144 [INFO] Step[50/2713]: training loss : 0.9338275337219238 TRAIN  loss dict:  {'classification_loss': 0.9338275337219238}
2025-01-14 23:09:45,848 [INFO] Step[100/2713]: training loss : 0.9324730050563812 TRAIN  loss dict:  {'classification_loss': 0.9324730050563812}
2025-01-14 23:09:59,458 [INFO] Step[150/2713]: training loss : 0.9376892256736755 TRAIN  loss dict:  {'classification_loss': 0.9376892256736755}
2025-01-14 23:10:13,678 [INFO] Step[200/2713]: training loss : 0.9341053712368012 TRAIN  loss dict:  {'classification_loss': 0.9341053712368012}
2025-01-14 23:10:27,025 [INFO] Step[250/2713]: training loss : 0.940959666967392 TRAIN  loss dict:  {'classification_loss': 0.940959666967392}
2025-01-14 23:10:40,652 [INFO] Step[300/2713]: training loss : 0.9333311796188355 TRAIN  loss dict:  {'classification_loss': 0.9333311796188355}
2025-01-14 23:10:54,684 [INFO] Step[350/2713]: training loss : 0.9329999053478241 TRAIN  loss dict:  {'classification_loss': 0.9329999053478241}
2025-01-14 23:11:08,336 [INFO] Step[400/2713]: training loss : 0.9330606019496918 TRAIN  loss dict:  {'classification_loss': 0.9330606019496918}
2025-01-14 23:11:24,800 [INFO] Step[450/2713]: training loss : 0.9355997800827026 TRAIN  loss dict:  {'classification_loss': 0.9355997800827026}
2025-01-14 23:11:38,817 [INFO] Step[500/2713]: training loss : 0.9341431593894959 TRAIN  loss dict:  {'classification_loss': 0.9341431593894959}
2025-01-14 23:11:52,575 [INFO] Step[550/2713]: training loss : 0.9345908749103546 TRAIN  loss dict:  {'classification_loss': 0.9345908749103546}
2025-01-14 23:12:06,457 [INFO] Step[600/2713]: training loss : 0.9331703710556031 TRAIN  loss dict:  {'classification_loss': 0.9331703710556031}
2025-01-14 23:12:19,921 [INFO] Step[650/2713]: training loss : 0.9330032169818878 TRAIN  loss dict:  {'classification_loss': 0.9330032169818878}
2025-01-14 23:12:33,537 [INFO] Step[700/2713]: training loss : 0.9353530073165893 TRAIN  loss dict:  {'classification_loss': 0.9353530073165893}
2025-01-14 23:12:47,014 [INFO] Step[750/2713]: training loss : 0.9332813858985901 TRAIN  loss dict:  {'classification_loss': 0.9332813858985901}
2025-01-14 23:13:00,729 [INFO] Step[800/2713]: training loss : 0.9352476251125336 TRAIN  loss dict:  {'classification_loss': 0.9352476251125336}
2025-01-14 23:13:14,280 [INFO] Step[850/2713]: training loss : 0.9324127840995788 TRAIN  loss dict:  {'classification_loss': 0.9324127840995788}
2025-01-14 23:13:27,837 [INFO] Step[900/2713]: training loss : 0.9334185826778412 TRAIN  loss dict:  {'classification_loss': 0.9334185826778412}
2025-01-14 23:13:42,044 [INFO] Step[950/2713]: training loss : 0.9483661806583404 TRAIN  loss dict:  {'classification_loss': 0.9483661806583404}
2025-01-14 23:13:55,497 [INFO] Step[1000/2713]: training loss : 0.9333624613285064 TRAIN  loss dict:  {'classification_loss': 0.9333624613285064}
2025-01-14 23:14:09,453 [INFO] Step[1050/2713]: training loss : 0.9327536296844482 TRAIN  loss dict:  {'classification_loss': 0.9327536296844482}
2025-01-14 23:14:23,666 [INFO] Step[1100/2713]: training loss : 0.9336006963253021 TRAIN  loss dict:  {'classification_loss': 0.9336006963253021}
2025-01-14 23:14:37,606 [INFO] Step[1150/2713]: training loss : 0.9338981926441192 TRAIN  loss dict:  {'classification_loss': 0.9338981926441192}
2025-01-14 23:14:51,622 [INFO] Step[1200/2713]: training loss : 0.9325149142742157 TRAIN  loss dict:  {'classification_loss': 0.9325149142742157}
2025-01-14 23:15:04,987 [INFO] Step[1250/2713]: training loss : 0.9344275414943695 TRAIN  loss dict:  {'classification_loss': 0.9344275414943695}
2025-01-14 23:15:18,663 [INFO] Step[1300/2713]: training loss : 0.9361223506927491 TRAIN  loss dict:  {'classification_loss': 0.9361223506927491}
2025-01-14 23:15:32,141 [INFO] Step[1350/2713]: training loss : 0.9359323656558991 TRAIN  loss dict:  {'classification_loss': 0.9359323656558991}
2025-01-14 23:15:45,994 [INFO] Step[1400/2713]: training loss : 0.9324755692481994 TRAIN  loss dict:  {'classification_loss': 0.9324755692481994}
2025-01-14 23:15:59,289 [INFO] Step[1450/2713]: training loss : 0.9354156005382538 TRAIN  loss dict:  {'classification_loss': 0.9354156005382538}
2025-01-14 23:16:12,584 [INFO] Step[1500/2713]: training loss : 0.932406896352768 TRAIN  loss dict:  {'classification_loss': 0.932406896352768}
2025-01-14 23:16:26,232 [INFO] Step[1550/2713]: training loss : 0.9345191514492035 TRAIN  loss dict:  {'classification_loss': 0.9345191514492035}
2025-01-14 23:16:40,100 [INFO] Step[1600/2713]: training loss : 0.9344674360752105 TRAIN  loss dict:  {'classification_loss': 0.9344674360752105}
2025-01-14 23:16:53,942 [INFO] Step[1650/2713]: training loss : 0.9322703611850739 TRAIN  loss dict:  {'classification_loss': 0.9322703611850739}
2025-01-14 23:17:07,506 [INFO] Step[1700/2713]: training loss : 0.9339743852615356 TRAIN  loss dict:  {'classification_loss': 0.9339743852615356}
2025-01-14 23:17:21,139 [INFO] Step[1750/2713]: training loss : 0.93387486577034 TRAIN  loss dict:  {'classification_loss': 0.93387486577034}
2025-01-14 23:17:35,365 [INFO] Step[1800/2713]: training loss : 0.9632276296615601 TRAIN  loss dict:  {'classification_loss': 0.9632276296615601}
2025-01-14 23:17:49,014 [INFO] Step[1850/2713]: training loss : 0.9380716836452484 TRAIN  loss dict:  {'classification_loss': 0.9380716836452484}
2025-01-14 23:18:02,860 [INFO] Step[1900/2713]: training loss : 0.9563281428813935 TRAIN  loss dict:  {'classification_loss': 0.9563281428813935}
2025-01-14 23:18:16,682 [INFO] Step[1950/2713]: training loss : 0.9376068115234375 TRAIN  loss dict:  {'classification_loss': 0.9376068115234375}
2025-01-14 23:18:30,703 [INFO] Step[2000/2713]: training loss : 0.9391771256923676 TRAIN  loss dict:  {'classification_loss': 0.9391771256923676}
2025-01-14 23:18:44,618 [INFO] Step[2050/2713]: training loss : 0.9348648643493652 TRAIN  loss dict:  {'classification_loss': 0.9348648643493652}
2025-01-14 23:18:58,203 [INFO] Step[2100/2713]: training loss : 0.9325280869007111 TRAIN  loss dict:  {'classification_loss': 0.9325280869007111}
2025-01-14 23:19:12,014 [INFO] Step[2150/2713]: training loss : 0.9348435294628143 TRAIN  loss dict:  {'classification_loss': 0.9348435294628143}
2025-01-14 23:19:25,696 [INFO] Step[2200/2713]: training loss : 0.9334148013591766 TRAIN  loss dict:  {'classification_loss': 0.9334148013591766}
2025-01-14 23:19:39,660 [INFO] Step[2250/2713]: training loss : 0.9371673929691314 TRAIN  loss dict:  {'classification_loss': 0.9371673929691314}
2025-01-14 23:19:53,473 [INFO] Step[2300/2713]: training loss : 0.9592148232460022 TRAIN  loss dict:  {'classification_loss': 0.9592148232460022}
2025-01-14 23:20:07,745 [INFO] Step[2350/2713]: training loss : 0.9322489845752716 TRAIN  loss dict:  {'classification_loss': 0.9322489845752716}
2025-01-14 23:20:21,897 [INFO] Step[2400/2713]: training loss : 0.9379569530487061 TRAIN  loss dict:  {'classification_loss': 0.9379569530487061}
2025-01-14 23:20:37,666 [INFO] Step[2450/2713]: training loss : 0.9318540966510773 TRAIN  loss dict:  {'classification_loss': 0.9318540966510773}
2025-01-14 23:20:52,936 [INFO] Step[2500/2713]: training loss : 0.9336515653133393 TRAIN  loss dict:  {'classification_loss': 0.9336515653133393}
2025-01-14 23:21:07,206 [INFO] Step[2550/2713]: training loss : 0.9317583692073822 TRAIN  loss dict:  {'classification_loss': 0.9317583692073822}
2025-01-14 23:21:20,812 [INFO] Step[2600/2713]: training loss : 0.9331539380550384 TRAIN  loss dict:  {'classification_loss': 0.9331539380550384}
2025-01-14 23:21:34,488 [INFO] Step[2650/2713]: training loss : 0.9375219142436981 TRAIN  loss dict:  {'classification_loss': 0.9375219142436981}
2025-01-14 23:21:47,795 [INFO] Step[2700/2713]: training loss : 0.9343041455745698 TRAIN  loss dict:  {'classification_loss': 0.9343041455745698}
2025-01-14 23:23:04,814 [INFO] Label accuracies statistics:
2025-01-14 23:23:04,814 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.75, 23: 0.5, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.25, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.0, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 0.5, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 23:23:04,816 [INFO] [52] TRAIN  loss: 0.9360422061634731 acc: 0.9993856739157144
2025-01-14 23:23:04,816 [INFO] [52] TRAIN  loss dict: {'classification_loss': 0.9360422061634731}
2025-01-14 23:23:04,816 [INFO] [52] VALIDATION loss: 1.8107459678461677 VALIDATION acc: 0.7987460815047022
2025-01-14 23:23:04,816 [INFO] [52] VALIDATION loss dict: {'classification_loss': 1.8107459678461677}
2025-01-14 23:23:04,816 [INFO] 
2025-01-14 23:23:48,558 [INFO] Step[50/2713]: training loss : 0.964038735628128 TRAIN  loss dict:  {'classification_loss': 0.964038735628128}
2025-01-14 23:24:02,291 [INFO] Step[100/2713]: training loss : 0.9664778411388397 TRAIN  loss dict:  {'classification_loss': 0.9664778411388397}
2025-01-14 23:24:15,693 [INFO] Step[150/2713]: training loss : 0.9324360358715057 TRAIN  loss dict:  {'classification_loss': 0.9324360358715057}
2025-01-14 23:24:29,815 [INFO] Step[200/2713]: training loss : 0.932408561706543 TRAIN  loss dict:  {'classification_loss': 0.932408561706543}
2025-01-14 23:24:44,099 [INFO] Step[250/2713]: training loss : 0.9335366368293763 TRAIN  loss dict:  {'classification_loss': 0.9335366368293763}
2025-01-14 23:24:58,055 [INFO] Step[300/2713]: training loss : 0.9330239701271057 TRAIN  loss dict:  {'classification_loss': 0.9330239701271057}
2025-01-14 23:25:12,054 [INFO] Step[350/2713]: training loss : 0.93340980052948 TRAIN  loss dict:  {'classification_loss': 0.93340980052948}
2025-01-14 23:25:25,930 [INFO] Step[400/2713]: training loss : 0.9317987620830536 TRAIN  loss dict:  {'classification_loss': 0.9317987620830536}
2025-01-14 23:25:39,912 [INFO] Step[450/2713]: training loss : 0.9335519444942474 TRAIN  loss dict:  {'classification_loss': 0.9335519444942474}
2025-01-14 23:25:54,218 [INFO] Step[500/2713]: training loss : 0.9466565310955047 TRAIN  loss dict:  {'classification_loss': 0.9466565310955047}
2025-01-14 23:26:07,750 [INFO] Step[550/2713]: training loss : 0.9342164444923401 TRAIN  loss dict:  {'classification_loss': 0.9342164444923401}
2025-01-14 23:26:21,738 [INFO] Step[600/2713]: training loss : 0.9638266456127167 TRAIN  loss dict:  {'classification_loss': 0.9638266456127167}
2025-01-14 23:26:35,646 [INFO] Step[650/2713]: training loss : 0.932823394536972 TRAIN  loss dict:  {'classification_loss': 0.932823394536972}
2025-01-14 23:26:49,485 [INFO] Step[700/2713]: training loss : 0.9363287889957428 TRAIN  loss dict:  {'classification_loss': 0.9363287889957428}
2025-01-14 23:27:03,093 [INFO] Step[750/2713]: training loss : 0.9335729134082794 TRAIN  loss dict:  {'classification_loss': 0.9335729134082794}
2025-01-14 23:27:16,843 [INFO] Step[800/2713]: training loss : 0.9362073230743408 TRAIN  loss dict:  {'classification_loss': 0.9362073230743408}
2025-01-14 23:27:31,100 [INFO] Step[850/2713]: training loss : 0.9334592843055725 TRAIN  loss dict:  {'classification_loss': 0.9334592843055725}
2025-01-14 23:27:44,360 [INFO] Step[900/2713]: training loss : 0.9345962905883789 TRAIN  loss dict:  {'classification_loss': 0.9345962905883789}
2025-01-14 23:27:57,636 [INFO] Step[950/2713]: training loss : 0.9350373387336731 TRAIN  loss dict:  {'classification_loss': 0.9350373387336731}
2025-01-14 23:28:11,243 [INFO] Step[1000/2713]: training loss : 0.9322795462608338 TRAIN  loss dict:  {'classification_loss': 0.9322795462608338}
2025-01-14 23:28:25,103 [INFO] Step[1050/2713]: training loss : 0.9343643891811371 TRAIN  loss dict:  {'classification_loss': 0.9343643891811371}
2025-01-14 23:28:38,671 [INFO] Step[1100/2713]: training loss : 0.9500633656978608 TRAIN  loss dict:  {'classification_loss': 0.9500633656978608}
2025-01-14 23:28:52,334 [INFO] Step[1150/2713]: training loss : 0.9320609974861145 TRAIN  loss dict:  {'classification_loss': 0.9320609974861145}
2025-01-14 23:29:06,297 [INFO] Step[1200/2713]: training loss : 0.942651959657669 TRAIN  loss dict:  {'classification_loss': 0.942651959657669}
2025-01-14 23:29:19,936 [INFO] Step[1250/2713]: training loss : 0.936508686542511 TRAIN  loss dict:  {'classification_loss': 0.936508686542511}
2025-01-14 23:29:33,766 [INFO] Step[1300/2713]: training loss : 0.9533342373371124 TRAIN  loss dict:  {'classification_loss': 0.9533342373371124}
2025-01-14 23:29:47,033 [INFO] Step[1350/2713]: training loss : 0.9348759591579437 TRAIN  loss dict:  {'classification_loss': 0.9348759591579437}
2025-01-14 23:30:00,352 [INFO] Step[1400/2713]: training loss : 0.936336442232132 TRAIN  loss dict:  {'classification_loss': 0.936336442232132}
2025-01-14 23:30:13,956 [INFO] Step[1450/2713]: training loss : 0.9331886863708496 TRAIN  loss dict:  {'classification_loss': 0.9331886863708496}
2025-01-14 23:30:27,323 [INFO] Step[1500/2713]: training loss : 0.934447294473648 TRAIN  loss dict:  {'classification_loss': 0.934447294473648}
2025-01-14 23:30:41,323 [INFO] Step[1550/2713]: training loss : 0.9329150438308715 TRAIN  loss dict:  {'classification_loss': 0.9329150438308715}
2025-01-14 23:30:54,940 [INFO] Step[1600/2713]: training loss : 0.9344386744499207 TRAIN  loss dict:  {'classification_loss': 0.9344386744499207}
2025-01-14 23:31:08,232 [INFO] Step[1650/2713]: training loss : 0.9360794472694397 TRAIN  loss dict:  {'classification_loss': 0.9360794472694397}
2025-01-14 23:31:21,710 [INFO] Step[1700/2713]: training loss : 0.9377115547657013 TRAIN  loss dict:  {'classification_loss': 0.9377115547657013}
2025-01-14 23:31:35,729 [INFO] Step[1750/2713]: training loss : 0.934299522638321 TRAIN  loss dict:  {'classification_loss': 0.934299522638321}
2025-01-14 23:31:49,618 [INFO] Step[1800/2713]: training loss : 0.9336060464382172 TRAIN  loss dict:  {'classification_loss': 0.9336060464382172}
2025-01-14 23:32:03,512 [INFO] Step[1850/2713]: training loss : 0.9327810561656952 TRAIN  loss dict:  {'classification_loss': 0.9327810561656952}
2025-01-14 23:32:17,601 [INFO] Step[1900/2713]: training loss : 0.9416522407531738 TRAIN  loss dict:  {'classification_loss': 0.9416522407531738}
2025-01-14 23:32:31,821 [INFO] Step[1950/2713]: training loss : 0.9554297542572021 TRAIN  loss dict:  {'classification_loss': 0.9554297542572021}
2025-01-14 23:32:45,474 [INFO] Step[2000/2713]: training loss : 0.9364601314067841 TRAIN  loss dict:  {'classification_loss': 0.9364601314067841}
2025-01-14 23:32:59,276 [INFO] Step[2050/2713]: training loss : 0.9360589182376862 TRAIN  loss dict:  {'classification_loss': 0.9360589182376862}
2025-01-14 23:33:13,118 [INFO] Step[2100/2713]: training loss : 0.9347507083415985 TRAIN  loss dict:  {'classification_loss': 0.9347507083415985}
2025-01-14 23:33:26,621 [INFO] Step[2150/2713]: training loss : 0.9327942574024201 TRAIN  loss dict:  {'classification_loss': 0.9327942574024201}
2025-01-14 23:33:40,202 [INFO] Step[2200/2713]: training loss : 0.9336248683929443 TRAIN  loss dict:  {'classification_loss': 0.9336248683929443}
2025-01-14 23:33:53,813 [INFO] Step[2250/2713]: training loss : 0.9358314180374145 TRAIN  loss dict:  {'classification_loss': 0.9358314180374145}
2025-01-14 23:34:07,292 [INFO] Step[2300/2713]: training loss : 0.9331473791599274 TRAIN  loss dict:  {'classification_loss': 0.9331473791599274}
2025-01-14 23:34:20,544 [INFO] Step[2350/2713]: training loss : 0.9687998640537262 TRAIN  loss dict:  {'classification_loss': 0.9687998640537262}
2025-01-14 23:34:34,194 [INFO] Step[2400/2713]: training loss : 0.9336981225013733 TRAIN  loss dict:  {'classification_loss': 0.9336981225013733}
2025-01-14 23:34:48,426 [INFO] Step[2450/2713]: training loss : 0.9361117267608643 TRAIN  loss dict:  {'classification_loss': 0.9361117267608643}
2025-01-14 23:35:02,202 [INFO] Step[2500/2713]: training loss : 0.9355678355693817 TRAIN  loss dict:  {'classification_loss': 0.9355678355693817}
2025-01-14 23:35:15,480 [INFO] Step[2550/2713]: training loss : 0.9381245601177216 TRAIN  loss dict:  {'classification_loss': 0.9381245601177216}
2025-01-14 23:35:29,278 [INFO] Step[2600/2713]: training loss : 0.9334977090358734 TRAIN  loss dict:  {'classification_loss': 0.9334977090358734}
2025-01-14 23:35:42,586 [INFO] Step[2650/2713]: training loss : 0.9425599348545074 TRAIN  loss dict:  {'classification_loss': 0.9425599348545074}
2025-01-14 23:35:56,551 [INFO] Step[2700/2713]: training loss : 0.9330193686485291 TRAIN  loss dict:  {'classification_loss': 0.9330193686485291}
2025-01-14 23:37:13,167 [INFO] Label accuracies statistics:
2025-01-14 23:37:13,167 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.0, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 0.75, 256: 0.75, 257: 0.75, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.5, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.5, 377: 1.0, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 23:37:13,169 [INFO] [53] TRAIN  loss: 0.9383282126687876 acc: 0.998771347831429
2025-01-14 23:37:13,169 [INFO] [53] TRAIN  loss dict: {'classification_loss': 0.9383282126687876}
2025-01-14 23:37:13,169 [INFO] [53] VALIDATION loss: 1.828765467257428 VALIDATION acc: 0.7956112852664576
2025-01-14 23:37:13,169 [INFO] [53] VALIDATION loss dict: {'classification_loss': 1.828765467257428}
2025-01-14 23:37:13,169 [INFO] 
2025-01-14 23:37:32,211 [INFO] Step[50/2713]: training loss : 0.9327143359184266 TRAIN  loss dict:  {'classification_loss': 0.9327143359184266}
2025-01-14 23:37:45,552 [INFO] Step[100/2713]: training loss : 0.9342961609363556 TRAIN  loss dict:  {'classification_loss': 0.9342961609363556}
2025-01-14 23:37:59,524 [INFO] Step[150/2713]: training loss : 0.9370435512065888 TRAIN  loss dict:  {'classification_loss': 0.9370435512065888}
2025-01-14 23:38:13,173 [INFO] Step[200/2713]: training loss : 0.9327159750461579 TRAIN  loss dict:  {'classification_loss': 0.9327159750461579}
2025-01-14 23:38:26,979 [INFO] Step[250/2713]: training loss : 0.9332813572883606 TRAIN  loss dict:  {'classification_loss': 0.9332813572883606}
2025-01-14 23:38:40,563 [INFO] Step[300/2713]: training loss : 0.9348534369468688 TRAIN  loss dict:  {'classification_loss': 0.9348534369468688}
2025-01-14 23:38:54,234 [INFO] Step[350/2713]: training loss : 0.9353559303283692 TRAIN  loss dict:  {'classification_loss': 0.9353559303283692}
2025-01-14 23:39:08,088 [INFO] Step[400/2713]: training loss : 0.934789822101593 TRAIN  loss dict:  {'classification_loss': 0.934789822101593}
2025-01-14 23:39:21,509 [INFO] Step[450/2713]: training loss : 0.934817544221878 TRAIN  loss dict:  {'classification_loss': 0.934817544221878}
2025-01-14 23:39:34,731 [INFO] Step[500/2713]: training loss : 0.9322447633743286 TRAIN  loss dict:  {'classification_loss': 0.9322447633743286}
2025-01-14 23:39:47,974 [INFO] Step[550/2713]: training loss : 0.9353918218612671 TRAIN  loss dict:  {'classification_loss': 0.9353918218612671}
2025-01-14 23:40:01,834 [INFO] Step[600/2713]: training loss : 0.9338510358333587 TRAIN  loss dict:  {'classification_loss': 0.9338510358333587}
2025-01-14 23:40:15,789 [INFO] Step[650/2713]: training loss : 0.9498960185050964 TRAIN  loss dict:  {'classification_loss': 0.9498960185050964}
2025-01-14 23:40:30,020 [INFO] Step[700/2713]: training loss : 0.9344745552539826 TRAIN  loss dict:  {'classification_loss': 0.9344745552539826}
2025-01-14 23:40:43,549 [INFO] Step[750/2713]: training loss : 0.9335365283489228 TRAIN  loss dict:  {'classification_loss': 0.9335365283489228}
2025-01-14 23:40:57,475 [INFO] Step[800/2713]: training loss : 0.9330278837680817 TRAIN  loss dict:  {'classification_loss': 0.9330278837680817}
2025-01-14 23:41:11,446 [INFO] Step[850/2713]: training loss : 0.9598780715465546 TRAIN  loss dict:  {'classification_loss': 0.9598780715465546}
2025-01-14 23:41:25,081 [INFO] Step[900/2713]: training loss : 0.9342225396633148 TRAIN  loss dict:  {'classification_loss': 0.9342225396633148}
2025-01-14 23:41:38,581 [INFO] Step[950/2713]: training loss : 0.9359784054756165 TRAIN  loss dict:  {'classification_loss': 0.9359784054756165}
2025-01-14 23:41:51,997 [INFO] Step[1000/2713]: training loss : 0.9316845989227295 TRAIN  loss dict:  {'classification_loss': 0.9316845989227295}
2025-01-14 23:42:05,373 [INFO] Step[1050/2713]: training loss : 0.9348593068122864 TRAIN  loss dict:  {'classification_loss': 0.9348593068122864}
2025-01-14 23:42:19,264 [INFO] Step[1100/2713]: training loss : 0.9426745367050171 TRAIN  loss dict:  {'classification_loss': 0.9426745367050171}
2025-01-14 23:42:33,122 [INFO] Step[1150/2713]: training loss : 0.9355512416362762 TRAIN  loss dict:  {'classification_loss': 0.9355512416362762}
2025-01-14 23:42:46,312 [INFO] Step[1200/2713]: training loss : 0.9407567131519318 TRAIN  loss dict:  {'classification_loss': 0.9407567131519318}
2025-01-14 23:42:59,680 [INFO] Step[1250/2713]: training loss : 0.9553371250629425 TRAIN  loss dict:  {'classification_loss': 0.9553371250629425}
2025-01-14 23:43:13,813 [INFO] Step[1300/2713]: training loss : 0.9372830736637116 TRAIN  loss dict:  {'classification_loss': 0.9372830736637116}
2025-01-14 23:43:27,291 [INFO] Step[1350/2713]: training loss : 0.9356486761569976 TRAIN  loss dict:  {'classification_loss': 0.9356486761569976}
2025-01-14 23:43:40,563 [INFO] Step[1400/2713]: training loss : 0.9396395266056061 TRAIN  loss dict:  {'classification_loss': 0.9396395266056061}
2025-01-14 23:43:55,407 [INFO] Step[1450/2713]: training loss : 0.9352243745326996 TRAIN  loss dict:  {'classification_loss': 0.9352243745326996}
2025-01-14 23:44:11,538 [INFO] Step[1500/2713]: training loss : 0.934029061794281 TRAIN  loss dict:  {'classification_loss': 0.934029061794281}
2025-01-14 23:44:25,337 [INFO] Step[1550/2713]: training loss : 0.9324176013469696 TRAIN  loss dict:  {'classification_loss': 0.9324176013469696}
2025-01-14 23:44:39,253 [INFO] Step[1600/2713]: training loss : 0.9331375420093536 TRAIN  loss dict:  {'classification_loss': 0.9331375420093536}
2025-01-14 23:44:53,237 [INFO] Step[1650/2713]: training loss : 0.9639092910289765 TRAIN  loss dict:  {'classification_loss': 0.9639092910289765}
2025-01-14 23:45:06,486 [INFO] Step[1700/2713]: training loss : 0.9620593357086181 TRAIN  loss dict:  {'classification_loss': 0.9620593357086181}
2025-01-14 23:45:20,562 [INFO] Step[1750/2713]: training loss : 0.9341394829750062 TRAIN  loss dict:  {'classification_loss': 0.9341394829750062}
2025-01-14 23:45:34,484 [INFO] Step[1800/2713]: training loss : 0.9339960110187531 TRAIN  loss dict:  {'classification_loss': 0.9339960110187531}
2025-01-14 23:45:48,182 [INFO] Step[1850/2713]: training loss : 0.9338342308998108 TRAIN  loss dict:  {'classification_loss': 0.9338342308998108}
2025-01-14 23:46:04,609 [INFO] Step[1900/2713]: training loss : 0.9324469304084778 TRAIN  loss dict:  {'classification_loss': 0.9324469304084778}
2025-01-14 23:46:18,155 [INFO] Step[1950/2713]: training loss : 0.9339623355865478 TRAIN  loss dict:  {'classification_loss': 0.9339623355865478}
2025-01-14 23:46:31,984 [INFO] Step[2000/2713]: training loss : 0.9345812904834747 TRAIN  loss dict:  {'classification_loss': 0.9345812904834747}
2025-01-14 23:46:45,484 [INFO] Step[2050/2713]: training loss : 0.9316552758216858 TRAIN  loss dict:  {'classification_loss': 0.9316552758216858}
2025-01-14 23:46:58,983 [INFO] Step[2100/2713]: training loss : 0.9338628661632538 TRAIN  loss dict:  {'classification_loss': 0.9338628661632538}
2025-01-14 23:47:12,435 [INFO] Step[2150/2713]: training loss : 0.9382594347000122 TRAIN  loss dict:  {'classification_loss': 0.9382594347000122}
2025-01-14 23:47:26,731 [INFO] Step[2200/2713]: training loss : 0.931079318523407 TRAIN  loss dict:  {'classification_loss': 0.931079318523407}
2025-01-14 23:47:40,422 [INFO] Step[2250/2713]: training loss : 0.9359187698364257 TRAIN  loss dict:  {'classification_loss': 0.9359187698364257}
2025-01-14 23:47:54,265 [INFO] Step[2300/2713]: training loss : 0.9410840702056885 TRAIN  loss dict:  {'classification_loss': 0.9410840702056885}
2025-01-14 23:48:08,495 [INFO] Step[2350/2713]: training loss : 0.9380178606510162 TRAIN  loss dict:  {'classification_loss': 0.9380178606510162}
2025-01-14 23:48:22,588 [INFO] Step[2400/2713]: training loss : 0.9424651682376861 TRAIN  loss dict:  {'classification_loss': 0.9424651682376861}
2025-01-14 23:48:35,754 [INFO] Step[2450/2713]: training loss : 0.9336375021934509 TRAIN  loss dict:  {'classification_loss': 0.9336375021934509}
2025-01-14 23:48:49,600 [INFO] Step[2500/2713]: training loss : 0.9365635931491851 TRAIN  loss dict:  {'classification_loss': 0.9365635931491851}
2025-01-14 23:49:03,193 [INFO] Step[2550/2713]: training loss : 0.9337710046768188 TRAIN  loss dict:  {'classification_loss': 0.9337710046768188}
2025-01-14 23:49:16,796 [INFO] Step[2600/2713]: training loss : 0.939181443452835 TRAIN  loss dict:  {'classification_loss': 0.939181443452835}
2025-01-14 23:49:30,526 [INFO] Step[2650/2713]: training loss : 0.937553688287735 TRAIN  loss dict:  {'classification_loss': 0.937553688287735}
2025-01-14 23:49:44,167 [INFO] Step[2700/2713]: training loss : 0.933903112411499 TRAIN  loss dict:  {'classification_loss': 0.933903112411499}
2025-01-14 23:51:13,464 [INFO] Label accuracies statistics:
2025-01-14 23:51:13,464 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 0.75, 184: 1.0, 185: 0.75, 186: 0.5, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 1.0, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-14 23:51:13,466 [INFO] [54] TRAIN  loss: 0.93732515600305 acc: 0.998771347831429
2025-01-14 23:51:13,467 [INFO] [54] TRAIN  loss dict: {'classification_loss': 0.93732515600305}
2025-01-14 23:51:13,467 [INFO] [54] VALIDATION loss: 1.7967938919712727 VALIDATION acc: 0.8087774294670846
2025-01-14 23:51:13,467 [INFO] [54] VALIDATION loss dict: {'classification_loss': 1.7967938919712727}
2025-01-14 23:51:13,467 [INFO] 
2025-01-14 23:51:32,494 [INFO] Step[50/2713]: training loss : 0.9370261752605438 TRAIN  loss dict:  {'classification_loss': 0.9370261752605438}
2025-01-14 23:51:46,382 [INFO] Step[100/2713]: training loss : 0.9317178404331208 TRAIN  loss dict:  {'classification_loss': 0.9317178404331208}
2025-01-14 23:51:59,946 [INFO] Step[150/2713]: training loss : 0.9337178170681 TRAIN  loss dict:  {'classification_loss': 0.9337178170681}
2025-01-14 23:52:13,902 [INFO] Step[200/2713]: training loss : 0.9349447166919709 TRAIN  loss dict:  {'classification_loss': 0.9349447166919709}
2025-01-14 23:52:27,483 [INFO] Step[250/2713]: training loss : 0.9343825030326843 TRAIN  loss dict:  {'classification_loss': 0.9343825030326843}
2025-01-14 23:52:40,766 [INFO] Step[300/2713]: training loss : 0.9594577848911285 TRAIN  loss dict:  {'classification_loss': 0.9594577848911285}
2025-01-14 23:52:54,017 [INFO] Step[350/2713]: training loss : 0.9327414524555206 TRAIN  loss dict:  {'classification_loss': 0.9327414524555206}
2025-01-14 23:53:07,930 [INFO] Step[400/2713]: training loss : 0.9354188513755798 TRAIN  loss dict:  {'classification_loss': 0.9354188513755798}
2025-01-14 23:53:21,341 [INFO] Step[450/2713]: training loss : 0.9344882273674011 TRAIN  loss dict:  {'classification_loss': 0.9344882273674011}
2025-01-14 23:53:35,431 [INFO] Step[500/2713]: training loss : 0.9723138320446014 TRAIN  loss dict:  {'classification_loss': 0.9723138320446014}
2025-01-14 23:53:49,619 [INFO] Step[550/2713]: training loss : 0.9333919978141785 TRAIN  loss dict:  {'classification_loss': 0.9333919978141785}
2025-01-14 23:54:03,473 [INFO] Step[600/2713]: training loss : 0.9332608902454376 TRAIN  loss dict:  {'classification_loss': 0.9332608902454376}
2025-01-14 23:54:17,236 [INFO] Step[650/2713]: training loss : 0.9330738294124603 TRAIN  loss dict:  {'classification_loss': 0.9330738294124603}
2025-01-14 23:54:30,876 [INFO] Step[700/2713]: training loss : 0.9319313609600067 TRAIN  loss dict:  {'classification_loss': 0.9319313609600067}
2025-01-14 23:54:44,457 [INFO] Step[750/2713]: training loss : 0.9340174686908722 TRAIN  loss dict:  {'classification_loss': 0.9340174686908722}
2025-01-14 23:54:58,257 [INFO] Step[800/2713]: training loss : 0.9340531969070435 TRAIN  loss dict:  {'classification_loss': 0.9340531969070435}
2025-01-14 23:55:12,296 [INFO] Step[850/2713]: training loss : 0.9339335250854492 TRAIN  loss dict:  {'classification_loss': 0.9339335250854492}
2025-01-14 23:55:26,376 [INFO] Step[900/2713]: training loss : 0.9329203915596008 TRAIN  loss dict:  {'classification_loss': 0.9329203915596008}
2025-01-14 23:55:39,831 [INFO] Step[950/2713]: training loss : 0.9334486627578735 TRAIN  loss dict:  {'classification_loss': 0.9334486627578735}
2025-01-14 23:55:54,118 [INFO] Step[1000/2713]: training loss : 0.9333310031890869 TRAIN  loss dict:  {'classification_loss': 0.9333310031890869}
2025-01-14 23:56:07,708 [INFO] Step[1050/2713]: training loss : 0.9334565329551697 TRAIN  loss dict:  {'classification_loss': 0.9334565329551697}
2025-01-14 23:56:21,453 [INFO] Step[1100/2713]: training loss : 0.932229552268982 TRAIN  loss dict:  {'classification_loss': 0.932229552268982}
2025-01-14 23:56:34,898 [INFO] Step[1150/2713]: training loss : 0.9463168823719025 TRAIN  loss dict:  {'classification_loss': 0.9463168823719025}
2025-01-14 23:56:48,788 [INFO] Step[1200/2713]: training loss : 0.9461030161380768 TRAIN  loss dict:  {'classification_loss': 0.9461030161380768}
2025-01-14 23:57:02,121 [INFO] Step[1250/2713]: training loss : 0.9336700081825257 TRAIN  loss dict:  {'classification_loss': 0.9336700081825257}
2025-01-14 23:57:15,361 [INFO] Step[1300/2713]: training loss : 0.9357362985610962 TRAIN  loss dict:  {'classification_loss': 0.9357362985610962}
2025-01-14 23:57:29,100 [INFO] Step[1350/2713]: training loss : 0.933968061208725 TRAIN  loss dict:  {'classification_loss': 0.933968061208725}
2025-01-14 23:57:42,524 [INFO] Step[1400/2713]: training loss : 0.9530078458786011 TRAIN  loss dict:  {'classification_loss': 0.9530078458786011}
2025-01-14 23:57:56,052 [INFO] Step[1450/2713]: training loss : 0.9345944023132324 TRAIN  loss dict:  {'classification_loss': 0.9345944023132324}
2025-01-14 23:58:10,014 [INFO] Step[1500/2713]: training loss : 0.9324893426895141 TRAIN  loss dict:  {'classification_loss': 0.9324893426895141}
2025-01-14 23:58:24,225 [INFO] Step[1550/2713]: training loss : 0.9582589614391327 TRAIN  loss dict:  {'classification_loss': 0.9582589614391327}
2025-01-14 23:58:37,881 [INFO] Step[1600/2713]: training loss : 0.9453477334976196 TRAIN  loss dict:  {'classification_loss': 0.9453477334976196}
2025-01-14 23:58:51,155 [INFO] Step[1650/2713]: training loss : 0.9390599596500396 TRAIN  loss dict:  {'classification_loss': 0.9390599596500396}
2025-01-14 23:59:04,580 [INFO] Step[1700/2713]: training loss : 0.937622641324997 TRAIN  loss dict:  {'classification_loss': 0.937622641324997}
2025-01-14 23:59:18,200 [INFO] Step[1750/2713]: training loss : 0.9355587112903595 TRAIN  loss dict:  {'classification_loss': 0.9355587112903595}
2025-01-14 23:59:32,497 [INFO] Step[1800/2713]: training loss : 0.9324232017993928 TRAIN  loss dict:  {'classification_loss': 0.9324232017993928}
2025-01-14 23:59:46,394 [INFO] Step[1850/2713]: training loss : 0.9357184374332428 TRAIN  loss dict:  {'classification_loss': 0.9357184374332428}
2025-01-15 00:00:00,278 [INFO] Step[1900/2713]: training loss : 0.9349030423164367 TRAIN  loss dict:  {'classification_loss': 0.9349030423164367}
2025-01-15 00:00:13,497 [INFO] Step[1950/2713]: training loss : 0.9348365068435669 TRAIN  loss dict:  {'classification_loss': 0.9348365068435669}
2025-01-15 00:00:26,990 [INFO] Step[2000/2713]: training loss : 0.9412420892715454 TRAIN  loss dict:  {'classification_loss': 0.9412420892715454}
2025-01-15 00:00:40,864 [INFO] Step[2050/2713]: training loss : 0.9336468505859375 TRAIN  loss dict:  {'classification_loss': 0.9336468505859375}
2025-01-15 00:00:54,492 [INFO] Step[2100/2713]: training loss : 0.9323027598857879 TRAIN  loss dict:  {'classification_loss': 0.9323027598857879}
2025-01-15 00:01:08,581 [INFO] Step[2150/2713]: training loss : 0.9357579147815704 TRAIN  loss dict:  {'classification_loss': 0.9357579147815704}
2025-01-15 00:01:22,464 [INFO] Step[2200/2713]: training loss : 0.936624585390091 TRAIN  loss dict:  {'classification_loss': 0.936624585390091}
2025-01-15 00:01:35,654 [INFO] Step[2250/2713]: training loss : 0.9363687264919281 TRAIN  loss dict:  {'classification_loss': 0.9363687264919281}
2025-01-15 00:01:48,780 [INFO] Step[2300/2713]: training loss : 0.9351356673240662 TRAIN  loss dict:  {'classification_loss': 0.9351356673240662}
2025-01-15 00:02:02,459 [INFO] Step[2350/2713]: training loss : 0.9361811232566833 TRAIN  loss dict:  {'classification_loss': 0.9361811232566833}
2025-01-15 00:02:15,802 [INFO] Step[2400/2713]: training loss : 0.9522555375099182 TRAIN  loss dict:  {'classification_loss': 0.9522555375099182}
2025-01-15 00:02:29,367 [INFO] Step[2450/2713]: training loss : 0.9512598574161529 TRAIN  loss dict:  {'classification_loss': 0.9512598574161529}
2025-01-15 00:02:43,174 [INFO] Step[2500/2713]: training loss : 0.9390083515644073 TRAIN  loss dict:  {'classification_loss': 0.9390083515644073}
2025-01-15 00:02:57,297 [INFO] Step[2550/2713]: training loss : 0.9513243985176086 TRAIN  loss dict:  {'classification_loss': 0.9513243985176086}
2025-01-15 00:03:11,383 [INFO] Step[2600/2713]: training loss : 0.9329691386222839 TRAIN  loss dict:  {'classification_loss': 0.9329691386222839}
2025-01-15 00:03:25,024 [INFO] Step[2650/2713]: training loss : 0.9342818975448608 TRAIN  loss dict:  {'classification_loss': 0.9342818975448608}
2025-01-15 00:03:38,374 [INFO] Step[2700/2713]: training loss : 0.9369062507152557 TRAIN  loss dict:  {'classification_loss': 0.9369062507152557}
2025-01-15 00:04:54,902 [INFO] Label accuracies statistics:
2025-01-15 00:04:54,902 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.5, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 1.0, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 0.75, 256: 1.0, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 0.5, 304: 0.5, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 00:04:54,904 [INFO] [55] TRAIN  loss: 0.9381413002581865 acc: 0.998771347831429
2025-01-15 00:04:54,904 [INFO] [55] TRAIN  loss dict: {'classification_loss': 0.9381413002581865}
2025-01-15 00:04:54,904 [INFO] [55] VALIDATION loss: 1.7868476187376152 VALIDATION acc: 0.8081504702194358
2025-01-15 00:04:54,904 [INFO] [55] VALIDATION loss dict: {'classification_loss': 1.7868476187376152}
2025-01-15 00:04:54,904 [INFO] 
2025-01-15 00:05:23,948 [INFO] Step[50/2713]: training loss : 0.9329029774665832 TRAIN  loss dict:  {'classification_loss': 0.9329029774665832}
2025-01-15 00:05:37,795 [INFO] Step[100/2713]: training loss : 0.9328846335411072 TRAIN  loss dict:  {'classification_loss': 0.9328846335411072}
2025-01-15 00:05:51,838 [INFO] Step[150/2713]: training loss : 0.9647794759273529 TRAIN  loss dict:  {'classification_loss': 0.9647794759273529}
2025-01-15 00:06:07,890 [INFO] Step[200/2713]: training loss : 0.932617678642273 TRAIN  loss dict:  {'classification_loss': 0.932617678642273}
2025-01-15 00:06:21,999 [INFO] Step[250/2713]: training loss : 0.9339297461509705 TRAIN  loss dict:  {'classification_loss': 0.9339297461509705}
2025-01-15 00:06:35,533 [INFO] Step[300/2713]: training loss : 0.9373270630836487 TRAIN  loss dict:  {'classification_loss': 0.9373270630836487}
2025-01-15 00:06:49,398 [INFO] Step[350/2713]: training loss : 0.9344106817245483 TRAIN  loss dict:  {'classification_loss': 0.9344106817245483}
2025-01-15 00:07:02,767 [INFO] Step[400/2713]: training loss : 0.9423763740062714 TRAIN  loss dict:  {'classification_loss': 0.9423763740062714}
2025-01-15 00:07:15,994 [INFO] Step[450/2713]: training loss : 0.931617089509964 TRAIN  loss dict:  {'classification_loss': 0.931617089509964}
2025-01-15 00:07:29,541 [INFO] Step[500/2713]: training loss : 0.9345883762836457 TRAIN  loss dict:  {'classification_loss': 0.9345883762836457}
2025-01-15 00:07:42,760 [INFO] Step[550/2713]: training loss : 0.9458618712425232 TRAIN  loss dict:  {'classification_loss': 0.9458618712425232}
2025-01-15 00:07:56,236 [INFO] Step[600/2713]: training loss : 0.9337076628208161 TRAIN  loss dict:  {'classification_loss': 0.9337076628208161}
2025-01-15 00:08:09,829 [INFO] Step[650/2713]: training loss : 0.934002628326416 TRAIN  loss dict:  {'classification_loss': 0.934002628326416}
2025-01-15 00:08:23,593 [INFO] Step[700/2713]: training loss : 0.9336626851558685 TRAIN  loss dict:  {'classification_loss': 0.9336626851558685}
2025-01-15 00:08:37,841 [INFO] Step[750/2713]: training loss : 0.9324702262878418 TRAIN  loss dict:  {'classification_loss': 0.9324702262878418}
2025-01-15 00:08:51,775 [INFO] Step[800/2713]: training loss : 0.9557213950157165 TRAIN  loss dict:  {'classification_loss': 0.9557213950157165}
2025-01-15 00:09:05,546 [INFO] Step[850/2713]: training loss : 0.9343816757202148 TRAIN  loss dict:  {'classification_loss': 0.9343816757202148}
2025-01-15 00:09:19,400 [INFO] Step[900/2713]: training loss : 0.9329690098762512 TRAIN  loss dict:  {'classification_loss': 0.9329690098762512}
2025-01-15 00:09:33,074 [INFO] Step[950/2713]: training loss : 0.932827730178833 TRAIN  loss dict:  {'classification_loss': 0.932827730178833}
2025-01-15 00:09:46,259 [INFO] Step[1000/2713]: training loss : 0.9315493810176849 TRAIN  loss dict:  {'classification_loss': 0.9315493810176849}
2025-01-15 00:09:59,572 [INFO] Step[1050/2713]: training loss : 0.93292813539505 TRAIN  loss dict:  {'classification_loss': 0.93292813539505}
2025-01-15 00:10:12,775 [INFO] Step[1100/2713]: training loss : 0.9462760043144226 TRAIN  loss dict:  {'classification_loss': 0.9462760043144226}
2025-01-15 00:10:26,345 [INFO] Step[1150/2713]: training loss : 0.9329512751102448 TRAIN  loss dict:  {'classification_loss': 0.9329512751102448}
2025-01-15 00:10:39,984 [INFO] Step[1200/2713]: training loss : 0.9374558699131011 TRAIN  loss dict:  {'classification_loss': 0.9374558699131011}
2025-01-15 00:10:53,219 [INFO] Step[1250/2713]: training loss : 0.9313751780986785 TRAIN  loss dict:  {'classification_loss': 0.9313751780986785}
2025-01-15 00:11:06,994 [INFO] Step[1300/2713]: training loss : 0.9385671555995941 TRAIN  loss dict:  {'classification_loss': 0.9385671555995941}
2025-01-15 00:11:21,020 [INFO] Step[1350/2713]: training loss : 0.9318313944339752 TRAIN  loss dict:  {'classification_loss': 0.9318313944339752}
2025-01-15 00:11:35,140 [INFO] Step[1400/2713]: training loss : 0.9311049675941467 TRAIN  loss dict:  {'classification_loss': 0.9311049675941467}
2025-01-15 00:11:48,406 [INFO] Step[1450/2713]: training loss : 0.9662306702136993 TRAIN  loss dict:  {'classification_loss': 0.9662306702136993}
2025-01-15 00:12:01,656 [INFO] Step[1500/2713]: training loss : 0.935241414308548 TRAIN  loss dict:  {'classification_loss': 0.935241414308548}
2025-01-15 00:12:15,179 [INFO] Step[1550/2713]: training loss : 0.9343580818176269 TRAIN  loss dict:  {'classification_loss': 0.9343580818176269}
2025-01-15 00:12:28,809 [INFO] Step[1600/2713]: training loss : 0.9356608593463898 TRAIN  loss dict:  {'classification_loss': 0.9356608593463898}
2025-01-15 00:12:42,354 [INFO] Step[1650/2713]: training loss : 0.9340123057365417 TRAIN  loss dict:  {'classification_loss': 0.9340123057365417}
2025-01-15 00:12:56,176 [INFO] Step[1700/2713]: training loss : 0.9384850192070008 TRAIN  loss dict:  {'classification_loss': 0.9384850192070008}
2025-01-15 00:13:10,363 [INFO] Step[1750/2713]: training loss : 0.9587091743946076 TRAIN  loss dict:  {'classification_loss': 0.9587091743946076}
2025-01-15 00:13:24,484 [INFO] Step[1800/2713]: training loss : 0.9451203298568726 TRAIN  loss dict:  {'classification_loss': 0.9451203298568726}
2025-01-15 00:13:38,257 [INFO] Step[1850/2713]: training loss : 0.9347337877750397 TRAIN  loss dict:  {'classification_loss': 0.9347337877750397}
2025-01-15 00:13:51,887 [INFO] Step[1900/2713]: training loss : 0.9376318717002868 TRAIN  loss dict:  {'classification_loss': 0.9376318717002868}
2025-01-15 00:14:05,327 [INFO] Step[1950/2713]: training loss : 0.9341871738433838 TRAIN  loss dict:  {'classification_loss': 0.9341871738433838}
2025-01-15 00:14:18,522 [INFO] Step[2000/2713]: training loss : 0.9334976541996002 TRAIN  loss dict:  {'classification_loss': 0.9334976541996002}
2025-01-15 00:14:32,085 [INFO] Step[2050/2713]: training loss : 0.9328729951381683 TRAIN  loss dict:  {'classification_loss': 0.9328729951381683}
2025-01-15 00:14:45,794 [INFO] Step[2100/2713]: training loss : 0.9333935630321503 TRAIN  loss dict:  {'classification_loss': 0.9333935630321503}
2025-01-15 00:15:00,024 [INFO] Step[2150/2713]: training loss : 0.9388443338871002 TRAIN  loss dict:  {'classification_loss': 0.9388443338871002}
2025-01-15 00:15:13,444 [INFO] Step[2200/2713]: training loss : 0.9332615780830383 TRAIN  loss dict:  {'classification_loss': 0.9332615780830383}
2025-01-15 00:15:27,540 [INFO] Step[2250/2713]: training loss : 0.9610610687732697 TRAIN  loss dict:  {'classification_loss': 0.9610610687732697}
2025-01-15 00:15:41,266 [INFO] Step[2300/2713]: training loss : 0.9629860436916351 TRAIN  loss dict:  {'classification_loss': 0.9629860436916351}
2025-01-15 00:15:55,144 [INFO] Step[2350/2713]: training loss : 0.9337289929389954 TRAIN  loss dict:  {'classification_loss': 0.9337289929389954}
2025-01-15 00:16:09,159 [INFO] Step[2400/2713]: training loss : 0.9324519908428193 TRAIN  loss dict:  {'classification_loss': 0.9324519908428193}
2025-01-15 00:16:22,403 [INFO] Step[2450/2713]: training loss : 0.9328370571136475 TRAIN  loss dict:  {'classification_loss': 0.9328370571136475}
2025-01-15 00:16:36,059 [INFO] Step[2500/2713]: training loss : 0.9437214827537537 TRAIN  loss dict:  {'classification_loss': 0.9437214827537537}
2025-01-15 00:16:49,292 [INFO] Step[2550/2713]: training loss : 0.9327715349197387 TRAIN  loss dict:  {'classification_loss': 0.9327715349197387}
2025-01-15 00:17:02,766 [INFO] Step[2600/2713]: training loss : 0.933710902929306 TRAIN  loss dict:  {'classification_loss': 0.933710902929306}
2025-01-15 00:17:16,305 [INFO] Step[2650/2713]: training loss : 0.9334788286685943 TRAIN  loss dict:  {'classification_loss': 0.9334788286685943}
2025-01-15 00:17:29,602 [INFO] Step[2700/2713]: training loss : 0.9400972163677216 TRAIN  loss dict:  {'classification_loss': 0.9400972163677216}
2025-01-15 00:19:08,163 [INFO] Label accuracies statistics:
2025-01-15 00:19:08,163 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.5, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 0.75, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.5, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.5, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-15 00:19:08,165 [INFO] [56] TRAIN  loss: 0.9381492956951013 acc: 0.9985256173977147
2025-01-15 00:19:08,165 [INFO] [56] TRAIN  loss dict: {'classification_loss': 0.9381492956951013}
2025-01-15 00:19:08,165 [INFO] [56] VALIDATION loss: 1.7786402601496618 VALIDATION acc: 0.8081504702194358
2025-01-15 00:19:08,165 [INFO] [56] VALIDATION loss dict: {'classification_loss': 1.7786402601496618}
2025-01-15 00:19:08,165 [INFO] 
2025-01-15 00:19:31,638 [INFO] Step[50/2713]: training loss : 0.9382093524932862 TRAIN  loss dict:  {'classification_loss': 0.9382093524932862}
2025-01-15 00:19:45,555 [INFO] Step[100/2713]: training loss : 0.9328749990463256 TRAIN  loss dict:  {'classification_loss': 0.9328749990463256}
2025-01-15 00:19:58,884 [INFO] Step[150/2713]: training loss : 0.9320283317565918 TRAIN  loss dict:  {'classification_loss': 0.9320283317565918}
2025-01-15 00:20:12,435 [INFO] Step[200/2713]: training loss : 0.9318394470214844 TRAIN  loss dict:  {'classification_loss': 0.9318394470214844}
2025-01-15 00:20:25,619 [INFO] Step[250/2713]: training loss : 0.9382371199131012 TRAIN  loss dict:  {'classification_loss': 0.9382371199131012}
2025-01-15 00:20:39,657 [INFO] Step[300/2713]: training loss : 0.9373280429840087 TRAIN  loss dict:  {'classification_loss': 0.9373280429840087}
2025-01-15 00:20:53,125 [INFO] Step[350/2713]: training loss : 0.9320389676094055 TRAIN  loss dict:  {'classification_loss': 0.9320389676094055}
2025-01-15 00:21:06,523 [INFO] Step[400/2713]: training loss : 0.9351008629798889 TRAIN  loss dict:  {'classification_loss': 0.9351008629798889}
2025-01-15 00:21:19,681 [INFO] Step[450/2713]: training loss : 0.9318377184867859 TRAIN  loss dict:  {'classification_loss': 0.9318377184867859}
2025-01-15 00:21:33,195 [INFO] Step[500/2713]: training loss : 0.9561853921413421 TRAIN  loss dict:  {'classification_loss': 0.9561853921413421}
2025-01-15 00:21:46,763 [INFO] Step[550/2713]: training loss : 0.9359141087532044 TRAIN  loss dict:  {'classification_loss': 0.9359141087532044}
2025-01-15 00:22:00,549 [INFO] Step[600/2713]: training loss : 0.9325238394737244 TRAIN  loss dict:  {'classification_loss': 0.9325238394737244}
2025-01-15 00:22:14,365 [INFO] Step[650/2713]: training loss : 0.942115570306778 TRAIN  loss dict:  {'classification_loss': 0.942115570306778}
2025-01-15 00:22:27,958 [INFO] Step[700/2713]: training loss : 0.9450830769538879 TRAIN  loss dict:  {'classification_loss': 0.9450830769538879}
2025-01-15 00:22:41,735 [INFO] Step[750/2713]: training loss : 0.9331422662734985 TRAIN  loss dict:  {'classification_loss': 0.9331422662734985}
2025-01-15 00:22:55,805 [INFO] Step[800/2713]: training loss : 0.935109372138977 TRAIN  loss dict:  {'classification_loss': 0.935109372138977}
2025-01-15 00:23:09,602 [INFO] Step[850/2713]: training loss : 0.9435427367687226 TRAIN  loss dict:  {'classification_loss': 0.9435427367687226}
2025-01-15 00:23:24,405 [INFO] Step[900/2713]: training loss : 0.9433902752399445 TRAIN  loss dict:  {'classification_loss': 0.9433902752399445}
2025-01-15 00:23:39,325 [INFO] Step[950/2713]: training loss : 0.9322950625419617 TRAIN  loss dict:  {'classification_loss': 0.9322950625419617}
2025-01-15 00:23:52,691 [INFO] Step[1000/2713]: training loss : 0.9381950259208679 TRAIN  loss dict:  {'classification_loss': 0.9381950259208679}
2025-01-15 00:24:06,753 [INFO] Step[1050/2713]: training loss : 0.9348626017570496 TRAIN  loss dict:  {'classification_loss': 0.9348626017570496}
2025-01-15 00:24:20,147 [INFO] Step[1100/2713]: training loss : 0.9641927886009216 TRAIN  loss dict:  {'classification_loss': 0.9641927886009216}
2025-01-15 00:24:33,472 [INFO] Step[1150/2713]: training loss : 0.9423427295684814 TRAIN  loss dict:  {'classification_loss': 0.9423427295684814}
2025-01-15 00:24:46,661 [INFO] Step[1200/2713]: training loss : 0.9325859689712525 TRAIN  loss dict:  {'classification_loss': 0.9325859689712525}
2025-01-15 00:25:00,454 [INFO] Step[1250/2713]: training loss : 0.9310079550743103 TRAIN  loss dict:  {'classification_loss': 0.9310079550743103}
2025-01-15 00:25:13,815 [INFO] Step[1300/2713]: training loss : 0.9327940130233765 TRAIN  loss dict:  {'classification_loss': 0.9327940130233765}
2025-01-15 00:25:27,825 [INFO] Step[1350/2713]: training loss : 0.9401634395122528 TRAIN  loss dict:  {'classification_loss': 0.9401634395122528}
2025-01-15 00:25:41,372 [INFO] Step[1400/2713]: training loss : 0.9326882064342499 TRAIN  loss dict:  {'classification_loss': 0.9326882064342499}
2025-01-15 00:25:54,992 [INFO] Step[1450/2713]: training loss : 0.9525186395645142 TRAIN  loss dict:  {'classification_loss': 0.9525186395645142}
2025-01-15 00:26:08,650 [INFO] Step[1500/2713]: training loss : 0.937784550189972 TRAIN  loss dict:  {'classification_loss': 0.937784550189972}
2025-01-15 00:26:21,886 [INFO] Step[1550/2713]: training loss : 0.9324953866004944 TRAIN  loss dict:  {'classification_loss': 0.9324953866004944}
2025-01-15 00:26:35,607 [INFO] Step[1600/2713]: training loss : 0.933727103471756 TRAIN  loss dict:  {'classification_loss': 0.933727103471756}
2025-01-15 00:26:49,701 [INFO] Step[1650/2713]: training loss : 0.9379653608798981 TRAIN  loss dict:  {'classification_loss': 0.9379653608798981}
2025-01-15 00:27:03,189 [INFO] Step[1700/2713]: training loss : 0.9329376697540284 TRAIN  loss dict:  {'classification_loss': 0.9329376697540284}
2025-01-15 00:27:17,050 [INFO] Step[1750/2713]: training loss : 0.9332401120662689 TRAIN  loss dict:  {'classification_loss': 0.9332401120662689}
2025-01-15 00:27:30,940 [INFO] Step[1800/2713]: training loss : 0.934458817243576 TRAIN  loss dict:  {'classification_loss': 0.934458817243576}
2025-01-15 00:27:44,394 [INFO] Step[1850/2713]: training loss : 0.9324413239955902 TRAIN  loss dict:  {'classification_loss': 0.9324413239955902}
2025-01-15 00:27:58,277 [INFO] Step[1900/2713]: training loss : 0.9328402709960938 TRAIN  loss dict:  {'classification_loss': 0.9328402709960938}
2025-01-15 00:28:11,687 [INFO] Step[1950/2713]: training loss : 0.9397044026851654 TRAIN  loss dict:  {'classification_loss': 0.9397044026851654}
2025-01-15 00:28:25,479 [INFO] Step[2000/2713]: training loss : 0.9319536471366883 TRAIN  loss dict:  {'classification_loss': 0.9319536471366883}
2025-01-15 00:28:39,592 [INFO] Step[2050/2713]: training loss : 0.9480621814727783 TRAIN  loss dict:  {'classification_loss': 0.9480621814727783}
2025-01-15 00:28:53,549 [INFO] Step[2100/2713]: training loss : 0.9325198531150818 TRAIN  loss dict:  {'classification_loss': 0.9325198531150818}
2025-01-15 00:29:06,887 [INFO] Step[2150/2713]: training loss : 0.9328059816360473 TRAIN  loss dict:  {'classification_loss': 0.9328059816360473}
2025-01-15 00:29:20,409 [INFO] Step[2200/2713]: training loss : 0.9309855401515961 TRAIN  loss dict:  {'classification_loss': 0.9309855401515961}
2025-01-15 00:29:34,251 [INFO] Step[2250/2713]: training loss : 0.932882491350174 TRAIN  loss dict:  {'classification_loss': 0.932882491350174}
2025-01-15 00:29:48,359 [INFO] Step[2300/2713]: training loss : 0.9507639718055725 TRAIN  loss dict:  {'classification_loss': 0.9507639718055725}
2025-01-15 00:30:02,142 [INFO] Step[2350/2713]: training loss : 0.931364175081253 TRAIN  loss dict:  {'classification_loss': 0.931364175081253}
2025-01-15 00:30:15,691 [INFO] Step[2400/2713]: training loss : 0.9325115489959717 TRAIN  loss dict:  {'classification_loss': 0.9325115489959717}
2025-01-15 00:30:29,641 [INFO] Step[2450/2713]: training loss : 0.9328059947490692 TRAIN  loss dict:  {'classification_loss': 0.9328059947490692}
2025-01-15 00:30:42,870 [INFO] Step[2500/2713]: training loss : 0.935134791135788 TRAIN  loss dict:  {'classification_loss': 0.935134791135788}
2025-01-15 00:30:56,934 [INFO] Step[2550/2713]: training loss : 0.9341537177562713 TRAIN  loss dict:  {'classification_loss': 0.9341537177562713}
2025-01-15 00:31:10,277 [INFO] Step[2600/2713]: training loss : 0.9323770391941071 TRAIN  loss dict:  {'classification_loss': 0.9323770391941071}
2025-01-15 00:31:23,793 [INFO] Step[2650/2713]: training loss : 0.9352035987377166 TRAIN  loss dict:  {'classification_loss': 0.9352035987377166}
2025-01-15 00:31:37,344 [INFO] Step[2700/2713]: training loss : 0.931603718996048 TRAIN  loss dict:  {'classification_loss': 0.931603718996048}
2025-01-15 00:33:05,022 [INFO] Label accuracies statistics:
2025-01-15 00:33:05,022 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 0.75, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.75, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.25, 395: 0.75, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-15 00:33:06,355 [INFO] [57] TRAIN  loss: 0.9367131386961481 acc: 0.9985256173977147
2025-01-15 00:33:06,355 [INFO] [57] TRAIN  loss dict: {'classification_loss': 0.9367131386961481}
2025-01-15 00:33:06,355 [INFO] [57] VALIDATION loss: 1.7464483838556404 VALIDATION acc: 0.8175548589341692
2025-01-15 00:33:06,355 [INFO] [57] VALIDATION loss dict: {'classification_loss': 1.7464483838556404}
2025-01-15 00:33:06,355 [INFO] 
2025-01-15 00:33:43,490 [INFO] Step[50/2713]: training loss : 0.9327595794200897 TRAIN  loss dict:  {'classification_loss': 0.9327595794200897}
2025-01-15 00:33:57,075 [INFO] Step[100/2713]: training loss : 0.9361885213851928 TRAIN  loss dict:  {'classification_loss': 0.9361885213851928}
2025-01-15 00:34:10,581 [INFO] Step[150/2713]: training loss : 0.937771178483963 TRAIN  loss dict:  {'classification_loss': 0.937771178483963}
2025-01-15 00:34:24,998 [INFO] Step[200/2713]: training loss : 0.9319590222835541 TRAIN  loss dict:  {'classification_loss': 0.9319590222835541}
2025-01-15 00:34:40,268 [INFO] Step[250/2713]: training loss : 0.9325606644153595 TRAIN  loss dict:  {'classification_loss': 0.9325606644153595}
2025-01-15 00:34:53,853 [INFO] Step[300/2713]: training loss : 0.9308724594116211 TRAIN  loss dict:  {'classification_loss': 0.9308724594116211}
2025-01-15 00:35:07,814 [INFO] Step[350/2713]: training loss : 0.9325512051582336 TRAIN  loss dict:  {'classification_loss': 0.9325512051582336}
2025-01-15 00:35:21,170 [INFO] Step[400/2713]: training loss : 0.9323088181018829 TRAIN  loss dict:  {'classification_loss': 0.9323088181018829}
2025-01-15 00:35:35,156 [INFO] Step[450/2713]: training loss : 0.9324047827720642 TRAIN  loss dict:  {'classification_loss': 0.9324047827720642}
2025-01-15 00:35:48,899 [INFO] Step[500/2713]: training loss : 0.9369193041324615 TRAIN  loss dict:  {'classification_loss': 0.9369193041324615}
2025-01-15 00:36:02,781 [INFO] Step[550/2713]: training loss : 0.9332199883460999 TRAIN  loss dict:  {'classification_loss': 0.9332199883460999}
2025-01-15 00:36:16,420 [INFO] Step[600/2713]: training loss : 0.9329398167133331 TRAIN  loss dict:  {'classification_loss': 0.9329398167133331}
2025-01-15 00:36:30,328 [INFO] Step[650/2713]: training loss : 0.9332103514671326 TRAIN  loss dict:  {'classification_loss': 0.9332103514671326}
2025-01-15 00:36:43,654 [INFO] Step[700/2713]: training loss : 0.9332325708866119 TRAIN  loss dict:  {'classification_loss': 0.9332325708866119}
2025-01-15 00:36:57,145 [INFO] Step[750/2713]: training loss : 0.9383723592758179 TRAIN  loss dict:  {'classification_loss': 0.9383723592758179}
2025-01-15 00:37:10,598 [INFO] Step[800/2713]: training loss : 0.9353575730323791 TRAIN  loss dict:  {'classification_loss': 0.9353575730323791}
2025-01-15 00:37:23,776 [INFO] Step[850/2713]: training loss : 0.9341598200798035 TRAIN  loss dict:  {'classification_loss': 0.9341598200798035}
2025-01-15 00:37:37,245 [INFO] Step[900/2713]: training loss : 0.9352309346199036 TRAIN  loss dict:  {'classification_loss': 0.9352309346199036}
2025-01-15 00:37:51,433 [INFO] Step[950/2713]: training loss : 0.9330839848518372 TRAIN  loss dict:  {'classification_loss': 0.9330839848518372}
2025-01-15 00:38:05,034 [INFO] Step[1000/2713]: training loss : 0.9324193358421325 TRAIN  loss dict:  {'classification_loss': 0.9324193358421325}
2025-01-15 00:38:18,825 [INFO] Step[1050/2713]: training loss : 0.9313241398334503 TRAIN  loss dict:  {'classification_loss': 0.9313241398334503}
2025-01-15 00:38:32,002 [INFO] Step[1100/2713]: training loss : 0.9322959113121033 TRAIN  loss dict:  {'classification_loss': 0.9322959113121033}
2025-01-15 00:38:45,527 [INFO] Step[1150/2713]: training loss : 0.9319783937931061 TRAIN  loss dict:  {'classification_loss': 0.9319783937931061}
2025-01-15 00:38:59,302 [INFO] Step[1200/2713]: training loss : 0.9328893697261811 TRAIN  loss dict:  {'classification_loss': 0.9328893697261811}
2025-01-15 00:39:13,503 [INFO] Step[1250/2713]: training loss : 0.9494438314437866 TRAIN  loss dict:  {'classification_loss': 0.9494438314437866}
2025-01-15 00:39:27,316 [INFO] Step[1300/2713]: training loss : 0.9328647303581238 TRAIN  loss dict:  {'classification_loss': 0.9328647303581238}
2025-01-15 00:39:40,794 [INFO] Step[1350/2713]: training loss : 0.9339316356182098 TRAIN  loss dict:  {'classification_loss': 0.9339316356182098}
2025-01-15 00:39:54,385 [INFO] Step[1400/2713]: training loss : 0.9356122720241546 TRAIN  loss dict:  {'classification_loss': 0.9356122720241546}
2025-01-15 00:40:08,180 [INFO] Step[1450/2713]: training loss : 0.9338982665538788 TRAIN  loss dict:  {'classification_loss': 0.9338982665538788}
2025-01-15 00:40:21,888 [INFO] Step[1500/2713]: training loss : 0.9322074615955352 TRAIN  loss dict:  {'classification_loss': 0.9322074615955352}
2025-01-15 00:40:35,054 [INFO] Step[1550/2713]: training loss : 0.9358556008338929 TRAIN  loss dict:  {'classification_loss': 0.9358556008338929}
2025-01-15 00:40:48,716 [INFO] Step[1600/2713]: training loss : 0.9332605624198913 TRAIN  loss dict:  {'classification_loss': 0.9332605624198913}
2025-01-15 00:41:02,720 [INFO] Step[1650/2713]: training loss : 0.9454701924324036 TRAIN  loss dict:  {'classification_loss': 0.9454701924324036}
2025-01-15 00:41:15,862 [INFO] Step[1700/2713]: training loss : 0.9335262703895569 TRAIN  loss dict:  {'classification_loss': 0.9335262703895569}
2025-01-15 00:41:29,451 [INFO] Step[1750/2713]: training loss : 0.9334662532806397 TRAIN  loss dict:  {'classification_loss': 0.9334662532806397}
2025-01-15 00:41:43,529 [INFO] Step[1800/2713]: training loss : 0.9320949912071228 TRAIN  loss dict:  {'classification_loss': 0.9320949912071228}
2025-01-15 00:41:56,960 [INFO] Step[1850/2713]: training loss : 0.9422943449020386 TRAIN  loss dict:  {'classification_loss': 0.9422943449020386}
2025-01-15 00:42:10,775 [INFO] Step[1900/2713]: training loss : 0.9321499466896057 TRAIN  loss dict:  {'classification_loss': 0.9321499466896057}
2025-01-15 00:42:23,970 [INFO] Step[1950/2713]: training loss : 0.9339754855632783 TRAIN  loss dict:  {'classification_loss': 0.9339754855632783}
2025-01-15 00:42:37,963 [INFO] Step[2000/2713]: training loss : 0.9468398475646973 TRAIN  loss dict:  {'classification_loss': 0.9468398475646973}
2025-01-15 00:42:51,543 [INFO] Step[2050/2713]: training loss : 0.9312141764163971 TRAIN  loss dict:  {'classification_loss': 0.9312141764163971}
2025-01-15 00:43:05,367 [INFO] Step[2100/2713]: training loss : 0.9339313185214997 TRAIN  loss dict:  {'classification_loss': 0.9339313185214997}
2025-01-15 00:43:19,085 [INFO] Step[2150/2713]: training loss : 0.934167958498001 TRAIN  loss dict:  {'classification_loss': 0.934167958498001}
2025-01-15 00:43:32,654 [INFO] Step[2200/2713]: training loss : 0.9350365328788758 TRAIN  loss dict:  {'classification_loss': 0.9350365328788758}
2025-01-15 00:43:46,489 [INFO] Step[2250/2713]: training loss : 0.9400488257408142 TRAIN  loss dict:  {'classification_loss': 0.9400488257408142}
2025-01-15 00:43:59,824 [INFO] Step[2300/2713]: training loss : 0.9329532480239868 TRAIN  loss dict:  {'classification_loss': 0.9329532480239868}
2025-01-15 00:44:13,682 [INFO] Step[2350/2713]: training loss : 0.932703902721405 TRAIN  loss dict:  {'classification_loss': 0.932703902721405}
2025-01-15 00:44:27,057 [INFO] Step[2400/2713]: training loss : 0.9345574164390564 TRAIN  loss dict:  {'classification_loss': 0.9345574164390564}
2025-01-15 00:44:40,279 [INFO] Step[2450/2713]: training loss : 0.9334182012081146 TRAIN  loss dict:  {'classification_loss': 0.9334182012081146}
2025-01-15 00:44:54,107 [INFO] Step[2500/2713]: training loss : 0.9322431707382202 TRAIN  loss dict:  {'classification_loss': 0.9322431707382202}
2025-01-15 00:45:08,000 [INFO] Step[2550/2713]: training loss : 0.934244692325592 TRAIN  loss dict:  {'classification_loss': 0.934244692325592}
2025-01-15 00:45:22,534 [INFO] Step[2600/2713]: training loss : 0.9330903649330139 TRAIN  loss dict:  {'classification_loss': 0.9330903649330139}
2025-01-15 00:45:38,399 [INFO] Step[2650/2713]: training loss : 0.9556428635120392 TRAIN  loss dict:  {'classification_loss': 0.9556428635120392}
2025-01-15 00:45:52,140 [INFO] Step[2700/2713]: training loss : 0.9331150496006012 TRAIN  loss dict:  {'classification_loss': 0.9331150496006012}
2025-01-15 00:47:08,794 [INFO] Label accuracies statistics:
2025-01-15 00:47:08,794 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.0, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 00:47:08,796 [INFO] [58] TRAIN  loss: 0.934966105093079 acc: 0.9993856739157144
2025-01-15 00:47:08,796 [INFO] [58] TRAIN  loss dict: {'classification_loss': 0.934966105093079}
2025-01-15 00:47:08,796 [INFO] [58] VALIDATION loss: 1.797864596086337 VALIDATION acc: 0.806269592476489
2025-01-15 00:47:08,796 [INFO] [58] VALIDATION loss dict: {'classification_loss': 1.797864596086337}
2025-01-15 00:47:08,796 [INFO] 
2025-01-15 00:47:36,586 [INFO] Step[50/2713]: training loss : 0.9317317736148835 TRAIN  loss dict:  {'classification_loss': 0.9317317736148835}
2025-01-15 00:47:50,610 [INFO] Step[100/2713]: training loss : 0.9416874253749847 TRAIN  loss dict:  {'classification_loss': 0.9416874253749847}
2025-01-15 00:48:04,536 [INFO] Step[150/2713]: training loss : 0.9313979184627533 TRAIN  loss dict:  {'classification_loss': 0.9313979184627533}
2025-01-15 00:48:18,599 [INFO] Step[200/2713]: training loss : 0.9316833364963532 TRAIN  loss dict:  {'classification_loss': 0.9316833364963532}
2025-01-15 00:48:31,948 [INFO] Step[250/2713]: training loss : 0.9323695647716522 TRAIN  loss dict:  {'classification_loss': 0.9323695647716522}
2025-01-15 00:48:45,749 [INFO] Step[300/2713]: training loss : 0.9334641015529632 TRAIN  loss dict:  {'classification_loss': 0.9334641015529632}
2025-01-15 00:48:59,174 [INFO] Step[350/2713]: training loss : 0.932715185880661 TRAIN  loss dict:  {'classification_loss': 0.932715185880661}
2025-01-15 00:49:12,350 [INFO] Step[400/2713]: training loss : 0.9360431015491486 TRAIN  loss dict:  {'classification_loss': 0.9360431015491486}
2025-01-15 00:49:26,229 [INFO] Step[450/2713]: training loss : 0.9324195563793183 TRAIN  loss dict:  {'classification_loss': 0.9324195563793183}
2025-01-15 00:49:39,553 [INFO] Step[500/2713]: training loss : 0.9342384397983551 TRAIN  loss dict:  {'classification_loss': 0.9342384397983551}
2025-01-15 00:49:53,243 [INFO] Step[550/2713]: training loss : 0.932625812292099 TRAIN  loss dict:  {'classification_loss': 0.932625812292099}
2025-01-15 00:50:07,370 [INFO] Step[600/2713]: training loss : 0.9360455167293549 TRAIN  loss dict:  {'classification_loss': 0.9360455167293549}
2025-01-15 00:50:20,692 [INFO] Step[650/2713]: training loss : 0.9354263663291931 TRAIN  loss dict:  {'classification_loss': 0.9354263663291931}
2025-01-15 00:50:34,132 [INFO] Step[700/2713]: training loss : 0.9334485149383545 TRAIN  loss dict:  {'classification_loss': 0.9334485149383545}
2025-01-15 00:50:47,793 [INFO] Step[750/2713]: training loss : 0.9318215751647949 TRAIN  loss dict:  {'classification_loss': 0.9318215751647949}
2025-01-15 00:51:01,283 [INFO] Step[800/2713]: training loss : 0.9334815275669098 TRAIN  loss dict:  {'classification_loss': 0.9334815275669098}
2025-01-15 00:51:14,801 [INFO] Step[850/2713]: training loss : 0.9327090322971344 TRAIN  loss dict:  {'classification_loss': 0.9327090322971344}
2025-01-15 00:51:28,414 [INFO] Step[900/2713]: training loss : 0.9325163018703461 TRAIN  loss dict:  {'classification_loss': 0.9325163018703461}
2025-01-15 00:51:42,332 [INFO] Step[950/2713]: training loss : 0.9325670778751374 TRAIN  loss dict:  {'classification_loss': 0.9325670778751374}
2025-01-15 00:51:56,465 [INFO] Step[1000/2713]: training loss : 0.962043787240982 TRAIN  loss dict:  {'classification_loss': 0.962043787240982}
2025-01-15 00:52:09,825 [INFO] Step[1050/2713]: training loss : 0.9329527032375335 TRAIN  loss dict:  {'classification_loss': 0.9329527032375335}
2025-01-15 00:52:22,835 [INFO] Step[1100/2713]: training loss : 0.9358859431743621 TRAIN  loss dict:  {'classification_loss': 0.9358859431743621}
2025-01-15 00:52:36,300 [INFO] Step[1150/2713]: training loss : 0.9696661126613617 TRAIN  loss dict:  {'classification_loss': 0.9696661126613617}
2025-01-15 00:52:50,027 [INFO] Step[1200/2713]: training loss : 0.9313592076301574 TRAIN  loss dict:  {'classification_loss': 0.9313592076301574}
2025-01-15 00:53:03,514 [INFO] Step[1250/2713]: training loss : 0.9372004449367524 TRAIN  loss dict:  {'classification_loss': 0.9372004449367524}
2025-01-15 00:53:16,814 [INFO] Step[1300/2713]: training loss : 0.9378662133216857 TRAIN  loss dict:  {'classification_loss': 0.9378662133216857}
2025-01-15 00:53:30,330 [INFO] Step[1350/2713]: training loss : 0.9342147696018219 TRAIN  loss dict:  {'classification_loss': 0.9342147696018219}
2025-01-15 00:53:43,643 [INFO] Step[1400/2713]: training loss : 0.9341840612888336 TRAIN  loss dict:  {'classification_loss': 0.9341840612888336}
2025-01-15 00:53:56,846 [INFO] Step[1450/2713]: training loss : 0.9314670443534852 TRAIN  loss dict:  {'classification_loss': 0.9314670443534852}
2025-01-15 00:54:10,794 [INFO] Step[1500/2713]: training loss : 0.9352434921264648 TRAIN  loss dict:  {'classification_loss': 0.9352434921264648}
2025-01-15 00:54:23,982 [INFO] Step[1550/2713]: training loss : 0.9516483509540558 TRAIN  loss dict:  {'classification_loss': 0.9516483509540558}
2025-01-15 00:54:37,182 [INFO] Step[1600/2713]: training loss : 0.9347753667831421 TRAIN  loss dict:  {'classification_loss': 0.9347753667831421}
2025-01-15 00:54:51,224 [INFO] Step[1650/2713]: training loss : 0.9318499112129212 TRAIN  loss dict:  {'classification_loss': 0.9318499112129212}
2025-01-15 00:55:04,609 [INFO] Step[1700/2713]: training loss : 0.9323166131973266 TRAIN  loss dict:  {'classification_loss': 0.9323166131973266}
2025-01-15 00:55:17,810 [INFO] Step[1750/2713]: training loss : 0.9322616350650788 TRAIN  loss dict:  {'classification_loss': 0.9322616350650788}
2025-01-15 00:55:31,611 [INFO] Step[1800/2713]: training loss : 0.9341415393352509 TRAIN  loss dict:  {'classification_loss': 0.9341415393352509}
2025-01-15 00:55:45,266 [INFO] Step[1850/2713]: training loss : 0.9320716214179993 TRAIN  loss dict:  {'classification_loss': 0.9320716214179993}
2025-01-15 00:55:59,180 [INFO] Step[1900/2713]: training loss : 0.937512389421463 TRAIN  loss dict:  {'classification_loss': 0.937512389421463}
2025-01-15 00:56:12,852 [INFO] Step[1950/2713]: training loss : 0.9325945723056793 TRAIN  loss dict:  {'classification_loss': 0.9325945723056793}
2025-01-15 00:56:26,618 [INFO] Step[2000/2713]: training loss : 0.958750706911087 TRAIN  loss dict:  {'classification_loss': 0.958750706911087}
2025-01-15 00:56:40,826 [INFO] Step[2050/2713]: training loss : 0.9325527369976043 TRAIN  loss dict:  {'classification_loss': 0.9325527369976043}
2025-01-15 00:56:54,157 [INFO] Step[2100/2713]: training loss : 0.9327769231796265 TRAIN  loss dict:  {'classification_loss': 0.9327769231796265}
2025-01-15 00:57:08,228 [INFO] Step[2150/2713]: training loss : 0.9314099335670472 TRAIN  loss dict:  {'classification_loss': 0.9314099335670472}
2025-01-15 00:57:22,223 [INFO] Step[2200/2713]: training loss : 0.9458592736721039 TRAIN  loss dict:  {'classification_loss': 0.9458592736721039}
2025-01-15 00:57:36,185 [INFO] Step[2250/2713]: training loss : 0.9337715709209442 TRAIN  loss dict:  {'classification_loss': 0.9337715709209442}
2025-01-15 00:57:49,816 [INFO] Step[2300/2713]: training loss : 0.9342179143428803 TRAIN  loss dict:  {'classification_loss': 0.9342179143428803}
2025-01-15 00:58:03,369 [INFO] Step[2350/2713]: training loss : 0.9330561077594757 TRAIN  loss dict:  {'classification_loss': 0.9330561077594757}
2025-01-15 00:58:17,429 [INFO] Step[2400/2713]: training loss : 0.9314626741409302 TRAIN  loss dict:  {'classification_loss': 0.9314626741409302}
2025-01-15 00:58:30,913 [INFO] Step[2450/2713]: training loss : 0.9324355351924897 TRAIN  loss dict:  {'classification_loss': 0.9324355351924897}
2025-01-15 00:58:44,204 [INFO] Step[2500/2713]: training loss : 0.9346140670776367 TRAIN  loss dict:  {'classification_loss': 0.9346140670776367}
2025-01-15 00:58:57,909 [INFO] Step[2550/2713]: training loss : 0.9327555441856384 TRAIN  loss dict:  {'classification_loss': 0.9327555441856384}
2025-01-15 00:59:11,719 [INFO] Step[2600/2713]: training loss : 0.9316273272037506 TRAIN  loss dict:  {'classification_loss': 0.9316273272037506}
2025-01-15 00:59:25,184 [INFO] Step[2650/2713]: training loss : 0.9334844052791595 TRAIN  loss dict:  {'classification_loss': 0.9334844052791595}
2025-01-15 00:59:39,279 [INFO] Step[2700/2713]: training loss : 0.9353692674636841 TRAIN  loss dict:  {'classification_loss': 0.9353692674636841}
2025-01-15 01:00:58,035 [INFO] Label accuracies statistics:
2025-01-15 01:00:58,035 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.0, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.75, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.5, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 0.75, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 01:00:58,037 [INFO] [59] TRAIN  loss: 0.9357990753242023 acc: 0.9993856739157144
2025-01-15 01:00:58,037 [INFO] [59] TRAIN  loss dict: {'classification_loss': 0.9357990753242023}
2025-01-15 01:00:58,037 [INFO] [59] VALIDATION loss: 1.791048932120316 VALIDATION acc: 0.8018808777429467
2025-01-15 01:00:58,037 [INFO] [59] VALIDATION loss dict: {'classification_loss': 1.791048932120316}
2025-01-15 01:00:58,037 [INFO] 
2025-01-15 01:01:31,631 [INFO] Step[50/2713]: training loss : 0.93197683095932 TRAIN  loss dict:  {'classification_loss': 0.93197683095932}
2025-01-15 01:01:44,860 [INFO] Step[100/2713]: training loss : 0.9370276904106141 TRAIN  loss dict:  {'classification_loss': 0.9370276904106141}
2025-01-15 01:01:59,058 [INFO] Step[150/2713]: training loss : 0.9320568442344666 TRAIN  loss dict:  {'classification_loss': 0.9320568442344666}
2025-01-15 01:02:13,394 [INFO] Step[200/2713]: training loss : 0.9319155204296112 TRAIN  loss dict:  {'classification_loss': 0.9319155204296112}
2025-01-15 01:02:27,286 [INFO] Step[250/2713]: training loss : 0.9431895470619202 TRAIN  loss dict:  {'classification_loss': 0.9431895470619202}
2025-01-15 01:02:41,335 [INFO] Step[300/2713]: training loss : 0.954271113872528 TRAIN  loss dict:  {'classification_loss': 0.954271113872528}
2025-01-15 01:02:54,665 [INFO] Step[350/2713]: training loss : 0.9371265268325806 TRAIN  loss dict:  {'classification_loss': 0.9371265268325806}
2025-01-15 01:03:07,900 [INFO] Step[400/2713]: training loss : 0.9327010238170623 TRAIN  loss dict:  {'classification_loss': 0.9327010238170623}
2025-01-15 01:03:21,131 [INFO] Step[450/2713]: training loss : 0.9335400998592377 TRAIN  loss dict:  {'classification_loss': 0.9335400998592377}
2025-01-15 01:03:34,749 [INFO] Step[500/2713]: training loss : 0.93279754281044 TRAIN  loss dict:  {'classification_loss': 0.93279754281044}
2025-01-15 01:03:48,610 [INFO] Step[550/2713]: training loss : 0.9340688145160675 TRAIN  loss dict:  {'classification_loss': 0.9340688145160675}
2025-01-15 01:04:02,254 [INFO] Step[600/2713]: training loss : 0.9325399827957154 TRAIN  loss dict:  {'classification_loss': 0.9325399827957154}
2025-01-15 01:04:15,967 [INFO] Step[650/2713]: training loss : 0.9319714498519898 TRAIN  loss dict:  {'classification_loss': 0.9319714498519898}
2025-01-15 01:04:29,696 [INFO] Step[700/2713]: training loss : 0.9315445268154144 TRAIN  loss dict:  {'classification_loss': 0.9315445268154144}
2025-01-15 01:04:43,387 [INFO] Step[750/2713]: training loss : 0.961941112279892 TRAIN  loss dict:  {'classification_loss': 0.961941112279892}
2025-01-15 01:04:56,727 [INFO] Step[800/2713]: training loss : 0.9336966502666474 TRAIN  loss dict:  {'classification_loss': 0.9336966502666474}
2025-01-15 01:05:10,136 [INFO] Step[850/2713]: training loss : 0.9357227766513825 TRAIN  loss dict:  {'classification_loss': 0.9357227766513825}
2025-01-15 01:05:23,556 [INFO] Step[900/2713]: training loss : 0.9312448120117187 TRAIN  loss dict:  {'classification_loss': 0.9312448120117187}
2025-01-15 01:05:37,198 [INFO] Step[950/2713]: training loss : 0.9326417708396911 TRAIN  loss dict:  {'classification_loss': 0.9326417708396911}
2025-01-15 01:05:51,089 [INFO] Step[1000/2713]: training loss : 0.9317308151721955 TRAIN  loss dict:  {'classification_loss': 0.9317308151721955}
2025-01-15 01:06:05,021 [INFO] Step[1050/2713]: training loss : 0.9366557216644287 TRAIN  loss dict:  {'classification_loss': 0.9366557216644287}
2025-01-15 01:06:18,554 [INFO] Step[1100/2713]: training loss : 0.9528362262248993 TRAIN  loss dict:  {'classification_loss': 0.9528362262248993}
2025-01-15 01:06:31,942 [INFO] Step[1150/2713]: training loss : 0.9312858545780182 TRAIN  loss dict:  {'classification_loss': 0.9312858545780182}
2025-01-15 01:06:45,397 [INFO] Step[1200/2713]: training loss : 0.9414438486099244 TRAIN  loss dict:  {'classification_loss': 0.9414438486099244}
2025-01-15 01:06:58,704 [INFO] Step[1250/2713]: training loss : 0.9300784277915954 TRAIN  loss dict:  {'classification_loss': 0.9300784277915954}
2025-01-15 01:07:12,462 [INFO] Step[1300/2713]: training loss : 0.9336695635318756 TRAIN  loss dict:  {'classification_loss': 0.9336695635318756}
2025-01-15 01:07:26,333 [INFO] Step[1350/2713]: training loss : 0.9366791570186614 TRAIN  loss dict:  {'classification_loss': 0.9366791570186614}
2025-01-15 01:07:40,205 [INFO] Step[1400/2713]: training loss : 0.9465213632583618 TRAIN  loss dict:  {'classification_loss': 0.9465213632583618}
2025-01-15 01:07:54,374 [INFO] Step[1450/2713]: training loss : 0.9316298544406891 TRAIN  loss dict:  {'classification_loss': 0.9316298544406891}
2025-01-15 01:08:07,880 [INFO] Step[1500/2713]: training loss : 0.9303925955295562 TRAIN  loss dict:  {'classification_loss': 0.9303925955295562}
2025-01-15 01:08:21,847 [INFO] Step[1550/2713]: training loss : 0.941364575624466 TRAIN  loss dict:  {'classification_loss': 0.941364575624466}
2025-01-15 01:08:35,647 [INFO] Step[1600/2713]: training loss : 0.9423345804214478 TRAIN  loss dict:  {'classification_loss': 0.9423345804214478}
2025-01-15 01:08:49,377 [INFO] Step[1650/2713]: training loss : 0.9467605531215668 TRAIN  loss dict:  {'classification_loss': 0.9467605531215668}
2025-01-15 01:09:02,584 [INFO] Step[1700/2713]: training loss : 0.9315433132648469 TRAIN  loss dict:  {'classification_loss': 0.9315433132648469}
2025-01-15 01:09:16,310 [INFO] Step[1750/2713]: training loss : 0.9326599597930908 TRAIN  loss dict:  {'classification_loss': 0.9326599597930908}
2025-01-15 01:09:29,887 [INFO] Step[1800/2713]: training loss : 0.9325544095039368 TRAIN  loss dict:  {'classification_loss': 0.9325544095039368}
2025-01-15 01:09:43,845 [INFO] Step[1850/2713]: training loss : 0.9418100965023041 TRAIN  loss dict:  {'classification_loss': 0.9418100965023041}
2025-01-15 01:09:57,014 [INFO] Step[1900/2713]: training loss : 0.9329486286640167 TRAIN  loss dict:  {'classification_loss': 0.9329486286640167}
2025-01-15 01:10:10,955 [INFO] Step[1950/2713]: training loss : 0.9319328427314758 TRAIN  loss dict:  {'classification_loss': 0.9319328427314758}
2025-01-15 01:10:24,691 [INFO] Step[2000/2713]: training loss : 0.9333664977550507 TRAIN  loss dict:  {'classification_loss': 0.9333664977550507}
2025-01-15 01:10:38,061 [INFO] Step[2050/2713]: training loss : 0.9324079191684723 TRAIN  loss dict:  {'classification_loss': 0.9324079191684723}
2025-01-15 01:10:51,763 [INFO] Step[2100/2713]: training loss : 0.9478289234638214 TRAIN  loss dict:  {'classification_loss': 0.9478289234638214}
2025-01-15 01:11:05,008 [INFO] Step[2150/2713]: training loss : 0.9319512617588043 TRAIN  loss dict:  {'classification_loss': 0.9319512617588043}
2025-01-15 01:11:18,400 [INFO] Step[2200/2713]: training loss : 0.949355251789093 TRAIN  loss dict:  {'classification_loss': 0.949355251789093}
2025-01-15 01:11:32,050 [INFO] Step[2250/2713]: training loss : 0.9345089590549469 TRAIN  loss dict:  {'classification_loss': 0.9345089590549469}
2025-01-15 01:11:45,611 [INFO] Step[2300/2713]: training loss : 0.9354493033885956 TRAIN  loss dict:  {'classification_loss': 0.9354493033885956}
2025-01-15 01:11:59,157 [INFO] Step[2350/2713]: training loss : 0.9382347655296326 TRAIN  loss dict:  {'classification_loss': 0.9382347655296326}
2025-01-15 01:12:12,309 [INFO] Step[2400/2713]: training loss : 0.9339775848388672 TRAIN  loss dict:  {'classification_loss': 0.9339775848388672}
2025-01-15 01:12:25,839 [INFO] Step[2450/2713]: training loss : 0.9337725639343262 TRAIN  loss dict:  {'classification_loss': 0.9337725639343262}
2025-01-15 01:12:39,036 [INFO] Step[2500/2713]: training loss : 0.9333168804645539 TRAIN  loss dict:  {'classification_loss': 0.9333168804645539}
2025-01-15 01:12:52,285 [INFO] Step[2550/2713]: training loss : 0.9455724155902863 TRAIN  loss dict:  {'classification_loss': 0.9455724155902863}
2025-01-15 01:13:05,606 [INFO] Step[2600/2713]: training loss : 0.9351093173027039 TRAIN  loss dict:  {'classification_loss': 0.9351093173027039}
2025-01-15 01:13:19,602 [INFO] Step[2650/2713]: training loss : 0.9330906105041504 TRAIN  loss dict:  {'classification_loss': 0.9330906105041504}
2025-01-15 01:13:33,160 [INFO] Step[2700/2713]: training loss : 0.9335572707653046 TRAIN  loss dict:  {'classification_loss': 0.9335572707653046}
2025-01-15 01:14:50,900 [INFO] Label accuracies statistics:
2025-01-15 01:14:50,900 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.25, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.5, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 01:14:50,902 [INFO] [60] TRAIN  loss: 0.9366426992100013 acc: 0.9985256173977147
2025-01-15 01:14:50,902 [INFO] [60] TRAIN  loss dict: {'classification_loss': 0.9366426992100013}
2025-01-15 01:14:50,902 [INFO] [60] VALIDATION loss: 1.8166693609235878 VALIDATION acc: 0.7924764890282132
2025-01-15 01:14:50,902 [INFO] [60] VALIDATION loss dict: {'classification_loss': 1.8166693609235878}
2025-01-15 01:14:50,902 [INFO] 
2025-01-15 01:15:29,773 [INFO] Step[50/2713]: training loss : 0.9332281458377838 TRAIN  loss dict:  {'classification_loss': 0.9332281458377838}
2025-01-15 01:15:43,097 [INFO] Step[100/2713]: training loss : 0.9328875386714935 TRAIN  loss dict:  {'classification_loss': 0.9328875386714935}
2025-01-15 01:15:56,348 [INFO] Step[150/2713]: training loss : 0.9342776513099671 TRAIN  loss dict:  {'classification_loss': 0.9342776513099671}
2025-01-15 01:16:10,418 [INFO] Step[200/2713]: training loss : 0.9306016111373902 TRAIN  loss dict:  {'classification_loss': 0.9306016111373902}
2025-01-15 01:16:24,451 [INFO] Step[250/2713]: training loss : 0.9395230555534363 TRAIN  loss dict:  {'classification_loss': 0.9395230555534363}
2025-01-15 01:16:37,975 [INFO] Step[300/2713]: training loss : 0.9295000743865967 TRAIN  loss dict:  {'classification_loss': 0.9295000743865967}
2025-01-15 01:16:52,121 [INFO] Step[350/2713]: training loss : 0.9336336743831635 TRAIN  loss dict:  {'classification_loss': 0.9336336743831635}
2025-01-15 01:17:05,681 [INFO] Step[400/2713]: training loss : 0.932370924949646 TRAIN  loss dict:  {'classification_loss': 0.932370924949646}
2025-01-15 01:17:18,882 [INFO] Step[450/2713]: training loss : 0.9357352483272553 TRAIN  loss dict:  {'classification_loss': 0.9357352483272553}
2025-01-15 01:17:33,034 [INFO] Step[500/2713]: training loss : 0.9307239997386932 TRAIN  loss dict:  {'classification_loss': 0.9307239997386932}
2025-01-15 01:17:46,883 [INFO] Step[550/2713]: training loss : 0.9316154479980469 TRAIN  loss dict:  {'classification_loss': 0.9316154479980469}
2025-01-15 01:18:00,269 [INFO] Step[600/2713]: training loss : 0.9305590462684631 TRAIN  loss dict:  {'classification_loss': 0.9305590462684631}
2025-01-15 01:18:14,201 [INFO] Step[650/2713]: training loss : 0.9308861827850342 TRAIN  loss dict:  {'classification_loss': 0.9308861827850342}
2025-01-15 01:18:27,422 [INFO] Step[700/2713]: training loss : 0.9311378180980683 TRAIN  loss dict:  {'classification_loss': 0.9311378180980683}
2025-01-15 01:18:41,015 [INFO] Step[750/2713]: training loss : 0.9327508306503296 TRAIN  loss dict:  {'classification_loss': 0.9327508306503296}
2025-01-15 01:18:54,511 [INFO] Step[800/2713]: training loss : 0.9309871435165405 TRAIN  loss dict:  {'classification_loss': 0.9309871435165405}
2025-01-15 01:19:08,000 [INFO] Step[850/2713]: training loss : 0.9351371121406555 TRAIN  loss dict:  {'classification_loss': 0.9351371121406555}
2025-01-15 01:19:21,411 [INFO] Step[900/2713]: training loss : 0.932354930639267 TRAIN  loss dict:  {'classification_loss': 0.932354930639267}
2025-01-15 01:19:34,731 [INFO] Step[950/2713]: training loss : 0.9316536939144134 TRAIN  loss dict:  {'classification_loss': 0.9316536939144134}
2025-01-15 01:19:48,305 [INFO] Step[1000/2713]: training loss : 0.9311209070682526 TRAIN  loss dict:  {'classification_loss': 0.9311209070682526}
2025-01-15 01:20:02,161 [INFO] Step[1050/2713]: training loss : 0.9305298590660095 TRAIN  loss dict:  {'classification_loss': 0.9305298590660095}
2025-01-15 01:20:16,423 [INFO] Step[1100/2713]: training loss : 0.9402229714393616 TRAIN  loss dict:  {'classification_loss': 0.9402229714393616}
2025-01-15 01:20:30,206 [INFO] Step[1150/2713]: training loss : 0.9352077877521515 TRAIN  loss dict:  {'classification_loss': 0.9352077877521515}
2025-01-15 01:20:43,782 [INFO] Step[1200/2713]: training loss : 0.9310367858409881 TRAIN  loss dict:  {'classification_loss': 0.9310367858409881}
2025-01-15 01:20:57,707 [INFO] Step[1250/2713]: training loss : 0.9311954450607299 TRAIN  loss dict:  {'classification_loss': 0.9311954450607299}
2025-01-15 01:21:11,541 [INFO] Step[1300/2713]: training loss : 0.9312959110736847 TRAIN  loss dict:  {'classification_loss': 0.9312959110736847}
2025-01-15 01:21:25,451 [INFO] Step[1350/2713]: training loss : 0.9302232193946839 TRAIN  loss dict:  {'classification_loss': 0.9302232193946839}
2025-01-15 01:21:39,417 [INFO] Step[1400/2713]: training loss : 0.9318648886680603 TRAIN  loss dict:  {'classification_loss': 0.9318648886680603}
2025-01-15 01:21:53,466 [INFO] Step[1450/2713]: training loss : 0.9318155992031097 TRAIN  loss dict:  {'classification_loss': 0.9318155992031097}
2025-01-15 01:22:07,163 [INFO] Step[1500/2713]: training loss : 0.9318229305744171 TRAIN  loss dict:  {'classification_loss': 0.9318229305744171}
2025-01-15 01:22:20,988 [INFO] Step[1550/2713]: training loss : 0.9317024195194245 TRAIN  loss dict:  {'classification_loss': 0.9317024195194245}
2025-01-15 01:22:34,678 [INFO] Step[1600/2713]: training loss : 0.9332372450828552 TRAIN  loss dict:  {'classification_loss': 0.9332372450828552}
2025-01-15 01:22:48,936 [INFO] Step[1650/2713]: training loss : 0.9309540855884552 TRAIN  loss dict:  {'classification_loss': 0.9309540855884552}
2025-01-15 01:23:03,129 [INFO] Step[1700/2713]: training loss : 0.9315091705322266 TRAIN  loss dict:  {'classification_loss': 0.9315091705322266}
2025-01-15 01:23:17,333 [INFO] Step[1750/2713]: training loss : 0.9593930220603943 TRAIN  loss dict:  {'classification_loss': 0.9593930220603943}
2025-01-15 01:23:31,026 [INFO] Step[1800/2713]: training loss : 0.9314169812202454 TRAIN  loss dict:  {'classification_loss': 0.9314169812202454}
2025-01-15 01:23:44,834 [INFO] Step[1850/2713]: training loss : 0.9438304197788239 TRAIN  loss dict:  {'classification_loss': 0.9438304197788239}
2025-01-15 01:23:58,739 [INFO] Step[1900/2713]: training loss : 0.9299410939216614 TRAIN  loss dict:  {'classification_loss': 0.9299410939216614}
2025-01-15 01:24:11,965 [INFO] Step[1950/2713]: training loss : 0.9317674720287323 TRAIN  loss dict:  {'classification_loss': 0.9317674720287323}
2025-01-15 01:24:25,522 [INFO] Step[2000/2713]: training loss : 0.9306950509548187 TRAIN  loss dict:  {'classification_loss': 0.9306950509548187}
2025-01-15 01:24:39,213 [INFO] Step[2050/2713]: training loss : 0.9316883039474487 TRAIN  loss dict:  {'classification_loss': 0.9316883039474487}
2025-01-15 01:24:52,760 [INFO] Step[2100/2713]: training loss : 0.935663241147995 TRAIN  loss dict:  {'classification_loss': 0.935663241147995}
2025-01-15 01:25:06,487 [INFO] Step[2150/2713]: training loss : 0.9345507895946503 TRAIN  loss dict:  {'classification_loss': 0.9345507895946503}
2025-01-15 01:25:20,260 [INFO] Step[2200/2713]: training loss : 0.9324152994155884 TRAIN  loss dict:  {'classification_loss': 0.9324152994155884}
2025-01-15 01:25:34,231 [INFO] Step[2250/2713]: training loss : 0.9307552015781403 TRAIN  loss dict:  {'classification_loss': 0.9307552015781403}
2025-01-15 01:25:47,872 [INFO] Step[2300/2713]: training loss : 0.9585508096218109 TRAIN  loss dict:  {'classification_loss': 0.9585508096218109}
2025-01-15 01:26:01,906 [INFO] Step[2350/2713]: training loss : 0.9318712091445923 TRAIN  loss dict:  {'classification_loss': 0.9318712091445923}
2025-01-15 01:26:15,894 [INFO] Step[2400/2713]: training loss : 0.9593279433250427 TRAIN  loss dict:  {'classification_loss': 0.9593279433250427}
2025-01-15 01:26:29,535 [INFO] Step[2450/2713]: training loss : 0.9305706119537354 TRAIN  loss dict:  {'classification_loss': 0.9305706119537354}
2025-01-15 01:26:43,357 [INFO] Step[2500/2713]: training loss : 0.9312497353553773 TRAIN  loss dict:  {'classification_loss': 0.9312497353553773}
2025-01-15 01:26:56,590 [INFO] Step[2550/2713]: training loss : 0.9325917112827301 TRAIN  loss dict:  {'classification_loss': 0.9325917112827301}
2025-01-15 01:27:09,761 [INFO] Step[2600/2713]: training loss : 0.9299777603149414 TRAIN  loss dict:  {'classification_loss': 0.9299777603149414}
2025-01-15 01:27:23,259 [INFO] Step[2650/2713]: training loss : 0.9307128834724426 TRAIN  loss dict:  {'classification_loss': 0.9307128834724426}
2025-01-15 01:27:37,018 [INFO] Step[2700/2713]: training loss : 0.933486340045929 TRAIN  loss dict:  {'classification_loss': 0.933486340045929}
2025-01-15 01:28:53,314 [INFO] Label accuracies statistics:
2025-01-15 01:28:53,314 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.25, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 01:28:53,316 [INFO] [61] TRAIN  loss: 0.9339586431761312 acc: 0.9995085391325715
2025-01-15 01:28:53,316 [INFO] [61] TRAIN  loss dict: {'classification_loss': 0.9339586431761312}
2025-01-15 01:28:53,316 [INFO] [61] VALIDATION loss: 1.8012474999391943 VALIDATION acc: 0.799373040752351
2025-01-15 01:28:53,316 [INFO] [61] VALIDATION loss dict: {'classification_loss': 1.8012474999391943}
2025-01-15 01:28:53,316 [INFO] 
2025-01-15 01:29:19,606 [INFO] Step[50/2713]: training loss : 0.9351892971992493 TRAIN  loss dict:  {'classification_loss': 0.9351892971992493}
2025-01-15 01:29:33,478 [INFO] Step[100/2713]: training loss : 0.9343332421779632 TRAIN  loss dict:  {'classification_loss': 0.9343332421779632}
2025-01-15 01:29:47,097 [INFO] Step[150/2713]: training loss : 0.9320305454730987 TRAIN  loss dict:  {'classification_loss': 0.9320305454730987}
2025-01-15 01:30:00,708 [INFO] Step[200/2713]: training loss : 0.9319402575492859 TRAIN  loss dict:  {'classification_loss': 0.9319402575492859}
2025-01-15 01:30:14,428 [INFO] Step[250/2713]: training loss : 0.9308796620368958 TRAIN  loss dict:  {'classification_loss': 0.9308796620368958}
2025-01-15 01:30:27,954 [INFO] Step[300/2713]: training loss : 0.939988659620285 TRAIN  loss dict:  {'classification_loss': 0.939988659620285}
2025-01-15 01:30:41,951 [INFO] Step[350/2713]: training loss : 0.9316469454765319 TRAIN  loss dict:  {'classification_loss': 0.9316469454765319}
2025-01-15 01:30:55,628 [INFO] Step[400/2713]: training loss : 0.9317713534832001 TRAIN  loss dict:  {'classification_loss': 0.9317713534832001}
2025-01-15 01:31:08,802 [INFO] Step[450/2713]: training loss : 0.9305973935127259 TRAIN  loss dict:  {'classification_loss': 0.9305973935127259}
2025-01-15 01:31:22,702 [INFO] Step[500/2713]: training loss : 0.9315589463710785 TRAIN  loss dict:  {'classification_loss': 0.9315589463710785}
2025-01-15 01:31:36,373 [INFO] Step[550/2713]: training loss : 0.930075716972351 TRAIN  loss dict:  {'classification_loss': 0.930075716972351}
2025-01-15 01:31:50,245 [INFO] Step[600/2713]: training loss : 0.9310299801826477 TRAIN  loss dict:  {'classification_loss': 0.9310299801826477}
2025-01-15 01:32:03,480 [INFO] Step[650/2713]: training loss : 0.9303054523468017 TRAIN  loss dict:  {'classification_loss': 0.9303054523468017}
2025-01-15 01:32:16,683 [INFO] Step[700/2713]: training loss : 0.9302275037765503 TRAIN  loss dict:  {'classification_loss': 0.9302275037765503}
2025-01-15 01:32:30,464 [INFO] Step[750/2713]: training loss : 0.9298646235466004 TRAIN  loss dict:  {'classification_loss': 0.9298646235466004}
2025-01-15 01:32:44,136 [INFO] Step[800/2713]: training loss : 0.931764862537384 TRAIN  loss dict:  {'classification_loss': 0.931764862537384}
2025-01-15 01:32:58,300 [INFO] Step[850/2713]: training loss : 0.9303760492801666 TRAIN  loss dict:  {'classification_loss': 0.9303760492801666}
2025-01-15 01:33:11,992 [INFO] Step[900/2713]: training loss : 0.9296972358226776 TRAIN  loss dict:  {'classification_loss': 0.9296972358226776}
2025-01-15 01:33:25,353 [INFO] Step[950/2713]: training loss : 0.9402824091911316 TRAIN  loss dict:  {'classification_loss': 0.9402824091911316}
2025-01-15 01:33:38,953 [INFO] Step[1000/2713]: training loss : 0.9353042447566986 TRAIN  loss dict:  {'classification_loss': 0.9353042447566986}
2025-01-15 01:33:52,136 [INFO] Step[1050/2713]: training loss : 0.9308131086826325 TRAIN  loss dict:  {'classification_loss': 0.9308131086826325}
2025-01-15 01:34:05,819 [INFO] Step[1100/2713]: training loss : 0.9318057632446289 TRAIN  loss dict:  {'classification_loss': 0.9318057632446289}
2025-01-15 01:34:19,728 [INFO] Step[1150/2713]: training loss : 0.9303504025936127 TRAIN  loss dict:  {'classification_loss': 0.9303504025936127}
2025-01-15 01:34:33,513 [INFO] Step[1200/2713]: training loss : 0.9322621905803681 TRAIN  loss dict:  {'classification_loss': 0.9322621905803681}
2025-01-15 01:34:47,141 [INFO] Step[1250/2713]: training loss : 0.9308897686004639 TRAIN  loss dict:  {'classification_loss': 0.9308897686004639}
2025-01-15 01:35:01,719 [INFO] Step[1300/2713]: training loss : 0.9317129075527191 TRAIN  loss dict:  {'classification_loss': 0.9317129075527191}
2025-01-15 01:35:18,125 [INFO] Step[1350/2713]: training loss : 0.9309438502788544 TRAIN  loss dict:  {'classification_loss': 0.9309438502788544}
2025-01-15 01:35:32,242 [INFO] Step[1400/2713]: training loss : 0.9395752716064453 TRAIN  loss dict:  {'classification_loss': 0.9395752716064453}
2025-01-15 01:35:45,714 [INFO] Step[1450/2713]: training loss : 0.9313142085075379 TRAIN  loss dict:  {'classification_loss': 0.9313142085075379}
2025-01-15 01:35:59,218 [INFO] Step[1500/2713]: training loss : 0.9345580148696899 TRAIN  loss dict:  {'classification_loss': 0.9345580148696899}
2025-01-15 01:36:12,407 [INFO] Step[1550/2713]: training loss : 0.9441205024719238 TRAIN  loss dict:  {'classification_loss': 0.9441205024719238}
2025-01-15 01:36:26,268 [INFO] Step[1600/2713]: training loss : 0.9318262755870819 TRAIN  loss dict:  {'classification_loss': 0.9318262755870819}
2025-01-15 01:36:40,106 [INFO] Step[1650/2713]: training loss : 0.9334626436233521 TRAIN  loss dict:  {'classification_loss': 0.9334626436233521}
2025-01-15 01:36:54,160 [INFO] Step[1700/2713]: training loss : 0.9369385945796966 TRAIN  loss dict:  {'classification_loss': 0.9369385945796966}
2025-01-15 01:37:07,652 [INFO] Step[1750/2713]: training loss : 0.9306133985519409 TRAIN  loss dict:  {'classification_loss': 0.9306133985519409}
2025-01-15 01:37:21,401 [INFO] Step[1800/2713]: training loss : 0.9304045486450195 TRAIN  loss dict:  {'classification_loss': 0.9304045486450195}
2025-01-15 01:37:34,737 [INFO] Step[1850/2713]: training loss : 0.9335110020637513 TRAIN  loss dict:  {'classification_loss': 0.9335110020637513}
2025-01-15 01:37:48,620 [INFO] Step[1900/2713]: training loss : 0.938952944278717 TRAIN  loss dict:  {'classification_loss': 0.938952944278717}
2025-01-15 01:38:02,320 [INFO] Step[1950/2713]: training loss : 0.9468763601779938 TRAIN  loss dict:  {'classification_loss': 0.9468763601779938}
2025-01-15 01:38:15,825 [INFO] Step[2000/2713]: training loss : 0.9312591862678528 TRAIN  loss dict:  {'classification_loss': 0.9312591862678528}
2025-01-15 01:38:29,320 [INFO] Step[2050/2713]: training loss : 0.9312510550022125 TRAIN  loss dict:  {'classification_loss': 0.9312510550022125}
2025-01-15 01:38:43,114 [INFO] Step[2100/2713]: training loss : 0.9298794937133789 TRAIN  loss dict:  {'classification_loss': 0.9298794937133789}
2025-01-15 01:38:56,612 [INFO] Step[2150/2713]: training loss : 0.9310710632801056 TRAIN  loss dict:  {'classification_loss': 0.9310710632801056}
2025-01-15 01:39:09,930 [INFO] Step[2200/2713]: training loss : 0.9308331096172333 TRAIN  loss dict:  {'classification_loss': 0.9308331096172333}
2025-01-15 01:39:23,517 [INFO] Step[2250/2713]: training loss : 0.930895984172821 TRAIN  loss dict:  {'classification_loss': 0.930895984172821}
2025-01-15 01:39:37,341 [INFO] Step[2300/2713]: training loss : 0.9309544813632965 TRAIN  loss dict:  {'classification_loss': 0.9309544813632965}
2025-01-15 01:39:50,888 [INFO] Step[2350/2713]: training loss : 0.9330564987659454 TRAIN  loss dict:  {'classification_loss': 0.9330564987659454}
2025-01-15 01:40:04,294 [INFO] Step[2400/2713]: training loss : 0.9313586127758026 TRAIN  loss dict:  {'classification_loss': 0.9313586127758026}
2025-01-15 01:40:18,172 [INFO] Step[2450/2713]: training loss : 0.9319559466838837 TRAIN  loss dict:  {'classification_loss': 0.9319559466838837}
2025-01-15 01:40:31,403 [INFO] Step[2500/2713]: training loss : 0.9354051578044892 TRAIN  loss dict:  {'classification_loss': 0.9354051578044892}
2025-01-15 01:40:44,608 [INFO] Step[2550/2713]: training loss : 0.9301363933086395 TRAIN  loss dict:  {'classification_loss': 0.9301363933086395}
2025-01-15 01:40:58,343 [INFO] Step[2600/2713]: training loss : 0.9341663718223572 TRAIN  loss dict:  {'classification_loss': 0.9341663718223572}
2025-01-15 01:41:12,140 [INFO] Step[2650/2713]: training loss : 0.9309129428863525 TRAIN  loss dict:  {'classification_loss': 0.9309129428863525}
2025-01-15 01:41:25,905 [INFO] Step[2700/2713]: training loss : 0.9724875843524933 TRAIN  loss dict:  {'classification_loss': 0.9724875843524933}
2025-01-15 01:42:43,420 [INFO] Label accuracies statistics:
2025-01-15 01:42:43,420 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 0.75, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.5, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.5, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.25, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 01:42:43,422 [INFO] [62] TRAIN  loss: 0.9335969938723926 acc: 0.9990170782651432
2025-01-15 01:42:43,422 [INFO] [62] TRAIN  loss dict: {'classification_loss': 0.9335969938723926}
2025-01-15 01:42:43,422 [INFO] [62] VALIDATION loss: 1.8404677936008997 VALIDATION acc: 0.7981191222570533
2025-01-15 01:42:43,422 [INFO] [62] VALIDATION loss dict: {'classification_loss': 1.8404677936008997}
2025-01-15 01:42:43,422 [INFO] 
2025-01-15 01:43:02,075 [INFO] Step[50/2713]: training loss : 0.9560150456428528 TRAIN  loss dict:  {'classification_loss': 0.9560150456428528}
2025-01-15 01:43:15,640 [INFO] Step[100/2713]: training loss : 0.9941931986808776 TRAIN  loss dict:  {'classification_loss': 0.9941931986808776}
2025-01-15 01:43:29,192 [INFO] Step[150/2713]: training loss : 0.9328376817703247 TRAIN  loss dict:  {'classification_loss': 0.9328376817703247}
2025-01-15 01:43:42,358 [INFO] Step[200/2713]: training loss : 0.9317180168628693 TRAIN  loss dict:  {'classification_loss': 0.9317180168628693}
2025-01-15 01:43:55,569 [INFO] Step[250/2713]: training loss : 0.93028156042099 TRAIN  loss dict:  {'classification_loss': 0.93028156042099}
2025-01-15 01:44:09,342 [INFO] Step[300/2713]: training loss : 0.930821840763092 TRAIN  loss dict:  {'classification_loss': 0.930821840763092}
2025-01-15 01:44:23,239 [INFO] Step[350/2713]: training loss : 0.9319121360778808 TRAIN  loss dict:  {'classification_loss': 0.9319121360778808}
2025-01-15 01:44:36,696 [INFO] Step[400/2713]: training loss : 0.9340308439731598 TRAIN  loss dict:  {'classification_loss': 0.9340308439731598}
2025-01-15 01:44:50,802 [INFO] Step[450/2713]: training loss : 0.9298362112045289 TRAIN  loss dict:  {'classification_loss': 0.9298362112045289}
2025-01-15 01:45:04,409 [INFO] Step[500/2713]: training loss : 0.9305043411254883 TRAIN  loss dict:  {'classification_loss': 0.9305043411254883}
2025-01-15 01:45:18,098 [INFO] Step[550/2713]: training loss : 0.9312391555309296 TRAIN  loss dict:  {'classification_loss': 0.9312391555309296}
2025-01-15 01:45:31,273 [INFO] Step[600/2713]: training loss : 0.9449086952209472 TRAIN  loss dict:  {'classification_loss': 0.9449086952209472}
2025-01-15 01:45:45,029 [INFO] Step[650/2713]: training loss : 0.9322883915901184 TRAIN  loss dict:  {'classification_loss': 0.9322883915901184}
2025-01-15 01:45:58,643 [INFO] Step[700/2713]: training loss : 0.9296629917621613 TRAIN  loss dict:  {'classification_loss': 0.9296629917621613}
2025-01-15 01:46:12,293 [INFO] Step[750/2713]: training loss : 0.9301870429515838 TRAIN  loss dict:  {'classification_loss': 0.9301870429515838}
2025-01-15 01:46:25,754 [INFO] Step[800/2713]: training loss : 0.9345786368846893 TRAIN  loss dict:  {'classification_loss': 0.9345786368846893}
2025-01-15 01:46:39,600 [INFO] Step[850/2713]: training loss : 0.9325148224830627 TRAIN  loss dict:  {'classification_loss': 0.9325148224830627}
2025-01-15 01:46:52,757 [INFO] Step[900/2713]: training loss : 0.9305731189250946 TRAIN  loss dict:  {'classification_loss': 0.9305731189250946}
2025-01-15 01:47:06,283 [INFO] Step[950/2713]: training loss : 0.9545413589477539 TRAIN  loss dict:  {'classification_loss': 0.9545413589477539}
2025-01-15 01:47:20,126 [INFO] Step[1000/2713]: training loss : 0.9339202082157135 TRAIN  loss dict:  {'classification_loss': 0.9339202082157135}
2025-01-15 01:47:33,868 [INFO] Step[1050/2713]: training loss : 0.9311336636543274 TRAIN  loss dict:  {'classification_loss': 0.9311336636543274}
2025-01-15 01:47:47,489 [INFO] Step[1100/2713]: training loss : 0.9299937224388123 TRAIN  loss dict:  {'classification_loss': 0.9299937224388123}
2025-01-15 01:48:01,290 [INFO] Step[1150/2713]: training loss : 0.9369537448883056 TRAIN  loss dict:  {'classification_loss': 0.9369537448883056}
2025-01-15 01:48:17,428 [INFO] Step[1200/2713]: training loss : 0.9340680563449859 TRAIN  loss dict:  {'classification_loss': 0.9340680563449859}
2025-01-15 01:48:31,632 [INFO] Step[1250/2713]: training loss : 0.9314044964313507 TRAIN  loss dict:  {'classification_loss': 0.9314044964313507}
2025-01-15 01:48:44,805 [INFO] Step[1300/2713]: training loss : 0.9363075411319732 TRAIN  loss dict:  {'classification_loss': 0.9363075411319732}
2025-01-15 01:48:57,940 [INFO] Step[1350/2713]: training loss : 0.9306512987613678 TRAIN  loss dict:  {'classification_loss': 0.9306512987613678}
2025-01-15 01:49:11,361 [INFO] Step[1400/2713]: training loss : 0.9304283440113068 TRAIN  loss dict:  {'classification_loss': 0.9304283440113068}
2025-01-15 01:49:25,108 [INFO] Step[1450/2713]: training loss : 0.9529045748710633 TRAIN  loss dict:  {'classification_loss': 0.9529045748710633}
2025-01-15 01:49:38,519 [INFO] Step[1500/2713]: training loss : 0.9346476638317108 TRAIN  loss dict:  {'classification_loss': 0.9346476638317108}
2025-01-15 01:49:52,006 [INFO] Step[1550/2713]: training loss : 0.9414736318588257 TRAIN  loss dict:  {'classification_loss': 0.9414736318588257}
2025-01-15 01:50:05,539 [INFO] Step[1600/2713]: training loss : 0.9312831294536591 TRAIN  loss dict:  {'classification_loss': 0.9312831294536591}
2025-01-15 01:50:19,131 [INFO] Step[1650/2713]: training loss : 0.9355048203468322 TRAIN  loss dict:  {'classification_loss': 0.9355048203468322}
2025-01-15 01:50:32,577 [INFO] Step[1700/2713]: training loss : 0.9307587504386902 TRAIN  loss dict:  {'classification_loss': 0.9307587504386902}
2025-01-15 01:50:45,735 [INFO] Step[1750/2713]: training loss : 0.9425556075572967 TRAIN  loss dict:  {'classification_loss': 0.9425556075572967}
2025-01-15 01:50:59,149 [INFO] Step[1800/2713]: training loss : 0.9309356033802032 TRAIN  loss dict:  {'classification_loss': 0.9309356033802032}
2025-01-15 01:51:13,291 [INFO] Step[1850/2713]: training loss : 0.9300650143623352 TRAIN  loss dict:  {'classification_loss': 0.9300650143623352}
2025-01-15 01:51:26,509 [INFO] Step[1900/2713]: training loss : 0.9305827307701111 TRAIN  loss dict:  {'classification_loss': 0.9305827307701111}
2025-01-15 01:51:39,960 [INFO] Step[1950/2713]: training loss : 0.9331535887718201 TRAIN  loss dict:  {'classification_loss': 0.9331535887718201}
2025-01-15 01:51:53,505 [INFO] Step[2000/2713]: training loss : 0.9327251768112182 TRAIN  loss dict:  {'classification_loss': 0.9327251768112182}
2025-01-15 01:52:07,580 [INFO] Step[2050/2713]: training loss : 0.9429638683795929 TRAIN  loss dict:  {'classification_loss': 0.9429638683795929}
2025-01-15 01:52:20,899 [INFO] Step[2100/2713]: training loss : 0.9315391623973847 TRAIN  loss dict:  {'classification_loss': 0.9315391623973847}
2025-01-15 01:52:34,637 [INFO] Step[2150/2713]: training loss : 0.9302492249011993 TRAIN  loss dict:  {'classification_loss': 0.9302492249011993}
2025-01-15 01:52:48,390 [INFO] Step[2200/2713]: training loss : 0.9304428470134735 TRAIN  loss dict:  {'classification_loss': 0.9304428470134735}
2025-01-15 01:53:02,027 [INFO] Step[2250/2713]: training loss : 0.9518064367771149 TRAIN  loss dict:  {'classification_loss': 0.9518064367771149}
2025-01-15 01:53:15,383 [INFO] Step[2300/2713]: training loss : 0.9298494899272919 TRAIN  loss dict:  {'classification_loss': 0.9298494899272919}
2025-01-15 01:53:28,748 [INFO] Step[2350/2713]: training loss : 0.9331383275985717 TRAIN  loss dict:  {'classification_loss': 0.9331383275985717}
2025-01-15 01:53:41,961 [INFO] Step[2400/2713]: training loss : 0.9310397636890412 TRAIN  loss dict:  {'classification_loss': 0.9310397636890412}
2025-01-15 01:53:55,396 [INFO] Step[2450/2713]: training loss : 0.9313768291473389 TRAIN  loss dict:  {'classification_loss': 0.9313768291473389}
2025-01-15 01:54:08,896 [INFO] Step[2500/2713]: training loss : 0.9373541092872619 TRAIN  loss dict:  {'classification_loss': 0.9373541092872619}
2025-01-15 01:54:22,392 [INFO] Step[2550/2713]: training loss : 0.9402679014205932 TRAIN  loss dict:  {'classification_loss': 0.9402679014205932}
2025-01-15 01:54:35,591 [INFO] Step[2600/2713]: training loss : 0.9307358407974243 TRAIN  loss dict:  {'classification_loss': 0.9307358407974243}
2025-01-15 01:54:48,765 [INFO] Step[2650/2713]: training loss : 0.9308158075809478 TRAIN  loss dict:  {'classification_loss': 0.9308158075809478}
2025-01-15 01:55:02,570 [INFO] Step[2700/2713]: training loss : 0.9308632981777191 TRAIN  loss dict:  {'classification_loss': 0.9308632981777191}
2025-01-15 01:56:22,194 [INFO] Label accuracies statistics:
2025-01-15 01:56:22,194 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 0.75, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.25, 261: 0.75, 262: 1.0, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.25, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 01:56:22,196 [INFO] [63] TRAIN  loss: 0.9356433748100413 acc: 0.9984027521808576
2025-01-15 01:56:22,196 [INFO] [63] TRAIN  loss dict: {'classification_loss': 0.9356433748100413}
2025-01-15 01:56:22,196 [INFO] [63] VALIDATION loss: 1.8017531042932569 VALIDATION acc: 0.8056426332288401
2025-01-15 01:56:22,196 [INFO] [63] VALIDATION loss dict: {'classification_loss': 1.8017531042932569}
2025-01-15 01:56:22,196 [INFO] 
2025-01-15 01:56:41,195 [INFO] Step[50/2713]: training loss : 0.9461378991603852 TRAIN  loss dict:  {'classification_loss': 0.9461378991603852}
2025-01-15 01:56:54,374 [INFO] Step[100/2713]: training loss : 0.9310455369949341 TRAIN  loss dict:  {'classification_loss': 0.9310455369949341}
2025-01-15 01:57:08,280 [INFO] Step[150/2713]: training loss : 0.9299071645736694 TRAIN  loss dict:  {'classification_loss': 0.9299071645736694}
2025-01-15 01:57:21,973 [INFO] Step[200/2713]: training loss : 0.9318586361408233 TRAIN  loss dict:  {'classification_loss': 0.9318586361408233}
2025-01-15 01:57:35,435 [INFO] Step[250/2713]: training loss : 0.9316705119609833 TRAIN  loss dict:  {'classification_loss': 0.9316705119609833}
2025-01-15 01:57:49,203 [INFO] Step[300/2713]: training loss : 0.9303350877761841 TRAIN  loss dict:  {'classification_loss': 0.9303350877761841}
2025-01-15 01:58:03,014 [INFO] Step[350/2713]: training loss : 0.930212346315384 TRAIN  loss dict:  {'classification_loss': 0.930212346315384}
2025-01-15 01:58:16,508 [INFO] Step[400/2713]: training loss : 0.9338713419437409 TRAIN  loss dict:  {'classification_loss': 0.9338713419437409}
2025-01-15 01:58:29,978 [INFO] Step[450/2713]: training loss : 0.9317482328414917 TRAIN  loss dict:  {'classification_loss': 0.9317482328414917}
2025-01-15 01:58:44,146 [INFO] Step[500/2713]: training loss : 0.9303132128715516 TRAIN  loss dict:  {'classification_loss': 0.9303132128715516}
2025-01-15 01:58:57,588 [INFO] Step[550/2713]: training loss : 0.9303793013095856 TRAIN  loss dict:  {'classification_loss': 0.9303793013095856}
2025-01-15 01:59:11,098 [INFO] Step[600/2713]: training loss : 0.9305580890178681 TRAIN  loss dict:  {'classification_loss': 0.9305580890178681}
2025-01-15 01:59:24,486 [INFO] Step[650/2713]: training loss : 0.9314651012420654 TRAIN  loss dict:  {'classification_loss': 0.9314651012420654}
2025-01-15 01:59:38,462 [INFO] Step[700/2713]: training loss : 0.9308999300003051 TRAIN  loss dict:  {'classification_loss': 0.9308999300003051}
2025-01-15 01:59:52,459 [INFO] Step[750/2713]: training loss : 0.9312835788726807 TRAIN  loss dict:  {'classification_loss': 0.9312835788726807}
2025-01-15 02:00:05,868 [INFO] Step[800/2713]: training loss : 0.9301510846614838 TRAIN  loss dict:  {'classification_loss': 0.9301510846614838}
2025-01-15 02:00:19,609 [INFO] Step[850/2713]: training loss : 0.9307954967021942 TRAIN  loss dict:  {'classification_loss': 0.9307954967021942}
2025-01-15 02:00:33,302 [INFO] Step[900/2713]: training loss : 0.932833741903305 TRAIN  loss dict:  {'classification_loss': 0.932833741903305}
2025-01-15 02:00:46,519 [INFO] Step[950/2713]: training loss : 0.9306015491485595 TRAIN  loss dict:  {'classification_loss': 0.9306015491485595}
2025-01-15 02:01:00,316 [INFO] Step[1000/2713]: training loss : 0.9336323654651641 TRAIN  loss dict:  {'classification_loss': 0.9336323654651641}
2025-01-15 02:01:13,821 [INFO] Step[1050/2713]: training loss : 0.9304655981063843 TRAIN  loss dict:  {'classification_loss': 0.9304655981063843}
2025-01-15 02:01:27,441 [INFO] Step[1100/2713]: training loss : 0.9297812783718109 TRAIN  loss dict:  {'classification_loss': 0.9297812783718109}
2025-01-15 02:01:41,312 [INFO] Step[1150/2713]: training loss : 0.932159663438797 TRAIN  loss dict:  {'classification_loss': 0.932159663438797}
2025-01-15 02:01:54,532 [INFO] Step[1200/2713]: training loss : 0.9299204218387603 TRAIN  loss dict:  {'classification_loss': 0.9299204218387603}
2025-01-15 02:02:08,248 [INFO] Step[1250/2713]: training loss : 0.9294553411006927 TRAIN  loss dict:  {'classification_loss': 0.9294553411006927}
2025-01-15 02:02:21,838 [INFO] Step[1300/2713]: training loss : 0.9336162102222443 TRAIN  loss dict:  {'classification_loss': 0.9336162102222443}
2025-01-15 02:02:37,432 [INFO] Step[1350/2713]: training loss : 0.9352290642261505 TRAIN  loss dict:  {'classification_loss': 0.9352290642261505}
2025-01-15 02:02:52,342 [INFO] Step[1400/2713]: training loss : 0.9307318580150604 TRAIN  loss dict:  {'classification_loss': 0.9307318580150604}
2025-01-15 02:03:05,573 [INFO] Step[1450/2713]: training loss : 0.9305968713760376 TRAIN  loss dict:  {'classification_loss': 0.9305968713760376}
2025-01-15 02:03:18,783 [INFO] Step[1500/2713]: training loss : 0.9519609498977661 TRAIN  loss dict:  {'classification_loss': 0.9519609498977661}
2025-01-15 02:03:32,315 [INFO] Step[1550/2713]: training loss : 0.9310636878013611 TRAIN  loss dict:  {'classification_loss': 0.9310636878013611}
2025-01-15 02:03:45,812 [INFO] Step[1600/2713]: training loss : 0.9320566022396087 TRAIN  loss dict:  {'classification_loss': 0.9320566022396087}
2025-01-15 02:03:59,835 [INFO] Step[1650/2713]: training loss : 0.9307597041130066 TRAIN  loss dict:  {'classification_loss': 0.9307597041130066}
2025-01-15 02:04:13,041 [INFO] Step[1700/2713]: training loss : 0.9300942933559417 TRAIN  loss dict:  {'classification_loss': 0.9300942933559417}
2025-01-15 02:04:26,547 [INFO] Step[1750/2713]: training loss : 0.9326672375202179 TRAIN  loss dict:  {'classification_loss': 0.9326672375202179}
2025-01-15 02:04:40,160 [INFO] Step[1800/2713]: training loss : 0.9309278070926666 TRAIN  loss dict:  {'classification_loss': 0.9309278070926666}
2025-01-15 02:04:53,719 [INFO] Step[1850/2713]: training loss : 0.9303141033649445 TRAIN  loss dict:  {'classification_loss': 0.9303141033649445}
2025-01-15 02:05:07,131 [INFO] Step[1900/2713]: training loss : 0.9336926805973053 TRAIN  loss dict:  {'classification_loss': 0.9336926805973053}
2025-01-15 02:05:20,339 [INFO] Step[1950/2713]: training loss : 0.9567923259735107 TRAIN  loss dict:  {'classification_loss': 0.9567923259735107}
2025-01-15 02:05:34,098 [INFO] Step[2000/2713]: training loss : 0.9306351900100708 TRAIN  loss dict:  {'classification_loss': 0.9306351900100708}
2025-01-15 02:05:48,115 [INFO] Step[2050/2713]: training loss : 0.9366627037525177 TRAIN  loss dict:  {'classification_loss': 0.9366627037525177}
2025-01-15 02:06:01,694 [INFO] Step[2100/2713]: training loss : 0.9304466688632965 TRAIN  loss dict:  {'classification_loss': 0.9304466688632965}
2025-01-15 02:06:14,897 [INFO] Step[2150/2713]: training loss : 0.9307206463813782 TRAIN  loss dict:  {'classification_loss': 0.9307206463813782}
2025-01-15 02:06:28,857 [INFO] Step[2200/2713]: training loss : 0.9295874309539794 TRAIN  loss dict:  {'classification_loss': 0.9295874309539794}
2025-01-15 02:06:42,787 [INFO] Step[2250/2713]: training loss : 0.9312836921215057 TRAIN  loss dict:  {'classification_loss': 0.9312836921215057}
2025-01-15 02:06:56,990 [INFO] Step[2300/2713]: training loss : 0.9311829745769501 TRAIN  loss dict:  {'classification_loss': 0.9311829745769501}
2025-01-15 02:07:11,178 [INFO] Step[2350/2713]: training loss : 0.9305796384811401 TRAIN  loss dict:  {'classification_loss': 0.9305796384811401}
2025-01-15 02:07:24,960 [INFO] Step[2400/2713]: training loss : 0.9304848635196685 TRAIN  loss dict:  {'classification_loss': 0.9304848635196685}
2025-01-15 02:07:38,748 [INFO] Step[2450/2713]: training loss : 0.9300475919246673 TRAIN  loss dict:  {'classification_loss': 0.9300475919246673}
2025-01-15 02:07:52,266 [INFO] Step[2500/2713]: training loss : 0.9303470969200134 TRAIN  loss dict:  {'classification_loss': 0.9303470969200134}
2025-01-15 02:08:05,862 [INFO] Step[2550/2713]: training loss : 0.9319424760341645 TRAIN  loss dict:  {'classification_loss': 0.9319424760341645}
2025-01-15 02:08:19,420 [INFO] Step[2600/2713]: training loss : 0.9315318667888641 TRAIN  loss dict:  {'classification_loss': 0.9315318667888641}
2025-01-15 02:08:33,271 [INFO] Step[2650/2713]: training loss : 0.9303065371513367 TRAIN  loss dict:  {'classification_loss': 0.9303065371513367}
2025-01-15 02:08:47,485 [INFO] Step[2700/2713]: training loss : 0.9298386764526367 TRAIN  loss dict:  {'classification_loss': 0.9298386764526367}
2025-01-15 02:10:03,795 [INFO] Label accuracies statistics:
2025-01-15 02:10:03,795 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 02:10:03,797 [INFO] [64] TRAIN  loss: 0.9323815883765834 acc: 0.9996314043494287
2025-01-15 02:10:03,797 [INFO] [64] TRAIN  loss dict: {'classification_loss': 0.9323815883765834}
2025-01-15 02:10:03,797 [INFO] [64] VALIDATION loss: 1.8344456621802838 VALIDATION acc: 0.7968652037617555
2025-01-15 02:10:03,797 [INFO] [64] VALIDATION loss dict: {'classification_loss': 1.8344456621802838}
2025-01-15 02:10:03,798 [INFO] 
2025-01-15 02:10:22,639 [INFO] Step[50/2713]: training loss : 0.929939032793045 TRAIN  loss dict:  {'classification_loss': 0.929939032793045}
2025-01-15 02:10:36,542 [INFO] Step[100/2713]: training loss : 0.9299868905544281 TRAIN  loss dict:  {'classification_loss': 0.9299868905544281}
2025-01-15 02:10:50,392 [INFO] Step[150/2713]: training loss : 0.9310430467128754 TRAIN  loss dict:  {'classification_loss': 0.9310430467128754}
2025-01-15 02:11:03,979 [INFO] Step[200/2713]: training loss : 0.9309408760070801 TRAIN  loss dict:  {'classification_loss': 0.9309408760070801}
2025-01-15 02:11:17,927 [INFO] Step[250/2713]: training loss : 0.9311537754535675 TRAIN  loss dict:  {'classification_loss': 0.9311537754535675}
2025-01-15 02:11:32,151 [INFO] Step[300/2713]: training loss : 0.9305813550949097 TRAIN  loss dict:  {'classification_loss': 0.9305813550949097}
2025-01-15 02:11:46,027 [INFO] Step[350/2713]: training loss : 0.9298526716232299 TRAIN  loss dict:  {'classification_loss': 0.9298526716232299}
2025-01-15 02:11:59,516 [INFO] Step[400/2713]: training loss : 0.9316680645942688 TRAIN  loss dict:  {'classification_loss': 0.9316680645942688}
2025-01-15 02:12:13,174 [INFO] Step[450/2713]: training loss : 0.9311431300640106 TRAIN  loss dict:  {'classification_loss': 0.9311431300640106}
2025-01-15 02:12:26,684 [INFO] Step[500/2713]: training loss : 0.9313564944267273 TRAIN  loss dict:  {'classification_loss': 0.9313564944267273}
2025-01-15 02:12:40,713 [INFO] Step[550/2713]: training loss : 0.9308002650737762 TRAIN  loss dict:  {'classification_loss': 0.9308002650737762}
2025-01-15 02:12:54,086 [INFO] Step[600/2713]: training loss : 0.9307155299186707 TRAIN  loss dict:  {'classification_loss': 0.9307155299186707}
2025-01-15 02:13:07,393 [INFO] Step[650/2713]: training loss : 0.9296981716156005 TRAIN  loss dict:  {'classification_loss': 0.9296981716156005}
2025-01-15 02:13:21,515 [INFO] Step[700/2713]: training loss : 0.9297238802909851 TRAIN  loss dict:  {'classification_loss': 0.9297238802909851}
2025-01-15 02:13:35,622 [INFO] Step[750/2713]: training loss : 0.9296507108211517 TRAIN  loss dict:  {'classification_loss': 0.9296507108211517}
2025-01-15 02:13:49,611 [INFO] Step[800/2713]: training loss : 0.9318059396743774 TRAIN  loss dict:  {'classification_loss': 0.9318059396743774}
2025-01-15 02:14:03,189 [INFO] Step[850/2713]: training loss : 0.9309514236450195 TRAIN  loss dict:  {'classification_loss': 0.9309514236450195}
2025-01-15 02:14:16,778 [INFO] Step[900/2713]: training loss : 0.9309166014194489 TRAIN  loss dict:  {'classification_loss': 0.9309166014194489}
2025-01-15 02:14:30,019 [INFO] Step[950/2713]: training loss : 0.9293965196609497 TRAIN  loss dict:  {'classification_loss': 0.9293965196609497}
2025-01-15 02:14:43,242 [INFO] Step[1000/2713]: training loss : 0.9300235545635224 TRAIN  loss dict:  {'classification_loss': 0.9300235545635224}
2025-01-15 02:14:57,022 [INFO] Step[1050/2713]: training loss : 0.9319676041603089 TRAIN  loss dict:  {'classification_loss': 0.9319676041603089}
2025-01-15 02:15:11,211 [INFO] Step[1100/2713]: training loss : 0.9300168550014496 TRAIN  loss dict:  {'classification_loss': 0.9300168550014496}
2025-01-15 02:15:25,045 [INFO] Step[1150/2713]: training loss : 0.9305768144130707 TRAIN  loss dict:  {'classification_loss': 0.9305768144130707}
2025-01-15 02:15:38,287 [INFO] Step[1200/2713]: training loss : 0.9308046889305115 TRAIN  loss dict:  {'classification_loss': 0.9308046889305115}
2025-01-15 02:15:51,531 [INFO] Step[1250/2713]: training loss : 0.9306065130233765 TRAIN  loss dict:  {'classification_loss': 0.9306065130233765}
2025-01-15 02:16:05,540 [INFO] Step[1300/2713]: training loss : 0.9329242694377899 TRAIN  loss dict:  {'classification_loss': 0.9329242694377899}
2025-01-15 02:16:19,773 [INFO] Step[1350/2713]: training loss : 0.9305072975158691 TRAIN  loss dict:  {'classification_loss': 0.9305072975158691}
2025-01-15 02:16:34,046 [INFO] Step[1400/2713]: training loss : 0.9519302475452424 TRAIN  loss dict:  {'classification_loss': 0.9519302475452424}
2025-01-15 02:16:48,170 [INFO] Step[1450/2713]: training loss : 0.9299805736541749 TRAIN  loss dict:  {'classification_loss': 0.9299805736541749}
2025-01-15 02:17:01,432 [INFO] Step[1500/2713]: training loss : 0.9299762856960296 TRAIN  loss dict:  {'classification_loss': 0.9299762856960296}
2025-01-15 02:17:15,013 [INFO] Step[1550/2713]: training loss : 0.9302168130874634 TRAIN  loss dict:  {'classification_loss': 0.9302168130874634}
2025-01-15 02:17:28,355 [INFO] Step[1600/2713]: training loss : 0.9327024400234223 TRAIN  loss dict:  {'classification_loss': 0.9327024400234223}
2025-01-15 02:17:41,846 [INFO] Step[1650/2713]: training loss : 0.9313340723514557 TRAIN  loss dict:  {'classification_loss': 0.9313340723514557}
2025-01-15 02:17:56,054 [INFO] Step[1700/2713]: training loss : 0.9408299767971039 TRAIN  loss dict:  {'classification_loss': 0.9408299767971039}
2025-01-15 02:18:09,691 [INFO] Step[1750/2713]: training loss : 0.9290920090675354 TRAIN  loss dict:  {'classification_loss': 0.9290920090675354}
2025-01-15 02:18:23,687 [INFO] Step[1800/2713]: training loss : 0.948805992603302 TRAIN  loss dict:  {'classification_loss': 0.948805992603302}
2025-01-15 02:18:36,905 [INFO] Step[1850/2713]: training loss : 0.9318809461593628 TRAIN  loss dict:  {'classification_loss': 0.9318809461593628}
2025-01-15 02:18:50,619 [INFO] Step[1900/2713]: training loss : 0.9366970670223236 TRAIN  loss dict:  {'classification_loss': 0.9366970670223236}
2025-01-15 02:19:04,753 [INFO] Step[1950/2713]: training loss : 0.9380366456508636 TRAIN  loss dict:  {'classification_loss': 0.9380366456508636}
2025-01-15 02:19:18,543 [INFO] Step[2000/2713]: training loss : 0.9446601998806 TRAIN  loss dict:  {'classification_loss': 0.9446601998806}
2025-01-15 02:19:32,787 [INFO] Step[2050/2713]: training loss : 0.9303682112693786 TRAIN  loss dict:  {'classification_loss': 0.9303682112693786}
2025-01-15 02:19:46,191 [INFO] Step[2100/2713]: training loss : 0.9296740436553955 TRAIN  loss dict:  {'classification_loss': 0.9296740436553955}
2025-01-15 02:20:00,197 [INFO] Step[2150/2713]: training loss : 0.9298885083198547 TRAIN  loss dict:  {'classification_loss': 0.9298885083198547}
2025-01-15 02:20:14,086 [INFO] Step[2200/2713]: training loss : 0.9306770288944244 TRAIN  loss dict:  {'classification_loss': 0.9306770288944244}
2025-01-15 02:20:27,749 [INFO] Step[2250/2713]: training loss : 0.9409246683120728 TRAIN  loss dict:  {'classification_loss': 0.9409246683120728}
2025-01-15 02:20:41,677 [INFO] Step[2300/2713]: training loss : 0.9310305666923523 TRAIN  loss dict:  {'classification_loss': 0.9310305666923523}
2025-01-15 02:20:55,195 [INFO] Step[2350/2713]: training loss : 0.9304639315605163 TRAIN  loss dict:  {'classification_loss': 0.9304639315605163}
2025-01-15 02:21:09,315 [INFO] Step[2400/2713]: training loss : 0.929741176366806 TRAIN  loss dict:  {'classification_loss': 0.929741176366806}
2025-01-15 02:21:23,234 [INFO] Step[2450/2713]: training loss : 0.9311237657070159 TRAIN  loss dict:  {'classification_loss': 0.9311237657070159}
2025-01-15 02:21:37,118 [INFO] Step[2500/2713]: training loss : 0.9442893540859223 TRAIN  loss dict:  {'classification_loss': 0.9442893540859223}
2025-01-15 02:21:50,755 [INFO] Step[2550/2713]: training loss : 0.9307842826843262 TRAIN  loss dict:  {'classification_loss': 0.9307842826843262}
2025-01-15 02:22:04,005 [INFO] Step[2600/2713]: training loss : 0.9322451722621917 TRAIN  loss dict:  {'classification_loss': 0.9322451722621917}
2025-01-15 02:22:17,479 [INFO] Step[2650/2713]: training loss : 0.930287766456604 TRAIN  loss dict:  {'classification_loss': 0.930287766456604}
2025-01-15 02:22:31,116 [INFO] Step[2700/2713]: training loss : 0.932285658121109 TRAIN  loss dict:  {'classification_loss': 0.932285658121109}
2025-01-15 02:23:47,139 [INFO] Label accuracies statistics:
2025-01-15 02:23:47,139 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 02:23:47,141 [INFO] [65] TRAIN  loss: 0.9325553677506523 acc: 0.9991399434820002
2025-01-15 02:23:47,141 [INFO] [65] TRAIN  loss dict: {'classification_loss': 0.9325553677506523}
2025-01-15 02:23:47,141 [INFO] [65] VALIDATION loss: 1.7923717129051238 VALIDATION acc: 0.8075235109717869
2025-01-15 02:23:47,141 [INFO] [65] VALIDATION loss dict: {'classification_loss': 1.7923717129051238}
2025-01-15 02:23:47,141 [INFO] 
2025-01-15 02:24:05,795 [INFO] Step[50/2713]: training loss : 0.9294324338436126 TRAIN  loss dict:  {'classification_loss': 0.9294324338436126}
2025-01-15 02:24:19,121 [INFO] Step[100/2713]: training loss : 0.9308773493766784 TRAIN  loss dict:  {'classification_loss': 0.9308773493766784}
2025-01-15 02:24:32,967 [INFO] Step[150/2713]: training loss : 0.960627658367157 TRAIN  loss dict:  {'classification_loss': 0.960627658367157}
2025-01-15 02:24:46,522 [INFO] Step[200/2713]: training loss : 0.9301220846176147 TRAIN  loss dict:  {'classification_loss': 0.9301220846176147}
2025-01-15 02:25:00,658 [INFO] Step[250/2713]: training loss : 0.9534638118743897 TRAIN  loss dict:  {'classification_loss': 0.9534638118743897}
2025-01-15 02:25:14,212 [INFO] Step[300/2713]: training loss : 0.9329156875610352 TRAIN  loss dict:  {'classification_loss': 0.9329156875610352}
2025-01-15 02:25:27,360 [INFO] Step[350/2713]: training loss : 0.9301633250713348 TRAIN  loss dict:  {'classification_loss': 0.9301633250713348}
2025-01-15 02:25:40,540 [INFO] Step[400/2713]: training loss : 0.9305463433265686 TRAIN  loss dict:  {'classification_loss': 0.9305463433265686}
2025-01-15 02:25:54,654 [INFO] Step[450/2713]: training loss : 0.9321197271347046 TRAIN  loss dict:  {'classification_loss': 0.9321197271347046}
2025-01-15 02:26:08,243 [INFO] Step[500/2713]: training loss : 0.9300505971908569 TRAIN  loss dict:  {'classification_loss': 0.9300505971908569}
2025-01-15 02:26:22,433 [INFO] Step[550/2713]: training loss : 0.936851041316986 TRAIN  loss dict:  {'classification_loss': 0.936851041316986}
2025-01-15 02:26:36,223 [INFO] Step[600/2713]: training loss : 0.9320480728149414 TRAIN  loss dict:  {'classification_loss': 0.9320480728149414}
2025-01-15 02:26:50,072 [INFO] Step[650/2713]: training loss : 0.9330188536643982 TRAIN  loss dict:  {'classification_loss': 0.9330188536643982}
2025-01-15 02:27:03,711 [INFO] Step[700/2713]: training loss : 0.9348744416236877 TRAIN  loss dict:  {'classification_loss': 0.9348744416236877}
2025-01-15 02:27:17,040 [INFO] Step[750/2713]: training loss : 0.9326421749591828 TRAIN  loss dict:  {'classification_loss': 0.9326421749591828}
2025-01-15 02:27:30,500 [INFO] Step[800/2713]: training loss : 0.9329620325565338 TRAIN  loss dict:  {'classification_loss': 0.9329620325565338}
2025-01-15 02:27:44,071 [INFO] Step[850/2713]: training loss : 0.9315035688877106 TRAIN  loss dict:  {'classification_loss': 0.9315035688877106}
2025-01-15 02:27:57,999 [INFO] Step[900/2713]: training loss : 0.9283562326431274 TRAIN  loss dict:  {'classification_loss': 0.9283562326431274}
2025-01-15 02:28:11,256 [INFO] Step[950/2713]: training loss : 0.9305140089988708 TRAIN  loss dict:  {'classification_loss': 0.9305140089988708}
2025-01-15 02:28:24,390 [INFO] Step[1000/2713]: training loss : 0.9305532002449035 TRAIN  loss dict:  {'classification_loss': 0.9305532002449035}
2025-01-15 02:28:38,471 [INFO] Step[1050/2713]: training loss : 0.9307179141044617 TRAIN  loss dict:  {'classification_loss': 0.9307179141044617}
2025-01-15 02:28:51,906 [INFO] Step[1100/2713]: training loss : 0.9414082133769989 TRAIN  loss dict:  {'classification_loss': 0.9414082133769989}
2025-01-15 02:29:05,087 [INFO] Step[1150/2713]: training loss : 0.9675571370124817 TRAIN  loss dict:  {'classification_loss': 0.9675571370124817}
2025-01-15 02:29:18,981 [INFO] Step[1200/2713]: training loss : 0.9299249291419983 TRAIN  loss dict:  {'classification_loss': 0.9299249291419983}
2025-01-15 02:29:32,904 [INFO] Step[1250/2713]: training loss : 0.9293302440643311 TRAIN  loss dict:  {'classification_loss': 0.9293302440643311}
2025-01-15 02:29:46,569 [INFO] Step[1300/2713]: training loss : 0.9311890494823456 TRAIN  loss dict:  {'classification_loss': 0.9311890494823456}
2025-01-15 02:29:59,751 [INFO] Step[1350/2713]: training loss : 0.9297525644302368 TRAIN  loss dict:  {'classification_loss': 0.9297525644302368}
2025-01-15 02:30:13,071 [INFO] Step[1400/2713]: training loss : 0.93958566904068 TRAIN  loss dict:  {'classification_loss': 0.93958566904068}
2025-01-15 02:30:26,759 [INFO] Step[1450/2713]: training loss : 0.9298346984386444 TRAIN  loss dict:  {'classification_loss': 0.9298346984386444}
2025-01-15 02:30:40,179 [INFO] Step[1500/2713]: training loss : 0.9304655432701111 TRAIN  loss dict:  {'classification_loss': 0.9304655432701111}
2025-01-15 02:30:53,513 [INFO] Step[1550/2713]: training loss : 0.9307618570327759 TRAIN  loss dict:  {'classification_loss': 0.9307618570327759}
2025-01-15 02:31:07,438 [INFO] Step[1600/2713]: training loss : 0.9300649523735046 TRAIN  loss dict:  {'classification_loss': 0.9300649523735046}
2025-01-15 02:31:21,048 [INFO] Step[1650/2713]: training loss : 0.9307757127285003 TRAIN  loss dict:  {'classification_loss': 0.9307757127285003}
2025-01-15 02:31:34,489 [INFO] Step[1700/2713]: training loss : 0.9318329918384552 TRAIN  loss dict:  {'classification_loss': 0.9318329918384552}
2025-01-15 02:31:47,691 [INFO] Step[1750/2713]: training loss : 0.9389401030540466 TRAIN  loss dict:  {'classification_loss': 0.9389401030540466}
2025-01-15 02:32:01,216 [INFO] Step[1800/2713]: training loss : 0.9469577729701996 TRAIN  loss dict:  {'classification_loss': 0.9469577729701996}
2025-01-15 02:32:14,768 [INFO] Step[1850/2713]: training loss : 0.93063432097435 TRAIN  loss dict:  {'classification_loss': 0.93063432097435}
2025-01-15 02:32:28,396 [INFO] Step[1900/2713]: training loss : 0.9301320815086365 TRAIN  loss dict:  {'classification_loss': 0.9301320815086365}
2025-01-15 02:32:41,789 [INFO] Step[1950/2713]: training loss : 0.9616912913322448 TRAIN  loss dict:  {'classification_loss': 0.9616912913322448}
2025-01-15 02:32:54,982 [INFO] Step[2000/2713]: training loss : 0.9323971045017242 TRAIN  loss dict:  {'classification_loss': 0.9323971045017242}
2025-01-15 02:33:08,159 [INFO] Step[2050/2713]: training loss : 0.935809143781662 TRAIN  loss dict:  {'classification_loss': 0.935809143781662}
2025-01-15 02:33:21,550 [INFO] Step[2100/2713]: training loss : 0.9332595133781433 TRAIN  loss dict:  {'classification_loss': 0.9332595133781433}
2025-01-15 02:33:35,336 [INFO] Step[2150/2713]: training loss : 0.9307158076763153 TRAIN  loss dict:  {'classification_loss': 0.9307158076763153}
2025-01-15 02:33:49,211 [INFO] Step[2200/2713]: training loss : 0.9320547032356262 TRAIN  loss dict:  {'classification_loss': 0.9320547032356262}
2025-01-15 02:34:02,408 [INFO] Step[2250/2713]: training loss : 0.930659967660904 TRAIN  loss dict:  {'classification_loss': 0.930659967660904}
2025-01-15 02:34:16,292 [INFO] Step[2300/2713]: training loss : 0.9305264794826508 TRAIN  loss dict:  {'classification_loss': 0.9305264794826508}
2025-01-15 02:34:30,258 [INFO] Step[2350/2713]: training loss : 0.9291515862941742 TRAIN  loss dict:  {'classification_loss': 0.9291515862941742}
2025-01-15 02:34:43,611 [INFO] Step[2400/2713]: training loss : 0.941241191625595 TRAIN  loss dict:  {'classification_loss': 0.941241191625595}
2025-01-15 02:34:57,060 [INFO] Step[2450/2713]: training loss : 0.9314743423461914 TRAIN  loss dict:  {'classification_loss': 0.9314743423461914}
2025-01-15 02:35:10,517 [INFO] Step[2500/2713]: training loss : 0.9457975661754608 TRAIN  loss dict:  {'classification_loss': 0.9457975661754608}
2025-01-15 02:35:23,796 [INFO] Step[2550/2713]: training loss : 0.9303637051582336 TRAIN  loss dict:  {'classification_loss': 0.9303637051582336}
2025-01-15 02:35:37,953 [INFO] Step[2600/2713]: training loss : 0.9455450439453125 TRAIN  loss dict:  {'classification_loss': 0.9455450439453125}
2025-01-15 02:35:51,849 [INFO] Step[2650/2713]: training loss : 0.9335012590885162 TRAIN  loss dict:  {'classification_loss': 0.9335012590885162}
2025-01-15 02:36:05,749 [INFO] Step[2700/2713]: training loss : 0.9306960248947144 TRAIN  loss dict:  {'classification_loss': 0.9306960248947144}
2025-01-15 02:37:20,710 [INFO] Label accuracies statistics:
2025-01-15 02:37:20,710 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.5, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.0, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.5, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 02:37:20,712 [INFO] [66] TRAIN  loss: 0.9349433364043849 acc: 0.9984027521808576
2025-01-15 02:37:20,712 [INFO] [66] TRAIN  loss dict: {'classification_loss': 0.9349433364043849}
2025-01-15 02:37:20,712 [INFO] [66] VALIDATION loss: 1.8197675462728156 VALIDATION acc: 0.793730407523511
2025-01-15 02:37:20,712 [INFO] [66] VALIDATION loss dict: {'classification_loss': 1.8197675462728156}
2025-01-15 02:37:20,712 [INFO] 
2025-01-15 02:37:38,999 [INFO] Step[50/2713]: training loss : 0.9300302112102509 TRAIN  loss dict:  {'classification_loss': 0.9300302112102509}
2025-01-15 02:37:52,891 [INFO] Step[100/2713]: training loss : 0.9402459001541138 TRAIN  loss dict:  {'classification_loss': 0.9402459001541138}
2025-01-15 02:38:07,026 [INFO] Step[150/2713]: training loss : 0.9295605432987213 TRAIN  loss dict:  {'classification_loss': 0.9295605432987213}
2025-01-15 02:38:20,244 [INFO] Step[200/2713]: training loss : 0.9316133570671081 TRAIN  loss dict:  {'classification_loss': 0.9316133570671081}
2025-01-15 02:38:34,020 [INFO] Step[250/2713]: training loss : 0.9314785766601562 TRAIN  loss dict:  {'classification_loss': 0.9314785766601562}
2025-01-15 02:38:47,744 [INFO] Step[300/2713]: training loss : 0.9328389763832092 TRAIN  loss dict:  {'classification_loss': 0.9328389763832092}
2025-01-15 02:39:01,540 [INFO] Step[350/2713]: training loss : 0.9309780013561249 TRAIN  loss dict:  {'classification_loss': 0.9309780013561249}
2025-01-15 02:39:15,114 [INFO] Step[400/2713]: training loss : 0.929540981054306 TRAIN  loss dict:  {'classification_loss': 0.929540981054306}
2025-01-15 02:39:28,691 [INFO] Step[450/2713]: training loss : 0.9291988480091095 TRAIN  loss dict:  {'classification_loss': 0.9291988480091095}
2025-01-15 02:39:42,469 [INFO] Step[500/2713]: training loss : 0.9312260782718659 TRAIN  loss dict:  {'classification_loss': 0.9312260782718659}
2025-01-15 02:39:56,340 [INFO] Step[550/2713]: training loss : 0.9302727007865905 TRAIN  loss dict:  {'classification_loss': 0.9302727007865905}
2025-01-15 02:40:10,099 [INFO] Step[600/2713]: training loss : 0.9333267974853515 TRAIN  loss dict:  {'classification_loss': 0.9333267974853515}
2025-01-15 02:40:23,747 [INFO] Step[650/2713]: training loss : 0.9310745036602021 TRAIN  loss dict:  {'classification_loss': 0.9310745036602021}
2025-01-15 02:40:37,453 [INFO] Step[700/2713]: training loss : 0.9355255806446076 TRAIN  loss dict:  {'classification_loss': 0.9355255806446076}
2025-01-15 02:40:50,734 [INFO] Step[750/2713]: training loss : 0.9323163115978241 TRAIN  loss dict:  {'classification_loss': 0.9323163115978241}
2025-01-15 02:41:04,302 [INFO] Step[800/2713]: training loss : 0.9318284213542938 TRAIN  loss dict:  {'classification_loss': 0.9318284213542938}
2025-01-15 02:41:18,176 [INFO] Step[850/2713]: training loss : 0.9308769547939301 TRAIN  loss dict:  {'classification_loss': 0.9308769547939301}
2025-01-15 02:41:32,053 [INFO] Step[900/2713]: training loss : 0.9451821291446686 TRAIN  loss dict:  {'classification_loss': 0.9451821291446686}
2025-01-15 02:41:46,203 [INFO] Step[950/2713]: training loss : 0.9300333893299103 TRAIN  loss dict:  {'classification_loss': 0.9300333893299103}
2025-01-15 02:42:00,122 [INFO] Step[1000/2713]: training loss : 0.9305368387699127 TRAIN  loss dict:  {'classification_loss': 0.9305368387699127}
2025-01-15 02:42:13,752 [INFO] Step[1050/2713]: training loss : 0.932785793542862 TRAIN  loss dict:  {'classification_loss': 0.932785793542862}
2025-01-15 02:42:27,859 [INFO] Step[1100/2713]: training loss : 0.9381329476833343 TRAIN  loss dict:  {'classification_loss': 0.9381329476833343}
2025-01-15 02:42:41,039 [INFO] Step[1150/2713]: training loss : 0.9323677122592926 TRAIN  loss dict:  {'classification_loss': 0.9323677122592926}
2025-01-15 02:42:54,635 [INFO] Step[1200/2713]: training loss : 0.9313714516162872 TRAIN  loss dict:  {'classification_loss': 0.9313714516162872}
2025-01-15 02:43:08,531 [INFO] Step[1250/2713]: training loss : 0.9304051458835602 TRAIN  loss dict:  {'classification_loss': 0.9304051458835602}
2025-01-15 02:43:22,354 [INFO] Step[1300/2713]: training loss : 0.9314042830467224 TRAIN  loss dict:  {'classification_loss': 0.9314042830467224}
2025-01-15 02:43:36,435 [INFO] Step[1350/2713]: training loss : 0.9301458513736724 TRAIN  loss dict:  {'classification_loss': 0.9301458513736724}
2025-01-15 02:43:50,582 [INFO] Step[1400/2713]: training loss : 0.9310103774070739 TRAIN  loss dict:  {'classification_loss': 0.9310103774070739}
2025-01-15 02:44:04,036 [INFO] Step[1450/2713]: training loss : 0.9304038786888122 TRAIN  loss dict:  {'classification_loss': 0.9304038786888122}
2025-01-15 02:44:17,172 [INFO] Step[1500/2713]: training loss : 0.9593585157394409 TRAIN  loss dict:  {'classification_loss': 0.9593585157394409}
2025-01-15 02:44:30,720 [INFO] Step[1550/2713]: training loss : 0.9339151084423065 TRAIN  loss dict:  {'classification_loss': 0.9339151084423065}
2025-01-15 02:44:44,358 [INFO] Step[1600/2713]: training loss : 0.9292779338359832 TRAIN  loss dict:  {'classification_loss': 0.9292779338359832}
2025-01-15 02:44:57,818 [INFO] Step[1650/2713]: training loss : 0.9330018305778504 TRAIN  loss dict:  {'classification_loss': 0.9330018305778504}
2025-01-15 02:45:11,902 [INFO] Step[1700/2713]: training loss : 0.9322289049625396 TRAIN  loss dict:  {'classification_loss': 0.9322289049625396}
2025-01-15 02:45:25,724 [INFO] Step[1750/2713]: training loss : 0.9310779321193695 TRAIN  loss dict:  {'classification_loss': 0.9310779321193695}
2025-01-15 02:45:39,082 [INFO] Step[1800/2713]: training loss : 0.9296238625049591 TRAIN  loss dict:  {'classification_loss': 0.9296238625049591}
2025-01-15 02:45:53,073 [INFO] Step[1850/2713]: training loss : 0.9316031765937806 TRAIN  loss dict:  {'classification_loss': 0.9316031765937806}
2025-01-15 02:46:07,172 [INFO] Step[1900/2713]: training loss : 0.9346963346004487 TRAIN  loss dict:  {'classification_loss': 0.9346963346004487}
2025-01-15 02:46:20,796 [INFO] Step[1950/2713]: training loss : 0.9306786072254181 TRAIN  loss dict:  {'classification_loss': 0.9306786072254181}
2025-01-15 02:46:34,300 [INFO] Step[2000/2713]: training loss : 0.9309120202064514 TRAIN  loss dict:  {'classification_loss': 0.9309120202064514}
2025-01-15 02:46:48,108 [INFO] Step[2050/2713]: training loss : 0.9312910795211792 TRAIN  loss dict:  {'classification_loss': 0.9312910795211792}
2025-01-15 02:47:01,683 [INFO] Step[2100/2713]: training loss : 0.9301879823207855 TRAIN  loss dict:  {'classification_loss': 0.9301879823207855}
2025-01-15 02:47:15,319 [INFO] Step[2150/2713]: training loss : 0.9303647100925445 TRAIN  loss dict:  {'classification_loss': 0.9303647100925445}
2025-01-15 02:47:29,102 [INFO] Step[2200/2713]: training loss : 0.9303174221515655 TRAIN  loss dict:  {'classification_loss': 0.9303174221515655}
2025-01-15 02:47:42,409 [INFO] Step[2250/2713]: training loss : 0.9295816159248352 TRAIN  loss dict:  {'classification_loss': 0.9295816159248352}
2025-01-15 02:47:55,872 [INFO] Step[2300/2713]: training loss : 0.9472280979156494 TRAIN  loss dict:  {'classification_loss': 0.9472280979156494}
2025-01-15 02:48:09,962 [INFO] Step[2350/2713]: training loss : 0.9310595226287842 TRAIN  loss dict:  {'classification_loss': 0.9310595226287842}
2025-01-15 02:48:23,519 [INFO] Step[2400/2713]: training loss : 0.9309625673294067 TRAIN  loss dict:  {'classification_loss': 0.9309625673294067}
2025-01-15 02:48:37,314 [INFO] Step[2450/2713]: training loss : 0.929989538192749 TRAIN  loss dict:  {'classification_loss': 0.929989538192749}
2025-01-15 02:48:50,894 [INFO] Step[2500/2713]: training loss : 0.9305398070812225 TRAIN  loss dict:  {'classification_loss': 0.9305398070812225}
2025-01-15 02:49:04,097 [INFO] Step[2550/2713]: training loss : 0.9319443964958191 TRAIN  loss dict:  {'classification_loss': 0.9319443964958191}
2025-01-15 02:49:17,248 [INFO] Step[2600/2713]: training loss : 0.9307207632064819 TRAIN  loss dict:  {'classification_loss': 0.9307207632064819}
2025-01-15 02:49:30,416 [INFO] Step[2650/2713]: training loss : 0.9299331986904145 TRAIN  loss dict:  {'classification_loss': 0.9299331986904145}
2025-01-15 02:49:43,913 [INFO] Step[2700/2713]: training loss : 0.9323469305038452 TRAIN  loss dict:  {'classification_loss': 0.9323469305038452}
2025-01-15 02:50:59,848 [INFO] Label accuracies statistics:
2025-01-15 02:50:59,848 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.25, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.0, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 02:50:59,850 [INFO] [67] TRAIN  loss: 0.9325530991271287 acc: 0.9995085391325715
2025-01-15 02:50:59,850 [INFO] [67] TRAIN  loss dict: {'classification_loss': 0.9325530991271287}
2025-01-15 02:50:59,850 [INFO] [67] VALIDATION loss: 1.8297347335663057 VALIDATION acc: 0.8025078369905956
2025-01-15 02:50:59,850 [INFO] [67] VALIDATION loss dict: {'classification_loss': 1.8297347335663057}
2025-01-15 02:50:59,851 [INFO] 
2025-01-15 02:51:18,164 [INFO] Step[50/2713]: training loss : 0.9303875231742859 TRAIN  loss dict:  {'classification_loss': 0.9303875231742859}
2025-01-15 02:51:31,856 [INFO] Step[100/2713]: training loss : 0.9306839418411255 TRAIN  loss dict:  {'classification_loss': 0.9306839418411255}
2025-01-15 02:51:45,043 [INFO] Step[150/2713]: training loss : 0.9307173085212708 TRAIN  loss dict:  {'classification_loss': 0.9307173085212708}
2025-01-15 02:51:58,520 [INFO] Step[200/2713]: training loss : 0.930367443561554 TRAIN  loss dict:  {'classification_loss': 0.930367443561554}
2025-01-15 02:52:12,122 [INFO] Step[250/2713]: training loss : 0.9301295590400696 TRAIN  loss dict:  {'classification_loss': 0.9301295590400696}
2025-01-15 02:52:25,316 [INFO] Step[300/2713]: training loss : 0.9297947990894317 TRAIN  loss dict:  {'classification_loss': 0.9297947990894317}
2025-01-15 02:52:38,725 [INFO] Step[350/2713]: training loss : 0.9388818061351776 TRAIN  loss dict:  {'classification_loss': 0.9388818061351776}
2025-01-15 02:52:52,152 [INFO] Step[400/2713]: training loss : 0.929145575761795 TRAIN  loss dict:  {'classification_loss': 0.929145575761795}
2025-01-15 02:53:05,851 [INFO] Step[450/2713]: training loss : 0.929612146615982 TRAIN  loss dict:  {'classification_loss': 0.929612146615982}
2025-01-15 02:53:19,623 [INFO] Step[500/2713]: training loss : 0.9329850971698761 TRAIN  loss dict:  {'classification_loss': 0.9329850971698761}
2025-01-15 02:53:33,209 [INFO] Step[550/2713]: training loss : 0.9332861375808715 TRAIN  loss dict:  {'classification_loss': 0.9332861375808715}
2025-01-15 02:53:47,103 [INFO] Step[600/2713]: training loss : 0.9297605323791504 TRAIN  loss dict:  {'classification_loss': 0.9297605323791504}
2025-01-15 02:54:00,549 [INFO] Step[650/2713]: training loss : 0.9314998853206634 TRAIN  loss dict:  {'classification_loss': 0.9314998853206634}
2025-01-15 02:54:14,125 [INFO] Step[700/2713]: training loss : 0.930051999092102 TRAIN  loss dict:  {'classification_loss': 0.930051999092102}
2025-01-15 02:54:27,667 [INFO] Step[750/2713]: training loss : 0.9299852240085602 TRAIN  loss dict:  {'classification_loss': 0.9299852240085602}
2025-01-15 02:54:41,171 [INFO] Step[800/2713]: training loss : 0.9310506010055541 TRAIN  loss dict:  {'classification_loss': 0.9310506010055541}
2025-01-15 02:54:54,340 [INFO] Step[850/2713]: training loss : 0.9318302273750305 TRAIN  loss dict:  {'classification_loss': 0.9318302273750305}
2025-01-15 02:55:07,749 [INFO] Step[900/2713]: training loss : 0.9309673714637756 TRAIN  loss dict:  {'classification_loss': 0.9309673714637756}
2025-01-15 02:55:21,102 [INFO] Step[950/2713]: training loss : 0.9303863215446472 TRAIN  loss dict:  {'classification_loss': 0.9303863215446472}
2025-01-15 02:55:35,159 [INFO] Step[1000/2713]: training loss : 0.9297769558429718 TRAIN  loss dict:  {'classification_loss': 0.9297769558429718}
2025-01-15 02:55:49,017 [INFO] Step[1050/2713]: training loss : 0.9303273379802703 TRAIN  loss dict:  {'classification_loss': 0.9303273379802703}
2025-01-15 02:56:02,145 [INFO] Step[1100/2713]: training loss : 0.9422904849052429 TRAIN  loss dict:  {'classification_loss': 0.9422904849052429}
2025-01-15 02:56:15,628 [INFO] Step[1150/2713]: training loss : 0.9297858035564422 TRAIN  loss dict:  {'classification_loss': 0.9297858035564422}
2025-01-15 02:56:29,506 [INFO] Step[1200/2713]: training loss : 0.9370147335529327 TRAIN  loss dict:  {'classification_loss': 0.9370147335529327}
2025-01-15 02:56:43,268 [INFO] Step[1250/2713]: training loss : 0.9332689702510834 TRAIN  loss dict:  {'classification_loss': 0.9332689702510834}
2025-01-15 02:56:57,401 [INFO] Step[1300/2713]: training loss : 0.9294596660137177 TRAIN  loss dict:  {'classification_loss': 0.9294596660137177}
2025-01-15 02:57:10,946 [INFO] Step[1350/2713]: training loss : 0.9305002844333649 TRAIN  loss dict:  {'classification_loss': 0.9305002844333649}
2025-01-15 02:57:24,771 [INFO] Step[1400/2713]: training loss : 0.9300780999660492 TRAIN  loss dict:  {'classification_loss': 0.9300780999660492}
2025-01-15 02:57:38,438 [INFO] Step[1450/2713]: training loss : 0.9304339492321014 TRAIN  loss dict:  {'classification_loss': 0.9304339492321014}
2025-01-15 02:57:52,101 [INFO] Step[1500/2713]: training loss : 0.931324895620346 TRAIN  loss dict:  {'classification_loss': 0.931324895620346}
2025-01-15 02:58:06,151 [INFO] Step[1550/2713]: training loss : 0.933977757692337 TRAIN  loss dict:  {'classification_loss': 0.933977757692337}
2025-01-15 02:58:19,966 [INFO] Step[1600/2713]: training loss : 0.9305352032184601 TRAIN  loss dict:  {'classification_loss': 0.9305352032184601}
2025-01-15 02:58:33,455 [INFO] Step[1650/2713]: training loss : 0.9301689887046813 TRAIN  loss dict:  {'classification_loss': 0.9301689887046813}
2025-01-15 02:58:46,688 [INFO] Step[1700/2713]: training loss : 0.9305223536491394 TRAIN  loss dict:  {'classification_loss': 0.9305223536491394}
2025-01-15 02:59:00,447 [INFO] Step[1750/2713]: training loss : 0.9316379761695862 TRAIN  loss dict:  {'classification_loss': 0.9316379761695862}
2025-01-15 02:59:13,714 [INFO] Step[1800/2713]: training loss : 0.944980925321579 TRAIN  loss dict:  {'classification_loss': 0.944980925321579}
2025-01-15 02:59:27,239 [INFO] Step[1850/2713]: training loss : 0.9369863045215606 TRAIN  loss dict:  {'classification_loss': 0.9369863045215606}
2025-01-15 02:59:40,646 [INFO] Step[1900/2713]: training loss : 0.9291338968276978 TRAIN  loss dict:  {'classification_loss': 0.9291338968276978}
2025-01-15 02:59:54,370 [INFO] Step[1950/2713]: training loss : 0.933486533164978 TRAIN  loss dict:  {'classification_loss': 0.933486533164978}
2025-01-15 03:00:07,488 [INFO] Step[2000/2713]: training loss : 0.9297988283634185 TRAIN  loss dict:  {'classification_loss': 0.9297988283634185}
2025-01-15 03:00:20,962 [INFO] Step[2050/2713]: training loss : 0.9302939677238464 TRAIN  loss dict:  {'classification_loss': 0.9302939677238464}
2025-01-15 03:00:34,144 [INFO] Step[2100/2713]: training loss : 0.9296813464164734 TRAIN  loss dict:  {'classification_loss': 0.9296813464164734}
2025-01-15 03:00:47,532 [INFO] Step[2150/2713]: training loss : 0.9296752035617828 TRAIN  loss dict:  {'classification_loss': 0.9296752035617828}
2025-01-15 03:01:01,195 [INFO] Step[2200/2713]: training loss : 0.9297640359401703 TRAIN  loss dict:  {'classification_loss': 0.9297640359401703}
2025-01-15 03:01:14,742 [INFO] Step[2250/2713]: training loss : 0.9300017249584198 TRAIN  loss dict:  {'classification_loss': 0.9300017249584198}
2025-01-15 03:01:28,662 [INFO] Step[2300/2713]: training loss : 0.9297399246692657 TRAIN  loss dict:  {'classification_loss': 0.9297399246692657}
2025-01-15 03:01:42,423 [INFO] Step[2350/2713]: training loss : 0.9302944469451905 TRAIN  loss dict:  {'classification_loss': 0.9302944469451905}
2025-01-15 03:01:56,483 [INFO] Step[2400/2713]: training loss : 0.9290534245967865 TRAIN  loss dict:  {'classification_loss': 0.9290534245967865}
2025-01-15 03:02:09,714 [INFO] Step[2450/2713]: training loss : 0.9296649503707886 TRAIN  loss dict:  {'classification_loss': 0.9296649503707886}
2025-01-15 03:02:23,643 [INFO] Step[2500/2713]: training loss : 0.930500841140747 TRAIN  loss dict:  {'classification_loss': 0.930500841140747}
2025-01-15 03:02:37,510 [INFO] Step[2550/2713]: training loss : 0.9312478053569794 TRAIN  loss dict:  {'classification_loss': 0.9312478053569794}
2025-01-15 03:02:51,366 [INFO] Step[2600/2713]: training loss : 0.9299107170104981 TRAIN  loss dict:  {'classification_loss': 0.9299107170104981}
2025-01-15 03:03:04,819 [INFO] Step[2650/2713]: training loss : 0.9299604678153992 TRAIN  loss dict:  {'classification_loss': 0.9299604678153992}
2025-01-15 03:03:17,991 [INFO] Step[2700/2713]: training loss : 0.930244197845459 TRAIN  loss dict:  {'classification_loss': 0.930244197845459}
2025-01-15 03:04:33,839 [INFO] Label accuracies statistics:
2025-01-15 03:04:33,839 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.0, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 0.25, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.25, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 03:04:33,841 [INFO] [68] TRAIN  loss: 0.9314120069140663 acc: 0.9996314043494287
2025-01-15 03:04:33,841 [INFO] [68] TRAIN  loss dict: {'classification_loss': 0.9314120069140663}
2025-01-15 03:04:33,841 [INFO] [68] VALIDATION loss: 1.8717507848838217 VALIDATION acc: 0.7949843260188088
2025-01-15 03:04:33,841 [INFO] [68] VALIDATION loss dict: {'classification_loss': 1.8717507848838217}
2025-01-15 03:04:33,841 [INFO] 
2025-01-15 03:04:52,001 [INFO] Step[50/2713]: training loss : 0.9537862217426301 TRAIN  loss dict:  {'classification_loss': 0.9537862217426301}
2025-01-15 03:05:05,178 [INFO] Step[100/2713]: training loss : 0.9305854094028473 TRAIN  loss dict:  {'classification_loss': 0.9305854094028473}
2025-01-15 03:05:18,717 [INFO] Step[150/2713]: training loss : 0.9310703551769257 TRAIN  loss dict:  {'classification_loss': 0.9310703551769257}
2025-01-15 03:05:32,354 [INFO] Step[200/2713]: training loss : 0.9299009394645691 TRAIN  loss dict:  {'classification_loss': 0.9299009394645691}
2025-01-15 03:05:46,624 [INFO] Step[250/2713]: training loss : 0.9300381326675415 TRAIN  loss dict:  {'classification_loss': 0.9300381326675415}
2025-01-15 03:06:00,187 [INFO] Step[300/2713]: training loss : 0.9302646887302398 TRAIN  loss dict:  {'classification_loss': 0.9302646887302398}
2025-01-15 03:06:13,681 [INFO] Step[350/2713]: training loss : 0.9302359211444855 TRAIN  loss dict:  {'classification_loss': 0.9302359211444855}
2025-01-15 03:06:27,320 [INFO] Step[400/2713]: training loss : 0.9319035124778747 TRAIN  loss dict:  {'classification_loss': 0.9319035124778747}
2025-01-15 03:06:41,032 [INFO] Step[450/2713]: training loss : 0.9303395926952363 TRAIN  loss dict:  {'classification_loss': 0.9303395926952363}
2025-01-15 03:06:54,940 [INFO] Step[500/2713]: training loss : 0.9303867304325104 TRAIN  loss dict:  {'classification_loss': 0.9303867304325104}
2025-01-15 03:07:08,879 [INFO] Step[550/2713]: training loss : 0.9299813401699066 TRAIN  loss dict:  {'classification_loss': 0.9299813401699066}
2025-01-15 03:07:22,370 [INFO] Step[600/2713]: training loss : 0.9292315256595611 TRAIN  loss dict:  {'classification_loss': 0.9292315256595611}
2025-01-15 03:07:35,969 [INFO] Step[650/2713]: training loss : 0.9294969367980958 TRAIN  loss dict:  {'classification_loss': 0.9294969367980958}
2025-01-15 03:07:49,560 [INFO] Step[700/2713]: training loss : 0.9291519975662231 TRAIN  loss dict:  {'classification_loss': 0.9291519975662231}
2025-01-15 03:08:03,635 [INFO] Step[750/2713]: training loss : 0.9301714754104614 TRAIN  loss dict:  {'classification_loss': 0.9301714754104614}
2025-01-15 03:08:17,379 [INFO] Step[800/2713]: training loss : 0.9452014636993408 TRAIN  loss dict:  {'classification_loss': 0.9452014636993408}
2025-01-15 03:08:31,274 [INFO] Step[850/2713]: training loss : 0.9297166049480439 TRAIN  loss dict:  {'classification_loss': 0.9297166049480439}
2025-01-15 03:08:45,070 [INFO] Step[900/2713]: training loss : 0.9559487974643708 TRAIN  loss dict:  {'classification_loss': 0.9559487974643708}
2025-01-15 03:08:59,039 [INFO] Step[950/2713]: training loss : 0.93206458568573 TRAIN  loss dict:  {'classification_loss': 0.93206458568573}
2025-01-15 03:09:12,670 [INFO] Step[1000/2713]: training loss : 0.9303580236434936 TRAIN  loss dict:  {'classification_loss': 0.9303580236434936}
2025-01-15 03:09:25,935 [INFO] Step[1050/2713]: training loss : 0.9427614891529084 TRAIN  loss dict:  {'classification_loss': 0.9427614891529084}
2025-01-15 03:09:39,730 [INFO] Step[1100/2713]: training loss : 0.931562546491623 TRAIN  loss dict:  {'classification_loss': 0.931562546491623}
2025-01-15 03:09:53,800 [INFO] Step[1150/2713]: training loss : 0.9298825991153717 TRAIN  loss dict:  {'classification_loss': 0.9298825991153717}
2025-01-15 03:10:07,496 [INFO] Step[1200/2713]: training loss : 0.9295711851119995 TRAIN  loss dict:  {'classification_loss': 0.9295711851119995}
2025-01-15 03:10:21,343 [INFO] Step[1250/2713]: training loss : 0.9294157564640045 TRAIN  loss dict:  {'classification_loss': 0.9294157564640045}
2025-01-15 03:10:34,747 [INFO] Step[1300/2713]: training loss : 0.9304998648166657 TRAIN  loss dict:  {'classification_loss': 0.9304998648166657}
2025-01-15 03:10:48,941 [INFO] Step[1350/2713]: training loss : 0.9295283043384552 TRAIN  loss dict:  {'classification_loss': 0.9295283043384552}
2025-01-15 03:11:02,810 [INFO] Step[1400/2713]: training loss : 0.9298720288276673 TRAIN  loss dict:  {'classification_loss': 0.9298720288276673}
2025-01-15 03:11:16,321 [INFO] Step[1450/2713]: training loss : 0.9334376406669617 TRAIN  loss dict:  {'classification_loss': 0.9334376406669617}
2025-01-15 03:11:29,918 [INFO] Step[1500/2713]: training loss : 0.9451990962028504 TRAIN  loss dict:  {'classification_loss': 0.9451990962028504}
2025-01-15 03:11:43,457 [INFO] Step[1550/2713]: training loss : 0.9306408989429474 TRAIN  loss dict:  {'classification_loss': 0.9306408989429474}
2025-01-15 03:11:57,111 [INFO] Step[1600/2713]: training loss : 0.9290448212623597 TRAIN  loss dict:  {'classification_loss': 0.9290448212623597}
2025-01-15 03:12:10,689 [INFO] Step[1650/2713]: training loss : 0.9294125485420227 TRAIN  loss dict:  {'classification_loss': 0.9294125485420227}
2025-01-15 03:12:24,280 [INFO] Step[1700/2713]: training loss : 0.9294937515258789 TRAIN  loss dict:  {'classification_loss': 0.9294937515258789}
2025-01-15 03:12:38,474 [INFO] Step[1750/2713]: training loss : 0.9300604498386383 TRAIN  loss dict:  {'classification_loss': 0.9300604498386383}
2025-01-15 03:12:51,720 [INFO] Step[1800/2713]: training loss : 0.9292852926254273 TRAIN  loss dict:  {'classification_loss': 0.9292852926254273}
2025-01-15 03:13:05,339 [INFO] Step[1850/2713]: training loss : 0.9295544278621674 TRAIN  loss dict:  {'classification_loss': 0.9295544278621674}
2025-01-15 03:13:18,780 [INFO] Step[1900/2713]: training loss : 0.9310068726539612 TRAIN  loss dict:  {'classification_loss': 0.9310068726539612}
2025-01-15 03:13:32,319 [INFO] Step[1950/2713]: training loss : 0.9297603821754455 TRAIN  loss dict:  {'classification_loss': 0.9297603821754455}
2025-01-15 03:13:45,908 [INFO] Step[2000/2713]: training loss : 0.9309648406505585 TRAIN  loss dict:  {'classification_loss': 0.9309648406505585}
2025-01-15 03:14:00,023 [INFO] Step[2050/2713]: training loss : 0.9297038030624389 TRAIN  loss dict:  {'classification_loss': 0.9297038030624389}
2025-01-15 03:14:13,535 [INFO] Step[2100/2713]: training loss : 0.9296231627464294 TRAIN  loss dict:  {'classification_loss': 0.9296231627464294}
2025-01-15 03:14:27,515 [INFO] Step[2150/2713]: training loss : 0.9290595042705536 TRAIN  loss dict:  {'classification_loss': 0.9290595042705536}
2025-01-15 03:14:41,445 [INFO] Step[2200/2713]: training loss : 0.9319149029254913 TRAIN  loss dict:  {'classification_loss': 0.9319149029254913}
2025-01-15 03:14:55,290 [INFO] Step[2250/2713]: training loss : 0.9301891791820526 TRAIN  loss dict:  {'classification_loss': 0.9301891791820526}
2025-01-15 03:15:08,941 [INFO] Step[2300/2713]: training loss : 0.9307712173461914 TRAIN  loss dict:  {'classification_loss': 0.9307712173461914}
2025-01-15 03:15:22,376 [INFO] Step[2350/2713]: training loss : 0.9329795467853547 TRAIN  loss dict:  {'classification_loss': 0.9329795467853547}
2025-01-15 03:15:35,566 [INFO] Step[2400/2713]: training loss : 0.9297307717800141 TRAIN  loss dict:  {'classification_loss': 0.9297307717800141}
2025-01-15 03:15:49,189 [INFO] Step[2450/2713]: training loss : 0.9302840721607208 TRAIN  loss dict:  {'classification_loss': 0.9302840721607208}
2025-01-15 03:16:03,008 [INFO] Step[2500/2713]: training loss : 0.9293125283718109 TRAIN  loss dict:  {'classification_loss': 0.9293125283718109}
2025-01-15 03:16:16,198 [INFO] Step[2550/2713]: training loss : 0.9298812866210937 TRAIN  loss dict:  {'classification_loss': 0.9298812866210937}
2025-01-15 03:16:29,972 [INFO] Step[2600/2713]: training loss : 0.9335103476047516 TRAIN  loss dict:  {'classification_loss': 0.9335103476047516}
2025-01-15 03:16:43,181 [INFO] Step[2650/2713]: training loss : 0.9297222340106964 TRAIN  loss dict:  {'classification_loss': 0.9297222340106964}
2025-01-15 03:16:57,305 [INFO] Step[2700/2713]: training loss : 0.9303797376155853 TRAIN  loss dict:  {'classification_loss': 0.9303797376155853}
2025-01-15 03:18:13,180 [INFO] Label accuracies statistics:
2025-01-15 03:18:13,180 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.0, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 03:18:13,183 [INFO] [69] TRAIN  loss: 0.9319844384980825 acc: 0.9993856739157144
2025-01-15 03:18:13,183 [INFO] [69] TRAIN  loss dict: {'classification_loss': 0.9319844384980825}
2025-01-15 03:18:13,183 [INFO] [69] VALIDATION loss: 1.8832522710239081 VALIDATION acc: 0.7874608150470219
2025-01-15 03:18:13,183 [INFO] [69] VALIDATION loss dict: {'classification_loss': 1.8832522710239081}
2025-01-15 03:18:13,184 [INFO] 
2025-01-15 03:18:31,821 [INFO] Step[50/2713]: training loss : 0.9296226358413696 TRAIN  loss dict:  {'classification_loss': 0.9296226358413696}
2025-01-15 03:18:45,846 [INFO] Step[100/2713]: training loss : 0.92980917096138 TRAIN  loss dict:  {'classification_loss': 0.92980917096138}
2025-01-15 03:18:59,141 [INFO] Step[150/2713]: training loss : 0.9300684201717376 TRAIN  loss dict:  {'classification_loss': 0.9300684201717376}
2025-01-15 03:19:13,091 [INFO] Step[200/2713]: training loss : 0.9303487873077393 TRAIN  loss dict:  {'classification_loss': 0.9303487873077393}
2025-01-15 03:19:26,761 [INFO] Step[250/2713]: training loss : 0.9521752393245697 TRAIN  loss dict:  {'classification_loss': 0.9521752393245697}
2025-01-15 03:19:40,412 [INFO] Step[300/2713]: training loss : 0.9298817205429077 TRAIN  loss dict:  {'classification_loss': 0.9298817205429077}
2025-01-15 03:19:54,460 [INFO] Step[350/2713]: training loss : 0.95748180270195 TRAIN  loss dict:  {'classification_loss': 0.95748180270195}
2025-01-15 03:20:08,019 [INFO] Step[400/2713]: training loss : 0.9295325779914856 TRAIN  loss dict:  {'classification_loss': 0.9295325779914856}
2025-01-15 03:20:22,011 [INFO] Step[450/2713]: training loss : 0.9302011978626251 TRAIN  loss dict:  {'classification_loss': 0.9302011978626251}
2025-01-15 03:20:35,541 [INFO] Step[500/2713]: training loss : 0.9288143849372864 TRAIN  loss dict:  {'classification_loss': 0.9288143849372864}
2025-01-15 03:20:48,992 [INFO] Step[550/2713]: training loss : 0.9302288055419922 TRAIN  loss dict:  {'classification_loss': 0.9302288055419922}
2025-01-15 03:21:02,791 [INFO] Step[600/2713]: training loss : 0.9292592465877533 TRAIN  loss dict:  {'classification_loss': 0.9292592465877533}
2025-01-15 03:21:16,363 [INFO] Step[650/2713]: training loss : 0.9317453694343567 TRAIN  loss dict:  {'classification_loss': 0.9317453694343567}
2025-01-15 03:21:30,080 [INFO] Step[700/2713]: training loss : 0.9296913301944733 TRAIN  loss dict:  {'classification_loss': 0.9296913301944733}
2025-01-15 03:21:44,051 [INFO] Step[750/2713]: training loss : 0.9316659510135651 TRAIN  loss dict:  {'classification_loss': 0.9316659510135651}
2025-01-15 03:21:57,860 [INFO] Step[800/2713]: training loss : 0.9292964386940002 TRAIN  loss dict:  {'classification_loss': 0.9292964386940002}
2025-01-15 03:22:11,677 [INFO] Step[850/2713]: training loss : 0.9292810845375061 TRAIN  loss dict:  {'classification_loss': 0.9292810845375061}
2025-01-15 03:22:25,860 [INFO] Step[900/2713]: training loss : 0.9292176222801208 TRAIN  loss dict:  {'classification_loss': 0.9292176222801208}
2025-01-15 03:22:39,517 [INFO] Step[950/2713]: training loss : 0.9304369151592254 TRAIN  loss dict:  {'classification_loss': 0.9304369151592254}
2025-01-15 03:22:53,009 [INFO] Step[1000/2713]: training loss : 0.9293929922580719 TRAIN  loss dict:  {'classification_loss': 0.9293929922580719}
2025-01-15 03:23:07,195 [INFO] Step[1050/2713]: training loss : 0.9372552609443665 TRAIN  loss dict:  {'classification_loss': 0.9372552609443665}
2025-01-15 03:23:21,052 [INFO] Step[1100/2713]: training loss : 0.9297335267066955 TRAIN  loss dict:  {'classification_loss': 0.9297335267066955}
2025-01-15 03:23:34,285 [INFO] Step[1150/2713]: training loss : 0.9297618091106414 TRAIN  loss dict:  {'classification_loss': 0.9297618091106414}
2025-01-15 03:23:47,807 [INFO] Step[1200/2713]: training loss : 0.9295840525627136 TRAIN  loss dict:  {'classification_loss': 0.9295840525627136}
2025-01-15 03:24:01,686 [INFO] Step[1250/2713]: training loss : 0.9294041192531586 TRAIN  loss dict:  {'classification_loss': 0.9294041192531586}
2025-01-15 03:24:15,767 [INFO] Step[1300/2713]: training loss : 0.93138392329216 TRAIN  loss dict:  {'classification_loss': 0.93138392329216}
2025-01-15 03:24:29,022 [INFO] Step[1350/2713]: training loss : 0.92972003698349 TRAIN  loss dict:  {'classification_loss': 0.92972003698349}
2025-01-15 03:24:42,217 [INFO] Step[1400/2713]: training loss : 0.9295336711406708 TRAIN  loss dict:  {'classification_loss': 0.9295336711406708}
2025-01-15 03:24:55,945 [INFO] Step[1450/2713]: training loss : 0.9298979067802429 TRAIN  loss dict:  {'classification_loss': 0.9298979067802429}
2025-01-15 03:25:09,793 [INFO] Step[1500/2713]: training loss : 0.9296558749675751 TRAIN  loss dict:  {'classification_loss': 0.9296558749675751}
2025-01-15 03:25:23,873 [INFO] Step[1550/2713]: training loss : 0.933824315071106 TRAIN  loss dict:  {'classification_loss': 0.933824315071106}
2025-01-15 03:25:37,582 [INFO] Step[1600/2713]: training loss : 0.931392867565155 TRAIN  loss dict:  {'classification_loss': 0.931392867565155}
2025-01-15 03:25:50,792 [INFO] Step[1650/2713]: training loss : 0.9287118279933929 TRAIN  loss dict:  {'classification_loss': 0.9287118279933929}
2025-01-15 03:26:04,412 [INFO] Step[1700/2713]: training loss : 0.9307587778568268 TRAIN  loss dict:  {'classification_loss': 0.9307587778568268}
2025-01-15 03:26:18,090 [INFO] Step[1750/2713]: training loss : 0.9299322056770325 TRAIN  loss dict:  {'classification_loss': 0.9299322056770325}
2025-01-15 03:26:31,572 [INFO] Step[1800/2713]: training loss : 0.9679828202724456 TRAIN  loss dict:  {'classification_loss': 0.9679828202724456}
2025-01-15 03:26:45,036 [INFO] Step[1850/2713]: training loss : 0.9304763793945312 TRAIN  loss dict:  {'classification_loss': 0.9304763793945312}
2025-01-15 03:26:59,009 [INFO] Step[1900/2713]: training loss : 0.9311387979984284 TRAIN  loss dict:  {'classification_loss': 0.9311387979984284}
2025-01-15 03:27:12,684 [INFO] Step[1950/2713]: training loss : 0.9356191277503967 TRAIN  loss dict:  {'classification_loss': 0.9356191277503967}
2025-01-15 03:27:26,321 [INFO] Step[2000/2713]: training loss : 0.9310755872726441 TRAIN  loss dict:  {'classification_loss': 0.9310755872726441}
2025-01-15 03:27:39,836 [INFO] Step[2050/2713]: training loss : 0.9304802656173706 TRAIN  loss dict:  {'classification_loss': 0.9304802656173706}
2025-01-15 03:27:53,380 [INFO] Step[2100/2713]: training loss : 0.9296240544319153 TRAIN  loss dict:  {'classification_loss': 0.9296240544319153}
2025-01-15 03:28:07,480 [INFO] Step[2150/2713]: training loss : 0.9305242300033569 TRAIN  loss dict:  {'classification_loss': 0.9305242300033569}
2025-01-15 03:28:21,378 [INFO] Step[2200/2713]: training loss : 0.9552298319339753 TRAIN  loss dict:  {'classification_loss': 0.9552298319339753}
2025-01-15 03:28:34,978 [INFO] Step[2250/2713]: training loss : 0.9293442249298096 TRAIN  loss dict:  {'classification_loss': 0.9293442249298096}
2025-01-15 03:28:48,523 [INFO] Step[2300/2713]: training loss : 0.930726865530014 TRAIN  loss dict:  {'classification_loss': 0.930726865530014}
2025-01-15 03:29:02,202 [INFO] Step[2350/2713]: training loss : 0.9323334944248199 TRAIN  loss dict:  {'classification_loss': 0.9323334944248199}
2025-01-15 03:29:15,981 [INFO] Step[2400/2713]: training loss : 0.9295395183563232 TRAIN  loss dict:  {'classification_loss': 0.9295395183563232}
2025-01-15 03:29:30,076 [INFO] Step[2450/2713]: training loss : 0.9596961545944214 TRAIN  loss dict:  {'classification_loss': 0.9596961545944214}
2025-01-15 03:29:43,925 [INFO] Step[2500/2713]: training loss : 0.9360602033138276 TRAIN  loss dict:  {'classification_loss': 0.9360602033138276}
2025-01-15 03:29:57,467 [INFO] Step[2550/2713]: training loss : 0.9294841599464416 TRAIN  loss dict:  {'classification_loss': 0.9294841599464416}
2025-01-15 03:30:10,921 [INFO] Step[2600/2713]: training loss : 0.9517304563522339 TRAIN  loss dict:  {'classification_loss': 0.9517304563522339}
2025-01-15 03:30:24,179 [INFO] Step[2650/2713]: training loss : 0.929593608379364 TRAIN  loss dict:  {'classification_loss': 0.929593608379364}
2025-01-15 03:30:37,680 [INFO] Step[2700/2713]: training loss : 0.9311648714542389 TRAIN  loss dict:  {'classification_loss': 0.9311648714542389}
2025-01-15 03:31:54,358 [INFO] Label accuracies statistics:
2025-01-15 03:31:54,359 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.5, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.5, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 03:31:54,360 [INFO] [70] TRAIN  loss: 0.9335262290542959 acc: 0.9990170782651432
2025-01-15 03:31:54,360 [INFO] [70] TRAIN  loss dict: {'classification_loss': 0.9335262290542959}
2025-01-15 03:31:54,361 [INFO] [70] VALIDATION loss: 1.85458507125539 VALIDATION acc: 0.7943573667711599
2025-01-15 03:31:54,361 [INFO] [70] VALIDATION loss dict: {'classification_loss': 1.85458507125539}
2025-01-15 03:31:54,361 [INFO] 
2025-01-15 03:32:12,753 [INFO] Step[50/2713]: training loss : 0.9334572327136993 TRAIN  loss dict:  {'classification_loss': 0.9334572327136993}
2025-01-15 03:32:26,578 [INFO] Step[100/2713]: training loss : 0.9303378343582154 TRAIN  loss dict:  {'classification_loss': 0.9303378343582154}
2025-01-15 03:32:40,314 [INFO] Step[150/2713]: training loss : 0.9308497619628906 TRAIN  loss dict:  {'classification_loss': 0.9308497619628906}
2025-01-15 03:32:54,106 [INFO] Step[200/2713]: training loss : 0.9287919843196869 TRAIN  loss dict:  {'classification_loss': 0.9287919843196869}
2025-01-15 03:33:07,653 [INFO] Step[250/2713]: training loss : 0.9578920209407806 TRAIN  loss dict:  {'classification_loss': 0.9578920209407806}
2025-01-15 03:33:21,536 [INFO] Step[300/2713]: training loss : 0.9295942211151123 TRAIN  loss dict:  {'classification_loss': 0.9295942211151123}
2025-01-15 03:33:35,342 [INFO] Step[350/2713]: training loss : 0.931092973947525 TRAIN  loss dict:  {'classification_loss': 0.931092973947525}
2025-01-15 03:33:48,818 [INFO] Step[400/2713]: training loss : 0.9289551186561584 TRAIN  loss dict:  {'classification_loss': 0.9289551186561584}
2025-01-15 03:34:02,627 [INFO] Step[450/2713]: training loss : 0.9346387839317322 TRAIN  loss dict:  {'classification_loss': 0.9346387839317322}
2025-01-15 03:34:16,365 [INFO] Step[500/2713]: training loss : 0.933111093044281 TRAIN  loss dict:  {'classification_loss': 0.933111093044281}
2025-01-15 03:34:29,849 [INFO] Step[550/2713]: training loss : 0.9304282021522522 TRAIN  loss dict:  {'classification_loss': 0.9304282021522522}
2025-01-15 03:34:43,033 [INFO] Step[600/2713]: training loss : 0.9295778822898865 TRAIN  loss dict:  {'classification_loss': 0.9295778822898865}
2025-01-15 03:34:56,556 [INFO] Step[650/2713]: training loss : 0.9300433361530304 TRAIN  loss dict:  {'classification_loss': 0.9300433361530304}
2025-01-15 03:35:10,316 [INFO] Step[700/2713]: training loss : 0.9300717413425446 TRAIN  loss dict:  {'classification_loss': 0.9300717413425446}
2025-01-15 03:35:24,240 [INFO] Step[750/2713]: training loss : 0.9297821497917176 TRAIN  loss dict:  {'classification_loss': 0.9297821497917176}
2025-01-15 03:35:38,141 [INFO] Step[800/2713]: training loss : 0.9284277617931366 TRAIN  loss dict:  {'classification_loss': 0.9284277617931366}
2025-01-15 03:35:51,849 [INFO] Step[850/2713]: training loss : 0.9313754260540008 TRAIN  loss dict:  {'classification_loss': 0.9313754260540008}
2025-01-15 03:36:05,428 [INFO] Step[900/2713]: training loss : 0.928467663526535 TRAIN  loss dict:  {'classification_loss': 0.928467663526535}
2025-01-15 03:36:19,597 [INFO] Step[950/2713]: training loss : 0.9284319758415223 TRAIN  loss dict:  {'classification_loss': 0.9284319758415223}
2025-01-15 03:36:33,767 [INFO] Step[1000/2713]: training loss : 0.9288740253448486 TRAIN  loss dict:  {'classification_loss': 0.9288740253448486}
2025-01-15 03:36:47,117 [INFO] Step[1050/2713]: training loss : 0.9296787416934967 TRAIN  loss dict:  {'classification_loss': 0.9296787416934967}
2025-01-15 03:37:00,921 [INFO] Step[1100/2713]: training loss : 0.9310875678062439 TRAIN  loss dict:  {'classification_loss': 0.9310875678062439}
2025-01-15 03:37:14,860 [INFO] Step[1150/2713]: training loss : 0.9315074515342713 TRAIN  loss dict:  {'classification_loss': 0.9315074515342713}
2025-01-15 03:37:28,460 [INFO] Step[1200/2713]: training loss : 0.9337895405292511 TRAIN  loss dict:  {'classification_loss': 0.9337895405292511}
2025-01-15 03:37:42,117 [INFO] Step[1250/2713]: training loss : 0.9312653565406799 TRAIN  loss dict:  {'classification_loss': 0.9312653565406799}
2025-01-15 03:37:55,459 [INFO] Step[1300/2713]: training loss : 0.9298598837852478 TRAIN  loss dict:  {'classification_loss': 0.9298598837852478}
2025-01-15 03:38:08,809 [INFO] Step[1350/2713]: training loss : 0.9290409755706787 TRAIN  loss dict:  {'classification_loss': 0.9290409755706787}
2025-01-15 03:38:22,556 [INFO] Step[1400/2713]: training loss : 0.9360454273223877 TRAIN  loss dict:  {'classification_loss': 0.9360454273223877}
2025-01-15 03:38:36,780 [INFO] Step[1450/2713]: training loss : 0.9288805866241455 TRAIN  loss dict:  {'classification_loss': 0.9288805866241455}
2025-01-15 03:38:50,696 [INFO] Step[1500/2713]: training loss : 0.9293399202823639 TRAIN  loss dict:  {'classification_loss': 0.9293399202823639}
2025-01-15 03:39:04,286 [INFO] Step[1550/2713]: training loss : 0.9285509490966797 TRAIN  loss dict:  {'classification_loss': 0.9285509490966797}
2025-01-15 03:39:17,443 [INFO] Step[1600/2713]: training loss : 0.9291423499584198 TRAIN  loss dict:  {'classification_loss': 0.9291423499584198}
2025-01-15 03:39:31,402 [INFO] Step[1650/2713]: training loss : 0.9392722988128662 TRAIN  loss dict:  {'classification_loss': 0.9392722988128662}
2025-01-15 03:39:45,092 [INFO] Step[1700/2713]: training loss : 0.9292100691795349 TRAIN  loss dict:  {'classification_loss': 0.9292100691795349}
2025-01-15 03:39:58,559 [INFO] Step[1750/2713]: training loss : 0.9292486083507537 TRAIN  loss dict:  {'classification_loss': 0.9292486083507537}
2025-01-15 03:40:12,359 [INFO] Step[1800/2713]: training loss : 0.9291532397270202 TRAIN  loss dict:  {'classification_loss': 0.9291532397270202}
2025-01-15 03:40:25,509 [INFO] Step[1850/2713]: training loss : 0.928793123960495 TRAIN  loss dict:  {'classification_loss': 0.928793123960495}
2025-01-15 03:40:38,990 [INFO] Step[1900/2713]: training loss : 0.943605363368988 TRAIN  loss dict:  {'classification_loss': 0.943605363368988}
2025-01-15 03:40:52,644 [INFO] Step[1950/2713]: training loss : 0.9293520617485046 TRAIN  loss dict:  {'classification_loss': 0.9293520617485046}
2025-01-15 03:41:06,463 [INFO] Step[2000/2713]: training loss : 0.9328740406036377 TRAIN  loss dict:  {'classification_loss': 0.9328740406036377}
2025-01-15 03:41:20,408 [INFO] Step[2050/2713]: training loss : 0.9293356311321258 TRAIN  loss dict:  {'classification_loss': 0.9293356311321258}
2025-01-15 03:41:33,976 [INFO] Step[2100/2713]: training loss : 0.9347643256187439 TRAIN  loss dict:  {'classification_loss': 0.9347643256187439}
2025-01-15 03:41:47,512 [INFO] Step[2150/2713]: training loss : 0.9285461735725403 TRAIN  loss dict:  {'classification_loss': 0.9285461735725403}
2025-01-15 03:42:01,390 [INFO] Step[2200/2713]: training loss : 0.9296529138088226 TRAIN  loss dict:  {'classification_loss': 0.9296529138088226}
2025-01-15 03:42:15,164 [INFO] Step[2250/2713]: training loss : 0.9398494386672973 TRAIN  loss dict:  {'classification_loss': 0.9398494386672973}
2025-01-15 03:42:28,840 [INFO] Step[2300/2713]: training loss : 0.9385927259922028 TRAIN  loss dict:  {'classification_loss': 0.9385927259922028}
2025-01-15 03:42:42,796 [INFO] Step[2350/2713]: training loss : 0.9295103061199188 TRAIN  loss dict:  {'classification_loss': 0.9295103061199188}
2025-01-15 03:42:56,780 [INFO] Step[2400/2713]: training loss : 0.9290882909297943 TRAIN  loss dict:  {'classification_loss': 0.9290882909297943}
2025-01-15 03:43:10,732 [INFO] Step[2450/2713]: training loss : 0.9316869413852692 TRAIN  loss dict:  {'classification_loss': 0.9316869413852692}
2025-01-15 03:43:24,099 [INFO] Step[2500/2713]: training loss : 0.9336019170284271 TRAIN  loss dict:  {'classification_loss': 0.9336019170284271}
2025-01-15 03:43:38,086 [INFO] Step[2550/2713]: training loss : 0.9313790082931519 TRAIN  loss dict:  {'classification_loss': 0.9313790082931519}
2025-01-15 03:43:51,278 [INFO] Step[2600/2713]: training loss : 0.9292036533355713 TRAIN  loss dict:  {'classification_loss': 0.9292036533355713}
2025-01-15 03:44:04,466 [INFO] Step[2650/2713]: training loss : 0.9544382429122925 TRAIN  loss dict:  {'classification_loss': 0.9544382429122925}
2025-01-15 03:44:18,688 [INFO] Step[2700/2713]: training loss : 0.9298679721355438 TRAIN  loss dict:  {'classification_loss': 0.9298679721355438}
2025-01-15 03:45:34,979 [INFO] Label accuracies statistics:
2025-01-15 03:45:34,979 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.25, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 03:45:34,981 [INFO] [71] TRAIN  loss: 0.9320856425712085 acc: 0.9991399434820002
2025-01-15 03:45:34,981 [INFO] [71] TRAIN  loss dict: {'classification_loss': 0.9320856425712085}
2025-01-15 03:45:34,981 [INFO] [71] VALIDATION loss: 1.7784418416650671 VALIDATION acc: 0.8094043887147335
2025-01-15 03:45:34,981 [INFO] [71] VALIDATION loss dict: {'classification_loss': 1.7784418416650671}
2025-01-15 03:45:34,982 [INFO] 
2025-01-15 03:45:53,550 [INFO] Step[50/2713]: training loss : 0.9302479922771454 TRAIN  loss dict:  {'classification_loss': 0.9302479922771454}
2025-01-15 03:46:06,690 [INFO] Step[100/2713]: training loss : 0.9300735998153686 TRAIN  loss dict:  {'classification_loss': 0.9300735998153686}
2025-01-15 03:46:20,374 [INFO] Step[150/2713]: training loss : 0.9440147411823273 TRAIN  loss dict:  {'classification_loss': 0.9440147411823273}
2025-01-15 03:46:33,814 [INFO] Step[200/2713]: training loss : 0.9290527439117432 TRAIN  loss dict:  {'classification_loss': 0.9290527439117432}
2025-01-15 03:46:47,723 [INFO] Step[250/2713]: training loss : 0.9384135591983795 TRAIN  loss dict:  {'classification_loss': 0.9384135591983795}
2025-01-15 03:47:01,555 [INFO] Step[300/2713]: training loss : 0.9364180314540863 TRAIN  loss dict:  {'classification_loss': 0.9364180314540863}
2025-01-15 03:47:14,992 [INFO] Step[350/2713]: training loss : 0.9296343171596527 TRAIN  loss dict:  {'classification_loss': 0.9296343171596527}
2025-01-15 03:47:28,808 [INFO] Step[400/2713]: training loss : 0.9299784004688263 TRAIN  loss dict:  {'classification_loss': 0.9299784004688263}
2025-01-15 03:47:42,195 [INFO] Step[450/2713]: training loss : 0.9293195402622223 TRAIN  loss dict:  {'classification_loss': 0.9293195402622223}
2025-01-15 03:47:55,441 [INFO] Step[500/2713]: training loss : 0.9293945348262787 TRAIN  loss dict:  {'classification_loss': 0.9293945348262787}
2025-01-15 03:48:09,000 [INFO] Step[550/2713]: training loss : 0.9345903897285461 TRAIN  loss dict:  {'classification_loss': 0.9345903897285461}
2025-01-15 03:48:23,052 [INFO] Step[600/2713]: training loss : 0.9292439687252044 TRAIN  loss dict:  {'classification_loss': 0.9292439687252044}
2025-01-15 03:48:36,658 [INFO] Step[650/2713]: training loss : 0.932442022562027 TRAIN  loss dict:  {'classification_loss': 0.932442022562027}
2025-01-15 03:48:50,451 [INFO] Step[700/2713]: training loss : 0.9287516963481903 TRAIN  loss dict:  {'classification_loss': 0.9287516963481903}
2025-01-15 03:49:04,150 [INFO] Step[750/2713]: training loss : 0.9298891818523407 TRAIN  loss dict:  {'classification_loss': 0.9298891818523407}
2025-01-15 03:49:17,636 [INFO] Step[800/2713]: training loss : 0.9309468126296997 TRAIN  loss dict:  {'classification_loss': 0.9309468126296997}
2025-01-15 03:49:30,806 [INFO] Step[850/2713]: training loss : 0.9301317811012269 TRAIN  loss dict:  {'classification_loss': 0.9301317811012269}
2025-01-15 03:49:44,473 [INFO] Step[900/2713]: training loss : 0.9287694156169891 TRAIN  loss dict:  {'classification_loss': 0.9287694156169891}
2025-01-15 03:49:57,915 [INFO] Step[950/2713]: training loss : 0.92945605635643 TRAIN  loss dict:  {'classification_loss': 0.92945605635643}
2025-01-15 03:50:11,296 [INFO] Step[1000/2713]: training loss : 0.9312088942527771 TRAIN  loss dict:  {'classification_loss': 0.9312088942527771}
2025-01-15 03:50:24,426 [INFO] Step[1050/2713]: training loss : 0.9308035087585449 TRAIN  loss dict:  {'classification_loss': 0.9308035087585449}
2025-01-15 03:50:38,356 [INFO] Step[1100/2713]: training loss : 0.9298695850372315 TRAIN  loss dict:  {'classification_loss': 0.9298695850372315}
2025-01-15 03:50:51,796 [INFO] Step[1150/2713]: training loss : 0.932628663778305 TRAIN  loss dict:  {'classification_loss': 0.932628663778305}
2025-01-15 03:51:05,150 [INFO] Step[1200/2713]: training loss : 0.9309449779987335 TRAIN  loss dict:  {'classification_loss': 0.9309449779987335}
2025-01-15 03:51:19,194 [INFO] Step[1250/2713]: training loss : 0.9293229031562805 TRAIN  loss dict:  {'classification_loss': 0.9293229031562805}
2025-01-15 03:51:32,365 [INFO] Step[1300/2713]: training loss : 0.9287819743156434 TRAIN  loss dict:  {'classification_loss': 0.9287819743156434}
2025-01-15 03:51:46,417 [INFO] Step[1350/2713]: training loss : 0.9282332599163056 TRAIN  loss dict:  {'classification_loss': 0.9282332599163056}
2025-01-15 03:51:59,906 [INFO] Step[1400/2713]: training loss : 0.9290548372268677 TRAIN  loss dict:  {'classification_loss': 0.9290548372268677}
2025-01-15 03:52:13,916 [INFO] Step[1450/2713]: training loss : 0.929945377111435 TRAIN  loss dict:  {'classification_loss': 0.929945377111435}
2025-01-15 03:52:28,043 [INFO] Step[1500/2713]: training loss : 0.9289556694030762 TRAIN  loss dict:  {'classification_loss': 0.9289556694030762}
2025-01-15 03:52:42,177 [INFO] Step[1550/2713]: training loss : 0.9291131246089935 TRAIN  loss dict:  {'classification_loss': 0.9291131246089935}
2025-01-15 03:52:55,425 [INFO] Step[1600/2713]: training loss : 0.9295919525623322 TRAIN  loss dict:  {'classification_loss': 0.9295919525623322}
2025-01-15 03:53:09,354 [INFO] Step[1650/2713]: training loss : 0.9286419713497162 TRAIN  loss dict:  {'classification_loss': 0.9286419713497162}
2025-01-15 03:53:22,550 [INFO] Step[1700/2713]: training loss : 0.9287144374847413 TRAIN  loss dict:  {'classification_loss': 0.9287144374847413}
2025-01-15 03:53:36,127 [INFO] Step[1750/2713]: training loss : 0.9288309240341186 TRAIN  loss dict:  {'classification_loss': 0.9288309240341186}
2025-01-15 03:53:49,888 [INFO] Step[1800/2713]: training loss : 0.931237461566925 TRAIN  loss dict:  {'classification_loss': 0.931237461566925}
2025-01-15 03:54:03,830 [INFO] Step[1850/2713]: training loss : 0.9279444420337677 TRAIN  loss dict:  {'classification_loss': 0.9279444420337677}
2025-01-15 03:54:17,170 [INFO] Step[1900/2713]: training loss : 0.9312613427639007 TRAIN  loss dict:  {'classification_loss': 0.9312613427639007}
2025-01-15 03:54:30,635 [INFO] Step[1950/2713]: training loss : 0.9288291084766388 TRAIN  loss dict:  {'classification_loss': 0.9288291084766388}
2025-01-15 03:54:44,164 [INFO] Step[2000/2713]: training loss : 0.9289742922782898 TRAIN  loss dict:  {'classification_loss': 0.9289742922782898}
2025-01-15 03:54:58,277 [INFO] Step[2050/2713]: training loss : 0.9285254526138306 TRAIN  loss dict:  {'classification_loss': 0.9285254526138306}
2025-01-15 03:55:12,402 [INFO] Step[2100/2713]: training loss : 0.928324909210205 TRAIN  loss dict:  {'classification_loss': 0.928324909210205}
2025-01-15 03:55:25,853 [INFO] Step[2150/2713]: training loss : 0.9296126568317413 TRAIN  loss dict:  {'classification_loss': 0.9296126568317413}
2025-01-15 03:55:39,865 [INFO] Step[2200/2713]: training loss : 0.928501831293106 TRAIN  loss dict:  {'classification_loss': 0.928501831293106}
2025-01-15 03:55:53,040 [INFO] Step[2250/2713]: training loss : 0.9288985395431518 TRAIN  loss dict:  {'classification_loss': 0.9288985395431518}
2025-01-15 03:56:06,204 [INFO] Step[2300/2713]: training loss : 0.9284541678428649 TRAIN  loss dict:  {'classification_loss': 0.9284541678428649}
2025-01-15 03:56:19,873 [INFO] Step[2350/2713]: training loss : 0.9289140331745148 TRAIN  loss dict:  {'classification_loss': 0.9289140331745148}
2025-01-15 03:56:33,814 [INFO] Step[2400/2713]: training loss : 0.9288599348068237 TRAIN  loss dict:  {'classification_loss': 0.9288599348068237}
2025-01-15 03:56:47,456 [INFO] Step[2450/2713]: training loss : 0.9280016255378724 TRAIN  loss dict:  {'classification_loss': 0.9280016255378724}
2025-01-15 03:57:01,415 [INFO] Step[2500/2713]: training loss : 0.9284632289409638 TRAIN  loss dict:  {'classification_loss': 0.9284632289409638}
2025-01-15 03:57:14,988 [INFO] Step[2550/2713]: training loss : 0.9305764937400818 TRAIN  loss dict:  {'classification_loss': 0.9305764937400818}
2025-01-15 03:57:28,546 [INFO] Step[2600/2713]: training loss : 0.9300351953506469 TRAIN  loss dict:  {'classification_loss': 0.9300351953506469}
2025-01-15 03:57:42,104 [INFO] Step[2650/2713]: training loss : 0.928867460489273 TRAIN  loss dict:  {'classification_loss': 0.928867460489273}
2025-01-15 03:57:55,899 [INFO] Step[2700/2713]: training loss : 0.9290919482707978 TRAIN  loss dict:  {'classification_loss': 0.9290919482707978}
2025-01-15 03:59:11,565 [INFO] Label accuracies statistics:
2025-01-15 03:59:11,565 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.5, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.75, 282: 1.0, 283: 0.75, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 03:59:11,567 [INFO] [72] TRAIN  loss: 0.9301597878935589 acc: 0.9996314043494287
2025-01-15 03:59:11,567 [INFO] [72] TRAIN  loss dict: {'classification_loss': 0.9301597878935589}
2025-01-15 03:59:11,567 [INFO] [72] VALIDATION loss: 1.7804634606927858 VALIDATION acc: 0.8175548589341692
2025-01-15 03:59:11,567 [INFO] [72] VALIDATION loss dict: {'classification_loss': 1.7804634606927858}
2025-01-15 03:59:11,567 [INFO] 
2025-01-15 03:59:30,796 [INFO] Step[50/2713]: training loss : 0.92885373711586 TRAIN  loss dict:  {'classification_loss': 0.92885373711586}
2025-01-15 03:59:44,287 [INFO] Step[100/2713]: training loss : 0.9284288716316224 TRAIN  loss dict:  {'classification_loss': 0.9284288716316224}
2025-01-15 03:59:57,419 [INFO] Step[150/2713]: training loss : 0.9294075667858124 TRAIN  loss dict:  {'classification_loss': 0.9294075667858124}
2025-01-15 04:00:10,886 [INFO] Step[200/2713]: training loss : 0.9287889885902405 TRAIN  loss dict:  {'classification_loss': 0.9287889885902405}
2025-01-15 04:00:24,830 [INFO] Step[250/2713]: training loss : 0.9282581496238709 TRAIN  loss dict:  {'classification_loss': 0.9282581496238709}
2025-01-15 04:00:38,107 [INFO] Step[300/2713]: training loss : 0.9293318331241608 TRAIN  loss dict:  {'classification_loss': 0.9293318331241608}
2025-01-15 04:00:51,592 [INFO] Step[350/2713]: training loss : 0.9285360312461853 TRAIN  loss dict:  {'classification_loss': 0.9285360312461853}
2025-01-15 04:01:05,116 [INFO] Step[400/2713]: training loss : 0.9303980851173401 TRAIN  loss dict:  {'classification_loss': 0.9303980851173401}
2025-01-15 04:01:19,060 [INFO] Step[450/2713]: training loss : 0.9279059863090515 TRAIN  loss dict:  {'classification_loss': 0.9279059863090515}
2025-01-15 04:01:32,818 [INFO] Step[500/2713]: training loss : 0.9288330161571503 TRAIN  loss dict:  {'classification_loss': 0.9288330161571503}
2025-01-15 04:01:46,671 [INFO] Step[550/2713]: training loss : 0.9286777591705322 TRAIN  loss dict:  {'classification_loss': 0.9286777591705322}
2025-01-15 04:02:00,561 [INFO] Step[600/2713]: training loss : 0.9290488660335541 TRAIN  loss dict:  {'classification_loss': 0.9290488660335541}
2025-01-15 04:02:14,616 [INFO] Step[650/2713]: training loss : 0.9298976337909699 TRAIN  loss dict:  {'classification_loss': 0.9298976337909699}
2025-01-15 04:02:28,350 [INFO] Step[700/2713]: training loss : 0.9287826371192932 TRAIN  loss dict:  {'classification_loss': 0.9287826371192932}
2025-01-15 04:02:44,997 [INFO] Step[750/2713]: training loss : 0.9460263085365296 TRAIN  loss dict:  {'classification_loss': 0.9460263085365296}
2025-01-15 04:02:59,006 [INFO] Step[800/2713]: training loss : 0.9282449805736541 TRAIN  loss dict:  {'classification_loss': 0.9282449805736541}
2025-01-15 04:03:13,153 [INFO] Step[850/2713]: training loss : 0.9305867671966552 TRAIN  loss dict:  {'classification_loss': 0.9305867671966552}
2025-01-15 04:03:27,021 [INFO] Step[900/2713]: training loss : 0.9302158343791962 TRAIN  loss dict:  {'classification_loss': 0.9302158343791962}
2025-01-15 04:03:40,390 [INFO] Step[950/2713]: training loss : 0.9299956250190735 TRAIN  loss dict:  {'classification_loss': 0.9299956250190735}
2025-01-15 04:03:54,190 [INFO] Step[1000/2713]: training loss : 0.9286690211296081 TRAIN  loss dict:  {'classification_loss': 0.9286690211296081}
2025-01-15 04:04:07,966 [INFO] Step[1050/2713]: training loss : 0.9315207993984223 TRAIN  loss dict:  {'classification_loss': 0.9315207993984223}
2025-01-15 04:04:21,919 [INFO] Step[1100/2713]: training loss : 0.9299545466899872 TRAIN  loss dict:  {'classification_loss': 0.9299545466899872}
2025-01-15 04:04:35,408 [INFO] Step[1150/2713]: training loss : 0.9294749784469605 TRAIN  loss dict:  {'classification_loss': 0.9294749784469605}
2025-01-15 04:04:49,189 [INFO] Step[1200/2713]: training loss : 0.9293883109092712 TRAIN  loss dict:  {'classification_loss': 0.9293883109092712}
2025-01-15 04:05:02,944 [INFO] Step[1250/2713]: training loss : 0.9287270772457122 TRAIN  loss dict:  {'classification_loss': 0.9287270772457122}
2025-01-15 04:05:16,631 [INFO] Step[1300/2713]: training loss : 0.9285404849052429 TRAIN  loss dict:  {'classification_loss': 0.9285404849052429}
2025-01-15 04:05:30,610 [INFO] Step[1350/2713]: training loss : 0.9286663460731507 TRAIN  loss dict:  {'classification_loss': 0.9286663460731507}
2025-01-15 04:05:44,662 [INFO] Step[1400/2713]: training loss : 0.929766651391983 TRAIN  loss dict:  {'classification_loss': 0.929766651391983}
2025-01-15 04:05:58,461 [INFO] Step[1450/2713]: training loss : 0.9292936253547669 TRAIN  loss dict:  {'classification_loss': 0.9292936253547669}
2025-01-15 04:06:11,623 [INFO] Step[1500/2713]: training loss : 0.9289858400821686 TRAIN  loss dict:  {'classification_loss': 0.9289858400821686}
2025-01-15 04:06:25,242 [INFO] Step[1550/2713]: training loss : 0.9295827376842499 TRAIN  loss dict:  {'classification_loss': 0.9295827376842499}
2025-01-15 04:06:39,069 [INFO] Step[1600/2713]: training loss : 0.9287549841403961 TRAIN  loss dict:  {'classification_loss': 0.9287549841403961}
2025-01-15 04:06:52,757 [INFO] Step[1650/2713]: training loss : 0.928378005027771 TRAIN  loss dict:  {'classification_loss': 0.928378005027771}
2025-01-15 04:07:06,303 [INFO] Step[1700/2713]: training loss : 0.9281528985500336 TRAIN  loss dict:  {'classification_loss': 0.9281528985500336}
2025-01-15 04:07:19,567 [INFO] Step[1750/2713]: training loss : 0.9290266931056976 TRAIN  loss dict:  {'classification_loss': 0.9290266931056976}
2025-01-15 04:07:33,682 [INFO] Step[1800/2713]: training loss : 0.9293048965930939 TRAIN  loss dict:  {'classification_loss': 0.9293048965930939}
2025-01-15 04:07:47,271 [INFO] Step[1850/2713]: training loss : 0.9278726053237915 TRAIN  loss dict:  {'classification_loss': 0.9278726053237915}
2025-01-15 04:08:00,795 [INFO] Step[1900/2713]: training loss : 0.9286575186252594 TRAIN  loss dict:  {'classification_loss': 0.9286575186252594}
2025-01-15 04:08:14,463 [INFO] Step[1950/2713]: training loss : 0.9278812670707702 TRAIN  loss dict:  {'classification_loss': 0.9278812670707702}
2025-01-15 04:08:27,840 [INFO] Step[2000/2713]: training loss : 0.928799957036972 TRAIN  loss dict:  {'classification_loss': 0.928799957036972}
2025-01-15 04:08:41,801 [INFO] Step[2050/2713]: training loss : 0.9291320896148682 TRAIN  loss dict:  {'classification_loss': 0.9291320896148682}
2025-01-15 04:08:55,087 [INFO] Step[2100/2713]: training loss : 0.9288659238815308 TRAIN  loss dict:  {'classification_loss': 0.9288659238815308}
2025-01-15 04:09:08,600 [INFO] Step[2150/2713]: training loss : 0.9289512455463409 TRAIN  loss dict:  {'classification_loss': 0.9289512455463409}
2025-01-15 04:09:22,527 [INFO] Step[2200/2713]: training loss : 0.9438451325893402 TRAIN  loss dict:  {'classification_loss': 0.9438451325893402}
2025-01-15 04:09:36,222 [INFO] Step[2250/2713]: training loss : 0.9284543061256408 TRAIN  loss dict:  {'classification_loss': 0.9284543061256408}
2025-01-15 04:09:49,761 [INFO] Step[2300/2713]: training loss : 0.9288978886604309 TRAIN  loss dict:  {'classification_loss': 0.9288978886604309}
2025-01-15 04:10:03,455 [INFO] Step[2350/2713]: training loss : 0.9317445957660675 TRAIN  loss dict:  {'classification_loss': 0.9317445957660675}
2025-01-15 04:10:16,759 [INFO] Step[2400/2713]: training loss : 0.9292374408245087 TRAIN  loss dict:  {'classification_loss': 0.9292374408245087}
2025-01-15 04:10:30,514 [INFO] Step[2450/2713]: training loss : 0.9307780456542969 TRAIN  loss dict:  {'classification_loss': 0.9307780456542969}
2025-01-15 04:10:44,154 [INFO] Step[2500/2713]: training loss : 0.929281919002533 TRAIN  loss dict:  {'classification_loss': 0.929281919002533}
2025-01-15 04:10:57,364 [INFO] Step[2550/2713]: training loss : 0.9305852317810058 TRAIN  loss dict:  {'classification_loss': 0.9305852317810058}
2025-01-15 04:11:11,521 [INFO] Step[2600/2713]: training loss : 0.9301785099506378 TRAIN  loss dict:  {'classification_loss': 0.9301785099506378}
2025-01-15 04:11:25,268 [INFO] Step[2650/2713]: training loss : 0.9322016906738281 TRAIN  loss dict:  {'classification_loss': 0.9322016906738281}
2025-01-15 04:11:38,799 [INFO] Step[2700/2713]: training loss : 0.9284176027774811 TRAIN  loss dict:  {'classification_loss': 0.9284176027774811}
2025-01-15 04:12:54,798 [INFO] Label accuracies statistics:
2025-01-15 04:12:54,798 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.5, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 04:12:54,800 [INFO] [73] TRAIN  loss: 0.9298118365193222 acc: 0.9997542695662858
2025-01-15 04:12:54,800 [INFO] [73] TRAIN  loss dict: {'classification_loss': 0.9298118365193222}
2025-01-15 04:12:54,800 [INFO] [73] VALIDATION loss: 1.8395196974725652 VALIDATION acc: 0.8
2025-01-15 04:12:54,800 [INFO] [73] VALIDATION loss dict: {'classification_loss': 1.8395196974725652}
2025-01-15 04:12:54,800 [INFO] 
2025-01-15 04:13:13,130 [INFO] Step[50/2713]: training loss : 0.939089138507843 TRAIN  loss dict:  {'classification_loss': 0.939089138507843}
2025-01-15 04:13:26,857 [INFO] Step[100/2713]: training loss : 0.9292988395690918 TRAIN  loss dict:  {'classification_loss': 0.9292988395690918}
2025-01-15 04:13:40,982 [INFO] Step[150/2713]: training loss : 0.9475975525379181 TRAIN  loss dict:  {'classification_loss': 0.9475975525379181}
2025-01-15 04:13:54,869 [INFO] Step[200/2713]: training loss : 0.9291464829444885 TRAIN  loss dict:  {'classification_loss': 0.9291464829444885}
2025-01-15 04:14:08,429 [INFO] Step[250/2713]: training loss : 0.9279214096069336 TRAIN  loss dict:  {'classification_loss': 0.9279214096069336}
2025-01-15 04:14:22,223 [INFO] Step[300/2713]: training loss : 0.9292322385311127 TRAIN  loss dict:  {'classification_loss': 0.9292322385311127}
2025-01-15 04:14:35,527 [INFO] Step[350/2713]: training loss : 0.9284562122821808 TRAIN  loss dict:  {'classification_loss': 0.9284562122821808}
2025-01-15 04:14:49,606 [INFO] Step[400/2713]: training loss : 0.9293944871425629 TRAIN  loss dict:  {'classification_loss': 0.9293944871425629}
2025-01-15 04:15:03,473 [INFO] Step[450/2713]: training loss : 0.9293432068824768 TRAIN  loss dict:  {'classification_loss': 0.9293432068824768}
2025-01-15 04:15:17,418 [INFO] Step[500/2713]: training loss : 0.9284053337574005 TRAIN  loss dict:  {'classification_loss': 0.9284053337574005}
2025-01-15 04:15:31,294 [INFO] Step[550/2713]: training loss : 0.928851535320282 TRAIN  loss dict:  {'classification_loss': 0.928851535320282}
2025-01-15 04:15:44,812 [INFO] Step[600/2713]: training loss : 0.9286049938201905 TRAIN  loss dict:  {'classification_loss': 0.9286049938201905}
2025-01-15 04:15:58,334 [INFO] Step[650/2713]: training loss : 0.931699185371399 TRAIN  loss dict:  {'classification_loss': 0.931699185371399}
2025-01-15 04:16:11,971 [INFO] Step[700/2713]: training loss : 0.9289665484428405 TRAIN  loss dict:  {'classification_loss': 0.9289665484428405}
2025-01-15 04:16:25,701 [INFO] Step[750/2713]: training loss : 0.9285688757896423 TRAIN  loss dict:  {'classification_loss': 0.9285688757896423}
2025-01-15 04:16:39,219 [INFO] Step[800/2713]: training loss : 0.9623150312900544 TRAIN  loss dict:  {'classification_loss': 0.9623150312900544}
2025-01-15 04:16:52,843 [INFO] Step[850/2713]: training loss : 0.9313900673389435 TRAIN  loss dict:  {'classification_loss': 0.9313900673389435}
2025-01-15 04:17:06,435 [INFO] Step[900/2713]: training loss : 0.9290516698360443 TRAIN  loss dict:  {'classification_loss': 0.9290516698360443}
2025-01-15 04:17:20,083 [INFO] Step[950/2713]: training loss : 0.928260531425476 TRAIN  loss dict:  {'classification_loss': 0.928260531425476}
2025-01-15 04:17:34,109 [INFO] Step[1000/2713]: training loss : 0.9298011434078216 TRAIN  loss dict:  {'classification_loss': 0.9298011434078216}
2025-01-15 04:17:50,305 [INFO] Step[1050/2713]: training loss : 0.9286406004428863 TRAIN  loss dict:  {'classification_loss': 0.9286406004428863}
2025-01-15 04:18:04,596 [INFO] Step[1100/2713]: training loss : 0.9422808587551117 TRAIN  loss dict:  {'classification_loss': 0.9422808587551117}
2025-01-15 04:18:18,067 [INFO] Step[1150/2713]: training loss : 0.928609231710434 TRAIN  loss dict:  {'classification_loss': 0.928609231710434}
2025-01-15 04:18:31,989 [INFO] Step[1200/2713]: training loss : 0.9301398360729217 TRAIN  loss dict:  {'classification_loss': 0.9301398360729217}
2025-01-15 04:18:45,541 [INFO] Step[1250/2713]: training loss : 0.9288651311397552 TRAIN  loss dict:  {'classification_loss': 0.9288651311397552}
2025-01-15 04:18:59,725 [INFO] Step[1300/2713]: training loss : 0.9296655368804931 TRAIN  loss dict:  {'classification_loss': 0.9296655368804931}
2025-01-15 04:19:13,351 [INFO] Step[1350/2713]: training loss : 0.9289148473739623 TRAIN  loss dict:  {'classification_loss': 0.9289148473739623}
2025-01-15 04:19:27,130 [INFO] Step[1400/2713]: training loss : 0.9287465870380401 TRAIN  loss dict:  {'classification_loss': 0.9287465870380401}
2025-01-15 04:19:40,719 [INFO] Step[1450/2713]: training loss : 0.9298017203807831 TRAIN  loss dict:  {'classification_loss': 0.9298017203807831}
2025-01-15 04:19:54,561 [INFO] Step[1500/2713]: training loss : 0.9319685089588166 TRAIN  loss dict:  {'classification_loss': 0.9319685089588166}
2025-01-15 04:20:07,861 [INFO] Step[1550/2713]: training loss : 0.9284319150447845 TRAIN  loss dict:  {'classification_loss': 0.9284319150447845}
2025-01-15 04:20:21,371 [INFO] Step[1600/2713]: training loss : 0.9314901399612426 TRAIN  loss dict:  {'classification_loss': 0.9314901399612426}
2025-01-15 04:20:35,214 [INFO] Step[1650/2713]: training loss : 0.9290033531188965 TRAIN  loss dict:  {'classification_loss': 0.9290033531188965}
2025-01-15 04:20:48,635 [INFO] Step[1700/2713]: training loss : 0.9296968448162078 TRAIN  loss dict:  {'classification_loss': 0.9296968448162078}
2025-01-15 04:21:01,871 [INFO] Step[1750/2713]: training loss : 0.928930504322052 TRAIN  loss dict:  {'classification_loss': 0.928930504322052}
2025-01-15 04:21:15,029 [INFO] Step[1800/2713]: training loss : 0.9295095586776734 TRAIN  loss dict:  {'classification_loss': 0.9295095586776734}
2025-01-15 04:21:28,226 [INFO] Step[1850/2713]: training loss : 0.929254766702652 TRAIN  loss dict:  {'classification_loss': 0.929254766702652}
2025-01-15 04:21:42,374 [INFO] Step[1900/2713]: training loss : 0.9308111870288849 TRAIN  loss dict:  {'classification_loss': 0.9308111870288849}
2025-01-15 04:21:56,383 [INFO] Step[1950/2713]: training loss : 0.928281227350235 TRAIN  loss dict:  {'classification_loss': 0.928281227350235}
2025-01-15 04:22:09,516 [INFO] Step[2000/2713]: training loss : 0.9368612134456634 TRAIN  loss dict:  {'classification_loss': 0.9368612134456634}
2025-01-15 04:22:23,344 [INFO] Step[2050/2713]: training loss : 0.9333037889003754 TRAIN  loss dict:  {'classification_loss': 0.9333037889003754}
2025-01-15 04:22:36,506 [INFO] Step[2100/2713]: training loss : 0.9293516170978546 TRAIN  loss dict:  {'classification_loss': 0.9293516170978546}
2025-01-15 04:22:50,301 [INFO] Step[2150/2713]: training loss : 0.9286471164226532 TRAIN  loss dict:  {'classification_loss': 0.9286471164226532}
2025-01-15 04:23:03,494 [INFO] Step[2200/2713]: training loss : 0.9287236452102661 TRAIN  loss dict:  {'classification_loss': 0.9287236452102661}
2025-01-15 04:23:16,726 [INFO] Step[2250/2713]: training loss : 0.9298434925079345 TRAIN  loss dict:  {'classification_loss': 0.9298434925079345}
2025-01-15 04:23:30,306 [INFO] Step[2300/2713]: training loss : 0.9292409539222717 TRAIN  loss dict:  {'classification_loss': 0.9292409539222717}
2025-01-15 04:23:44,142 [INFO] Step[2350/2713]: training loss : 0.9288705337047577 TRAIN  loss dict:  {'classification_loss': 0.9288705337047577}
2025-01-15 04:23:57,888 [INFO] Step[2400/2713]: training loss : 0.9292370343208313 TRAIN  loss dict:  {'classification_loss': 0.9292370343208313}
2025-01-15 04:24:11,896 [INFO] Step[2450/2713]: training loss : 0.9292320871353149 TRAIN  loss dict:  {'classification_loss': 0.9292320871353149}
2025-01-15 04:24:25,093 [INFO] Step[2500/2713]: training loss : 0.9292038953304291 TRAIN  loss dict:  {'classification_loss': 0.9292038953304291}
2025-01-15 04:24:39,000 [INFO] Step[2550/2713]: training loss : 0.9292753207683563 TRAIN  loss dict:  {'classification_loss': 0.9292753207683563}
2025-01-15 04:24:53,081 [INFO] Step[2600/2713]: training loss : 0.9292580270767212 TRAIN  loss dict:  {'classification_loss': 0.9292580270767212}
2025-01-15 04:25:06,638 [INFO] Step[2650/2713]: training loss : 0.9298110783100129 TRAIN  loss dict:  {'classification_loss': 0.9298110783100129}
2025-01-15 04:25:20,404 [INFO] Step[2700/2713]: training loss : 0.9337756538391113 TRAIN  loss dict:  {'classification_loss': 0.9337756538391113}
2025-01-15 04:26:36,824 [INFO] Label accuracies statistics:
2025-01-15 04:26:36,824 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 04:26:36,826 [INFO] [74] TRAIN  loss: 0.9309780133020996 acc: 0.9995085391325715
2025-01-15 04:26:36,826 [INFO] [74] TRAIN  loss dict: {'classification_loss': 0.9309780133020996}
2025-01-15 04:26:36,826 [INFO] [74] VALIDATION loss: 1.8273815291940718 VALIDATION acc: 0.799373040752351
2025-01-15 04:26:36,826 [INFO] [74] VALIDATION loss dict: {'classification_loss': 1.8273815291940718}
2025-01-15 04:26:36,827 [INFO] 
2025-01-15 04:26:55,590 [INFO] Step[50/2713]: training loss : 0.9286997318267822 TRAIN  loss dict:  {'classification_loss': 0.9286997318267822}
2025-01-15 04:27:08,894 [INFO] Step[100/2713]: training loss : 0.9288910901546479 TRAIN  loss dict:  {'classification_loss': 0.9288910901546479}
2025-01-15 04:27:22,772 [INFO] Step[150/2713]: training loss : 0.928830338716507 TRAIN  loss dict:  {'classification_loss': 0.928830338716507}
2025-01-15 04:27:36,304 [INFO] Step[200/2713]: training loss : 0.9286088669300079 TRAIN  loss dict:  {'classification_loss': 0.9286088669300079}
2025-01-15 04:27:49,489 [INFO] Step[250/2713]: training loss : 0.9288978064060212 TRAIN  loss dict:  {'classification_loss': 0.9288978064060212}
2025-01-15 04:28:03,269 [INFO] Step[300/2713]: training loss : 0.9288776159286499 TRAIN  loss dict:  {'classification_loss': 0.9288776159286499}
2025-01-15 04:28:16,770 [INFO] Step[350/2713]: training loss : 0.9347520291805267 TRAIN  loss dict:  {'classification_loss': 0.9347520291805267}
2025-01-15 04:28:30,317 [INFO] Step[400/2713]: training loss : 0.9291313576698303 TRAIN  loss dict:  {'classification_loss': 0.9291313576698303}
2025-01-15 04:28:43,830 [INFO] Step[450/2713]: training loss : 0.9286334300041199 TRAIN  loss dict:  {'classification_loss': 0.9286334300041199}
2025-01-15 04:28:57,498 [INFO] Step[500/2713]: training loss : 0.9284772133827209 TRAIN  loss dict:  {'classification_loss': 0.9284772133827209}
2025-01-15 04:29:10,926 [INFO] Step[550/2713]: training loss : 0.9288653826713562 TRAIN  loss dict:  {'classification_loss': 0.9288653826713562}
2025-01-15 04:29:24,993 [INFO] Step[600/2713]: training loss : 0.9283414888381958 TRAIN  loss dict:  {'classification_loss': 0.9283414888381958}
2025-01-15 04:29:38,846 [INFO] Step[650/2713]: training loss : 0.9284611439704895 TRAIN  loss dict:  {'classification_loss': 0.9284611439704895}
2025-01-15 04:29:54,869 [INFO] Step[700/2713]: training loss : 0.9293699860572815 TRAIN  loss dict:  {'classification_loss': 0.9293699860572815}
2025-01-15 04:30:09,092 [INFO] Step[750/2713]: training loss : 0.9284182119369507 TRAIN  loss dict:  {'classification_loss': 0.9284182119369507}
2025-01-15 04:30:23,264 [INFO] Step[800/2713]: training loss : 0.9289316201210022 TRAIN  loss dict:  {'classification_loss': 0.9289316201210022}
2025-01-15 04:30:36,941 [INFO] Step[850/2713]: training loss : 0.9286794435977935 TRAIN  loss dict:  {'classification_loss': 0.9286794435977935}
2025-01-15 04:30:50,746 [INFO] Step[900/2713]: training loss : 0.9423287200927735 TRAIN  loss dict:  {'classification_loss': 0.9423287200927735}
2025-01-15 04:31:05,008 [INFO] Step[950/2713]: training loss : 0.9323370277881622 TRAIN  loss dict:  {'classification_loss': 0.9323370277881622}
2025-01-15 04:31:19,232 [INFO] Step[1000/2713]: training loss : 0.9293044030666351 TRAIN  loss dict:  {'classification_loss': 0.9293044030666351}
2025-01-15 04:31:33,169 [INFO] Step[1050/2713]: training loss : 0.9287102067470551 TRAIN  loss dict:  {'classification_loss': 0.9287102067470551}
2025-01-15 04:31:46,810 [INFO] Step[1100/2713]: training loss : 0.9295297622680664 TRAIN  loss dict:  {'classification_loss': 0.9295297622680664}
2025-01-15 04:32:00,216 [INFO] Step[1150/2713]: training loss : 0.9288682687282562 TRAIN  loss dict:  {'classification_loss': 0.9288682687282562}
2025-01-15 04:32:14,090 [INFO] Step[1200/2713]: training loss : 0.9285373413562774 TRAIN  loss dict:  {'classification_loss': 0.9285373413562774}
2025-01-15 04:32:27,472 [INFO] Step[1250/2713]: training loss : 0.9294944381713868 TRAIN  loss dict:  {'classification_loss': 0.9294944381713868}
2025-01-15 04:32:41,008 [INFO] Step[1300/2713]: training loss : 0.9283319187164306 TRAIN  loss dict:  {'classification_loss': 0.9283319187164306}
2025-01-15 04:32:54,605 [INFO] Step[1350/2713]: training loss : 0.928737336397171 TRAIN  loss dict:  {'classification_loss': 0.928737336397171}
2025-01-15 04:33:07,870 [INFO] Step[1400/2713]: training loss : 0.9286381459236145 TRAIN  loss dict:  {'classification_loss': 0.9286381459236145}
2025-01-15 04:33:21,230 [INFO] Step[1450/2713]: training loss : 0.9287682008743287 TRAIN  loss dict:  {'classification_loss': 0.9287682008743287}
2025-01-15 04:33:36,346 [INFO] Step[1500/2713]: training loss : 0.9282336127758026 TRAIN  loss dict:  {'classification_loss': 0.9282336127758026}
2025-01-15 04:33:51,608 [INFO] Step[1550/2713]: training loss : 0.9282741212844848 TRAIN  loss dict:  {'classification_loss': 0.9282741212844848}
2025-01-15 04:34:05,134 [INFO] Step[1600/2713]: training loss : 0.9343160378932953 TRAIN  loss dict:  {'classification_loss': 0.9343160378932953}
2025-01-15 04:34:18,564 [INFO] Step[1650/2713]: training loss : 0.9287889134883881 TRAIN  loss dict:  {'classification_loss': 0.9287889134883881}
2025-01-15 04:34:32,429 [INFO] Step[1700/2713]: training loss : 0.9293909549713135 TRAIN  loss dict:  {'classification_loss': 0.9293909549713135}
2025-01-15 04:34:46,347 [INFO] Step[1750/2713]: training loss : 0.9278962039947509 TRAIN  loss dict:  {'classification_loss': 0.9278962039947509}
2025-01-15 04:35:00,231 [INFO] Step[1800/2713]: training loss : 0.9374957036972046 TRAIN  loss dict:  {'classification_loss': 0.9374957036972046}
2025-01-15 04:35:13,730 [INFO] Step[1850/2713]: training loss : 0.9329682981967926 TRAIN  loss dict:  {'classification_loss': 0.9329682981967926}
2025-01-15 04:35:27,696 [INFO] Step[1900/2713]: training loss : 0.9280094063282013 TRAIN  loss dict:  {'classification_loss': 0.9280094063282013}
2025-01-15 04:35:41,255 [INFO] Step[1950/2713]: training loss : 0.9302607917785645 TRAIN  loss dict:  {'classification_loss': 0.9302607917785645}
2025-01-15 04:35:54,522 [INFO] Step[2000/2713]: training loss : 0.9330436539649963 TRAIN  loss dict:  {'classification_loss': 0.9330436539649963}
2025-01-15 04:36:07,992 [INFO] Step[2050/2713]: training loss : 0.9290044724941253 TRAIN  loss dict:  {'classification_loss': 0.9290044724941253}
2025-01-15 04:36:21,831 [INFO] Step[2100/2713]: training loss : 0.9282877600193024 TRAIN  loss dict:  {'classification_loss': 0.9282877600193024}
2025-01-15 04:36:35,456 [INFO] Step[2150/2713]: training loss : 0.9287107360363006 TRAIN  loss dict:  {'classification_loss': 0.9287107360363006}
2025-01-15 04:36:49,198 [INFO] Step[2200/2713]: training loss : 0.9295650136470794 TRAIN  loss dict:  {'classification_loss': 0.9295650136470794}
2025-01-15 04:37:02,617 [INFO] Step[2250/2713]: training loss : 0.9299157691001892 TRAIN  loss dict:  {'classification_loss': 0.9299157691001892}
2025-01-15 04:37:16,521 [INFO] Step[2300/2713]: training loss : 0.9291330194473266 TRAIN  loss dict:  {'classification_loss': 0.9291330194473266}
2025-01-15 04:37:30,391 [INFO] Step[2350/2713]: training loss : 0.9287799906730652 TRAIN  loss dict:  {'classification_loss': 0.9287799906730652}
2025-01-15 04:37:44,100 [INFO] Step[2400/2713]: training loss : 0.9281597030162811 TRAIN  loss dict:  {'classification_loss': 0.9281597030162811}
2025-01-15 04:37:57,291 [INFO] Step[2450/2713]: training loss : 0.9289682388305665 TRAIN  loss dict:  {'classification_loss': 0.9289682388305665}
2025-01-15 04:38:10,814 [INFO] Step[2500/2713]: training loss : 0.9292165696620941 TRAIN  loss dict:  {'classification_loss': 0.9292165696620941}
2025-01-15 04:38:24,422 [INFO] Step[2550/2713]: training loss : 0.9287099456787109 TRAIN  loss dict:  {'classification_loss': 0.9287099456787109}
2025-01-15 04:38:37,776 [INFO] Step[2600/2713]: training loss : 0.9295396053791046 TRAIN  loss dict:  {'classification_loss': 0.9295396053791046}
2025-01-15 04:38:51,471 [INFO] Step[2650/2713]: training loss : 0.9286508440971375 TRAIN  loss dict:  {'classification_loss': 0.9286508440971375}
2025-01-15 04:39:04,660 [INFO] Step[2700/2713]: training loss : 0.9297214066982269 TRAIN  loss dict:  {'classification_loss': 0.9297214066982269}
2025-01-15 04:40:21,174 [INFO] Label accuracies statistics:
2025-01-15 04:40:21,174 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.3333333333333333, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.25, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 04:40:21,177 [INFO] [75] TRAIN  loss: 0.9296995534081324 acc: 0.9997542695662858
2025-01-15 04:40:21,177 [INFO] [75] TRAIN  loss dict: {'classification_loss': 0.9296995534081324}
2025-01-15 04:40:21,177 [INFO] [75] VALIDATION loss: 1.835425373979081 VALIDATION acc: 0.8006269592476489
2025-01-15 04:40:21,177 [INFO] [75] VALIDATION loss dict: {'classification_loss': 1.835425373979081}
2025-01-15 04:40:21,177 [INFO] 
2025-01-15 04:40:39,194 [INFO] Step[50/2713]: training loss : 0.9287474143505097 TRAIN  loss dict:  {'classification_loss': 0.9287474143505097}
2025-01-15 04:40:52,661 [INFO] Step[100/2713]: training loss : 0.9334340178966523 TRAIN  loss dict:  {'classification_loss': 0.9334340178966523}
2025-01-15 04:41:06,199 [INFO] Step[150/2713]: training loss : 0.9283818650245667 TRAIN  loss dict:  {'classification_loss': 0.9283818650245667}
2025-01-15 04:41:19,540 [INFO] Step[200/2713]: training loss : 0.9546047139167786 TRAIN  loss dict:  {'classification_loss': 0.9546047139167786}
2025-01-15 04:41:33,682 [INFO] Step[250/2713]: training loss : 0.931532723903656 TRAIN  loss dict:  {'classification_loss': 0.931532723903656}
2025-01-15 04:41:47,261 [INFO] Step[300/2713]: training loss : 0.9290737402439118 TRAIN  loss dict:  {'classification_loss': 0.9290737402439118}
2025-01-15 04:42:00,910 [INFO] Step[350/2713]: training loss : 0.9282308292388916 TRAIN  loss dict:  {'classification_loss': 0.9282308292388916}
2025-01-15 04:42:14,478 [INFO] Step[400/2713]: training loss : 0.9290771961212159 TRAIN  loss dict:  {'classification_loss': 0.9290771961212159}
2025-01-15 04:42:27,726 [INFO] Step[450/2713]: training loss : 0.9308465218544006 TRAIN  loss dict:  {'classification_loss': 0.9308465218544006}
2025-01-15 04:42:41,228 [INFO] Step[500/2713]: training loss : 0.9286174178123474 TRAIN  loss dict:  {'classification_loss': 0.9286174178123474}
2025-01-15 04:42:54,758 [INFO] Step[550/2713]: training loss : 0.9291619944572449 TRAIN  loss dict:  {'classification_loss': 0.9291619944572449}
2025-01-15 04:43:08,310 [INFO] Step[600/2713]: training loss : 0.9277070605754852 TRAIN  loss dict:  {'classification_loss': 0.9277070605754852}
2025-01-15 04:43:22,321 [INFO] Step[650/2713]: training loss : 0.9292865037918091 TRAIN  loss dict:  {'classification_loss': 0.9292865037918091}
2025-01-15 04:43:36,095 [INFO] Step[700/2713]: training loss : 0.927815524339676 TRAIN  loss dict:  {'classification_loss': 0.927815524339676}
2025-01-15 04:43:49,873 [INFO] Step[750/2713]: training loss : 0.9285651206970215 TRAIN  loss dict:  {'classification_loss': 0.9285651206970215}
2025-01-15 04:44:03,794 [INFO] Step[800/2713]: training loss : 0.9284608316421509 TRAIN  loss dict:  {'classification_loss': 0.9284608316421509}
2025-01-15 04:44:16,968 [INFO] Step[850/2713]: training loss : 0.9290185081958771 TRAIN  loss dict:  {'classification_loss': 0.9290185081958771}
2025-01-15 04:44:30,153 [INFO] Step[900/2713]: training loss : 0.9281471729278564 TRAIN  loss dict:  {'classification_loss': 0.9281471729278564}
2025-01-15 04:44:43,723 [INFO] Step[950/2713]: training loss : 0.9289326405525208 TRAIN  loss dict:  {'classification_loss': 0.9289326405525208}
2025-01-15 04:44:56,899 [INFO] Step[1000/2713]: training loss : 0.9301825141906739 TRAIN  loss dict:  {'classification_loss': 0.9301825141906739}
2025-01-15 04:45:10,087 [INFO] Step[1050/2713]: training loss : 0.9288600409030914 TRAIN  loss dict:  {'classification_loss': 0.9288600409030914}
2025-01-15 04:45:23,972 [INFO] Step[1100/2713]: training loss : 0.9286753785610199 TRAIN  loss dict:  {'classification_loss': 0.9286753785610199}
2025-01-15 04:45:37,680 [INFO] Step[1150/2713]: training loss : 0.9302311587333679 TRAIN  loss dict:  {'classification_loss': 0.9302311587333679}
2025-01-15 04:45:51,888 [INFO] Step[1200/2713]: training loss : 0.928227173089981 TRAIN  loss dict:  {'classification_loss': 0.928227173089981}
2025-01-15 04:46:05,774 [INFO] Step[1250/2713]: training loss : 0.9292347180843353 TRAIN  loss dict:  {'classification_loss': 0.9292347180843353}
2025-01-15 04:46:20,028 [INFO] Step[1300/2713]: training loss : 0.929175797700882 TRAIN  loss dict:  {'classification_loss': 0.929175797700882}
2025-01-15 04:46:33,800 [INFO] Step[1350/2713]: training loss : 0.9298736429214478 TRAIN  loss dict:  {'classification_loss': 0.9298736429214478}
2025-01-15 04:46:47,504 [INFO] Step[1400/2713]: training loss : 0.9287215840816497 TRAIN  loss dict:  {'classification_loss': 0.9287215840816497}
2025-01-15 04:47:01,333 [INFO] Step[1450/2713]: training loss : 0.9290853667259217 TRAIN  loss dict:  {'classification_loss': 0.9290853667259217}
2025-01-15 04:47:14,805 [INFO] Step[1500/2713]: training loss : 0.9288070225715637 TRAIN  loss dict:  {'classification_loss': 0.9288070225715637}
2025-01-15 04:47:28,398 [INFO] Step[1550/2713]: training loss : 0.9281584167480469 TRAIN  loss dict:  {'classification_loss': 0.9281584167480469}
2025-01-15 04:47:42,111 [INFO] Step[1600/2713]: training loss : 0.9285003733634949 TRAIN  loss dict:  {'classification_loss': 0.9285003733634949}
2025-01-15 04:47:56,311 [INFO] Step[1650/2713]: training loss : 0.9289340317249298 TRAIN  loss dict:  {'classification_loss': 0.9289340317249298}
2025-01-15 04:48:10,027 [INFO] Step[1700/2713]: training loss : 0.9747166752815246 TRAIN  loss dict:  {'classification_loss': 0.9747166752815246}
2025-01-15 04:48:23,993 [INFO] Step[1750/2713]: training loss : 0.9286322653293609 TRAIN  loss dict:  {'classification_loss': 0.9286322653293609}
2025-01-15 04:48:37,806 [INFO] Step[1800/2713]: training loss : 0.9372790682315827 TRAIN  loss dict:  {'classification_loss': 0.9372790682315827}
2025-01-15 04:48:51,184 [INFO] Step[1850/2713]: training loss : 0.9284924089908599 TRAIN  loss dict:  {'classification_loss': 0.9284924089908599}
2025-01-15 04:49:05,045 [INFO] Step[1900/2713]: training loss : 0.9293092906475067 TRAIN  loss dict:  {'classification_loss': 0.9293092906475067}
2025-01-15 04:49:18,554 [INFO] Step[1950/2713]: training loss : 0.9301060950756073 TRAIN  loss dict:  {'classification_loss': 0.9301060950756073}
2025-01-15 04:49:32,200 [INFO] Step[2000/2713]: training loss : 0.9284240961074829 TRAIN  loss dict:  {'classification_loss': 0.9284240961074829}
2025-01-15 04:49:45,862 [INFO] Step[2050/2713]: training loss : 0.9285500454902649 TRAIN  loss dict:  {'classification_loss': 0.9285500454902649}
2025-01-15 04:50:00,068 [INFO] Step[2100/2713]: training loss : 0.929723391532898 TRAIN  loss dict:  {'classification_loss': 0.929723391532898}
2025-01-15 04:50:13,659 [INFO] Step[2150/2713]: training loss : 0.9286009335517883 TRAIN  loss dict:  {'classification_loss': 0.9286009335517883}
2025-01-15 04:50:27,505 [INFO] Step[2200/2713]: training loss : 0.9290847837924957 TRAIN  loss dict:  {'classification_loss': 0.9290847837924957}
2025-01-15 04:50:41,584 [INFO] Step[2250/2713]: training loss : 0.9291170847415924 TRAIN  loss dict:  {'classification_loss': 0.9291170847415924}
2025-01-15 04:50:55,826 [INFO] Step[2300/2713]: training loss : 0.9290623784065246 TRAIN  loss dict:  {'classification_loss': 0.9290623784065246}
2025-01-15 04:51:09,282 [INFO] Step[2350/2713]: training loss : 0.9287561786174774 TRAIN  loss dict:  {'classification_loss': 0.9287561786174774}
2025-01-15 04:51:22,653 [INFO] Step[2400/2713]: training loss : 0.9494072580337525 TRAIN  loss dict:  {'classification_loss': 0.9494072580337525}
2025-01-15 04:51:36,580 [INFO] Step[2450/2713]: training loss : 0.9285964119434357 TRAIN  loss dict:  {'classification_loss': 0.9285964119434357}
2025-01-15 04:51:49,927 [INFO] Step[2500/2713]: training loss : 0.9282375538349151 TRAIN  loss dict:  {'classification_loss': 0.9282375538349151}
2025-01-15 04:52:03,804 [INFO] Step[2550/2713]: training loss : 0.9287332201004028 TRAIN  loss dict:  {'classification_loss': 0.9287332201004028}
2025-01-15 04:52:17,987 [INFO] Step[2600/2713]: training loss : 0.9280994784832001 TRAIN  loss dict:  {'classification_loss': 0.9280994784832001}
2025-01-15 04:52:31,729 [INFO] Step[2650/2713]: training loss : 0.9286294841766357 TRAIN  loss dict:  {'classification_loss': 0.9286294841766357}
2025-01-15 04:52:45,582 [INFO] Step[2700/2713]: training loss : 0.9282136154174805 TRAIN  loss dict:  {'classification_loss': 0.9282136154174805}
2025-01-15 04:54:02,622 [INFO] Label accuracies statistics:
2025-01-15 04:54:02,622 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 04:54:02,624 [INFO] [76] TRAIN  loss: 0.9308406107767543 acc: 0.9995085391325715
2025-01-15 04:54:02,624 [INFO] [76] TRAIN  loss dict: {'classification_loss': 0.9308406107767543}
2025-01-15 04:54:02,624 [INFO] [76] VALIDATION loss: 1.8622863964926928 VALIDATION acc: 0.793730407523511
2025-01-15 04:54:02,624 [INFO] [76] VALIDATION loss dict: {'classification_loss': 1.8622863964926928}
2025-01-15 04:54:02,624 [INFO] 
2025-01-15 04:54:21,015 [INFO] Step[50/2713]: training loss : 0.930277978181839 TRAIN  loss dict:  {'classification_loss': 0.930277978181839}
2025-01-15 04:54:35,057 [INFO] Step[100/2713]: training loss : 0.9515998160839081 TRAIN  loss dict:  {'classification_loss': 0.9515998160839081}
2025-01-15 04:54:48,406 [INFO] Step[150/2713]: training loss : 0.9289800870418549 TRAIN  loss dict:  {'classification_loss': 0.9289800870418549}
2025-01-15 04:55:02,085 [INFO] Step[200/2713]: training loss : 0.9282786238193512 TRAIN  loss dict:  {'classification_loss': 0.9282786238193512}
2025-01-15 04:55:15,292 [INFO] Step[250/2713]: training loss : 0.9336242008209229 TRAIN  loss dict:  {'classification_loss': 0.9336242008209229}
2025-01-15 04:55:28,468 [INFO] Step[300/2713]: training loss : 0.9289153897762299 TRAIN  loss dict:  {'classification_loss': 0.9289153897762299}
2025-01-15 04:55:41,994 [INFO] Step[350/2713]: training loss : 0.9289701998233795 TRAIN  loss dict:  {'classification_loss': 0.9289701998233795}
2025-01-15 04:55:55,268 [INFO] Step[400/2713]: training loss : 0.9301854598522187 TRAIN  loss dict:  {'classification_loss': 0.9301854598522187}
2025-01-15 04:56:09,122 [INFO] Step[450/2713]: training loss : 0.9295419383049012 TRAIN  loss dict:  {'classification_loss': 0.9295419383049012}
2025-01-15 04:56:22,318 [INFO] Step[500/2713]: training loss : 0.9287703263759614 TRAIN  loss dict:  {'classification_loss': 0.9287703263759614}
2025-01-15 04:56:35,523 [INFO] Step[550/2713]: training loss : 0.9289201533794403 TRAIN  loss dict:  {'classification_loss': 0.9289201533794403}
2025-01-15 04:56:48,789 [INFO] Step[600/2713]: training loss : 0.9284183657169343 TRAIN  loss dict:  {'classification_loss': 0.9284183657169343}
2025-01-15 04:57:02,568 [INFO] Step[650/2713]: training loss : 0.929655030965805 TRAIN  loss dict:  {'classification_loss': 0.929655030965805}
2025-01-15 04:57:16,715 [INFO] Step[700/2713]: training loss : 0.9291799294948578 TRAIN  loss dict:  {'classification_loss': 0.9291799294948578}
2025-01-15 04:57:30,330 [INFO] Step[750/2713]: training loss : 0.9285039103031159 TRAIN  loss dict:  {'classification_loss': 0.9285039103031159}
2025-01-15 04:57:44,398 [INFO] Step[800/2713]: training loss : 0.928450356721878 TRAIN  loss dict:  {'classification_loss': 0.928450356721878}
2025-01-15 04:57:58,566 [INFO] Step[850/2713]: training loss : 0.9290126121044159 TRAIN  loss dict:  {'classification_loss': 0.9290126121044159}
2025-01-15 04:58:11,966 [INFO] Step[900/2713]: training loss : 0.9293700766563415 TRAIN  loss dict:  {'classification_loss': 0.9293700766563415}
2025-01-15 04:58:25,748 [INFO] Step[950/2713]: training loss : 0.9314947903156281 TRAIN  loss dict:  {'classification_loss': 0.9314947903156281}
2025-01-15 04:58:39,331 [INFO] Step[1000/2713]: training loss : 0.9278734445571899 TRAIN  loss dict:  {'classification_loss': 0.9278734445571899}
2025-01-15 04:58:53,023 [INFO] Step[1050/2713]: training loss : 0.9280402863025665 TRAIN  loss dict:  {'classification_loss': 0.9280402863025665}
2025-01-15 04:59:07,186 [INFO] Step[1100/2713]: training loss : 0.9277535474300385 TRAIN  loss dict:  {'classification_loss': 0.9277535474300385}
2025-01-15 04:59:20,812 [INFO] Step[1150/2713]: training loss : 0.9375721883773803 TRAIN  loss dict:  {'classification_loss': 0.9375721883773803}
2025-01-15 04:59:34,701 [INFO] Step[1200/2713]: training loss : 0.9281162703037262 TRAIN  loss dict:  {'classification_loss': 0.9281162703037262}
2025-01-15 04:59:48,534 [INFO] Step[1250/2713]: training loss : 0.9274849736690521 TRAIN  loss dict:  {'classification_loss': 0.9274849736690521}
2025-01-15 05:00:02,173 [INFO] Step[1300/2713]: training loss : 0.9289539909362793 TRAIN  loss dict:  {'classification_loss': 0.9289539909362793}
2025-01-15 05:00:15,786 [INFO] Step[1350/2713]: training loss : 0.9278784811496734 TRAIN  loss dict:  {'classification_loss': 0.9278784811496734}
2025-01-15 05:00:29,186 [INFO] Step[1400/2713]: training loss : 0.9287956333160401 TRAIN  loss dict:  {'classification_loss': 0.9287956333160401}
2025-01-15 05:00:42,842 [INFO] Step[1450/2713]: training loss : 0.929458681344986 TRAIN  loss dict:  {'classification_loss': 0.929458681344986}
2025-01-15 05:00:56,798 [INFO] Step[1500/2713]: training loss : 0.9285900509357452 TRAIN  loss dict:  {'classification_loss': 0.9285900509357452}
2025-01-15 05:01:10,989 [INFO] Step[1550/2713]: training loss : 0.9289717018604279 TRAIN  loss dict:  {'classification_loss': 0.9289717018604279}
2025-01-15 05:01:24,879 [INFO] Step[1600/2713]: training loss : 0.9295883667469025 TRAIN  loss dict:  {'classification_loss': 0.9295883667469025}
2025-01-15 05:01:39,114 [INFO] Step[1650/2713]: training loss : 0.9278248655796051 TRAIN  loss dict:  {'classification_loss': 0.9278248655796051}
2025-01-15 05:01:53,342 [INFO] Step[1700/2713]: training loss : 0.9288123083114624 TRAIN  loss dict:  {'classification_loss': 0.9288123083114624}
2025-01-15 05:02:07,662 [INFO] Step[1750/2713]: training loss : 0.9275116777420044 TRAIN  loss dict:  {'classification_loss': 0.9275116777420044}
2025-01-15 05:02:21,024 [INFO] Step[1800/2713]: training loss : 0.9299899697303772 TRAIN  loss dict:  {'classification_loss': 0.9299899697303772}
2025-01-15 05:02:34,715 [INFO] Step[1850/2713]: training loss : 0.9283750820159912 TRAIN  loss dict:  {'classification_loss': 0.9283750820159912}
2025-01-15 05:02:48,101 [INFO] Step[1900/2713]: training loss : 0.9290148138999939 TRAIN  loss dict:  {'classification_loss': 0.9290148138999939}
2025-01-15 05:03:01,486 [INFO] Step[1950/2713]: training loss : 0.9561774516105652 TRAIN  loss dict:  {'classification_loss': 0.9561774516105652}
2025-01-15 05:03:14,834 [INFO] Step[2000/2713]: training loss : 0.9293195879459382 TRAIN  loss dict:  {'classification_loss': 0.9293195879459382}
2025-01-15 05:03:28,437 [INFO] Step[2050/2713]: training loss : 0.9280954110622406 TRAIN  loss dict:  {'classification_loss': 0.9280954110622406}
2025-01-15 05:03:42,144 [INFO] Step[2100/2713]: training loss : 0.9290137219429017 TRAIN  loss dict:  {'classification_loss': 0.9290137219429017}
2025-01-15 05:03:56,163 [INFO] Step[2150/2713]: training loss : 0.9571972608566284 TRAIN  loss dict:  {'classification_loss': 0.9571972608566284}
2025-01-15 05:04:10,362 [INFO] Step[2200/2713]: training loss : 0.928085345029831 TRAIN  loss dict:  {'classification_loss': 0.928085345029831}
2025-01-15 05:04:23,898 [INFO] Step[2250/2713]: training loss : 0.9284018468856812 TRAIN  loss dict:  {'classification_loss': 0.9284018468856812}
2025-01-15 05:04:37,190 [INFO] Step[2300/2713]: training loss : 0.9280215334892273 TRAIN  loss dict:  {'classification_loss': 0.9280215334892273}
2025-01-15 05:04:50,756 [INFO] Step[2350/2713]: training loss : 0.9280687022209168 TRAIN  loss dict:  {'classification_loss': 0.9280687022209168}
2025-01-15 05:05:04,386 [INFO] Step[2400/2713]: training loss : 0.9280948173999787 TRAIN  loss dict:  {'classification_loss': 0.9280948173999787}
2025-01-15 05:05:18,168 [INFO] Step[2450/2713]: training loss : 0.9276326477527619 TRAIN  loss dict:  {'classification_loss': 0.9276326477527619}
2025-01-15 05:05:32,319 [INFO] Step[2500/2713]: training loss : 0.9283251166343689 TRAIN  loss dict:  {'classification_loss': 0.9283251166343689}
2025-01-15 05:05:45,799 [INFO] Step[2550/2713]: training loss : 0.9286785089969635 TRAIN  loss dict:  {'classification_loss': 0.9286785089969635}
2025-01-15 05:06:01,515 [INFO] Step[2600/2713]: training loss : 0.9294356524944305 TRAIN  loss dict:  {'classification_loss': 0.9294356524944305}
2025-01-15 05:06:17,073 [INFO] Step[2650/2713]: training loss : 0.9756855523586273 TRAIN  loss dict:  {'classification_loss': 0.9756855523586273}
2025-01-15 05:06:30,254 [INFO] Step[2700/2713]: training loss : 0.930503443479538 TRAIN  loss dict:  {'classification_loss': 0.930503443479538}
2025-01-15 05:07:46,422 [INFO] Label accuracies statistics:
2025-01-15 05:07:46,422 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 1.0, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.5, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.5, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 05:07:46,424 [INFO] [77] TRAIN  loss: 0.9313448467159728 acc: 0.9992628086988573
2025-01-15 05:07:46,424 [INFO] [77] TRAIN  loss dict: {'classification_loss': 0.9313448467159728}
2025-01-15 05:07:46,425 [INFO] [77] VALIDATION loss: 1.8349592859359611 VALIDATION acc: 0.7949843260188088
2025-01-15 05:07:46,425 [INFO] [77] VALIDATION loss dict: {'classification_loss': 1.8349592859359611}
2025-01-15 05:07:46,425 [INFO] 
2025-01-15 05:08:04,976 [INFO] Step[50/2713]: training loss : 0.9287259566783905 TRAIN  loss dict:  {'classification_loss': 0.9287259566783905}
2025-01-15 05:08:18,253 [INFO] Step[100/2713]: training loss : 0.9291644823551178 TRAIN  loss dict:  {'classification_loss': 0.9291644823551178}
2025-01-15 05:08:32,496 [INFO] Step[150/2713]: training loss : 0.9290506827831269 TRAIN  loss dict:  {'classification_loss': 0.9290506827831269}
2025-01-15 05:08:46,078 [INFO] Step[200/2713]: training loss : 0.9287642300128937 TRAIN  loss dict:  {'classification_loss': 0.9287642300128937}
2025-01-15 05:09:00,000 [INFO] Step[250/2713]: training loss : 0.959217985868454 TRAIN  loss dict:  {'classification_loss': 0.959217985868454}
2025-01-15 05:09:13,597 [INFO] Step[300/2713]: training loss : 0.9294217121601105 TRAIN  loss dict:  {'classification_loss': 0.9294217121601105}
2025-01-15 05:09:27,787 [INFO] Step[350/2713]: training loss : 0.929432624578476 TRAIN  loss dict:  {'classification_loss': 0.929432624578476}
2025-01-15 05:09:41,321 [INFO] Step[400/2713]: training loss : 0.9292460465431214 TRAIN  loss dict:  {'classification_loss': 0.9292460465431214}
2025-01-15 05:09:55,503 [INFO] Step[450/2713]: training loss : 0.9310840988159179 TRAIN  loss dict:  {'classification_loss': 0.9310840988159179}
2025-01-15 05:10:09,222 [INFO] Step[500/2713]: training loss : 0.9363800406455993 TRAIN  loss dict:  {'classification_loss': 0.9363800406455993}
2025-01-15 05:10:22,469 [INFO] Step[550/2713]: training loss : 0.9288070642948151 TRAIN  loss dict:  {'classification_loss': 0.9288070642948151}
2025-01-15 05:10:35,686 [INFO] Step[600/2713]: training loss : 0.9289560556411743 TRAIN  loss dict:  {'classification_loss': 0.9289560556411743}
2025-01-15 05:10:49,260 [INFO] Step[650/2713]: training loss : 0.9532432520389557 TRAIN  loss dict:  {'classification_loss': 0.9532432520389557}
2025-01-15 05:11:02,995 [INFO] Step[700/2713]: training loss : 0.9285056722164154 TRAIN  loss dict:  {'classification_loss': 0.9285056722164154}
2025-01-15 05:11:17,026 [INFO] Step[750/2713]: training loss : 0.9595869255065917 TRAIN  loss dict:  {'classification_loss': 0.9595869255065917}
2025-01-15 05:11:30,435 [INFO] Step[800/2713]: training loss : 0.9297572410106659 TRAIN  loss dict:  {'classification_loss': 0.9297572410106659}
2025-01-15 05:11:44,417 [INFO] Step[850/2713]: training loss : 0.9289669036865235 TRAIN  loss dict:  {'classification_loss': 0.9289669036865235}
2025-01-15 05:11:58,317 [INFO] Step[900/2713]: training loss : 0.9285909128189087 TRAIN  loss dict:  {'classification_loss': 0.9285909128189087}
2025-01-15 05:12:11,990 [INFO] Step[950/2713]: training loss : 0.9281223690509797 TRAIN  loss dict:  {'classification_loss': 0.9281223690509797}
2025-01-15 05:12:25,260 [INFO] Step[1000/2713]: training loss : 0.9289901292324066 TRAIN  loss dict:  {'classification_loss': 0.9289901292324066}
2025-01-15 05:12:38,487 [INFO] Step[1050/2713]: training loss : 0.9379324662685394 TRAIN  loss dict:  {'classification_loss': 0.9379324662685394}
2025-01-15 05:12:51,732 [INFO] Step[1100/2713]: training loss : 0.9276753985881805 TRAIN  loss dict:  {'classification_loss': 0.9276753985881805}
2025-01-15 05:13:05,591 [INFO] Step[1150/2713]: training loss : 0.9323386037349701 TRAIN  loss dict:  {'classification_loss': 0.9323386037349701}
2025-01-15 05:13:19,256 [INFO] Step[1200/2713]: training loss : 0.9289006590843201 TRAIN  loss dict:  {'classification_loss': 0.9289006590843201}
2025-01-15 05:13:33,195 [INFO] Step[1250/2713]: training loss : 0.9288229370117187 TRAIN  loss dict:  {'classification_loss': 0.9288229370117187}
2025-01-15 05:13:47,170 [INFO] Step[1300/2713]: training loss : 0.9291919732093811 TRAIN  loss dict:  {'classification_loss': 0.9291919732093811}
2025-01-15 05:14:00,734 [INFO] Step[1350/2713]: training loss : 0.928466100692749 TRAIN  loss dict:  {'classification_loss': 0.928466100692749}
2025-01-15 05:14:14,645 [INFO] Step[1400/2713]: training loss : 0.9290196990966797 TRAIN  loss dict:  {'classification_loss': 0.9290196990966797}
2025-01-15 05:14:28,573 [INFO] Step[1450/2713]: training loss : 0.9291787409782409 TRAIN  loss dict:  {'classification_loss': 0.9291787409782409}
2025-01-15 05:14:42,096 [INFO] Step[1500/2713]: training loss : 0.9280017745494843 TRAIN  loss dict:  {'classification_loss': 0.9280017745494843}
2025-01-15 05:14:55,665 [INFO] Step[1550/2713]: training loss : 0.928071905374527 TRAIN  loss dict:  {'classification_loss': 0.928071905374527}
2025-01-15 05:15:09,493 [INFO] Step[1600/2713]: training loss : 0.9278780019283295 TRAIN  loss dict:  {'classification_loss': 0.9278780019283295}
2025-01-15 05:15:23,337 [INFO] Step[1650/2713]: training loss : 0.9276047444343567 TRAIN  loss dict:  {'classification_loss': 0.9276047444343567}
2025-01-15 05:15:37,061 [INFO] Step[1700/2713]: training loss : 0.9276491498947144 TRAIN  loss dict:  {'classification_loss': 0.9276491498947144}
2025-01-15 05:15:50,325 [INFO] Step[1750/2713]: training loss : 0.9275299072265625 TRAIN  loss dict:  {'classification_loss': 0.9275299072265625}
2025-01-15 05:16:03,885 [INFO] Step[1800/2713]: training loss : 0.9279926288127899 TRAIN  loss dict:  {'classification_loss': 0.9279926288127899}
2025-01-15 05:16:17,539 [INFO] Step[1850/2713]: training loss : 0.9293999695777893 TRAIN  loss dict:  {'classification_loss': 0.9293999695777893}
2025-01-15 05:16:31,270 [INFO] Step[1900/2713]: training loss : 0.9285124957561492 TRAIN  loss dict:  {'classification_loss': 0.9285124957561492}
2025-01-15 05:16:45,120 [INFO] Step[1950/2713]: training loss : 0.9295446074008942 TRAIN  loss dict:  {'classification_loss': 0.9295446074008942}
2025-01-15 05:16:58,640 [INFO] Step[2000/2713]: training loss : 0.9282498991489411 TRAIN  loss dict:  {'classification_loss': 0.9282498991489411}
2025-01-15 05:17:12,823 [INFO] Step[2050/2713]: training loss : 0.9289006340503693 TRAIN  loss dict:  {'classification_loss': 0.9289006340503693}
2025-01-15 05:17:26,507 [INFO] Step[2100/2713]: training loss : 0.928069314956665 TRAIN  loss dict:  {'classification_loss': 0.928069314956665}
2025-01-15 05:17:40,035 [INFO] Step[2150/2713]: training loss : 0.9288673067092895 TRAIN  loss dict:  {'classification_loss': 0.9288673067092895}
2025-01-15 05:17:53,492 [INFO] Step[2200/2713]: training loss : 0.9306630539894104 TRAIN  loss dict:  {'classification_loss': 0.9306630539894104}
2025-01-15 05:18:07,318 [INFO] Step[2250/2713]: training loss : 0.9281887578964233 TRAIN  loss dict:  {'classification_loss': 0.9281887578964233}
2025-01-15 05:18:21,047 [INFO] Step[2300/2713]: training loss : 0.9284743070602417 TRAIN  loss dict:  {'classification_loss': 0.9284743070602417}
2025-01-15 05:18:34,732 [INFO] Step[2350/2713]: training loss : 0.9274327743053437 TRAIN  loss dict:  {'classification_loss': 0.9274327743053437}
2025-01-15 05:18:48,283 [INFO] Step[2400/2713]: training loss : 0.9599418079853058 TRAIN  loss dict:  {'classification_loss': 0.9599418079853058}
2025-01-15 05:19:02,150 [INFO] Step[2450/2713]: training loss : 0.9279985523223877 TRAIN  loss dict:  {'classification_loss': 0.9279985523223877}
2025-01-15 05:19:15,684 [INFO] Step[2500/2713]: training loss : 0.9285839521884918 TRAIN  loss dict:  {'classification_loss': 0.9285839521884918}
2025-01-15 05:19:29,462 [INFO] Step[2550/2713]: training loss : 0.9283378565311432 TRAIN  loss dict:  {'classification_loss': 0.9283378565311432}
2025-01-15 05:19:42,810 [INFO] Step[2600/2713]: training loss : 0.9284707140922547 TRAIN  loss dict:  {'classification_loss': 0.9284707140922547}
2025-01-15 05:19:57,063 [INFO] Step[2650/2713]: training loss : 0.928739949464798 TRAIN  loss dict:  {'classification_loss': 0.928739949464798}
2025-01-15 05:20:10,922 [INFO] Step[2700/2713]: training loss : 0.9290486001968383 TRAIN  loss dict:  {'classification_loss': 0.9290486001968383}
2025-01-15 05:21:26,886 [INFO] Label accuracies statistics:
2025-01-15 05:21:26,886 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 05:21:26,888 [INFO] [78] TRAIN  loss: 0.9312341975805464 acc: 0.9993856739157144
2025-01-15 05:21:26,888 [INFO] [78] TRAIN  loss dict: {'classification_loss': 0.9312341975805464}
2025-01-15 05:21:26,888 [INFO] [78] VALIDATION loss: 1.7766417266058743 VALIDATION acc: 0.8075235109717869
2025-01-15 05:21:26,888 [INFO] [78] VALIDATION loss dict: {'classification_loss': 1.7766417266058743}
2025-01-15 05:21:26,888 [INFO] 
2025-01-15 05:21:45,307 [INFO] Step[50/2713]: training loss : 0.9475978636741638 TRAIN  loss dict:  {'classification_loss': 0.9475978636741638}
2025-01-15 05:21:58,853 [INFO] Step[100/2713]: training loss : 0.9281366443634034 TRAIN  loss dict:  {'classification_loss': 0.9281366443634034}
2025-01-15 05:22:12,743 [INFO] Step[150/2713]: training loss : 0.928362250328064 TRAIN  loss dict:  {'classification_loss': 0.928362250328064}
2025-01-15 05:22:26,957 [INFO] Step[200/2713]: training loss : 0.9277449309825897 TRAIN  loss dict:  {'classification_loss': 0.9277449309825897}
2025-01-15 05:22:40,539 [INFO] Step[250/2713]: training loss : 0.928543313741684 TRAIN  loss dict:  {'classification_loss': 0.928543313741684}
2025-01-15 05:22:54,364 [INFO] Step[300/2713]: training loss : 0.9287539958953858 TRAIN  loss dict:  {'classification_loss': 0.9287539958953858}
2025-01-15 05:23:07,926 [INFO] Step[350/2713]: training loss : 0.9592128968238831 TRAIN  loss dict:  {'classification_loss': 0.9592128968238831}
2025-01-15 05:23:21,559 [INFO] Step[400/2713]: training loss : 0.9283105111122132 TRAIN  loss dict:  {'classification_loss': 0.9283105111122132}
2025-01-15 05:23:35,453 [INFO] Step[450/2713]: training loss : 0.9289913165569306 TRAIN  loss dict:  {'classification_loss': 0.9289913165569306}
2025-01-15 05:23:48,830 [INFO] Step[500/2713]: training loss : 0.9281808197498321 TRAIN  loss dict:  {'classification_loss': 0.9281808197498321}
2025-01-15 05:24:02,417 [INFO] Step[550/2713]: training loss : 0.9289764416217804 TRAIN  loss dict:  {'classification_loss': 0.9289764416217804}
2025-01-15 05:24:15,958 [INFO] Step[600/2713]: training loss : 0.9284857559204102 TRAIN  loss dict:  {'classification_loss': 0.9284857559204102}
2025-01-15 05:24:29,917 [INFO] Step[650/2713]: training loss : 0.9301872611045837 TRAIN  loss dict:  {'classification_loss': 0.9301872611045837}
2025-01-15 05:24:43,216 [INFO] Step[700/2713]: training loss : 0.9295312130451202 TRAIN  loss dict:  {'classification_loss': 0.9295312130451202}
2025-01-15 05:24:57,304 [INFO] Step[750/2713]: training loss : 0.9289605641365051 TRAIN  loss dict:  {'classification_loss': 0.9289605641365051}
2025-01-15 05:25:10,511 [INFO] Step[800/2713]: training loss : 0.9291113519668579 TRAIN  loss dict:  {'classification_loss': 0.9291113519668579}
2025-01-15 05:25:23,906 [INFO] Step[850/2713]: training loss : 0.9287973737716675 TRAIN  loss dict:  {'classification_loss': 0.9287973737716675}
2025-01-15 05:25:37,682 [INFO] Step[900/2713]: training loss : 0.928792634010315 TRAIN  loss dict:  {'classification_loss': 0.928792634010315}
2025-01-15 05:25:51,572 [INFO] Step[950/2713]: training loss : 0.9288166403770447 TRAIN  loss dict:  {'classification_loss': 0.9288166403770447}
2025-01-15 05:26:05,139 [INFO] Step[1000/2713]: training loss : 0.9328673958778382 TRAIN  loss dict:  {'classification_loss': 0.9328673958778382}
2025-01-15 05:26:19,024 [INFO] Step[1050/2713]: training loss : 0.9289344775676728 TRAIN  loss dict:  {'classification_loss': 0.9289344775676728}
2025-01-15 05:26:32,283 [INFO] Step[1100/2713]: training loss : 0.9297379100322724 TRAIN  loss dict:  {'classification_loss': 0.9297379100322724}
2025-01-15 05:26:45,569 [INFO] Step[1150/2713]: training loss : 0.9284915471076965 TRAIN  loss dict:  {'classification_loss': 0.9284915471076965}
2025-01-15 05:26:59,715 [INFO] Step[1200/2713]: training loss : 0.9282281863689422 TRAIN  loss dict:  {'classification_loss': 0.9282281863689422}
2025-01-15 05:27:13,900 [INFO] Step[1250/2713]: training loss : 0.9277910971641541 TRAIN  loss dict:  {'classification_loss': 0.9277910971641541}
2025-01-15 05:27:28,077 [INFO] Step[1300/2713]: training loss : 0.928202577829361 TRAIN  loss dict:  {'classification_loss': 0.928202577829361}
2025-01-15 05:27:41,775 [INFO] Step[1350/2713]: training loss : 0.9336220133304596 TRAIN  loss dict:  {'classification_loss': 0.9336220133304596}
2025-01-15 05:27:55,788 [INFO] Step[1400/2713]: training loss : 0.9284211456775665 TRAIN  loss dict:  {'classification_loss': 0.9284211456775665}
2025-01-15 05:28:09,706 [INFO] Step[1450/2713]: training loss : 0.9303216886520386 TRAIN  loss dict:  {'classification_loss': 0.9303216886520386}
2025-01-15 05:28:23,647 [INFO] Step[1500/2713]: training loss : 0.9289376413822175 TRAIN  loss dict:  {'classification_loss': 0.9289376413822175}
2025-01-15 05:28:37,742 [INFO] Step[1550/2713]: training loss : 0.9286646902561188 TRAIN  loss dict:  {'classification_loss': 0.9286646902561188}
2025-01-15 05:28:51,396 [INFO] Step[1600/2713]: training loss : 0.9288642370700836 TRAIN  loss dict:  {'classification_loss': 0.9288642370700836}
2025-01-15 05:29:05,345 [INFO] Step[1650/2713]: training loss : 0.9275669205188751 TRAIN  loss dict:  {'classification_loss': 0.9275669205188751}
2025-01-15 05:29:19,101 [INFO] Step[1700/2713]: training loss : 0.9286724817752838 TRAIN  loss dict:  {'classification_loss': 0.9286724817752838}
2025-01-15 05:29:32,983 [INFO] Step[1750/2713]: training loss : 0.9357149636745453 TRAIN  loss dict:  {'classification_loss': 0.9357149636745453}
2025-01-15 05:29:47,141 [INFO] Step[1800/2713]: training loss : 0.9288893342018127 TRAIN  loss dict:  {'classification_loss': 0.9288893342018127}
2025-01-15 05:30:00,692 [INFO] Step[1850/2713]: training loss : 0.9270194590091705 TRAIN  loss dict:  {'classification_loss': 0.9270194590091705}
2025-01-15 05:30:14,743 [INFO] Step[1900/2713]: training loss : 0.9285613119602203 TRAIN  loss dict:  {'classification_loss': 0.9285613119602203}
2025-01-15 05:30:28,342 [INFO] Step[1950/2713]: training loss : 0.928234521150589 TRAIN  loss dict:  {'classification_loss': 0.928234521150589}
2025-01-15 05:30:42,283 [INFO] Step[2000/2713]: training loss : 0.928016265630722 TRAIN  loss dict:  {'classification_loss': 0.928016265630722}
2025-01-15 05:30:55,811 [INFO] Step[2050/2713]: training loss : 0.9296128463745117 TRAIN  loss dict:  {'classification_loss': 0.9296128463745117}
2025-01-15 05:31:09,282 [INFO] Step[2100/2713]: training loss : 0.9285723578929901 TRAIN  loss dict:  {'classification_loss': 0.9285723578929901}
2025-01-15 05:31:22,878 [INFO] Step[2150/2713]: training loss : 0.9289607274532318 TRAIN  loss dict:  {'classification_loss': 0.9289607274532318}
2025-01-15 05:31:36,412 [INFO] Step[2200/2713]: training loss : 0.9285421895980835 TRAIN  loss dict:  {'classification_loss': 0.9285421895980835}
2025-01-15 05:31:50,037 [INFO] Step[2250/2713]: training loss : 0.9283728849887848 TRAIN  loss dict:  {'classification_loss': 0.9283728849887848}
2025-01-15 05:32:03,742 [INFO] Step[2300/2713]: training loss : 0.9287768423557281 TRAIN  loss dict:  {'classification_loss': 0.9287768423557281}
2025-01-15 05:32:17,480 [INFO] Step[2350/2713]: training loss : 0.9325738441944122 TRAIN  loss dict:  {'classification_loss': 0.9325738441944122}
2025-01-15 05:32:31,040 [INFO] Step[2400/2713]: training loss : 0.9306807518005371 TRAIN  loss dict:  {'classification_loss': 0.9306807518005371}
2025-01-15 05:32:45,117 [INFO] Step[2450/2713]: training loss : 0.9277901697158814 TRAIN  loss dict:  {'classification_loss': 0.9277901697158814}
2025-01-15 05:32:58,885 [INFO] Step[2500/2713]: training loss : 0.9285508024692536 TRAIN  loss dict:  {'classification_loss': 0.9285508024692536}
2025-01-15 05:33:12,497 [INFO] Step[2550/2713]: training loss : 0.9292563247680664 TRAIN  loss dict:  {'classification_loss': 0.9292563247680664}
2025-01-15 05:33:26,023 [INFO] Step[2600/2713]: training loss : 0.9284840786457061 TRAIN  loss dict:  {'classification_loss': 0.9284840786457061}
2025-01-15 05:33:39,564 [INFO] Step[2650/2713]: training loss : 0.9285046172142029 TRAIN  loss dict:  {'classification_loss': 0.9285046172142029}
2025-01-15 05:33:56,134 [INFO] Step[2700/2713]: training loss : 0.928499276638031 TRAIN  loss dict:  {'classification_loss': 0.928499276638031}
2025-01-15 05:35:13,075 [INFO] Label accuracies statistics:
2025-01-15 05:35:13,075 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 05:35:13,077 [INFO] [79] TRAIN  loss: 0.9299600800376425 acc: 0.9996314043494287
2025-01-15 05:35:13,077 [INFO] [79] TRAIN  loss dict: {'classification_loss': 0.9299600800376425}
2025-01-15 05:35:13,077 [INFO] [79] VALIDATION loss: 1.801712017870487 VALIDATION acc: 0.8012539184952978
2025-01-15 05:35:13,077 [INFO] [79] VALIDATION loss dict: {'classification_loss': 1.801712017870487}
2025-01-15 05:35:13,077 [INFO] 
2025-01-15 05:35:31,502 [INFO] Step[50/2713]: training loss : 0.9287433075904846 TRAIN  loss dict:  {'classification_loss': 0.9287433075904846}
2025-01-15 05:35:45,009 [INFO] Step[100/2713]: training loss : 0.9279090178012848 TRAIN  loss dict:  {'classification_loss': 0.9279090178012848}
2025-01-15 05:35:59,175 [INFO] Step[150/2713]: training loss : 0.9281685042381287 TRAIN  loss dict:  {'classification_loss': 0.9281685042381287}
2025-01-15 05:36:13,009 [INFO] Step[200/2713]: training loss : 0.9280353736877441 TRAIN  loss dict:  {'classification_loss': 0.9280353736877441}
2025-01-15 05:36:26,656 [INFO] Step[250/2713]: training loss : 0.9286419653892517 TRAIN  loss dict:  {'classification_loss': 0.9286419653892517}
2025-01-15 05:36:40,140 [INFO] Step[300/2713]: training loss : 0.9274428272247315 TRAIN  loss dict:  {'classification_loss': 0.9274428272247315}
2025-01-15 05:36:53,590 [INFO] Step[350/2713]: training loss : 0.9278553450107574 TRAIN  loss dict:  {'classification_loss': 0.9278553450107574}
2025-01-15 05:37:07,204 [INFO] Step[400/2713]: training loss : 0.928254885673523 TRAIN  loss dict:  {'classification_loss': 0.928254885673523}
2025-01-15 05:37:20,873 [INFO] Step[450/2713]: training loss : 0.9276882135868072 TRAIN  loss dict:  {'classification_loss': 0.9276882135868072}
2025-01-15 05:37:34,831 [INFO] Step[500/2713]: training loss : 0.9361605978012085 TRAIN  loss dict:  {'classification_loss': 0.9361605978012085}
2025-01-15 05:37:48,639 [INFO] Step[550/2713]: training loss : 0.9287678408622742 TRAIN  loss dict:  {'classification_loss': 0.9287678408622742}
2025-01-15 05:38:03,737 [INFO] Step[600/2713]: training loss : 0.9278256714344024 TRAIN  loss dict:  {'classification_loss': 0.9278256714344024}
2025-01-15 05:38:18,784 [INFO] Step[650/2713]: training loss : 0.9278697323799133 TRAIN  loss dict:  {'classification_loss': 0.9278697323799133}
2025-01-15 05:38:32,498 [INFO] Step[700/2713]: training loss : 0.9278312563896179 TRAIN  loss dict:  {'classification_loss': 0.9278312563896179}
2025-01-15 05:38:46,457 [INFO] Step[750/2713]: training loss : 0.9280174601078034 TRAIN  loss dict:  {'classification_loss': 0.9280174601078034}
2025-01-15 05:39:00,097 [INFO] Step[800/2713]: training loss : 0.9284205484390259 TRAIN  loss dict:  {'classification_loss': 0.9284205484390259}
2025-01-15 05:39:13,850 [INFO] Step[850/2713]: training loss : 0.9278413438796997 TRAIN  loss dict:  {'classification_loss': 0.9278413438796997}
2025-01-15 05:39:27,458 [INFO] Step[900/2713]: training loss : 0.9287171363830566 TRAIN  loss dict:  {'classification_loss': 0.9287171363830566}
2025-01-15 05:39:41,737 [INFO] Step[950/2713]: training loss : 0.9282408261299133 TRAIN  loss dict:  {'classification_loss': 0.9282408261299133}
2025-01-15 05:39:55,103 [INFO] Step[1000/2713]: training loss : 0.9295841443538666 TRAIN  loss dict:  {'classification_loss': 0.9295841443538666}
2025-01-15 05:40:09,120 [INFO] Step[1050/2713]: training loss : 0.9287398838996888 TRAIN  loss dict:  {'classification_loss': 0.9287398838996888}
2025-01-15 05:40:23,243 [INFO] Step[1100/2713]: training loss : 0.9279982852935791 TRAIN  loss dict:  {'classification_loss': 0.9279982852935791}
2025-01-15 05:40:37,496 [INFO] Step[1150/2713]: training loss : 0.9537462162971496 TRAIN  loss dict:  {'classification_loss': 0.9537462162971496}
2025-01-15 05:40:51,274 [INFO] Step[1200/2713]: training loss : 0.9291226089000701 TRAIN  loss dict:  {'classification_loss': 0.9291226089000701}
2025-01-15 05:41:05,533 [INFO] Step[1250/2713]: training loss : 0.9291061413288116 TRAIN  loss dict:  {'classification_loss': 0.9291061413288116}
2025-01-15 05:41:18,823 [INFO] Step[1300/2713]: training loss : 0.9275655138492584 TRAIN  loss dict:  {'classification_loss': 0.9275655138492584}
2025-01-15 05:41:32,298 [INFO] Step[1350/2713]: training loss : 0.9277787983417511 TRAIN  loss dict:  {'classification_loss': 0.9277787983417511}
2025-01-15 05:41:45,904 [INFO] Step[1400/2713]: training loss : 0.9285654163360596 TRAIN  loss dict:  {'classification_loss': 0.9285654163360596}
2025-01-15 05:41:59,829 [INFO] Step[1450/2713]: training loss : 0.9284584689140319 TRAIN  loss dict:  {'classification_loss': 0.9284584689140319}
2025-01-15 05:42:13,306 [INFO] Step[1500/2713]: training loss : 0.9289688777923584 TRAIN  loss dict:  {'classification_loss': 0.9289688777923584}
2025-01-15 05:42:26,910 [INFO] Step[1550/2713]: training loss : 0.92752476811409 TRAIN  loss dict:  {'classification_loss': 0.92752476811409}
2025-01-15 05:42:41,119 [INFO] Step[1600/2713]: training loss : 0.9282916724681854 TRAIN  loss dict:  {'classification_loss': 0.9282916724681854}
2025-01-15 05:42:54,949 [INFO] Step[1650/2713]: training loss : 0.9317083263397217 TRAIN  loss dict:  {'classification_loss': 0.9317083263397217}
2025-01-15 05:43:08,891 [INFO] Step[1700/2713]: training loss : 0.9286265075206757 TRAIN  loss dict:  {'classification_loss': 0.9286265075206757}
2025-01-15 05:43:22,644 [INFO] Step[1750/2713]: training loss : 0.9287268269062042 TRAIN  loss dict:  {'classification_loss': 0.9287268269062042}
2025-01-15 05:43:36,496 [INFO] Step[1800/2713]: training loss : 0.928696128129959 TRAIN  loss dict:  {'classification_loss': 0.928696128129959}
2025-01-15 05:43:49,989 [INFO] Step[1850/2713]: training loss : 0.927934775352478 TRAIN  loss dict:  {'classification_loss': 0.927934775352478}
2025-01-15 05:44:03,503 [INFO] Step[1900/2713]: training loss : 0.9286850905418396 TRAIN  loss dict:  {'classification_loss': 0.9286850905418396}
2025-01-15 05:44:17,090 [INFO] Step[1950/2713]: training loss : 0.9279969477653504 TRAIN  loss dict:  {'classification_loss': 0.9279969477653504}
2025-01-15 05:44:31,001 [INFO] Step[2000/2713]: training loss : 0.9287435102462769 TRAIN  loss dict:  {'classification_loss': 0.9287435102462769}
2025-01-15 05:44:44,270 [INFO] Step[2050/2713]: training loss : 0.9306166398525239 TRAIN  loss dict:  {'classification_loss': 0.9306166398525239}
2025-01-15 05:44:57,537 [INFO] Step[2100/2713]: training loss : 0.9293173730373383 TRAIN  loss dict:  {'classification_loss': 0.9293173730373383}
2025-01-15 05:45:11,086 [INFO] Step[2150/2713]: training loss : 0.968447333574295 TRAIN  loss dict:  {'classification_loss': 0.968447333574295}
2025-01-15 05:45:24,808 [INFO] Step[2200/2713]: training loss : 0.9283124017715454 TRAIN  loss dict:  {'classification_loss': 0.9283124017715454}
2025-01-15 05:45:38,876 [INFO] Step[2250/2713]: training loss : 0.9282224977016449 TRAIN  loss dict:  {'classification_loss': 0.9282224977016449}
2025-01-15 05:45:52,833 [INFO] Step[2300/2713]: training loss : 0.927590047121048 TRAIN  loss dict:  {'classification_loss': 0.927590047121048}
2025-01-15 05:46:06,601 [INFO] Step[2350/2713]: training loss : 0.9289736592769623 TRAIN  loss dict:  {'classification_loss': 0.9289736592769623}
2025-01-15 05:46:20,205 [INFO] Step[2400/2713]: training loss : 0.9293288803100586 TRAIN  loss dict:  {'classification_loss': 0.9293288803100586}
2025-01-15 05:46:34,353 [INFO] Step[2450/2713]: training loss : 0.9276988279819488 TRAIN  loss dict:  {'classification_loss': 0.9276988279819488}
2025-01-15 05:46:48,150 [INFO] Step[2500/2713]: training loss : 0.9281738078594208 TRAIN  loss dict:  {'classification_loss': 0.9281738078594208}
2025-01-15 05:47:01,668 [INFO] Step[2550/2713]: training loss : 0.92825345993042 TRAIN  loss dict:  {'classification_loss': 0.92825345993042}
2025-01-15 05:47:15,329 [INFO] Step[2600/2713]: training loss : 0.9288791990280152 TRAIN  loss dict:  {'classification_loss': 0.9288791990280152}
2025-01-15 05:47:29,097 [INFO] Step[2650/2713]: training loss : 0.9289360761642456 TRAIN  loss dict:  {'classification_loss': 0.9289360761642456}
2025-01-15 05:47:43,026 [INFO] Step[2700/2713]: training loss : 0.928973023891449 TRAIN  loss dict:  {'classification_loss': 0.928973023891449}
2025-01-15 05:49:01,106 [INFO] Label accuracies statistics:
2025-01-15 05:49:01,107 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.25, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 05:49:01,108 [INFO] [80] TRAIN  loss: 0.9298246511224887 acc: 0.9996314043494287
2025-01-15 05:49:01,108 [INFO] [80] TRAIN  loss dict: {'classification_loss': 0.9298246511224887}
2025-01-15 05:49:01,109 [INFO] [80] VALIDATION loss: 1.7955579854043804 VALIDATION acc: 0.8081504702194358
2025-01-15 05:49:01,109 [INFO] [80] VALIDATION loss dict: {'classification_loss': 1.7955579854043804}
2025-01-15 05:49:01,109 [INFO] 
2025-01-15 05:49:20,198 [INFO] Step[50/2713]: training loss : 0.929107905626297 TRAIN  loss dict:  {'classification_loss': 0.929107905626297}
2025-01-15 05:49:33,951 [INFO] Step[100/2713]: training loss : 0.9271995234489441 TRAIN  loss dict:  {'classification_loss': 0.9271995234489441}
2025-01-15 05:49:47,445 [INFO] Step[150/2713]: training loss : 0.9277050733566284 TRAIN  loss dict:  {'classification_loss': 0.9277050733566284}
2025-01-15 05:50:01,348 [INFO] Step[200/2713]: training loss : 0.9274554789066315 TRAIN  loss dict:  {'classification_loss': 0.9274554789066315}
2025-01-15 05:50:15,086 [INFO] Step[250/2713]: training loss : 0.9279374325275421 TRAIN  loss dict:  {'classification_loss': 0.9279374325275421}
2025-01-15 05:50:29,077 [INFO] Step[300/2713]: training loss : 0.9293279826641083 TRAIN  loss dict:  {'classification_loss': 0.9293279826641083}
2025-01-15 05:50:43,010 [INFO] Step[350/2713]: training loss : 0.9277878165245056 TRAIN  loss dict:  {'classification_loss': 0.9277878165245056}
2025-01-15 05:50:57,266 [INFO] Step[400/2713]: training loss : 0.9332234227657318 TRAIN  loss dict:  {'classification_loss': 0.9332234227657318}
2025-01-15 05:51:11,277 [INFO] Step[450/2713]: training loss : 0.9278819966316223 TRAIN  loss dict:  {'classification_loss': 0.9278819966316223}
2025-01-15 05:51:25,233 [INFO] Step[500/2713]: training loss : 0.9286980283260345 TRAIN  loss dict:  {'classification_loss': 0.9286980283260345}
2025-01-15 05:51:38,524 [INFO] Step[550/2713]: training loss : 0.9286337900161743 TRAIN  loss dict:  {'classification_loss': 0.9286337900161743}
2025-01-15 05:51:52,161 [INFO] Step[600/2713]: training loss : 0.9281778061389923 TRAIN  loss dict:  {'classification_loss': 0.9281778061389923}
2025-01-15 05:52:06,288 [INFO] Step[650/2713]: training loss : 0.9292381000518799 TRAIN  loss dict:  {'classification_loss': 0.9292381000518799}
2025-01-15 05:52:20,008 [INFO] Step[700/2713]: training loss : 0.9272730398178101 TRAIN  loss dict:  {'classification_loss': 0.9272730398178101}
2025-01-15 05:52:33,269 [INFO] Step[750/2713]: training loss : 0.9284666669368744 TRAIN  loss dict:  {'classification_loss': 0.9284666669368744}
2025-01-15 05:52:46,907 [INFO] Step[800/2713]: training loss : 0.9284408295154571 TRAIN  loss dict:  {'classification_loss': 0.9284408295154571}
2025-01-15 05:53:01,120 [INFO] Step[850/2713]: training loss : 0.9284286761283874 TRAIN  loss dict:  {'classification_loss': 0.9284286761283874}
2025-01-15 05:53:15,119 [INFO] Step[900/2713]: training loss : 0.92889683842659 TRAIN  loss dict:  {'classification_loss': 0.92889683842659}
2025-01-15 05:53:28,648 [INFO] Step[950/2713]: training loss : 0.9279989290237427 TRAIN  loss dict:  {'classification_loss': 0.9279989290237427}
2025-01-15 05:53:42,644 [INFO] Step[1000/2713]: training loss : 0.9306872987747192 TRAIN  loss dict:  {'classification_loss': 0.9306872987747192}
2025-01-15 05:53:56,329 [INFO] Step[1050/2713]: training loss : 0.9273242843151093 TRAIN  loss dict:  {'classification_loss': 0.9273242843151093}
2025-01-15 05:54:09,711 [INFO] Step[1100/2713]: training loss : 0.927617243528366 TRAIN  loss dict:  {'classification_loss': 0.927617243528366}
2025-01-15 05:54:23,409 [INFO] Step[1150/2713]: training loss : 0.9285178625583649 TRAIN  loss dict:  {'classification_loss': 0.9285178625583649}
2025-01-15 05:54:36,869 [INFO] Step[1200/2713]: training loss : 0.9354743921756744 TRAIN  loss dict:  {'classification_loss': 0.9354743921756744}
2025-01-15 05:54:50,852 [INFO] Step[1250/2713]: training loss : 0.9311528956890106 TRAIN  loss dict:  {'classification_loss': 0.9311528956890106}
2025-01-15 05:55:04,081 [INFO] Step[1300/2713]: training loss : 0.928043771982193 TRAIN  loss dict:  {'classification_loss': 0.928043771982193}
2025-01-15 05:55:17,709 [INFO] Step[1350/2713]: training loss : 0.9283167326450348 TRAIN  loss dict:  {'classification_loss': 0.9283167326450348}
2025-01-15 05:55:31,168 [INFO] Step[1400/2713]: training loss : 0.9272017681598663 TRAIN  loss dict:  {'classification_loss': 0.9272017681598663}
2025-01-15 05:55:44,895 [INFO] Step[1450/2713]: training loss : 0.9274422359466553 TRAIN  loss dict:  {'classification_loss': 0.9274422359466553}
2025-01-15 05:55:58,401 [INFO] Step[1500/2713]: training loss : 0.9288311231136323 TRAIN  loss dict:  {'classification_loss': 0.9288311231136323}
2025-01-15 05:56:11,679 [INFO] Step[1550/2713]: training loss : 0.9286899471282959 TRAIN  loss dict:  {'classification_loss': 0.9286899471282959}
2025-01-15 05:56:25,280 [INFO] Step[1600/2713]: training loss : 0.9280645322799682 TRAIN  loss dict:  {'classification_loss': 0.9280645322799682}
2025-01-15 05:56:38,940 [INFO] Step[1650/2713]: training loss : 0.9610639190673829 TRAIN  loss dict:  {'classification_loss': 0.9610639190673829}
2025-01-15 05:56:52,190 [INFO] Step[1700/2713]: training loss : 0.9275329256057739 TRAIN  loss dict:  {'classification_loss': 0.9275329256057739}
2025-01-15 05:57:05,711 [INFO] Step[1750/2713]: training loss : 0.927957866191864 TRAIN  loss dict:  {'classification_loss': 0.927957866191864}
2025-01-15 05:57:20,642 [INFO] Step[1800/2713]: training loss : 0.9271864163875579 TRAIN  loss dict:  {'classification_loss': 0.9271864163875579}
2025-01-15 05:57:36,871 [INFO] Step[1850/2713]: training loss : 0.9292103290557862 TRAIN  loss dict:  {'classification_loss': 0.9292103290557862}
2025-01-15 05:57:50,834 [INFO] Step[1900/2713]: training loss : 0.9305592453479767 TRAIN  loss dict:  {'classification_loss': 0.9305592453479767}
2025-01-15 05:58:04,252 [INFO] Step[1950/2713]: training loss : 0.9278505647182465 TRAIN  loss dict:  {'classification_loss': 0.9278505647182465}
2025-01-15 05:58:17,755 [INFO] Step[2000/2713]: training loss : 0.9279246973991394 TRAIN  loss dict:  {'classification_loss': 0.9279246973991394}
2025-01-15 05:58:31,723 [INFO] Step[2050/2713]: training loss : 0.9273764955997467 TRAIN  loss dict:  {'classification_loss': 0.9273764955997467}
2025-01-15 05:58:45,006 [INFO] Step[2100/2713]: training loss : 0.9277308869361878 TRAIN  loss dict:  {'classification_loss': 0.9277308869361878}
2025-01-15 05:58:58,284 [INFO] Step[2150/2713]: training loss : 0.9279462385177613 TRAIN  loss dict:  {'classification_loss': 0.9279462385177613}
2025-01-15 05:59:11,523 [INFO] Step[2200/2713]: training loss : 0.9274049675464631 TRAIN  loss dict:  {'classification_loss': 0.9274049675464631}
2025-01-15 05:59:24,861 [INFO] Step[2250/2713]: training loss : 0.9341809856891632 TRAIN  loss dict:  {'classification_loss': 0.9341809856891632}
2025-01-15 05:59:38,968 [INFO] Step[2300/2713]: training loss : 0.9287498259544372 TRAIN  loss dict:  {'classification_loss': 0.9287498259544372}
2025-01-15 05:59:52,188 [INFO] Step[2350/2713]: training loss : 0.928246146440506 TRAIN  loss dict:  {'classification_loss': 0.928246146440506}
2025-01-15 06:00:06,108 [INFO] Step[2400/2713]: training loss : 0.9279322099685668 TRAIN  loss dict:  {'classification_loss': 0.9279322099685668}
2025-01-15 06:00:19,532 [INFO] Step[2450/2713]: training loss : 0.9280638921260834 TRAIN  loss dict:  {'classification_loss': 0.9280638921260834}
2025-01-15 06:00:33,438 [INFO] Step[2500/2713]: training loss : 0.9278255808353424 TRAIN  loss dict:  {'classification_loss': 0.9278255808353424}
2025-01-15 06:00:47,666 [INFO] Step[2550/2713]: training loss : 0.9272126162052154 TRAIN  loss dict:  {'classification_loss': 0.9272126162052154}
2025-01-15 06:01:01,210 [INFO] Step[2600/2713]: training loss : 0.9272566902637481 TRAIN  loss dict:  {'classification_loss': 0.9272566902637481}
2025-01-15 06:01:14,748 [INFO] Step[2650/2713]: training loss : 0.9275871992111206 TRAIN  loss dict:  {'classification_loss': 0.9275871992111206}
2025-01-15 06:01:28,042 [INFO] Step[2700/2713]: training loss : 0.9276330411434174 TRAIN  loss dict:  {'classification_loss': 0.9276330411434174}
2025-01-15 06:02:44,017 [INFO] Label accuracies statistics:
2025-01-15 06:02:44,017 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 06:02:44,019 [INFO] [81] TRAIN  loss: 0.929132040646346 acc: 0.9997542695662858
2025-01-15 06:02:44,019 [INFO] [81] TRAIN  loss dict: {'classification_loss': 0.929132040646346}
2025-01-15 06:02:44,019 [INFO] [81] VALIDATION loss: 1.7662313647736283 VALIDATION acc: 0.8131661442006269
2025-01-15 06:02:44,019 [INFO] [81] VALIDATION loss dict: {'classification_loss': 1.7662313647736283}
2025-01-15 06:02:44,019 [INFO] 
2025-01-15 06:03:02,241 [INFO] Step[50/2713]: training loss : 0.9288323187828064 TRAIN  loss dict:  {'classification_loss': 0.9288323187828064}
2025-01-15 06:03:15,385 [INFO] Step[100/2713]: training loss : 0.9283609688282013 TRAIN  loss dict:  {'classification_loss': 0.9283609688282013}
2025-01-15 06:03:28,877 [INFO] Step[150/2713]: training loss : 0.9281338727474213 TRAIN  loss dict:  {'classification_loss': 0.9281338727474213}
2025-01-15 06:03:42,472 [INFO] Step[200/2713]: training loss : 0.9281353163719177 TRAIN  loss dict:  {'classification_loss': 0.9281353163719177}
2025-01-15 06:03:56,100 [INFO] Step[250/2713]: training loss : 0.9276847100257873 TRAIN  loss dict:  {'classification_loss': 0.9276847100257873}
2025-01-15 06:04:10,170 [INFO] Step[300/2713]: training loss : 0.9272734618186951 TRAIN  loss dict:  {'classification_loss': 0.9272734618186951}
2025-01-15 06:04:24,238 [INFO] Step[350/2713]: training loss : 0.9277644622325897 TRAIN  loss dict:  {'classification_loss': 0.9277644622325897}
2025-01-15 06:04:38,209 [INFO] Step[400/2713]: training loss : 0.9283248913288117 TRAIN  loss dict:  {'classification_loss': 0.9283248913288117}
2025-01-15 06:04:51,901 [INFO] Step[450/2713]: training loss : 0.927745691537857 TRAIN  loss dict:  {'classification_loss': 0.927745691537857}
2025-01-15 06:05:05,471 [INFO] Step[500/2713]: training loss : 0.9275443196296692 TRAIN  loss dict:  {'classification_loss': 0.9275443196296692}
2025-01-15 06:05:19,410 [INFO] Step[550/2713]: training loss : 0.9281870520114899 TRAIN  loss dict:  {'classification_loss': 0.9281870520114899}
2025-01-15 06:05:33,244 [INFO] Step[600/2713]: training loss : 0.9271752810478211 TRAIN  loss dict:  {'classification_loss': 0.9271752810478211}
2025-01-15 06:05:46,573 [INFO] Step[650/2713]: training loss : 0.9467497241497039 TRAIN  loss dict:  {'classification_loss': 0.9467497241497039}
2025-01-15 06:06:00,181 [INFO] Step[700/2713]: training loss : 0.9281762158870697 TRAIN  loss dict:  {'classification_loss': 0.9281762158870697}
2025-01-15 06:06:13,817 [INFO] Step[750/2713]: training loss : 0.9275251805782319 TRAIN  loss dict:  {'classification_loss': 0.9275251805782319}
2025-01-15 06:06:27,310 [INFO] Step[800/2713]: training loss : 0.9323184645175934 TRAIN  loss dict:  {'classification_loss': 0.9323184645175934}
2025-01-15 06:06:40,861 [INFO] Step[850/2713]: training loss : 0.9555674588680267 TRAIN  loss dict:  {'classification_loss': 0.9555674588680267}
2025-01-15 06:06:54,253 [INFO] Step[900/2713]: training loss : 0.9304673230648041 TRAIN  loss dict:  {'classification_loss': 0.9304673230648041}
2025-01-15 06:07:07,986 [INFO] Step[950/2713]: training loss : 0.927953280210495 TRAIN  loss dict:  {'classification_loss': 0.927953280210495}
2025-01-15 06:07:21,243 [INFO] Step[1000/2713]: training loss : 0.9280648303031921 TRAIN  loss dict:  {'classification_loss': 0.9280648303031921}
2025-01-15 06:07:34,743 [INFO] Step[1050/2713]: training loss : 0.9285851323604584 TRAIN  loss dict:  {'classification_loss': 0.9285851323604584}
2025-01-15 06:07:48,370 [INFO] Step[1100/2713]: training loss : 0.934867103099823 TRAIN  loss dict:  {'classification_loss': 0.934867103099823}
2025-01-15 06:08:01,908 [INFO] Step[1150/2713]: training loss : 0.9283453607559204 TRAIN  loss dict:  {'classification_loss': 0.9283453607559204}
2025-01-15 06:08:15,591 [INFO] Step[1200/2713]: training loss : 0.9280132830142975 TRAIN  loss dict:  {'classification_loss': 0.9280132830142975}
2025-01-15 06:08:29,631 [INFO] Step[1250/2713]: training loss : 0.9274116837978363 TRAIN  loss dict:  {'classification_loss': 0.9274116837978363}
2025-01-15 06:08:43,406 [INFO] Step[1300/2713]: training loss : 0.9274547040462494 TRAIN  loss dict:  {'classification_loss': 0.9274547040462494}
2025-01-15 06:08:56,941 [INFO] Step[1350/2713]: training loss : 0.9293608343601227 TRAIN  loss dict:  {'classification_loss': 0.9293608343601227}
2025-01-15 06:09:10,144 [INFO] Step[1400/2713]: training loss : 0.9289901697635651 TRAIN  loss dict:  {'classification_loss': 0.9289901697635651}
2025-01-15 06:09:23,404 [INFO] Step[1450/2713]: training loss : 0.9278050351142884 TRAIN  loss dict:  {'classification_loss': 0.9278050351142884}
2025-01-15 06:09:36,695 [INFO] Step[1500/2713]: training loss : 0.9290235459804534 TRAIN  loss dict:  {'classification_loss': 0.9290235459804534}
2025-01-15 06:09:50,585 [INFO] Step[1550/2713]: training loss : 0.9277195727825165 TRAIN  loss dict:  {'classification_loss': 0.9277195727825165}
2025-01-15 06:10:04,414 [INFO] Step[1600/2713]: training loss : 0.9400711691379547 TRAIN  loss dict:  {'classification_loss': 0.9400711691379547}
2025-01-15 06:10:18,317 [INFO] Step[1650/2713]: training loss : 0.9272945690155029 TRAIN  loss dict:  {'classification_loss': 0.9272945690155029}
2025-01-15 06:10:32,128 [INFO] Step[1700/2713]: training loss : 0.9280780959129333 TRAIN  loss dict:  {'classification_loss': 0.9280780959129333}
2025-01-15 06:10:45,527 [INFO] Step[1750/2713]: training loss : 0.928007835149765 TRAIN  loss dict:  {'classification_loss': 0.928007835149765}
2025-01-15 06:10:58,817 [INFO] Step[1800/2713]: training loss : 0.9280601930618286 TRAIN  loss dict:  {'classification_loss': 0.9280601930618286}
2025-01-15 06:11:12,176 [INFO] Step[1850/2713]: training loss : 0.9280946207046509 TRAIN  loss dict:  {'classification_loss': 0.9280946207046509}
2025-01-15 06:11:25,847 [INFO] Step[1900/2713]: training loss : 0.9376390278339386 TRAIN  loss dict:  {'classification_loss': 0.9376390278339386}
2025-01-15 06:11:39,409 [INFO] Step[1950/2713]: training loss : 0.9287404227256775 TRAIN  loss dict:  {'classification_loss': 0.9287404227256775}
2025-01-15 06:11:53,223 [INFO] Step[2000/2713]: training loss : 0.9283009517192841 TRAIN  loss dict:  {'classification_loss': 0.9283009517192841}
2025-01-15 06:12:06,452 [INFO] Step[2050/2713]: training loss : 0.9288949120044708 TRAIN  loss dict:  {'classification_loss': 0.9288949120044708}
2025-01-15 06:12:23,349 [INFO] Step[2100/2713]: training loss : 0.9283893620967865 TRAIN  loss dict:  {'classification_loss': 0.9283893620967865}
2025-01-15 06:12:37,280 [INFO] Step[2150/2713]: training loss : 0.9277253484725952 TRAIN  loss dict:  {'classification_loss': 0.9277253484725952}
2025-01-15 06:12:51,108 [INFO] Step[2200/2713]: training loss : 0.9289191377162933 TRAIN  loss dict:  {'classification_loss': 0.9289191377162933}
2025-01-15 06:13:05,164 [INFO] Step[2250/2713]: training loss : 0.9284723973274231 TRAIN  loss dict:  {'classification_loss': 0.9284723973274231}
2025-01-15 06:13:18,427 [INFO] Step[2300/2713]: training loss : 0.9276969981193542 TRAIN  loss dict:  {'classification_loss': 0.9276969981193542}
2025-01-15 06:13:32,059 [INFO] Step[2350/2713]: training loss : 0.9288015520572662 TRAIN  loss dict:  {'classification_loss': 0.9288015520572662}
2025-01-15 06:13:45,902 [INFO] Step[2400/2713]: training loss : 0.9286599338054657 TRAIN  loss dict:  {'classification_loss': 0.9286599338054657}
2025-01-15 06:13:59,531 [INFO] Step[2450/2713]: training loss : 0.9354671323299408 TRAIN  loss dict:  {'classification_loss': 0.9354671323299408}
2025-01-15 06:14:13,141 [INFO] Step[2500/2713]: training loss : 0.9290159201622009 TRAIN  loss dict:  {'classification_loss': 0.9290159201622009}
2025-01-15 06:14:26,570 [INFO] Step[2550/2713]: training loss : 0.9279608023166657 TRAIN  loss dict:  {'classification_loss': 0.9279608023166657}
2025-01-15 06:14:40,178 [INFO] Step[2600/2713]: training loss : 0.9283672213554383 TRAIN  loss dict:  {'classification_loss': 0.9283672213554383}
2025-01-15 06:14:53,920 [INFO] Step[2650/2713]: training loss : 0.9277774631977082 TRAIN  loss dict:  {'classification_loss': 0.9277774631977082}
2025-01-15 06:15:07,256 [INFO] Step[2700/2713]: training loss : 0.927276520729065 TRAIN  loss dict:  {'classification_loss': 0.927276520729065}
2025-01-15 06:16:23,143 [INFO] Label accuracies statistics:
2025-01-15 06:16:23,143 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.5, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 06:16:23,145 [INFO] [82] TRAIN  loss: 0.9297496204968533 acc: 0.9992628086988573
2025-01-15 06:16:23,145 [INFO] [82] TRAIN  loss dict: {'classification_loss': 0.9297496204968533}
2025-01-15 06:16:23,145 [INFO] [82] VALIDATION loss: 1.7924598111469943 VALIDATION acc: 0.8081504702194358
2025-01-15 06:16:23,145 [INFO] [82] VALIDATION loss dict: {'classification_loss': 1.7924598111469943}
2025-01-15 06:16:23,145 [INFO] 
2025-01-15 06:16:42,827 [INFO] Step[50/2713]: training loss : 0.9284773802757263 TRAIN  loss dict:  {'classification_loss': 0.9284773802757263}
2025-01-15 06:16:56,491 [INFO] Step[100/2713]: training loss : 0.9277460956573487 TRAIN  loss dict:  {'classification_loss': 0.9277460956573487}
2025-01-15 06:17:10,711 [INFO] Step[150/2713]: training loss : 0.927799528837204 TRAIN  loss dict:  {'classification_loss': 0.927799528837204}
2025-01-15 06:17:24,397 [INFO] Step[200/2713]: training loss : 0.9276538729667664 TRAIN  loss dict:  {'classification_loss': 0.9276538729667664}
2025-01-15 06:17:38,061 [INFO] Step[250/2713]: training loss : 0.9279628837108612 TRAIN  loss dict:  {'classification_loss': 0.9279628837108612}
2025-01-15 06:17:51,619 [INFO] Step[300/2713]: training loss : 0.9282038199901581 TRAIN  loss dict:  {'classification_loss': 0.9282038199901581}
2025-01-15 06:18:04,983 [INFO] Step[350/2713]: training loss : 0.9277468597888947 TRAIN  loss dict:  {'classification_loss': 0.9277468597888947}
2025-01-15 06:18:19,114 [INFO] Step[400/2713]: training loss : 0.9278290748596192 TRAIN  loss dict:  {'classification_loss': 0.9278290748596192}
2025-01-15 06:18:32,640 [INFO] Step[450/2713]: training loss : 0.9285133504867553 TRAIN  loss dict:  {'classification_loss': 0.9285133504867553}
2025-01-15 06:18:46,363 [INFO] Step[500/2713]: training loss : 0.9271619284152984 TRAIN  loss dict:  {'classification_loss': 0.9271619284152984}
2025-01-15 06:19:00,201 [INFO] Step[550/2713]: training loss : 0.9348413395881653 TRAIN  loss dict:  {'classification_loss': 0.9348413395881653}
2025-01-15 06:19:13,868 [INFO] Step[600/2713]: training loss : 0.9282172441482544 TRAIN  loss dict:  {'classification_loss': 0.9282172441482544}
2025-01-15 06:19:27,538 [INFO] Step[650/2713]: training loss : 0.927614369392395 TRAIN  loss dict:  {'classification_loss': 0.927614369392395}
2025-01-15 06:19:41,099 [INFO] Step[700/2713]: training loss : 0.9272227144241333 TRAIN  loss dict:  {'classification_loss': 0.9272227144241333}
2025-01-15 06:19:54,709 [INFO] Step[750/2713]: training loss : 0.9280235242843627 TRAIN  loss dict:  {'classification_loss': 0.9280235242843627}
2025-01-15 06:20:07,950 [INFO] Step[800/2713]: training loss : 0.9281160819530487 TRAIN  loss dict:  {'classification_loss': 0.9281160819530487}
2025-01-15 06:20:21,517 [INFO] Step[850/2713]: training loss : 0.9365831625461578 TRAIN  loss dict:  {'classification_loss': 0.9365831625461578}
2025-01-15 06:20:34,640 [INFO] Step[900/2713]: training loss : 0.9283590662479401 TRAIN  loss dict:  {'classification_loss': 0.9283590662479401}
2025-01-15 06:20:48,070 [INFO] Step[950/2713]: training loss : 0.9286607503890991 TRAIN  loss dict:  {'classification_loss': 0.9286607503890991}
2025-01-15 06:21:01,801 [INFO] Step[1000/2713]: training loss : 0.9277658951282501 TRAIN  loss dict:  {'classification_loss': 0.9277658951282501}
2025-01-15 06:21:15,091 [INFO] Step[1050/2713]: training loss : 0.927784481048584 TRAIN  loss dict:  {'classification_loss': 0.927784481048584}
2025-01-15 06:21:28,300 [INFO] Step[1100/2713]: training loss : 0.9279768419265747 TRAIN  loss dict:  {'classification_loss': 0.9279768419265747}
2025-01-15 06:21:41,835 [INFO] Step[1150/2713]: training loss : 0.9272929096221924 TRAIN  loss dict:  {'classification_loss': 0.9272929096221924}
2025-01-15 06:21:55,684 [INFO] Step[1200/2713]: training loss : 0.9280183947086335 TRAIN  loss dict:  {'classification_loss': 0.9280183947086335}
2025-01-15 06:22:09,078 [INFO] Step[1250/2713]: training loss : 0.9277296769618988 TRAIN  loss dict:  {'classification_loss': 0.9277296769618988}
2025-01-15 06:22:22,993 [INFO] Step[1300/2713]: training loss : 0.9281804251670838 TRAIN  loss dict:  {'classification_loss': 0.9281804251670838}
2025-01-15 06:22:36,579 [INFO] Step[1350/2713]: training loss : 0.9285627579689026 TRAIN  loss dict:  {'classification_loss': 0.9285627579689026}
2025-01-15 06:22:50,507 [INFO] Step[1400/2713]: training loss : 0.9277676713466644 TRAIN  loss dict:  {'classification_loss': 0.9277676713466644}
2025-01-15 06:23:04,179 [INFO] Step[1450/2713]: training loss : 0.9280645632743836 TRAIN  loss dict:  {'classification_loss': 0.9280645632743836}
2025-01-15 06:23:17,878 [INFO] Step[1500/2713]: training loss : 0.9280632996559143 TRAIN  loss dict:  {'classification_loss': 0.9280632996559143}
2025-01-15 06:23:32,049 [INFO] Step[1550/2713]: training loss : 0.9481354105472565 TRAIN  loss dict:  {'classification_loss': 0.9481354105472565}
2025-01-15 06:23:45,659 [INFO] Step[1600/2713]: training loss : 0.928153361082077 TRAIN  loss dict:  {'classification_loss': 0.928153361082077}
2025-01-15 06:23:59,282 [INFO] Step[1650/2713]: training loss : 0.9277688646316529 TRAIN  loss dict:  {'classification_loss': 0.9277688646316529}
2025-01-15 06:24:12,944 [INFO] Step[1700/2713]: training loss : 0.9281497955322265 TRAIN  loss dict:  {'classification_loss': 0.9281497955322265}
2025-01-15 06:24:26,183 [INFO] Step[1750/2713]: training loss : 0.927942533493042 TRAIN  loss dict:  {'classification_loss': 0.927942533493042}
2025-01-15 06:24:39,296 [INFO] Step[1800/2713]: training loss : 0.9278957998752594 TRAIN  loss dict:  {'classification_loss': 0.9278957998752594}
2025-01-15 06:24:52,869 [INFO] Step[1850/2713]: training loss : 0.9279455411434173 TRAIN  loss dict:  {'classification_loss': 0.9279455411434173}
2025-01-15 06:25:06,420 [INFO] Step[1900/2713]: training loss : 0.9294672083854675 TRAIN  loss dict:  {'classification_loss': 0.9294672083854675}
2025-01-15 06:25:19,925 [INFO] Step[1950/2713]: training loss : 0.927884488105774 TRAIN  loss dict:  {'classification_loss': 0.927884488105774}
2025-01-15 06:25:33,112 [INFO] Step[2000/2713]: training loss : 0.9280491447448731 TRAIN  loss dict:  {'classification_loss': 0.9280491447448731}
2025-01-15 06:25:46,611 [INFO] Step[2050/2713]: training loss : 0.927968418598175 TRAIN  loss dict:  {'classification_loss': 0.927968418598175}
2025-01-15 06:26:00,054 [INFO] Step[2100/2713]: training loss : 0.9277724647521972 TRAIN  loss dict:  {'classification_loss': 0.9277724647521972}
2025-01-15 06:26:13,624 [INFO] Step[2150/2713]: training loss : 0.927208993434906 TRAIN  loss dict:  {'classification_loss': 0.927208993434906}
2025-01-15 06:26:27,213 [INFO] Step[2200/2713]: training loss : 0.9330119502544403 TRAIN  loss dict:  {'classification_loss': 0.9330119502544403}
2025-01-15 06:26:41,019 [INFO] Step[2250/2713]: training loss : 0.9282823741436005 TRAIN  loss dict:  {'classification_loss': 0.9282823741436005}
2025-01-15 06:26:54,874 [INFO] Step[2300/2713]: training loss : 0.9269266831874847 TRAIN  loss dict:  {'classification_loss': 0.9269266831874847}
2025-01-15 06:27:08,142 [INFO] Step[2350/2713]: training loss : 0.9287105238437653 TRAIN  loss dict:  {'classification_loss': 0.9287105238437653}
2025-01-15 06:27:22,193 [INFO] Step[2400/2713]: training loss : 0.9277113318443299 TRAIN  loss dict:  {'classification_loss': 0.9277113318443299}
2025-01-15 06:27:35,877 [INFO] Step[2450/2713]: training loss : 0.928709808588028 TRAIN  loss dict:  {'classification_loss': 0.928709808588028}
2025-01-15 06:27:49,653 [INFO] Step[2500/2713]: training loss : 0.9275422406196594 TRAIN  loss dict:  {'classification_loss': 0.9275422406196594}
2025-01-15 06:28:03,880 [INFO] Step[2550/2713]: training loss : 0.9273821330070495 TRAIN  loss dict:  {'classification_loss': 0.9273821330070495}
2025-01-15 06:28:17,629 [INFO] Step[2600/2713]: training loss : 0.9296342003345489 TRAIN  loss dict:  {'classification_loss': 0.9296342003345489}
2025-01-15 06:28:31,547 [INFO] Step[2650/2713]: training loss : 0.9296944320201874 TRAIN  loss dict:  {'classification_loss': 0.9296944320201874}
2025-01-15 06:28:45,153 [INFO] Step[2700/2713]: training loss : 0.9344810271263122 TRAIN  loss dict:  {'classification_loss': 0.9344810271263122}
2025-01-15 06:30:01,855 [INFO] Label accuracies statistics:
2025-01-15 06:30:01,855 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.5, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.0, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.5, 339: 0.75, 340: 0.5, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 06:30:01,857 [INFO] [83] TRAIN  loss: 0.9288890428135327 acc: 0.9998771347831429
2025-01-15 06:30:01,857 [INFO] [83] TRAIN  loss dict: {'classification_loss': 0.9288890428135327}
2025-01-15 06:30:01,857 [INFO] [83] VALIDATION loss: 1.8407428976064337 VALIDATION acc: 0.7981191222570533
2025-01-15 06:30:01,857 [INFO] [83] VALIDATION loss dict: {'classification_loss': 1.8407428976064337}
2025-01-15 06:30:01,857 [INFO] 
2025-01-15 06:30:20,131 [INFO] Step[50/2713]: training loss : 0.9323537528514863 TRAIN  loss dict:  {'classification_loss': 0.9323537528514863}
2025-01-15 06:30:33,648 [INFO] Step[100/2713]: training loss : 0.9280836617946625 TRAIN  loss dict:  {'classification_loss': 0.9280836617946625}
2025-01-15 06:30:47,813 [INFO] Step[150/2713]: training loss : 0.9275103569030761 TRAIN  loss dict:  {'classification_loss': 0.9275103569030761}
2025-01-15 06:31:01,823 [INFO] Step[200/2713]: training loss : 0.9271831917762756 TRAIN  loss dict:  {'classification_loss': 0.9271831917762756}
2025-01-15 06:31:15,497 [INFO] Step[250/2713]: training loss : 0.9273381161689759 TRAIN  loss dict:  {'classification_loss': 0.9273381161689759}
2025-01-15 06:31:29,210 [INFO] Step[300/2713]: training loss : 0.9305071854591369 TRAIN  loss dict:  {'classification_loss': 0.9305071854591369}
2025-01-15 06:31:42,929 [INFO] Step[350/2713]: training loss : 0.9277980256080628 TRAIN  loss dict:  {'classification_loss': 0.9277980256080628}
2025-01-15 06:31:56,614 [INFO] Step[400/2713]: training loss : 0.9276335704326629 TRAIN  loss dict:  {'classification_loss': 0.9276335704326629}
2025-01-15 06:32:09,953 [INFO] Step[450/2713]: training loss : 0.9277914869785309 TRAIN  loss dict:  {'classification_loss': 0.9277914869785309}
2025-01-15 06:32:23,454 [INFO] Step[500/2713]: training loss : 0.9273296558856964 TRAIN  loss dict:  {'classification_loss': 0.9273296558856964}
2025-01-15 06:32:37,464 [INFO] Step[550/2713]: training loss : 0.9705074143409729 TRAIN  loss dict:  {'classification_loss': 0.9705074143409729}
2025-01-15 06:32:51,482 [INFO] Step[600/2713]: training loss : 0.9282529544830322 TRAIN  loss dict:  {'classification_loss': 0.9282529544830322}
2025-01-15 06:33:04,911 [INFO] Step[650/2713]: training loss : 0.9281077122688294 TRAIN  loss dict:  {'classification_loss': 0.9281077122688294}
2025-01-15 06:33:18,876 [INFO] Step[700/2713]: training loss : 0.929687762260437 TRAIN  loss dict:  {'classification_loss': 0.929687762260437}
2025-01-15 06:33:32,521 [INFO] Step[750/2713]: training loss : 0.9289575040340423 TRAIN  loss dict:  {'classification_loss': 0.9289575040340423}
2025-01-15 06:33:46,134 [INFO] Step[800/2713]: training loss : 0.9282622146606445 TRAIN  loss dict:  {'classification_loss': 0.9282622146606445}
2025-01-15 06:33:59,596 [INFO] Step[850/2713]: training loss : 0.9276382493972778 TRAIN  loss dict:  {'classification_loss': 0.9276382493972778}
2025-01-15 06:34:13,002 [INFO] Step[900/2713]: training loss : 0.928665782213211 TRAIN  loss dict:  {'classification_loss': 0.928665782213211}
2025-01-15 06:34:26,208 [INFO] Step[950/2713]: training loss : 0.928407859802246 TRAIN  loss dict:  {'classification_loss': 0.928407859802246}
2025-01-15 06:34:39,295 [INFO] Step[1000/2713]: training loss : 0.9282529985904694 TRAIN  loss dict:  {'classification_loss': 0.9282529985904694}
2025-01-15 06:34:53,130 [INFO] Step[1050/2713]: training loss : 0.9278756773471832 TRAIN  loss dict:  {'classification_loss': 0.9278756773471832}
2025-01-15 06:35:06,705 [INFO] Step[1100/2713]: training loss : 0.928406400680542 TRAIN  loss dict:  {'classification_loss': 0.928406400680542}
2025-01-15 06:35:20,485 [INFO] Step[1150/2713]: training loss : 0.9275196516513824 TRAIN  loss dict:  {'classification_loss': 0.9275196516513824}
2025-01-15 06:35:33,730 [INFO] Step[1200/2713]: training loss : 0.9273042225837708 TRAIN  loss dict:  {'classification_loss': 0.9273042225837708}
2025-01-15 06:35:46,971 [INFO] Step[1250/2713]: training loss : 0.9285888481140137 TRAIN  loss dict:  {'classification_loss': 0.9285888481140137}
2025-01-15 06:36:00,521 [INFO] Step[1300/2713]: training loss : 0.9282692956924439 TRAIN  loss dict:  {'classification_loss': 0.9282692956924439}
2025-01-15 06:36:14,311 [INFO] Step[1350/2713]: training loss : 0.9275385689735413 TRAIN  loss dict:  {'classification_loss': 0.9275385689735413}
2025-01-15 06:36:27,769 [INFO] Step[1400/2713]: training loss : 0.9276803040504455 TRAIN  loss dict:  {'classification_loss': 0.9276803040504455}
2025-01-15 06:36:41,779 [INFO] Step[1450/2713]: training loss : 0.9274239814281464 TRAIN  loss dict:  {'classification_loss': 0.9274239814281464}
2025-01-15 06:36:55,348 [INFO] Step[1500/2713]: training loss : 0.9273794293403625 TRAIN  loss dict:  {'classification_loss': 0.9273794293403625}
2025-01-15 06:37:08,847 [INFO] Step[1550/2713]: training loss : 0.9283303272724152 TRAIN  loss dict:  {'classification_loss': 0.9283303272724152}
2025-01-15 06:37:22,818 [INFO] Step[1600/2713]: training loss : 0.9273903977870941 TRAIN  loss dict:  {'classification_loss': 0.9273903977870941}
2025-01-15 06:37:36,795 [INFO] Step[1650/2713]: training loss : 0.9279566884040833 TRAIN  loss dict:  {'classification_loss': 0.9279566884040833}
2025-01-15 06:37:50,635 [INFO] Step[1700/2713]: training loss : 0.9274398565292359 TRAIN  loss dict:  {'classification_loss': 0.9274398565292359}
2025-01-15 06:38:04,253 [INFO] Step[1750/2713]: training loss : 0.9283158278465271 TRAIN  loss dict:  {'classification_loss': 0.9283158278465271}
2025-01-15 06:38:17,601 [INFO] Step[1800/2713]: training loss : 0.9301605463027954 TRAIN  loss dict:  {'classification_loss': 0.9301605463027954}
2025-01-15 06:38:31,826 [INFO] Step[1850/2713]: training loss : 0.9272649383544922 TRAIN  loss dict:  {'classification_loss': 0.9272649383544922}
2025-01-15 06:38:45,442 [INFO] Step[1900/2713]: training loss : 0.9284030961990356 TRAIN  loss dict:  {'classification_loss': 0.9284030961990356}
2025-01-15 06:38:58,703 [INFO] Step[1950/2713]: training loss : 0.9285802209377289 TRAIN  loss dict:  {'classification_loss': 0.9285802209377289}
2025-01-15 06:39:12,262 [INFO] Step[2000/2713]: training loss : 0.9277659285068512 TRAIN  loss dict:  {'classification_loss': 0.9277659285068512}
2025-01-15 06:39:25,467 [INFO] Step[2050/2713]: training loss : 0.9272012376785278 TRAIN  loss dict:  {'classification_loss': 0.9272012376785278}
2025-01-15 06:39:39,071 [INFO] Step[2100/2713]: training loss : 0.927441394329071 TRAIN  loss dict:  {'classification_loss': 0.927441394329071}
2025-01-15 06:39:52,636 [INFO] Step[2150/2713]: training loss : 0.9277453088760376 TRAIN  loss dict:  {'classification_loss': 0.9277453088760376}
2025-01-15 06:40:06,555 [INFO] Step[2200/2713]: training loss : 0.9280719149112702 TRAIN  loss dict:  {'classification_loss': 0.9280719149112702}
2025-01-15 06:40:20,508 [INFO] Step[2250/2713]: training loss : 0.9284395968914032 TRAIN  loss dict:  {'classification_loss': 0.9284395968914032}
2025-01-15 06:40:34,677 [INFO] Step[2300/2713]: training loss : 0.9285144996643067 TRAIN  loss dict:  {'classification_loss': 0.9285144996643067}
2025-01-15 06:40:48,947 [INFO] Step[2350/2713]: training loss : 0.9271986865997315 TRAIN  loss dict:  {'classification_loss': 0.9271986865997315}
2025-01-15 06:41:02,978 [INFO] Step[2400/2713]: training loss : 0.9301421129703522 TRAIN  loss dict:  {'classification_loss': 0.9301421129703522}
2025-01-15 06:41:16,877 [INFO] Step[2450/2713]: training loss : 0.9276342260837555 TRAIN  loss dict:  {'classification_loss': 0.9276342260837555}
2025-01-15 06:41:30,485 [INFO] Step[2500/2713]: training loss : 0.9274342370033264 TRAIN  loss dict:  {'classification_loss': 0.9274342370033264}
2025-01-15 06:41:44,705 [INFO] Step[2550/2713]: training loss : 0.9276698493957519 TRAIN  loss dict:  {'classification_loss': 0.9276698493957519}
2025-01-15 06:41:58,165 [INFO] Step[2600/2713]: training loss : 0.9273019790649414 TRAIN  loss dict:  {'classification_loss': 0.9273019790649414}
2025-01-15 06:42:12,063 [INFO] Step[2650/2713]: training loss : 0.9271183300018311 TRAIN  loss dict:  {'classification_loss': 0.9271183300018311}
2025-01-15 06:42:25,234 [INFO] Step[2700/2713]: training loss : 0.9288876461982727 TRAIN  loss dict:  {'classification_loss': 0.9288876461982727}
2025-01-15 06:43:41,371 [INFO] Label accuracies statistics:
2025-01-15 06:43:41,372 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 06:43:42,535 [INFO] [84] TRAIN  loss: 0.9288982446897615 acc: 0.9998771347831429
2025-01-15 06:43:42,535 [INFO] [84] TRAIN  loss dict: {'classification_loss': 0.9288982446897615}
2025-01-15 06:43:42,535 [INFO] [84] VALIDATION loss: 1.7469370069360375 VALIDATION acc: 0.819435736677116
2025-01-15 06:43:42,535 [INFO] [84] VALIDATION loss dict: {'classification_loss': 1.7469370069360375}
2025-01-15 06:43:42,535 [INFO] 
2025-01-15 06:44:01,445 [INFO] Step[50/2713]: training loss : 0.9272612953186035 TRAIN  loss dict:  {'classification_loss': 0.9272612953186035}
2025-01-15 06:44:15,439 [INFO] Step[100/2713]: training loss : 0.927593299150467 TRAIN  loss dict:  {'classification_loss': 0.927593299150467}
2025-01-15 06:44:29,211 [INFO] Step[150/2713]: training loss : 0.9279789936542511 TRAIN  loss dict:  {'classification_loss': 0.9279789936542511}
2025-01-15 06:44:42,949 [INFO] Step[200/2713]: training loss : 0.9276302480697631 TRAIN  loss dict:  {'classification_loss': 0.9276302480697631}
2025-01-15 06:44:57,133 [INFO] Step[250/2713]: training loss : 0.9272394323348999 TRAIN  loss dict:  {'classification_loss': 0.9272394323348999}
2025-01-15 06:45:11,182 [INFO] Step[300/2713]: training loss : 0.9279487335681915 TRAIN  loss dict:  {'classification_loss': 0.9279487335681915}
2025-01-15 06:45:25,200 [INFO] Step[350/2713]: training loss : 0.927347502708435 TRAIN  loss dict:  {'classification_loss': 0.927347502708435}
2025-01-15 06:45:38,753 [INFO] Step[400/2713]: training loss : 0.9276449036598206 TRAIN  loss dict:  {'classification_loss': 0.9276449036598206}
2025-01-15 06:45:52,698 [INFO] Step[450/2713]: training loss : 0.9270925641059875 TRAIN  loss dict:  {'classification_loss': 0.9270925641059875}
2025-01-15 06:46:06,409 [INFO] Step[500/2713]: training loss : 0.9273297464847565 TRAIN  loss dict:  {'classification_loss': 0.9273297464847565}
2025-01-15 06:46:19,668 [INFO] Step[550/2713]: training loss : 0.9366320848464966 TRAIN  loss dict:  {'classification_loss': 0.9366320848464966}
2025-01-15 06:46:33,155 [INFO] Step[600/2713]: training loss : 0.9279652643203735 TRAIN  loss dict:  {'classification_loss': 0.9279652643203735}
2025-01-15 06:46:46,906 [INFO] Step[650/2713]: training loss : 0.9286061978340149 TRAIN  loss dict:  {'classification_loss': 0.9286061978340149}
2025-01-15 06:47:00,652 [INFO] Step[700/2713]: training loss : 0.9276075088977813 TRAIN  loss dict:  {'classification_loss': 0.9276075088977813}
2025-01-15 06:47:14,548 [INFO] Step[750/2713]: training loss : 0.9280080139636994 TRAIN  loss dict:  {'classification_loss': 0.9280080139636994}
2025-01-15 06:47:28,227 [INFO] Step[800/2713]: training loss : 0.9277061676979065 TRAIN  loss dict:  {'classification_loss': 0.9277061676979065}
2025-01-15 06:47:41,996 [INFO] Step[850/2713]: training loss : 0.9269327557086945 TRAIN  loss dict:  {'classification_loss': 0.9269327557086945}
2025-01-15 06:47:55,682 [INFO] Step[900/2713]: training loss : 0.9289361763000489 TRAIN  loss dict:  {'classification_loss': 0.9289361763000489}
2025-01-15 06:48:09,489 [INFO] Step[950/2713]: training loss : 0.92763946890831 TRAIN  loss dict:  {'classification_loss': 0.92763946890831}
2025-01-15 06:48:23,351 [INFO] Step[1000/2713]: training loss : 0.9276896381378174 TRAIN  loss dict:  {'classification_loss': 0.9276896381378174}
2025-01-15 06:48:37,088 [INFO] Step[1050/2713]: training loss : 0.9286417818069458 TRAIN  loss dict:  {'classification_loss': 0.9286417818069458}
2025-01-15 06:48:50,720 [INFO] Step[1100/2713]: training loss : 0.9282304179668427 TRAIN  loss dict:  {'classification_loss': 0.9282304179668427}
2025-01-15 06:49:04,769 [INFO] Step[1150/2713]: training loss : 0.9284569227695465 TRAIN  loss dict:  {'classification_loss': 0.9284569227695465}
2025-01-15 06:49:18,025 [INFO] Step[1200/2713]: training loss : 0.9277380251884461 TRAIN  loss dict:  {'classification_loss': 0.9277380251884461}
2025-01-15 06:49:31,560 [INFO] Step[1250/2713]: training loss : 0.9268997657299042 TRAIN  loss dict:  {'classification_loss': 0.9268997657299042}
2025-01-15 06:49:45,332 [INFO] Step[1300/2713]: training loss : 0.9276531314849854 TRAIN  loss dict:  {'classification_loss': 0.9276531314849854}
2025-01-15 06:50:01,177 [INFO] Step[1350/2713]: training loss : 0.9272992026805877 TRAIN  loss dict:  {'classification_loss': 0.9272992026805877}
2025-01-15 06:50:15,205 [INFO] Step[1400/2713]: training loss : 0.9374958443641662 TRAIN  loss dict:  {'classification_loss': 0.9374958443641662}
2025-01-15 06:50:29,181 [INFO] Step[1450/2713]: training loss : 0.9273290979862213 TRAIN  loss dict:  {'classification_loss': 0.9273290979862213}
2025-01-15 06:50:42,878 [INFO] Step[1500/2713]: training loss : 0.9284240198135376 TRAIN  loss dict:  {'classification_loss': 0.9284240198135376}
2025-01-15 06:50:56,326 [INFO] Step[1550/2713]: training loss : 0.9273683083057404 TRAIN  loss dict:  {'classification_loss': 0.9273683083057404}
2025-01-15 06:51:09,806 [INFO] Step[1600/2713]: training loss : 0.9274436950683593 TRAIN  loss dict:  {'classification_loss': 0.9274436950683593}
2025-01-15 06:51:23,253 [INFO] Step[1650/2713]: training loss : 0.9280832517147064 TRAIN  loss dict:  {'classification_loss': 0.9280832517147064}
2025-01-15 06:51:36,950 [INFO] Step[1700/2713]: training loss : 0.9271945059299469 TRAIN  loss dict:  {'classification_loss': 0.9271945059299469}
2025-01-15 06:51:50,835 [INFO] Step[1750/2713]: training loss : 0.927527779340744 TRAIN  loss dict:  {'classification_loss': 0.927527779340744}
2025-01-15 06:52:04,658 [INFO] Step[1800/2713]: training loss : 0.9276005530357361 TRAIN  loss dict:  {'classification_loss': 0.9276005530357361}
2025-01-15 06:52:18,399 [INFO] Step[1850/2713]: training loss : 0.9282489585876464 TRAIN  loss dict:  {'classification_loss': 0.9282489585876464}
2025-01-15 06:52:32,104 [INFO] Step[1900/2713]: training loss : 0.9285458242893219 TRAIN  loss dict:  {'classification_loss': 0.9285458242893219}
2025-01-15 06:52:46,107 [INFO] Step[1950/2713]: training loss : 0.9271459567546845 TRAIN  loss dict:  {'classification_loss': 0.9271459567546845}
2025-01-15 06:52:59,767 [INFO] Step[2000/2713]: training loss : 0.9274871599674225 TRAIN  loss dict:  {'classification_loss': 0.9274871599674225}
2025-01-15 06:53:13,433 [INFO] Step[2050/2713]: training loss : 0.9285288333892823 TRAIN  loss dict:  {'classification_loss': 0.9285288333892823}
2025-01-15 06:53:27,682 [INFO] Step[2100/2713]: training loss : 0.9277220618724823 TRAIN  loss dict:  {'classification_loss': 0.9277220618724823}
2025-01-15 06:53:41,673 [INFO] Step[2150/2713]: training loss : 0.9280054807662964 TRAIN  loss dict:  {'classification_loss': 0.9280054807662964}
2025-01-15 06:53:58,142 [INFO] Step[2200/2713]: training loss : 0.9270734870433808 TRAIN  loss dict:  {'classification_loss': 0.9270734870433808}
2025-01-15 06:54:11,895 [INFO] Step[2250/2713]: training loss : 0.927821284532547 TRAIN  loss dict:  {'classification_loss': 0.927821284532547}
2025-01-15 06:54:25,527 [INFO] Step[2300/2713]: training loss : 0.9293924129009247 TRAIN  loss dict:  {'classification_loss': 0.9293924129009247}
2025-01-15 06:54:39,252 [INFO] Step[2350/2713]: training loss : 0.9280668747425079 TRAIN  loss dict:  {'classification_loss': 0.9280668747425079}
2025-01-15 06:54:52,730 [INFO] Step[2400/2713]: training loss : 0.930158361196518 TRAIN  loss dict:  {'classification_loss': 0.930158361196518}
2025-01-15 06:55:06,185 [INFO] Step[2450/2713]: training loss : 0.9285888516902924 TRAIN  loss dict:  {'classification_loss': 0.9285888516902924}
2025-01-15 06:55:19,432 [INFO] Step[2500/2713]: training loss : 0.9279615497589111 TRAIN  loss dict:  {'classification_loss': 0.9279615497589111}
2025-01-15 06:55:33,139 [INFO] Step[2550/2713]: training loss : 0.9269664096832275 TRAIN  loss dict:  {'classification_loss': 0.9269664096832275}
2025-01-15 06:55:47,324 [INFO] Step[2600/2713]: training loss : 0.9271301901340485 TRAIN  loss dict:  {'classification_loss': 0.9271301901340485}
2025-01-15 06:56:01,055 [INFO] Step[2650/2713]: training loss : 0.9268869972229004 TRAIN  loss dict:  {'classification_loss': 0.9268869972229004}
2025-01-15 06:56:15,267 [INFO] Step[2700/2713]: training loss : 0.9384110295772552 TRAIN  loss dict:  {'classification_loss': 0.9384110295772552}
2025-01-15 06:57:31,352 [INFO] Label accuracies statistics:
2025-01-15 06:57:31,352 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.75, 72: 0.75, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 1.0, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 06:57:31,354 [INFO] [85] TRAIN  loss: 0.9283415885855691 acc: 0.9997542695662858
2025-01-15 06:57:31,354 [INFO] [85] TRAIN  loss dict: {'classification_loss': 0.9283415885855691}
2025-01-15 06:57:31,354 [INFO] [85] VALIDATION loss: 1.7692107210930128 VALIDATION acc: 0.8169278996865204
2025-01-15 06:57:31,354 [INFO] [85] VALIDATION loss dict: {'classification_loss': 1.7692107210930128}
2025-01-15 06:57:31,355 [INFO] 
2025-01-15 06:57:50,166 [INFO] Step[50/2713]: training loss : 0.9274315619468689 TRAIN  loss dict:  {'classification_loss': 0.9274315619468689}
2025-01-15 06:58:04,284 [INFO] Step[100/2713]: training loss : 0.9287186992168427 TRAIN  loss dict:  {'classification_loss': 0.9287186992168427}
2025-01-15 06:58:18,005 [INFO] Step[150/2713]: training loss : 0.9273705291748047 TRAIN  loss dict:  {'classification_loss': 0.9273705291748047}
2025-01-15 06:58:31,688 [INFO] Step[200/2713]: training loss : 0.9302264368534088 TRAIN  loss dict:  {'classification_loss': 0.9302264368534088}
2025-01-15 06:58:45,175 [INFO] Step[250/2713]: training loss : 0.9272554874420166 TRAIN  loss dict:  {'classification_loss': 0.9272554874420166}
2025-01-15 06:58:58,877 [INFO] Step[300/2713]: training loss : 0.9272107195854187 TRAIN  loss dict:  {'classification_loss': 0.9272107195854187}
2025-01-15 06:59:12,408 [INFO] Step[350/2713]: training loss : 0.9286774933338166 TRAIN  loss dict:  {'classification_loss': 0.9286774933338166}
2025-01-15 06:59:25,572 [INFO] Step[400/2713]: training loss : 0.9273010909557342 TRAIN  loss dict:  {'classification_loss': 0.9273010909557342}
2025-01-15 06:59:39,452 [INFO] Step[450/2713]: training loss : 0.9272740066051484 TRAIN  loss dict:  {'classification_loss': 0.9272740066051484}
2025-01-15 06:59:55,500 [INFO] Step[500/2713]: training loss : 0.9275983166694641 TRAIN  loss dict:  {'classification_loss': 0.9275983166694641}
2025-01-15 07:00:09,058 [INFO] Step[550/2713]: training loss : 0.9275990390777588 TRAIN  loss dict:  {'classification_loss': 0.9275990390777588}
2025-01-15 07:00:22,884 [INFO] Step[600/2713]: training loss : 0.9274555778503418 TRAIN  loss dict:  {'classification_loss': 0.9274555778503418}
2025-01-15 07:00:36,985 [INFO] Step[650/2713]: training loss : 0.9280483019351959 TRAIN  loss dict:  {'classification_loss': 0.9280483019351959}
2025-01-15 07:00:50,840 [INFO] Step[700/2713]: training loss : 0.927579562664032 TRAIN  loss dict:  {'classification_loss': 0.927579562664032}
2025-01-15 07:01:04,487 [INFO] Step[750/2713]: training loss : 0.9274159777164459 TRAIN  loss dict:  {'classification_loss': 0.9274159777164459}
2025-01-15 07:01:18,443 [INFO] Step[800/2713]: training loss : 0.9281778764724732 TRAIN  loss dict:  {'classification_loss': 0.9281778764724732}
2025-01-15 07:01:32,312 [INFO] Step[850/2713]: training loss : 0.9281892621517182 TRAIN  loss dict:  {'classification_loss': 0.9281892621517182}
2025-01-15 07:01:46,549 [INFO] Step[900/2713]: training loss : 0.9280476593971252 TRAIN  loss dict:  {'classification_loss': 0.9280476593971252}
2025-01-15 07:02:00,500 [INFO] Step[950/2713]: training loss : 0.9278113389015198 TRAIN  loss dict:  {'classification_loss': 0.9278113389015198}
2025-01-15 07:02:14,094 [INFO] Step[1000/2713]: training loss : 0.9276547431945801 TRAIN  loss dict:  {'classification_loss': 0.9276547431945801}
2025-01-15 07:02:27,579 [INFO] Step[1050/2713]: training loss : 0.9361434435844421 TRAIN  loss dict:  {'classification_loss': 0.9361434435844421}
2025-01-15 07:02:40,845 [INFO] Step[1100/2713]: training loss : 0.9267750024795532 TRAIN  loss dict:  {'classification_loss': 0.9267750024795532}
2025-01-15 07:02:54,481 [INFO] Step[1150/2713]: training loss : 0.9271634960174561 TRAIN  loss dict:  {'classification_loss': 0.9271634960174561}
2025-01-15 07:03:07,977 [INFO] Step[1200/2713]: training loss : 0.9280580139160156 TRAIN  loss dict:  {'classification_loss': 0.9280580139160156}
2025-01-15 07:03:21,556 [INFO] Step[1250/2713]: training loss : 0.9274653398990631 TRAIN  loss dict:  {'classification_loss': 0.9274653398990631}
2025-01-15 07:03:34,731 [INFO] Step[1300/2713]: training loss : 0.9275848698616028 TRAIN  loss dict:  {'classification_loss': 0.9275848698616028}
2025-01-15 07:03:48,361 [INFO] Step[1350/2713]: training loss : 0.9274395847320557 TRAIN  loss dict:  {'classification_loss': 0.9274395847320557}
2025-01-15 07:04:02,087 [INFO] Step[1400/2713]: training loss : 0.9278081202507019 TRAIN  loss dict:  {'classification_loss': 0.9278081202507019}
2025-01-15 07:04:16,047 [INFO] Step[1450/2713]: training loss : 0.932329216003418 TRAIN  loss dict:  {'classification_loss': 0.932329216003418}
2025-01-15 07:04:29,875 [INFO] Step[1500/2713]: training loss : 0.927292423248291 TRAIN  loss dict:  {'classification_loss': 0.927292423248291}
2025-01-15 07:04:43,515 [INFO] Step[1550/2713]: training loss : 0.9287980210781097 TRAIN  loss dict:  {'classification_loss': 0.9287980210781097}
2025-01-15 07:04:57,816 [INFO] Step[1600/2713]: training loss : 0.9311760544776917 TRAIN  loss dict:  {'classification_loss': 0.9311760544776917}
2025-01-15 07:05:11,347 [INFO] Step[1650/2713]: training loss : 0.9279534304141999 TRAIN  loss dict:  {'classification_loss': 0.9279534304141999}
2025-01-15 07:05:24,902 [INFO] Step[1700/2713]: training loss : 0.928546189069748 TRAIN  loss dict:  {'classification_loss': 0.928546189069748}
2025-01-15 07:05:38,177 [INFO] Step[1750/2713]: training loss : 0.9278747415542603 TRAIN  loss dict:  {'classification_loss': 0.9278747415542603}
2025-01-15 07:05:51,380 [INFO] Step[1800/2713]: training loss : 0.9290780913829804 TRAIN  loss dict:  {'classification_loss': 0.9290780913829804}
2025-01-15 07:06:06,809 [INFO] Step[1850/2713]: training loss : 0.9277126669883728 TRAIN  loss dict:  {'classification_loss': 0.9277126669883728}
2025-01-15 07:06:21,824 [INFO] Step[1900/2713]: training loss : 0.927765861749649 TRAIN  loss dict:  {'classification_loss': 0.927765861749649}
2025-01-15 07:06:35,099 [INFO] Step[1950/2713]: training loss : 0.9302136635780335 TRAIN  loss dict:  {'classification_loss': 0.9302136635780335}
2025-01-15 07:06:49,003 [INFO] Step[2000/2713]: training loss : 0.9567273783683777 TRAIN  loss dict:  {'classification_loss': 0.9567273783683777}
2025-01-15 07:07:03,569 [INFO] Step[2050/2713]: training loss : 0.9274144756793976 TRAIN  loss dict:  {'classification_loss': 0.9274144756793976}
2025-01-15 07:07:19,532 [INFO] Step[2100/2713]: training loss : 0.9278423082828522 TRAIN  loss dict:  {'classification_loss': 0.9278423082828522}
2025-01-15 07:07:33,440 [INFO] Step[2150/2713]: training loss : 0.9275768041610718 TRAIN  loss dict:  {'classification_loss': 0.9275768041610718}
2025-01-15 07:07:47,655 [INFO] Step[2200/2713]: training loss : 0.9278986036777497 TRAIN  loss dict:  {'classification_loss': 0.9278986036777497}
2025-01-15 07:08:00,797 [INFO] Step[2250/2713]: training loss : 0.9275505208969116 TRAIN  loss dict:  {'classification_loss': 0.9275505208969116}
2025-01-15 07:08:14,433 [INFO] Step[2300/2713]: training loss : 0.926992769241333 TRAIN  loss dict:  {'classification_loss': 0.926992769241333}
2025-01-15 07:08:28,153 [INFO] Step[2350/2713]: training loss : 0.9281552028656006 TRAIN  loss dict:  {'classification_loss': 0.9281552028656006}
2025-01-15 07:08:42,107 [INFO] Step[2400/2713]: training loss : 0.9472631752490998 TRAIN  loss dict:  {'classification_loss': 0.9472631752490998}
2025-01-15 07:08:55,581 [INFO] Step[2450/2713]: training loss : 0.9274077582359314 TRAIN  loss dict:  {'classification_loss': 0.9274077582359314}
2025-01-15 07:09:09,091 [INFO] Step[2500/2713]: training loss : 0.9268875896930695 TRAIN  loss dict:  {'classification_loss': 0.9268875896930695}
2025-01-15 07:09:22,687 [INFO] Step[2550/2713]: training loss : 0.9273505246639252 TRAIN  loss dict:  {'classification_loss': 0.9273505246639252}
2025-01-15 07:09:36,500 [INFO] Step[2600/2713]: training loss : 0.9272931015491486 TRAIN  loss dict:  {'classification_loss': 0.9272931015491486}
2025-01-15 07:09:49,726 [INFO] Step[2650/2713]: training loss : 0.9271347332000732 TRAIN  loss dict:  {'classification_loss': 0.9271347332000732}
2025-01-15 07:10:03,121 [INFO] Step[2700/2713]: training loss : 0.9274136316776276 TRAIN  loss dict:  {'classification_loss': 0.9274136316776276}
2025-01-15 07:11:20,464 [INFO] Label accuracies statistics:
2025-01-15 07:11:20,464 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 07:11:20,466 [INFO] [86] TRAIN  loss: 0.9289823453715487 acc: 0.9996314043494287
2025-01-15 07:11:20,466 [INFO] [86] TRAIN  loss dict: {'classification_loss': 0.9289823453715487}
2025-01-15 07:11:20,466 [INFO] [86] VALIDATION loss: 1.7761553762326563 VALIDATION acc: 0.812539184952978
2025-01-15 07:11:20,466 [INFO] [86] VALIDATION loss dict: {'classification_loss': 1.7761553762326563}
2025-01-15 07:11:20,466 [INFO] 
2025-01-15 07:11:44,308 [INFO] Step[50/2713]: training loss : 0.9278629875183105 TRAIN  loss dict:  {'classification_loss': 0.9278629875183105}
2025-01-15 07:11:57,775 [INFO] Step[100/2713]: training loss : 0.927233874797821 TRAIN  loss dict:  {'classification_loss': 0.927233874797821}
2025-01-15 07:12:11,349 [INFO] Step[150/2713]: training loss : 0.9273603141307831 TRAIN  loss dict:  {'classification_loss': 0.9273603141307831}
2025-01-15 07:12:24,839 [INFO] Step[200/2713]: training loss : 0.9270015478134155 TRAIN  loss dict:  {'classification_loss': 0.9270015478134155}
2025-01-15 07:12:38,060 [INFO] Step[250/2713]: training loss : 0.9272318947315216 TRAIN  loss dict:  {'classification_loss': 0.9272318947315216}
2025-01-15 07:12:52,071 [INFO] Step[300/2713]: training loss : 0.9272972345352173 TRAIN  loss dict:  {'classification_loss': 0.9272972345352173}
2025-01-15 07:13:06,083 [INFO] Step[350/2713]: training loss : 0.9274489176273346 TRAIN  loss dict:  {'classification_loss': 0.9274489176273346}
2025-01-15 07:13:20,192 [INFO] Step[400/2713]: training loss : 0.9281801950931549 TRAIN  loss dict:  {'classification_loss': 0.9281801950931549}
2025-01-15 07:13:33,768 [INFO] Step[450/2713]: training loss : 0.9277529466152191 TRAIN  loss dict:  {'classification_loss': 0.9277529466152191}
2025-01-15 07:13:47,281 [INFO] Step[500/2713]: training loss : 0.9292873752117157 TRAIN  loss dict:  {'classification_loss': 0.9292873752117157}
2025-01-15 07:14:01,280 [INFO] Step[550/2713]: training loss : 0.9271613037586213 TRAIN  loss dict:  {'classification_loss': 0.9271613037586213}
2025-01-15 07:14:14,922 [INFO] Step[600/2713]: training loss : 0.9298878312110901 TRAIN  loss dict:  {'classification_loss': 0.9298878312110901}
2025-01-15 07:14:28,815 [INFO] Step[650/2713]: training loss : 0.9275291550159455 TRAIN  loss dict:  {'classification_loss': 0.9275291550159455}
2025-01-15 07:14:42,009 [INFO] Step[700/2713]: training loss : 0.9269503307342529 TRAIN  loss dict:  {'classification_loss': 0.9269503307342529}
2025-01-15 07:14:56,146 [INFO] Step[750/2713]: training loss : 0.9282628810405731 TRAIN  loss dict:  {'classification_loss': 0.9282628810405731}
2025-01-15 07:15:10,030 [INFO] Step[800/2713]: training loss : 0.9274681711196899 TRAIN  loss dict:  {'classification_loss': 0.9274681711196899}
2025-01-15 07:15:23,515 [INFO] Step[850/2713]: training loss : 0.92741082072258 TRAIN  loss dict:  {'classification_loss': 0.92741082072258}
2025-01-15 07:15:37,171 [INFO] Step[900/2713]: training loss : 0.9273985004425049 TRAIN  loss dict:  {'classification_loss': 0.9273985004425049}
2025-01-15 07:15:51,124 [INFO] Step[950/2713]: training loss : 0.9273821759223938 TRAIN  loss dict:  {'classification_loss': 0.9273821759223938}
2025-01-15 07:16:04,728 [INFO] Step[1000/2713]: training loss : 0.927068065404892 TRAIN  loss dict:  {'classification_loss': 0.927068065404892}
2025-01-15 07:16:18,574 [INFO] Step[1050/2713]: training loss : 0.93507049202919 TRAIN  loss dict:  {'classification_loss': 0.93507049202919}
2025-01-15 07:16:32,651 [INFO] Step[1100/2713]: training loss : 0.9288206815719604 TRAIN  loss dict:  {'classification_loss': 0.9288206815719604}
2025-01-15 07:16:46,111 [INFO] Step[1150/2713]: training loss : 0.9283616471290589 TRAIN  loss dict:  {'classification_loss': 0.9283616471290589}
2025-01-15 07:17:00,301 [INFO] Step[1200/2713]: training loss : 0.9272234272956849 TRAIN  loss dict:  {'classification_loss': 0.9272234272956849}
2025-01-15 07:17:13,536 [INFO] Step[1250/2713]: training loss : 0.9277128446102142 TRAIN  loss dict:  {'classification_loss': 0.9277128446102142}
2025-01-15 07:17:27,132 [INFO] Step[1300/2713]: training loss : 0.9282475888729096 TRAIN  loss dict:  {'classification_loss': 0.9282475888729096}
2025-01-15 07:17:40,766 [INFO] Step[1350/2713]: training loss : 0.9275630378723144 TRAIN  loss dict:  {'classification_loss': 0.9275630378723144}
2025-01-15 07:17:54,018 [INFO] Step[1400/2713]: training loss : 0.9282939577102661 TRAIN  loss dict:  {'classification_loss': 0.9282939577102661}
2025-01-15 07:18:07,809 [INFO] Step[1450/2713]: training loss : 0.9277028894424438 TRAIN  loss dict:  {'classification_loss': 0.9277028894424438}
2025-01-15 07:18:21,572 [INFO] Step[1500/2713]: training loss : 0.9280532395839691 TRAIN  loss dict:  {'classification_loss': 0.9280532395839691}
2025-01-15 07:18:35,533 [INFO] Step[1550/2713]: training loss : 0.9266504406929016 TRAIN  loss dict:  {'classification_loss': 0.9266504406929016}
2025-01-15 07:18:49,468 [INFO] Step[1600/2713]: training loss : 0.9277015709877015 TRAIN  loss dict:  {'classification_loss': 0.9277015709877015}
2025-01-15 07:19:03,181 [INFO] Step[1650/2713]: training loss : 0.9268119525909424 TRAIN  loss dict:  {'classification_loss': 0.9268119525909424}
2025-01-15 07:19:16,938 [INFO] Step[1700/2713]: training loss : 0.9272212135791779 TRAIN  loss dict:  {'classification_loss': 0.9272212135791779}
2025-01-15 07:19:30,703 [INFO] Step[1750/2713]: training loss : 0.9277785658836365 TRAIN  loss dict:  {'classification_loss': 0.9277785658836365}
2025-01-15 07:19:44,276 [INFO] Step[1800/2713]: training loss : 0.9280637240409851 TRAIN  loss dict:  {'classification_loss': 0.9280637240409851}
2025-01-15 07:19:57,883 [INFO] Step[1850/2713]: training loss : 0.928366471529007 TRAIN  loss dict:  {'classification_loss': 0.928366471529007}
2025-01-15 07:20:11,678 [INFO] Step[1900/2713]: training loss : 0.9279881572723389 TRAIN  loss dict:  {'classification_loss': 0.9279881572723389}
2025-01-15 07:20:25,001 [INFO] Step[1950/2713]: training loss : 0.9273399531841278 TRAIN  loss dict:  {'classification_loss': 0.9273399531841278}
2025-01-15 07:20:38,277 [INFO] Step[2000/2713]: training loss : 0.9276636302471161 TRAIN  loss dict:  {'classification_loss': 0.9276636302471161}
2025-01-15 07:20:51,759 [INFO] Step[2050/2713]: training loss : 0.9271167016029358 TRAIN  loss dict:  {'classification_loss': 0.9271167016029358}
2025-01-15 07:21:05,303 [INFO] Step[2100/2713]: training loss : 0.9274232029914856 TRAIN  loss dict:  {'classification_loss': 0.9274232029914856}
2025-01-15 07:21:19,142 [INFO] Step[2150/2713]: training loss : 0.9277242207527161 TRAIN  loss dict:  {'classification_loss': 0.9277242207527161}
2025-01-15 07:21:33,189 [INFO] Step[2200/2713]: training loss : 0.9271584892272949 TRAIN  loss dict:  {'classification_loss': 0.9271584892272949}
2025-01-15 07:21:46,805 [INFO] Step[2250/2713]: training loss : 0.9275035893917084 TRAIN  loss dict:  {'classification_loss': 0.9275035893917084}
2025-01-15 07:22:01,036 [INFO] Step[2300/2713]: training loss : 0.9277306687831879 TRAIN  loss dict:  {'classification_loss': 0.9277306687831879}
2025-01-15 07:22:15,271 [INFO] Step[2350/2713]: training loss : 0.9280165004730224 TRAIN  loss dict:  {'classification_loss': 0.9280165004730224}
2025-01-15 07:22:29,520 [INFO] Step[2400/2713]: training loss : 0.9272996306419372 TRAIN  loss dict:  {'classification_loss': 0.9272996306419372}
2025-01-15 07:22:43,260 [INFO] Step[2450/2713]: training loss : 0.9275832104682923 TRAIN  loss dict:  {'classification_loss': 0.9275832104682923}
2025-01-15 07:22:57,057 [INFO] Step[2500/2713]: training loss : 0.9272597467899323 TRAIN  loss dict:  {'classification_loss': 0.9272597467899323}
2025-01-15 07:23:11,010 [INFO] Step[2550/2713]: training loss : 0.9272513616085053 TRAIN  loss dict:  {'classification_loss': 0.9272513616085053}
2025-01-15 07:23:25,251 [INFO] Step[2600/2713]: training loss : 0.9274865734577179 TRAIN  loss dict:  {'classification_loss': 0.9274865734577179}
2025-01-15 07:23:38,834 [INFO] Step[2650/2713]: training loss : 0.9275003159046173 TRAIN  loss dict:  {'classification_loss': 0.9275003159046173}
2025-01-15 07:23:52,080 [INFO] Step[2700/2713]: training loss : 0.9274461138248443 TRAIN  loss dict:  {'classification_loss': 0.9274461138248443}
2025-01-15 07:25:08,569 [INFO] Label accuracies statistics:
2025-01-15 07:25:08,570 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 1.0, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.25, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 07:25:09,735 [INFO] [87] TRAIN  loss: 0.9277844226232895 acc: 0.9998771347831429
2025-01-15 07:25:09,735 [INFO] [87] TRAIN  loss dict: {'classification_loss': 0.9277844226232895}
2025-01-15 07:25:09,735 [INFO] [87] VALIDATION loss: 1.7692588640558988 VALIDATION acc: 0.8213166144200627
2025-01-15 07:25:09,735 [INFO] [87] VALIDATION loss dict: {'classification_loss': 1.7692588640558988}
2025-01-15 07:25:09,735 [INFO] 
2025-01-15 07:25:28,515 [INFO] Step[50/2713]: training loss : 0.9274457585811615 TRAIN  loss dict:  {'classification_loss': 0.9274457585811615}
2025-01-15 07:25:42,243 [INFO] Step[100/2713]: training loss : 0.9272877585887909 TRAIN  loss dict:  {'classification_loss': 0.9272877585887909}
2025-01-15 07:25:55,669 [INFO] Step[150/2713]: training loss : 0.9276750648021698 TRAIN  loss dict:  {'classification_loss': 0.9276750648021698}
2025-01-15 07:26:09,319 [INFO] Step[200/2713]: training loss : 0.9283150792121887 TRAIN  loss dict:  {'classification_loss': 0.9283150792121887}
2025-01-15 07:26:22,997 [INFO] Step[250/2713]: training loss : 0.9310654377937317 TRAIN  loss dict:  {'classification_loss': 0.9310654377937317}
2025-01-15 07:26:36,580 [INFO] Step[300/2713]: training loss : 0.9271123158931732 TRAIN  loss dict:  {'classification_loss': 0.9271123158931732}
2025-01-15 07:26:50,535 [INFO] Step[350/2713]: training loss : 0.9276416778564454 TRAIN  loss dict:  {'classification_loss': 0.9276416778564454}
2025-01-15 07:27:04,047 [INFO] Step[400/2713]: training loss : 0.92738774061203 TRAIN  loss dict:  {'classification_loss': 0.92738774061203}
2025-01-15 07:27:17,782 [INFO] Step[450/2713]: training loss : 0.9327228856086731 TRAIN  loss dict:  {'classification_loss': 0.9327228856086731}
2025-01-15 07:27:31,519 [INFO] Step[500/2713]: training loss : 0.9272212207317352 TRAIN  loss dict:  {'classification_loss': 0.9272212207317352}
2025-01-15 07:27:45,144 [INFO] Step[550/2713]: training loss : 0.927076427936554 TRAIN  loss dict:  {'classification_loss': 0.927076427936554}
2025-01-15 07:27:59,345 [INFO] Step[600/2713]: training loss : 0.9278451037406922 TRAIN  loss dict:  {'classification_loss': 0.9278451037406922}
2025-01-15 07:28:13,176 [INFO] Step[650/2713]: training loss : 0.9272128212451934 TRAIN  loss dict:  {'classification_loss': 0.9272128212451934}
2025-01-15 07:28:26,633 [INFO] Step[700/2713]: training loss : 0.9270643138885498 TRAIN  loss dict:  {'classification_loss': 0.9270643138885498}
2025-01-15 07:28:40,564 [INFO] Step[750/2713]: training loss : 0.9273235583305359 TRAIN  loss dict:  {'classification_loss': 0.9273235583305359}
2025-01-15 07:28:54,079 [INFO] Step[800/2713]: training loss : 0.9272622668743133 TRAIN  loss dict:  {'classification_loss': 0.9272622668743133}
2025-01-15 07:29:07,411 [INFO] Step[850/2713]: training loss : 0.9473751890659332 TRAIN  loss dict:  {'classification_loss': 0.9473751890659332}
2025-01-15 07:29:21,566 [INFO] Step[900/2713]: training loss : 0.9278910160064697 TRAIN  loss dict:  {'classification_loss': 0.9278910160064697}
2025-01-15 07:29:35,211 [INFO] Step[950/2713]: training loss : 0.9269498801231384 TRAIN  loss dict:  {'classification_loss': 0.9269498801231384}
2025-01-15 07:29:48,614 [INFO] Step[1000/2713]: training loss : 0.9294925034046173 TRAIN  loss dict:  {'classification_loss': 0.9294925034046173}
2025-01-15 07:30:02,454 [INFO] Step[1050/2713]: training loss : 0.9273893487453461 TRAIN  loss dict:  {'classification_loss': 0.9273893487453461}
2025-01-15 07:30:16,208 [INFO] Step[1100/2713]: training loss : 0.9275967526435852 TRAIN  loss dict:  {'classification_loss': 0.9275967526435852}
2025-01-15 07:30:30,062 [INFO] Step[1150/2713]: training loss : 0.9271807217597962 TRAIN  loss dict:  {'classification_loss': 0.9271807217597962}
2025-01-15 07:30:43,650 [INFO] Step[1200/2713]: training loss : 0.9274856173992156 TRAIN  loss dict:  {'classification_loss': 0.9274856173992156}
2025-01-15 07:30:57,353 [INFO] Step[1250/2713]: training loss : 0.9275142323970794 TRAIN  loss dict:  {'classification_loss': 0.9275142323970794}
2025-01-15 07:31:11,047 [INFO] Step[1300/2713]: training loss : 0.9274651277065277 TRAIN  loss dict:  {'classification_loss': 0.9274651277065277}
2025-01-15 07:31:24,728 [INFO] Step[1350/2713]: training loss : 0.929174667596817 TRAIN  loss dict:  {'classification_loss': 0.929174667596817}
2025-01-15 07:31:38,217 [INFO] Step[1400/2713]: training loss : 0.9264970815181732 TRAIN  loss dict:  {'classification_loss': 0.9264970815181732}
2025-01-15 07:31:51,813 [INFO] Step[1450/2713]: training loss : 0.9281314277648925 TRAIN  loss dict:  {'classification_loss': 0.9281314277648925}
2025-01-15 07:32:05,705 [INFO] Step[1500/2713]: training loss : 0.9285213458538055 TRAIN  loss dict:  {'classification_loss': 0.9285213458538055}
2025-01-15 07:32:19,234 [INFO] Step[1550/2713]: training loss : 0.92706169962883 TRAIN  loss dict:  {'classification_loss': 0.92706169962883}
2025-01-15 07:32:32,644 [INFO] Step[1600/2713]: training loss : 0.92738494515419 TRAIN  loss dict:  {'classification_loss': 0.92738494515419}
2025-01-15 07:32:47,076 [INFO] Step[1650/2713]: training loss : 0.9273681402206421 TRAIN  loss dict:  {'classification_loss': 0.9273681402206421}
2025-01-15 07:33:02,692 [INFO] Step[1700/2713]: training loss : 0.9283316659927369 TRAIN  loss dict:  {'classification_loss': 0.9283316659927369}
2025-01-15 07:33:15,837 [INFO] Step[1750/2713]: training loss : 0.9273830366134643 TRAIN  loss dict:  {'classification_loss': 0.9273830366134643}
2025-01-15 07:33:29,313 [INFO] Step[1800/2713]: training loss : 0.926851909160614 TRAIN  loss dict:  {'classification_loss': 0.926851909160614}
2025-01-15 07:33:43,132 [INFO] Step[1850/2713]: training loss : 0.9274422085285187 TRAIN  loss dict:  {'classification_loss': 0.9274422085285187}
2025-01-15 07:33:57,296 [INFO] Step[1900/2713]: training loss : 0.9275922930240631 TRAIN  loss dict:  {'classification_loss': 0.9275922930240631}
2025-01-15 07:34:11,443 [INFO] Step[1950/2713]: training loss : 0.9280748021602631 TRAIN  loss dict:  {'classification_loss': 0.9280748021602631}
2025-01-15 07:34:24,633 [INFO] Step[2000/2713]: training loss : 0.9273234152793884 TRAIN  loss dict:  {'classification_loss': 0.9273234152793884}
2025-01-15 07:34:38,248 [INFO] Step[2050/2713]: training loss : 0.9282809519767761 TRAIN  loss dict:  {'classification_loss': 0.9282809519767761}
2025-01-15 07:34:51,659 [INFO] Step[2100/2713]: training loss : 0.9289573192596435 TRAIN  loss dict:  {'classification_loss': 0.9289573192596435}
2025-01-15 07:35:05,650 [INFO] Step[2150/2713]: training loss : 0.927119882106781 TRAIN  loss dict:  {'classification_loss': 0.927119882106781}
2025-01-15 07:35:19,074 [INFO] Step[2200/2713]: training loss : 0.9275471591949462 TRAIN  loss dict:  {'classification_loss': 0.9275471591949462}
2025-01-15 07:35:32,755 [INFO] Step[2250/2713]: training loss : 0.9274179863929749 TRAIN  loss dict:  {'classification_loss': 0.9274179863929749}
2025-01-15 07:35:46,407 [INFO] Step[2300/2713]: training loss : 0.9273465478420257 TRAIN  loss dict:  {'classification_loss': 0.9273465478420257}
2025-01-15 07:36:00,246 [INFO] Step[2350/2713]: training loss : 0.9278725218772889 TRAIN  loss dict:  {'classification_loss': 0.9278725218772889}
2025-01-15 07:36:14,011 [INFO] Step[2400/2713]: training loss : 0.9289298987388611 TRAIN  loss dict:  {'classification_loss': 0.9289298987388611}
2025-01-15 07:36:27,546 [INFO] Step[2450/2713]: training loss : 0.9275988614559174 TRAIN  loss dict:  {'classification_loss': 0.9275988614559174}
2025-01-15 07:36:40,945 [INFO] Step[2500/2713]: training loss : 0.9273030114173889 TRAIN  loss dict:  {'classification_loss': 0.9273030114173889}
2025-01-15 07:36:54,943 [INFO] Step[2550/2713]: training loss : 0.9325127339363098 TRAIN  loss dict:  {'classification_loss': 0.9325127339363098}
2025-01-15 07:37:08,478 [INFO] Step[2600/2713]: training loss : 0.9268371760845184 TRAIN  loss dict:  {'classification_loss': 0.9268371760845184}
2025-01-15 07:37:21,869 [INFO] Step[2650/2713]: training loss : 0.9297587811946869 TRAIN  loss dict:  {'classification_loss': 0.9297587811946869}
2025-01-15 07:37:35,046 [INFO] Step[2700/2713]: training loss : 0.9274201536178589 TRAIN  loss dict:  {'classification_loss': 0.9274201536178589}
2025-01-15 07:38:50,609 [INFO] Label accuracies statistics:
2025-01-15 07:38:50,609 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.25, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 07:38:50,611 [INFO] [88] TRAIN  loss: 0.9282608514907744 acc: 0.9998771347831429
2025-01-15 07:38:50,611 [INFO] [88] TRAIN  loss dict: {'classification_loss': 0.9282608514907744}
2025-01-15 07:38:50,611 [INFO] [88] VALIDATION loss: 1.8215679373956264 VALIDATION acc: 0.8087774294670846
2025-01-15 07:38:50,611 [INFO] [88] VALIDATION loss dict: {'classification_loss': 1.8215679373956264}
2025-01-15 07:38:50,611 [INFO] 
2025-01-15 07:39:10,081 [INFO] Step[50/2713]: training loss : 0.927420197725296 TRAIN  loss dict:  {'classification_loss': 0.927420197725296}
2025-01-15 07:39:23,726 [INFO] Step[100/2713]: training loss : 0.9278018832206726 TRAIN  loss dict:  {'classification_loss': 0.9278018832206726}
2025-01-15 07:39:37,906 [INFO] Step[150/2713]: training loss : 0.9278789794445038 TRAIN  loss dict:  {'classification_loss': 0.9278789794445038}
2025-01-15 07:39:51,193 [INFO] Step[200/2713]: training loss : 0.9274780428409577 TRAIN  loss dict:  {'classification_loss': 0.9274780428409577}
2025-01-15 07:40:05,004 [INFO] Step[250/2713]: training loss : 0.9293146550655365 TRAIN  loss dict:  {'classification_loss': 0.9293146550655365}
2025-01-15 07:40:18,592 [INFO] Step[300/2713]: training loss : 0.9347484350204468 TRAIN  loss dict:  {'classification_loss': 0.9347484350204468}
2025-01-15 07:40:32,476 [INFO] Step[350/2713]: training loss : 0.9280832946300507 TRAIN  loss dict:  {'classification_loss': 0.9280832946300507}
2025-01-15 07:40:45,910 [INFO] Step[400/2713]: training loss : 0.9284714353084564 TRAIN  loss dict:  {'classification_loss': 0.9284714353084564}
2025-01-15 07:40:59,359 [INFO] Step[450/2713]: training loss : 0.9274481642246246 TRAIN  loss dict:  {'classification_loss': 0.9274481642246246}
2025-01-15 07:41:12,711 [INFO] Step[500/2713]: training loss : 0.9271540081501007 TRAIN  loss dict:  {'classification_loss': 0.9271540081501007}
2025-01-15 07:41:26,641 [INFO] Step[550/2713]: training loss : 0.9282134366035462 TRAIN  loss dict:  {'classification_loss': 0.9282134366035462}
2025-01-15 07:41:40,128 [INFO] Step[600/2713]: training loss : 0.9269499158859253 TRAIN  loss dict:  {'classification_loss': 0.9269499158859253}
2025-01-15 07:41:53,321 [INFO] Step[650/2713]: training loss : 0.9302836334705353 TRAIN  loss dict:  {'classification_loss': 0.9302836334705353}
2025-01-15 07:42:06,837 [INFO] Step[700/2713]: training loss : 0.9281806695461273 TRAIN  loss dict:  {'classification_loss': 0.9281806695461273}
2025-01-15 07:42:20,545 [INFO] Step[750/2713]: training loss : 0.9271229231357574 TRAIN  loss dict:  {'classification_loss': 0.9271229231357574}
2025-01-15 07:42:34,663 [INFO] Step[800/2713]: training loss : 0.9276232218742371 TRAIN  loss dict:  {'classification_loss': 0.9276232218742371}
2025-01-15 07:42:48,351 [INFO] Step[850/2713]: training loss : 0.9276299941539764 TRAIN  loss dict:  {'classification_loss': 0.9276299941539764}
2025-01-15 07:43:01,568 [INFO] Step[900/2713]: training loss : 0.9271377789974212 TRAIN  loss dict:  {'classification_loss': 0.9271377789974212}
2025-01-15 07:43:15,362 [INFO] Step[950/2713]: training loss : 0.9278124153614045 TRAIN  loss dict:  {'classification_loss': 0.9278124153614045}
2025-01-15 07:43:29,061 [INFO] Step[1000/2713]: training loss : 0.9267068243026734 TRAIN  loss dict:  {'classification_loss': 0.9267068243026734}
2025-01-15 07:43:42,218 [INFO] Step[1050/2713]: training loss : 0.9270466542243958 TRAIN  loss dict:  {'classification_loss': 0.9270466542243958}
2025-01-15 07:43:55,430 [INFO] Step[1100/2713]: training loss : 0.9268831300735474 TRAIN  loss dict:  {'classification_loss': 0.9268831300735474}
2025-01-15 07:44:09,208 [INFO] Step[1150/2713]: training loss : 0.9275629687309265 TRAIN  loss dict:  {'classification_loss': 0.9275629687309265}
2025-01-15 07:44:23,267 [INFO] Step[1200/2713]: training loss : 0.9269250571727753 TRAIN  loss dict:  {'classification_loss': 0.9269250571727753}
2025-01-15 07:44:37,149 [INFO] Step[1250/2713]: training loss : 0.9273557424545288 TRAIN  loss dict:  {'classification_loss': 0.9273557424545288}
2025-01-15 07:44:51,277 [INFO] Step[1300/2713]: training loss : 0.9275897133350373 TRAIN  loss dict:  {'classification_loss': 0.9275897133350373}
2025-01-15 07:45:04,600 [INFO] Step[1350/2713]: training loss : 0.9272919654846191 TRAIN  loss dict:  {'classification_loss': 0.9272919654846191}
2025-01-15 07:45:18,510 [INFO] Step[1400/2713]: training loss : 0.9273354661464691 TRAIN  loss dict:  {'classification_loss': 0.9273354661464691}
2025-01-15 07:45:32,244 [INFO] Step[1450/2713]: training loss : 0.9275794064998627 TRAIN  loss dict:  {'classification_loss': 0.9275794064998627}
2025-01-15 07:45:46,424 [INFO] Step[1500/2713]: training loss : 0.927395625114441 TRAIN  loss dict:  {'classification_loss': 0.927395625114441}
2025-01-15 07:46:00,002 [INFO] Step[1550/2713]: training loss : 0.9282103908061982 TRAIN  loss dict:  {'classification_loss': 0.9282103908061982}
2025-01-15 07:46:13,879 [INFO] Step[1600/2713]: training loss : 0.9273606491088867 TRAIN  loss dict:  {'classification_loss': 0.9273606491088867}
2025-01-15 07:46:27,321 [INFO] Step[1650/2713]: training loss : 0.9267405223846436 TRAIN  loss dict:  {'classification_loss': 0.9267405223846436}
2025-01-15 07:46:41,447 [INFO] Step[1700/2713]: training loss : 0.9278433930873871 TRAIN  loss dict:  {'classification_loss': 0.9278433930873871}
2025-01-15 07:46:55,427 [INFO] Step[1750/2713]: training loss : 0.9281387710571289 TRAIN  loss dict:  {'classification_loss': 0.9281387710571289}
2025-01-15 07:47:09,211 [INFO] Step[1800/2713]: training loss : 0.9279253017902375 TRAIN  loss dict:  {'classification_loss': 0.9279253017902375}
2025-01-15 07:47:22,789 [INFO] Step[1850/2713]: training loss : 0.9269750940799714 TRAIN  loss dict:  {'classification_loss': 0.9269750940799714}
2025-01-15 07:47:36,304 [INFO] Step[1900/2713]: training loss : 0.9285872435569763 TRAIN  loss dict:  {'classification_loss': 0.9285872435569763}
2025-01-15 07:47:50,062 [INFO] Step[1950/2713]: training loss : 0.927298080921173 TRAIN  loss dict:  {'classification_loss': 0.927298080921173}
2025-01-15 07:48:03,913 [INFO] Step[2000/2713]: training loss : 0.9270436024665832 TRAIN  loss dict:  {'classification_loss': 0.9270436024665832}
2025-01-15 07:48:17,106 [INFO] Step[2050/2713]: training loss : 0.9312097978591919 TRAIN  loss dict:  {'classification_loss': 0.9312097978591919}
2025-01-15 07:48:30,262 [INFO] Step[2100/2713]: training loss : 0.9274179065227508 TRAIN  loss dict:  {'classification_loss': 0.9274179065227508}
2025-01-15 07:48:43,870 [INFO] Step[2150/2713]: training loss : 0.9286014509201049 TRAIN  loss dict:  {'classification_loss': 0.9286014509201049}
2025-01-15 07:48:57,026 [INFO] Step[2200/2713]: training loss : 0.9275713324546814 TRAIN  loss dict:  {'classification_loss': 0.9275713324546814}
2025-01-15 07:49:10,498 [INFO] Step[2250/2713]: training loss : 0.9272623407840729 TRAIN  loss dict:  {'classification_loss': 0.9272623407840729}
2025-01-15 07:49:24,396 [INFO] Step[2300/2713]: training loss : 0.9273379361629486 TRAIN  loss dict:  {'classification_loss': 0.9273379361629486}
2025-01-15 07:49:38,125 [INFO] Step[2350/2713]: training loss : 0.9274797976016999 TRAIN  loss dict:  {'classification_loss': 0.9274797976016999}
2025-01-15 07:49:51,291 [INFO] Step[2400/2713]: training loss : 0.9277463161945343 TRAIN  loss dict:  {'classification_loss': 0.9277463161945343}
2025-01-15 07:50:04,826 [INFO] Step[2450/2713]: training loss : 0.9270735681056976 TRAIN  loss dict:  {'classification_loss': 0.9270735681056976}
2025-01-15 07:50:18,442 [INFO] Step[2500/2713]: training loss : 0.9275989437103271 TRAIN  loss dict:  {'classification_loss': 0.9275989437103271}
2025-01-15 07:50:31,598 [INFO] Step[2550/2713]: training loss : 0.9276721954345704 TRAIN  loss dict:  {'classification_loss': 0.9276721954345704}
2025-01-15 07:50:45,700 [INFO] Step[2600/2713]: training loss : 0.9281062054634094 TRAIN  loss dict:  {'classification_loss': 0.9281062054634094}
2025-01-15 07:50:59,689 [INFO] Step[2650/2713]: training loss : 0.9276151990890503 TRAIN  loss dict:  {'classification_loss': 0.9276151990890503}
2025-01-15 07:51:16,323 [INFO] Step[2700/2713]: training loss : 0.9278052806854248 TRAIN  loss dict:  {'classification_loss': 0.9278052806854248}
2025-01-15 07:52:32,424 [INFO] Label accuracies statistics:
2025-01-15 07:52:32,424 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.75, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 1.0, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 07:52:32,426 [INFO] [89] TRAIN  loss: 0.9278311010352632 acc: 0.9998771347831429
2025-01-15 07:52:32,426 [INFO] [89] TRAIN  loss dict: {'classification_loss': 0.9278311010352632}
2025-01-15 07:52:32,426 [INFO] [89] VALIDATION loss: 1.815251643720426 VALIDATION acc: 0.8068965517241379
2025-01-15 07:52:32,426 [INFO] [89] VALIDATION loss dict: {'classification_loss': 1.815251643720426}
2025-01-15 07:52:32,426 [INFO] 
2025-01-15 07:52:50,798 [INFO] Step[50/2713]: training loss : 0.9277386939525605 TRAIN  loss dict:  {'classification_loss': 0.9277386939525605}
2025-01-15 07:53:04,794 [INFO] Step[100/2713]: training loss : 0.9268535375595093 TRAIN  loss dict:  {'classification_loss': 0.9268535375595093}
2025-01-15 07:53:18,063 [INFO] Step[150/2713]: training loss : 0.9276939535140991 TRAIN  loss dict:  {'classification_loss': 0.9276939535140991}
2025-01-15 07:53:31,472 [INFO] Step[200/2713]: training loss : 0.9285247874259949 TRAIN  loss dict:  {'classification_loss': 0.9285247874259949}
2025-01-15 07:53:45,246 [INFO] Step[250/2713]: training loss : 0.9273067188262939 TRAIN  loss dict:  {'classification_loss': 0.9273067188262939}
2025-01-15 07:53:58,966 [INFO] Step[300/2713]: training loss : 0.9270828926563263 TRAIN  loss dict:  {'classification_loss': 0.9270828926563263}
2025-01-15 07:54:12,522 [INFO] Step[350/2713]: training loss : 0.9267546820640564 TRAIN  loss dict:  {'classification_loss': 0.9267546820640564}
2025-01-15 07:54:26,271 [INFO] Step[400/2713]: training loss : 0.9280374491214752 TRAIN  loss dict:  {'classification_loss': 0.9280374491214752}
2025-01-15 07:54:39,861 [INFO] Step[450/2713]: training loss : 0.927816172838211 TRAIN  loss dict:  {'classification_loss': 0.927816172838211}
2025-01-15 07:54:53,516 [INFO] Step[500/2713]: training loss : 0.928639544248581 TRAIN  loss dict:  {'classification_loss': 0.928639544248581}
2025-01-15 07:55:07,267 [INFO] Step[550/2713]: training loss : 0.9268646442890167 TRAIN  loss dict:  {'classification_loss': 0.9268646442890167}
2025-01-15 07:55:21,150 [INFO] Step[600/2713]: training loss : 0.9273368299007416 TRAIN  loss dict:  {'classification_loss': 0.9273368299007416}
2025-01-15 07:55:35,057 [INFO] Step[650/2713]: training loss : 0.9277626836299896 TRAIN  loss dict:  {'classification_loss': 0.9277626836299896}
2025-01-15 07:55:48,585 [INFO] Step[700/2713]: training loss : 0.9275979733467102 TRAIN  loss dict:  {'classification_loss': 0.9275979733467102}
2025-01-15 07:56:02,297 [INFO] Step[750/2713]: training loss : 0.9268415117263794 TRAIN  loss dict:  {'classification_loss': 0.9268415117263794}
2025-01-15 07:56:16,137 [INFO] Step[800/2713]: training loss : 0.9272694754600525 TRAIN  loss dict:  {'classification_loss': 0.9272694754600525}
2025-01-15 07:56:29,897 [INFO] Step[850/2713]: training loss : 0.927296051979065 TRAIN  loss dict:  {'classification_loss': 0.927296051979065}
2025-01-15 07:56:43,711 [INFO] Step[900/2713]: training loss : 0.9277297508716583 TRAIN  loss dict:  {'classification_loss': 0.9277297508716583}
2025-01-15 07:56:57,231 [INFO] Step[950/2713]: training loss : 0.9270463824272156 TRAIN  loss dict:  {'classification_loss': 0.9270463824272156}
2025-01-15 07:57:11,317 [INFO] Step[1000/2713]: training loss : 0.9281512260437011 TRAIN  loss dict:  {'classification_loss': 0.9281512260437011}
2025-01-15 07:57:24,811 [INFO] Step[1050/2713]: training loss : 0.9271529495716095 TRAIN  loss dict:  {'classification_loss': 0.9271529495716095}
2025-01-15 07:57:38,351 [INFO] Step[1100/2713]: training loss : 0.9270077538490296 TRAIN  loss dict:  {'classification_loss': 0.9270077538490296}
2025-01-15 07:57:51,970 [INFO] Step[1150/2713]: training loss : 0.927954980134964 TRAIN  loss dict:  {'classification_loss': 0.927954980134964}
2025-01-15 07:58:06,040 [INFO] Step[1200/2713]: training loss : 0.9269946074485779 TRAIN  loss dict:  {'classification_loss': 0.9269946074485779}
2025-01-15 07:58:20,128 [INFO] Step[1250/2713]: training loss : 0.9275459408760071 TRAIN  loss dict:  {'classification_loss': 0.9275459408760071}
2025-01-15 07:58:34,199 [INFO] Step[1300/2713]: training loss : 0.9287254548072815 TRAIN  loss dict:  {'classification_loss': 0.9287254548072815}
2025-01-15 07:58:47,824 [INFO] Step[1350/2713]: training loss : 0.9270776093006134 TRAIN  loss dict:  {'classification_loss': 0.9270776093006134}
2025-01-15 07:59:01,062 [INFO] Step[1400/2713]: training loss : 0.9274559414386749 TRAIN  loss dict:  {'classification_loss': 0.9274559414386749}
2025-01-15 07:59:14,245 [INFO] Step[1450/2713]: training loss : 0.9267858755588532 TRAIN  loss dict:  {'classification_loss': 0.9267858755588532}
2025-01-15 07:59:27,891 [INFO] Step[1500/2713]: training loss : 0.9273046803474426 TRAIN  loss dict:  {'classification_loss': 0.9273046803474426}
2025-01-15 07:59:41,878 [INFO] Step[1550/2713]: training loss : 0.927076518535614 TRAIN  loss dict:  {'classification_loss': 0.927076518535614}
2025-01-15 07:59:55,321 [INFO] Step[1600/2713]: training loss : 0.9272149360179901 TRAIN  loss dict:  {'classification_loss': 0.9272149360179901}
2025-01-15 08:00:08,960 [INFO] Step[1650/2713]: training loss : 0.927487404346466 TRAIN  loss dict:  {'classification_loss': 0.927487404346466}
2025-01-15 08:00:22,534 [INFO] Step[1700/2713]: training loss : 0.9273723351955414 TRAIN  loss dict:  {'classification_loss': 0.9273723351955414}
2025-01-15 08:00:35,918 [INFO] Step[1750/2713]: training loss : 0.9278382420539856 TRAIN  loss dict:  {'classification_loss': 0.9278382420539856}
2025-01-15 08:00:49,121 [INFO] Step[1800/2713]: training loss : 0.9280806493759155 TRAIN  loss dict:  {'classification_loss': 0.9280806493759155}
2025-01-15 08:01:02,547 [INFO] Step[1850/2713]: training loss : 0.9277057123184204 TRAIN  loss dict:  {'classification_loss': 0.9277057123184204}
2025-01-15 08:01:17,777 [INFO] Step[1900/2713]: training loss : 0.9282661783695221 TRAIN  loss dict:  {'classification_loss': 0.9282661783695221}
2025-01-15 08:01:34,328 [INFO] Step[1950/2713]: training loss : 0.9281401109695434 TRAIN  loss dict:  {'classification_loss': 0.9281401109695434}
2025-01-15 08:01:48,518 [INFO] Step[2000/2713]: training loss : 0.9269975006580353 TRAIN  loss dict:  {'classification_loss': 0.9269975006580353}
2025-01-15 08:02:02,373 [INFO] Step[2050/2713]: training loss : 0.9268504285812378 TRAIN  loss dict:  {'classification_loss': 0.9268504285812378}
2025-01-15 08:02:15,845 [INFO] Step[2100/2713]: training loss : 0.9289987802505493 TRAIN  loss dict:  {'classification_loss': 0.9289987802505493}
2025-01-15 08:02:29,837 [INFO] Step[2150/2713]: training loss : 0.927657961845398 TRAIN  loss dict:  {'classification_loss': 0.927657961845398}
2025-01-15 08:02:43,855 [INFO] Step[2200/2713]: training loss : 0.9278219652175903 TRAIN  loss dict:  {'classification_loss': 0.9278219652175903}
2025-01-15 08:02:57,027 [INFO] Step[2250/2713]: training loss : 0.9282250452041626 TRAIN  loss dict:  {'classification_loss': 0.9282250452041626}
2025-01-15 08:03:10,304 [INFO] Step[2300/2713]: training loss : 0.9271595621109009 TRAIN  loss dict:  {'classification_loss': 0.9271595621109009}
2025-01-15 08:03:24,143 [INFO] Step[2350/2713]: training loss : 0.9273959922790528 TRAIN  loss dict:  {'classification_loss': 0.9273959922790528}
2025-01-15 08:03:38,130 [INFO] Step[2400/2713]: training loss : 0.9289827013015747 TRAIN  loss dict:  {'classification_loss': 0.9289827013015747}
2025-01-15 08:03:51,517 [INFO] Step[2450/2713]: training loss : 0.9281695342063904 TRAIN  loss dict:  {'classification_loss': 0.9281695342063904}
2025-01-15 08:04:05,387 [INFO] Step[2500/2713]: training loss : 0.9276840078830719 TRAIN  loss dict:  {'classification_loss': 0.9276840078830719}
2025-01-15 08:04:18,858 [INFO] Step[2550/2713]: training loss : 0.9270748698711395 TRAIN  loss dict:  {'classification_loss': 0.9270748698711395}
2025-01-15 08:04:32,066 [INFO] Step[2600/2713]: training loss : 0.9369067180156708 TRAIN  loss dict:  {'classification_loss': 0.9369067180156708}
2025-01-15 08:04:48,472 [INFO] Step[2650/2713]: training loss : 0.9273197388648987 TRAIN  loss dict:  {'classification_loss': 0.9273197388648987}
2025-01-15 08:05:03,572 [INFO] Step[2700/2713]: training loss : 0.9270172035694122 TRAIN  loss dict:  {'classification_loss': 0.9270172035694122}
2025-01-15 08:06:19,507 [INFO] Label accuracies statistics:
2025-01-15 08:06:19,507 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.5, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.0, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.75, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 08:06:19,509 [INFO] [90] TRAIN  loss: 0.9277327191421039 acc: 1.0
2025-01-15 08:06:19,509 [INFO] [90] TRAIN  loss dict: {'classification_loss': 0.9277327191421039}
2025-01-15 08:06:19,510 [INFO] [90] VALIDATION loss: 1.8006946500530816 VALIDATION acc: 0.8119122257053292
2025-01-15 08:06:19,510 [INFO] [90] VALIDATION loss dict: {'classification_loss': 1.8006946500530816}
2025-01-15 08:06:19,510 [INFO] 
2025-01-15 08:06:38,508 [INFO] Step[50/2713]: training loss : 0.9295488727092743 TRAIN  loss dict:  {'classification_loss': 0.9295488727092743}
2025-01-15 08:06:51,811 [INFO] Step[100/2713]: training loss : 0.9274049186706543 TRAIN  loss dict:  {'classification_loss': 0.9274049186706543}
2025-01-15 08:07:05,275 [INFO] Step[150/2713]: training loss : 0.9268618679046631 TRAIN  loss dict:  {'classification_loss': 0.9268618679046631}
2025-01-15 08:07:18,773 [INFO] Step[200/2713]: training loss : 0.9270000827312469 TRAIN  loss dict:  {'classification_loss': 0.9270000827312469}
2025-01-15 08:07:32,711 [INFO] Step[250/2713]: training loss : 0.9269471597671509 TRAIN  loss dict:  {'classification_loss': 0.9269471597671509}
2025-01-15 08:07:46,472 [INFO] Step[300/2713]: training loss : 0.9273042595386505 TRAIN  loss dict:  {'classification_loss': 0.9273042595386505}
2025-01-15 08:07:59,602 [INFO] Step[350/2713]: training loss : 0.9269269132614135 TRAIN  loss dict:  {'classification_loss': 0.9269269132614135}
2025-01-15 08:08:13,689 [INFO] Step[400/2713]: training loss : 0.9268818378448487 TRAIN  loss dict:  {'classification_loss': 0.9268818378448487}
2025-01-15 08:08:27,577 [INFO] Step[450/2713]: training loss : 0.9269786977767944 TRAIN  loss dict:  {'classification_loss': 0.9269786977767944}
2025-01-15 08:08:41,123 [INFO] Step[500/2713]: training loss : 0.9269343626499176 TRAIN  loss dict:  {'classification_loss': 0.9269343626499176}
2025-01-15 08:08:54,693 [INFO] Step[550/2713]: training loss : 0.9271901023387908 TRAIN  loss dict:  {'classification_loss': 0.9271901023387908}
2025-01-15 08:09:08,554 [INFO] Step[600/2713]: training loss : 0.9269687938690185 TRAIN  loss dict:  {'classification_loss': 0.9269687938690185}
2025-01-15 08:09:22,241 [INFO] Step[650/2713]: training loss : 0.9266124737262725 TRAIN  loss dict:  {'classification_loss': 0.9266124737262725}
2025-01-15 08:09:35,950 [INFO] Step[700/2713]: training loss : 0.9287971031665802 TRAIN  loss dict:  {'classification_loss': 0.9287971031665802}
2025-01-15 08:09:49,924 [INFO] Step[750/2713]: training loss : 0.9271501803398132 TRAIN  loss dict:  {'classification_loss': 0.9271501803398132}
2025-01-15 08:10:03,874 [INFO] Step[800/2713]: training loss : 0.9271521270275116 TRAIN  loss dict:  {'classification_loss': 0.9271521270275116}
2025-01-15 08:10:17,537 [INFO] Step[850/2713]: training loss : 0.9271641421318054 TRAIN  loss dict:  {'classification_loss': 0.9271641421318054}
2025-01-15 08:10:31,543 [INFO] Step[900/2713]: training loss : 0.9272146725654602 TRAIN  loss dict:  {'classification_loss': 0.9272146725654602}
2025-01-15 08:10:48,190 [INFO] Step[950/2713]: training loss : 0.9649705100059509 TRAIN  loss dict:  {'classification_loss': 0.9649705100059509}
2025-01-15 08:11:02,770 [INFO] Step[1000/2713]: training loss : 0.9272861588001251 TRAIN  loss dict:  {'classification_loss': 0.9272861588001251}
2025-01-15 08:11:16,968 [INFO] Step[1050/2713]: training loss : 0.9271284210681915 TRAIN  loss dict:  {'classification_loss': 0.9271284210681915}
2025-01-15 08:11:30,481 [INFO] Step[1100/2713]: training loss : 0.9269962787628174 TRAIN  loss dict:  {'classification_loss': 0.9269962787628174}
2025-01-15 08:11:44,138 [INFO] Step[1150/2713]: training loss : 0.9266736280918121 TRAIN  loss dict:  {'classification_loss': 0.9266736280918121}
2025-01-15 08:11:57,492 [INFO] Step[1200/2713]: training loss : 0.9272728312015533 TRAIN  loss dict:  {'classification_loss': 0.9272728312015533}
2025-01-15 08:12:10,669 [INFO] Step[1250/2713]: training loss : 0.926976979970932 TRAIN  loss dict:  {'classification_loss': 0.926976979970932}
2025-01-15 08:12:24,137 [INFO] Step[1300/2713]: training loss : 0.9282744038105011 TRAIN  loss dict:  {'classification_loss': 0.9282744038105011}
2025-01-15 08:12:37,323 [INFO] Step[1350/2713]: training loss : 0.9268218123912811 TRAIN  loss dict:  {'classification_loss': 0.9268218123912811}
2025-01-15 08:12:50,445 [INFO] Step[1400/2713]: training loss : 0.9290878665447235 TRAIN  loss dict:  {'classification_loss': 0.9290878665447235}
2025-01-15 08:13:04,278 [INFO] Step[1450/2713]: training loss : 0.9278005480766296 TRAIN  loss dict:  {'classification_loss': 0.9278005480766296}
2025-01-15 08:13:17,764 [INFO] Step[1500/2713]: training loss : 0.9272967064380646 TRAIN  loss dict:  {'classification_loss': 0.9272967064380646}
2025-01-15 08:13:31,670 [INFO] Step[1550/2713]: training loss : 0.9265384984016418 TRAIN  loss dict:  {'classification_loss': 0.9265384984016418}
2025-01-15 08:13:45,125 [INFO] Step[1600/2713]: training loss : 0.9268893325328826 TRAIN  loss dict:  {'classification_loss': 0.9268893325328826}
2025-01-15 08:13:58,657 [INFO] Step[1650/2713]: training loss : 0.9284071171283722 TRAIN  loss dict:  {'classification_loss': 0.9284071171283722}
2025-01-15 08:14:11,796 [INFO] Step[1700/2713]: training loss : 0.9272037160396576 TRAIN  loss dict:  {'classification_loss': 0.9272037160396576}
2025-01-15 08:14:24,960 [INFO] Step[1750/2713]: training loss : 0.9275697672367096 TRAIN  loss dict:  {'classification_loss': 0.9275697672367096}
2025-01-15 08:14:38,727 [INFO] Step[1800/2713]: training loss : 0.9274878644943237 TRAIN  loss dict:  {'classification_loss': 0.9274878644943237}
2025-01-15 08:14:52,308 [INFO] Step[1850/2713]: training loss : 0.927062064409256 TRAIN  loss dict:  {'classification_loss': 0.927062064409256}
2025-01-15 08:15:05,949 [INFO] Step[1900/2713]: training loss : 0.9265535235404968 TRAIN  loss dict:  {'classification_loss': 0.9265535235404968}
2025-01-15 08:15:19,393 [INFO] Step[1950/2713]: training loss : 0.9273386085033417 TRAIN  loss dict:  {'classification_loss': 0.9273386085033417}
2025-01-15 08:15:32,550 [INFO] Step[2000/2713]: training loss : 0.9267652785778046 TRAIN  loss dict:  {'classification_loss': 0.9267652785778046}
2025-01-15 08:15:46,606 [INFO] Step[2050/2713]: training loss : 0.9529246413707733 TRAIN  loss dict:  {'classification_loss': 0.9529246413707733}
2025-01-15 08:16:00,481 [INFO] Step[2100/2713]: training loss : 0.9279871928691864 TRAIN  loss dict:  {'classification_loss': 0.9279871928691864}
2025-01-15 08:16:13,939 [INFO] Step[2150/2713]: training loss : 0.9268040943145752 TRAIN  loss dict:  {'classification_loss': 0.9268040943145752}
2025-01-15 08:16:27,430 [INFO] Step[2200/2713]: training loss : 0.9272238373756408 TRAIN  loss dict:  {'classification_loss': 0.9272238373756408}
2025-01-15 08:16:40,624 [INFO] Step[2250/2713]: training loss : 0.9274020946025848 TRAIN  loss dict:  {'classification_loss': 0.9274020946025848}
2025-01-15 08:16:54,206 [INFO] Step[2300/2713]: training loss : 0.9270226454734802 TRAIN  loss dict:  {'classification_loss': 0.9270226454734802}
2025-01-15 08:17:07,830 [INFO] Step[2350/2713]: training loss : 0.9274949610233307 TRAIN  loss dict:  {'classification_loss': 0.9274949610233307}
2025-01-15 08:17:20,990 [INFO] Step[2400/2713]: training loss : 0.9264735114574433 TRAIN  loss dict:  {'classification_loss': 0.9264735114574433}
2025-01-15 08:17:35,041 [INFO] Step[2450/2713]: training loss : 0.9272092878818512 TRAIN  loss dict:  {'classification_loss': 0.9272092878818512}
2025-01-15 08:17:48,867 [INFO] Step[2500/2713]: training loss : 0.9261766803264618 TRAIN  loss dict:  {'classification_loss': 0.9261766803264618}
2025-01-15 08:18:02,434 [INFO] Step[2550/2713]: training loss : 0.9276407968997955 TRAIN  loss dict:  {'classification_loss': 0.9276407968997955}
2025-01-15 08:18:16,313 [INFO] Step[2600/2713]: training loss : 0.9265810942649841 TRAIN  loss dict:  {'classification_loss': 0.9265810942649841}
2025-01-15 08:18:30,170 [INFO] Step[2650/2713]: training loss : 0.9270399856567383 TRAIN  loss dict:  {'classification_loss': 0.9270399856567383}
2025-01-15 08:18:43,605 [INFO] Step[2700/2713]: training loss : 0.926698991060257 TRAIN  loss dict:  {'classification_loss': 0.926698991060257}
2025-01-15 08:20:02,349 [INFO] Label accuracies statistics:
2025-01-15 08:20:02,349 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.0, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 08:20:02,351 [INFO] [91] TRAIN  loss: 0.9284016108583183 acc: 0.9996314043494287
2025-01-15 08:20:02,351 [INFO] [91] TRAIN  loss dict: {'classification_loss': 0.9284016108583183}
2025-01-15 08:20:02,351 [INFO] [91] VALIDATION loss: 1.8109216666535328 VALIDATION acc: 0.806269592476489
2025-01-15 08:20:02,351 [INFO] [91] VALIDATION loss dict: {'classification_loss': 1.8109216666535328}
2025-01-15 08:20:02,351 [INFO] 
2025-01-15 08:20:21,339 [INFO] Step[50/2713]: training loss : 0.9269538080692291 TRAIN  loss dict:  {'classification_loss': 0.9269538080692291}
2025-01-15 08:20:34,951 [INFO] Step[100/2713]: training loss : 0.9271872746944427 TRAIN  loss dict:  {'classification_loss': 0.9271872746944427}
2025-01-15 08:20:48,807 [INFO] Step[150/2713]: training loss : 0.9275931537151336 TRAIN  loss dict:  {'classification_loss': 0.9275931537151336}
2025-01-15 08:21:02,278 [INFO] Step[200/2713]: training loss : 0.9270371794700623 TRAIN  loss dict:  {'classification_loss': 0.9270371794700623}
2025-01-15 08:21:15,984 [INFO] Step[250/2713]: training loss : 0.9271409559249878 TRAIN  loss dict:  {'classification_loss': 0.9271409559249878}
2025-01-15 08:21:29,610 [INFO] Step[300/2713]: training loss : 0.9278133428096771 TRAIN  loss dict:  {'classification_loss': 0.9278133428096771}
2025-01-15 08:21:43,143 [INFO] Step[350/2713]: training loss : 0.9272204685211182 TRAIN  loss dict:  {'classification_loss': 0.9272204685211182}
2025-01-15 08:21:56,965 [INFO] Step[400/2713]: training loss : 0.9266361737251282 TRAIN  loss dict:  {'classification_loss': 0.9266361737251282}
2025-01-15 08:22:10,568 [INFO] Step[450/2713]: training loss : 0.9267968213558198 TRAIN  loss dict:  {'classification_loss': 0.9267968213558198}
2025-01-15 08:22:24,443 [INFO] Step[500/2713]: training loss : 0.9267879676818848 TRAIN  loss dict:  {'classification_loss': 0.9267879676818848}
2025-01-15 08:22:38,311 [INFO] Step[550/2713]: training loss : 0.9361553359031677 TRAIN  loss dict:  {'classification_loss': 0.9361553359031677}
2025-01-15 08:22:51,827 [INFO] Step[600/2713]: training loss : 0.9276836943626404 TRAIN  loss dict:  {'classification_loss': 0.9276836943626404}
2025-01-15 08:23:05,615 [INFO] Step[650/2713]: training loss : 0.926669385433197 TRAIN  loss dict:  {'classification_loss': 0.926669385433197}
2025-01-15 08:23:18,891 [INFO] Step[700/2713]: training loss : 0.9281237316131592 TRAIN  loss dict:  {'classification_loss': 0.9281237316131592}
2025-01-15 08:23:32,683 [INFO] Step[750/2713]: training loss : 0.9271264910697937 TRAIN  loss dict:  {'classification_loss': 0.9271264910697937}
2025-01-15 08:23:46,759 [INFO] Step[800/2713]: training loss : 0.9275021171569824 TRAIN  loss dict:  {'classification_loss': 0.9275021171569824}
2025-01-15 08:24:00,159 [INFO] Step[850/2713]: training loss : 0.9268406653404235 TRAIN  loss dict:  {'classification_loss': 0.9268406653404235}
2025-01-15 08:24:13,892 [INFO] Step[900/2713]: training loss : 0.9271892201900482 TRAIN  loss dict:  {'classification_loss': 0.9271892201900482}
2025-01-15 08:24:27,639 [INFO] Step[950/2713]: training loss : 0.9265252244472504 TRAIN  loss dict:  {'classification_loss': 0.9265252244472504}
2025-01-15 08:24:41,293 [INFO] Step[1000/2713]: training loss : 0.9270880961418152 TRAIN  loss dict:  {'classification_loss': 0.9270880961418152}
2025-01-15 08:24:55,473 [INFO] Step[1050/2713]: training loss : 0.9268192958831787 TRAIN  loss dict:  {'classification_loss': 0.9268192958831787}
2025-01-15 08:25:08,954 [INFO] Step[1100/2713]: training loss : 0.9268837988376617 TRAIN  loss dict:  {'classification_loss': 0.9268837988376617}
2025-01-15 08:25:22,700 [INFO] Step[1150/2713]: training loss : 0.9269898355007171 TRAIN  loss dict:  {'classification_loss': 0.9269898355007171}
2025-01-15 08:25:35,970 [INFO] Step[1200/2713]: training loss : 0.9278360795974732 TRAIN  loss dict:  {'classification_loss': 0.9278360795974732}
2025-01-15 08:25:49,995 [INFO] Step[1250/2713]: training loss : 0.9274467539787292 TRAIN  loss dict:  {'classification_loss': 0.9274467539787292}
2025-01-15 08:26:03,233 [INFO] Step[1300/2713]: training loss : 0.9272150611877441 TRAIN  loss dict:  {'classification_loss': 0.9272150611877441}
2025-01-15 08:26:16,661 [INFO] Step[1350/2713]: training loss : 0.9274119436740875 TRAIN  loss dict:  {'classification_loss': 0.9274119436740875}
2025-01-15 08:26:30,110 [INFO] Step[1400/2713]: training loss : 0.9269611918926239 TRAIN  loss dict:  {'classification_loss': 0.9269611918926239}
2025-01-15 08:26:43,296 [INFO] Step[1450/2713]: training loss : 0.9287490105628967 TRAIN  loss dict:  {'classification_loss': 0.9287490105628967}
2025-01-15 08:26:57,052 [INFO] Step[1500/2713]: training loss : 0.9274106621742249 TRAIN  loss dict:  {'classification_loss': 0.9274106621742249}
2025-01-15 08:27:10,663 [INFO] Step[1550/2713]: training loss : 0.9268924641609192 TRAIN  loss dict:  {'classification_loss': 0.9268924641609192}
2025-01-15 08:27:24,181 [INFO] Step[1600/2713]: training loss : 0.9275163125991821 TRAIN  loss dict:  {'classification_loss': 0.9275163125991821}
2025-01-15 08:27:37,334 [INFO] Step[1650/2713]: training loss : 0.9265765047073364 TRAIN  loss dict:  {'classification_loss': 0.9265765047073364}
2025-01-15 08:27:50,895 [INFO] Step[1700/2713]: training loss : 0.9277086651325226 TRAIN  loss dict:  {'classification_loss': 0.9277086651325226}
2025-01-15 08:28:04,048 [INFO] Step[1750/2713]: training loss : 0.9266776764392852 TRAIN  loss dict:  {'classification_loss': 0.9266776764392852}
2025-01-15 08:28:17,633 [INFO] Step[1800/2713]: training loss : 0.9273817205429077 TRAIN  loss dict:  {'classification_loss': 0.9273817205429077}
2025-01-15 08:28:31,205 [INFO] Step[1850/2713]: training loss : 0.9269576668739319 TRAIN  loss dict:  {'classification_loss': 0.9269576668739319}
2025-01-15 08:28:44,894 [INFO] Step[1900/2713]: training loss : 0.9280164301395416 TRAIN  loss dict:  {'classification_loss': 0.9280164301395416}
2025-01-15 08:28:58,513 [INFO] Step[1950/2713]: training loss : 0.9271878600120544 TRAIN  loss dict:  {'classification_loss': 0.9271878600120544}
2025-01-15 08:29:11,711 [INFO] Step[2000/2713]: training loss : 0.9267794823646546 TRAIN  loss dict:  {'classification_loss': 0.9267794823646546}
2025-01-15 08:29:25,511 [INFO] Step[2050/2713]: training loss : 0.927227783203125 TRAIN  loss dict:  {'classification_loss': 0.927227783203125}
2025-01-15 08:29:38,680 [INFO] Step[2100/2713]: training loss : 0.9268752336502075 TRAIN  loss dict:  {'classification_loss': 0.9268752336502075}
2025-01-15 08:29:52,382 [INFO] Step[2150/2713]: training loss : 0.9274238908290863 TRAIN  loss dict:  {'classification_loss': 0.9274238908290863}
2025-01-15 08:30:05,912 [INFO] Step[2200/2713]: training loss : 0.9277380275726318 TRAIN  loss dict:  {'classification_loss': 0.9277380275726318}
2025-01-15 08:30:19,620 [INFO] Step[2250/2713]: training loss : 0.9268875324726105 TRAIN  loss dict:  {'classification_loss': 0.9268875324726105}
2025-01-15 08:30:33,362 [INFO] Step[2300/2713]: training loss : 0.9266774392127991 TRAIN  loss dict:  {'classification_loss': 0.9266774392127991}
2025-01-15 08:30:46,571 [INFO] Step[2350/2713]: training loss : 0.9270569431781769 TRAIN  loss dict:  {'classification_loss': 0.9270569431781769}
2025-01-15 08:31:00,593 [INFO] Step[2400/2713]: training loss : 0.927026071548462 TRAIN  loss dict:  {'classification_loss': 0.927026071548462}
2025-01-15 08:31:14,167 [INFO] Step[2450/2713]: training loss : 0.9281531655788422 TRAIN  loss dict:  {'classification_loss': 0.9281531655788422}
2025-01-15 08:31:27,902 [INFO] Step[2500/2713]: training loss : 0.927019544839859 TRAIN  loss dict:  {'classification_loss': 0.927019544839859}
2025-01-15 08:31:41,519 [INFO] Step[2550/2713]: training loss : 0.9270358347892761 TRAIN  loss dict:  {'classification_loss': 0.9270358347892761}
2025-01-15 08:31:55,023 [INFO] Step[2600/2713]: training loss : 0.9269370591640472 TRAIN  loss dict:  {'classification_loss': 0.9269370591640472}
2025-01-15 08:32:08,593 [INFO] Step[2650/2713]: training loss : 0.9267859196662903 TRAIN  loss dict:  {'classification_loss': 0.9267859196662903}
2025-01-15 08:32:21,790 [INFO] Step[2700/2713]: training loss : 0.9262529957294464 TRAIN  loss dict:  {'classification_loss': 0.9262529957294464}
2025-01-15 08:33:37,236 [INFO] Label accuracies statistics:
2025-01-15 08:33:37,236 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 0.75, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 08:33:37,238 [INFO] [92] TRAIN  loss: 0.9273434344183419 acc: 0.9998771347831429
2025-01-15 08:33:37,238 [INFO] [92] TRAIN  loss dict: {'classification_loss': 0.9273434344183419}
2025-01-15 08:33:37,238 [INFO] [92] VALIDATION loss: 1.8082218441299927 VALIDATION acc: 0.8068965517241379
2025-01-15 08:33:37,238 [INFO] [92] VALIDATION loss dict: {'classification_loss': 1.8082218441299927}
2025-01-15 08:33:37,238 [INFO] 
2025-01-15 08:33:55,995 [INFO] Step[50/2713]: training loss : 0.9263630342483521 TRAIN  loss dict:  {'classification_loss': 0.9263630342483521}
2025-01-15 08:34:10,224 [INFO] Step[100/2713]: training loss : 0.9275644969940186 TRAIN  loss dict:  {'classification_loss': 0.9275644969940186}
2025-01-15 08:34:23,433 [INFO] Step[150/2713]: training loss : 0.9272087347507477 TRAIN  loss dict:  {'classification_loss': 0.9272087347507477}
2025-01-15 08:34:37,269 [INFO] Step[200/2713]: training loss : 0.9273135077953338 TRAIN  loss dict:  {'classification_loss': 0.9273135077953338}
2025-01-15 08:34:50,912 [INFO] Step[250/2713]: training loss : 0.9268922460079193 TRAIN  loss dict:  {'classification_loss': 0.9268922460079193}
2025-01-15 08:35:04,752 [INFO] Step[300/2713]: training loss : 0.9275632977485657 TRAIN  loss dict:  {'classification_loss': 0.9275632977485657}
2025-01-15 08:35:18,410 [INFO] Step[350/2713]: training loss : 0.926717301607132 TRAIN  loss dict:  {'classification_loss': 0.926717301607132}
2025-01-15 08:35:31,876 [INFO] Step[400/2713]: training loss : 0.9273715186119079 TRAIN  loss dict:  {'classification_loss': 0.9273715186119079}
2025-01-15 08:35:45,668 [INFO] Step[450/2713]: training loss : 0.9272486257553101 TRAIN  loss dict:  {'classification_loss': 0.9272486257553101}
2025-01-15 08:35:59,505 [INFO] Step[500/2713]: training loss : 0.9272134470939636 TRAIN  loss dict:  {'classification_loss': 0.9272134470939636}
2025-01-15 08:36:13,445 [INFO] Step[550/2713]: training loss : 0.927247771024704 TRAIN  loss dict:  {'classification_loss': 0.927247771024704}
2025-01-15 08:36:29,223 [INFO] Step[600/2713]: training loss : 0.9273818719387055 TRAIN  loss dict:  {'classification_loss': 0.9273818719387055}
2025-01-15 08:36:43,942 [INFO] Step[650/2713]: training loss : 0.9434224450588227 TRAIN  loss dict:  {'classification_loss': 0.9434224450588227}
2025-01-15 08:36:57,506 [INFO] Step[700/2713]: training loss : 0.9271782612800599 TRAIN  loss dict:  {'classification_loss': 0.9271782612800599}
2025-01-15 08:37:11,097 [INFO] Step[750/2713]: training loss : 0.9270514965057373 TRAIN  loss dict:  {'classification_loss': 0.9270514965057373}
2025-01-15 08:37:25,002 [INFO] Step[800/2713]: training loss : 0.9267439937591553 TRAIN  loss dict:  {'classification_loss': 0.9267439937591553}
2025-01-15 08:37:38,582 [INFO] Step[850/2713]: training loss : 0.9273233699798584 TRAIN  loss dict:  {'classification_loss': 0.9273233699798584}
2025-01-15 08:37:52,444 [INFO] Step[900/2713]: training loss : 0.9267158710956573 TRAIN  loss dict:  {'classification_loss': 0.9267158710956573}
2025-01-15 08:38:06,347 [INFO] Step[950/2713]: training loss : 0.9273369312286377 TRAIN  loss dict:  {'classification_loss': 0.9273369312286377}
2025-01-15 08:38:20,196 [INFO] Step[1000/2713]: training loss : 0.9279265153408051 TRAIN  loss dict:  {'classification_loss': 0.9279265153408051}
2025-01-15 08:38:33,608 [INFO] Step[1050/2713]: training loss : 0.9267527401447296 TRAIN  loss dict:  {'classification_loss': 0.9267527401447296}
2025-01-15 08:38:47,400 [INFO] Step[1100/2713]: training loss : 0.9266124224662781 TRAIN  loss dict:  {'classification_loss': 0.9266124224662781}
2025-01-15 08:39:00,998 [INFO] Step[1150/2713]: training loss : 0.9265541636943817 TRAIN  loss dict:  {'classification_loss': 0.9265541636943817}
2025-01-15 08:39:14,899 [INFO] Step[1200/2713]: training loss : 0.9275523281097412 TRAIN  loss dict:  {'classification_loss': 0.9275523281097412}
2025-01-15 08:39:29,016 [INFO] Step[1250/2713]: training loss : 0.9271913599967957 TRAIN  loss dict:  {'classification_loss': 0.9271913599967957}
2025-01-15 08:39:42,485 [INFO] Step[1300/2713]: training loss : 0.9274087059497833 TRAIN  loss dict:  {'classification_loss': 0.9274087059497833}
2025-01-15 08:39:56,407 [INFO] Step[1350/2713]: training loss : 0.9271349203586579 TRAIN  loss dict:  {'classification_loss': 0.9271349203586579}
2025-01-15 08:40:10,506 [INFO] Step[1400/2713]: training loss : 0.9272273206710815 TRAIN  loss dict:  {'classification_loss': 0.9272273206710815}
2025-01-15 08:40:23,884 [INFO] Step[1450/2713]: training loss : 0.9273756945133209 TRAIN  loss dict:  {'classification_loss': 0.9273756945133209}
2025-01-15 08:40:37,441 [INFO] Step[1500/2713]: training loss : 0.9267705571651459 TRAIN  loss dict:  {'classification_loss': 0.9267705571651459}
2025-01-15 08:40:51,654 [INFO] Step[1550/2713]: training loss : 0.9296665585041046 TRAIN  loss dict:  {'classification_loss': 0.9296665585041046}
2025-01-15 08:41:05,838 [INFO] Step[1600/2713]: training loss : 0.927954752445221 TRAIN  loss dict:  {'classification_loss': 0.927954752445221}
2025-01-15 08:41:19,492 [INFO] Step[1650/2713]: training loss : 0.9270503735542297 TRAIN  loss dict:  {'classification_loss': 0.9270503735542297}
2025-01-15 08:41:33,335 [INFO] Step[1700/2713]: training loss : 0.9273199331760407 TRAIN  loss dict:  {'classification_loss': 0.9273199331760407}
2025-01-15 08:41:47,163 [INFO] Step[1750/2713]: training loss : 0.9302478075027466 TRAIN  loss dict:  {'classification_loss': 0.9302478075027466}
2025-01-15 08:42:00,879 [INFO] Step[1800/2713]: training loss : 0.9271475338935852 TRAIN  loss dict:  {'classification_loss': 0.9271475338935852}
2025-01-15 08:42:14,795 [INFO] Step[1850/2713]: training loss : 0.9284661674499511 TRAIN  loss dict:  {'classification_loss': 0.9284661674499511}
2025-01-15 08:42:28,468 [INFO] Step[1900/2713]: training loss : 0.9275183296203613 TRAIN  loss dict:  {'classification_loss': 0.9275183296203613}
2025-01-15 08:42:41,669 [INFO] Step[1950/2713]: training loss : 0.9272063148021698 TRAIN  loss dict:  {'classification_loss': 0.9272063148021698}
2025-01-15 08:42:55,249 [INFO] Step[2000/2713]: training loss : 0.9272930562496186 TRAIN  loss dict:  {'classification_loss': 0.9272930562496186}
2025-01-15 08:43:09,139 [INFO] Step[2050/2713]: training loss : 0.9267839920520783 TRAIN  loss dict:  {'classification_loss': 0.9267839920520783}
2025-01-15 08:43:22,761 [INFO] Step[2100/2713]: training loss : 0.9280188083648682 TRAIN  loss dict:  {'classification_loss': 0.9280188083648682}
2025-01-15 08:43:36,267 [INFO] Step[2150/2713]: training loss : 0.9277486705780029 TRAIN  loss dict:  {'classification_loss': 0.9277486705780029}
2025-01-15 08:43:49,684 [INFO] Step[2200/2713]: training loss : 0.9267867827415466 TRAIN  loss dict:  {'classification_loss': 0.9267867827415466}
2025-01-15 08:44:03,211 [INFO] Step[2250/2713]: training loss : 0.9282993102073669 TRAIN  loss dict:  {'classification_loss': 0.9282993102073669}
2025-01-15 08:44:16,790 [INFO] Step[2300/2713]: training loss : 0.9296942949295044 TRAIN  loss dict:  {'classification_loss': 0.9296942949295044}
2025-01-15 08:44:30,611 [INFO] Step[2350/2713]: training loss : 0.9273392307758331 TRAIN  loss dict:  {'classification_loss': 0.9273392307758331}
2025-01-15 08:44:44,772 [INFO] Step[2400/2713]: training loss : 0.9278960692882537 TRAIN  loss dict:  {'classification_loss': 0.9278960692882537}
2025-01-15 08:44:58,619 [INFO] Step[2450/2713]: training loss : 0.9275371539592743 TRAIN  loss dict:  {'classification_loss': 0.9275371539592743}
2025-01-15 08:45:12,426 [INFO] Step[2500/2713]: training loss : 0.9267974591255188 TRAIN  loss dict:  {'classification_loss': 0.9267974591255188}
2025-01-15 08:45:25,957 [INFO] Step[2550/2713]: training loss : 0.9274313509464264 TRAIN  loss dict:  {'classification_loss': 0.9274313509464264}
2025-01-15 08:45:39,860 [INFO] Step[2600/2713]: training loss : 0.927336401939392 TRAIN  loss dict:  {'classification_loss': 0.927336401939392}
2025-01-15 08:45:53,496 [INFO] Step[2650/2713]: training loss : 0.9269807207584381 TRAIN  loss dict:  {'classification_loss': 0.9269807207584381}
2025-01-15 08:46:07,386 [INFO] Step[2700/2713]: training loss : 0.9277647352218628 TRAIN  loss dict:  {'classification_loss': 0.9277647352218628}
2025-01-15 08:47:23,792 [INFO] Label accuracies statistics:
2025-01-15 08:47:23,792 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.75, 262: 1.0, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 1.0, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 08:47:23,794 [INFO] [93] TRAIN  loss: 0.9277126302896634 acc: 0.9998771347831429
2025-01-15 08:47:23,794 [INFO] [93] TRAIN  loss dict: {'classification_loss': 0.9277126302896634}
2025-01-15 08:47:23,794 [INFO] [93] VALIDATION loss: 1.7590703601227666 VALIDATION acc: 0.819435736677116
2025-01-15 08:47:23,794 [INFO] [93] VALIDATION loss dict: {'classification_loss': 1.7590703601227666}
2025-01-15 08:47:23,794 [INFO] 
2025-01-15 08:47:42,482 [INFO] Step[50/2713]: training loss : 0.9299399840831757 TRAIN  loss dict:  {'classification_loss': 0.9299399840831757}
2025-01-15 08:47:56,125 [INFO] Step[100/2713]: training loss : 0.9269698143005372 TRAIN  loss dict:  {'classification_loss': 0.9269698143005372}
2025-01-15 08:48:09,783 [INFO] Step[150/2713]: training loss : 0.9269917345046997 TRAIN  loss dict:  {'classification_loss': 0.9269917345046997}
2025-01-15 08:48:23,308 [INFO] Step[200/2713]: training loss : 0.9267990124225617 TRAIN  loss dict:  {'classification_loss': 0.9267990124225617}
2025-01-15 08:48:36,808 [INFO] Step[250/2713]: training loss : 0.92699422955513 TRAIN  loss dict:  {'classification_loss': 0.92699422955513}
2025-01-15 08:48:50,935 [INFO] Step[300/2713]: training loss : 0.9275679075717926 TRAIN  loss dict:  {'classification_loss': 0.9275679075717926}
2025-01-15 08:49:04,274 [INFO] Step[350/2713]: training loss : 0.9274039912223816 TRAIN  loss dict:  {'classification_loss': 0.9274039912223816}
2025-01-15 08:49:18,477 [INFO] Step[400/2713]: training loss : 0.9269867551326751 TRAIN  loss dict:  {'classification_loss': 0.9269867551326751}
2025-01-15 08:49:32,274 [INFO] Step[450/2713]: training loss : 0.9265461361408234 TRAIN  loss dict:  {'classification_loss': 0.9265461361408234}
2025-01-15 08:49:46,272 [INFO] Step[500/2713]: training loss : 0.9271316969394684 TRAIN  loss dict:  {'classification_loss': 0.9271316969394684}
2025-01-15 08:49:59,699 [INFO] Step[550/2713]: training loss : 0.927866929769516 TRAIN  loss dict:  {'classification_loss': 0.927866929769516}
2025-01-15 08:50:13,165 [INFO] Step[600/2713]: training loss : 0.926978497505188 TRAIN  loss dict:  {'classification_loss': 0.926978497505188}
2025-01-15 08:50:26,755 [INFO] Step[650/2713]: training loss : 0.9270054137706757 TRAIN  loss dict:  {'classification_loss': 0.9270054137706757}
2025-01-15 08:50:40,269 [INFO] Step[700/2713]: training loss : 0.9267980587482453 TRAIN  loss dict:  {'classification_loss': 0.9267980587482453}
2025-01-15 08:50:54,299 [INFO] Step[750/2713]: training loss : 0.92709144115448 TRAIN  loss dict:  {'classification_loss': 0.92709144115448}
2025-01-15 08:51:08,534 [INFO] Step[800/2713]: training loss : 0.9272335338592529 TRAIN  loss dict:  {'classification_loss': 0.9272335338592529}
2025-01-15 08:51:22,334 [INFO] Step[850/2713]: training loss : 0.9266997265815735 TRAIN  loss dict:  {'classification_loss': 0.9266997265815735}
2025-01-15 08:51:35,765 [INFO] Step[900/2713]: training loss : 0.9265889215469361 TRAIN  loss dict:  {'classification_loss': 0.9265889215469361}
2025-01-15 08:51:49,476 [INFO] Step[950/2713]: training loss : 0.9271435928344727 TRAIN  loss dict:  {'classification_loss': 0.9271435928344727}
2025-01-15 08:52:03,554 [INFO] Step[1000/2713]: training loss : 0.9263778877258301 TRAIN  loss dict:  {'classification_loss': 0.9263778877258301}
2025-01-15 08:52:17,280 [INFO] Step[1050/2713]: training loss : 0.9268909168243408 TRAIN  loss dict:  {'classification_loss': 0.9268909168243408}
2025-01-15 08:52:31,053 [INFO] Step[1100/2713]: training loss : 0.9271805763244629 TRAIN  loss dict:  {'classification_loss': 0.9271805763244629}
2025-01-15 08:52:44,392 [INFO] Step[1150/2713]: training loss : 0.9268192756175995 TRAIN  loss dict:  {'classification_loss': 0.9268192756175995}
2025-01-15 08:52:58,206 [INFO] Step[1200/2713]: training loss : 0.9275264656543731 TRAIN  loss dict:  {'classification_loss': 0.9275264656543731}
2025-01-15 08:53:12,145 [INFO] Step[1250/2713]: training loss : 0.9268662989139557 TRAIN  loss dict:  {'classification_loss': 0.9268662989139557}
2025-01-15 08:53:28,538 [INFO] Step[1300/2713]: training loss : 0.9267501270771027 TRAIN  loss dict:  {'classification_loss': 0.9267501270771027}
2025-01-15 08:53:41,797 [INFO] Step[1350/2713]: training loss : 0.9270299005508423 TRAIN  loss dict:  {'classification_loss': 0.9270299005508423}
2025-01-15 08:53:54,968 [INFO] Step[1400/2713]: training loss : 0.9270481479167938 TRAIN  loss dict:  {'classification_loss': 0.9270481479167938}
2025-01-15 08:54:08,697 [INFO] Step[1450/2713]: training loss : 0.9271308124065399 TRAIN  loss dict:  {'classification_loss': 0.9271308124065399}
2025-01-15 08:54:22,173 [INFO] Step[1500/2713]: training loss : 0.9266846799850463 TRAIN  loss dict:  {'classification_loss': 0.9266846799850463}
2025-01-15 08:54:35,978 [INFO] Step[1550/2713]: training loss : 0.9287504398822785 TRAIN  loss dict:  {'classification_loss': 0.9287504398822785}
2025-01-15 08:54:49,439 [INFO] Step[1600/2713]: training loss : 0.9263152885437012 TRAIN  loss dict:  {'classification_loss': 0.9263152885437012}
2025-01-15 08:55:03,196 [INFO] Step[1650/2713]: training loss : 0.9269787108898163 TRAIN  loss dict:  {'classification_loss': 0.9269787108898163}
2025-01-15 08:55:16,705 [INFO] Step[1700/2713]: training loss : 0.9268812847137451 TRAIN  loss dict:  {'classification_loss': 0.9268812847137451}
2025-01-15 08:55:30,983 [INFO] Step[1750/2713]: training loss : 0.927096186876297 TRAIN  loss dict:  {'classification_loss': 0.927096186876297}
2025-01-15 08:55:47,686 [INFO] Step[1800/2713]: training loss : 0.9265923929214478 TRAIN  loss dict:  {'classification_loss': 0.9265923929214478}
2025-01-15 08:56:01,634 [INFO] Step[1850/2713]: training loss : 0.926615571975708 TRAIN  loss dict:  {'classification_loss': 0.926615571975708}
2025-01-15 08:56:15,770 [INFO] Step[1900/2713]: training loss : 0.9291318845748902 TRAIN  loss dict:  {'classification_loss': 0.9291318845748902}
2025-01-15 08:56:29,976 [INFO] Step[1950/2713]: training loss : 0.9279836797714234 TRAIN  loss dict:  {'classification_loss': 0.9279836797714234}
2025-01-15 08:56:43,862 [INFO] Step[2000/2713]: training loss : 0.9274832320213318 TRAIN  loss dict:  {'classification_loss': 0.9274832320213318}
2025-01-15 08:56:57,236 [INFO] Step[2050/2713]: training loss : 0.9268750369548797 TRAIN  loss dict:  {'classification_loss': 0.9268750369548797}
2025-01-15 08:57:10,978 [INFO] Step[2100/2713]: training loss : 0.926657702922821 TRAIN  loss dict:  {'classification_loss': 0.926657702922821}
2025-01-15 08:57:24,713 [INFO] Step[2150/2713]: training loss : 0.9277805042266846 TRAIN  loss dict:  {'classification_loss': 0.9277805042266846}
2025-01-15 08:57:38,114 [INFO] Step[2200/2713]: training loss : 0.9274419105052948 TRAIN  loss dict:  {'classification_loss': 0.9274419105052948}
2025-01-15 08:57:51,709 [INFO] Step[2250/2713]: training loss : 0.9265049660205841 TRAIN  loss dict:  {'classification_loss': 0.9265049660205841}
2025-01-15 08:58:05,807 [INFO] Step[2300/2713]: training loss : 0.927554556131363 TRAIN  loss dict:  {'classification_loss': 0.927554556131363}
2025-01-15 08:58:19,817 [INFO] Step[2350/2713]: training loss : 0.9286784934997558 TRAIN  loss dict:  {'classification_loss': 0.9286784934997558}
2025-01-15 08:58:33,118 [INFO] Step[2400/2713]: training loss : 0.92711341381073 TRAIN  loss dict:  {'classification_loss': 0.92711341381073}
2025-01-15 08:58:46,755 [INFO] Step[2450/2713]: training loss : 0.9266934728622437 TRAIN  loss dict:  {'classification_loss': 0.9266934728622437}
2025-01-15 08:59:00,447 [INFO] Step[2500/2713]: training loss : 0.9271765172481536 TRAIN  loss dict:  {'classification_loss': 0.9271765172481536}
2025-01-15 08:59:14,335 [INFO] Step[2550/2713]: training loss : 0.9267259728908539 TRAIN  loss dict:  {'classification_loss': 0.9267259728908539}
2025-01-15 08:59:28,323 [INFO] Step[2600/2713]: training loss : 0.9273031580448151 TRAIN  loss dict:  {'classification_loss': 0.9273031580448151}
2025-01-15 08:59:41,894 [INFO] Step[2650/2713]: training loss : 0.9267340016365051 TRAIN  loss dict:  {'classification_loss': 0.9267340016365051}
2025-01-15 08:59:55,460 [INFO] Step[2700/2713]: training loss : 0.9274119627475739 TRAIN  loss dict:  {'classification_loss': 0.9274119627475739}
2025-01-15 09:01:12,335 [INFO] Label accuracies statistics:
2025-01-15 09:01:12,335 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.0, 204: 0.75, 205: 1.0, 206: 0.5, 207: 1.0, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 1.0, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 09:01:12,337 [INFO] [94] TRAIN  loss: 0.9271799118546072 acc: 1.0
2025-01-15 09:01:12,337 [INFO] [94] TRAIN  loss dict: {'classification_loss': 0.9271799118546072}
2025-01-15 09:01:12,337 [INFO] [94] VALIDATION loss: 1.829427366306011 VALIDATION acc: 0.8075235109717869
2025-01-15 09:01:12,337 [INFO] [94] VALIDATION loss dict: {'classification_loss': 1.829427366306011}
2025-01-15 09:01:12,337 [INFO] 
2025-01-15 09:01:30,746 [INFO] Step[50/2713]: training loss : 0.9287350046634674 TRAIN  loss dict:  {'classification_loss': 0.9287350046634674}
2025-01-15 09:01:44,731 [INFO] Step[100/2713]: training loss : 0.9267664885520935 TRAIN  loss dict:  {'classification_loss': 0.9267664885520935}
2025-01-15 09:01:58,493 [INFO] Step[150/2713]: training loss : 0.9323354470729828 TRAIN  loss dict:  {'classification_loss': 0.9323354470729828}
2025-01-15 09:02:12,236 [INFO] Step[200/2713]: training loss : 0.9267067205905914 TRAIN  loss dict:  {'classification_loss': 0.9267067205905914}
2025-01-15 09:02:26,149 [INFO] Step[250/2713]: training loss : 0.9269563615322113 TRAIN  loss dict:  {'classification_loss': 0.9269563615322113}
2025-01-15 09:02:39,685 [INFO] Step[300/2713]: training loss : 0.9266136074066162 TRAIN  loss dict:  {'classification_loss': 0.9266136074066162}
2025-01-15 09:02:52,899 [INFO] Step[350/2713]: training loss : 0.9314284527301788 TRAIN  loss dict:  {'classification_loss': 0.9314284527301788}
2025-01-15 09:03:07,041 [INFO] Step[400/2713]: training loss : 0.9269400799274444 TRAIN  loss dict:  {'classification_loss': 0.9269400799274444}
2025-01-15 09:03:21,269 [INFO] Step[450/2713]: training loss : 0.9285600996017456 TRAIN  loss dict:  {'classification_loss': 0.9285600996017456}
2025-01-15 09:03:34,591 [INFO] Step[500/2713]: training loss : 0.9278241157531738 TRAIN  loss dict:  {'classification_loss': 0.9278241157531738}
2025-01-15 09:03:48,640 [INFO] Step[550/2713]: training loss : 0.9294661295413971 TRAIN  loss dict:  {'classification_loss': 0.9294661295413971}
2025-01-15 09:04:02,904 [INFO] Step[600/2713]: training loss : 0.9268733644485474 TRAIN  loss dict:  {'classification_loss': 0.9268733644485474}
2025-01-15 09:04:17,083 [INFO] Step[650/2713]: training loss : 0.9270024025440216 TRAIN  loss dict:  {'classification_loss': 0.9270024025440216}
2025-01-15 09:04:30,817 [INFO] Step[700/2713]: training loss : 0.9270102453231811 TRAIN  loss dict:  {'classification_loss': 0.9270102453231811}
2025-01-15 09:04:44,043 [INFO] Step[750/2713]: training loss : 0.9268669891357422 TRAIN  loss dict:  {'classification_loss': 0.9268669891357422}
2025-01-15 09:04:57,406 [INFO] Step[800/2713]: training loss : 0.9269147002696991 TRAIN  loss dict:  {'classification_loss': 0.9269147002696991}
2025-01-15 09:05:10,747 [INFO] Step[850/2713]: training loss : 0.926885941028595 TRAIN  loss dict:  {'classification_loss': 0.926885941028595}
2025-01-15 09:05:23,974 [INFO] Step[900/2713]: training loss : 0.9266244161128998 TRAIN  loss dict:  {'classification_loss': 0.9266244161128998}
2025-01-15 09:05:38,057 [INFO] Step[950/2713]: training loss : 0.9262829744815826 TRAIN  loss dict:  {'classification_loss': 0.9262829744815826}
2025-01-15 09:05:51,752 [INFO] Step[1000/2713]: training loss : 0.9271167266368866 TRAIN  loss dict:  {'classification_loss': 0.9271167266368866}
2025-01-15 09:06:05,265 [INFO] Step[1050/2713]: training loss : 0.9608630084991455 TRAIN  loss dict:  {'classification_loss': 0.9608630084991455}
2025-01-15 09:06:18,936 [INFO] Step[1100/2713]: training loss : 0.9269586062431335 TRAIN  loss dict:  {'classification_loss': 0.9269586062431335}
2025-01-15 09:06:32,693 [INFO] Step[1150/2713]: training loss : 0.9269417130947113 TRAIN  loss dict:  {'classification_loss': 0.9269417130947113}
2025-01-15 09:06:45,916 [INFO] Step[1200/2713]: training loss : 0.9270697844028473 TRAIN  loss dict:  {'classification_loss': 0.9270697844028473}
2025-01-15 09:06:59,177 [INFO] Step[1250/2713]: training loss : 0.9392218613624572 TRAIN  loss dict:  {'classification_loss': 0.9392218613624572}
2025-01-15 09:07:12,716 [INFO] Step[1300/2713]: training loss : 0.9272568666934967 TRAIN  loss dict:  {'classification_loss': 0.9272568666934967}
2025-01-15 09:07:25,926 [INFO] Step[1350/2713]: training loss : 0.9269313383102417 TRAIN  loss dict:  {'classification_loss': 0.9269313383102417}
2025-01-15 09:07:39,436 [INFO] Step[1400/2713]: training loss : 0.9262806189060211 TRAIN  loss dict:  {'classification_loss': 0.9262806189060211}
2025-01-15 09:07:53,666 [INFO] Step[1450/2713]: training loss : 0.9270676529407501 TRAIN  loss dict:  {'classification_loss': 0.9270676529407501}
2025-01-15 09:08:07,582 [INFO] Step[1500/2713]: training loss : 0.9272409331798553 TRAIN  loss dict:  {'classification_loss': 0.9272409331798553}
2025-01-15 09:08:21,140 [INFO] Step[1550/2713]: training loss : 0.9269105315208435 TRAIN  loss dict:  {'classification_loss': 0.9269105315208435}
2025-01-15 09:08:34,887 [INFO] Step[1600/2713]: training loss : 0.9266661202907562 TRAIN  loss dict:  {'classification_loss': 0.9266661202907562}
2025-01-15 09:08:48,931 [INFO] Step[1650/2713]: training loss : 0.9268190920352936 TRAIN  loss dict:  {'classification_loss': 0.9268190920352936}
2025-01-15 09:09:02,122 [INFO] Step[1700/2713]: training loss : 0.927223014831543 TRAIN  loss dict:  {'classification_loss': 0.927223014831543}
2025-01-15 09:09:15,805 [INFO] Step[1750/2713]: training loss : 0.9389949345588684 TRAIN  loss dict:  {'classification_loss': 0.9389949345588684}
2025-01-15 09:09:29,394 [INFO] Step[1800/2713]: training loss : 0.9271579992771148 TRAIN  loss dict:  {'classification_loss': 0.9271579992771148}
2025-01-15 09:09:42,777 [INFO] Step[1850/2713]: training loss : 0.9270006060600281 TRAIN  loss dict:  {'classification_loss': 0.9270006060600281}
2025-01-15 09:09:56,145 [INFO] Step[1900/2713]: training loss : 0.9270838212966919 TRAIN  loss dict:  {'classification_loss': 0.9270838212966919}
2025-01-15 09:10:09,895 [INFO] Step[1950/2713]: training loss : 0.9268221426010131 TRAIN  loss dict:  {'classification_loss': 0.9268221426010131}
2025-01-15 09:10:23,410 [INFO] Step[2000/2713]: training loss : 0.9269820880889893 TRAIN  loss dict:  {'classification_loss': 0.9269820880889893}
2025-01-15 09:10:37,525 [INFO] Step[2050/2713]: training loss : 0.9268510842323303 TRAIN  loss dict:  {'classification_loss': 0.9268510842323303}
2025-01-15 09:10:51,126 [INFO] Step[2100/2713]: training loss : 0.9262417244911194 TRAIN  loss dict:  {'classification_loss': 0.9262417244911194}
2025-01-15 09:11:04,713 [INFO] Step[2150/2713]: training loss : 0.9265420663356781 TRAIN  loss dict:  {'classification_loss': 0.9265420663356781}
2025-01-15 09:11:18,166 [INFO] Step[2200/2713]: training loss : 0.9272383117675781 TRAIN  loss dict:  {'classification_loss': 0.9272383117675781}
2025-01-15 09:11:31,676 [INFO] Step[2250/2713]: training loss : 0.9268348121643066 TRAIN  loss dict:  {'classification_loss': 0.9268348121643066}
2025-01-15 09:11:45,474 [INFO] Step[2300/2713]: training loss : 0.9283093559741974 TRAIN  loss dict:  {'classification_loss': 0.9283093559741974}
2025-01-15 09:11:59,036 [INFO] Step[2350/2713]: training loss : 0.926799465417862 TRAIN  loss dict:  {'classification_loss': 0.926799465417862}
2025-01-15 09:12:12,322 [INFO] Step[2400/2713]: training loss : 0.9266533088684082 TRAIN  loss dict:  {'classification_loss': 0.9266533088684082}
2025-01-15 09:12:26,222 [INFO] Step[2450/2713]: training loss : 0.9268866145610809 TRAIN  loss dict:  {'classification_loss': 0.9268866145610809}
2025-01-15 09:12:39,505 [INFO] Step[2500/2713]: training loss : 0.9269699430465699 TRAIN  loss dict:  {'classification_loss': 0.9269699430465699}
2025-01-15 09:12:53,018 [INFO] Step[2550/2713]: training loss : 0.9264782786369323 TRAIN  loss dict:  {'classification_loss': 0.9264782786369323}
2025-01-15 09:13:06,487 [INFO] Step[2600/2713]: training loss : 0.9265554034709931 TRAIN  loss dict:  {'classification_loss': 0.9265554034709931}
2025-01-15 09:13:20,206 [INFO] Step[2650/2713]: training loss : 0.9268313705921173 TRAIN  loss dict:  {'classification_loss': 0.9268313705921173}
2025-01-15 09:13:34,171 [INFO] Step[2700/2713]: training loss : 0.9266013920307159 TRAIN  loss dict:  {'classification_loss': 0.9266013920307159}
2025-01-15 09:14:50,195 [INFO] Label accuracies statistics:
2025-01-15 09:14:50,195 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.75, 207: 1.0, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.5, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 1.0, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 09:14:50,197 [INFO] [95] TRAIN  loss: 0.9282845217189444 acc: 0.9995085391325715
2025-01-15 09:14:50,197 [INFO] [95] TRAIN  loss dict: {'classification_loss': 0.9282845217189444}
2025-01-15 09:14:50,197 [INFO] [95] VALIDATION loss: 1.7758358865976334 VALIDATION acc: 0.8163009404388715
2025-01-15 09:14:50,197 [INFO] [95] VALIDATION loss dict: {'classification_loss': 1.7758358865976334}
2025-01-15 09:14:50,197 [INFO] 
2025-01-15 09:15:09,246 [INFO] Step[50/2713]: training loss : 0.9274019503593445 TRAIN  loss dict:  {'classification_loss': 0.9274019503593445}
2025-01-15 09:15:23,083 [INFO] Step[100/2713]: training loss : 0.9267681872844696 TRAIN  loss dict:  {'classification_loss': 0.9267681872844696}
2025-01-15 09:15:37,018 [INFO] Step[150/2713]: training loss : 0.9268465304374695 TRAIN  loss dict:  {'classification_loss': 0.9268465304374695}
2025-01-15 09:15:51,180 [INFO] Step[200/2713]: training loss : 0.9365235316753387 TRAIN  loss dict:  {'classification_loss': 0.9365235316753387}
2025-01-15 09:16:05,020 [INFO] Step[250/2713]: training loss : 0.9273181927204132 TRAIN  loss dict:  {'classification_loss': 0.9273181927204132}
2025-01-15 09:16:18,642 [INFO] Step[300/2713]: training loss : 0.9269324266910552 TRAIN  loss dict:  {'classification_loss': 0.9269324266910552}
2025-01-15 09:16:32,407 [INFO] Step[350/2713]: training loss : 0.9265727591514588 TRAIN  loss dict:  {'classification_loss': 0.9265727591514588}
2025-01-15 09:16:46,194 [INFO] Step[400/2713]: training loss : 0.9269146478176117 TRAIN  loss dict:  {'classification_loss': 0.9269146478176117}
2025-01-15 09:17:00,378 [INFO] Step[450/2713]: training loss : 0.9289312446117401 TRAIN  loss dict:  {'classification_loss': 0.9289312446117401}
2025-01-15 09:17:14,557 [INFO] Step[500/2713]: training loss : 0.9276763999462128 TRAIN  loss dict:  {'classification_loss': 0.9276763999462128}
2025-01-15 09:17:28,024 [INFO] Step[550/2713]: training loss : 0.9269718503952027 TRAIN  loss dict:  {'classification_loss': 0.9269718503952027}
2025-01-15 09:17:41,906 [INFO] Step[600/2713]: training loss : 0.9266508519649506 TRAIN  loss dict:  {'classification_loss': 0.9266508519649506}
2025-01-15 09:17:55,459 [INFO] Step[650/2713]: training loss : 0.9306810486316681 TRAIN  loss dict:  {'classification_loss': 0.9306810486316681}
2025-01-15 09:18:09,608 [INFO] Step[700/2713]: training loss : 0.9267284762859345 TRAIN  loss dict:  {'classification_loss': 0.9267284762859345}
2025-01-15 09:18:23,552 [INFO] Step[750/2713]: training loss : 0.9275936424732208 TRAIN  loss dict:  {'classification_loss': 0.9275936424732208}
2025-01-15 09:18:36,979 [INFO] Step[800/2713]: training loss : 0.9266034555435181 TRAIN  loss dict:  {'classification_loss': 0.9266034555435181}
2025-01-15 09:18:50,966 [INFO] Step[850/2713]: training loss : 0.9265865123271942 TRAIN  loss dict:  {'classification_loss': 0.9265865123271942}
2025-01-15 09:19:05,136 [INFO] Step[900/2713]: training loss : 0.9267886281013489 TRAIN  loss dict:  {'classification_loss': 0.9267886281013489}
2025-01-15 09:19:18,396 [INFO] Step[950/2713]: training loss : 0.9268227994441987 TRAIN  loss dict:  {'classification_loss': 0.9268227994441987}
2025-01-15 09:19:32,435 [INFO] Step[1000/2713]: training loss : 0.9265386080741882 TRAIN  loss dict:  {'classification_loss': 0.9265386080741882}
2025-01-15 09:19:45,868 [INFO] Step[1050/2713]: training loss : 0.9286754393577575 TRAIN  loss dict:  {'classification_loss': 0.9286754393577575}
2025-01-15 09:19:59,751 [INFO] Step[1100/2713]: training loss : 0.9271394383907318 TRAIN  loss dict:  {'classification_loss': 0.9271394383907318}
2025-01-15 09:20:13,858 [INFO] Step[1150/2713]: training loss : 0.92775683760643 TRAIN  loss dict:  {'classification_loss': 0.92775683760643}
2025-01-15 09:20:27,698 [INFO] Step[1200/2713]: training loss : 0.9265890324115753 TRAIN  loss dict:  {'classification_loss': 0.9265890324115753}
2025-01-15 09:20:41,191 [INFO] Step[1250/2713]: training loss : 0.9275362300872803 TRAIN  loss dict:  {'classification_loss': 0.9275362300872803}
2025-01-15 09:20:55,233 [INFO] Step[1300/2713]: training loss : 0.9267182278633118 TRAIN  loss dict:  {'classification_loss': 0.9267182278633118}
2025-01-15 09:21:08,473 [INFO] Step[1350/2713]: training loss : 0.9316742360591889 TRAIN  loss dict:  {'classification_loss': 0.9316742360591889}
2025-01-15 09:21:22,268 [INFO] Step[1400/2713]: training loss : 0.9268694257736206 TRAIN  loss dict:  {'classification_loss': 0.9268694257736206}
2025-01-15 09:21:35,503 [INFO] Step[1450/2713]: training loss : 0.9269111502170563 TRAIN  loss dict:  {'classification_loss': 0.9269111502170563}
2025-01-15 09:21:49,611 [INFO] Step[1500/2713]: training loss : 0.927207680940628 TRAIN  loss dict:  {'classification_loss': 0.927207680940628}
2025-01-15 09:22:03,474 [INFO] Step[1550/2713]: training loss : 0.9261853873729706 TRAIN  loss dict:  {'classification_loss': 0.9261853873729706}
2025-01-15 09:22:18,701 [INFO] Step[1600/2713]: training loss : 0.9426898002624512 TRAIN  loss dict:  {'classification_loss': 0.9426898002624512}
2025-01-15 09:22:33,907 [INFO] Step[1650/2713]: training loss : 0.9273207223415375 TRAIN  loss dict:  {'classification_loss': 0.9273207223415375}
2025-01-15 09:22:47,270 [INFO] Step[1700/2713]: training loss : 0.9267379879951477 TRAIN  loss dict:  {'classification_loss': 0.9267379879951477}
2025-01-15 09:23:01,439 [INFO] Step[1750/2713]: training loss : 0.9273213338851929 TRAIN  loss dict:  {'classification_loss': 0.9273213338851929}
2025-01-15 09:23:14,794 [INFO] Step[1800/2713]: training loss : 0.9279744505882264 TRAIN  loss dict:  {'classification_loss': 0.9279744505882264}
2025-01-15 09:23:28,202 [INFO] Step[1850/2713]: training loss : 0.9274530303478241 TRAIN  loss dict:  {'classification_loss': 0.9274530303478241}
2025-01-15 09:23:42,361 [INFO] Step[1900/2713]: training loss : 0.9274837589263916 TRAIN  loss dict:  {'classification_loss': 0.9274837589263916}
2025-01-15 09:23:56,530 [INFO] Step[1950/2713]: training loss : 0.9274587094783783 TRAIN  loss dict:  {'classification_loss': 0.9274587094783783}
2025-01-15 09:24:10,261 [INFO] Step[2000/2713]: training loss : 0.9270693111419678 TRAIN  loss dict:  {'classification_loss': 0.9270693111419678}
2025-01-15 09:24:23,782 [INFO] Step[2050/2713]: training loss : 0.9266083824634552 TRAIN  loss dict:  {'classification_loss': 0.9266083824634552}
2025-01-15 09:24:37,684 [INFO] Step[2100/2713]: training loss : 0.9268076086044311 TRAIN  loss dict:  {'classification_loss': 0.9268076086044311}
2025-01-15 09:24:51,193 [INFO] Step[2150/2713]: training loss : 0.9270492351055145 TRAIN  loss dict:  {'classification_loss': 0.9270492351055145}
2025-01-15 09:25:04,457 [INFO] Step[2200/2713]: training loss : 0.9282991123199463 TRAIN  loss dict:  {'classification_loss': 0.9282991123199463}
2025-01-15 09:25:18,642 [INFO] Step[2250/2713]: training loss : 0.9271069741249085 TRAIN  loss dict:  {'classification_loss': 0.9271069741249085}
2025-01-15 09:25:32,458 [INFO] Step[2300/2713]: training loss : 0.9262910079956055 TRAIN  loss dict:  {'classification_loss': 0.9262910079956055}
2025-01-15 09:25:45,715 [INFO] Step[2350/2713]: training loss : 0.926811410188675 TRAIN  loss dict:  {'classification_loss': 0.926811410188675}
2025-01-15 09:25:58,887 [INFO] Step[2400/2713]: training loss : 0.9264949440956116 TRAIN  loss dict:  {'classification_loss': 0.9264949440956116}
2025-01-15 09:26:12,530 [INFO] Step[2450/2713]: training loss : 0.9270647203922272 TRAIN  loss dict:  {'classification_loss': 0.9270647203922272}
2025-01-15 09:26:25,710 [INFO] Step[2500/2713]: training loss : 0.9273101210594177 TRAIN  loss dict:  {'classification_loss': 0.9273101210594177}
2025-01-15 09:26:38,857 [INFO] Step[2550/2713]: training loss : 0.9270769715309143 TRAIN  loss dict:  {'classification_loss': 0.9270769715309143}
2025-01-15 09:26:52,169 [INFO] Step[2600/2713]: training loss : 0.9265516173839569 TRAIN  loss dict:  {'classification_loss': 0.9265516173839569}
2025-01-15 09:27:06,381 [INFO] Step[2650/2713]: training loss : 0.9267092728614807 TRAIN  loss dict:  {'classification_loss': 0.9267092728614807}
2025-01-15 09:27:19,966 [INFO] Step[2700/2713]: training loss : 0.9271057176589966 TRAIN  loss dict:  {'classification_loss': 0.9271057176589966}
2025-01-15 09:28:36,048 [INFO] Label accuracies statistics:
2025-01-15 09:28:36,048 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 1.0, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 09:28:36,050 [INFO] [96] TRAIN  loss: 0.9276973733460819 acc: 0.9997542695662858
2025-01-15 09:28:36,050 [INFO] [96] TRAIN  loss dict: {'classification_loss': 0.9276973733460819}
2025-01-15 09:28:36,050 [INFO] [96] VALIDATION loss: 1.7672300567304282 VALIDATION acc: 0.8169278996865204
2025-01-15 09:28:36,050 [INFO] [96] VALIDATION loss dict: {'classification_loss': 1.7672300567304282}
2025-01-15 09:28:36,050 [INFO] 
2025-01-15 09:28:55,367 [INFO] Step[50/2713]: training loss : 0.9271820521354676 TRAIN  loss dict:  {'classification_loss': 0.9271820521354676}
2025-01-15 09:29:08,959 [INFO] Step[100/2713]: training loss : 0.9268991136550904 TRAIN  loss dict:  {'classification_loss': 0.9268991136550904}
2025-01-15 09:29:22,576 [INFO] Step[150/2713]: training loss : 0.9265626060962677 TRAIN  loss dict:  {'classification_loss': 0.9265626060962677}
2025-01-15 09:29:36,577 [INFO] Step[200/2713]: training loss : 0.9272174608707427 TRAIN  loss dict:  {'classification_loss': 0.9272174608707427}
2025-01-15 09:29:49,767 [INFO] Step[250/2713]: training loss : 0.9277234160900116 TRAIN  loss dict:  {'classification_loss': 0.9277234160900116}
2025-01-15 09:30:03,357 [INFO] Step[300/2713]: training loss : 0.9269967997074127 TRAIN  loss dict:  {'classification_loss': 0.9269967997074127}
2025-01-15 09:30:17,304 [INFO] Step[350/2713]: training loss : 0.9267102873325348 TRAIN  loss dict:  {'classification_loss': 0.9267102873325348}
2025-01-15 09:30:31,383 [INFO] Step[400/2713]: training loss : 0.9269133865833282 TRAIN  loss dict:  {'classification_loss': 0.9269133865833282}
2025-01-15 09:30:44,637 [INFO] Step[450/2713]: training loss : 0.9267942941188813 TRAIN  loss dict:  {'classification_loss': 0.9267942941188813}
2025-01-15 09:30:57,789 [INFO] Step[500/2713]: training loss : 0.9617004168033599 TRAIN  loss dict:  {'classification_loss': 0.9617004168033599}
2025-01-15 09:31:11,249 [INFO] Step[550/2713]: training loss : 0.9266004586219787 TRAIN  loss dict:  {'classification_loss': 0.9266004586219787}
2025-01-15 09:31:24,496 [INFO] Step[600/2713]: training loss : 0.9269140410423279 TRAIN  loss dict:  {'classification_loss': 0.9269140410423279}
2025-01-15 09:31:38,400 [INFO] Step[650/2713]: training loss : 0.9268704378604888 TRAIN  loss dict:  {'classification_loss': 0.9268704378604888}
2025-01-15 09:31:51,705 [INFO] Step[700/2713]: training loss : 0.926198947429657 TRAIN  loss dict:  {'classification_loss': 0.926198947429657}
2025-01-15 09:32:05,596 [INFO] Step[750/2713]: training loss : 0.928665635585785 TRAIN  loss dict:  {'classification_loss': 0.928665635585785}
2025-01-15 09:32:19,216 [INFO] Step[800/2713]: training loss : 0.9268796348571777 TRAIN  loss dict:  {'classification_loss': 0.9268796348571777}
2025-01-15 09:32:33,440 [INFO] Step[850/2713]: training loss : 0.9273494362831116 TRAIN  loss dict:  {'classification_loss': 0.9273494362831116}
2025-01-15 09:32:47,040 [INFO] Step[900/2713]: training loss : 0.9268433332443238 TRAIN  loss dict:  {'classification_loss': 0.9268433332443238}
2025-01-15 09:33:00,450 [INFO] Step[950/2713]: training loss : 0.9271591436862946 TRAIN  loss dict:  {'classification_loss': 0.9271591436862946}
2025-01-15 09:33:14,206 [INFO] Step[1000/2713]: training loss : 0.9266432809829712 TRAIN  loss dict:  {'classification_loss': 0.9266432809829712}
2025-01-15 09:33:27,863 [INFO] Step[1050/2713]: training loss : 0.9268996167182922 TRAIN  loss dict:  {'classification_loss': 0.9268996167182922}
2025-01-15 09:33:41,459 [INFO] Step[1100/2713]: training loss : 0.9272176468372345 TRAIN  loss dict:  {'classification_loss': 0.9272176468372345}
2025-01-15 09:33:55,151 [INFO] Step[1150/2713]: training loss : 0.9266325783729553 TRAIN  loss dict:  {'classification_loss': 0.9266325783729553}
2025-01-15 09:34:09,042 [INFO] Step[1200/2713]: training loss : 0.9266967153549195 TRAIN  loss dict:  {'classification_loss': 0.9266967153549195}
2025-01-15 09:34:22,777 [INFO] Step[1250/2713]: training loss : 0.92666743516922 TRAIN  loss dict:  {'classification_loss': 0.92666743516922}
2025-01-15 09:34:36,699 [INFO] Step[1300/2713]: training loss : 0.9281603562831878 TRAIN  loss dict:  {'classification_loss': 0.9281603562831878}
2025-01-15 09:34:50,289 [INFO] Step[1350/2713]: training loss : 0.9273367369174957 TRAIN  loss dict:  {'classification_loss': 0.9273367369174957}
2025-01-15 09:35:03,824 [INFO] Step[1400/2713]: training loss : 0.9267504727840423 TRAIN  loss dict:  {'classification_loss': 0.9267504727840423}
2025-01-15 09:35:17,651 [INFO] Step[1450/2713]: training loss : 0.9295485234260559 TRAIN  loss dict:  {'classification_loss': 0.9295485234260559}
2025-01-15 09:35:31,579 [INFO] Step[1500/2713]: training loss : 0.9270620584487915 TRAIN  loss dict:  {'classification_loss': 0.9270620584487915}
2025-01-15 09:35:45,383 [INFO] Step[1550/2713]: training loss : 0.9265953123569488 TRAIN  loss dict:  {'classification_loss': 0.9265953123569488}
2025-01-15 09:35:58,707 [INFO] Step[1600/2713]: training loss : 0.9263612520694733 TRAIN  loss dict:  {'classification_loss': 0.9263612520694733}
2025-01-15 09:36:12,204 [INFO] Step[1650/2713]: training loss : 0.9268245387077332 TRAIN  loss dict:  {'classification_loss': 0.9268245387077332}
2025-01-15 09:36:26,024 [INFO] Step[1700/2713]: training loss : 0.9270676040649414 TRAIN  loss dict:  {'classification_loss': 0.9270676040649414}
2025-01-15 09:36:40,093 [INFO] Step[1750/2713]: training loss : 0.9267589366436004 TRAIN  loss dict:  {'classification_loss': 0.9267589366436004}
2025-01-15 09:36:53,549 [INFO] Step[1800/2713]: training loss : 0.9269842517375946 TRAIN  loss dict:  {'classification_loss': 0.9269842517375946}
2025-01-15 09:37:07,177 [INFO] Step[1850/2713]: training loss : 0.9268977475166321 TRAIN  loss dict:  {'classification_loss': 0.9268977475166321}
2025-01-15 09:37:23,137 [INFO] Step[1900/2713]: training loss : 0.9267341768741608 TRAIN  loss dict:  {'classification_loss': 0.9267341768741608}
2025-01-15 09:37:36,965 [INFO] Step[1950/2713]: training loss : 0.9268687808513641 TRAIN  loss dict:  {'classification_loss': 0.9268687808513641}
2025-01-15 09:37:50,644 [INFO] Step[2000/2713]: training loss : 0.9271443593502045 TRAIN  loss dict:  {'classification_loss': 0.9271443593502045}
2025-01-15 09:38:04,196 [INFO] Step[2050/2713]: training loss : 0.9267479729652405 TRAIN  loss dict:  {'classification_loss': 0.9267479729652405}
2025-01-15 09:38:17,801 [INFO] Step[2100/2713]: training loss : 0.9265428304672241 TRAIN  loss dict:  {'classification_loss': 0.9265428304672241}
2025-01-15 09:38:31,023 [INFO] Step[2150/2713]: training loss : 0.9272596096992493 TRAIN  loss dict:  {'classification_loss': 0.9272596096992493}
2025-01-15 09:38:44,320 [INFO] Step[2200/2713]: training loss : 0.9268475103378296 TRAIN  loss dict:  {'classification_loss': 0.9268475103378296}
2025-01-15 09:38:57,545 [INFO] Step[2250/2713]: training loss : 0.927026686668396 TRAIN  loss dict:  {'classification_loss': 0.927026686668396}
2025-01-15 09:39:11,085 [INFO] Step[2300/2713]: training loss : 0.9280078196525574 TRAIN  loss dict:  {'classification_loss': 0.9280078196525574}
2025-01-15 09:39:24,291 [INFO] Step[2350/2713]: training loss : 0.9268905138969421 TRAIN  loss dict:  {'classification_loss': 0.9268905138969421}
2025-01-15 09:39:38,147 [INFO] Step[2400/2713]: training loss : 0.9264161896705627 TRAIN  loss dict:  {'classification_loss': 0.9264161896705627}
2025-01-15 09:39:52,033 [INFO] Step[2450/2713]: training loss : 0.9267149567604065 TRAIN  loss dict:  {'classification_loss': 0.9267149567604065}
2025-01-15 09:40:05,866 [INFO] Step[2500/2713]: training loss : 0.9267095613479615 TRAIN  loss dict:  {'classification_loss': 0.9267095613479615}
2025-01-15 09:40:19,537 [INFO] Step[2550/2713]: training loss : 0.9268334436416626 TRAIN  loss dict:  {'classification_loss': 0.9268334436416626}
2025-01-15 09:40:33,027 [INFO] Step[2600/2713]: training loss : 0.9265689337253571 TRAIN  loss dict:  {'classification_loss': 0.9265689337253571}
2025-01-15 09:40:46,239 [INFO] Step[2650/2713]: training loss : 0.926821380853653 TRAIN  loss dict:  {'classification_loss': 0.926821380853653}
2025-01-15 09:40:59,794 [INFO] Step[2700/2713]: training loss : 0.9268104314804078 TRAIN  loss dict:  {'classification_loss': 0.9268104314804078}
2025-01-15 09:42:15,710 [INFO] Label accuracies statistics:
2025-01-15 09:42:15,710 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.25, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 09:42:15,712 [INFO] [97] TRAIN  loss: 0.9276285844010882 acc: 0.9998771347831429
2025-01-15 09:42:15,712 [INFO] [97] TRAIN  loss dict: {'classification_loss': 0.9276285844010882}
2025-01-15 09:42:15,712 [INFO] [97] VALIDATION loss: 1.7816737218010694 VALIDATION acc: 0.8144200626959248
2025-01-15 09:42:15,712 [INFO] [97] VALIDATION loss dict: {'classification_loss': 1.7816737218010694}
2025-01-15 09:42:15,712 [INFO] 
2025-01-15 09:42:34,929 [INFO] Step[50/2713]: training loss : 0.9266551613807679 TRAIN  loss dict:  {'classification_loss': 0.9266551613807679}
2025-01-15 09:42:49,195 [INFO] Step[100/2713]: training loss : 0.9266256427764893 TRAIN  loss dict:  {'classification_loss': 0.9266256427764893}
2025-01-15 09:43:03,033 [INFO] Step[150/2713]: training loss : 0.9270086741447449 TRAIN  loss dict:  {'classification_loss': 0.9270086741447449}
2025-01-15 09:43:16,402 [INFO] Step[200/2713]: training loss : 0.9268228125572204 TRAIN  loss dict:  {'classification_loss': 0.9268228125572204}
2025-01-15 09:43:30,600 [INFO] Step[250/2713]: training loss : 0.9267151570320129 TRAIN  loss dict:  {'classification_loss': 0.9267151570320129}
2025-01-15 09:43:44,389 [INFO] Step[300/2713]: training loss : 0.9267566132545472 TRAIN  loss dict:  {'classification_loss': 0.9267566132545472}
2025-01-15 09:43:57,843 [INFO] Step[350/2713]: training loss : 0.9325446772575379 TRAIN  loss dict:  {'classification_loss': 0.9325446772575379}
2025-01-15 09:44:11,054 [INFO] Step[400/2713]: training loss : 0.9268949782848358 TRAIN  loss dict:  {'classification_loss': 0.9268949782848358}
2025-01-15 09:44:24,781 [INFO] Step[450/2713]: training loss : 0.9271758151054382 TRAIN  loss dict:  {'classification_loss': 0.9271758151054382}
2025-01-15 09:44:38,934 [INFO] Step[500/2713]: training loss : 0.9263457608222961 TRAIN  loss dict:  {'classification_loss': 0.9263457608222961}
2025-01-15 09:44:52,537 [INFO] Step[550/2713]: training loss : 0.9273779785633087 TRAIN  loss dict:  {'classification_loss': 0.9273779785633087}
2025-01-15 09:45:05,751 [INFO] Step[600/2713]: training loss : 0.928336021900177 TRAIN  loss dict:  {'classification_loss': 0.928336021900177}
2025-01-15 09:45:19,596 [INFO] Step[650/2713]: training loss : 0.9413048803806305 TRAIN  loss dict:  {'classification_loss': 0.9413048803806305}
2025-01-15 09:45:33,452 [INFO] Step[700/2713]: training loss : 0.9270019197463989 TRAIN  loss dict:  {'classification_loss': 0.9270019197463989}
2025-01-15 09:45:47,254 [INFO] Step[750/2713]: training loss : 0.9269829750061035 TRAIN  loss dict:  {'classification_loss': 0.9269829750061035}
2025-01-15 09:46:00,847 [INFO] Step[800/2713]: training loss : 0.9271800577640533 TRAIN  loss dict:  {'classification_loss': 0.9271800577640533}
2025-01-15 09:46:15,063 [INFO] Step[850/2713]: training loss : 0.9266911923885346 TRAIN  loss dict:  {'classification_loss': 0.9266911923885346}
2025-01-15 09:46:28,819 [INFO] Step[900/2713]: training loss : 0.9263699877262116 TRAIN  loss dict:  {'classification_loss': 0.9263699877262116}
2025-01-15 09:46:42,033 [INFO] Step[950/2713]: training loss : 0.9272154569625854 TRAIN  loss dict:  {'classification_loss': 0.9272154569625854}
2025-01-15 09:46:55,640 [INFO] Step[1000/2713]: training loss : 0.9463237988948822 TRAIN  loss dict:  {'classification_loss': 0.9463237988948822}
2025-01-15 09:47:09,353 [INFO] Step[1050/2713]: training loss : 0.9264659261703492 TRAIN  loss dict:  {'classification_loss': 0.9264659261703492}
2025-01-15 09:47:23,080 [INFO] Step[1100/2713]: training loss : 0.9270999073982239 TRAIN  loss dict:  {'classification_loss': 0.9270999073982239}
2025-01-15 09:47:36,953 [INFO] Step[1150/2713]: training loss : 0.9269079482555389 TRAIN  loss dict:  {'classification_loss': 0.9269079482555389}
2025-01-15 09:47:50,638 [INFO] Step[1200/2713]: training loss : 0.9269757461547852 TRAIN  loss dict:  {'classification_loss': 0.9269757461547852}
2025-01-15 09:48:04,437 [INFO] Step[1250/2713]: training loss : 0.9267211973667144 TRAIN  loss dict:  {'classification_loss': 0.9267211973667144}
2025-01-15 09:48:18,442 [INFO] Step[1300/2713]: training loss : 0.9270100498199463 TRAIN  loss dict:  {'classification_loss': 0.9270100498199463}
2025-01-15 09:48:31,862 [INFO] Step[1350/2713]: training loss : 0.9269322979450226 TRAIN  loss dict:  {'classification_loss': 0.9269322979450226}
2025-01-15 09:48:45,171 [INFO] Step[1400/2713]: training loss : 0.928259893655777 TRAIN  loss dict:  {'classification_loss': 0.928259893655777}
2025-01-15 09:48:59,182 [INFO] Step[1450/2713]: training loss : 0.9262962532043457 TRAIN  loss dict:  {'classification_loss': 0.9262962532043457}
2025-01-15 09:49:13,081 [INFO] Step[1500/2713]: training loss : 0.9329162180423737 TRAIN  loss dict:  {'classification_loss': 0.9329162180423737}
2025-01-15 09:49:26,764 [INFO] Step[1550/2713]: training loss : 0.9277729845046997 TRAIN  loss dict:  {'classification_loss': 0.9277729845046997}
2025-01-15 09:49:40,248 [INFO] Step[1600/2713]: training loss : 0.9268204772472382 TRAIN  loss dict:  {'classification_loss': 0.9268204772472382}
2025-01-15 09:49:53,830 [INFO] Step[1650/2713]: training loss : 0.9267440819740296 TRAIN  loss dict:  {'classification_loss': 0.9267440819740296}
2025-01-15 09:50:07,005 [INFO] Step[1700/2713]: training loss : 0.9271277582645416 TRAIN  loss dict:  {'classification_loss': 0.9271277582645416}
2025-01-15 09:50:21,069 [INFO] Step[1750/2713]: training loss : 0.9272141623497009 TRAIN  loss dict:  {'classification_loss': 0.9272141623497009}
2025-01-15 09:50:34,816 [INFO] Step[1800/2713]: training loss : 0.9278978526592254 TRAIN  loss dict:  {'classification_loss': 0.9278978526592254}
2025-01-15 09:50:48,596 [INFO] Step[1850/2713]: training loss : 0.9266024935245514 TRAIN  loss dict:  {'classification_loss': 0.9266024935245514}
2025-01-15 09:51:02,243 [INFO] Step[1900/2713]: training loss : 0.9272857117652893 TRAIN  loss dict:  {'classification_loss': 0.9272857117652893}
2025-01-15 09:51:15,935 [INFO] Step[1950/2713]: training loss : 0.9270677196979523 TRAIN  loss dict:  {'classification_loss': 0.9270677196979523}
2025-01-15 09:51:29,475 [INFO] Step[2000/2713]: training loss : 0.9266227471828461 TRAIN  loss dict:  {'classification_loss': 0.9266227471828461}
2025-01-15 09:51:43,491 [INFO] Step[2050/2713]: training loss : 0.926005551815033 TRAIN  loss dict:  {'classification_loss': 0.926005551815033}
2025-01-15 09:51:57,098 [INFO] Step[2100/2713]: training loss : 0.9262488102912902 TRAIN  loss dict:  {'classification_loss': 0.9262488102912902}
2025-01-15 09:52:10,322 [INFO] Step[2150/2713]: training loss : 0.9262788414955139 TRAIN  loss dict:  {'classification_loss': 0.9262788414955139}
2025-01-15 09:52:23,843 [INFO] Step[2200/2713]: training loss : 0.9273515117168426 TRAIN  loss dict:  {'classification_loss': 0.9273515117168426}
2025-01-15 09:52:37,630 [INFO] Step[2250/2713]: training loss : 0.9267701876163482 TRAIN  loss dict:  {'classification_loss': 0.9267701876163482}
2025-01-15 09:52:50,987 [INFO] Step[2300/2713]: training loss : 0.9269351518154144 TRAIN  loss dict:  {'classification_loss': 0.9269351518154144}
2025-01-15 09:53:04,706 [INFO] Step[2350/2713]: training loss : 0.9265081810951233 TRAIN  loss dict:  {'classification_loss': 0.9265081810951233}
2025-01-15 09:53:18,339 [INFO] Step[2400/2713]: training loss : 0.9266620218753815 TRAIN  loss dict:  {'classification_loss': 0.9266620218753815}
2025-01-15 09:53:31,987 [INFO] Step[2450/2713]: training loss : 0.9263628101348877 TRAIN  loss dict:  {'classification_loss': 0.9263628101348877}
2025-01-15 09:53:45,687 [INFO] Step[2500/2713]: training loss : 0.9264725184440613 TRAIN  loss dict:  {'classification_loss': 0.9264725184440613}
2025-01-15 09:53:59,222 [INFO] Step[2550/2713]: training loss : 0.9268244409561157 TRAIN  loss dict:  {'classification_loss': 0.9268244409561157}
2025-01-15 09:54:13,193 [INFO] Step[2600/2713]: training loss : 0.9300415205955506 TRAIN  loss dict:  {'classification_loss': 0.9300415205955506}
2025-01-15 09:54:26,665 [INFO] Step[2650/2713]: training loss : 0.9271194183826447 TRAIN  loss dict:  {'classification_loss': 0.9271194183826447}
2025-01-15 09:54:40,559 [INFO] Step[2700/2713]: training loss : 0.9284019958972931 TRAIN  loss dict:  {'classification_loss': 0.9284019958972931}
2025-01-15 09:55:56,602 [INFO] Label accuracies statistics:
2025-01-15 09:55:56,602 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 0.5, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.25, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 09:55:56,604 [INFO] [98] TRAIN  loss: 0.9278302868705283 acc: 0.9996314043494287
2025-01-15 09:55:56,604 [INFO] [98] TRAIN  loss dict: {'classification_loss': 0.9278302868705283}
2025-01-15 09:55:56,604 [INFO] [98] VALIDATION loss: 1.7829327601239198 VALIDATION acc: 0.8112852664576803
2025-01-15 09:55:56,604 [INFO] [98] VALIDATION loss dict: {'classification_loss': 1.7829327601239198}
2025-01-15 09:55:56,604 [INFO] 
2025-01-15 09:56:15,570 [INFO] Step[50/2713]: training loss : 0.9274851512908936 TRAIN  loss dict:  {'classification_loss': 0.9274851512908936}
2025-01-15 09:56:28,997 [INFO] Step[100/2713]: training loss : 0.9271759819984436 TRAIN  loss dict:  {'classification_loss': 0.9271759819984436}
2025-01-15 09:56:42,345 [INFO] Step[150/2713]: training loss : 0.9281845390796661 TRAIN  loss dict:  {'classification_loss': 0.9281845390796661}
2025-01-15 09:56:55,971 [INFO] Step[200/2713]: training loss : 0.9265784204006196 TRAIN  loss dict:  {'classification_loss': 0.9265784204006196}
2025-01-15 09:57:09,625 [INFO] Step[250/2713]: training loss : 0.9262999284267426 TRAIN  loss dict:  {'classification_loss': 0.9262999284267426}
2025-01-15 09:57:23,677 [INFO] Step[300/2713]: training loss : 0.9266429102420807 TRAIN  loss dict:  {'classification_loss': 0.9266429102420807}
2025-01-15 09:57:37,458 [INFO] Step[350/2713]: training loss : 0.9266545093059539 TRAIN  loss dict:  {'classification_loss': 0.9266545093059539}
2025-01-15 09:57:50,894 [INFO] Step[400/2713]: training loss : 0.9268197739124298 TRAIN  loss dict:  {'classification_loss': 0.9268197739124298}
2025-01-15 09:58:04,599 [INFO] Step[450/2713]: training loss : 0.9278538024425507 TRAIN  loss dict:  {'classification_loss': 0.9278538024425507}
2025-01-15 09:58:18,079 [INFO] Step[500/2713]: training loss : 0.9265992867946625 TRAIN  loss dict:  {'classification_loss': 0.9265992867946625}
2025-01-15 09:58:32,188 [INFO] Step[550/2713]: training loss : 0.9271490085124969 TRAIN  loss dict:  {'classification_loss': 0.9271490085124969}
2025-01-15 09:58:45,348 [INFO] Step[600/2713]: training loss : 0.9267134702205658 TRAIN  loss dict:  {'classification_loss': 0.9267134702205658}
2025-01-15 09:58:59,172 [INFO] Step[650/2713]: training loss : 0.9270575737953186 TRAIN  loss dict:  {'classification_loss': 0.9270575737953186}
2025-01-15 09:59:12,855 [INFO] Step[700/2713]: training loss : 0.9265779900550842 TRAIN  loss dict:  {'classification_loss': 0.9265779900550842}
2025-01-15 09:59:26,312 [INFO] Step[750/2713]: training loss : 0.9520556604862214 TRAIN  loss dict:  {'classification_loss': 0.9520556604862214}
2025-01-15 09:59:40,141 [INFO] Step[800/2713]: training loss : 0.9264875030517579 TRAIN  loss dict:  {'classification_loss': 0.9264875030517579}
2025-01-15 09:59:56,024 [INFO] Step[850/2713]: training loss : 0.925885660648346 TRAIN  loss dict:  {'classification_loss': 0.925885660648346}
2025-01-15 10:00:09,562 [INFO] Step[900/2713]: training loss : 0.9267511057853699 TRAIN  loss dict:  {'classification_loss': 0.9267511057853699}
2025-01-15 10:00:23,034 [INFO] Step[950/2713]: training loss : 0.9272232592105866 TRAIN  loss dict:  {'classification_loss': 0.9272232592105866}
2025-01-15 10:00:36,773 [INFO] Step[1000/2713]: training loss : 0.9269613015651703 TRAIN  loss dict:  {'classification_loss': 0.9269613015651703}
2025-01-15 10:00:50,534 [INFO] Step[1050/2713]: training loss : 0.9276526308059693 TRAIN  loss dict:  {'classification_loss': 0.9276526308059693}
2025-01-15 10:01:03,920 [INFO] Step[1100/2713]: training loss : 0.9268668568134308 TRAIN  loss dict:  {'classification_loss': 0.9268668568134308}
2025-01-15 10:01:17,105 [INFO] Step[1150/2713]: training loss : 0.927176057100296 TRAIN  loss dict:  {'classification_loss': 0.927176057100296}
2025-01-15 10:01:30,290 [INFO] Step[1200/2713]: training loss : 0.9275494706630707 TRAIN  loss dict:  {'classification_loss': 0.9275494706630707}
2025-01-15 10:01:43,878 [INFO] Step[1250/2713]: training loss : 0.9269077384471893 TRAIN  loss dict:  {'classification_loss': 0.9269077384471893}
2025-01-15 10:01:57,698 [INFO] Step[1300/2713]: training loss : 0.9263476288318634 TRAIN  loss dict:  {'classification_loss': 0.9263476288318634}
2025-01-15 10:02:11,268 [INFO] Step[1350/2713]: training loss : 0.9702630054950714 TRAIN  loss dict:  {'classification_loss': 0.9702630054950714}
2025-01-15 10:02:24,935 [INFO] Step[1400/2713]: training loss : 0.926266747713089 TRAIN  loss dict:  {'classification_loss': 0.926266747713089}
2025-01-15 10:02:38,132 [INFO] Step[1450/2713]: training loss : 0.9265546596050263 TRAIN  loss dict:  {'classification_loss': 0.9265546596050263}
2025-01-15 10:02:52,157 [INFO] Step[1500/2713]: training loss : 0.9268151962757111 TRAIN  loss dict:  {'classification_loss': 0.9268151962757111}
2025-01-15 10:03:05,817 [INFO] Step[1550/2713]: training loss : 0.9263461744785308 TRAIN  loss dict:  {'classification_loss': 0.9263461744785308}
2025-01-15 10:03:19,889 [INFO] Step[1600/2713]: training loss : 0.9268165147304535 TRAIN  loss dict:  {'classification_loss': 0.9268165147304535}
2025-01-15 10:03:33,449 [INFO] Step[1650/2713]: training loss : 0.9262361431121826 TRAIN  loss dict:  {'classification_loss': 0.9262361431121826}
2025-01-15 10:03:47,131 [INFO] Step[1700/2713]: training loss : 0.9267582452297211 TRAIN  loss dict:  {'classification_loss': 0.9267582452297211}
2025-01-15 10:04:00,554 [INFO] Step[1750/2713]: training loss : 0.926811455488205 TRAIN  loss dict:  {'classification_loss': 0.926811455488205}
2025-01-15 10:04:14,165 [INFO] Step[1800/2713]: training loss : 0.9266916942596436 TRAIN  loss dict:  {'classification_loss': 0.9266916942596436}
2025-01-15 10:04:27,671 [INFO] Step[1850/2713]: training loss : 0.9267791426181793 TRAIN  loss dict:  {'classification_loss': 0.9267791426181793}
2025-01-15 10:04:41,183 [INFO] Step[1900/2713]: training loss : 0.9265417158603668 TRAIN  loss dict:  {'classification_loss': 0.9265417158603668}
2025-01-15 10:04:55,287 [INFO] Step[1950/2713]: training loss : 0.9261710822582245 TRAIN  loss dict:  {'classification_loss': 0.9261710822582245}
2025-01-15 10:05:09,199 [INFO] Step[2000/2713]: training loss : 0.9269616651535034 TRAIN  loss dict:  {'classification_loss': 0.9269616651535034}
2025-01-15 10:05:23,322 [INFO] Step[2050/2713]: training loss : 0.9274504411220551 TRAIN  loss dict:  {'classification_loss': 0.9274504411220551}
2025-01-15 10:05:36,968 [INFO] Step[2100/2713]: training loss : 0.9275608253479004 TRAIN  loss dict:  {'classification_loss': 0.9275608253479004}
2025-01-15 10:05:51,619 [INFO] Step[2150/2713]: training loss : 0.9272475671768189 TRAIN  loss dict:  {'classification_loss': 0.9272475671768189}
2025-01-15 10:06:07,816 [INFO] Step[2200/2713]: training loss : 0.9268513596057892 TRAIN  loss dict:  {'classification_loss': 0.9268513596057892}
2025-01-15 10:06:21,517 [INFO] Step[2250/2713]: training loss : 0.9264609539508819 TRAIN  loss dict:  {'classification_loss': 0.9264609539508819}
2025-01-15 10:06:34,958 [INFO] Step[2300/2713]: training loss : 0.9263525879383088 TRAIN  loss dict:  {'classification_loss': 0.9263525879383088}
2025-01-15 10:06:48,937 [INFO] Step[2350/2713]: training loss : 0.927653386592865 TRAIN  loss dict:  {'classification_loss': 0.927653386592865}
2025-01-15 10:07:02,533 [INFO] Step[2400/2713]: training loss : 0.9265292477607727 TRAIN  loss dict:  {'classification_loss': 0.9265292477607727}
2025-01-15 10:07:16,103 [INFO] Step[2450/2713]: training loss : 0.9630077421665192 TRAIN  loss dict:  {'classification_loss': 0.9630077421665192}
2025-01-15 10:07:29,634 [INFO] Step[2500/2713]: training loss : 0.9269001102447509 TRAIN  loss dict:  {'classification_loss': 0.9269001102447509}
2025-01-15 10:07:42,837 [INFO] Step[2550/2713]: training loss : 0.9271709394454956 TRAIN  loss dict:  {'classification_loss': 0.9271709394454956}
2025-01-15 10:07:55,987 [INFO] Step[2600/2713]: training loss : 0.9276296424865723 TRAIN  loss dict:  {'classification_loss': 0.9276296424865723}
2025-01-15 10:08:09,204 [INFO] Step[2650/2713]: training loss : 0.9267145442962647 TRAIN  loss dict:  {'classification_loss': 0.9267145442962647}
2025-01-15 10:08:22,573 [INFO] Step[2700/2713]: training loss : 0.9270562148094177 TRAIN  loss dict:  {'classification_loss': 0.9270562148094177}
2025-01-15 10:09:38,683 [INFO] Label accuracies statistics:
2025-01-15 10:09:38,683 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.5, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 10:09:38,685 [INFO] [99] TRAIN  loss: 0.9288116518176002 acc: 0.9996314043494287
2025-01-15 10:09:38,685 [INFO] [99] TRAIN  loss dict: {'classification_loss': 0.9288116518176002}
2025-01-15 10:09:38,685 [INFO] [99] VALIDATION loss: 1.7591961947150696 VALIDATION acc: 0.819435736677116
2025-01-15 10:09:38,685 [INFO] [99] VALIDATION loss dict: {'classification_loss': 1.7591961947150696}
2025-01-15 10:09:38,685 [INFO] 
2025-01-15 10:09:56,601 [INFO] Step[50/2713]: training loss : 0.9266088438034058 TRAIN  loss dict:  {'classification_loss': 0.9266088438034058}
2025-01-15 10:10:10,109 [INFO] Step[100/2713]: training loss : 0.9278339648246765 TRAIN  loss dict:  {'classification_loss': 0.9278339648246765}
2025-01-15 10:10:23,305 [INFO] Step[150/2713]: training loss : 0.9261976945400238 TRAIN  loss dict:  {'classification_loss': 0.9261976945400238}
2025-01-15 10:10:36,916 [INFO] Step[200/2713]: training loss : 0.9260048866271973 TRAIN  loss dict:  {'classification_loss': 0.9260048866271973}
2025-01-15 10:10:50,396 [INFO] Step[250/2713]: training loss : 0.9266917848587036 TRAIN  loss dict:  {'classification_loss': 0.9266917848587036}
2025-01-15 10:11:04,206 [INFO] Step[300/2713]: training loss : 0.9266709494590759 TRAIN  loss dict:  {'classification_loss': 0.9266709494590759}
2025-01-15 10:11:17,390 [INFO] Step[350/2713]: training loss : 0.9266695475578308 TRAIN  loss dict:  {'classification_loss': 0.9266695475578308}
2025-01-15 10:11:30,977 [INFO] Step[400/2713]: training loss : 0.926997857093811 TRAIN  loss dict:  {'classification_loss': 0.926997857093811}
2025-01-15 10:11:44,158 [INFO] Step[450/2713]: training loss : 0.9270008444786072 TRAIN  loss dict:  {'classification_loss': 0.9270008444786072}
2025-01-15 10:11:57,551 [INFO] Step[500/2713]: training loss : 0.9267772483825684 TRAIN  loss dict:  {'classification_loss': 0.9267772483825684}
2025-01-15 10:12:11,235 [INFO] Step[550/2713]: training loss : 0.92715336561203 TRAIN  loss dict:  {'classification_loss': 0.92715336561203}
2025-01-15 10:12:24,978 [INFO] Step[600/2713]: training loss : 0.9263479459285736 TRAIN  loss dict:  {'classification_loss': 0.9263479459285736}
2025-01-15 10:12:38,214 [INFO] Step[650/2713]: training loss : 0.9268337178230286 TRAIN  loss dict:  {'classification_loss': 0.9268337178230286}
2025-01-15 10:12:51,823 [INFO] Step[700/2713]: training loss : 0.9265025210380554 TRAIN  loss dict:  {'classification_loss': 0.9265025210380554}
2025-01-15 10:13:05,040 [INFO] Step[750/2713]: training loss : 0.9262301433086395 TRAIN  loss dict:  {'classification_loss': 0.9262301433086395}
2025-01-15 10:13:18,727 [INFO] Step[800/2713]: training loss : 0.926375447511673 TRAIN  loss dict:  {'classification_loss': 0.926375447511673}
2025-01-15 10:13:32,345 [INFO] Step[850/2713]: training loss : 0.9266804742813111 TRAIN  loss dict:  {'classification_loss': 0.9266804742813111}
2025-01-15 10:13:45,651 [INFO] Step[900/2713]: training loss : 0.9282676315307617 TRAIN  loss dict:  {'classification_loss': 0.9282676315307617}
2025-01-15 10:13:58,888 [INFO] Step[950/2713]: training loss : 0.9279117810726166 TRAIN  loss dict:  {'classification_loss': 0.9279117810726166}
2025-01-15 10:14:12,556 [INFO] Step[1000/2713]: training loss : 0.9261642217636108 TRAIN  loss dict:  {'classification_loss': 0.9261642217636108}
2025-01-15 10:14:26,011 [INFO] Step[1050/2713]: training loss : 0.9266306614875793 TRAIN  loss dict:  {'classification_loss': 0.9266306614875793}
2025-01-15 10:14:39,547 [INFO] Step[1100/2713]: training loss : 0.9267476534843445 TRAIN  loss dict:  {'classification_loss': 0.9267476534843445}
2025-01-15 10:14:53,418 [INFO] Step[1150/2713]: training loss : 0.9267953515052796 TRAIN  loss dict:  {'classification_loss': 0.9267953515052796}
2025-01-15 10:15:07,128 [INFO] Step[1200/2713]: training loss : 0.9276590168476104 TRAIN  loss dict:  {'classification_loss': 0.9276590168476104}
2025-01-15 10:15:21,014 [INFO] Step[1250/2713]: training loss : 0.9262437319755554 TRAIN  loss dict:  {'classification_loss': 0.9262437319755554}
2025-01-15 10:15:34,845 [INFO] Step[1300/2713]: training loss : 0.9270408570766449 TRAIN  loss dict:  {'classification_loss': 0.9270408570766449}
2025-01-15 10:15:48,011 [INFO] Step[1350/2713]: training loss : 0.9272853183746338 TRAIN  loss dict:  {'classification_loss': 0.9272853183746338}
2025-01-15 10:16:01,811 [INFO] Step[1400/2713]: training loss : 0.928392276763916 TRAIN  loss dict:  {'classification_loss': 0.928392276763916}
2025-01-15 10:16:15,725 [INFO] Step[1450/2713]: training loss : 0.9266308116912841 TRAIN  loss dict:  {'classification_loss': 0.9266308116912841}
2025-01-15 10:16:29,538 [INFO] Step[1500/2713]: training loss : 0.926817432641983 TRAIN  loss dict:  {'classification_loss': 0.926817432641983}
2025-01-15 10:16:43,651 [INFO] Step[1550/2713]: training loss : 0.9261193084716797 TRAIN  loss dict:  {'classification_loss': 0.9261193084716797}
2025-01-15 10:16:57,451 [INFO] Step[1600/2713]: training loss : 0.9264879691600799 TRAIN  loss dict:  {'classification_loss': 0.9264879691600799}
2025-01-15 10:17:10,858 [INFO] Step[1650/2713]: training loss : 0.9267846357822418 TRAIN  loss dict:  {'classification_loss': 0.9267846357822418}
2025-01-15 10:17:24,059 [INFO] Step[1700/2713]: training loss : 0.9267255234718322 TRAIN  loss dict:  {'classification_loss': 0.9267255234718322}
2025-01-15 10:17:38,170 [INFO] Step[1750/2713]: training loss : 0.9263390910625457 TRAIN  loss dict:  {'classification_loss': 0.9263390910625457}
2025-01-15 10:17:52,126 [INFO] Step[1800/2713]: training loss : 0.9261496555805206 TRAIN  loss dict:  {'classification_loss': 0.9261496555805206}
2025-01-15 10:18:05,501 [INFO] Step[1850/2713]: training loss : 0.9266886925697326 TRAIN  loss dict:  {'classification_loss': 0.9266886925697326}
2025-01-15 10:18:19,163 [INFO] Step[1900/2713]: training loss : 0.9266947996616364 TRAIN  loss dict:  {'classification_loss': 0.9266947996616364}
2025-01-15 10:18:33,474 [INFO] Step[1950/2713]: training loss : 0.9269325542449951 TRAIN  loss dict:  {'classification_loss': 0.9269325542449951}
2025-01-15 10:18:46,774 [INFO] Step[2000/2713]: training loss : 0.926512120962143 TRAIN  loss dict:  {'classification_loss': 0.926512120962143}
2025-01-15 10:19:00,689 [INFO] Step[2050/2713]: training loss : 0.9268539917469024 TRAIN  loss dict:  {'classification_loss': 0.9268539917469024}
2025-01-15 10:19:14,923 [INFO] Step[2100/2713]: training loss : 0.9268695652484894 TRAIN  loss dict:  {'classification_loss': 0.9268695652484894}
2025-01-15 10:19:28,597 [INFO] Step[2150/2713]: training loss : 0.9262436497211456 TRAIN  loss dict:  {'classification_loss': 0.9262436497211456}
2025-01-15 10:19:42,269 [INFO] Step[2200/2713]: training loss : 0.9268445980548858 TRAIN  loss dict:  {'classification_loss': 0.9268445980548858}
2025-01-15 10:19:56,251 [INFO] Step[2250/2713]: training loss : 0.926572835445404 TRAIN  loss dict:  {'classification_loss': 0.926572835445404}
2025-01-15 10:20:12,599 [INFO] Step[2300/2713]: training loss : 0.9274128103256225 TRAIN  loss dict:  {'classification_loss': 0.9274128103256225}
2025-01-15 10:20:26,033 [INFO] Step[2350/2713]: training loss : 0.9267671310901642 TRAIN  loss dict:  {'classification_loss': 0.9267671310901642}
2025-01-15 10:20:39,710 [INFO] Step[2400/2713]: training loss : 0.9268633460998535 TRAIN  loss dict:  {'classification_loss': 0.9268633460998535}
2025-01-15 10:20:53,339 [INFO] Step[2450/2713]: training loss : 0.9270370602607727 TRAIN  loss dict:  {'classification_loss': 0.9270370602607727}
2025-01-15 10:21:07,543 [INFO] Step[2500/2713]: training loss : 0.9353616988658905 TRAIN  loss dict:  {'classification_loss': 0.9353616988658905}
2025-01-15 10:21:20,786 [INFO] Step[2550/2713]: training loss : 0.9260627317428589 TRAIN  loss dict:  {'classification_loss': 0.9260627317428589}
2025-01-15 10:21:34,777 [INFO] Step[2600/2713]: training loss : 0.9261637604236603 TRAIN  loss dict:  {'classification_loss': 0.9261637604236603}
2025-01-15 10:21:48,638 [INFO] Step[2650/2713]: training loss : 0.9271870076656341 TRAIN  loss dict:  {'classification_loss': 0.9271870076656341}
2025-01-15 10:22:01,886 [INFO] Step[2700/2713]: training loss : 0.9265073704719543 TRAIN  loss dict:  {'classification_loss': 0.9265073704719543}
2025-01-15 10:23:18,295 [INFO] Label accuracies statistics:
2025-01-15 10:23:18,295 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.5, 395: 0.5, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 10:23:18,297 [INFO] [100] TRAIN  loss: 0.9269353817452199 acc: 0.9998771347831429
2025-01-15 10:23:18,297 [INFO] [100] TRAIN  loss dict: {'classification_loss': 0.9269353817452199}
2025-01-15 10:23:18,297 [INFO] [100] VALIDATION loss: 1.742769645456981 VALIDATION acc: 0.819435736677116
2025-01-15 10:23:18,297 [INFO] [100] VALIDATION loss dict: {'classification_loss': 1.742769645456981}
2025-01-15 10:23:18,297 [INFO] 
2025-01-15 10:23:37,177 [INFO] Step[50/2713]: training loss : 0.9270786201953888 TRAIN  loss dict:  {'classification_loss': 0.9270786201953888}
2025-01-15 10:23:50,700 [INFO] Step[100/2713]: training loss : 0.9262546813488006 TRAIN  loss dict:  {'classification_loss': 0.9262546813488006}
2025-01-15 10:24:04,396 [INFO] Step[150/2713]: training loss : 0.9264929282665253 TRAIN  loss dict:  {'classification_loss': 0.9264929282665253}
2025-01-15 10:24:18,330 [INFO] Step[200/2713]: training loss : 0.9264326560497284 TRAIN  loss dict:  {'classification_loss': 0.9264326560497284}
2025-01-15 10:24:31,912 [INFO] Step[250/2713]: training loss : 0.927497775554657 TRAIN  loss dict:  {'classification_loss': 0.927497775554657}
2025-01-15 10:24:45,450 [INFO] Step[300/2713]: training loss : 0.9262781345844269 TRAIN  loss dict:  {'classification_loss': 0.9262781345844269}
2025-01-15 10:24:58,921 [INFO] Step[350/2713]: training loss : 0.9263561522960663 TRAIN  loss dict:  {'classification_loss': 0.9263561522960663}
2025-01-15 10:25:12,744 [INFO] Step[400/2713]: training loss : 0.9264350390434265 TRAIN  loss dict:  {'classification_loss': 0.9264350390434265}
2025-01-15 10:25:26,451 [INFO] Step[450/2713]: training loss : 0.9266325867176056 TRAIN  loss dict:  {'classification_loss': 0.9266325867176056}
2025-01-15 10:25:40,005 [INFO] Step[500/2713]: training loss : 0.9261671686172486 TRAIN  loss dict:  {'classification_loss': 0.9261671686172486}
2025-01-15 10:25:54,213 [INFO] Step[550/2713]: training loss : 0.9266984379291534 TRAIN  loss dict:  {'classification_loss': 0.9266984379291534}
2025-01-15 10:26:08,188 [INFO] Step[600/2713]: training loss : 0.9266105031967163 TRAIN  loss dict:  {'classification_loss': 0.9266105031967163}
2025-01-15 10:26:21,889 [INFO] Step[650/2713]: training loss : 0.9263157188892365 TRAIN  loss dict:  {'classification_loss': 0.9263157188892365}
2025-01-15 10:26:35,584 [INFO] Step[700/2713]: training loss : 0.9264879584312439 TRAIN  loss dict:  {'classification_loss': 0.9264879584312439}
2025-01-15 10:26:49,598 [INFO] Step[750/2713]: training loss : 0.9264274120330811 TRAIN  loss dict:  {'classification_loss': 0.9264274120330811}
2025-01-15 10:27:03,680 [INFO] Step[800/2713]: training loss : 0.9385728812217713 TRAIN  loss dict:  {'classification_loss': 0.9385728812217713}
2025-01-15 10:27:17,317 [INFO] Step[850/2713]: training loss : 0.9269794118404389 TRAIN  loss dict:  {'classification_loss': 0.9269794118404389}
2025-01-15 10:27:30,592 [INFO] Step[900/2713]: training loss : 0.9265510439872742 TRAIN  loss dict:  {'classification_loss': 0.9265510439872742}
2025-01-15 10:27:44,497 [INFO] Step[950/2713]: training loss : 0.9268140733242035 TRAIN  loss dict:  {'classification_loss': 0.9268140733242035}
2025-01-15 10:27:57,774 [INFO] Step[1000/2713]: training loss : 0.9265195941925048 TRAIN  loss dict:  {'classification_loss': 0.9265195941925048}
2025-01-15 10:28:11,801 [INFO] Step[1050/2713]: training loss : 0.9260389161109924 TRAIN  loss dict:  {'classification_loss': 0.9260389161109924}
2025-01-15 10:28:25,345 [INFO] Step[1100/2713]: training loss : 0.9286362862586975 TRAIN  loss dict:  {'classification_loss': 0.9286362862586975}
2025-01-15 10:28:38,918 [INFO] Step[1150/2713]: training loss : 0.9271191167831421 TRAIN  loss dict:  {'classification_loss': 0.9271191167831421}
2025-01-15 10:28:52,550 [INFO] Step[1200/2713]: training loss : 0.9365244388580323 TRAIN  loss dict:  {'classification_loss': 0.9365244388580323}
2025-01-15 10:29:06,555 [INFO] Step[1250/2713]: training loss : 0.9259202361106873 TRAIN  loss dict:  {'classification_loss': 0.9259202361106873}
2025-01-15 10:29:20,569 [INFO] Step[1300/2713]: training loss : 0.9268310356140137 TRAIN  loss dict:  {'classification_loss': 0.9268310356140137}
2025-01-15 10:29:34,053 [INFO] Step[1350/2713]: training loss : 0.9264563512802124 TRAIN  loss dict:  {'classification_loss': 0.9264563512802124}
2025-01-15 10:29:49,425 [INFO] Step[1400/2713]: training loss : 0.9262042427062989 TRAIN  loss dict:  {'classification_loss': 0.9262042427062989}
2025-01-15 10:30:04,479 [INFO] Step[1450/2713]: training loss : 0.9268451488018036 TRAIN  loss dict:  {'classification_loss': 0.9268451488018036}
2025-01-15 10:30:18,341 [INFO] Step[1500/2713]: training loss : 0.9265504837036133 TRAIN  loss dict:  {'classification_loss': 0.9265504837036133}
2025-01-15 10:30:31,609 [INFO] Step[1550/2713]: training loss : 0.9269169509410858 TRAIN  loss dict:  {'classification_loss': 0.9269169509410858}
2025-01-15 10:30:45,382 [INFO] Step[1600/2713]: training loss : 0.9270055317878723 TRAIN  loss dict:  {'classification_loss': 0.9270055317878723}
2025-01-15 10:30:59,267 [INFO] Step[1650/2713]: training loss : 0.9263386440277099 TRAIN  loss dict:  {'classification_loss': 0.9263386440277099}
2025-01-15 10:31:12,853 [INFO] Step[1700/2713]: training loss : 0.9269269740581513 TRAIN  loss dict:  {'classification_loss': 0.9269269740581513}
2025-01-15 10:31:26,494 [INFO] Step[1750/2713]: training loss : 0.9265268802642822 TRAIN  loss dict:  {'classification_loss': 0.9265268802642822}
2025-01-15 10:31:40,561 [INFO] Step[1800/2713]: training loss : 0.9266553211212158 TRAIN  loss dict:  {'classification_loss': 0.9266553211212158}
2025-01-15 10:31:54,571 [INFO] Step[1850/2713]: training loss : 0.9263564050197601 TRAIN  loss dict:  {'classification_loss': 0.9263564050197601}
2025-01-15 10:32:08,207 [INFO] Step[1900/2713]: training loss : 0.9269658827781677 TRAIN  loss dict:  {'classification_loss': 0.9269658827781677}
2025-01-15 10:32:21,969 [INFO] Step[1950/2713]: training loss : 0.926132264137268 TRAIN  loss dict:  {'classification_loss': 0.926132264137268}
2025-01-15 10:32:35,575 [INFO] Step[2000/2713]: training loss : 0.9265195012092591 TRAIN  loss dict:  {'classification_loss': 0.9265195012092591}
2025-01-15 10:32:49,681 [INFO] Step[2050/2713]: training loss : 0.9268132781982422 TRAIN  loss dict:  {'classification_loss': 0.9268132781982422}
2025-01-15 10:33:03,655 [INFO] Step[2100/2713]: training loss : 0.9269392383098602 TRAIN  loss dict:  {'classification_loss': 0.9269392383098602}
2025-01-15 10:33:17,897 [INFO] Step[2150/2713]: training loss : 0.9264311063289642 TRAIN  loss dict:  {'classification_loss': 0.9264311063289642}
2025-01-15 10:33:31,891 [INFO] Step[2200/2713]: training loss : 0.9268528032302856 TRAIN  loss dict:  {'classification_loss': 0.9268528032302856}
2025-01-15 10:33:46,139 [INFO] Step[2250/2713]: training loss : 0.9260224342346192 TRAIN  loss dict:  {'classification_loss': 0.9260224342346192}
2025-01-15 10:34:00,068 [INFO] Step[2300/2713]: training loss : 0.9282378602027893 TRAIN  loss dict:  {'classification_loss': 0.9282378602027893}
2025-01-15 10:34:14,043 [INFO] Step[2350/2713]: training loss : 0.9277783024311066 TRAIN  loss dict:  {'classification_loss': 0.9277783024311066}
2025-01-15 10:34:27,501 [INFO] Step[2400/2713]: training loss : 0.9268630480766297 TRAIN  loss dict:  {'classification_loss': 0.9268630480766297}
2025-01-15 10:34:41,369 [INFO] Step[2450/2713]: training loss : 0.926229077577591 TRAIN  loss dict:  {'classification_loss': 0.926229077577591}
2025-01-15 10:34:54,858 [INFO] Step[2500/2713]: training loss : 0.928999103307724 TRAIN  loss dict:  {'classification_loss': 0.928999103307724}
2025-01-15 10:35:08,621 [INFO] Step[2550/2713]: training loss : 0.9267451572418213 TRAIN  loss dict:  {'classification_loss': 0.9267451572418213}
2025-01-15 10:35:22,208 [INFO] Step[2600/2713]: training loss : 0.9263440370559692 TRAIN  loss dict:  {'classification_loss': 0.9263440370559692}
2025-01-15 10:35:35,987 [INFO] Step[2650/2713]: training loss : 0.926259127855301 TRAIN  loss dict:  {'classification_loss': 0.926259127855301}
2025-01-15 10:35:50,237 [INFO] Step[2700/2713]: training loss : 0.9270329296588897 TRAIN  loss dict:  {'classification_loss': 0.9270329296588897}
2025-01-15 10:37:06,157 [INFO] Label accuracies statistics:
2025-01-15 10:37:06,157 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 0.5, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 1.0, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.0, 204: 0.75, 205: 0.75, 206: 0.75, 207: 1.0, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 1.0, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 10:37:06,159 [INFO] [101] TRAIN  loss: 0.9271206653210408 acc: 0.9997542695662858
2025-01-15 10:37:06,159 [INFO] [101] TRAIN  loss dict: {'classification_loss': 0.9271206653210408}
2025-01-15 10:37:06,159 [INFO] [101] VALIDATION loss: 1.7650829229811977 VALIDATION acc: 0.8131661442006269
2025-01-15 10:37:06,159 [INFO] [101] VALIDATION loss dict: {'classification_loss': 1.7650829229811977}
2025-01-15 10:37:06,159 [INFO] 
2025-01-15 10:37:25,005 [INFO] Step[50/2713]: training loss : 0.9265724432468414 TRAIN  loss dict:  {'classification_loss': 0.9265724432468414}
2025-01-15 10:37:38,961 [INFO] Step[100/2713]: training loss : 0.9259889256954194 TRAIN  loss dict:  {'classification_loss': 0.9259889256954194}
2025-01-15 10:37:52,976 [INFO] Step[150/2713]: training loss : 0.9263125729560852 TRAIN  loss dict:  {'classification_loss': 0.9263125729560852}
2025-01-15 10:38:06,512 [INFO] Step[200/2713]: training loss : 0.9270648384094238 TRAIN  loss dict:  {'classification_loss': 0.9270648384094238}
2025-01-15 10:38:20,281 [INFO] Step[250/2713]: training loss : 0.9275088667869568 TRAIN  loss dict:  {'classification_loss': 0.9275088667869568}
2025-01-15 10:38:33,861 [INFO] Step[300/2713]: training loss : 0.9268379151821137 TRAIN  loss dict:  {'classification_loss': 0.9268379151821137}
2025-01-15 10:38:47,143 [INFO] Step[350/2713]: training loss : 0.926633187532425 TRAIN  loss dict:  {'classification_loss': 0.926633187532425}
2025-01-15 10:39:00,787 [INFO] Step[400/2713]: training loss : 0.9262775421142578 TRAIN  loss dict:  {'classification_loss': 0.9262775421142578}
2025-01-15 10:39:14,446 [INFO] Step[450/2713]: training loss : 0.9270037341117859 TRAIN  loss dict:  {'classification_loss': 0.9270037341117859}
2025-01-15 10:39:28,102 [INFO] Step[500/2713]: training loss : 0.9263196492195129 TRAIN  loss dict:  {'classification_loss': 0.9263196492195129}
2025-01-15 10:39:42,070 [INFO] Step[550/2713]: training loss : 0.92671799659729 TRAIN  loss dict:  {'classification_loss': 0.92671799659729}
2025-01-15 10:39:56,096 [INFO] Step[600/2713]: training loss : 0.9276885294914246 TRAIN  loss dict:  {'classification_loss': 0.9276885294914246}
2025-01-15 10:40:10,056 [INFO] Step[650/2713]: training loss : 0.9268130159378052 TRAIN  loss dict:  {'classification_loss': 0.9268130159378052}
2025-01-15 10:40:24,079 [INFO] Step[700/2713]: training loss : 0.9274047577381134 TRAIN  loss dict:  {'classification_loss': 0.9274047577381134}
2025-01-15 10:40:37,763 [INFO] Step[750/2713]: training loss : 0.9268549430370331 TRAIN  loss dict:  {'classification_loss': 0.9268549430370331}
2025-01-15 10:40:51,541 [INFO] Step[800/2713]: training loss : 0.9265974140167237 TRAIN  loss dict:  {'classification_loss': 0.9265974140167237}
2025-01-15 10:41:05,188 [INFO] Step[850/2713]: training loss : 0.9267174828052521 TRAIN  loss dict:  {'classification_loss': 0.9267174828052521}
2025-01-15 10:41:18,842 [INFO] Step[900/2713]: training loss : 0.926108386516571 TRAIN  loss dict:  {'classification_loss': 0.926108386516571}
2025-01-15 10:41:32,748 [INFO] Step[950/2713]: training loss : 0.9267843890190125 TRAIN  loss dict:  {'classification_loss': 0.9267843890190125}
2025-01-15 10:41:46,559 [INFO] Step[1000/2713]: training loss : 0.9261323153972626 TRAIN  loss dict:  {'classification_loss': 0.9261323153972626}
2025-01-15 10:42:00,270 [INFO] Step[1050/2713]: training loss : 0.9263828420639038 TRAIN  loss dict:  {'classification_loss': 0.9263828420639038}
2025-01-15 10:42:14,439 [INFO] Step[1100/2713]: training loss : 0.9267068207263947 TRAIN  loss dict:  {'classification_loss': 0.9267068207263947}
2025-01-15 10:42:28,175 [INFO] Step[1150/2713]: training loss : 0.9266719722747803 TRAIN  loss dict:  {'classification_loss': 0.9266719722747803}
2025-01-15 10:42:42,099 [INFO] Step[1200/2713]: training loss : 0.926167231798172 TRAIN  loss dict:  {'classification_loss': 0.926167231798172}
2025-01-15 10:42:56,325 [INFO] Step[1250/2713]: training loss : 0.9268802750110626 TRAIN  loss dict:  {'classification_loss': 0.9268802750110626}
2025-01-15 10:43:10,512 [INFO] Step[1300/2713]: training loss : 0.9266979265213012 TRAIN  loss dict:  {'classification_loss': 0.9266979265213012}
2025-01-15 10:43:23,748 [INFO] Step[1350/2713]: training loss : 0.9275203740596771 TRAIN  loss dict:  {'classification_loss': 0.9275203740596771}
2025-01-15 10:43:37,794 [INFO] Step[1400/2713]: training loss : 0.9269451713562011 TRAIN  loss dict:  {'classification_loss': 0.9269451713562011}
2025-01-15 10:43:51,993 [INFO] Step[1450/2713]: training loss : 0.9260456466674805 TRAIN  loss dict:  {'classification_loss': 0.9260456466674805}
2025-01-15 10:44:05,683 [INFO] Step[1500/2713]: training loss : 0.9264331448078156 TRAIN  loss dict:  {'classification_loss': 0.9264331448078156}
2025-01-15 10:44:19,709 [INFO] Step[1550/2713]: training loss : 0.9263671326637268 TRAIN  loss dict:  {'classification_loss': 0.9263671326637268}
2025-01-15 10:44:33,289 [INFO] Step[1600/2713]: training loss : 0.9266528570652008 TRAIN  loss dict:  {'classification_loss': 0.9266528570652008}
2025-01-15 10:44:47,046 [INFO] Step[1650/2713]: training loss : 0.9263127040863037 TRAIN  loss dict:  {'classification_loss': 0.9263127040863037}
2025-01-15 10:45:00,601 [INFO] Step[1700/2713]: training loss : 0.9263501834869384 TRAIN  loss dict:  {'classification_loss': 0.9263501834869384}
2025-01-15 10:45:14,238 [INFO] Step[1750/2713]: training loss : 0.9270638060569764 TRAIN  loss dict:  {'classification_loss': 0.9270638060569764}
2025-01-15 10:45:28,219 [INFO] Step[1800/2713]: training loss : 0.9265177941322327 TRAIN  loss dict:  {'classification_loss': 0.9265177941322327}
2025-01-15 10:45:41,843 [INFO] Step[1850/2713]: training loss : 0.9260410964488983 TRAIN  loss dict:  {'classification_loss': 0.9260410964488983}
2025-01-15 10:45:55,524 [INFO] Step[1900/2713]: training loss : 0.9450991189479828 TRAIN  loss dict:  {'classification_loss': 0.9450991189479828}
2025-01-15 10:46:09,819 [INFO] Step[1950/2713]: training loss : 0.9262779223918914 TRAIN  loss dict:  {'classification_loss': 0.9262779223918914}
2025-01-15 10:46:23,678 [INFO] Step[2000/2713]: training loss : 0.926329904794693 TRAIN  loss dict:  {'classification_loss': 0.926329904794693}
2025-01-15 10:46:37,752 [INFO] Step[2050/2713]: training loss : 0.9274617207050323 TRAIN  loss dict:  {'classification_loss': 0.9274617207050323}
2025-01-15 10:46:51,533 [INFO] Step[2100/2713]: training loss : 0.9271514689922333 TRAIN  loss dict:  {'classification_loss': 0.9271514689922333}
2025-01-15 10:47:05,809 [INFO] Step[2150/2713]: training loss : 0.9264193952083588 TRAIN  loss dict:  {'classification_loss': 0.9264193952083588}
2025-01-15 10:47:19,947 [INFO] Step[2200/2713]: training loss : 0.9267572832107543 TRAIN  loss dict:  {'classification_loss': 0.9267572832107543}
2025-01-15 10:47:33,742 [INFO] Step[2250/2713]: training loss : 0.9264968371391297 TRAIN  loss dict:  {'classification_loss': 0.9264968371391297}
2025-01-15 10:47:47,283 [INFO] Step[2300/2713]: training loss : 0.9268770778179168 TRAIN  loss dict:  {'classification_loss': 0.9268770778179168}
2025-01-15 10:48:00,574 [INFO] Step[2350/2713]: training loss : 0.9267454195022583 TRAIN  loss dict:  {'classification_loss': 0.9267454195022583}
2025-01-15 10:48:13,840 [INFO] Step[2400/2713]: training loss : 0.9263733458518982 TRAIN  loss dict:  {'classification_loss': 0.9263733458518982}
2025-01-15 10:48:27,164 [INFO] Step[2450/2713]: training loss : 0.9267020308971405 TRAIN  loss dict:  {'classification_loss': 0.9267020308971405}
2025-01-15 10:48:41,435 [INFO] Step[2500/2713]: training loss : 0.9267218673229217 TRAIN  loss dict:  {'classification_loss': 0.9267218673229217}
2025-01-15 10:48:54,688 [INFO] Step[2550/2713]: training loss : 0.9261418437957764 TRAIN  loss dict:  {'classification_loss': 0.9261418437957764}
2025-01-15 10:49:08,093 [INFO] Step[2600/2713]: training loss : 0.9257337391376496 TRAIN  loss dict:  {'classification_loss': 0.9257337391376496}
2025-01-15 10:49:22,363 [INFO] Step[2650/2713]: training loss : 0.9265770435333252 TRAIN  loss dict:  {'classification_loss': 0.9265770435333252}
2025-01-15 10:49:36,561 [INFO] Step[2700/2713]: training loss : 0.9258250272274018 TRAIN  loss dict:  {'classification_loss': 0.9258250272274018}
2025-01-15 10:50:53,169 [INFO] Label accuracies statistics:
2025-01-15 10:50:53,169 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 1.0, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 1.0, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 1.0, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.25, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 10:50:53,170 [INFO] [102] TRAIN  loss: 0.9269575821795325 acc: 0.9998771347831429
2025-01-15 10:50:53,171 [INFO] [102] TRAIN  loss dict: {'classification_loss': 0.9269575821795325}
2025-01-15 10:50:53,171 [INFO] [102] VALIDATION loss: 1.7634491073457819 VALIDATION acc: 0.8188087774294671
2025-01-15 10:50:53,171 [INFO] [102] VALIDATION loss dict: {'classification_loss': 1.7634491073457819}
2025-01-15 10:50:53,171 [INFO] 
2025-01-15 10:51:11,980 [INFO] Step[50/2713]: training loss : 0.9273089611530304 TRAIN  loss dict:  {'classification_loss': 0.9273089611530304}
2025-01-15 10:51:25,275 [INFO] Step[100/2713]: training loss : 0.9261809468269349 TRAIN  loss dict:  {'classification_loss': 0.9261809468269349}
2025-01-15 10:51:38,835 [INFO] Step[150/2713]: training loss : 0.9260023641586304 TRAIN  loss dict:  {'classification_loss': 0.9260023641586304}
2025-01-15 10:51:52,364 [INFO] Step[200/2713]: training loss : 0.9267781841754913 TRAIN  loss dict:  {'classification_loss': 0.9267781841754913}
2025-01-15 10:52:06,068 [INFO] Step[250/2713]: training loss : 0.9269324123859406 TRAIN  loss dict:  {'classification_loss': 0.9269324123859406}
2025-01-15 10:52:19,529 [INFO] Step[300/2713]: training loss : 0.9265048432350159 TRAIN  loss dict:  {'classification_loss': 0.9265048432350159}
2025-01-15 10:52:33,683 [INFO] Step[350/2713]: training loss : 0.926961818933487 TRAIN  loss dict:  {'classification_loss': 0.926961818933487}
2025-01-15 10:52:47,866 [INFO] Step[400/2713]: training loss : 0.9259611022472382 TRAIN  loss dict:  {'classification_loss': 0.9259611022472382}
2025-01-15 10:53:01,454 [INFO] Step[450/2713]: training loss : 0.926399884223938 TRAIN  loss dict:  {'classification_loss': 0.926399884223938}
2025-01-15 10:53:15,206 [INFO] Step[500/2713]: training loss : 0.9268710029125213 TRAIN  loss dict:  {'classification_loss': 0.9268710029125213}
2025-01-15 10:53:29,122 [INFO] Step[550/2713]: training loss : 0.9267739522457122 TRAIN  loss dict:  {'classification_loss': 0.9267739522457122}
2025-01-15 10:53:42,775 [INFO] Step[600/2713]: training loss : 0.9267089700698853 TRAIN  loss dict:  {'classification_loss': 0.9267089700698853}
2025-01-15 10:53:56,358 [INFO] Step[650/2713]: training loss : 0.928570590019226 TRAIN  loss dict:  {'classification_loss': 0.928570590019226}
2025-01-15 10:54:09,625 [INFO] Step[700/2713]: training loss : 0.9268247079849243 TRAIN  loss dict:  {'classification_loss': 0.9268247079849243}
2025-01-15 10:54:22,869 [INFO] Step[750/2713]: training loss : 0.925908031463623 TRAIN  loss dict:  {'classification_loss': 0.925908031463623}
2025-01-15 10:54:36,442 [INFO] Step[800/2713]: training loss : 0.9266974425315857 TRAIN  loss dict:  {'classification_loss': 0.9266974425315857}
2025-01-15 10:54:50,557 [INFO] Step[850/2713]: training loss : 0.9267084431648255 TRAIN  loss dict:  {'classification_loss': 0.9267084431648255}
2025-01-15 10:55:03,850 [INFO] Step[900/2713]: training loss : 0.9284420824050903 TRAIN  loss dict:  {'classification_loss': 0.9284420824050903}
2025-01-15 10:55:17,393 [INFO] Step[950/2713]: training loss : 0.9262682604789734 TRAIN  loss dict:  {'classification_loss': 0.9262682604789734}
2025-01-15 10:55:31,153 [INFO] Step[1000/2713]: training loss : 0.9265792047977448 TRAIN  loss dict:  {'classification_loss': 0.9265792047977448}
2025-01-15 10:55:45,154 [INFO] Step[1050/2713]: training loss : 0.9274454963207245 TRAIN  loss dict:  {'classification_loss': 0.9274454963207245}
2025-01-15 10:55:58,573 [INFO] Step[1100/2713]: training loss : 0.9266977751255036 TRAIN  loss dict:  {'classification_loss': 0.9266977751255036}
2025-01-15 10:56:12,733 [INFO] Step[1150/2713]: training loss : 0.9264986407756806 TRAIN  loss dict:  {'classification_loss': 0.9264986407756806}
2025-01-15 10:56:26,566 [INFO] Step[1200/2713]: training loss : 0.9263106191158295 TRAIN  loss dict:  {'classification_loss': 0.9263106191158295}
2025-01-15 10:56:40,483 [INFO] Step[1250/2713]: training loss : 0.9261686658859253 TRAIN  loss dict:  {'classification_loss': 0.9261686658859253}
2025-01-15 10:56:54,067 [INFO] Step[1300/2713]: training loss : 0.9266054666042328 TRAIN  loss dict:  {'classification_loss': 0.9266054666042328}
2025-01-15 10:57:08,036 [INFO] Step[1350/2713]: training loss : 0.9266530013084412 TRAIN  loss dict:  {'classification_loss': 0.9266530013084412}
2025-01-15 10:57:21,558 [INFO] Step[1400/2713]: training loss : 0.9261920499801636 TRAIN  loss dict:  {'classification_loss': 0.9261920499801636}
2025-01-15 10:57:35,247 [INFO] Step[1450/2713]: training loss : 0.9260442841053009 TRAIN  loss dict:  {'classification_loss': 0.9260442841053009}
2025-01-15 10:57:48,982 [INFO] Step[1500/2713]: training loss : 0.9264990150928497 TRAIN  loss dict:  {'classification_loss': 0.9264990150928497}
2025-01-15 10:58:02,566 [INFO] Step[1550/2713]: training loss : 0.926329243183136 TRAIN  loss dict:  {'classification_loss': 0.926329243183136}
2025-01-15 10:58:16,299 [INFO] Step[1600/2713]: training loss : 0.9264206397533417 TRAIN  loss dict:  {'classification_loss': 0.9264206397533417}
2025-01-15 10:58:30,567 [INFO] Step[1650/2713]: training loss : 0.9261150395870209 TRAIN  loss dict:  {'classification_loss': 0.9261150395870209}
2025-01-15 10:58:43,887 [INFO] Step[1700/2713]: training loss : 0.9267209374904632 TRAIN  loss dict:  {'classification_loss': 0.9267209374904632}
2025-01-15 10:58:58,106 [INFO] Step[1750/2713]: training loss : 0.9263222920894623 TRAIN  loss dict:  {'classification_loss': 0.9263222920894623}
2025-01-15 10:59:12,154 [INFO] Step[1800/2713]: training loss : 0.9264036405086518 TRAIN  loss dict:  {'classification_loss': 0.9264036405086518}
2025-01-15 10:59:25,419 [INFO] Step[1850/2713]: training loss : 0.9258782589435577 TRAIN  loss dict:  {'classification_loss': 0.9258782589435577}
2025-01-15 10:59:39,060 [INFO] Step[1900/2713]: training loss : 0.9268495225906372 TRAIN  loss dict:  {'classification_loss': 0.9268495225906372}
2025-01-15 10:59:52,646 [INFO] Step[1950/2713]: training loss : 0.9257294464111329 TRAIN  loss dict:  {'classification_loss': 0.9257294464111329}
2025-01-15 11:00:06,284 [INFO] Step[2000/2713]: training loss : 0.926708139181137 TRAIN  loss dict:  {'classification_loss': 0.926708139181137}
2025-01-15 11:00:19,982 [INFO] Step[2050/2713]: training loss : 0.9261508870124817 TRAIN  loss dict:  {'classification_loss': 0.9261508870124817}
2025-01-15 11:00:33,919 [INFO] Step[2100/2713]: training loss : 0.9268702745437623 TRAIN  loss dict:  {'classification_loss': 0.9268702745437623}
2025-01-15 11:00:47,845 [INFO] Step[2150/2713]: training loss : 0.926934895515442 TRAIN  loss dict:  {'classification_loss': 0.926934895515442}
2025-01-15 11:01:01,409 [INFO] Step[2200/2713]: training loss : 0.9261832857131957 TRAIN  loss dict:  {'classification_loss': 0.9261832857131957}
2025-01-15 11:01:15,061 [INFO] Step[2250/2713]: training loss : 0.9262370383739471 TRAIN  loss dict:  {'classification_loss': 0.9262370383739471}
2025-01-15 11:01:28,790 [INFO] Step[2300/2713]: training loss : 0.9266026639938354 TRAIN  loss dict:  {'classification_loss': 0.9266026639938354}
2025-01-15 11:01:42,620 [INFO] Step[2350/2713]: training loss : 0.9266581475734711 TRAIN  loss dict:  {'classification_loss': 0.9266581475734711}
2025-01-15 11:01:55,845 [INFO] Step[2400/2713]: training loss : 0.9267146372795105 TRAIN  loss dict:  {'classification_loss': 0.9267146372795105}
2025-01-15 11:02:09,782 [INFO] Step[2450/2713]: training loss : 0.9261752557754517 TRAIN  loss dict:  {'classification_loss': 0.9261752557754517}
2025-01-15 11:02:24,081 [INFO] Step[2500/2713]: training loss : 0.9265165460109711 TRAIN  loss dict:  {'classification_loss': 0.9265165460109711}
2025-01-15 11:02:37,813 [INFO] Step[2550/2713]: training loss : 0.9263885128498077 TRAIN  loss dict:  {'classification_loss': 0.9263885128498077}
2025-01-15 11:02:51,672 [INFO] Step[2600/2713]: training loss : 0.9265587162971497 TRAIN  loss dict:  {'classification_loss': 0.9265587162971497}
2025-01-15 11:03:04,999 [INFO] Step[2650/2713]: training loss : 0.9264176034927368 TRAIN  loss dict:  {'classification_loss': 0.9264176034927368}
2025-01-15 11:03:18,993 [INFO] Step[2700/2713]: training loss : 0.9262347054481507 TRAIN  loss dict:  {'classification_loss': 0.9262347054481507}
2025-01-15 11:04:35,140 [INFO] Label accuracies statistics:
2025-01-15 11:04:35,140 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 11:04:35,142 [INFO] [103] TRAIN  loss: 0.926565435993421 acc: 1.0
2025-01-15 11:04:35,142 [INFO] [103] TRAIN  loss dict: {'classification_loss': 0.926565435993421}
2025-01-15 11:04:35,142 [INFO] [103] VALIDATION loss: 1.7553110008401083 VALIDATION acc: 0.8213166144200627
2025-01-15 11:04:35,142 [INFO] [103] VALIDATION loss dict: {'classification_loss': 1.7553110008401083}
2025-01-15 11:04:35,142 [INFO] 
2025-01-15 11:04:57,778 [INFO] Step[50/2713]: training loss : 0.92673348903656 TRAIN  loss dict:  {'classification_loss': 0.92673348903656}
2025-01-15 11:05:11,310 [INFO] Step[100/2713]: training loss : 0.9270586442947387 TRAIN  loss dict:  {'classification_loss': 0.9270586442947387}
2025-01-15 11:05:24,538 [INFO] Step[150/2713]: training loss : 0.9270939958095551 TRAIN  loss dict:  {'classification_loss': 0.9270939958095551}
2025-01-15 11:05:38,285 [INFO] Step[200/2713]: training loss : 0.9260732793807983 TRAIN  loss dict:  {'classification_loss': 0.9260732793807983}
2025-01-15 11:05:52,156 [INFO] Step[250/2713]: training loss : 0.9261694526672364 TRAIN  loss dict:  {'classification_loss': 0.9261694526672364}
2025-01-15 11:06:05,608 [INFO] Step[300/2713]: training loss : 0.9260328698158264 TRAIN  loss dict:  {'classification_loss': 0.9260328698158264}
2025-01-15 11:06:19,047 [INFO] Step[350/2713]: training loss : 0.9289469718933105 TRAIN  loss dict:  {'classification_loss': 0.9289469718933105}
2025-01-15 11:06:32,643 [INFO] Step[400/2713]: training loss : 0.9264210307598114 TRAIN  loss dict:  {'classification_loss': 0.9264210307598114}
2025-01-15 11:06:46,069 [INFO] Step[450/2713]: training loss : 0.9263194859027862 TRAIN  loss dict:  {'classification_loss': 0.9263194859027862}
2025-01-15 11:06:59,303 [INFO] Step[500/2713]: training loss : 0.9602101051807403 TRAIN  loss dict:  {'classification_loss': 0.9602101051807403}
2025-01-15 11:07:13,333 [INFO] Step[550/2713]: training loss : 0.9271136045455932 TRAIN  loss dict:  {'classification_loss': 0.9271136045455932}
2025-01-15 11:07:27,427 [INFO] Step[600/2713]: training loss : 0.9269919335842133 TRAIN  loss dict:  {'classification_loss': 0.9269919335842133}
2025-01-15 11:07:41,222 [INFO] Step[650/2713]: training loss : 0.9260387814044952 TRAIN  loss dict:  {'classification_loss': 0.9260387814044952}
2025-01-15 11:07:55,342 [INFO] Step[700/2713]: training loss : 0.9261544179916382 TRAIN  loss dict:  {'classification_loss': 0.9261544179916382}
2025-01-15 11:08:08,583 [INFO] Step[750/2713]: training loss : 0.9261123490333557 TRAIN  loss dict:  {'classification_loss': 0.9261123490333557}
2025-01-15 11:08:22,521 [INFO] Step[800/2713]: training loss : 0.9306427955627441 TRAIN  loss dict:  {'classification_loss': 0.9306427955627441}
2025-01-15 11:08:36,229 [INFO] Step[850/2713]: training loss : 0.9263783478736878 TRAIN  loss dict:  {'classification_loss': 0.9263783478736878}
2025-01-15 11:08:49,423 [INFO] Step[900/2713]: training loss : 0.9262540638446808 TRAIN  loss dict:  {'classification_loss': 0.9262540638446808}
2025-01-15 11:09:03,682 [INFO] Step[950/2713]: training loss : 0.9264838457107544 TRAIN  loss dict:  {'classification_loss': 0.9264838457107544}
2025-01-15 11:09:19,136 [INFO] Step[1000/2713]: training loss : 0.9262623846530914 TRAIN  loss dict:  {'classification_loss': 0.9262623846530914}
2025-01-15 11:09:32,680 [INFO] Step[1050/2713]: training loss : 0.9263251101970673 TRAIN  loss dict:  {'classification_loss': 0.9263251101970673}
2025-01-15 11:09:46,253 [INFO] Step[1100/2713]: training loss : 0.9264918315410614 TRAIN  loss dict:  {'classification_loss': 0.9264918315410614}
2025-01-15 11:09:59,899 [INFO] Step[1150/2713]: training loss : 0.9266444933414459 TRAIN  loss dict:  {'classification_loss': 0.9266444933414459}
2025-01-15 11:10:13,403 [INFO] Step[1200/2713]: training loss : 0.9300227069854736 TRAIN  loss dict:  {'classification_loss': 0.9300227069854736}
2025-01-15 11:10:27,052 [INFO] Step[1250/2713]: training loss : 0.9266921710968018 TRAIN  loss dict:  {'classification_loss': 0.9266921710968018}
2025-01-15 11:10:40,685 [INFO] Step[1300/2713]: training loss : 0.9273572623729706 TRAIN  loss dict:  {'classification_loss': 0.9273572623729706}
2025-01-15 11:10:54,632 [INFO] Step[1350/2713]: training loss : 0.9263427114486694 TRAIN  loss dict:  {'classification_loss': 0.9263427114486694}
2025-01-15 11:11:08,902 [INFO] Step[1400/2713]: training loss : 0.9267810428142548 TRAIN  loss dict:  {'classification_loss': 0.9267810428142548}
2025-01-15 11:11:22,720 [INFO] Step[1450/2713]: training loss : 0.9262451875209808 TRAIN  loss dict:  {'classification_loss': 0.9262451875209808}
2025-01-15 11:11:36,499 [INFO] Step[1500/2713]: training loss : 0.9270121049880982 TRAIN  loss dict:  {'classification_loss': 0.9270121049880982}
2025-01-15 11:11:50,095 [INFO] Step[1550/2713]: training loss : 0.9266679573059082 TRAIN  loss dict:  {'classification_loss': 0.9266679573059082}
2025-01-15 11:12:04,001 [INFO] Step[1600/2713]: training loss : 0.9266573464870453 TRAIN  loss dict:  {'classification_loss': 0.9266573464870453}
2025-01-15 11:12:17,298 [INFO] Step[1650/2713]: training loss : 0.9265233671665192 TRAIN  loss dict:  {'classification_loss': 0.9265233671665192}
2025-01-15 11:12:30,802 [INFO] Step[1700/2713]: training loss : 0.9264305758476258 TRAIN  loss dict:  {'classification_loss': 0.9264305758476258}
2025-01-15 11:12:44,518 [INFO] Step[1750/2713]: training loss : 0.9267515850067138 TRAIN  loss dict:  {'classification_loss': 0.9267515850067138}
2025-01-15 11:12:57,942 [INFO] Step[1800/2713]: training loss : 0.9270836710929871 TRAIN  loss dict:  {'classification_loss': 0.9270836710929871}
2025-01-15 11:13:11,808 [INFO] Step[1850/2713]: training loss : 0.9262765681743622 TRAIN  loss dict:  {'classification_loss': 0.9262765681743622}
2025-01-15 11:13:26,025 [INFO] Step[1900/2713]: training loss : 0.9264952456951141 TRAIN  loss dict:  {'classification_loss': 0.9264952456951141}
2025-01-15 11:13:40,039 [INFO] Step[1950/2713]: training loss : 0.9266874527931214 TRAIN  loss dict:  {'classification_loss': 0.9266874527931214}
2025-01-15 11:13:53,280 [INFO] Step[2000/2713]: training loss : 0.9263334226608276 TRAIN  loss dict:  {'classification_loss': 0.9263334226608276}
2025-01-15 11:14:06,891 [INFO] Step[2050/2713]: training loss : 0.9388539576530457 TRAIN  loss dict:  {'classification_loss': 0.9388539576530457}
2025-01-15 11:14:21,078 [INFO] Step[2100/2713]: training loss : 0.9260282802581787 TRAIN  loss dict:  {'classification_loss': 0.9260282802581787}
2025-01-15 11:14:34,729 [INFO] Step[2150/2713]: training loss : 0.9266667699813843 TRAIN  loss dict:  {'classification_loss': 0.9266667699813843}
2025-01-15 11:14:48,142 [INFO] Step[2200/2713]: training loss : 0.9265214788913727 TRAIN  loss dict:  {'classification_loss': 0.9265214788913727}
2025-01-15 11:15:01,971 [INFO] Step[2250/2713]: training loss : 0.9260306000709534 TRAIN  loss dict:  {'classification_loss': 0.9260306000709534}
2025-01-15 11:15:16,235 [INFO] Step[2300/2713]: training loss : 0.9263062572479248 TRAIN  loss dict:  {'classification_loss': 0.9263062572479248}
2025-01-15 11:15:30,258 [INFO] Step[2350/2713]: training loss : 0.9267845499515533 TRAIN  loss dict:  {'classification_loss': 0.9267845499515533}
2025-01-15 11:15:44,320 [INFO] Step[2400/2713]: training loss : 0.9268932974338532 TRAIN  loss dict:  {'classification_loss': 0.9268932974338532}
2025-01-15 11:15:57,576 [INFO] Step[2450/2713]: training loss : 0.9256869184970856 TRAIN  loss dict:  {'classification_loss': 0.9256869184970856}
2025-01-15 11:16:10,829 [INFO] Step[2500/2713]: training loss : 0.9265290808677673 TRAIN  loss dict:  {'classification_loss': 0.9265290808677673}
2025-01-15 11:16:24,111 [INFO] Step[2550/2713]: training loss : 0.9266718208789826 TRAIN  loss dict:  {'classification_loss': 0.9266718208789826}
2025-01-15 11:16:37,670 [INFO] Step[2600/2713]: training loss : 0.9263679516315461 TRAIN  loss dict:  {'classification_loss': 0.9263679516315461}
2025-01-15 11:16:50,877 [INFO] Step[2650/2713]: training loss : 0.9269549202919006 TRAIN  loss dict:  {'classification_loss': 0.9269549202919006}
2025-01-15 11:17:04,850 [INFO] Step[2700/2713]: training loss : 0.9306217908859253 TRAIN  loss dict:  {'classification_loss': 0.9306217908859253}
2025-01-15 11:18:21,149 [INFO] Label accuracies statistics:
2025-01-15 11:18:21,149 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.25, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.5, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 11:18:21,150 [INFO] [104] TRAIN  loss: 0.9276279570473721 acc: 0.9997542695662858
2025-01-15 11:18:21,150 [INFO] [104] TRAIN  loss dict: {'classification_loss': 0.9276279570473721}
2025-01-15 11:18:21,150 [INFO] [104] VALIDATION loss: 1.769033958589224 VALIDATION acc: 0.8163009404388715
2025-01-15 11:18:21,151 [INFO] [104] VALIDATION loss dict: {'classification_loss': 1.769033958589224}
2025-01-15 11:18:21,151 [INFO] 
2025-01-15 11:18:39,522 [INFO] Step[50/2713]: training loss : 0.9263459837436676 TRAIN  loss dict:  {'classification_loss': 0.9263459837436676}
2025-01-15 11:18:52,785 [INFO] Step[100/2713]: training loss : 0.9262976932525635 TRAIN  loss dict:  {'classification_loss': 0.9262976932525635}
2025-01-15 11:19:06,347 [INFO] Step[150/2713]: training loss : 0.9284270620346069 TRAIN  loss dict:  {'classification_loss': 0.9284270620346069}
2025-01-15 11:19:19,563 [INFO] Step[200/2713]: training loss : 0.9270707249641419 TRAIN  loss dict:  {'classification_loss': 0.9270707249641419}
2025-01-15 11:19:33,010 [INFO] Step[250/2713]: training loss : 0.9262967586517334 TRAIN  loss dict:  {'classification_loss': 0.9262967586517334}
2025-01-15 11:19:46,500 [INFO] Step[300/2713]: training loss : 0.9265197384357452 TRAIN  loss dict:  {'classification_loss': 0.9265197384357452}
2025-01-15 11:19:59,722 [INFO] Step[350/2713]: training loss : 0.9261702990531921 TRAIN  loss dict:  {'classification_loss': 0.9261702990531921}
2025-01-15 11:20:13,604 [INFO] Step[400/2713]: training loss : 0.9263859605789184 TRAIN  loss dict:  {'classification_loss': 0.9263859605789184}
2025-01-15 11:20:26,915 [INFO] Step[450/2713]: training loss : 0.926303790807724 TRAIN  loss dict:  {'classification_loss': 0.926303790807724}
2025-01-15 11:20:41,004 [INFO] Step[500/2713]: training loss : 0.9268976938724518 TRAIN  loss dict:  {'classification_loss': 0.9268976938724518}
2025-01-15 11:20:54,215 [INFO] Step[550/2713]: training loss : 0.9262774050235748 TRAIN  loss dict:  {'classification_loss': 0.9262774050235748}
2025-01-15 11:21:07,773 [INFO] Step[600/2713]: training loss : 0.9262123692035675 TRAIN  loss dict:  {'classification_loss': 0.9262123692035675}
2025-01-15 11:21:21,673 [INFO] Step[650/2713]: training loss : 0.9266296565532685 TRAIN  loss dict:  {'classification_loss': 0.9266296565532685}
2025-01-15 11:21:35,037 [INFO] Step[700/2713]: training loss : 0.9263228523731232 TRAIN  loss dict:  {'classification_loss': 0.9263228523731232}
2025-01-15 11:21:48,972 [INFO] Step[750/2713]: training loss : 0.9263578963279724 TRAIN  loss dict:  {'classification_loss': 0.9263578963279724}
2025-01-15 11:22:02,578 [INFO] Step[800/2713]: training loss : 0.9258767485618591 TRAIN  loss dict:  {'classification_loss': 0.9258767485618591}
2025-01-15 11:22:16,093 [INFO] Step[850/2713]: training loss : 0.9264171266555786 TRAIN  loss dict:  {'classification_loss': 0.9264171266555786}
2025-01-15 11:22:29,606 [INFO] Step[900/2713]: training loss : 0.9266014933586121 TRAIN  loss dict:  {'classification_loss': 0.9266014933586121}
2025-01-15 11:22:43,271 [INFO] Step[950/2713]: training loss : 0.9266137158870698 TRAIN  loss dict:  {'classification_loss': 0.9266137158870698}
2025-01-15 11:22:56,932 [INFO] Step[1000/2713]: training loss : 0.9258496415615082 TRAIN  loss dict:  {'classification_loss': 0.9258496415615082}
2025-01-15 11:23:10,584 [INFO] Step[1050/2713]: training loss : 0.9271756863594055 TRAIN  loss dict:  {'classification_loss': 0.9271756863594055}
2025-01-15 11:23:24,331 [INFO] Step[1100/2713]: training loss : 0.9263541400432587 TRAIN  loss dict:  {'classification_loss': 0.9263541400432587}
2025-01-15 11:23:37,806 [INFO] Step[1150/2713]: training loss : 0.9267935478687286 TRAIN  loss dict:  {'classification_loss': 0.9267935478687286}
2025-01-15 11:23:51,323 [INFO] Step[1200/2713]: training loss : 0.9264059793949128 TRAIN  loss dict:  {'classification_loss': 0.9264059793949128}
2025-01-15 11:24:04,543 [INFO] Step[1250/2713]: training loss : 0.92648264169693 TRAIN  loss dict:  {'classification_loss': 0.92648264169693}
2025-01-15 11:24:18,176 [INFO] Step[1300/2713]: training loss : 0.926795688867569 TRAIN  loss dict:  {'classification_loss': 0.926795688867569}
2025-01-15 11:24:31,408 [INFO] Step[1350/2713]: training loss : 0.9263866853713989 TRAIN  loss dict:  {'classification_loss': 0.9263866853713989}
2025-01-15 11:24:45,587 [INFO] Step[1400/2713]: training loss : 0.9264641261100769 TRAIN  loss dict:  {'classification_loss': 0.9264641261100769}
2025-01-15 11:24:59,452 [INFO] Step[1450/2713]: training loss : 0.9259546279907227 TRAIN  loss dict:  {'classification_loss': 0.9259546279907227}
2025-01-15 11:25:13,020 [INFO] Step[1500/2713]: training loss : 0.9266256654262542 TRAIN  loss dict:  {'classification_loss': 0.9266256654262542}
2025-01-15 11:25:26,460 [INFO] Step[1550/2713]: training loss : 0.9263799512386321 TRAIN  loss dict:  {'classification_loss': 0.9263799512386321}
2025-01-15 11:25:39,982 [INFO] Step[1600/2713]: training loss : 0.9258373069763184 TRAIN  loss dict:  {'classification_loss': 0.9258373069763184}
2025-01-15 11:25:53,260 [INFO] Step[1650/2713]: training loss : 0.9261154985427856 TRAIN  loss dict:  {'classification_loss': 0.9261154985427856}
2025-01-15 11:26:06,842 [INFO] Step[1700/2713]: training loss : 0.9266036486625672 TRAIN  loss dict:  {'classification_loss': 0.9266036486625672}
2025-01-15 11:26:20,805 [INFO] Step[1750/2713]: training loss : 0.925930677652359 TRAIN  loss dict:  {'classification_loss': 0.925930677652359}
2025-01-15 11:26:34,996 [INFO] Step[1800/2713]: training loss : 0.9269083046913147 TRAIN  loss dict:  {'classification_loss': 0.9269083046913147}
2025-01-15 11:26:49,015 [INFO] Step[1850/2713]: training loss : 0.9267629337310791 TRAIN  loss dict:  {'classification_loss': 0.9267629337310791}
2025-01-15 11:27:02,768 [INFO] Step[1900/2713]: training loss : 0.9260888516902923 TRAIN  loss dict:  {'classification_loss': 0.9260888516902923}
2025-01-15 11:27:16,030 [INFO] Step[1950/2713]: training loss : 0.9267386901378631 TRAIN  loss dict:  {'classification_loss': 0.9267386901378631}
2025-01-15 11:27:29,764 [INFO] Step[2000/2713]: training loss : 0.9263334190845489 TRAIN  loss dict:  {'classification_loss': 0.9263334190845489}
2025-01-15 11:27:43,768 [INFO] Step[2050/2713]: training loss : 0.9352206754684448 TRAIN  loss dict:  {'classification_loss': 0.9352206754684448}
2025-01-15 11:27:57,316 [INFO] Step[2100/2713]: training loss : 0.926334661245346 TRAIN  loss dict:  {'classification_loss': 0.926334661245346}
2025-01-15 11:28:10,948 [INFO] Step[2150/2713]: training loss : 0.9268460822105408 TRAIN  loss dict:  {'classification_loss': 0.9268460822105408}
2025-01-15 11:28:24,505 [INFO] Step[2200/2713]: training loss : 0.9260472846031189 TRAIN  loss dict:  {'classification_loss': 0.9260472846031189}
2025-01-15 11:28:38,444 [INFO] Step[2250/2713]: training loss : 0.9268337297439575 TRAIN  loss dict:  {'classification_loss': 0.9268337297439575}
2025-01-15 11:28:51,909 [INFO] Step[2300/2713]: training loss : 0.9261390149593354 TRAIN  loss dict:  {'classification_loss': 0.9261390149593354}
2025-01-15 11:29:05,706 [INFO] Step[2350/2713]: training loss : 0.9263496458530426 TRAIN  loss dict:  {'classification_loss': 0.9263496458530426}
2025-01-15 11:29:19,471 [INFO] Step[2400/2713]: training loss : 0.9260525703430176 TRAIN  loss dict:  {'classification_loss': 0.9260525703430176}
2025-01-15 11:29:32,697 [INFO] Step[2450/2713]: training loss : 0.9263744568824768 TRAIN  loss dict:  {'classification_loss': 0.9263744568824768}
2025-01-15 11:29:46,590 [INFO] Step[2500/2713]: training loss : 0.9311204993724823 TRAIN  loss dict:  {'classification_loss': 0.9311204993724823}
2025-01-15 11:30:00,296 [INFO] Step[2550/2713]: training loss : 0.9267782998085022 TRAIN  loss dict:  {'classification_loss': 0.9267782998085022}
2025-01-15 11:30:14,298 [INFO] Step[2600/2713]: training loss : 0.9266613960266114 TRAIN  loss dict:  {'classification_loss': 0.9266613960266114}
2025-01-15 11:30:27,556 [INFO] Step[2650/2713]: training loss : 0.9263078117370606 TRAIN  loss dict:  {'classification_loss': 0.9263078117370606}
2025-01-15 11:30:41,103 [INFO] Step[2700/2713]: training loss : 0.9268038988113403 TRAIN  loss dict:  {'classification_loss': 0.9268038988113403}
2025-01-15 11:31:57,912 [INFO] Label accuracies statistics:
2025-01-15 11:31:57,912 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 11:31:57,915 [INFO] [105] TRAIN  loss: 0.9267225652394856 acc: 0.9998771347831429
2025-01-15 11:31:57,915 [INFO] [105] TRAIN  loss dict: {'classification_loss': 0.9267225652394856}
2025-01-15 11:31:57,915 [INFO] [105] VALIDATION loss: 1.7690947763901903 VALIDATION acc: 0.8100313479623824
2025-01-15 11:31:57,915 [INFO] [105] VALIDATION loss dict: {'classification_loss': 1.7690947763901903}
2025-01-15 11:31:57,915 [INFO] 
2025-01-15 11:32:16,719 [INFO] Step[50/2713]: training loss : 0.9270428764820099 TRAIN  loss dict:  {'classification_loss': 0.9270428764820099}
2025-01-15 11:32:30,266 [INFO] Step[100/2713]: training loss : 0.9265740621089935 TRAIN  loss dict:  {'classification_loss': 0.9265740621089935}
2025-01-15 11:32:44,447 [INFO] Step[150/2713]: training loss : 0.9260005307197571 TRAIN  loss dict:  {'classification_loss': 0.9260005307197571}
2025-01-15 11:32:57,975 [INFO] Step[200/2713]: training loss : 0.9262569677829743 TRAIN  loss dict:  {'classification_loss': 0.9262569677829743}
2025-01-15 11:33:12,249 [INFO] Step[250/2713]: training loss : 0.9266673398017883 TRAIN  loss dict:  {'classification_loss': 0.9266673398017883}
2025-01-15 11:33:25,868 [INFO] Step[300/2713]: training loss : 0.9260049474239349 TRAIN  loss dict:  {'classification_loss': 0.9260049474239349}
2025-01-15 11:33:39,485 [INFO] Step[350/2713]: training loss : 0.9272618734836579 TRAIN  loss dict:  {'classification_loss': 0.9272618734836579}
2025-01-15 11:33:53,154 [INFO] Step[400/2713]: training loss : 0.9269367611408234 TRAIN  loss dict:  {'classification_loss': 0.9269367611408234}
2025-01-15 11:34:06,473 [INFO] Step[450/2713]: training loss : 0.9265306413173675 TRAIN  loss dict:  {'classification_loss': 0.9265306413173675}
2025-01-15 11:34:20,669 [INFO] Step[500/2713]: training loss : 0.9265502667427064 TRAIN  loss dict:  {'classification_loss': 0.9265502667427064}
2025-01-15 11:34:34,437 [INFO] Step[550/2713]: training loss : 0.9272181737422943 TRAIN  loss dict:  {'classification_loss': 0.9272181737422943}
2025-01-15 11:34:48,337 [INFO] Step[600/2713]: training loss : 0.9260240793228149 TRAIN  loss dict:  {'classification_loss': 0.9260240793228149}
2025-01-15 11:35:02,167 [INFO] Step[650/2713]: training loss : 0.9366635060310364 TRAIN  loss dict:  {'classification_loss': 0.9366635060310364}
2025-01-15 11:35:16,073 [INFO] Step[700/2713]: training loss : 0.9264098870754242 TRAIN  loss dict:  {'classification_loss': 0.9264098870754242}
2025-01-15 11:35:29,619 [INFO] Step[750/2713]: training loss : 0.9414473438262939 TRAIN  loss dict:  {'classification_loss': 0.9414473438262939}
2025-01-15 11:35:43,633 [INFO] Step[800/2713]: training loss : 0.9258886873722076 TRAIN  loss dict:  {'classification_loss': 0.9258886873722076}
2025-01-15 11:35:57,890 [INFO] Step[850/2713]: training loss : 0.9266296076774597 TRAIN  loss dict:  {'classification_loss': 0.9266296076774597}
2025-01-15 11:36:11,471 [INFO] Step[900/2713]: training loss : 0.9273086404800415 TRAIN  loss dict:  {'classification_loss': 0.9273086404800415}
2025-01-15 11:36:25,153 [INFO] Step[950/2713]: training loss : 0.9272291576862335 TRAIN  loss dict:  {'classification_loss': 0.9272291576862335}
2025-01-15 11:36:39,138 [INFO] Step[1000/2713]: training loss : 0.9263701283931732 TRAIN  loss dict:  {'classification_loss': 0.9263701283931732}
2025-01-15 11:36:52,627 [INFO] Step[1050/2713]: training loss : 0.9264507102966308 TRAIN  loss dict:  {'classification_loss': 0.9264507102966308}
2025-01-15 11:37:06,091 [INFO] Step[1100/2713]: training loss : 0.9264581906795502 TRAIN  loss dict:  {'classification_loss': 0.9264581906795502}
2025-01-15 11:37:19,689 [INFO] Step[1150/2713]: training loss : 0.9265074241161346 TRAIN  loss dict:  {'classification_loss': 0.9265074241161346}
2025-01-15 11:37:33,456 [INFO] Step[1200/2713]: training loss : 0.9266211712360382 TRAIN  loss dict:  {'classification_loss': 0.9266211712360382}
2025-01-15 11:37:47,732 [INFO] Step[1250/2713]: training loss : 0.92590167760849 TRAIN  loss dict:  {'classification_loss': 0.92590167760849}
2025-01-15 11:38:01,155 [INFO] Step[1300/2713]: training loss : 0.926635525226593 TRAIN  loss dict:  {'classification_loss': 0.926635525226593}
2025-01-15 11:38:15,269 [INFO] Step[1350/2713]: training loss : 0.9262474358081818 TRAIN  loss dict:  {'classification_loss': 0.9262474358081818}
2025-01-15 11:38:28,764 [INFO] Step[1400/2713]: training loss : 0.9267832636833191 TRAIN  loss dict:  {'classification_loss': 0.9267832636833191}
2025-01-15 11:38:42,588 [INFO] Step[1450/2713]: training loss : 0.9262622165679931 TRAIN  loss dict:  {'classification_loss': 0.9262622165679931}
2025-01-15 11:38:55,898 [INFO] Step[1500/2713]: training loss : 0.9263657891750335 TRAIN  loss dict:  {'classification_loss': 0.9263657891750335}
2025-01-15 11:39:09,821 [INFO] Step[1550/2713]: training loss : 0.9265022706985474 TRAIN  loss dict:  {'classification_loss': 0.9265022706985474}
2025-01-15 11:39:24,077 [INFO] Step[1600/2713]: training loss : 0.9266595673561097 TRAIN  loss dict:  {'classification_loss': 0.9266595673561097}
2025-01-15 11:39:37,493 [INFO] Step[1650/2713]: training loss : 0.9265526556968688 TRAIN  loss dict:  {'classification_loss': 0.9265526556968688}
2025-01-15 11:39:50,747 [INFO] Step[1700/2713]: training loss : 0.9268532192707062 TRAIN  loss dict:  {'classification_loss': 0.9268532192707062}
2025-01-15 11:40:04,059 [INFO] Step[1750/2713]: training loss : 0.926256867647171 TRAIN  loss dict:  {'classification_loss': 0.926256867647171}
2025-01-15 11:40:17,575 [INFO] Step[1800/2713]: training loss : 0.9266734313964844 TRAIN  loss dict:  {'classification_loss': 0.9266734313964844}
2025-01-15 11:40:31,580 [INFO] Step[1850/2713]: training loss : 0.9263870429992676 TRAIN  loss dict:  {'classification_loss': 0.9263870429992676}
2025-01-15 11:40:45,447 [INFO] Step[1900/2713]: training loss : 0.926305799484253 TRAIN  loss dict:  {'classification_loss': 0.926305799484253}
2025-01-15 11:40:59,305 [INFO] Step[1950/2713]: training loss : 0.9261418855190278 TRAIN  loss dict:  {'classification_loss': 0.9261418855190278}
2025-01-15 11:41:13,193 [INFO] Step[2000/2713]: training loss : 0.9264174818992614 TRAIN  loss dict:  {'classification_loss': 0.9264174818992614}
2025-01-15 11:41:26,501 [INFO] Step[2050/2713]: training loss : 0.9264883649349213 TRAIN  loss dict:  {'classification_loss': 0.9264883649349213}
2025-01-15 11:41:40,063 [INFO] Step[2100/2713]: training loss : 0.9260777378082276 TRAIN  loss dict:  {'classification_loss': 0.9260777378082276}
2025-01-15 11:41:53,667 [INFO] Step[2150/2713]: training loss : 0.9261911201477051 TRAIN  loss dict:  {'classification_loss': 0.9261911201477051}
2025-01-15 11:42:06,928 [INFO] Step[2200/2713]: training loss : 0.9265424931049346 TRAIN  loss dict:  {'classification_loss': 0.9265424931049346}
2025-01-15 11:42:20,786 [INFO] Step[2250/2713]: training loss : 0.927198873758316 TRAIN  loss dict:  {'classification_loss': 0.927198873758316}
2025-01-15 11:42:34,471 [INFO] Step[2300/2713]: training loss : 0.9261192882061005 TRAIN  loss dict:  {'classification_loss': 0.9261192882061005}
2025-01-15 11:42:48,503 [INFO] Step[2350/2713]: training loss : 0.9262103152275085 TRAIN  loss dict:  {'classification_loss': 0.9262103152275085}
2025-01-15 11:43:02,073 [INFO] Step[2400/2713]: training loss : 0.9268310356140137 TRAIN  loss dict:  {'classification_loss': 0.9268310356140137}
2025-01-15 11:43:15,784 [INFO] Step[2450/2713]: training loss : 0.9266987073421479 TRAIN  loss dict:  {'classification_loss': 0.9266987073421479}
2025-01-15 11:43:29,534 [INFO] Step[2500/2713]: training loss : 0.9264158940315247 TRAIN  loss dict:  {'classification_loss': 0.9264158940315247}
2025-01-15 11:43:43,190 [INFO] Step[2550/2713]: training loss : 0.9262346351146697 TRAIN  loss dict:  {'classification_loss': 0.9262346351146697}
2025-01-15 11:43:56,756 [INFO] Step[2600/2713]: training loss : 0.9260826396942139 TRAIN  loss dict:  {'classification_loss': 0.9260826396942139}
2025-01-15 11:44:10,624 [INFO] Step[2650/2713]: training loss : 0.9534113180637359 TRAIN  loss dict:  {'classification_loss': 0.9534113180637359}
2025-01-15 11:44:23,884 [INFO] Step[2700/2713]: training loss : 0.9265656447410584 TRAIN  loss dict:  {'classification_loss': 0.9265656447410584}
2025-01-15 11:45:40,303 [INFO] Label accuracies statistics:
2025-01-15 11:45:40,303 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 1.0, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 11:45:40,305 [INFO] [106] TRAIN  loss: 0.9274650539720889 acc: 0.9996314043494287
2025-01-15 11:45:40,305 [INFO] [106] TRAIN  loss dict: {'classification_loss': 0.9274650539720889}
2025-01-15 11:45:40,305 [INFO] [106] VALIDATION loss: 1.7556660072247785 VALIDATION acc: 0.819435736677116
2025-01-15 11:45:40,305 [INFO] [106] VALIDATION loss dict: {'classification_loss': 1.7556660072247785}
2025-01-15 11:45:40,305 [INFO] 
2025-01-15 11:45:59,439 [INFO] Step[50/2713]: training loss : 0.9264131927490235 TRAIN  loss dict:  {'classification_loss': 0.9264131927490235}
2025-01-15 11:46:13,370 [INFO] Step[100/2713]: training loss : 0.926688414812088 TRAIN  loss dict:  {'classification_loss': 0.926688414812088}
2025-01-15 11:46:27,367 [INFO] Step[150/2713]: training loss : 0.9261879098415374 TRAIN  loss dict:  {'classification_loss': 0.9261879098415374}
2025-01-15 11:46:40,902 [INFO] Step[200/2713]: training loss : 0.9259239602088928 TRAIN  loss dict:  {'classification_loss': 0.9259239602088928}
2025-01-15 11:46:55,164 [INFO] Step[250/2713]: training loss : 0.9261124920845032 TRAIN  loss dict:  {'classification_loss': 0.9261124920845032}
2025-01-15 11:47:08,984 [INFO] Step[300/2713]: training loss : 0.926399701833725 TRAIN  loss dict:  {'classification_loss': 0.926399701833725}
2025-01-15 11:47:22,222 [INFO] Step[350/2713]: training loss : 0.9273969900608062 TRAIN  loss dict:  {'classification_loss': 0.9273969900608062}
2025-01-15 11:47:35,710 [INFO] Step[400/2713]: training loss : 0.9263698506355286 TRAIN  loss dict:  {'classification_loss': 0.9263698506355286}
2025-01-15 11:47:49,961 [INFO] Step[450/2713]: training loss : 0.9260667836666108 TRAIN  loss dict:  {'classification_loss': 0.9260667836666108}
2025-01-15 11:48:03,890 [INFO] Step[500/2713]: training loss : 0.9259856534004212 TRAIN  loss dict:  {'classification_loss': 0.9259856534004212}
2025-01-15 11:48:17,674 [INFO] Step[550/2713]: training loss : 0.9264086854457855 TRAIN  loss dict:  {'classification_loss': 0.9264086854457855}
2025-01-15 11:48:31,645 [INFO] Step[600/2713]: training loss : 0.9265595030784607 TRAIN  loss dict:  {'classification_loss': 0.9265595030784607}
2025-01-15 11:48:45,525 [INFO] Step[650/2713]: training loss : 0.9273084628582001 TRAIN  loss dict:  {'classification_loss': 0.9273084628582001}
2025-01-15 11:48:59,208 [INFO] Step[700/2713]: training loss : 0.9260578036308289 TRAIN  loss dict:  {'classification_loss': 0.9260578036308289}
2025-01-15 11:49:13,241 [INFO] Step[750/2713]: training loss : 0.926638959646225 TRAIN  loss dict:  {'classification_loss': 0.926638959646225}
2025-01-15 11:49:27,533 [INFO] Step[800/2713]: training loss : 0.9259948897361755 TRAIN  loss dict:  {'classification_loss': 0.9259948897361755}
2025-01-15 11:49:41,818 [INFO] Step[850/2713]: training loss : 0.9266958224773407 TRAIN  loss dict:  {'classification_loss': 0.9266958224773407}
2025-01-15 11:49:55,930 [INFO] Step[900/2713]: training loss : 0.9263127088546753 TRAIN  loss dict:  {'classification_loss': 0.9263127088546753}
2025-01-15 11:50:09,293 [INFO] Step[950/2713]: training loss : 0.9415162897109985 TRAIN  loss dict:  {'classification_loss': 0.9415162897109985}
2025-01-15 11:50:23,557 [INFO] Step[1000/2713]: training loss : 0.926497859954834 TRAIN  loss dict:  {'classification_loss': 0.926497859954834}
2025-01-15 11:50:37,106 [INFO] Step[1050/2713]: training loss : 0.9261636698246002 TRAIN  loss dict:  {'classification_loss': 0.9261636698246002}
2025-01-15 11:50:51,189 [INFO] Step[1100/2713]: training loss : 0.9261843574047088 TRAIN  loss dict:  {'classification_loss': 0.9261843574047088}
2025-01-15 11:51:04,876 [INFO] Step[1150/2713]: training loss : 0.926156325340271 TRAIN  loss dict:  {'classification_loss': 0.926156325340271}
2025-01-15 11:51:18,562 [INFO] Step[1200/2713]: training loss : 0.9264022445678711 TRAIN  loss dict:  {'classification_loss': 0.9264022445678711}
2025-01-15 11:51:32,131 [INFO] Step[1250/2713]: training loss : 0.926913456916809 TRAIN  loss dict:  {'classification_loss': 0.926913456916809}
2025-01-15 11:51:46,256 [INFO] Step[1300/2713]: training loss : 0.9272226941585541 TRAIN  loss dict:  {'classification_loss': 0.9272226941585541}
2025-01-15 11:52:00,170 [INFO] Step[1350/2713]: training loss : 0.9259950613975525 TRAIN  loss dict:  {'classification_loss': 0.9259950613975525}
2025-01-15 11:52:14,285 [INFO] Step[1400/2713]: training loss : 0.9263945889472961 TRAIN  loss dict:  {'classification_loss': 0.9263945889472961}
2025-01-15 11:52:27,908 [INFO] Step[1450/2713]: training loss : 0.9262020194530487 TRAIN  loss dict:  {'classification_loss': 0.9262020194530487}
2025-01-15 11:52:41,805 [INFO] Step[1500/2713]: training loss : 0.9266158127784729 TRAIN  loss dict:  {'classification_loss': 0.9266158127784729}
2025-01-15 11:52:55,925 [INFO] Step[1550/2713]: training loss : 0.9268345475196839 TRAIN  loss dict:  {'classification_loss': 0.9268345475196839}
2025-01-15 11:53:09,817 [INFO] Step[1600/2713]: training loss : 0.9262106037139892 TRAIN  loss dict:  {'classification_loss': 0.9262106037139892}
2025-01-15 11:53:23,294 [INFO] Step[1650/2713]: training loss : 0.9261190938949585 TRAIN  loss dict:  {'classification_loss': 0.9261190938949585}
2025-01-15 11:53:37,093 [INFO] Step[1700/2713]: training loss : 0.926113430261612 TRAIN  loss dict:  {'classification_loss': 0.926113430261612}
2025-01-15 11:53:51,207 [INFO] Step[1750/2713]: training loss : 0.9267196869850158 TRAIN  loss dict:  {'classification_loss': 0.9267196869850158}
2025-01-15 11:54:05,075 [INFO] Step[1800/2713]: training loss : 0.9265489530563354 TRAIN  loss dict:  {'classification_loss': 0.9265489530563354}
2025-01-15 11:54:18,652 [INFO] Step[1850/2713]: training loss : 0.9266379630565643 TRAIN  loss dict:  {'classification_loss': 0.9266379630565643}
2025-01-15 11:54:31,972 [INFO] Step[1900/2713]: training loss : 0.9260976541042328 TRAIN  loss dict:  {'classification_loss': 0.9260976541042328}
2025-01-15 11:54:46,010 [INFO] Step[1950/2713]: training loss : 0.9265285444259643 TRAIN  loss dict:  {'classification_loss': 0.9265285444259643}
2025-01-15 11:54:59,528 [INFO] Step[2000/2713]: training loss : 0.9277092838287353 TRAIN  loss dict:  {'classification_loss': 0.9277092838287353}
2025-01-15 11:55:13,554 [INFO] Step[2050/2713]: training loss : 0.927920355796814 TRAIN  loss dict:  {'classification_loss': 0.927920355796814}
2025-01-15 11:55:27,869 [INFO] Step[2100/2713]: training loss : 0.9263958954811096 TRAIN  loss dict:  {'classification_loss': 0.9263958954811096}
2025-01-15 11:55:41,176 [INFO] Step[2150/2713]: training loss : 0.9263838768005371 TRAIN  loss dict:  {'classification_loss': 0.9263838768005371}
2025-01-15 11:55:54,973 [INFO] Step[2200/2713]: training loss : 0.926211519241333 TRAIN  loss dict:  {'classification_loss': 0.926211519241333}
2025-01-15 11:56:08,327 [INFO] Step[2250/2713]: training loss : 0.9263931548595429 TRAIN  loss dict:  {'classification_loss': 0.9263931548595429}
2025-01-15 11:56:21,990 [INFO] Step[2300/2713]: training loss : 0.9258109676837921 TRAIN  loss dict:  {'classification_loss': 0.9258109676837921}
2025-01-15 11:56:35,840 [INFO] Step[2350/2713]: training loss : 0.9263445401191711 TRAIN  loss dict:  {'classification_loss': 0.9263445401191711}
2025-01-15 11:56:49,442 [INFO] Step[2400/2713]: training loss : 0.9263501656055451 TRAIN  loss dict:  {'classification_loss': 0.9263501656055451}
2025-01-15 11:57:03,344 [INFO] Step[2450/2713]: training loss : 0.9266422522068024 TRAIN  loss dict:  {'classification_loss': 0.9266422522068024}
2025-01-15 11:57:16,687 [INFO] Step[2500/2713]: training loss : 0.9258884036540985 TRAIN  loss dict:  {'classification_loss': 0.9258884036540985}
2025-01-15 11:57:30,386 [INFO] Step[2550/2713]: training loss : 0.9261132156848908 TRAIN  loss dict:  {'classification_loss': 0.9261132156848908}
2025-01-15 11:57:44,307 [INFO] Step[2600/2713]: training loss : 0.9268638050556183 TRAIN  loss dict:  {'classification_loss': 0.9268638050556183}
2025-01-15 11:57:58,095 [INFO] Step[2650/2713]: training loss : 0.9261773872375488 TRAIN  loss dict:  {'classification_loss': 0.9261773872375488}
2025-01-15 11:58:12,045 [INFO] Step[2700/2713]: training loss : 0.9281031596660614 TRAIN  loss dict:  {'classification_loss': 0.9281031596660614}
2025-01-15 11:59:28,359 [INFO] Label accuracies statistics:
2025-01-15 11:59:28,359 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 11:59:28,361 [INFO] [107] TRAIN  loss: 0.9267535903029366 acc: 0.9998771347831429
2025-01-15 11:59:28,361 [INFO] [107] TRAIN  loss dict: {'classification_loss': 0.9267535903029366}
2025-01-15 11:59:28,361 [INFO] [107] VALIDATION loss: 1.7854850646248437 VALIDATION acc: 0.8144200626959248
2025-01-15 11:59:28,361 [INFO] [107] VALIDATION loss dict: {'classification_loss': 1.7854850646248437}
2025-01-15 11:59:28,362 [INFO] 
2025-01-15 11:59:47,144 [INFO] Step[50/2713]: training loss : 0.9263392841815948 TRAIN  loss dict:  {'classification_loss': 0.9263392841815948}
2025-01-15 12:00:01,795 [INFO] Step[100/2713]: training loss : 0.9258540749549866 TRAIN  loss dict:  {'classification_loss': 0.9258540749549866}
2025-01-15 12:00:18,833 [INFO] Step[150/2713]: training loss : 0.9265150260925293 TRAIN  loss dict:  {'classification_loss': 0.9265150260925293}
2025-01-15 12:00:32,977 [INFO] Step[200/2713]: training loss : 0.9260196018218995 TRAIN  loss dict:  {'classification_loss': 0.9260196018218995}
2025-01-15 12:00:46,941 [INFO] Step[250/2713]: training loss : 0.926084679365158 TRAIN  loss dict:  {'classification_loss': 0.926084679365158}
2025-01-15 12:01:00,121 [INFO] Step[300/2713]: training loss : 0.9264255392551423 TRAIN  loss dict:  {'classification_loss': 0.9264255392551423}
2025-01-15 12:01:13,655 [INFO] Step[350/2713]: training loss : 0.9263034617900848 TRAIN  loss dict:  {'classification_loss': 0.9263034617900848}
2025-01-15 12:01:27,176 [INFO] Step[400/2713]: training loss : 0.9268643689155579 TRAIN  loss dict:  {'classification_loss': 0.9268643689155579}
2025-01-15 12:01:40,874 [INFO] Step[450/2713]: training loss : 0.9267453062534332 TRAIN  loss dict:  {'classification_loss': 0.9267453062534332}
2025-01-15 12:01:54,617 [INFO] Step[500/2713]: training loss : 0.9258400452136993 TRAIN  loss dict:  {'classification_loss': 0.9258400452136993}
2025-01-15 12:02:08,031 [INFO] Step[550/2713]: training loss : 0.9267196893692017 TRAIN  loss dict:  {'classification_loss': 0.9267196893692017}
2025-01-15 12:02:21,543 [INFO] Step[600/2713]: training loss : 0.9265419816970826 TRAIN  loss dict:  {'classification_loss': 0.9265419816970826}
2025-01-15 12:02:35,495 [INFO] Step[650/2713]: training loss : 0.9266336131095886 TRAIN  loss dict:  {'classification_loss': 0.9266336131095886}
2025-01-15 12:02:49,390 [INFO] Step[700/2713]: training loss : 0.9273093450069427 TRAIN  loss dict:  {'classification_loss': 0.9273093450069427}
2025-01-15 12:03:03,211 [INFO] Step[750/2713]: training loss : 0.927042979001999 TRAIN  loss dict:  {'classification_loss': 0.927042979001999}
2025-01-15 12:03:16,444 [INFO] Step[800/2713]: training loss : 0.926167254447937 TRAIN  loss dict:  {'classification_loss': 0.926167254447937}
2025-01-15 12:03:30,168 [INFO] Step[850/2713]: training loss : 0.9276358163356782 TRAIN  loss dict:  {'classification_loss': 0.9276358163356782}
2025-01-15 12:03:44,214 [INFO] Step[900/2713]: training loss : 0.9270464015007019 TRAIN  loss dict:  {'classification_loss': 0.9270464015007019}
2025-01-15 12:03:57,877 [INFO] Step[950/2713]: training loss : 0.9269686222076416 TRAIN  loss dict:  {'classification_loss': 0.9269686222076416}
2025-01-15 12:04:11,777 [INFO] Step[1000/2713]: training loss : 0.9262877893447876 TRAIN  loss dict:  {'classification_loss': 0.9262877893447876}
2025-01-15 12:04:27,273 [INFO] Step[1050/2713]: training loss : 0.9260432922840118 TRAIN  loss dict:  {'classification_loss': 0.9260432922840118}
2025-01-15 12:04:43,797 [INFO] Step[1100/2713]: training loss : 0.9261928308010101 TRAIN  loss dict:  {'classification_loss': 0.9261928308010101}
2025-01-15 12:04:57,632 [INFO] Step[1150/2713]: training loss : 0.9263184368610382 TRAIN  loss dict:  {'classification_loss': 0.9263184368610382}
2025-01-15 12:05:11,505 [INFO] Step[1200/2713]: training loss : 0.9260705721378326 TRAIN  loss dict:  {'classification_loss': 0.9260705721378326}
2025-01-15 12:05:25,317 [INFO] Step[1250/2713]: training loss : 0.9261747360229492 TRAIN  loss dict:  {'classification_loss': 0.9261747360229492}
2025-01-15 12:05:39,493 [INFO] Step[1300/2713]: training loss : 0.927030086517334 TRAIN  loss dict:  {'classification_loss': 0.927030086517334}
2025-01-15 12:05:53,286 [INFO] Step[1350/2713]: training loss : 0.9264938127994538 TRAIN  loss dict:  {'classification_loss': 0.9264938127994538}
2025-01-15 12:06:07,431 [INFO] Step[1400/2713]: training loss : 0.9267013382911682 TRAIN  loss dict:  {'classification_loss': 0.9267013382911682}
2025-01-15 12:06:21,079 [INFO] Step[1450/2713]: training loss : 0.9267277538776397 TRAIN  loss dict:  {'classification_loss': 0.9267277538776397}
2025-01-15 12:06:34,798 [INFO] Step[1500/2713]: training loss : 0.9263087427616119 TRAIN  loss dict:  {'classification_loss': 0.9263087427616119}
2025-01-15 12:06:48,554 [INFO] Step[1550/2713]: training loss : 0.9265016901493073 TRAIN  loss dict:  {'classification_loss': 0.9265016901493073}
2025-01-15 12:07:02,196 [INFO] Step[1600/2713]: training loss : 0.9265016996860505 TRAIN  loss dict:  {'classification_loss': 0.9265016996860505}
2025-01-15 12:07:16,077 [INFO] Step[1650/2713]: training loss : 0.9264470970630646 TRAIN  loss dict:  {'classification_loss': 0.9264470970630646}
2025-01-15 12:07:30,130 [INFO] Step[1700/2713]: training loss : 0.9266046011447906 TRAIN  loss dict:  {'classification_loss': 0.9266046011447906}
2025-01-15 12:07:44,231 [INFO] Step[1750/2713]: training loss : 0.926032326221466 TRAIN  loss dict:  {'classification_loss': 0.926032326221466}
2025-01-15 12:07:58,031 [INFO] Step[1800/2713]: training loss : 0.9260861027240753 TRAIN  loss dict:  {'classification_loss': 0.9260861027240753}
2025-01-15 12:08:11,646 [INFO] Step[1850/2713]: training loss : 0.9265165972709656 TRAIN  loss dict:  {'classification_loss': 0.9265165972709656}
2025-01-15 12:08:25,418 [INFO] Step[1900/2713]: training loss : 0.9266604614257813 TRAIN  loss dict:  {'classification_loss': 0.9266604614257813}
2025-01-15 12:08:38,641 [INFO] Step[1950/2713]: training loss : 0.9274602210521699 TRAIN  loss dict:  {'classification_loss': 0.9274602210521699}
2025-01-15 12:08:52,291 [INFO] Step[2000/2713]: training loss : 0.9260420203208923 TRAIN  loss dict:  {'classification_loss': 0.9260420203208923}
2025-01-15 12:09:05,753 [INFO] Step[2050/2713]: training loss : 0.9263823091983795 TRAIN  loss dict:  {'classification_loss': 0.9263823091983795}
2025-01-15 12:09:19,483 [INFO] Step[2100/2713]: training loss : 0.9269139003753663 TRAIN  loss dict:  {'classification_loss': 0.9269139003753663}
2025-01-15 12:09:33,272 [INFO] Step[2150/2713]: training loss : 0.9266462194919586 TRAIN  loss dict:  {'classification_loss': 0.9266462194919586}
2025-01-15 12:09:46,867 [INFO] Step[2200/2713]: training loss : 0.9263022315502166 TRAIN  loss dict:  {'classification_loss': 0.9263022315502166}
2025-01-15 12:10:00,978 [INFO] Step[2250/2713]: training loss : 0.926576030254364 TRAIN  loss dict:  {'classification_loss': 0.926576030254364}
2025-01-15 12:10:14,776 [INFO] Step[2300/2713]: training loss : 0.9265603983402252 TRAIN  loss dict:  {'classification_loss': 0.9265603983402252}
2025-01-15 12:10:27,956 [INFO] Step[2350/2713]: training loss : 0.926139280796051 TRAIN  loss dict:  {'classification_loss': 0.926139280796051}
2025-01-15 12:10:41,541 [INFO] Step[2400/2713]: training loss : 0.9268160343170166 TRAIN  loss dict:  {'classification_loss': 0.9268160343170166}
2025-01-15 12:10:54,767 [INFO] Step[2450/2713]: training loss : 0.9265238833427429 TRAIN  loss dict:  {'classification_loss': 0.9265238833427429}
2025-01-15 12:11:08,157 [INFO] Step[2500/2713]: training loss : 0.9264299154281617 TRAIN  loss dict:  {'classification_loss': 0.9264299154281617}
2025-01-15 12:11:21,903 [INFO] Step[2550/2713]: training loss : 0.9259539258480072 TRAIN  loss dict:  {'classification_loss': 0.9259539258480072}
2025-01-15 12:11:35,555 [INFO] Step[2600/2713]: training loss : 0.9261259949207306 TRAIN  loss dict:  {'classification_loss': 0.9261259949207306}
2025-01-15 12:11:49,126 [INFO] Step[2650/2713]: training loss : 0.9259805059432984 TRAIN  loss dict:  {'classification_loss': 0.9259805059432984}
2025-01-15 12:12:02,641 [INFO] Step[2700/2713]: training loss : 0.926196061372757 TRAIN  loss dict:  {'classification_loss': 0.926196061372757}
2025-01-15 12:13:21,293 [INFO] Label accuracies statistics:
2025-01-15 12:13:21,293 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.5, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-15 12:13:22,428 [INFO] [108] TRAIN  loss: 0.9264724201001323 acc: 1.0
2025-01-15 12:13:22,428 [INFO] [108] TRAIN  loss dict: {'classification_loss': 0.9264724201001323}
2025-01-15 12:13:22,428 [INFO] [108] VALIDATION loss: 1.7359490335211718 VALIDATION acc: 0.8181818181818182
2025-01-15 12:13:22,428 [INFO] [108] VALIDATION loss dict: {'classification_loss': 1.7359490335211718}
2025-01-15 12:13:22,428 [INFO] 
2025-01-15 12:13:40,793 [INFO] Step[50/2713]: training loss : 0.9262462103366852 TRAIN  loss dict:  {'classification_loss': 0.9262462103366852}
2025-01-15 12:13:54,578 [INFO] Step[100/2713]: training loss : 0.9259065473079682 TRAIN  loss dict:  {'classification_loss': 0.9259065473079682}
2025-01-15 12:14:08,426 [INFO] Step[150/2713]: training loss : 0.9257418048381806 TRAIN  loss dict:  {'classification_loss': 0.9257418048381806}
2025-01-15 12:14:22,106 [INFO] Step[200/2713]: training loss : 0.9275365602970124 TRAIN  loss dict:  {'classification_loss': 0.9275365602970124}
2025-01-15 12:14:35,827 [INFO] Step[250/2713]: training loss : 0.9261345326900482 TRAIN  loss dict:  {'classification_loss': 0.9261345326900482}
2025-01-15 12:14:49,439 [INFO] Step[300/2713]: training loss : 0.927301652431488 TRAIN  loss dict:  {'classification_loss': 0.927301652431488}
2025-01-15 12:15:03,259 [INFO] Step[350/2713]: training loss : 0.9260156953334808 TRAIN  loss dict:  {'classification_loss': 0.9260156953334808}
2025-01-15 12:15:16,901 [INFO] Step[400/2713]: training loss : 0.9265689480304719 TRAIN  loss dict:  {'classification_loss': 0.9265689480304719}
2025-01-15 12:15:30,891 [INFO] Step[450/2713]: training loss : 0.9258040416240693 TRAIN  loss dict:  {'classification_loss': 0.9258040416240693}
2025-01-15 12:15:44,701 [INFO] Step[500/2713]: training loss : 0.9269855856895447 TRAIN  loss dict:  {'classification_loss': 0.9269855856895447}
2025-01-15 12:15:58,458 [INFO] Step[550/2713]: training loss : 0.9263996374607086 TRAIN  loss dict:  {'classification_loss': 0.9263996374607086}
2025-01-15 12:16:12,331 [INFO] Step[600/2713]: training loss : 0.9261618280410766 TRAIN  loss dict:  {'classification_loss': 0.9261618280410766}
2025-01-15 12:16:26,021 [INFO] Step[650/2713]: training loss : 0.9266313576698303 TRAIN  loss dict:  {'classification_loss': 0.9266313576698303}
2025-01-15 12:16:39,224 [INFO] Step[700/2713]: training loss : 0.9259422421455383 TRAIN  loss dict:  {'classification_loss': 0.9259422421455383}
2025-01-15 12:16:52,772 [INFO] Step[750/2713]: training loss : 0.9269598662853241 TRAIN  loss dict:  {'classification_loss': 0.9269598662853241}
2025-01-15 12:17:06,545 [INFO] Step[800/2713]: training loss : 0.9266644501686097 TRAIN  loss dict:  {'classification_loss': 0.9266644501686097}
2025-01-15 12:17:20,107 [INFO] Step[850/2713]: training loss : 0.927222627401352 TRAIN  loss dict:  {'classification_loss': 0.927222627401352}
2025-01-15 12:17:33,340 [INFO] Step[900/2713]: training loss : 0.9263772559165955 TRAIN  loss dict:  {'classification_loss': 0.9263772559165955}
2025-01-15 12:17:46,837 [INFO] Step[950/2713]: training loss : 0.9266889703273773 TRAIN  loss dict:  {'classification_loss': 0.9266889703273773}
2025-01-15 12:18:00,773 [INFO] Step[1000/2713]: training loss : 0.9263918113708496 TRAIN  loss dict:  {'classification_loss': 0.9263918113708496}
2025-01-15 12:18:14,340 [INFO] Step[1050/2713]: training loss : 0.9283817291259766 TRAIN  loss dict:  {'classification_loss': 0.9283817291259766}
2025-01-15 12:18:27,710 [INFO] Step[1100/2713]: training loss : 0.9265923511981964 TRAIN  loss dict:  {'classification_loss': 0.9265923511981964}
2025-01-15 12:18:41,432 [INFO] Step[1150/2713]: training loss : 0.9264900469779969 TRAIN  loss dict:  {'classification_loss': 0.9264900469779969}
2025-01-15 12:18:55,653 [INFO] Step[1200/2713]: training loss : 0.9264890956878662 TRAIN  loss dict:  {'classification_loss': 0.9264890956878662}
2025-01-15 12:19:09,019 [INFO] Step[1250/2713]: training loss : 0.9257857871055603 TRAIN  loss dict:  {'classification_loss': 0.9257857871055603}
2025-01-15 12:19:22,542 [INFO] Step[1300/2713]: training loss : 0.9263131022453308 TRAIN  loss dict:  {'classification_loss': 0.9263131022453308}
2025-01-15 12:19:36,149 [INFO] Step[1350/2713]: training loss : 0.9261400747299194 TRAIN  loss dict:  {'classification_loss': 0.9261400747299194}
2025-01-15 12:19:50,080 [INFO] Step[1400/2713]: training loss : 0.9262891495227814 TRAIN  loss dict:  {'classification_loss': 0.9262891495227814}
2025-01-15 12:20:03,689 [INFO] Step[1450/2713]: training loss : 0.9262286508083344 TRAIN  loss dict:  {'classification_loss': 0.9262286508083344}
2025-01-15 12:20:16,922 [INFO] Step[1500/2713]: training loss : 0.926382884979248 TRAIN  loss dict:  {'classification_loss': 0.926382884979248}
2025-01-15 12:20:30,384 [INFO] Step[1550/2713]: training loss : 0.9259305191040039 TRAIN  loss dict:  {'classification_loss': 0.9259305191040039}
2025-01-15 12:20:44,159 [INFO] Step[1600/2713]: training loss : 0.9263994824886322 TRAIN  loss dict:  {'classification_loss': 0.9263994824886322}
2025-01-15 12:20:57,951 [INFO] Step[1650/2713]: training loss : 0.9263161981105804 TRAIN  loss dict:  {'classification_loss': 0.9263161981105804}
2025-01-15 12:21:11,901 [INFO] Step[1700/2713]: training loss : 0.926069061756134 TRAIN  loss dict:  {'classification_loss': 0.926069061756134}
2025-01-15 12:21:25,881 [INFO] Step[1750/2713]: training loss : 0.9261390793323517 TRAIN  loss dict:  {'classification_loss': 0.9261390793323517}
2025-01-15 12:21:40,126 [INFO] Step[1800/2713]: training loss : 0.931559100151062 TRAIN  loss dict:  {'classification_loss': 0.931559100151062}
2025-01-15 12:21:53,374 [INFO] Step[1850/2713]: training loss : 0.9314480185508728 TRAIN  loss dict:  {'classification_loss': 0.9314480185508728}
2025-01-15 12:22:07,305 [INFO] Step[1900/2713]: training loss : 0.9260962390899659 TRAIN  loss dict:  {'classification_loss': 0.9260962390899659}
2025-01-15 12:22:21,055 [INFO] Step[1950/2713]: training loss : 0.9261291670799255 TRAIN  loss dict:  {'classification_loss': 0.9261291670799255}
2025-01-15 12:22:34,651 [INFO] Step[2000/2713]: training loss : 0.9268750524520875 TRAIN  loss dict:  {'classification_loss': 0.9268750524520875}
2025-01-15 12:22:48,458 [INFO] Step[2050/2713]: training loss : 0.9256915032863617 TRAIN  loss dict:  {'classification_loss': 0.9256915032863617}
2025-01-15 12:23:01,681 [INFO] Step[2100/2713]: training loss : 0.9262045347690582 TRAIN  loss dict:  {'classification_loss': 0.9262045347690582}
2025-01-15 12:23:14,908 [INFO] Step[2150/2713]: training loss : 0.9266308677196503 TRAIN  loss dict:  {'classification_loss': 0.9266308677196503}
2025-01-15 12:23:28,717 [INFO] Step[2200/2713]: training loss : 0.9282554268836976 TRAIN  loss dict:  {'classification_loss': 0.9282554268836976}
2025-01-15 12:23:42,276 [INFO] Step[2250/2713]: training loss : 0.9264637386798859 TRAIN  loss dict:  {'classification_loss': 0.9264637386798859}
2025-01-15 12:23:56,002 [INFO] Step[2300/2713]: training loss : 0.9267070424556733 TRAIN  loss dict:  {'classification_loss': 0.9267070424556733}
2025-01-15 12:24:10,267 [INFO] Step[2350/2713]: training loss : 0.9257595682144165 TRAIN  loss dict:  {'classification_loss': 0.9257595682144165}
2025-01-15 12:24:23,952 [INFO] Step[2400/2713]: training loss : 0.9261678051948548 TRAIN  loss dict:  {'classification_loss': 0.9261678051948548}
2025-01-15 12:24:37,713 [INFO] Step[2450/2713]: training loss : 0.92637690782547 TRAIN  loss dict:  {'classification_loss': 0.92637690782547}
2025-01-15 12:24:51,555 [INFO] Step[2500/2713]: training loss : 0.9260650599002838 TRAIN  loss dict:  {'classification_loss': 0.9260650599002838}
2025-01-15 12:25:05,067 [INFO] Step[2550/2713]: training loss : 0.9264957761764526 TRAIN  loss dict:  {'classification_loss': 0.9264957761764526}
2025-01-15 12:25:18,500 [INFO] Step[2600/2713]: training loss : 0.9264612543582916 TRAIN  loss dict:  {'classification_loss': 0.9264612543582916}
2025-01-15 12:25:31,752 [INFO] Step[2650/2713]: training loss : 0.9261255264282227 TRAIN  loss dict:  {'classification_loss': 0.9261255264282227}
2025-01-15 12:25:45,160 [INFO] Step[2700/2713]: training loss : 0.9267202186584472 TRAIN  loss dict:  {'classification_loss': 0.9267202186584472}
2025-01-15 12:27:01,724 [INFO] Label accuracies statistics:
2025-01-15 12:27:01,725 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 12:27:01,726 [INFO] [109] TRAIN  loss: 0.9266234237990431 acc: 1.0
2025-01-15 12:27:01,726 [INFO] [109] TRAIN  loss dict: {'classification_loss': 0.9266234237990431}
2025-01-15 12:27:01,727 [INFO] [109] VALIDATION loss: 1.7853256735138427 VALIDATION acc: 0.8100313479623824
2025-01-15 12:27:01,727 [INFO] [109] VALIDATION loss dict: {'classification_loss': 1.7853256735138427}
2025-01-15 12:27:01,727 [INFO] 
2025-01-15 12:27:19,660 [INFO] Step[50/2713]: training loss : 0.9262824261188507 TRAIN  loss dict:  {'classification_loss': 0.9262824261188507}
2025-01-15 12:27:33,363 [INFO] Step[100/2713]: training loss : 0.9258718824386597 TRAIN  loss dict:  {'classification_loss': 0.9258718824386597}
2025-01-15 12:27:47,296 [INFO] Step[150/2713]: training loss : 0.9288695919513702 TRAIN  loss dict:  {'classification_loss': 0.9288695919513702}
2025-01-15 12:28:01,091 [INFO] Step[200/2713]: training loss : 0.9262446188926696 TRAIN  loss dict:  {'classification_loss': 0.9262446188926696}
2025-01-15 12:28:15,316 [INFO] Step[250/2713]: training loss : 0.9261762833595276 TRAIN  loss dict:  {'classification_loss': 0.9261762833595276}
2025-01-15 12:28:29,357 [INFO] Step[300/2713]: training loss : 0.9265343856811523 TRAIN  loss dict:  {'classification_loss': 0.9265343856811523}
2025-01-15 12:28:43,092 [INFO] Step[350/2713]: training loss : 0.9260654878616333 TRAIN  loss dict:  {'classification_loss': 0.9260654878616333}
2025-01-15 12:28:57,019 [INFO] Step[400/2713]: training loss : 0.9263272154331207 TRAIN  loss dict:  {'classification_loss': 0.9263272154331207}
2025-01-15 12:29:10,868 [INFO] Step[450/2713]: training loss : 0.9262445282936096 TRAIN  loss dict:  {'classification_loss': 0.9262445282936096}
2025-01-15 12:29:24,576 [INFO] Step[500/2713]: training loss : 0.9258705139160156 TRAIN  loss dict:  {'classification_loss': 0.9258705139160156}
2025-01-15 12:29:38,472 [INFO] Step[550/2713]: training loss : 0.9266860342025757 TRAIN  loss dict:  {'classification_loss': 0.9266860342025757}
2025-01-15 12:29:52,454 [INFO] Step[600/2713]: training loss : 0.9267778420448303 TRAIN  loss dict:  {'classification_loss': 0.9267778420448303}
2025-01-15 12:30:06,552 [INFO] Step[650/2713]: training loss : 0.926545444726944 TRAIN  loss dict:  {'classification_loss': 0.926545444726944}
2025-01-15 12:30:20,463 [INFO] Step[700/2713]: training loss : 0.9262492644786835 TRAIN  loss dict:  {'classification_loss': 0.9262492644786835}
2025-01-15 12:30:34,447 [INFO] Step[750/2713]: training loss : 0.930603711605072 TRAIN  loss dict:  {'classification_loss': 0.930603711605072}
2025-01-15 12:30:48,409 [INFO] Step[800/2713]: training loss : 0.9263838303089141 TRAIN  loss dict:  {'classification_loss': 0.9263838303089141}
2025-01-15 12:31:02,003 [INFO] Step[850/2713]: training loss : 0.9265258204936981 TRAIN  loss dict:  {'classification_loss': 0.9265258204936981}
2025-01-15 12:31:15,663 [INFO] Step[900/2713]: training loss : 0.9267351853847504 TRAIN  loss dict:  {'classification_loss': 0.9267351853847504}
2025-01-15 12:31:29,308 [INFO] Step[950/2713]: training loss : 0.9265198075771331 TRAIN  loss dict:  {'classification_loss': 0.9265198075771331}
2025-01-15 12:31:43,260 [INFO] Step[1000/2713]: training loss : 0.926128375530243 TRAIN  loss dict:  {'classification_loss': 0.926128375530243}
2025-01-15 12:31:56,992 [INFO] Step[1050/2713]: training loss : 0.9263855457305908 TRAIN  loss dict:  {'classification_loss': 0.9263855457305908}
2025-01-15 12:32:10,356 [INFO] Step[1100/2713]: training loss : 0.9260862457752228 TRAIN  loss dict:  {'classification_loss': 0.9260862457752228}
2025-01-15 12:32:24,377 [INFO] Step[1150/2713]: training loss : 0.9264176261425018 TRAIN  loss dict:  {'classification_loss': 0.9264176261425018}
2025-01-15 12:32:38,704 [INFO] Step[1200/2713]: training loss : 0.9263920867443085 TRAIN  loss dict:  {'classification_loss': 0.9263920867443085}
2025-01-15 12:32:52,135 [INFO] Step[1250/2713]: training loss : 0.9263770234584808 TRAIN  loss dict:  {'classification_loss': 0.9263770234584808}
2025-01-15 12:33:05,912 [INFO] Step[1300/2713]: training loss : 0.9260696792602539 TRAIN  loss dict:  {'classification_loss': 0.9260696792602539}
2025-01-15 12:33:19,435 [INFO] Step[1350/2713]: training loss : 0.9264498388767243 TRAIN  loss dict:  {'classification_loss': 0.9264498388767243}
2025-01-15 12:33:32,862 [INFO] Step[1400/2713]: training loss : 0.9265255689620971 TRAIN  loss dict:  {'classification_loss': 0.9265255689620971}
2025-01-15 12:33:47,137 [INFO] Step[1450/2713]: training loss : 0.9263381350040436 TRAIN  loss dict:  {'classification_loss': 0.9263381350040436}
2025-01-15 12:34:00,817 [INFO] Step[1500/2713]: training loss : 0.9260935032367706 TRAIN  loss dict:  {'classification_loss': 0.9260935032367706}
2025-01-15 12:34:14,500 [INFO] Step[1550/2713]: training loss : 0.925706604719162 TRAIN  loss dict:  {'classification_loss': 0.925706604719162}
2025-01-15 12:34:28,373 [INFO] Step[1600/2713]: training loss : 0.925714966058731 TRAIN  loss dict:  {'classification_loss': 0.925714966058731}
2025-01-15 12:34:42,362 [INFO] Step[1650/2713]: training loss : 0.9263046061992646 TRAIN  loss dict:  {'classification_loss': 0.9263046061992646}
2025-01-15 12:34:55,994 [INFO] Step[1700/2713]: training loss : 0.9263656508922576 TRAIN  loss dict:  {'classification_loss': 0.9263656508922576}
2025-01-15 12:35:09,342 [INFO] Step[1750/2713]: training loss : 0.926933821439743 TRAIN  loss dict:  {'classification_loss': 0.926933821439743}
2025-01-15 12:35:22,795 [INFO] Step[1800/2713]: training loss : 0.9264081740379333 TRAIN  loss dict:  {'classification_loss': 0.9264081740379333}
2025-01-15 12:35:36,704 [INFO] Step[1850/2713]: training loss : 0.9267171823978424 TRAIN  loss dict:  {'classification_loss': 0.9267171823978424}
2025-01-15 12:35:50,002 [INFO] Step[1900/2713]: training loss : 0.9264829933643342 TRAIN  loss dict:  {'classification_loss': 0.9264829933643342}
2025-01-15 12:36:03,626 [INFO] Step[1950/2713]: training loss : 0.9271867263317108 TRAIN  loss dict:  {'classification_loss': 0.9271867263317108}
2025-01-15 12:36:16,897 [INFO] Step[2000/2713]: training loss : 0.9263566184043884 TRAIN  loss dict:  {'classification_loss': 0.9263566184043884}
2025-01-15 12:36:30,879 [INFO] Step[2050/2713]: training loss : 0.9266268038749694 TRAIN  loss dict:  {'classification_loss': 0.9266268038749694}
2025-01-15 12:36:44,692 [INFO] Step[2100/2713]: training loss : 0.9261555433273315 TRAIN  loss dict:  {'classification_loss': 0.9261555433273315}
2025-01-15 12:36:58,792 [INFO] Step[2150/2713]: training loss : 0.9259443533420563 TRAIN  loss dict:  {'classification_loss': 0.9259443533420563}
2025-01-15 12:37:13,057 [INFO] Step[2200/2713]: training loss : 0.926332905292511 TRAIN  loss dict:  {'classification_loss': 0.926332905292511}
2025-01-15 12:37:27,115 [INFO] Step[2250/2713]: training loss : 0.9270943236351014 TRAIN  loss dict:  {'classification_loss': 0.9270943236351014}
2025-01-15 12:37:40,570 [INFO] Step[2300/2713]: training loss : 0.9260850381851197 TRAIN  loss dict:  {'classification_loss': 0.9260850381851197}
2025-01-15 12:37:54,459 [INFO] Step[2350/2713]: training loss : 0.9265610206127167 TRAIN  loss dict:  {'classification_loss': 0.9265610206127167}
2025-01-15 12:38:08,085 [INFO] Step[2400/2713]: training loss : 0.9259820985794067 TRAIN  loss dict:  {'classification_loss': 0.9259820985794067}
2025-01-15 12:38:21,713 [INFO] Step[2450/2713]: training loss : 0.9265352606773376 TRAIN  loss dict:  {'classification_loss': 0.9265352606773376}
2025-01-15 12:38:35,360 [INFO] Step[2500/2713]: training loss : 0.9299676942825318 TRAIN  loss dict:  {'classification_loss': 0.9299676942825318}
2025-01-15 12:38:49,129 [INFO] Step[2550/2713]: training loss : 0.9260435330867768 TRAIN  loss dict:  {'classification_loss': 0.9260435330867768}
2025-01-15 12:39:02,814 [INFO] Step[2600/2713]: training loss : 0.9260387408733368 TRAIN  loss dict:  {'classification_loss': 0.9260387408733368}
2025-01-15 12:39:16,576 [INFO] Step[2650/2713]: training loss : 0.9262781476974488 TRAIN  loss dict:  {'classification_loss': 0.9262781476974488}
2025-01-15 12:39:30,887 [INFO] Step[2700/2713]: training loss : 0.9265927267074585 TRAIN  loss dict:  {'classification_loss': 0.9265927267074585}
2025-01-15 12:40:47,541 [INFO] Label accuracies statistics:
2025-01-15 12:40:47,541 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 1.0, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 12:40:47,543 [INFO] [110] TRAIN  loss: 0.9265434457681911 acc: 1.0
2025-01-15 12:40:47,543 [INFO] [110] TRAIN  loss dict: {'classification_loss': 0.9265434457681911}
2025-01-15 12:40:47,543 [INFO] [110] VALIDATION loss: 1.791445687749332 VALIDATION acc: 0.8144200626959248
2025-01-15 12:40:47,543 [INFO] [110] VALIDATION loss dict: {'classification_loss': 1.791445687749332}
2025-01-15 12:40:47,543 [INFO] 
2025-01-15 12:41:07,184 [INFO] Step[50/2713]: training loss : 0.9275035989284516 TRAIN  loss dict:  {'classification_loss': 0.9275035989284516}
2025-01-15 12:41:21,371 [INFO] Step[100/2713]: training loss : 0.9268036544322967 TRAIN  loss dict:  {'classification_loss': 0.9268036544322967}
2025-01-15 12:41:35,340 [INFO] Step[150/2713]: training loss : 0.9266464674472809 TRAIN  loss dict:  {'classification_loss': 0.9266464674472809}
2025-01-15 12:41:49,605 [INFO] Step[200/2713]: training loss : 0.9265334439277649 TRAIN  loss dict:  {'classification_loss': 0.9265334439277649}
2025-01-15 12:42:03,746 [INFO] Step[250/2713]: training loss : 0.925848798751831 TRAIN  loss dict:  {'classification_loss': 0.925848798751831}
2025-01-15 12:42:17,612 [INFO] Step[300/2713]: training loss : 0.9400833332538605 TRAIN  loss dict:  {'classification_loss': 0.9400833332538605}
2025-01-15 12:42:31,229 [INFO] Step[350/2713]: training loss : 0.9266618978977204 TRAIN  loss dict:  {'classification_loss': 0.9266618978977204}
2025-01-15 12:42:45,116 [INFO] Step[400/2713]: training loss : 0.925650327205658 TRAIN  loss dict:  {'classification_loss': 0.925650327205658}
2025-01-15 12:42:59,080 [INFO] Step[450/2713]: training loss : 0.926385155916214 TRAIN  loss dict:  {'classification_loss': 0.926385155916214}
2025-01-15 12:43:12,701 [INFO] Step[500/2713]: training loss : 0.9266050338745118 TRAIN  loss dict:  {'classification_loss': 0.9266050338745118}
2025-01-15 12:43:25,962 [INFO] Step[550/2713]: training loss : 0.9267601239681243 TRAIN  loss dict:  {'classification_loss': 0.9267601239681243}
2025-01-15 12:43:39,242 [INFO] Step[600/2713]: training loss : 0.9305572199821472 TRAIN  loss dict:  {'classification_loss': 0.9305572199821472}
2025-01-15 12:43:52,936 [INFO] Step[650/2713]: training loss : 0.9261601912975311 TRAIN  loss dict:  {'classification_loss': 0.9261601912975311}
2025-01-15 12:44:06,977 [INFO] Step[700/2713]: training loss : 0.9257788860797882 TRAIN  loss dict:  {'classification_loss': 0.9257788860797882}
2025-01-15 12:44:21,019 [INFO] Step[750/2713]: training loss : 0.92630828499794 TRAIN  loss dict:  {'classification_loss': 0.92630828499794}
2025-01-15 12:44:34,761 [INFO] Step[800/2713]: training loss : 0.9261511313915253 TRAIN  loss dict:  {'classification_loss': 0.9261511313915253}
2025-01-15 12:44:48,447 [INFO] Step[850/2713]: training loss : 0.9262112331390381 TRAIN  loss dict:  {'classification_loss': 0.9262112331390381}
2025-01-15 12:45:02,685 [INFO] Step[900/2713]: training loss : 0.9259667754173279 TRAIN  loss dict:  {'classification_loss': 0.9259667754173279}
2025-01-15 12:45:16,974 [INFO] Step[950/2713]: training loss : 0.9261593306064606 TRAIN  loss dict:  {'classification_loss': 0.9261593306064606}
2025-01-15 12:45:30,830 [INFO] Step[1000/2713]: training loss : 0.9267680859565735 TRAIN  loss dict:  {'classification_loss': 0.9267680859565735}
2025-01-15 12:45:44,104 [INFO] Step[1050/2713]: training loss : 0.9268398714065552 TRAIN  loss dict:  {'classification_loss': 0.9268398714065552}
2025-01-15 12:45:57,799 [INFO] Step[1100/2713]: training loss : 0.9261240601539612 TRAIN  loss dict:  {'classification_loss': 0.9261240601539612}
2025-01-15 12:46:11,507 [INFO] Step[1150/2713]: training loss : 0.9261744570732117 TRAIN  loss dict:  {'classification_loss': 0.9261744570732117}
2025-01-15 12:46:25,318 [INFO] Step[1200/2713]: training loss : 0.9268261134624481 TRAIN  loss dict:  {'classification_loss': 0.9268261134624481}
2025-01-15 12:46:38,880 [INFO] Step[1250/2713]: training loss : 0.9256387674808502 TRAIN  loss dict:  {'classification_loss': 0.9256387674808502}
2025-01-15 12:46:52,625 [INFO] Step[1300/2713]: training loss : 0.9267616248130799 TRAIN  loss dict:  {'classification_loss': 0.9267616248130799}
2025-01-15 12:47:06,391 [INFO] Step[1350/2713]: training loss : 0.9264902472496033 TRAIN  loss dict:  {'classification_loss': 0.9264902472496033}
2025-01-15 12:47:20,344 [INFO] Step[1400/2713]: training loss : 0.9262742948532104 TRAIN  loss dict:  {'classification_loss': 0.9262742948532104}
2025-01-15 12:47:34,010 [INFO] Step[1450/2713]: training loss : 0.9258280849456787 TRAIN  loss dict:  {'classification_loss': 0.9258280849456787}
2025-01-15 12:47:47,557 [INFO] Step[1500/2713]: training loss : 0.9261800682544709 TRAIN  loss dict:  {'classification_loss': 0.9261800682544709}
2025-01-15 12:48:01,134 [INFO] Step[1550/2713]: training loss : 0.9261692214012146 TRAIN  loss dict:  {'classification_loss': 0.9261692214012146}
2025-01-15 12:48:14,979 [INFO] Step[1600/2713]: training loss : 0.9262888658046723 TRAIN  loss dict:  {'classification_loss': 0.9262888658046723}
2025-01-15 12:48:28,977 [INFO] Step[1650/2713]: training loss : 0.9263681387901306 TRAIN  loss dict:  {'classification_loss': 0.9263681387901306}
2025-01-15 12:48:42,700 [INFO] Step[1700/2713]: training loss : 0.9261099004745483 TRAIN  loss dict:  {'classification_loss': 0.9261099004745483}
2025-01-15 12:48:56,718 [INFO] Step[1750/2713]: training loss : 0.9263564622402192 TRAIN  loss dict:  {'classification_loss': 0.9263564622402192}
2025-01-15 12:49:11,035 [INFO] Step[1800/2713]: training loss : 0.9260135638713837 TRAIN  loss dict:  {'classification_loss': 0.9260135638713837}
2025-01-15 12:49:24,548 [INFO] Step[1850/2713]: training loss : 0.9257972526550293 TRAIN  loss dict:  {'classification_loss': 0.9257972526550293}
2025-01-15 12:49:38,675 [INFO] Step[1900/2713]: training loss : 0.9260938942432404 TRAIN  loss dict:  {'classification_loss': 0.9260938942432404}
2025-01-15 12:49:51,952 [INFO] Step[1950/2713]: training loss : 0.926298452615738 TRAIN  loss dict:  {'classification_loss': 0.926298452615738}
2025-01-15 12:50:05,914 [INFO] Step[2000/2713]: training loss : 0.9259771227836608 TRAIN  loss dict:  {'classification_loss': 0.9259771227836608}
2025-01-15 12:50:19,846 [INFO] Step[2050/2713]: training loss : 0.9262986671924591 TRAIN  loss dict:  {'classification_loss': 0.9262986671924591}
2025-01-15 12:50:33,448 [INFO] Step[2100/2713]: training loss : 0.9259658682346344 TRAIN  loss dict:  {'classification_loss': 0.9259658682346344}
2025-01-15 12:50:46,723 [INFO] Step[2150/2713]: training loss : 0.9259001278877258 TRAIN  loss dict:  {'classification_loss': 0.9259001278877258}
2025-01-15 12:51:00,637 [INFO] Step[2200/2713]: training loss : 0.9267030191421509 TRAIN  loss dict:  {'classification_loss': 0.9267030191421509}
2025-01-15 12:51:14,335 [INFO] Step[2250/2713]: training loss : 0.9263891565799713 TRAIN  loss dict:  {'classification_loss': 0.9263891565799713}
2025-01-15 12:51:27,887 [INFO] Step[2300/2713]: training loss : 0.9257789719104766 TRAIN  loss dict:  {'classification_loss': 0.9257789719104766}
2025-01-15 12:51:41,921 [INFO] Step[2350/2713]: training loss : 0.9259008419513702 TRAIN  loss dict:  {'classification_loss': 0.9259008419513702}
2025-01-15 12:51:55,310 [INFO] Step[2400/2713]: training loss : 0.9259287226200104 TRAIN  loss dict:  {'classification_loss': 0.9259287226200104}
2025-01-15 12:52:09,611 [INFO] Step[2450/2713]: training loss : 0.9257833206653595 TRAIN  loss dict:  {'classification_loss': 0.9257833206653595}
2025-01-15 12:52:23,566 [INFO] Step[2500/2713]: training loss : 0.9257272779941559 TRAIN  loss dict:  {'classification_loss': 0.9257272779941559}
2025-01-15 12:52:37,485 [INFO] Step[2550/2713]: training loss : 0.927493714094162 TRAIN  loss dict:  {'classification_loss': 0.927493714094162}
2025-01-15 12:52:51,356 [INFO] Step[2600/2713]: training loss : 0.9262948083877564 TRAIN  loss dict:  {'classification_loss': 0.9262948083877564}
2025-01-15 12:53:05,434 [INFO] Step[2650/2713]: training loss : 0.9263439536094665 TRAIN  loss dict:  {'classification_loss': 0.9263439536094665}
2025-01-15 12:53:19,412 [INFO] Step[2700/2713]: training loss : 0.9261165726184845 TRAIN  loss dict:  {'classification_loss': 0.9261165726184845}
2025-01-15 12:54:35,897 [INFO] Label accuracies statistics:
2025-01-15 12:54:35,897 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 12:54:35,899 [INFO] [111] TRAIN  loss: 0.9266033374731539 acc: 0.9998771347831429
2025-01-15 12:54:35,899 [INFO] [111] TRAIN  loss dict: {'classification_loss': 0.9266033374731539}
2025-01-15 12:54:35,899 [INFO] [111] VALIDATION loss: 1.7858313317585708 VALIDATION acc: 0.8175548589341692
2025-01-15 12:54:35,899 [INFO] [111] VALIDATION loss dict: {'classification_loss': 1.7858313317585708}
2025-01-15 12:54:35,899 [INFO] 
2025-01-15 12:54:55,226 [INFO] Step[50/2713]: training loss : 0.9267265999317169 TRAIN  loss dict:  {'classification_loss': 0.9267265999317169}
2025-01-15 12:55:09,169 [INFO] Step[100/2713]: training loss : 0.9263605642318725 TRAIN  loss dict:  {'classification_loss': 0.9263605642318725}
2025-01-15 12:55:22,717 [INFO] Step[150/2713]: training loss : 0.9262266683578492 TRAIN  loss dict:  {'classification_loss': 0.9262266683578492}
2025-01-15 12:55:36,007 [INFO] Step[200/2713]: training loss : 0.9267479956150055 TRAIN  loss dict:  {'classification_loss': 0.9267479956150055}
2025-01-15 12:55:50,123 [INFO] Step[250/2713]: training loss : 0.9259242296218873 TRAIN  loss dict:  {'classification_loss': 0.9259242296218873}
2025-01-15 12:56:03,863 [INFO] Step[300/2713]: training loss : 0.9261528384685517 TRAIN  loss dict:  {'classification_loss': 0.9261528384685517}
2025-01-15 12:56:17,798 [INFO] Step[350/2713]: training loss : 0.9258425855636596 TRAIN  loss dict:  {'classification_loss': 0.9258425855636596}
2025-01-15 12:56:31,451 [INFO] Step[400/2713]: training loss : 0.9265250134468078 TRAIN  loss dict:  {'classification_loss': 0.9265250134468078}
2025-01-15 12:56:45,265 [INFO] Step[450/2713]: training loss : 0.9257667577266693 TRAIN  loss dict:  {'classification_loss': 0.9257667577266693}
2025-01-15 12:56:59,133 [INFO] Step[500/2713]: training loss : 0.9259471297264099 TRAIN  loss dict:  {'classification_loss': 0.9259471297264099}
2025-01-15 12:57:13,123 [INFO] Step[550/2713]: training loss : 0.9256984949111938 TRAIN  loss dict:  {'classification_loss': 0.9256984949111938}
2025-01-15 12:57:26,548 [INFO] Step[600/2713]: training loss : 0.9261171007156372 TRAIN  loss dict:  {'classification_loss': 0.9261171007156372}
2025-01-15 12:57:40,756 [INFO] Step[650/2713]: training loss : 0.9256769394874573 TRAIN  loss dict:  {'classification_loss': 0.9256769394874573}
2025-01-15 12:57:54,434 [INFO] Step[700/2713]: training loss : 0.9263345181941987 TRAIN  loss dict:  {'classification_loss': 0.9263345181941987}
2025-01-15 12:58:07,969 [INFO] Step[750/2713]: training loss : 0.9261722481250763 TRAIN  loss dict:  {'classification_loss': 0.9261722481250763}
2025-01-15 12:58:21,549 [INFO] Step[800/2713]: training loss : 0.9263580822944641 TRAIN  loss dict:  {'classification_loss': 0.9263580822944641}
2025-01-15 12:58:35,740 [INFO] Step[850/2713]: training loss : 0.9263244903087616 TRAIN  loss dict:  {'classification_loss': 0.9263244903087616}
2025-01-15 12:58:49,679 [INFO] Step[900/2713]: training loss : 0.9262950336933136 TRAIN  loss dict:  {'classification_loss': 0.9262950336933136}
2025-01-15 12:59:03,686 [INFO] Step[950/2713]: training loss : 0.9262411594390869 TRAIN  loss dict:  {'classification_loss': 0.9262411594390869}
2025-01-15 12:59:17,626 [INFO] Step[1000/2713]: training loss : 0.9294641780853271 TRAIN  loss dict:  {'classification_loss': 0.9294641780853271}
2025-01-15 12:59:31,098 [INFO] Step[1050/2713]: training loss : 0.926237633228302 TRAIN  loss dict:  {'classification_loss': 0.926237633228302}
2025-01-15 12:59:44,685 [INFO] Step[1100/2713]: training loss : 0.9286269927024842 TRAIN  loss dict:  {'classification_loss': 0.9286269927024842}
2025-01-15 12:59:58,974 [INFO] Step[1150/2713]: training loss : 0.9260441124439239 TRAIN  loss dict:  {'classification_loss': 0.9260441124439239}
2025-01-15 13:00:13,234 [INFO] Step[1200/2713]: training loss : 0.9257473683357239 TRAIN  loss dict:  {'classification_loss': 0.9257473683357239}
2025-01-15 13:00:27,096 [INFO] Step[1250/2713]: training loss : 0.9261800932884217 TRAIN  loss dict:  {'classification_loss': 0.9261800932884217}
2025-01-15 13:00:40,579 [INFO] Step[1300/2713]: training loss : 0.9259287750720978 TRAIN  loss dict:  {'classification_loss': 0.9259287750720978}
2025-01-15 13:00:53,901 [INFO] Step[1350/2713]: training loss : 0.9264114260673523 TRAIN  loss dict:  {'classification_loss': 0.9264114260673523}
2025-01-15 13:01:07,457 [INFO] Step[1400/2713]: training loss : 0.9259772944450378 TRAIN  loss dict:  {'classification_loss': 0.9259772944450378}
2025-01-15 13:01:21,447 [INFO] Step[1450/2713]: training loss : 0.9258104455471039 TRAIN  loss dict:  {'classification_loss': 0.9258104455471039}
2025-01-15 13:01:34,736 [INFO] Step[1500/2713]: training loss : 0.9262724292278289 TRAIN  loss dict:  {'classification_loss': 0.9262724292278289}
2025-01-15 13:01:48,049 [INFO] Step[1550/2713]: training loss : 0.9259585177898407 TRAIN  loss dict:  {'classification_loss': 0.9259585177898407}
2025-01-15 13:02:01,316 [INFO] Step[1600/2713]: training loss : 0.9258469617366791 TRAIN  loss dict:  {'classification_loss': 0.9258469617366791}
2025-01-15 13:02:14,967 [INFO] Step[1650/2713]: training loss : 0.9259163308143615 TRAIN  loss dict:  {'classification_loss': 0.9259163308143615}
2025-01-15 13:02:28,835 [INFO] Step[1700/2713]: training loss : 0.9261545157432556 TRAIN  loss dict:  {'classification_loss': 0.9261545157432556}
2025-01-15 13:02:42,794 [INFO] Step[1750/2713]: training loss : 0.9260552561283112 TRAIN  loss dict:  {'classification_loss': 0.9260552561283112}
2025-01-15 13:02:56,804 [INFO] Step[1800/2713]: training loss : 0.9260751569271087 TRAIN  loss dict:  {'classification_loss': 0.9260751569271087}
2025-01-15 13:03:10,605 [INFO] Step[1850/2713]: training loss : 0.9258525753021241 TRAIN  loss dict:  {'classification_loss': 0.9258525753021241}
2025-01-15 13:03:24,249 [INFO] Step[1900/2713]: training loss : 0.9265739405155182 TRAIN  loss dict:  {'classification_loss': 0.9265739405155182}
2025-01-15 13:03:37,975 [INFO] Step[1950/2713]: training loss : 0.9263832604885102 TRAIN  loss dict:  {'classification_loss': 0.9263832604885102}
2025-01-15 13:03:51,812 [INFO] Step[2000/2713]: training loss : 0.9263147437572479 TRAIN  loss dict:  {'classification_loss': 0.9263147437572479}
2025-01-15 13:04:05,675 [INFO] Step[2050/2713]: training loss : 0.9260378563404084 TRAIN  loss dict:  {'classification_loss': 0.9260378563404084}
2025-01-15 13:04:19,936 [INFO] Step[2100/2713]: training loss : 0.9264439022541047 TRAIN  loss dict:  {'classification_loss': 0.9264439022541047}
2025-01-15 13:04:33,802 [INFO] Step[2150/2713]: training loss : 0.9262844526767731 TRAIN  loss dict:  {'classification_loss': 0.9262844526767731}
2025-01-15 13:04:47,511 [INFO] Step[2200/2713]: training loss : 0.9271118092536926 TRAIN  loss dict:  {'classification_loss': 0.9271118092536926}
2025-01-15 13:05:01,057 [INFO] Step[2250/2713]: training loss : 0.9263923120498657 TRAIN  loss dict:  {'classification_loss': 0.9263923120498657}
2025-01-15 13:05:15,033 [INFO] Step[2300/2713]: training loss : 0.9260631597042084 TRAIN  loss dict:  {'classification_loss': 0.9260631597042084}
2025-01-15 13:05:29,037 [INFO] Step[2350/2713]: training loss : 0.9260227406024932 TRAIN  loss dict:  {'classification_loss': 0.9260227406024932}
2025-01-15 13:05:43,201 [INFO] Step[2400/2713]: training loss : 0.9257772612571716 TRAIN  loss dict:  {'classification_loss': 0.9257772612571716}
2025-01-15 13:05:57,298 [INFO] Step[2450/2713]: training loss : 0.9256552624702453 TRAIN  loss dict:  {'classification_loss': 0.9256552624702453}
2025-01-15 13:06:11,410 [INFO] Step[2500/2713]: training loss : 0.9259860181808471 TRAIN  loss dict:  {'classification_loss': 0.9259860181808471}
2025-01-15 13:06:25,718 [INFO] Step[2550/2713]: training loss : 0.926215535402298 TRAIN  loss dict:  {'classification_loss': 0.926215535402298}
2025-01-15 13:06:39,962 [INFO] Step[2600/2713]: training loss : 0.9257893240451813 TRAIN  loss dict:  {'classification_loss': 0.9257893240451813}
2025-01-15 13:06:53,387 [INFO] Step[2650/2713]: training loss : 0.9261297881603241 TRAIN  loss dict:  {'classification_loss': 0.9261297881603241}
2025-01-15 13:07:07,010 [INFO] Step[2700/2713]: training loss : 0.9260711109638214 TRAIN  loss dict:  {'classification_loss': 0.9260711109638214}
2025-01-15 13:08:23,705 [INFO] Label accuracies statistics:
2025-01-15 13:08:23,706 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 1.0, 262: 0.75, 263: 1.0, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 1.0, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 13:08:23,708 [INFO] [112] TRAIN  loss: 0.9262465807594499 acc: 1.0
2025-01-15 13:08:23,708 [INFO] [112] TRAIN  loss dict: {'classification_loss': 0.9262465807594499}
2025-01-15 13:08:23,708 [INFO] [112] VALIDATION loss: 1.7821058487533628 VALIDATION acc: 0.8106583072100313
2025-01-15 13:08:23,708 [INFO] [112] VALIDATION loss dict: {'classification_loss': 1.7821058487533628}
2025-01-15 13:08:23,708 [INFO] 
2025-01-15 13:08:42,029 [INFO] Step[50/2713]: training loss : 0.9258155953884125 TRAIN  loss dict:  {'classification_loss': 0.9258155953884125}
2025-01-15 13:08:55,220 [INFO] Step[100/2713]: training loss : 0.9258096170425415 TRAIN  loss dict:  {'classification_loss': 0.9258096170425415}
2025-01-15 13:09:09,203 [INFO] Step[150/2713]: training loss : 0.925952445268631 TRAIN  loss dict:  {'classification_loss': 0.925952445268631}
2025-01-15 13:09:22,920 [INFO] Step[200/2713]: training loss : 0.926339464187622 TRAIN  loss dict:  {'classification_loss': 0.926339464187622}
2025-01-15 13:09:36,400 [INFO] Step[250/2713]: training loss : 0.9261065196990966 TRAIN  loss dict:  {'classification_loss': 0.9261065196990966}
2025-01-15 13:09:50,013 [INFO] Step[300/2713]: training loss : 0.9260095262527466 TRAIN  loss dict:  {'classification_loss': 0.9260095262527466}
2025-01-15 13:10:04,024 [INFO] Step[350/2713]: training loss : 0.925749397277832 TRAIN  loss dict:  {'classification_loss': 0.925749397277832}
2025-01-15 13:10:17,853 [INFO] Step[400/2713]: training loss : 0.925856032371521 TRAIN  loss dict:  {'classification_loss': 0.925856032371521}
2025-01-15 13:10:31,802 [INFO] Step[450/2713]: training loss : 0.9262875485420227 TRAIN  loss dict:  {'classification_loss': 0.9262875485420227}
2025-01-15 13:10:45,187 [INFO] Step[500/2713]: training loss : 0.9259830915927887 TRAIN  loss dict:  {'classification_loss': 0.9259830915927887}
2025-01-15 13:10:58,699 [INFO] Step[550/2713]: training loss : 0.9262225079536438 TRAIN  loss dict:  {'classification_loss': 0.9262225079536438}
2025-01-15 13:11:12,281 [INFO] Step[600/2713]: training loss : 0.9258100140094757 TRAIN  loss dict:  {'classification_loss': 0.9258100140094757}
2025-01-15 13:11:25,896 [INFO] Step[650/2713]: training loss : 0.9261986768245697 TRAIN  loss dict:  {'classification_loss': 0.9261986768245697}
2025-01-15 13:11:39,146 [INFO] Step[700/2713]: training loss : 0.9256869995594025 TRAIN  loss dict:  {'classification_loss': 0.9256869995594025}
2025-01-15 13:11:53,079 [INFO] Step[750/2713]: training loss : 0.9259154951572418 TRAIN  loss dict:  {'classification_loss': 0.9259154951572418}
2025-01-15 13:12:06,978 [INFO] Step[800/2713]: training loss : 0.9260991215705872 TRAIN  loss dict:  {'classification_loss': 0.9260991215705872}
2025-01-15 13:12:23,228 [INFO] Step[850/2713]: training loss : 0.926337753534317 TRAIN  loss dict:  {'classification_loss': 0.926337753534317}
2025-01-15 13:12:37,732 [INFO] Step[900/2713]: training loss : 0.9272678780555725 TRAIN  loss dict:  {'classification_loss': 0.9272678780555725}
2025-01-15 13:12:51,189 [INFO] Step[950/2713]: training loss : 0.9257752180099488 TRAIN  loss dict:  {'classification_loss': 0.9257752180099488}
2025-01-15 13:13:05,049 [INFO] Step[1000/2713]: training loss : 0.9261037802696228 TRAIN  loss dict:  {'classification_loss': 0.9261037802696228}
2025-01-15 13:13:19,255 [INFO] Step[1050/2713]: training loss : 0.9261684715747833 TRAIN  loss dict:  {'classification_loss': 0.9261684715747833}
2025-01-15 13:13:32,790 [INFO] Step[1100/2713]: training loss : 0.92614741563797 TRAIN  loss dict:  {'classification_loss': 0.92614741563797}
2025-01-15 13:13:46,519 [INFO] Step[1150/2713]: training loss : 0.9258163440227508 TRAIN  loss dict:  {'classification_loss': 0.9258163440227508}
2025-01-15 13:14:00,556 [INFO] Step[1200/2713]: training loss : 0.9266202235221863 TRAIN  loss dict:  {'classification_loss': 0.9266202235221863}
2025-01-15 13:14:14,115 [INFO] Step[1250/2713]: training loss : 0.9256521427631378 TRAIN  loss dict:  {'classification_loss': 0.9256521427631378}
2025-01-15 13:14:28,002 [INFO] Step[1300/2713]: training loss : 0.925584625005722 TRAIN  loss dict:  {'classification_loss': 0.925584625005722}
2025-01-15 13:14:41,430 [INFO] Step[1350/2713]: training loss : 0.9260115253925324 TRAIN  loss dict:  {'classification_loss': 0.9260115253925324}
2025-01-15 13:14:55,099 [INFO] Step[1400/2713]: training loss : 0.9266516172885895 TRAIN  loss dict:  {'classification_loss': 0.9266516172885895}
2025-01-15 13:15:08,823 [INFO] Step[1450/2713]: training loss : 0.9261764216423035 TRAIN  loss dict:  {'classification_loss': 0.9261764216423035}
2025-01-15 13:15:22,778 [INFO] Step[1500/2713]: training loss : 0.9257246100902558 TRAIN  loss dict:  {'classification_loss': 0.9257246100902558}
2025-01-15 13:15:36,353 [INFO] Step[1550/2713]: training loss : 0.9259984564781188 TRAIN  loss dict:  {'classification_loss': 0.9259984564781188}
2025-01-15 13:15:50,628 [INFO] Step[1600/2713]: training loss : 0.926315678358078 TRAIN  loss dict:  {'classification_loss': 0.926315678358078}
2025-01-15 13:16:04,219 [INFO] Step[1650/2713]: training loss : 0.9258886790275573 TRAIN  loss dict:  {'classification_loss': 0.9258886790275573}
2025-01-15 13:16:17,871 [INFO] Step[1700/2713]: training loss : 0.9258026516437531 TRAIN  loss dict:  {'classification_loss': 0.9258026516437531}
2025-01-15 13:16:31,968 [INFO] Step[1750/2713]: training loss : 0.9262411403656006 TRAIN  loss dict:  {'classification_loss': 0.9262411403656006}
2025-01-15 13:16:45,452 [INFO] Step[1800/2713]: training loss : 0.9258657467365264 TRAIN  loss dict:  {'classification_loss': 0.9258657467365264}
2025-01-15 13:16:59,398 [INFO] Step[1850/2713]: training loss : 0.9258924078941345 TRAIN  loss dict:  {'classification_loss': 0.9258924078941345}
2025-01-15 13:17:13,417 [INFO] Step[1900/2713]: training loss : 0.9257974755764008 TRAIN  loss dict:  {'classification_loss': 0.9257974755764008}
2025-01-15 13:17:27,554 [INFO] Step[1950/2713]: training loss : 0.926542603969574 TRAIN  loss dict:  {'classification_loss': 0.926542603969574}
2025-01-15 13:17:41,751 [INFO] Step[2000/2713]: training loss : 0.925623643398285 TRAIN  loss dict:  {'classification_loss': 0.925623643398285}
2025-01-15 13:17:55,648 [INFO] Step[2050/2713]: training loss : 0.9262531197071076 TRAIN  loss dict:  {'classification_loss': 0.9262531197071076}
2025-01-15 13:18:09,806 [INFO] Step[2100/2713]: training loss : 0.9263477766513825 TRAIN  loss dict:  {'classification_loss': 0.9263477766513825}
2025-01-15 13:18:23,565 [INFO] Step[2150/2713]: training loss : 0.9263061308860778 TRAIN  loss dict:  {'classification_loss': 0.9263061308860778}
2025-01-15 13:18:37,227 [INFO] Step[2200/2713]: training loss : 0.927349545955658 TRAIN  loss dict:  {'classification_loss': 0.927349545955658}
2025-01-15 13:18:50,945 [INFO] Step[2250/2713]: training loss : 0.9263541281223298 TRAIN  loss dict:  {'classification_loss': 0.9263541281223298}
2025-01-15 13:19:04,851 [INFO] Step[2300/2713]: training loss : 0.925986407995224 TRAIN  loss dict:  {'classification_loss': 0.925986407995224}
2025-01-15 13:19:18,093 [INFO] Step[2350/2713]: training loss : 0.9262241387367248 TRAIN  loss dict:  {'classification_loss': 0.9262241387367248}
2025-01-15 13:19:31,648 [INFO] Step[2400/2713]: training loss : 0.92622927069664 TRAIN  loss dict:  {'classification_loss': 0.92622927069664}
2025-01-15 13:19:44,900 [INFO] Step[2450/2713]: training loss : 0.925584352016449 TRAIN  loss dict:  {'classification_loss': 0.925584352016449}
2025-01-15 13:19:58,101 [INFO] Step[2500/2713]: training loss : 0.9257442724704742 TRAIN  loss dict:  {'classification_loss': 0.9257442724704742}
2025-01-15 13:20:11,943 [INFO] Step[2550/2713]: training loss : 0.9263383615016937 TRAIN  loss dict:  {'classification_loss': 0.9263383615016937}
2025-01-15 13:20:25,647 [INFO] Step[2600/2713]: training loss : 0.9302437198162079 TRAIN  loss dict:  {'classification_loss': 0.9302437198162079}
2025-01-15 13:20:39,646 [INFO] Step[2650/2713]: training loss : 0.926428793668747 TRAIN  loss dict:  {'classification_loss': 0.926428793668747}
2025-01-15 13:20:53,344 [INFO] Step[2700/2713]: training loss : 0.9261560595035553 TRAIN  loss dict:  {'classification_loss': 0.9261560595035553}
2025-01-15 13:22:09,809 [INFO] Label accuracies statistics:
2025-01-15 13:22:09,809 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 13:22:09,811 [INFO] [113] TRAIN  loss: 0.9261740066009476 acc: 1.0
2025-01-15 13:22:09,811 [INFO] [113] TRAIN  loss dict: {'classification_loss': 0.9261740066009476}
2025-01-15 13:22:09,811 [INFO] [113] VALIDATION loss: 1.8265625283234102 VALIDATION acc: 0.8006269592476489
2025-01-15 13:22:09,811 [INFO] [113] VALIDATION loss dict: {'classification_loss': 1.8265625283234102}
2025-01-15 13:22:09,811 [INFO] 
2025-01-15 13:22:28,242 [INFO] Step[50/2713]: training loss : 0.9256662225723267 TRAIN  loss dict:  {'classification_loss': 0.9256662225723267}
2025-01-15 13:22:41,486 [INFO] Step[100/2713]: training loss : 0.9257834661006927 TRAIN  loss dict:  {'classification_loss': 0.9257834661006927}
2025-01-15 13:22:55,297 [INFO] Step[150/2713]: training loss : 0.9263301110267639 TRAIN  loss dict:  {'classification_loss': 0.9263301110267639}
2025-01-15 13:23:08,627 [INFO] Step[200/2713]: training loss : 0.9262084949016571 TRAIN  loss dict:  {'classification_loss': 0.9262084949016571}
2025-01-15 13:23:22,529 [INFO] Step[250/2713]: training loss : 0.926149537563324 TRAIN  loss dict:  {'classification_loss': 0.926149537563324}
2025-01-15 13:23:36,150 [INFO] Step[300/2713]: training loss : 0.9266384792327881 TRAIN  loss dict:  {'classification_loss': 0.9266384792327881}
2025-01-15 13:23:50,025 [INFO] Step[350/2713]: training loss : 0.9262054133415222 TRAIN  loss dict:  {'classification_loss': 0.9262054133415222}
2025-01-15 13:24:04,110 [INFO] Step[400/2713]: training loss : 0.9261193180084228 TRAIN  loss dict:  {'classification_loss': 0.9261193180084228}
2025-01-15 13:24:17,738 [INFO] Step[450/2713]: training loss : 0.9259932959079742 TRAIN  loss dict:  {'classification_loss': 0.9259932959079742}
2025-01-15 13:24:31,234 [INFO] Step[500/2713]: training loss : 0.9262149143218994 TRAIN  loss dict:  {'classification_loss': 0.9262149143218994}
2025-01-15 13:24:45,069 [INFO] Step[550/2713]: training loss : 0.9265385842323304 TRAIN  loss dict:  {'classification_loss': 0.9265385842323304}
2025-01-15 13:24:58,977 [INFO] Step[600/2713]: training loss : 0.9304004299640656 TRAIN  loss dict:  {'classification_loss': 0.9304004299640656}
2025-01-15 13:25:13,252 [INFO] Step[650/2713]: training loss : 0.9261536490917206 TRAIN  loss dict:  {'classification_loss': 0.9261536490917206}
2025-01-15 13:25:29,350 [INFO] Step[700/2713]: training loss : 0.9260674107074738 TRAIN  loss dict:  {'classification_loss': 0.9260674107074738}
2025-01-15 13:25:43,582 [INFO] Step[750/2713]: training loss : 0.9263762867450714 TRAIN  loss dict:  {'classification_loss': 0.9263762867450714}
2025-01-15 13:25:57,073 [INFO] Step[800/2713]: training loss : 0.9259250593185425 TRAIN  loss dict:  {'classification_loss': 0.9259250593185425}
2025-01-15 13:26:10,787 [INFO] Step[850/2713]: training loss : 0.9255480277538299 TRAIN  loss dict:  {'classification_loss': 0.9255480277538299}
2025-01-15 13:26:24,053 [INFO] Step[900/2713]: training loss : 0.9259126269817353 TRAIN  loss dict:  {'classification_loss': 0.9259126269817353}
2025-01-15 13:26:38,110 [INFO] Step[950/2713]: training loss : 0.9259988582134246 TRAIN  loss dict:  {'classification_loss': 0.9259988582134246}
2025-01-15 13:26:51,863 [INFO] Step[1000/2713]: training loss : 0.926458523273468 TRAIN  loss dict:  {'classification_loss': 0.926458523273468}
2025-01-15 13:27:05,107 [INFO] Step[1050/2713]: training loss : 0.9258594036102294 TRAIN  loss dict:  {'classification_loss': 0.9258594036102294}
2025-01-15 13:27:18,319 [INFO] Step[1100/2713]: training loss : 0.9290487170219421 TRAIN  loss dict:  {'classification_loss': 0.9290487170219421}
2025-01-15 13:27:31,662 [INFO] Step[1150/2713]: training loss : 0.926220840215683 TRAIN  loss dict:  {'classification_loss': 0.926220840215683}
2025-01-15 13:27:45,373 [INFO] Step[1200/2713]: training loss : 0.9267981624603272 TRAIN  loss dict:  {'classification_loss': 0.9267981624603272}
2025-01-15 13:27:58,943 [INFO] Step[1250/2713]: training loss : 0.926939526796341 TRAIN  loss dict:  {'classification_loss': 0.926939526796341}
2025-01-15 13:28:12,539 [INFO] Step[1300/2713]: training loss : 0.9266397273540496 TRAIN  loss dict:  {'classification_loss': 0.9266397273540496}
2025-01-15 13:28:26,236 [INFO] Step[1350/2713]: training loss : 0.9261573421955108 TRAIN  loss dict:  {'classification_loss': 0.9261573421955108}
2025-01-15 13:28:39,671 [INFO] Step[1400/2713]: training loss : 0.9260499179363251 TRAIN  loss dict:  {'classification_loss': 0.9260499179363251}
2025-01-15 13:28:53,318 [INFO] Step[1450/2713]: training loss : 0.926468757390976 TRAIN  loss dict:  {'classification_loss': 0.926468757390976}
2025-01-15 13:29:07,020 [INFO] Step[1500/2713]: training loss : 0.9256450700759887 TRAIN  loss dict:  {'classification_loss': 0.9256450700759887}
2025-01-15 13:29:20,601 [INFO] Step[1550/2713]: training loss : 0.9261897802352905 TRAIN  loss dict:  {'classification_loss': 0.9261897802352905}
2025-01-15 13:29:34,382 [INFO] Step[1600/2713]: training loss : 0.9260690796375275 TRAIN  loss dict:  {'classification_loss': 0.9260690796375275}
2025-01-15 13:29:48,454 [INFO] Step[1650/2713]: training loss : 0.9260562205314636 TRAIN  loss dict:  {'classification_loss': 0.9260562205314636}
2025-01-15 13:30:01,768 [INFO] Step[1700/2713]: training loss : 0.9270698869228363 TRAIN  loss dict:  {'classification_loss': 0.9270698869228363}
2025-01-15 13:30:16,011 [INFO] Step[1750/2713]: training loss : 0.9264546358585357 TRAIN  loss dict:  {'classification_loss': 0.9264546358585357}
2025-01-15 13:30:29,871 [INFO] Step[1800/2713]: training loss : 0.9259236145019532 TRAIN  loss dict:  {'classification_loss': 0.9259236145019532}
2025-01-15 13:30:43,651 [INFO] Step[1850/2713]: training loss : 0.92613001704216 TRAIN  loss dict:  {'classification_loss': 0.92613001704216}
2025-01-15 13:30:57,695 [INFO] Step[1900/2713]: training loss : 0.9263695192337036 TRAIN  loss dict:  {'classification_loss': 0.9263695192337036}
2025-01-15 13:31:11,956 [INFO] Step[1950/2713]: training loss : 0.925999950170517 TRAIN  loss dict:  {'classification_loss': 0.925999950170517}
2025-01-15 13:31:26,165 [INFO] Step[2000/2713]: training loss : 0.9263743579387664 TRAIN  loss dict:  {'classification_loss': 0.9263743579387664}
2025-01-15 13:31:39,829 [INFO] Step[2050/2713]: training loss : 0.9257223153114319 TRAIN  loss dict:  {'classification_loss': 0.9257223153114319}
2025-01-15 13:31:54,058 [INFO] Step[2100/2713]: training loss : 0.9255844402313232 TRAIN  loss dict:  {'classification_loss': 0.9255844402313232}
2025-01-15 13:32:08,346 [INFO] Step[2150/2713]: training loss : 0.9260411369800567 TRAIN  loss dict:  {'classification_loss': 0.9260411369800567}
2025-01-15 13:32:22,282 [INFO] Step[2200/2713]: training loss : 0.9259086930751801 TRAIN  loss dict:  {'classification_loss': 0.9259086930751801}
2025-01-15 13:32:35,524 [INFO] Step[2250/2713]: training loss : 0.9258174681663514 TRAIN  loss dict:  {'classification_loss': 0.9258174681663514}
2025-01-15 13:32:50,169 [INFO] Step[2300/2713]: training loss : 0.9257571244239807 TRAIN  loss dict:  {'classification_loss': 0.9257571244239807}
2025-01-15 13:33:05,135 [INFO] Step[2350/2713]: training loss : 0.9257486963272095 TRAIN  loss dict:  {'classification_loss': 0.9257486963272095}
2025-01-15 13:33:18,704 [INFO] Step[2400/2713]: training loss : 0.9260241174697876 TRAIN  loss dict:  {'classification_loss': 0.9260241174697876}
2025-01-15 13:33:32,527 [INFO] Step[2450/2713]: training loss : 0.9256543552875519 TRAIN  loss dict:  {'classification_loss': 0.9256543552875519}
2025-01-15 13:33:46,342 [INFO] Step[2500/2713]: training loss : 0.9270515954494476 TRAIN  loss dict:  {'classification_loss': 0.9270515954494476}
2025-01-15 13:34:00,144 [INFO] Step[2550/2713]: training loss : 0.926353657245636 TRAIN  loss dict:  {'classification_loss': 0.926353657245636}
2025-01-15 13:34:13,604 [INFO] Step[2600/2713]: training loss : 0.9263930201530457 TRAIN  loss dict:  {'classification_loss': 0.9263930201530457}
2025-01-15 13:34:27,222 [INFO] Step[2650/2713]: training loss : 0.9262657487392425 TRAIN  loss dict:  {'classification_loss': 0.9262657487392425}
2025-01-15 13:34:41,034 [INFO] Step[2700/2713]: training loss : 0.9260026621818542 TRAIN  loss dict:  {'classification_loss': 0.9260026621818542}
2025-01-15 13:35:57,278 [INFO] Label accuracies statistics:
2025-01-15 13:35:57,279 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 13:35:57,280 [INFO] [114] TRAIN  loss: 0.9262918551777546 acc: 1.0
2025-01-15 13:35:57,280 [INFO] [114] TRAIN  loss dict: {'classification_loss': 0.9262918551777546}
2025-01-15 13:35:57,281 [INFO] [114] VALIDATION loss: 1.773304045088309 VALIDATION acc: 0.8112852664576803
2025-01-15 13:35:57,281 [INFO] [114] VALIDATION loss dict: {'classification_loss': 1.773304045088309}
2025-01-15 13:35:57,281 [INFO] 
2025-01-15 13:36:16,135 [INFO] Step[50/2713]: training loss : 0.925968906879425 TRAIN  loss dict:  {'classification_loss': 0.925968906879425}
2025-01-15 13:36:29,945 [INFO] Step[100/2713]: training loss : 0.9285016870498657 TRAIN  loss dict:  {'classification_loss': 0.9285016870498657}
2025-01-15 13:36:46,493 [INFO] Step[150/2713]: training loss : 0.9262106812000275 TRAIN  loss dict:  {'classification_loss': 0.9262106812000275}
2025-01-15 13:37:00,843 [INFO] Step[200/2713]: training loss : 0.9258932888507843 TRAIN  loss dict:  {'classification_loss': 0.9258932888507843}
2025-01-15 13:37:14,760 [INFO] Step[250/2713]: training loss : 0.9266590702533722 TRAIN  loss dict:  {'classification_loss': 0.9266590702533722}
2025-01-15 13:37:28,718 [INFO] Step[300/2713]: training loss : 0.925972398519516 TRAIN  loss dict:  {'classification_loss': 0.925972398519516}
2025-01-15 13:37:41,978 [INFO] Step[350/2713]: training loss : 0.9257155311107635 TRAIN  loss dict:  {'classification_loss': 0.9257155311107635}
2025-01-15 13:37:55,777 [INFO] Step[400/2713]: training loss : 0.9260271108150482 TRAIN  loss dict:  {'classification_loss': 0.9260271108150482}
2025-01-15 13:38:09,657 [INFO] Step[450/2713]: training loss : 0.9260394716262818 TRAIN  loss dict:  {'classification_loss': 0.9260394716262818}
2025-01-15 13:38:23,550 [INFO] Step[500/2713]: training loss : 0.9260612881183624 TRAIN  loss dict:  {'classification_loss': 0.9260612881183624}
2025-01-15 13:38:37,191 [INFO] Step[550/2713]: training loss : 0.9264749562740326 TRAIN  loss dict:  {'classification_loss': 0.9264749562740326}
2025-01-15 13:38:51,380 [INFO] Step[600/2713]: training loss : 0.9257801449298859 TRAIN  loss dict:  {'classification_loss': 0.9257801449298859}
2025-01-15 13:39:04,793 [INFO] Step[650/2713]: training loss : 0.9260496592521668 TRAIN  loss dict:  {'classification_loss': 0.9260496592521668}
2025-01-15 13:39:18,247 [INFO] Step[700/2713]: training loss : 0.9262411499023437 TRAIN  loss dict:  {'classification_loss': 0.9262411499023437}
2025-01-15 13:39:31,954 [INFO] Step[750/2713]: training loss : 0.9259225952625275 TRAIN  loss dict:  {'classification_loss': 0.9259225952625275}
2025-01-15 13:39:45,593 [INFO] Step[800/2713]: training loss : 0.9262444257736206 TRAIN  loss dict:  {'classification_loss': 0.9262444257736206}
2025-01-15 13:39:59,645 [INFO] Step[850/2713]: training loss : 0.9263153040409088 TRAIN  loss dict:  {'classification_loss': 0.9263153040409088}
2025-01-15 13:40:13,865 [INFO] Step[900/2713]: training loss : 0.9265449213981628 TRAIN  loss dict:  {'classification_loss': 0.9265449213981628}
2025-01-15 13:40:27,637 [INFO] Step[950/2713]: training loss : 0.9260222327709198 TRAIN  loss dict:  {'classification_loss': 0.9260222327709198}
2025-01-15 13:40:41,644 [INFO] Step[1000/2713]: training loss : 0.9258727300167083 TRAIN  loss dict:  {'classification_loss': 0.9258727300167083}
2025-01-15 13:40:55,892 [INFO] Step[1050/2713]: training loss : 0.9254762256145477 TRAIN  loss dict:  {'classification_loss': 0.9254762256145477}
2025-01-15 13:41:09,142 [INFO] Step[1100/2713]: training loss : 0.9264168751239776 TRAIN  loss dict:  {'classification_loss': 0.9264168751239776}
2025-01-15 13:41:22,386 [INFO] Step[1150/2713]: training loss : 0.925531564950943 TRAIN  loss dict:  {'classification_loss': 0.925531564950943}
2025-01-15 13:41:36,256 [INFO] Step[1200/2713]: training loss : 0.9263736617565155 TRAIN  loss dict:  {'classification_loss': 0.9263736617565155}
2025-01-15 13:41:50,235 [INFO] Step[1250/2713]: training loss : 0.9262553775310516 TRAIN  loss dict:  {'classification_loss': 0.9262553775310516}
2025-01-15 13:42:04,447 [INFO] Step[1300/2713]: training loss : 0.9259075844287872 TRAIN  loss dict:  {'classification_loss': 0.9259075844287872}
2025-01-15 13:42:18,284 [INFO] Step[1350/2713]: training loss : 0.9266187775135041 TRAIN  loss dict:  {'classification_loss': 0.9266187775135041}
2025-01-15 13:42:32,176 [INFO] Step[1400/2713]: training loss : 0.9275244343280792 TRAIN  loss dict:  {'classification_loss': 0.9275244343280792}
2025-01-15 13:42:45,919 [INFO] Step[1450/2713]: training loss : 0.9260932934284211 TRAIN  loss dict:  {'classification_loss': 0.9260932934284211}
2025-01-15 13:42:59,890 [INFO] Step[1500/2713]: training loss : 0.9259361398220062 TRAIN  loss dict:  {'classification_loss': 0.9259361398220062}
2025-01-15 13:43:13,755 [INFO] Step[1550/2713]: training loss : 0.9253931510448455 TRAIN  loss dict:  {'classification_loss': 0.9253931510448455}
2025-01-15 13:43:27,469 [INFO] Step[1600/2713]: training loss : 0.9263226819038392 TRAIN  loss dict:  {'classification_loss': 0.9263226819038392}
2025-01-15 13:43:41,063 [INFO] Step[1650/2713]: training loss : 0.9267792057991028 TRAIN  loss dict:  {'classification_loss': 0.9267792057991028}
2025-01-15 13:43:54,889 [INFO] Step[1700/2713]: training loss : 0.9262473952770233 TRAIN  loss dict:  {'classification_loss': 0.9262473952770233}
2025-01-15 13:44:08,442 [INFO] Step[1750/2713]: training loss : 0.9275282847881318 TRAIN  loss dict:  {'classification_loss': 0.9275282847881318}
2025-01-15 13:44:22,443 [INFO] Step[1800/2713]: training loss : 0.9256516885757446 TRAIN  loss dict:  {'classification_loss': 0.9256516885757446}
2025-01-15 13:44:36,184 [INFO] Step[1850/2713]: training loss : 0.926246395111084 TRAIN  loss dict:  {'classification_loss': 0.926246395111084}
2025-01-15 13:44:50,022 [INFO] Step[1900/2713]: training loss : 0.926281715631485 TRAIN  loss dict:  {'classification_loss': 0.926281715631485}
2025-01-15 13:45:03,592 [INFO] Step[1950/2713]: training loss : 0.9257756733894348 TRAIN  loss dict:  {'classification_loss': 0.9257756733894348}
2025-01-15 13:45:17,310 [INFO] Step[2000/2713]: training loss : 0.9260979235172272 TRAIN  loss dict:  {'classification_loss': 0.9260979235172272}
2025-01-15 13:45:30,985 [INFO] Step[2050/2713]: training loss : 0.9257718026638031 TRAIN  loss dict:  {'classification_loss': 0.9257718026638031}
2025-01-15 13:45:44,735 [INFO] Step[2100/2713]: training loss : 0.9261690640449524 TRAIN  loss dict:  {'classification_loss': 0.9261690640449524}
2025-01-15 13:45:58,460 [INFO] Step[2150/2713]: training loss : 0.9265497958660126 TRAIN  loss dict:  {'classification_loss': 0.9265497958660126}
2025-01-15 13:46:12,316 [INFO] Step[2200/2713]: training loss : 0.9257558786869049 TRAIN  loss dict:  {'classification_loss': 0.9257558786869049}
2025-01-15 13:46:26,157 [INFO] Step[2250/2713]: training loss : 0.9263206541538238 TRAIN  loss dict:  {'classification_loss': 0.9263206541538238}
2025-01-15 13:46:39,372 [INFO] Step[2300/2713]: training loss : 0.9299082970619201 TRAIN  loss dict:  {'classification_loss': 0.9299082970619201}
2025-01-15 13:46:53,504 [INFO] Step[2350/2713]: training loss : 0.9261522412300109 TRAIN  loss dict:  {'classification_loss': 0.9261522412300109}
2025-01-15 13:47:08,436 [INFO] Step[2400/2713]: training loss : 0.9255477821826935 TRAIN  loss dict:  {'classification_loss': 0.9255477821826935}
2025-01-15 13:47:24,518 [INFO] Step[2450/2713]: training loss : 0.9262318336963653 TRAIN  loss dict:  {'classification_loss': 0.9262318336963653}
2025-01-15 13:47:37,815 [INFO] Step[2500/2713]: training loss : 0.926080881357193 TRAIN  loss dict:  {'classification_loss': 0.926080881357193}
2025-01-15 13:47:51,346 [INFO] Step[2550/2713]: training loss : 0.9264441728591919 TRAIN  loss dict:  {'classification_loss': 0.9264441728591919}
2025-01-15 13:48:05,249 [INFO] Step[2600/2713]: training loss : 0.925988540649414 TRAIN  loss dict:  {'classification_loss': 0.925988540649414}
2025-01-15 13:48:19,257 [INFO] Step[2650/2713]: training loss : 0.925695447921753 TRAIN  loss dict:  {'classification_loss': 0.925695447921753}
2025-01-15 13:48:35,129 [INFO] Step[2700/2713]: training loss : 0.9261115801334381 TRAIN  loss dict:  {'classification_loss': 0.9261115801334381}
2025-01-15 13:50:10,651 [INFO] Label accuracies statistics:
2025-01-15 13:50:10,651 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.5, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 13:50:10,653 [INFO] [115] TRAIN  loss: 0.9262561006296441 acc: 1.0
2025-01-15 13:50:10,653 [INFO] [115] TRAIN  loss dict: {'classification_loss': 0.9262561006296441}
2025-01-15 13:50:10,653 [INFO] [115] VALIDATION loss: 1.7540010746036256 VALIDATION acc: 0.8150470219435737
2025-01-15 13:50:10,653 [INFO] [115] VALIDATION loss dict: {'classification_loss': 1.7540010746036256}
2025-01-15 13:50:10,653 [INFO] 
2025-01-15 13:50:29,454 [INFO] Step[50/2713]: training loss : 0.9260353767871856 TRAIN  loss dict:  {'classification_loss': 0.9260353767871856}
2025-01-15 13:50:43,217 [INFO] Step[100/2713]: training loss : 0.9255247902870178 TRAIN  loss dict:  {'classification_loss': 0.9255247902870178}
2025-01-15 13:50:57,929 [INFO] Step[150/2713]: training loss : 0.926309643983841 TRAIN  loss dict:  {'classification_loss': 0.926309643983841}
2025-01-15 13:51:14,509 [INFO] Step[200/2713]: training loss : 0.925683147907257 TRAIN  loss dict:  {'classification_loss': 0.925683147907257}
2025-01-15 13:51:28,207 [INFO] Step[250/2713]: training loss : 0.9265443027019501 TRAIN  loss dict:  {'classification_loss': 0.9265443027019501}
2025-01-15 13:51:41,980 [INFO] Step[300/2713]: training loss : 0.9267525136470794 TRAIN  loss dict:  {'classification_loss': 0.9267525136470794}
2025-01-15 13:51:55,809 [INFO] Step[350/2713]: training loss : 0.9254426920413971 TRAIN  loss dict:  {'classification_loss': 0.9254426920413971}
2025-01-15 13:52:09,769 [INFO] Step[400/2713]: training loss : 0.9267982745170593 TRAIN  loss dict:  {'classification_loss': 0.9267982745170593}
2025-01-15 13:52:23,643 [INFO] Step[450/2713]: training loss : 0.9258023166656494 TRAIN  loss dict:  {'classification_loss': 0.9258023166656494}
2025-01-15 13:52:37,274 [INFO] Step[500/2713]: training loss : 0.9263832604885102 TRAIN  loss dict:  {'classification_loss': 0.9263832604885102}
2025-01-15 13:52:51,596 [INFO] Step[550/2713]: training loss : 0.9256452965736389 TRAIN  loss dict:  {'classification_loss': 0.9256452965736389}
2025-01-15 13:53:05,226 [INFO] Step[600/2713]: training loss : 0.9264390003681183 TRAIN  loss dict:  {'classification_loss': 0.9264390003681183}
2025-01-15 13:53:18,917 [INFO] Step[650/2713]: training loss : 0.9262906455993652 TRAIN  loss dict:  {'classification_loss': 0.9262906455993652}
2025-01-15 13:53:32,291 [INFO] Step[700/2713]: training loss : 0.9261321794986724 TRAIN  loss dict:  {'classification_loss': 0.9261321794986724}
2025-01-15 13:53:46,312 [INFO] Step[750/2713]: training loss : 0.9260974824428558 TRAIN  loss dict:  {'classification_loss': 0.9260974824428558}
2025-01-15 13:54:00,611 [INFO] Step[800/2713]: training loss : 0.9259635734558106 TRAIN  loss dict:  {'classification_loss': 0.9259635734558106}
2025-01-15 13:54:14,325 [INFO] Step[850/2713]: training loss : 0.9259401381015777 TRAIN  loss dict:  {'classification_loss': 0.9259401381015777}
2025-01-15 13:54:28,219 [INFO] Step[900/2713]: training loss : 0.9256292450428009 TRAIN  loss dict:  {'classification_loss': 0.9256292450428009}
2025-01-15 13:54:42,189 [INFO] Step[950/2713]: training loss : 0.9257658433914184 TRAIN  loss dict:  {'classification_loss': 0.9257658433914184}
2025-01-15 13:54:57,541 [INFO] Step[1000/2713]: training loss : 0.9258594930171966 TRAIN  loss dict:  {'classification_loss': 0.9258594930171966}
2025-01-15 13:55:13,855 [INFO] Step[1050/2713]: training loss : 0.9260676681995392 TRAIN  loss dict:  {'classification_loss': 0.9260676681995392}
2025-01-15 13:55:27,408 [INFO] Step[1100/2713]: training loss : 0.9257330799102783 TRAIN  loss dict:  {'classification_loss': 0.9257330799102783}
2025-01-15 13:55:41,052 [INFO] Step[1150/2713]: training loss : 0.9259351956844329 TRAIN  loss dict:  {'classification_loss': 0.9259351956844329}
2025-01-15 13:55:54,494 [INFO] Step[1200/2713]: training loss : 0.9254545521736145 TRAIN  loss dict:  {'classification_loss': 0.9254545521736145}
2025-01-15 13:56:08,110 [INFO] Step[1250/2713]: training loss : 0.9260116600990296 TRAIN  loss dict:  {'classification_loss': 0.9260116600990296}
2025-01-15 13:56:22,023 [INFO] Step[1300/2713]: training loss : 0.9260964512825012 TRAIN  loss dict:  {'classification_loss': 0.9260964512825012}
2025-01-15 13:56:35,674 [INFO] Step[1350/2713]: training loss : 0.9257952868938446 TRAIN  loss dict:  {'classification_loss': 0.9257952868938446}
2025-01-15 13:56:49,749 [INFO] Step[1400/2713]: training loss : 0.9256353545188903 TRAIN  loss dict:  {'classification_loss': 0.9256353545188903}
2025-01-15 13:57:04,024 [INFO] Step[1450/2713]: training loss : 0.9256725406646729 TRAIN  loss dict:  {'classification_loss': 0.9256725406646729}
2025-01-15 13:57:18,156 [INFO] Step[1500/2713]: training loss : 0.9257999587059021 TRAIN  loss dict:  {'classification_loss': 0.9257999587059021}
2025-01-15 13:57:31,994 [INFO] Step[1550/2713]: training loss : 0.9262993109226226 TRAIN  loss dict:  {'classification_loss': 0.9262993109226226}
2025-01-15 13:57:45,528 [INFO] Step[1600/2713]: training loss : 0.925812258720398 TRAIN  loss dict:  {'classification_loss': 0.925812258720398}
2025-01-15 13:57:59,025 [INFO] Step[1650/2713]: training loss : 0.9261366021633148 TRAIN  loss dict:  {'classification_loss': 0.9261366021633148}
2025-01-15 13:58:12,440 [INFO] Step[1700/2713]: training loss : 0.9260003125667572 TRAIN  loss dict:  {'classification_loss': 0.9260003125667572}
2025-01-15 13:58:25,751 [INFO] Step[1750/2713]: training loss : 0.9263246309757233 TRAIN  loss dict:  {'classification_loss': 0.9263246309757233}
2025-01-15 13:58:39,843 [INFO] Step[1800/2713]: training loss : 0.9262306010723114 TRAIN  loss dict:  {'classification_loss': 0.9262306010723114}
2025-01-15 13:58:53,800 [INFO] Step[1850/2713]: training loss : 0.9262461698055268 TRAIN  loss dict:  {'classification_loss': 0.9262461698055268}
2025-01-15 13:59:07,420 [INFO] Step[1900/2713]: training loss : 0.9263489472866059 TRAIN  loss dict:  {'classification_loss': 0.9263489472866059}
2025-01-15 13:59:21,225 [INFO] Step[1950/2713]: training loss : 0.92631098985672 TRAIN  loss dict:  {'classification_loss': 0.92631098985672}
2025-01-15 13:59:34,556 [INFO] Step[2000/2713]: training loss : 0.925703397989273 TRAIN  loss dict:  {'classification_loss': 0.925703397989273}
2025-01-15 13:59:47,886 [INFO] Step[2050/2713]: training loss : 0.9259922778606415 TRAIN  loss dict:  {'classification_loss': 0.9259922778606415}
2025-01-15 14:00:01,497 [INFO] Step[2100/2713]: training loss : 0.9259923350811005 TRAIN  loss dict:  {'classification_loss': 0.9259923350811005}
2025-01-15 14:00:15,430 [INFO] Step[2150/2713]: training loss : 0.9258847773075104 TRAIN  loss dict:  {'classification_loss': 0.9258847773075104}
2025-01-15 14:00:29,027 [INFO] Step[2200/2713]: training loss : 0.9263023388385773 TRAIN  loss dict:  {'classification_loss': 0.9263023388385773}
2025-01-15 14:00:42,526 [INFO] Step[2250/2713]: training loss : 0.9258419787883758 TRAIN  loss dict:  {'classification_loss': 0.9258419787883758}
2025-01-15 14:00:56,223 [INFO] Step[2300/2713]: training loss : 0.9265575659275055 TRAIN  loss dict:  {'classification_loss': 0.9265575659275055}
2025-01-15 14:01:10,303 [INFO] Step[2350/2713]: training loss : 0.925976300239563 TRAIN  loss dict:  {'classification_loss': 0.925976300239563}
2025-01-15 14:01:23,826 [INFO] Step[2400/2713]: training loss : 0.9261267781257629 TRAIN  loss dict:  {'classification_loss': 0.9261267781257629}
2025-01-15 14:01:37,144 [INFO] Step[2450/2713]: training loss : 0.925519790649414 TRAIN  loss dict:  {'classification_loss': 0.925519790649414}
2025-01-15 14:01:50,782 [INFO] Step[2500/2713]: training loss : 0.9259024596214295 TRAIN  loss dict:  {'classification_loss': 0.9259024596214295}
2025-01-15 14:02:04,882 [INFO] Step[2550/2713]: training loss : 0.9257587885856629 TRAIN  loss dict:  {'classification_loss': 0.9257587885856629}
2025-01-15 14:02:18,709 [INFO] Step[2600/2713]: training loss : 0.9258035099506379 TRAIN  loss dict:  {'classification_loss': 0.9258035099506379}
2025-01-15 14:02:31,998 [INFO] Step[2650/2713]: training loss : 0.9259488141536713 TRAIN  loss dict:  {'classification_loss': 0.9259488141536713}
2025-01-15 14:02:45,324 [INFO] Step[2700/2713]: training loss : 0.9259671807289124 TRAIN  loss dict:  {'classification_loss': 0.9259671807289124}
2025-01-15 14:04:01,655 [INFO] Label accuracies statistics:
2025-01-15 14:04:01,656 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 14:04:02,883 [INFO] [116] TRAIN  loss: 0.9260037347174552 acc: 1.0
2025-01-15 14:04:02,883 [INFO] [116] TRAIN  loss dict: {'classification_loss': 0.9260037347174552}
2025-01-15 14:04:02,883 [INFO] [116] VALIDATION loss: 1.7449298899872858 VALIDATION acc: 0.8225705329153605
2025-01-15 14:04:02,883 [INFO] [116] VALIDATION loss dict: {'classification_loss': 1.7449298899872858}
2025-01-15 14:04:02,883 [INFO] 
2025-01-15 14:04:21,726 [INFO] Step[50/2713]: training loss : 0.925742347240448 TRAIN  loss dict:  {'classification_loss': 0.925742347240448}
2025-01-15 14:04:35,335 [INFO] Step[100/2713]: training loss : 0.9263085675239563 TRAIN  loss dict:  {'classification_loss': 0.9263085675239563}
2025-01-15 14:04:49,140 [INFO] Step[150/2713]: training loss : 0.9262564039230347 TRAIN  loss dict:  {'classification_loss': 0.9262564039230347}
2025-01-15 14:05:02,964 [INFO] Step[200/2713]: training loss : 0.926224205493927 TRAIN  loss dict:  {'classification_loss': 0.926224205493927}
2025-01-15 14:05:16,522 [INFO] Step[250/2713]: training loss : 0.9260615122318268 TRAIN  loss dict:  {'classification_loss': 0.9260615122318268}
2025-01-15 14:05:30,121 [INFO] Step[300/2713]: training loss : 0.926144380569458 TRAIN  loss dict:  {'classification_loss': 0.926144380569458}
2025-01-15 14:05:43,999 [INFO] Step[350/2713]: training loss : 0.9256341052055359 TRAIN  loss dict:  {'classification_loss': 0.9256341052055359}
2025-01-15 14:05:57,880 [INFO] Step[400/2713]: training loss : 0.9324323451519012 TRAIN  loss dict:  {'classification_loss': 0.9324323451519012}
2025-01-15 14:06:11,517 [INFO] Step[450/2713]: training loss : 0.9261931228637695 TRAIN  loss dict:  {'classification_loss': 0.9261931228637695}
2025-01-15 14:06:25,067 [INFO] Step[500/2713]: training loss : 0.926443657875061 TRAIN  loss dict:  {'classification_loss': 0.926443657875061}
2025-01-15 14:06:38,324 [INFO] Step[550/2713]: training loss : 0.9258992624282837 TRAIN  loss dict:  {'classification_loss': 0.9258992624282837}
2025-01-15 14:06:52,306 [INFO] Step[600/2713]: training loss : 0.9277202033996582 TRAIN  loss dict:  {'classification_loss': 0.9277202033996582}
2025-01-15 14:07:06,106 [INFO] Step[650/2713]: training loss : 0.9258722054958344 TRAIN  loss dict:  {'classification_loss': 0.9258722054958344}
2025-01-15 14:07:20,094 [INFO] Step[700/2713]: training loss : 0.9259307253360748 TRAIN  loss dict:  {'classification_loss': 0.9259307253360748}
2025-01-15 14:07:33,945 [INFO] Step[750/2713]: training loss : 0.9258126974105835 TRAIN  loss dict:  {'classification_loss': 0.9258126974105835}
2025-01-15 14:07:47,534 [INFO] Step[800/2713]: training loss : 0.9261627423763276 TRAIN  loss dict:  {'classification_loss': 0.9261627423763276}
2025-01-15 14:08:00,781 [INFO] Step[850/2713]: training loss : 0.9257585322856903 TRAIN  loss dict:  {'classification_loss': 0.9257585322856903}
2025-01-15 14:08:14,014 [INFO] Step[900/2713]: training loss : 0.9264063608646392 TRAIN  loss dict:  {'classification_loss': 0.9264063608646392}
2025-01-15 14:08:27,280 [INFO] Step[950/2713]: training loss : 0.9262293815612793 TRAIN  loss dict:  {'classification_loss': 0.9262293815612793}
2025-01-15 14:08:41,565 [INFO] Step[1000/2713]: training loss : 0.9260051596164703 TRAIN  loss dict:  {'classification_loss': 0.9260051596164703}
2025-01-15 14:08:55,397 [INFO] Step[1050/2713]: training loss : 0.9259115827083587 TRAIN  loss dict:  {'classification_loss': 0.9259115827083587}
2025-01-15 14:09:09,688 [INFO] Step[1100/2713]: training loss : 0.9262409138679505 TRAIN  loss dict:  {'classification_loss': 0.9262409138679505}
2025-01-15 14:09:23,606 [INFO] Step[1150/2713]: training loss : 0.9271547663211822 TRAIN  loss dict:  {'classification_loss': 0.9271547663211822}
2025-01-15 14:09:37,117 [INFO] Step[1200/2713]: training loss : 0.9261337339878082 TRAIN  loss dict:  {'classification_loss': 0.9261337339878082}
2025-01-15 14:09:50,373 [INFO] Step[1250/2713]: training loss : 0.9259798181056976 TRAIN  loss dict:  {'classification_loss': 0.9259798181056976}
2025-01-15 14:10:04,053 [INFO] Step[1300/2713]: training loss : 0.9282149791717529 TRAIN  loss dict:  {'classification_loss': 0.9282149791717529}
2025-01-15 14:10:18,299 [INFO] Step[1350/2713]: training loss : 0.9261271286010743 TRAIN  loss dict:  {'classification_loss': 0.9261271286010743}
2025-01-15 14:10:32,100 [INFO] Step[1400/2713]: training loss : 0.948557118177414 TRAIN  loss dict:  {'classification_loss': 0.948557118177414}
2025-01-15 14:10:46,375 [INFO] Step[1450/2713]: training loss : 0.951362601518631 TRAIN  loss dict:  {'classification_loss': 0.951362601518631}
2025-01-15 14:11:00,337 [INFO] Step[1500/2713]: training loss : 0.9257473421096801 TRAIN  loss dict:  {'classification_loss': 0.9257473421096801}
2025-01-15 14:11:13,886 [INFO] Step[1550/2713]: training loss : 0.9261689209938049 TRAIN  loss dict:  {'classification_loss': 0.9261689209938049}
2025-01-15 14:11:27,387 [INFO] Step[1600/2713]: training loss : 0.9253668832778931 TRAIN  loss dict:  {'classification_loss': 0.9253668832778931}
2025-01-15 14:11:41,228 [INFO] Step[1650/2713]: training loss : 0.925738297700882 TRAIN  loss dict:  {'classification_loss': 0.925738297700882}
2025-01-15 14:11:54,844 [INFO] Step[1700/2713]: training loss : 0.925893850326538 TRAIN  loss dict:  {'classification_loss': 0.925893850326538}
2025-01-15 14:12:08,652 [INFO] Step[1750/2713]: training loss : 0.9262504243850708 TRAIN  loss dict:  {'classification_loss': 0.9262504243850708}
2025-01-15 14:12:21,857 [INFO] Step[1800/2713]: training loss : 0.9260366928577423 TRAIN  loss dict:  {'classification_loss': 0.9260366928577423}
2025-01-15 14:12:35,107 [INFO] Step[1850/2713]: training loss : 0.9260429167747497 TRAIN  loss dict:  {'classification_loss': 0.9260429167747497}
2025-01-15 14:12:51,507 [INFO] Step[1900/2713]: training loss : 0.9257610273361206 TRAIN  loss dict:  {'classification_loss': 0.9257610273361206}
2025-01-15 14:13:05,501 [INFO] Step[1950/2713]: training loss : 0.926044555902481 TRAIN  loss dict:  {'classification_loss': 0.926044555902481}
2025-01-15 14:13:19,207 [INFO] Step[2000/2713]: training loss : 0.9258893179893494 TRAIN  loss dict:  {'classification_loss': 0.9258893179893494}
2025-01-15 14:13:32,873 [INFO] Step[2050/2713]: training loss : 0.9263645291328431 TRAIN  loss dict:  {'classification_loss': 0.9263645291328431}
2025-01-15 14:13:46,847 [INFO] Step[2100/2713]: training loss : 0.926230376958847 TRAIN  loss dict:  {'classification_loss': 0.926230376958847}
2025-01-15 14:14:00,558 [INFO] Step[2150/2713]: training loss : 0.9256793260574341 TRAIN  loss dict:  {'classification_loss': 0.9256793260574341}
2025-01-15 14:14:14,432 [INFO] Step[2200/2713]: training loss : 0.9260807132720947 TRAIN  loss dict:  {'classification_loss': 0.9260807132720947}
2025-01-15 14:14:28,349 [INFO] Step[2250/2713]: training loss : 0.9257673346996307 TRAIN  loss dict:  {'classification_loss': 0.9257673346996307}
2025-01-15 14:14:41,940 [INFO] Step[2300/2713]: training loss : 0.9259275782108307 TRAIN  loss dict:  {'classification_loss': 0.9259275782108307}
2025-01-15 14:14:55,827 [INFO] Step[2350/2713]: training loss : 0.9257921683788299 TRAIN  loss dict:  {'classification_loss': 0.9257921683788299}
2025-01-15 14:15:09,977 [INFO] Step[2400/2713]: training loss : 0.9262609350681305 TRAIN  loss dict:  {'classification_loss': 0.9262609350681305}
2025-01-15 14:15:23,630 [INFO] Step[2450/2713]: training loss : 0.9259678745269775 TRAIN  loss dict:  {'classification_loss': 0.9259678745269775}
2025-01-15 14:15:37,157 [INFO] Step[2500/2713]: training loss : 0.9262623310089111 TRAIN  loss dict:  {'classification_loss': 0.9262623310089111}
2025-01-15 14:15:51,452 [INFO] Step[2550/2713]: training loss : 0.9264053153991699 TRAIN  loss dict:  {'classification_loss': 0.9264053153991699}
2025-01-15 14:16:05,693 [INFO] Step[2600/2713]: training loss : 0.9265255427360535 TRAIN  loss dict:  {'classification_loss': 0.9265255427360535}
2025-01-15 14:16:19,454 [INFO] Step[2650/2713]: training loss : 0.9262650871276855 TRAIN  loss dict:  {'classification_loss': 0.9262650871276855}
2025-01-15 14:16:33,487 [INFO] Step[2700/2713]: training loss : 0.9253651142120362 TRAIN  loss dict:  {'classification_loss': 0.9253651142120362}
2025-01-15 14:17:49,787 [INFO] Label accuracies statistics:
2025-01-15 14:17:49,788 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.25, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 14:17:49,790 [INFO] [117] TRAIN  loss: 0.9271216288379229 acc: 0.9997542695662858
2025-01-15 14:17:49,790 [INFO] [117] TRAIN  loss dict: {'classification_loss': 0.9271216288379229}
2025-01-15 14:17:49,790 [INFO] [117] VALIDATION loss: 1.759018572313445 VALIDATION acc: 0.8200626959247649
2025-01-15 14:17:49,790 [INFO] [117] VALIDATION loss dict: {'classification_loss': 1.759018572313445}
2025-01-15 14:17:49,790 [INFO] 
2025-01-15 14:18:08,354 [INFO] Step[50/2713]: training loss : 0.9257696378231048 TRAIN  loss dict:  {'classification_loss': 0.9257696378231048}
2025-01-15 14:18:22,140 [INFO] Step[100/2713]: training loss : 0.9256981658935547 TRAIN  loss dict:  {'classification_loss': 0.9256981658935547}
2025-01-15 14:18:35,420 [INFO] Step[150/2713]: training loss : 0.9258442723751068 TRAIN  loss dict:  {'classification_loss': 0.9258442723751068}
2025-01-15 14:18:49,313 [INFO] Step[200/2713]: training loss : 0.9260224449634552 TRAIN  loss dict:  {'classification_loss': 0.9260224449634552}
2025-01-15 14:19:02,709 [INFO] Step[250/2713]: training loss : 0.9262377655506134 TRAIN  loss dict:  {'classification_loss': 0.9262377655506134}
2025-01-15 14:19:16,300 [INFO] Step[300/2713]: training loss : 0.9258102869987488 TRAIN  loss dict:  {'classification_loss': 0.9258102869987488}
2025-01-15 14:19:29,635 [INFO] Step[350/2713]: training loss : 0.9262110900878906 TRAIN  loss dict:  {'classification_loss': 0.9262110900878906}
2025-01-15 14:19:42,878 [INFO] Step[400/2713]: training loss : 0.9259758162498474 TRAIN  loss dict:  {'classification_loss': 0.9259758162498474}
2025-01-15 14:19:56,888 [INFO] Step[450/2713]: training loss : 0.9264667558670044 TRAIN  loss dict:  {'classification_loss': 0.9264667558670044}
2025-01-15 14:20:10,800 [INFO] Step[500/2713]: training loss : 0.9259839236736298 TRAIN  loss dict:  {'classification_loss': 0.9259839236736298}
2025-01-15 14:20:25,025 [INFO] Step[550/2713]: training loss : 0.9255755591392517 TRAIN  loss dict:  {'classification_loss': 0.9255755591392517}
2025-01-15 14:20:38,626 [INFO] Step[600/2713]: training loss : 0.926200476884842 TRAIN  loss dict:  {'classification_loss': 0.926200476884842}
2025-01-15 14:20:52,319 [INFO] Step[650/2713]: training loss : 0.9257196521759034 TRAIN  loss dict:  {'classification_loss': 0.9257196521759034}
2025-01-15 14:21:05,896 [INFO] Step[700/2713]: training loss : 0.9261628937721252 TRAIN  loss dict:  {'classification_loss': 0.9261628937721252}
2025-01-15 14:21:19,356 [INFO] Step[750/2713]: training loss : 0.9263255059719085 TRAIN  loss dict:  {'classification_loss': 0.9263255059719085}
2025-01-15 14:21:32,722 [INFO] Step[800/2713]: training loss : 0.925816707611084 TRAIN  loss dict:  {'classification_loss': 0.925816707611084}
2025-01-15 14:21:45,937 [INFO] Step[850/2713]: training loss : 0.9256819009780883 TRAIN  loss dict:  {'classification_loss': 0.9256819009780883}
2025-01-15 14:21:59,138 [INFO] Step[900/2713]: training loss : 0.9258258676528931 TRAIN  loss dict:  {'classification_loss': 0.9258258676528931}
2025-01-15 14:22:13,086 [INFO] Step[950/2713]: training loss : 0.9261109781265259 TRAIN  loss dict:  {'classification_loss': 0.9261109781265259}
2025-01-15 14:22:27,032 [INFO] Step[1000/2713]: training loss : 0.9256406128406525 TRAIN  loss dict:  {'classification_loss': 0.9256406128406525}
2025-01-15 14:22:40,722 [INFO] Step[1050/2713]: training loss : 0.9260794937610626 TRAIN  loss dict:  {'classification_loss': 0.9260794937610626}
2025-01-15 14:22:54,677 [INFO] Step[1100/2713]: training loss : 0.9263539063930512 TRAIN  loss dict:  {'classification_loss': 0.9263539063930512}
2025-01-15 14:23:08,481 [INFO] Step[1150/2713]: training loss : 0.9256854236125946 TRAIN  loss dict:  {'classification_loss': 0.9256854236125946}
2025-01-15 14:23:22,014 [INFO] Step[1200/2713]: training loss : 0.9256543040275573 TRAIN  loss dict:  {'classification_loss': 0.9256543040275573}
2025-01-15 14:23:35,307 [INFO] Step[1250/2713]: training loss : 0.925836592912674 TRAIN  loss dict:  {'classification_loss': 0.925836592912674}
2025-01-15 14:23:49,469 [INFO] Step[1300/2713]: training loss : 0.9259728825092316 TRAIN  loss dict:  {'classification_loss': 0.9259728825092316}
2025-01-15 14:24:02,718 [INFO] Step[1350/2713]: training loss : 0.9261634528636933 TRAIN  loss dict:  {'classification_loss': 0.9261634528636933}
2025-01-15 14:24:16,246 [INFO] Step[1400/2713]: training loss : 0.9255988705158233 TRAIN  loss dict:  {'classification_loss': 0.9255988705158233}
2025-01-15 14:24:29,722 [INFO] Step[1450/2713]: training loss : 0.9259650456905365 TRAIN  loss dict:  {'classification_loss': 0.9259650456905365}
2025-01-15 14:24:43,300 [INFO] Step[1500/2713]: training loss : 0.926167528629303 TRAIN  loss dict:  {'classification_loss': 0.926167528629303}
2025-01-15 14:24:57,347 [INFO] Step[1550/2713]: training loss : 0.9262478291988373 TRAIN  loss dict:  {'classification_loss': 0.9262478291988373}
2025-01-15 14:25:10,664 [INFO] Step[1600/2713]: training loss : 0.925942599773407 TRAIN  loss dict:  {'classification_loss': 0.925942599773407}
2025-01-15 14:25:24,283 [INFO] Step[1650/2713]: training loss : 0.925951544046402 TRAIN  loss dict:  {'classification_loss': 0.925951544046402}
2025-01-15 14:25:37,428 [INFO] Step[1700/2713]: training loss : 0.9260131752490998 TRAIN  loss dict:  {'classification_loss': 0.9260131752490998}
2025-01-15 14:25:51,244 [INFO] Step[1750/2713]: training loss : 0.9257872641086579 TRAIN  loss dict:  {'classification_loss': 0.9257872641086579}
2025-01-15 14:26:04,876 [INFO] Step[1800/2713]: training loss : 0.9271978294849396 TRAIN  loss dict:  {'classification_loss': 0.9271978294849396}
2025-01-15 14:26:18,488 [INFO] Step[1850/2713]: training loss : 0.9260028648376465 TRAIN  loss dict:  {'classification_loss': 0.9260028648376465}
2025-01-15 14:26:32,807 [INFO] Step[1900/2713]: training loss : 0.9256033480167389 TRAIN  loss dict:  {'classification_loss': 0.9256033480167389}
2025-01-15 14:26:46,563 [INFO] Step[1950/2713]: training loss : 0.9259667313098907 TRAIN  loss dict:  {'classification_loss': 0.9259667313098907}
2025-01-15 14:27:00,241 [INFO] Step[2000/2713]: training loss : 0.9257121014595032 TRAIN  loss dict:  {'classification_loss': 0.9257121014595032}
2025-01-15 14:27:13,839 [INFO] Step[2050/2713]: training loss : 0.9260371875762939 TRAIN  loss dict:  {'classification_loss': 0.9260371875762939}
2025-01-15 14:27:27,285 [INFO] Step[2100/2713]: training loss : 0.9257002282142639 TRAIN  loss dict:  {'classification_loss': 0.9257002282142639}
2025-01-15 14:27:40,781 [INFO] Step[2150/2713]: training loss : 0.9256867277622223 TRAIN  loss dict:  {'classification_loss': 0.9256867277622223}
2025-01-15 14:27:54,545 [INFO] Step[2200/2713]: training loss : 0.9265577399730682 TRAIN  loss dict:  {'classification_loss': 0.9265577399730682}
2025-01-15 14:28:07,966 [INFO] Step[2250/2713]: training loss : 0.9260152232646942 TRAIN  loss dict:  {'classification_loss': 0.9260152232646942}
2025-01-15 14:28:21,276 [INFO] Step[2300/2713]: training loss : 0.9255627584457398 TRAIN  loss dict:  {'classification_loss': 0.9255627584457398}
2025-01-15 14:28:34,680 [INFO] Step[2350/2713]: training loss : 0.9258075761795044 TRAIN  loss dict:  {'classification_loss': 0.9258075761795044}
2025-01-15 14:28:48,240 [INFO] Step[2400/2713]: training loss : 0.925710768699646 TRAIN  loss dict:  {'classification_loss': 0.925710768699646}
2025-01-15 14:29:01,811 [INFO] Step[2450/2713]: training loss : 0.9259294414520264 TRAIN  loss dict:  {'classification_loss': 0.9259294414520264}
2025-01-15 14:29:15,299 [INFO] Step[2500/2713]: training loss : 0.9260208344459534 TRAIN  loss dict:  {'classification_loss': 0.9260208344459534}
2025-01-15 14:29:28,999 [INFO] Step[2550/2713]: training loss : 0.9263132858276367 TRAIN  loss dict:  {'classification_loss': 0.9263132858276367}
2025-01-15 14:29:42,706 [INFO] Step[2600/2713]: training loss : 0.9261393547058105 TRAIN  loss dict:  {'classification_loss': 0.9261393547058105}
2025-01-15 14:29:56,560 [INFO] Step[2650/2713]: training loss : 0.926140456199646 TRAIN  loss dict:  {'classification_loss': 0.926140456199646}
2025-01-15 14:30:10,168 [INFO] Step[2700/2713]: training loss : 0.9257963764667511 TRAIN  loss dict:  {'classification_loss': 0.9257963764667511}
2025-01-15 14:32:35,244 [INFO] Label accuracies statistics:
2025-01-15 14:32:35,244 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 14:32:35,251 [INFO] [118] TRAIN  loss: 0.9259714268688957 acc: 1.0
2025-01-15 14:32:35,252 [INFO] [118] TRAIN  loss dict: {'classification_loss': 0.9259714268688957}
2025-01-15 14:32:35,252 [INFO] [118] VALIDATION loss: 1.7626834561053972 VALIDATION acc: 0.8200626959247649
2025-01-15 14:32:35,253 [INFO] [118] VALIDATION loss dict: {'classification_loss': 1.7626834561053972}
2025-01-15 14:32:35,253 [INFO] 
2025-01-15 14:32:58,883 [INFO] Step[50/2713]: training loss : 0.9258427453041077 TRAIN  loss dict:  {'classification_loss': 0.9258427453041077}
2025-01-15 14:33:12,835 [INFO] Step[100/2713]: training loss : 0.9258456015586853 TRAIN  loss dict:  {'classification_loss': 0.9258456015586853}
2025-01-15 14:33:26,467 [INFO] Step[150/2713]: training loss : 0.9256919634342193 TRAIN  loss dict:  {'classification_loss': 0.9256919634342193}
2025-01-15 14:33:40,551 [INFO] Step[200/2713]: training loss : 0.9258928561210632 TRAIN  loss dict:  {'classification_loss': 0.9258928561210632}
2025-01-15 14:33:54,309 [INFO] Step[250/2713]: training loss : 0.9260484933853149 TRAIN  loss dict:  {'classification_loss': 0.9260484933853149}
2025-01-15 14:34:08,219 [INFO] Step[300/2713]: training loss : 0.9261883330345154 TRAIN  loss dict:  {'classification_loss': 0.9261883330345154}
2025-01-15 14:34:21,870 [INFO] Step[350/2713]: training loss : 0.925868912935257 TRAIN  loss dict:  {'classification_loss': 0.925868912935257}
2025-01-15 14:34:35,242 [INFO] Step[400/2713]: training loss : 0.9267723321914673 TRAIN  loss dict:  {'classification_loss': 0.9267723321914673}
2025-01-15 14:34:48,820 [INFO] Step[450/2713]: training loss : 0.9260376441478729 TRAIN  loss dict:  {'classification_loss': 0.9260376441478729}
2025-01-15 14:35:02,484 [INFO] Step[500/2713]: training loss : 0.9265971183776855 TRAIN  loss dict:  {'classification_loss': 0.9265971183776855}
2025-01-15 14:35:16,156 [INFO] Step[550/2713]: training loss : 0.9259672212600708 TRAIN  loss dict:  {'classification_loss': 0.9259672212600708}
2025-01-15 14:35:29,840 [INFO] Step[600/2713]: training loss : 0.9264346742630005 TRAIN  loss dict:  {'classification_loss': 0.9264346742630005}
2025-01-15 14:35:43,557 [INFO] Step[650/2713]: training loss : 0.9261766207218171 TRAIN  loss dict:  {'classification_loss': 0.9261766207218171}
2025-01-15 14:35:57,469 [INFO] Step[700/2713]: training loss : 0.9256181716918945 TRAIN  loss dict:  {'classification_loss': 0.9256181716918945}
2025-01-15 14:36:11,062 [INFO] Step[750/2713]: training loss : 0.9259745812416077 TRAIN  loss dict:  {'classification_loss': 0.9259745812416077}
2025-01-15 14:36:24,667 [INFO] Step[800/2713]: training loss : 0.9258660542964935 TRAIN  loss dict:  {'classification_loss': 0.9258660542964935}
2025-01-15 14:36:38,531 [INFO] Step[850/2713]: training loss : 0.9261681735515594 TRAIN  loss dict:  {'classification_loss': 0.9261681735515594}
2025-01-15 14:36:52,284 [INFO] Step[900/2713]: training loss : 0.9262037551403046 TRAIN  loss dict:  {'classification_loss': 0.9262037551403046}
2025-01-15 14:37:05,473 [INFO] Step[950/2713]: training loss : 0.9262489998340606 TRAIN  loss dict:  {'classification_loss': 0.9262489998340606}
2025-01-15 14:37:18,910 [INFO] Step[1000/2713]: training loss : 0.925752819776535 TRAIN  loss dict:  {'classification_loss': 0.925752819776535}
2025-01-15 14:37:32,851 [INFO] Step[1050/2713]: training loss : 0.9257146179676056 TRAIN  loss dict:  {'classification_loss': 0.9257146179676056}
2025-01-15 14:37:46,666 [INFO] Step[1100/2713]: training loss : 0.9257319939136505 TRAIN  loss dict:  {'classification_loss': 0.9257319939136505}
2025-01-15 14:40:55,653 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 3a 01dr)...


2025-01-15 14:41:19,920 [INFO] Step[50/2713]: training loss : 0.9292225050926208 TRAIN  loss dict:  {'classification_loss': 0.9292225050926208}
2025-01-15 14:41:31,957 [INFO] Step[100/2713]: training loss : 0.9333442914485931 TRAIN  loss dict:  {'classification_loss': 0.9333442914485931}
2025-01-15 14:41:43,980 [INFO] Step[150/2713]: training loss : 0.9323605811595916 TRAIN  loss dict:  {'classification_loss': 0.9323605811595916}
2025-01-15 14:41:55,996 [INFO] Step[200/2713]: training loss : 0.9781887221336365 TRAIN  loss dict:  {'classification_loss': 0.9781887221336365}
2025-01-15 14:42:08,052 [INFO] Step[250/2713]: training loss : 0.9354072034358978 TRAIN  loss dict:  {'classification_loss': 0.9354072034358978}
2025-01-15 14:42:20,031 [INFO] Step[300/2713]: training loss : 0.9759670209884643 TRAIN  loss dict:  {'classification_loss': 0.9759670209884643}
2025-01-15 14:42:32,068 [INFO] Step[350/2713]: training loss : 0.9371207225322723 TRAIN  loss dict:  {'classification_loss': 0.9371207225322723}
2025-01-15 14:42:44,072 [INFO] Step[400/2713]: training loss : 0.9380376815795899 TRAIN  loss dict:  {'classification_loss': 0.9380376815795899}
2025-01-15 14:42:56,054 [INFO] Step[450/2713]: training loss : 0.9389772069454193 TRAIN  loss dict:  {'classification_loss': 0.9389772069454193}
2025-01-15 14:43:08,112 [INFO] Step[500/2713]: training loss : 0.9381170475482941 TRAIN  loss dict:  {'classification_loss': 0.9381170475482941}
2025-01-15 14:43:20,174 [INFO] Step[550/2713]: training loss : 0.9448021709918976 TRAIN  loss dict:  {'classification_loss': 0.9448021709918976}
2025-01-15 14:43:32,270 [INFO] Step[600/2713]: training loss : 0.9810508251190185 TRAIN  loss dict:  {'classification_loss': 0.9810508251190185}
2025-01-15 14:43:44,249 [INFO] Step[650/2713]: training loss : 0.977006413936615 TRAIN  loss dict:  {'classification_loss': 0.977006413936615}
2025-01-15 14:43:56,315 [INFO] Step[700/2713]: training loss : 0.9616212260723114 TRAIN  loss dict:  {'classification_loss': 0.9616212260723114}
2025-01-15 14:44:08,386 [INFO] Step[750/2713]: training loss : 0.9678610932826995 TRAIN  loss dict:  {'classification_loss': 0.9678610932826995}
2025-01-15 14:50:45,336 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 3a 01dr)...


2025-01-15 14:51:09,930 [INFO] Step[50/2713]: training loss : 0.9292208361625671 TRAIN  loss dict:  {'classification_loss': 0.9292208361625671}
2025-01-15 14:51:21,644 [INFO] Step[100/2713]: training loss : 0.9332342827320099 TRAIN  loss dict:  {'classification_loss': 0.9332342827320099}
2025-01-15 14:51:33,436 [INFO] Step[150/2713]: training loss : 0.9324943816661835 TRAIN  loss dict:  {'classification_loss': 0.9324943816661835}
2025-01-15 14:51:45,227 [INFO] Step[200/2713]: training loss : 0.9782751393318176 TRAIN  loss dict:  {'classification_loss': 0.9782751393318176}
2025-01-15 14:51:57,062 [INFO] Step[250/2713]: training loss : 0.9367187464237213 TRAIN  loss dict:  {'classification_loss': 0.9367187464237213}
2025-01-15 14:52:08,930 [INFO] Step[300/2713]: training loss : 0.9819848227500916 TRAIN  loss dict:  {'classification_loss': 0.9819848227500916}
2025-01-15 14:52:20,821 [INFO] Step[350/2713]: training loss : 0.9377227342128753 TRAIN  loss dict:  {'classification_loss': 0.9377227342128753}
2025-01-15 14:52:32,680 [INFO] Step[400/2713]: training loss : 0.9384977984428405 TRAIN  loss dict:  {'classification_loss': 0.9384977984428405}
2025-01-15 14:52:44,554 [INFO] Step[450/2713]: training loss : 0.9393482637405396 TRAIN  loss dict:  {'classification_loss': 0.9393482637405396}
2025-01-15 14:52:56,383 [INFO] Step[500/2713]: training loss : 0.9362705934047699 TRAIN  loss dict:  {'classification_loss': 0.9362705934047699}
2025-01-15 14:53:08,268 [INFO] Step[550/2713]: training loss : 0.9717049932479859 TRAIN  loss dict:  {'classification_loss': 0.9717049932479859}
2025-01-15 14:53:20,113 [INFO] Step[600/2713]: training loss : 0.9771438133716583 TRAIN  loss dict:  {'classification_loss': 0.9771438133716583}
2025-01-15 14:53:31,982 [INFO] Step[650/2713]: training loss : 0.9796754574775696 TRAIN  loss dict:  {'classification_loss': 0.9796754574775696}
2025-01-15 14:53:43,856 [INFO] Step[700/2713]: training loss : 0.9396099889278412 TRAIN  loss dict:  {'classification_loss': 0.9396099889278412}
2025-01-15 14:53:55,732 [INFO] Step[750/2713]: training loss : 0.9398639810085296 TRAIN  loss dict:  {'classification_loss': 0.9398639810085296}
2025-01-15 14:54:07,598 [INFO] Step[800/2713]: training loss : 0.9549078667163848 TRAIN  loss dict:  {'classification_loss': 0.9549078667163848}
2025-01-15 14:54:19,495 [INFO] Step[850/2713]: training loss : 0.9505624747276307 TRAIN  loss dict:  {'classification_loss': 0.9505624747276307}
2025-01-15 14:54:31,380 [INFO] Step[900/2713]: training loss : 0.9823103749752045 TRAIN  loss dict:  {'classification_loss': 0.9823103749752045}
2025-01-15 14:54:43,245 [INFO] Step[950/2713]: training loss : 1.0890723156929016 TRAIN  loss dict:  {'classification_loss': 1.0890723156929016}
2025-01-15 14:54:55,127 [INFO] Step[1000/2713]: training loss : 0.9823418807983398 TRAIN  loss dict:  {'classification_loss': 0.9823418807983398}
2025-01-15 14:55:07,028 [INFO] Step[1050/2713]: training loss : 0.9768542313575744 TRAIN  loss dict:  {'classification_loss': 0.9768542313575744}
2025-01-15 14:55:19,003 [INFO] Step[1100/2713]: training loss : 0.9601227462291717 TRAIN  loss dict:  {'classification_loss': 0.9601227462291717}
2025-01-15 14:55:30,941 [INFO] Step[1150/2713]: training loss : 0.9767198371887207 TRAIN  loss dict:  {'classification_loss': 0.9767198371887207}
2025-01-15 14:55:42,843 [INFO] Step[1200/2713]: training loss : 0.9867328321933746 TRAIN  loss dict:  {'classification_loss': 0.9867328321933746}
2025-01-15 14:55:54,753 [INFO] Step[1250/2713]: training loss : 0.9573654913902283 TRAIN  loss dict:  {'classification_loss': 0.9573654913902283}
2025-01-15 14:56:06,612 [INFO] Step[1300/2713]: training loss : 1.0289091169834137 TRAIN  loss dict:  {'classification_loss': 1.0289091169834137}
2025-01-15 14:56:18,518 [INFO] Step[1350/2713]: training loss : 0.9561769950389862 TRAIN  loss dict:  {'classification_loss': 0.9561769950389862}
2025-01-15 14:56:30,395 [INFO] Step[1400/2713]: training loss : 0.9476447808742523 TRAIN  loss dict:  {'classification_loss': 0.9476447808742523}
2025-01-15 14:56:42,306 [INFO] Step[1450/2713]: training loss : 0.9861374413967132 TRAIN  loss dict:  {'classification_loss': 0.9861374413967132}
2025-01-15 14:56:54,227 [INFO] Step[1500/2713]: training loss : 0.977271066904068 TRAIN  loss dict:  {'classification_loss': 0.977271066904068}
2025-01-15 14:57:06,170 [INFO] Step[1550/2713]: training loss : 1.0132726955413818 TRAIN  loss dict:  {'classification_loss': 1.0132726955413818}
2025-01-15 14:57:18,078 [INFO] Step[1600/2713]: training loss : 0.9490563476085663 TRAIN  loss dict:  {'classification_loss': 0.9490563476085663}
2025-01-15 14:57:30,001 [INFO] Step[1650/2713]: training loss : 0.9615346384048462 TRAIN  loss dict:  {'classification_loss': 0.9615346384048462}
2025-01-15 14:57:41,936 [INFO] Step[1700/2713]: training loss : 0.9704623055458069 TRAIN  loss dict:  {'classification_loss': 0.9704623055458069}
2025-01-15 14:57:53,835 [INFO] Step[1750/2713]: training loss : 0.946307338476181 TRAIN  loss dict:  {'classification_loss': 0.946307338476181}
2025-01-15 14:58:05,699 [INFO] Step[1800/2713]: training loss : 0.9629568243026734 TRAIN  loss dict:  {'classification_loss': 0.9629568243026734}
2025-01-15 14:58:17,594 [INFO] Step[1850/2713]: training loss : 0.9470896279811859 TRAIN  loss dict:  {'classification_loss': 0.9470896279811859}
2025-01-15 14:58:29,498 [INFO] Step[1900/2713]: training loss : 0.9825504970550537 TRAIN  loss dict:  {'classification_loss': 0.9825504970550537}
2025-01-15 14:58:41,384 [INFO] Step[1950/2713]: training loss : 0.9729686915874481 TRAIN  loss dict:  {'classification_loss': 0.9729686915874481}
2025-01-15 14:58:53,349 [INFO] Step[2000/2713]: training loss : 1.0062745010852814 TRAIN  loss dict:  {'classification_loss': 1.0062745010852814}
2025-01-15 14:59:05,294 [INFO] Step[2050/2713]: training loss : 0.9633725297451019 TRAIN  loss dict:  {'classification_loss': 0.9633725297451019}
2025-01-15 14:59:17,177 [INFO] Step[2100/2713]: training loss : 1.1364241874217986 TRAIN  loss dict:  {'classification_loss': 1.1364241874217986}
2025-01-15 14:59:29,065 [INFO] Step[2150/2713]: training loss : 1.085873007774353 TRAIN  loss dict:  {'classification_loss': 1.085873007774353}
2025-01-15 14:59:40,962 [INFO] Step[2200/2713]: training loss : 1.1704317903518677 TRAIN  loss dict:  {'classification_loss': 1.1704317903518677}
2025-01-15 14:59:52,827 [INFO] Step[2250/2713]: training loss : 1.0057138681411744 TRAIN  loss dict:  {'classification_loss': 1.0057138681411744}
2025-01-15 15:00:04,730 [INFO] Step[2300/2713]: training loss : 0.9592046880722046 TRAIN  loss dict:  {'classification_loss': 0.9592046880722046}
2025-01-15 15:00:16,656 [INFO] Step[2350/2713]: training loss : 0.9766417801380157 TRAIN  loss dict:  {'classification_loss': 0.9766417801380157}
2025-01-15 15:00:28,553 [INFO] Step[2400/2713]: training loss : 0.9565624380111695 TRAIN  loss dict:  {'classification_loss': 0.9565624380111695}
2025-01-15 15:00:40,441 [INFO] Step[2450/2713]: training loss : 0.9666320872306824 TRAIN  loss dict:  {'classification_loss': 0.9666320872306824}
2025-01-15 15:00:52,343 [INFO] Step[2500/2713]: training loss : 0.9574439978599548 TRAIN  loss dict:  {'classification_loss': 0.9574439978599548}
2025-01-15 15:01:04,221 [INFO] Step[2550/2713]: training loss : 1.0341661381721496 TRAIN  loss dict:  {'classification_loss': 1.0341661381721496}
2025-01-15 15:01:16,109 [INFO] Step[2600/2713]: training loss : 0.9893101608753204 TRAIN  loss dict:  {'classification_loss': 0.9893101608753204}
2025-01-15 15:01:28,006 [INFO] Step[2650/2713]: training loss : 0.9518913567066193 TRAIN  loss dict:  {'classification_loss': 0.9518913567066193}
2025-01-15 15:01:39,887 [INFO] Step[2700/2713]: training loss : 1.029747315645218 TRAIN  loss dict:  {'classification_loss': 1.029747315645218}
2025-01-15 15:02:55,265 [INFO] Label accuracies statistics:
2025-01-15 15:02:55,265 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.5, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.25, 69: 0.5, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.5, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 1.0, 87: 0.5, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 1.0, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 0.75, 138: 1.0, 139: 0.5, 140: 0.75, 141: 0.5, 142: 0.5, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.5, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.5, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.0, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 0.75, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.5, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.25, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.5, 227: 0.5, 228: 0.5, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 0.75, 242: 0.0, 243: 0.75, 244: 0.5, 245: 0.25, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.25, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.0, 268: 0.25, 269: 0.5, 270: 1.0, 271: 0.25, 272: 0.75, 273: 0.5, 274: 1.0, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.0, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.5, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.25, 313: 0.5, 314: 0.75, 315: 1.0, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 1.0, 324: 0.75, 325: 0.5, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.0, 355: 0.5, 356: 0.25, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.5, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.0, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 15:02:57,546 [INFO] [1] TRAIN  loss: 0.9787254141258481 acc: 0.9900479174345743
2025-01-15 15:02:57,546 [INFO] [1] TRAIN  loss dict: {'classification_loss': 0.9787254141258481}
2025-01-15 15:02:57,547 [INFO] [1] VALIDATION loss: 2.2020305143039027 VALIDATION acc: 0.7329153605015674
2025-01-15 15:02:57,547 [INFO] [1] VALIDATION loss dict: {'classification_loss': 2.2020305143039027}
2025-01-15 15:02:57,547 [INFO] 
2025-01-15 15:03:15,214 [INFO] Step[50/2713]: training loss : 0.9944559574127197 TRAIN  loss dict:  {'classification_loss': 0.9944559574127197}
2025-01-15 15:03:27,050 [INFO] Step[100/2713]: training loss : 0.9721021223068237 TRAIN  loss dict:  {'classification_loss': 0.9721021223068237}
2025-01-15 15:03:38,985 [INFO] Step[150/2713]: training loss : 1.0006875431537627 TRAIN  loss dict:  {'classification_loss': 1.0006875431537627}
2025-01-15 15:03:50,890 [INFO] Step[200/2713]: training loss : 0.9695572423934936 TRAIN  loss dict:  {'classification_loss': 0.9695572423934936}
2025-01-15 15:04:02,837 [INFO] Step[250/2713]: training loss : 0.9834229826927186 TRAIN  loss dict:  {'classification_loss': 0.9834229826927186}
2025-01-15 15:04:14,802 [INFO] Step[300/2713]: training loss : 0.990959895849228 TRAIN  loss dict:  {'classification_loss': 0.990959895849228}
2025-01-15 15:04:26,693 [INFO] Step[350/2713]: training loss : 0.9578393495082855 TRAIN  loss dict:  {'classification_loss': 0.9578393495082855}
2025-01-15 15:04:38,636 [INFO] Step[400/2713]: training loss : 0.9698284649848938 TRAIN  loss dict:  {'classification_loss': 0.9698284649848938}
2025-01-15 15:04:50,581 [INFO] Step[450/2713]: training loss : 1.0217807745933534 TRAIN  loss dict:  {'classification_loss': 1.0217807745933534}
2025-01-15 15:05:02,515 [INFO] Step[500/2713]: training loss : 0.951383535861969 TRAIN  loss dict:  {'classification_loss': 0.951383535861969}
2025-01-15 15:05:14,453 [INFO] Step[550/2713]: training loss : 0.9724929082393646 TRAIN  loss dict:  {'classification_loss': 0.9724929082393646}
2025-01-15 15:05:26,360 [INFO] Step[600/2713]: training loss : 0.9784905302524567 TRAIN  loss dict:  {'classification_loss': 0.9784905302524567}
2025-01-15 15:05:38,323 [INFO] Step[650/2713]: training loss : 0.9866015923023224 TRAIN  loss dict:  {'classification_loss': 0.9866015923023224}
2025-01-15 15:05:50,233 [INFO] Step[700/2713]: training loss : 0.969348703622818 TRAIN  loss dict:  {'classification_loss': 0.969348703622818}
2025-01-15 15:06:02,165 [INFO] Step[750/2713]: training loss : 1.0170788049697876 TRAIN  loss dict:  {'classification_loss': 1.0170788049697876}
2025-01-15 15:06:14,092 [INFO] Step[800/2713]: training loss : 0.9829712343215943 TRAIN  loss dict:  {'classification_loss': 0.9829712343215943}
2025-01-15 15:06:26,022 [INFO] Step[850/2713]: training loss : 1.0191519236564637 TRAIN  loss dict:  {'classification_loss': 1.0191519236564637}
2025-01-15 15:06:37,925 [INFO] Step[900/2713]: training loss : 1.0170745658874512 TRAIN  loss dict:  {'classification_loss': 1.0170745658874512}
2025-01-15 15:06:49,853 [INFO] Step[950/2713]: training loss : 1.0271665811538697 TRAIN  loss dict:  {'classification_loss': 1.0271665811538697}
2025-01-15 15:07:01,762 [INFO] Step[1000/2713]: training loss : 1.0387959885597229 TRAIN  loss dict:  {'classification_loss': 1.0387959885597229}
2025-01-15 15:07:13,688 [INFO] Step[1050/2713]: training loss : 1.0100016748905183 TRAIN  loss dict:  {'classification_loss': 1.0100016748905183}
2025-01-15 15:07:25,623 [INFO] Step[1100/2713]: training loss : 1.0204287874698639 TRAIN  loss dict:  {'classification_loss': 1.0204287874698639}
2025-01-15 15:07:37,572 [INFO] Step[1150/2713]: training loss : 1.033826049566269 TRAIN  loss dict:  {'classification_loss': 1.033826049566269}
2025-01-15 15:07:49,494 [INFO] Step[1200/2713]: training loss : 1.1062604200839996 TRAIN  loss dict:  {'classification_loss': 1.1062604200839996}
2025-01-15 15:08:01,427 [INFO] Step[1250/2713]: training loss : 0.9707133865356445 TRAIN  loss dict:  {'classification_loss': 0.9707133865356445}
2025-01-15 15:08:13,370 [INFO] Step[1300/2713]: training loss : 0.9829965245723724 TRAIN  loss dict:  {'classification_loss': 0.9829965245723724}
2025-01-15 15:08:25,322 [INFO] Step[1350/2713]: training loss : 0.9997819352149964 TRAIN  loss dict:  {'classification_loss': 0.9997819352149964}
2025-01-15 15:08:37,232 [INFO] Step[1400/2713]: training loss : 0.9717138242721558 TRAIN  loss dict:  {'classification_loss': 0.9717138242721558}
2025-01-15 15:08:49,212 [INFO] Step[1450/2713]: training loss : 1.0449631750583648 TRAIN  loss dict:  {'classification_loss': 1.0449631750583648}
2025-01-15 15:09:01,119 [INFO] Step[1500/2713]: training loss : 1.072357211112976 TRAIN  loss dict:  {'classification_loss': 1.072357211112976}
2025-01-15 15:09:13,037 [INFO] Step[1550/2713]: training loss : 0.9843307781219482 TRAIN  loss dict:  {'classification_loss': 0.9843307781219482}
2025-01-15 15:09:24,965 [INFO] Step[1600/2713]: training loss : 0.997149795293808 TRAIN  loss dict:  {'classification_loss': 0.997149795293808}
2025-01-15 15:09:36,905 [INFO] Step[1650/2713]: training loss : 1.061621788740158 TRAIN  loss dict:  {'classification_loss': 1.061621788740158}
2025-01-15 15:09:48,844 [INFO] Step[1700/2713]: training loss : 1.0156779992580414 TRAIN  loss dict:  {'classification_loss': 1.0156779992580414}
2025-01-15 15:10:00,780 [INFO] Step[1750/2713]: training loss : 0.9888190925121307 TRAIN  loss dict:  {'classification_loss': 0.9888190925121307}
2025-01-15 15:10:12,700 [INFO] Step[1800/2713]: training loss : 0.9676408624649048 TRAIN  loss dict:  {'classification_loss': 0.9676408624649048}
2025-01-15 15:10:24,604 [INFO] Step[1850/2713]: training loss : 1.046069266796112 TRAIN  loss dict:  {'classification_loss': 1.046069266796112}
2025-01-15 15:10:36,502 [INFO] Step[1900/2713]: training loss : 1.037035015821457 TRAIN  loss dict:  {'classification_loss': 1.037035015821457}
2025-01-15 15:10:48,441 [INFO] Step[1950/2713]: training loss : 1.0033485746383668 TRAIN  loss dict:  {'classification_loss': 1.0033485746383668}
2025-01-15 15:11:00,335 [INFO] Step[2000/2713]: training loss : 0.9829285180568695 TRAIN  loss dict:  {'classification_loss': 0.9829285180568695}
2025-01-15 15:11:12,239 [INFO] Step[2050/2713]: training loss : 1.0555171954631806 TRAIN  loss dict:  {'classification_loss': 1.0555171954631806}
2025-01-15 15:11:24,174 [INFO] Step[2100/2713]: training loss : 1.0521309542655946 TRAIN  loss dict:  {'classification_loss': 1.0521309542655946}
2025-01-15 15:11:36,106 [INFO] Step[2150/2713]: training loss : 1.035584384202957 TRAIN  loss dict:  {'classification_loss': 1.035584384202957}
2025-01-15 15:11:48,013 [INFO] Step[2200/2713]: training loss : 0.9585123980045318 TRAIN  loss dict:  {'classification_loss': 0.9585123980045318}
2025-01-15 15:11:59,950 [INFO] Step[2250/2713]: training loss : 0.9793826997280121 TRAIN  loss dict:  {'classification_loss': 0.9793826997280121}
2025-01-15 15:12:11,846 [INFO] Step[2300/2713]: training loss : 0.9796010875701904 TRAIN  loss dict:  {'classification_loss': 0.9796010875701904}
2025-01-15 15:12:23,760 [INFO] Step[2350/2713]: training loss : 1.0005644869804382 TRAIN  loss dict:  {'classification_loss': 1.0005644869804382}
2025-01-15 15:12:35,650 [INFO] Step[2400/2713]: training loss : 0.9878584587574005 TRAIN  loss dict:  {'classification_loss': 0.9878584587574005}
2025-01-15 15:12:47,556 [INFO] Step[2450/2713]: training loss : 0.9487763369083404 TRAIN  loss dict:  {'classification_loss': 0.9487763369083404}
2025-01-15 15:12:59,491 [INFO] Step[2500/2713]: training loss : 1.022681200504303 TRAIN  loss dict:  {'classification_loss': 1.022681200504303}
2025-01-15 15:13:11,436 [INFO] Step[2550/2713]: training loss : 0.9472023963928222 TRAIN  loss dict:  {'classification_loss': 0.9472023963928222}
2025-01-15 15:13:23,340 [INFO] Step[2600/2713]: training loss : 1.0407372260093688 TRAIN  loss dict:  {'classification_loss': 1.0407372260093688}
2025-01-15 15:13:35,275 [INFO] Step[2650/2713]: training loss : 0.9588914692401886 TRAIN  loss dict:  {'classification_loss': 0.9588914692401886}
2025-01-15 15:13:47,185 [INFO] Step[2700/2713]: training loss : 0.9719986426830292 TRAIN  loss dict:  {'classification_loss': 0.9719986426830292}
2025-01-15 15:15:01,160 [INFO] Label accuracies statistics:
2025-01-15 15:15:01,161 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.5, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.25, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.5, 59: 0.75, 60: 0.25, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.5, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.25, 86: 0.25, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 0.75, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.25, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 0.5, 158: 0.3333333333333333, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.5, 185: 0.5, 186: 0.5, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.5, 191: 0.25, 192: 0.75, 193: 0.75, 194: 1.0, 195: 1.0, 196: 0.5, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.5, 248: 1.0, 249: 0.75, 250: 1.0, 251: 1.0, 252: 0.5, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 0.5, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.25, 274: 0.5, 275: 0.25, 276: 0.5, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 0.75, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 0.25, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.5, 324: 0.75, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.25, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.25, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 1.0, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.5, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 0.75, 389: 0.25, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-15 15:15:03,402 [INFO] [2] TRAIN  loss: 1.0007971665360942 acc: 0.9842732522422902
2025-01-15 15:15:03,402 [INFO] [2] TRAIN  loss dict: {'classification_loss': 1.0007971665360942}
2025-01-15 15:15:03,402 [INFO] [2] VALIDATION loss: 2.102427580302819 VALIDATION acc: 0.7435736677115987
2025-01-15 15:15:03,402 [INFO] [2] VALIDATION loss dict: {'classification_loss': 2.102427580302819}
2025-01-15 15:15:03,402 [INFO] 
2025-01-15 15:15:20,234 [INFO] Step[50/2713]: training loss : 0.9544287168979645 TRAIN  loss dict:  {'classification_loss': 0.9544287168979645}
2025-01-15 15:15:32,100 [INFO] Step[100/2713]: training loss : 0.9880762565135955 TRAIN  loss dict:  {'classification_loss': 0.9880762565135955}
2025-01-15 15:15:44,021 [INFO] Step[150/2713]: training loss : 0.9920132744312287 TRAIN  loss dict:  {'classification_loss': 0.9920132744312287}
2025-01-15 15:15:55,934 [INFO] Step[200/2713]: training loss : 1.0438415467739106 TRAIN  loss dict:  {'classification_loss': 1.0438415467739106}
2025-01-15 15:16:07,880 [INFO] Step[250/2713]: training loss : 0.9767258727550506 TRAIN  loss dict:  {'classification_loss': 0.9767258727550506}
2025-01-15 15:16:19,787 [INFO] Step[300/2713]: training loss : 1.0634514999389648 TRAIN  loss dict:  {'classification_loss': 1.0634514999389648}
2025-01-15 15:16:31,706 [INFO] Step[350/2713]: training loss : 0.9848177862167359 TRAIN  loss dict:  {'classification_loss': 0.9848177862167359}
2025-01-15 15:16:43,650 [INFO] Step[400/2713]: training loss : 0.9799270117282868 TRAIN  loss dict:  {'classification_loss': 0.9799270117282868}
2025-01-15 15:16:55,568 [INFO] Step[450/2713]: training loss : 0.9722326338291168 TRAIN  loss dict:  {'classification_loss': 0.9722326338291168}
2025-01-15 15:17:07,463 [INFO] Step[500/2713]: training loss : 1.015917464494705 TRAIN  loss dict:  {'classification_loss': 1.015917464494705}
2025-01-15 15:17:19,440 [INFO] Step[550/2713]: training loss : 0.9696648132801056 TRAIN  loss dict:  {'classification_loss': 0.9696648132801056}
2025-01-15 15:17:31,333 [INFO] Step[600/2713]: training loss : 0.9799231600761413 TRAIN  loss dict:  {'classification_loss': 0.9799231600761413}
2025-01-15 15:17:43,315 [INFO] Step[650/2713]: training loss : 1.0269921910762787 TRAIN  loss dict:  {'classification_loss': 1.0269921910762787}
2025-01-15 15:17:55,245 [INFO] Step[700/2713]: training loss : 0.9914016318321228 TRAIN  loss dict:  {'classification_loss': 0.9914016318321228}
2025-01-15 15:18:07,192 [INFO] Step[750/2713]: training loss : 1.000280213356018 TRAIN  loss dict:  {'classification_loss': 1.000280213356018}
2025-01-15 15:18:19,103 [INFO] Step[800/2713]: training loss : 0.9763817834854126 TRAIN  loss dict:  {'classification_loss': 0.9763817834854126}
2025-01-15 15:18:31,046 [INFO] Step[850/2713]: training loss : 0.960371743440628 TRAIN  loss dict:  {'classification_loss': 0.960371743440628}
2025-01-15 15:18:42,969 [INFO] Step[900/2713]: training loss : 0.9697205376625061 TRAIN  loss dict:  {'classification_loss': 0.9697205376625061}
2025-01-15 15:18:54,906 [INFO] Step[950/2713]: training loss : 0.9937450814247132 TRAIN  loss dict:  {'classification_loss': 0.9937450814247132}
2025-01-15 15:19:06,828 [INFO] Step[1000/2713]: training loss : 1.0287921822071076 TRAIN  loss dict:  {'classification_loss': 1.0287921822071076}
2025-01-15 15:19:18,776 [INFO] Step[1050/2713]: training loss : 0.9499473059177399 TRAIN  loss dict:  {'classification_loss': 0.9499473059177399}
2025-01-15 15:19:30,682 [INFO] Step[1100/2713]: training loss : 1.0832912981510163 TRAIN  loss dict:  {'classification_loss': 1.0832912981510163}
2025-01-15 15:19:42,569 [INFO] Step[1150/2713]: training loss : 0.9548966085910797 TRAIN  loss dict:  {'classification_loss': 0.9548966085910797}
2025-01-15 15:19:54,486 [INFO] Step[1200/2713]: training loss : 0.9838955569267273 TRAIN  loss dict:  {'classification_loss': 0.9838955569267273}
2025-01-15 15:20:06,422 [INFO] Step[1250/2713]: training loss : 0.9902814030647278 TRAIN  loss dict:  {'classification_loss': 0.9902814030647278}
2025-01-15 15:20:18,319 [INFO] Step[1300/2713]: training loss : 0.972852178812027 TRAIN  loss dict:  {'classification_loss': 0.972852178812027}
2025-01-15 15:20:30,246 [INFO] Step[1350/2713]: training loss : 0.9787481451034545 TRAIN  loss dict:  {'classification_loss': 0.9787481451034545}
2025-01-15 15:20:42,162 [INFO] Step[1400/2713]: training loss : 0.9746623122692109 TRAIN  loss dict:  {'classification_loss': 0.9746623122692109}
2025-01-15 15:20:54,052 [INFO] Step[1450/2713]: training loss : 1.0417596864700318 TRAIN  loss dict:  {'classification_loss': 1.0417596864700318}
2025-01-15 15:21:05,982 [INFO] Step[1500/2713]: training loss : 0.9960664188861847 TRAIN  loss dict:  {'classification_loss': 0.9960664188861847}
2025-01-15 15:21:17,896 [INFO] Step[1550/2713]: training loss : 0.9687864720821381 TRAIN  loss dict:  {'classification_loss': 0.9687864720821381}
2025-01-15 15:21:29,795 [INFO] Step[1600/2713]: training loss : 0.9963566160202026 TRAIN  loss dict:  {'classification_loss': 0.9963566160202026}
2025-01-15 15:21:41,727 [INFO] Step[1650/2713]: training loss : 0.9612910521030426 TRAIN  loss dict:  {'classification_loss': 0.9612910521030426}
2025-01-15 15:21:53,643 [INFO] Step[1700/2713]: training loss : 0.9595506501197815 TRAIN  loss dict:  {'classification_loss': 0.9595506501197815}
2025-01-15 15:22:05,515 [INFO] Step[1750/2713]: training loss : 0.9779730689525604 TRAIN  loss dict:  {'classification_loss': 0.9779730689525604}
2025-01-15 15:22:17,447 [INFO] Step[1800/2713]: training loss : 0.9910432231426239 TRAIN  loss dict:  {'classification_loss': 0.9910432231426239}
2025-01-15 15:22:29,355 [INFO] Step[1850/2713]: training loss : 0.9847735607624054 TRAIN  loss dict:  {'classification_loss': 0.9847735607624054}
2025-01-15 15:22:41,292 [INFO] Step[1900/2713]: training loss : 0.9700509297847748 TRAIN  loss dict:  {'classification_loss': 0.9700509297847748}
2025-01-15 15:22:53,218 [INFO] Step[1950/2713]: training loss : 0.9821480810642242 TRAIN  loss dict:  {'classification_loss': 0.9821480810642242}
2025-01-15 15:23:05,130 [INFO] Step[2000/2713]: training loss : 0.9828691494464874 TRAIN  loss dict:  {'classification_loss': 0.9828691494464874}
2025-01-15 15:23:17,053 [INFO] Step[2050/2713]: training loss : 0.9853275537490844 TRAIN  loss dict:  {'classification_loss': 0.9853275537490844}
2025-01-15 15:23:28,937 [INFO] Step[2100/2713]: training loss : 1.0183970737457275 TRAIN  loss dict:  {'classification_loss': 1.0183970737457275}
2025-01-15 15:23:40,872 [INFO] Step[2150/2713]: training loss : 1.06206218957901 TRAIN  loss dict:  {'classification_loss': 1.06206218957901}
2025-01-15 15:23:52,776 [INFO] Step[2200/2713]: training loss : 1.0180495381355286 TRAIN  loss dict:  {'classification_loss': 1.0180495381355286}
2025-01-15 15:24:04,736 [INFO] Step[2250/2713]: training loss : 0.9950881469249725 TRAIN  loss dict:  {'classification_loss': 0.9950881469249725}
2025-01-15 15:24:16,637 [INFO] Step[2300/2713]: training loss : 1.0309478974342345 TRAIN  loss dict:  {'classification_loss': 1.0309478974342345}
2025-01-15 15:24:28,565 [INFO] Step[2350/2713]: training loss : 1.0654975175857544 TRAIN  loss dict:  {'classification_loss': 1.0654975175857544}
2025-01-15 15:24:40,427 [INFO] Step[2400/2713]: training loss : 1.040800383090973 TRAIN  loss dict:  {'classification_loss': 1.040800383090973}
2025-01-15 15:24:52,391 [INFO] Step[2450/2713]: training loss : 1.0486719572544099 TRAIN  loss dict:  {'classification_loss': 1.0486719572544099}
2025-01-15 15:25:04,289 [INFO] Step[2500/2713]: training loss : 1.0440059399604797 TRAIN  loss dict:  {'classification_loss': 1.0440059399604797}
2025-01-15 15:25:16,234 [INFO] Step[2550/2713]: training loss : 0.9646024751663208 TRAIN  loss dict:  {'classification_loss': 0.9646024751663208}
2025-01-15 15:25:28,144 [INFO] Step[2600/2713]: training loss : 0.9602244460582733 TRAIN  loss dict:  {'classification_loss': 0.9602244460582733}
2025-01-15 15:25:40,067 [INFO] Step[2650/2713]: training loss : 1.0441739165782928 TRAIN  loss dict:  {'classification_loss': 1.0441739165782928}
2025-01-15 15:25:51,940 [INFO] Step[2700/2713]: training loss : 1.0087341499328613 TRAIN  loss dict:  {'classification_loss': 1.0087341499328613}
2025-01-15 15:27:06,006 [INFO] Label accuracies statistics:
2025-01-15 15:27:06,006 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.25, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 0.75, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 0.8, 100: 1.0, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 0.5, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 0.75, 129: 0.75, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.5, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.5, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.25, 207: 0.5, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.5, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.25, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.25, 244: 1.0, 245: 0.75, 246: 0.75, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.25, 261: 1.0, 262: 1.0, 263: 0.75, 264: 0.75, 265: 0.75, 266: 0.75, 267: 0.0, 268: 0.25, 269: 0.5, 270: 1.0, 271: 0.25, 272: 0.5, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.5, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.5, 287: 0.75, 288: 0.75, 289: 0.75, 290: 1.0, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 0.5, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.5, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 0.75, 330: 0.75, 331: 0.75, 332: 0.5, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.5, 337: 0.5, 338: 1.0, 339: 0.75, 340: 0.5, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.5, 347: 0.5, 348: 1.0, 349: 1.0, 350: 0.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 0.75, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.25, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.25, 395: 0.75, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 15:27:07,104 [INFO] [3] TRAIN  loss: 0.9977458482117306 acc: 0.9842732522422902
2025-01-15 15:27:07,104 [INFO] [3] TRAIN  loss dict: {'classification_loss': 0.9977458482117306}
2025-01-15 15:27:07,104 [INFO] [3] VALIDATION loss: 2.0897155258440434 VALIDATION acc: 0.7379310344827587
2025-01-15 15:27:07,104 [INFO] [3] VALIDATION loss dict: {'classification_loss': 2.0897155258440434}
2025-01-15 15:27:07,104 [INFO] 
2025-01-15 15:27:23,640 [INFO] Step[50/2713]: training loss : 1.015413500070572 TRAIN  loss dict:  {'classification_loss': 1.015413500070572}
2025-01-15 15:27:35,535 [INFO] Step[100/2713]: training loss : 0.9763912892341614 TRAIN  loss dict:  {'classification_loss': 0.9763912892341614}
2025-01-15 15:27:47,467 [INFO] Step[150/2713]: training loss : 0.9776386666297913 TRAIN  loss dict:  {'classification_loss': 0.9776386666297913}
2025-01-15 15:27:59,446 [INFO] Step[200/2713]: training loss : 0.9620042788982391 TRAIN  loss dict:  {'classification_loss': 0.9620042788982391}
2025-01-15 15:28:11,339 [INFO] Step[250/2713]: training loss : 0.9669591450691223 TRAIN  loss dict:  {'classification_loss': 0.9669591450691223}
2025-01-15 15:28:23,252 [INFO] Step[300/2713]: training loss : 1.0011574375629424 TRAIN  loss dict:  {'classification_loss': 1.0011574375629424}
2025-01-15 15:28:35,187 [INFO] Step[350/2713]: training loss : 1.0408521378040314 TRAIN  loss dict:  {'classification_loss': 1.0408521378040314}
2025-01-15 15:28:47,118 [INFO] Step[400/2713]: training loss : 0.9923300170898437 TRAIN  loss dict:  {'classification_loss': 0.9923300170898437}
2025-01-15 15:28:59,037 [INFO] Step[450/2713]: training loss : 1.0306697714328765 TRAIN  loss dict:  {'classification_loss': 1.0306697714328765}
2025-01-15 15:29:10,970 [INFO] Step[500/2713]: training loss : 1.0049698674678802 TRAIN  loss dict:  {'classification_loss': 1.0049698674678802}
2025-01-15 15:29:22,927 [INFO] Step[550/2713]: training loss : 1.0143223226070404 TRAIN  loss dict:  {'classification_loss': 1.0143223226070404}
2025-01-15 15:29:34,863 [INFO] Step[600/2713]: training loss : 1.0182666742801667 TRAIN  loss dict:  {'classification_loss': 1.0182666742801667}
2025-01-15 15:29:46,783 [INFO] Step[650/2713]: training loss : 1.026100639104843 TRAIN  loss dict:  {'classification_loss': 1.026100639104843}
2025-01-15 15:29:58,712 [INFO] Step[700/2713]: training loss : 0.9721751868724823 TRAIN  loss dict:  {'classification_loss': 0.9721751868724823}
2025-01-15 15:30:10,640 [INFO] Step[750/2713]: training loss : 0.9676350057125092 TRAIN  loss dict:  {'classification_loss': 0.9676350057125092}
2025-01-15 15:30:22,565 [INFO] Step[800/2713]: training loss : 1.0306776356697083 TRAIN  loss dict:  {'classification_loss': 1.0306776356697083}
2025-01-15 15:30:34,530 [INFO] Step[850/2713]: training loss : 1.034927512407303 TRAIN  loss dict:  {'classification_loss': 1.034927512407303}
2025-01-15 15:30:46,447 [INFO] Step[900/2713]: training loss : 1.0325724112987518 TRAIN  loss dict:  {'classification_loss': 1.0325724112987518}
2025-01-15 15:30:58,387 [INFO] Step[950/2713]: training loss : 0.9805526685714722 TRAIN  loss dict:  {'classification_loss': 0.9805526685714722}
2025-01-15 15:31:10,280 [INFO] Step[1000/2713]: training loss : 0.9849413394927978 TRAIN  loss dict:  {'classification_loss': 0.9849413394927978}
2025-01-15 15:31:22,191 [INFO] Step[1050/2713]: training loss : 0.9776808750629425 TRAIN  loss dict:  {'classification_loss': 0.9776808750629425}
2025-01-15 15:31:34,110 [INFO] Step[1100/2713]: training loss : 0.9463934636116028 TRAIN  loss dict:  {'classification_loss': 0.9463934636116028}
2025-01-15 15:31:46,052 [INFO] Step[1150/2713]: training loss : 0.9642473340034485 TRAIN  loss dict:  {'classification_loss': 0.9642473340034485}
2025-01-15 15:31:57,968 [INFO] Step[1200/2713]: training loss : 1.0131967091560363 TRAIN  loss dict:  {'classification_loss': 1.0131967091560363}
2025-01-15 15:32:09,869 [INFO] Step[1250/2713]: training loss : 1.0265565037727356 TRAIN  loss dict:  {'classification_loss': 1.0265565037727356}
2025-01-15 15:32:21,810 [INFO] Step[1300/2713]: training loss : 0.9955779826641082 TRAIN  loss dict:  {'classification_loss': 0.9955779826641082}
2025-01-15 15:32:33,703 [INFO] Step[1350/2713]: training loss : 0.9628265678882599 TRAIN  loss dict:  {'classification_loss': 0.9628265678882599}
2025-01-15 15:32:45,610 [INFO] Step[1400/2713]: training loss : 0.9921565532684327 TRAIN  loss dict:  {'classification_loss': 0.9921565532684327}
2025-01-15 15:32:57,532 [INFO] Step[1450/2713]: training loss : 0.9690484690666199 TRAIN  loss dict:  {'classification_loss': 0.9690484690666199}
2025-01-15 15:33:09,470 [INFO] Step[1500/2713]: training loss : 0.959785692691803 TRAIN  loss dict:  {'classification_loss': 0.959785692691803}
2025-01-15 15:33:21,378 [INFO] Step[1550/2713]: training loss : 1.0371229898929597 TRAIN  loss dict:  {'classification_loss': 1.0371229898929597}
2025-01-15 15:33:33,310 [INFO] Step[1600/2713]: training loss : 1.022876080274582 TRAIN  loss dict:  {'classification_loss': 1.022876080274582}
2025-01-15 15:33:45,220 [INFO] Step[1650/2713]: training loss : 0.977175235748291 TRAIN  loss dict:  {'classification_loss': 0.977175235748291}
2025-01-15 15:33:57,109 [INFO] Step[1700/2713]: training loss : 1.0108346676826476 TRAIN  loss dict:  {'classification_loss': 1.0108346676826476}
2025-01-15 15:34:09,016 [INFO] Step[1750/2713]: training loss : 1.0232865405082703 TRAIN  loss dict:  {'classification_loss': 1.0232865405082703}
2025-01-15 15:34:20,920 [INFO] Step[1800/2713]: training loss : 0.9542755162715912 TRAIN  loss dict:  {'classification_loss': 0.9542755162715912}
2025-01-15 15:34:32,878 [INFO] Step[1850/2713]: training loss : 1.0539105939865112 TRAIN  loss dict:  {'classification_loss': 1.0539105939865112}
2025-01-15 15:34:44,768 [INFO] Step[1900/2713]: training loss : 0.9992478811740875 TRAIN  loss dict:  {'classification_loss': 0.9992478811740875}
2025-01-15 15:34:56,683 [INFO] Step[1950/2713]: training loss : 1.0127173519134522 TRAIN  loss dict:  {'classification_loss': 1.0127173519134522}
2025-01-15 15:35:08,589 [INFO] Step[2000/2713]: training loss : 1.0135606050491333 TRAIN  loss dict:  {'classification_loss': 1.0135606050491333}
2025-01-15 15:35:20,521 [INFO] Step[2050/2713]: training loss : 1.015907006263733 TRAIN  loss dict:  {'classification_loss': 1.015907006263733}
2025-01-15 15:35:32,461 [INFO] Step[2100/2713]: training loss : 0.9648100554943084 TRAIN  loss dict:  {'classification_loss': 0.9648100554943084}
2025-01-15 15:35:44,370 [INFO] Step[2150/2713]: training loss : 0.9595024573802948 TRAIN  loss dict:  {'classification_loss': 0.9595024573802948}
2025-01-15 15:35:56,289 [INFO] Step[2200/2713]: training loss : 0.9765924775600433 TRAIN  loss dict:  {'classification_loss': 0.9765924775600433}
2025-01-15 15:36:08,202 [INFO] Step[2250/2713]: training loss : 0.9799084997177124 TRAIN  loss dict:  {'classification_loss': 0.9799084997177124}
2025-01-15 15:36:20,173 [INFO] Step[2300/2713]: training loss : 0.9646338832378387 TRAIN  loss dict:  {'classification_loss': 0.9646338832378387}
2025-01-15 15:36:32,090 [INFO] Step[2350/2713]: training loss : 0.9409057164192199 TRAIN  loss dict:  {'classification_loss': 0.9409057164192199}
2025-01-15 15:36:44,014 [INFO] Step[2400/2713]: training loss : 0.9572618281841279 TRAIN  loss dict:  {'classification_loss': 0.9572618281841279}
2025-01-15 15:36:55,940 [INFO] Step[2450/2713]: training loss : 0.990405319929123 TRAIN  loss dict:  {'classification_loss': 0.990405319929123}
2025-01-15 15:37:07,858 [INFO] Step[2500/2713]: training loss : 0.9815694940090179 TRAIN  loss dict:  {'classification_loss': 0.9815694940090179}
2025-01-15 15:37:19,813 [INFO] Step[2550/2713]: training loss : 0.9616801059246063 TRAIN  loss dict:  {'classification_loss': 0.9616801059246063}
2025-01-15 15:37:31,752 [INFO] Step[2600/2713]: training loss : 1.0308775401115418 TRAIN  loss dict:  {'classification_loss': 1.0308775401115418}
2025-01-15 15:37:43,670 [INFO] Step[2650/2713]: training loss : 1.026669476032257 TRAIN  loss dict:  {'classification_loss': 1.026669476032257}
2025-01-15 15:37:55,576 [INFO] Step[2700/2713]: training loss : 1.0042620694637299 TRAIN  loss dict:  {'classification_loss': 1.0042620694637299}
2025-01-15 15:39:09,005 [INFO] Label accuracies statistics:
2025-01-15 15:39:09,005 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.5, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.25, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 1.0, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 0.75, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.25, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 0.5, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.5, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.5, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.25, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.25, 196: 0.75, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.5, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.25, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.5, 240: 0.75, 241: 0.75, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.25, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.5, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.25, 264: 1.0, 265: 0.75, 266: 0.75, 267: 0.25, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.25, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.25, 292: 0.5, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.25, 301: 0.75, 302: 0.75, 303: 0.5, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 0.5, 309: 0.75, 310: 0.5, 311: 0.25, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.5, 327: 0.5, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.5, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.5, 356: 0.25, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.5, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.25, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-15 15:39:09,007 [INFO] [4] TRAIN  loss: 0.9942257000079157 acc: 0.9850104435434328
2025-01-15 15:39:09,007 [INFO] [4] TRAIN  loss dict: {'classification_loss': 0.9942257000079157}
2025-01-15 15:39:09,008 [INFO] [4] VALIDATION loss: 2.148487947601125 VALIDATION acc: 0.7272727272727273
2025-01-15 15:39:09,008 [INFO] [4] VALIDATION loss dict: {'classification_loss': 2.148487947601125}
2025-01-15 15:39:09,008 [INFO] 
2025-01-15 15:39:25,811 [INFO] Step[50/2713]: training loss : 0.9964098799228668 TRAIN  loss dict:  {'classification_loss': 0.9964098799228668}
2025-01-15 15:39:37,728 [INFO] Step[100/2713]: training loss : 1.023843935728073 TRAIN  loss dict:  {'classification_loss': 1.023843935728073}
2025-01-15 15:39:49,656 [INFO] Step[150/2713]: training loss : 1.0795602631568908 TRAIN  loss dict:  {'classification_loss': 1.0795602631568908}
2025-01-15 15:40:01,569 [INFO] Step[200/2713]: training loss : 0.9625457942485809 TRAIN  loss dict:  {'classification_loss': 0.9625457942485809}
2025-01-15 15:40:13,493 [INFO] Step[250/2713]: training loss : 1.0422782838344573 TRAIN  loss dict:  {'classification_loss': 1.0422782838344573}
2025-01-15 15:40:25,465 [INFO] Step[300/2713]: training loss : 1.112519121170044 TRAIN  loss dict:  {'classification_loss': 1.112519121170044}
2025-01-15 15:40:37,383 [INFO] Step[350/2713]: training loss : 0.9496166050434113 TRAIN  loss dict:  {'classification_loss': 0.9496166050434113}
2025-01-15 15:40:49,309 [INFO] Step[400/2713]: training loss : 1.0125020658969879 TRAIN  loss dict:  {'classification_loss': 1.0125020658969879}
2025-01-15 15:41:01,233 [INFO] Step[450/2713]: training loss : 1.0006578826904298 TRAIN  loss dict:  {'classification_loss': 1.0006578826904298}
2025-01-15 15:41:13,146 [INFO] Step[500/2713]: training loss : 0.9552394378185273 TRAIN  loss dict:  {'classification_loss': 0.9552394378185273}
2025-01-15 15:41:25,063 [INFO] Step[550/2713]: training loss : 0.9997976660728455 TRAIN  loss dict:  {'classification_loss': 0.9997976660728455}
2025-01-15 15:41:36,999 [INFO] Step[600/2713]: training loss : 0.9664086019992828 TRAIN  loss dict:  {'classification_loss': 0.9664086019992828}
2025-01-15 15:41:48,962 [INFO] Step[650/2713]: training loss : 1.0239440727233886 TRAIN  loss dict:  {'classification_loss': 1.0239440727233886}
2025-01-15 15:42:00,920 [INFO] Step[700/2713]: training loss : 1.035352327823639 TRAIN  loss dict:  {'classification_loss': 1.035352327823639}
2025-01-15 15:42:12,877 [INFO] Step[750/2713]: training loss : 0.9644963693618774 TRAIN  loss dict:  {'classification_loss': 0.9644963693618774}
2025-01-15 15:42:24,791 [INFO] Step[800/2713]: training loss : 0.9835229742527009 TRAIN  loss dict:  {'classification_loss': 0.9835229742527009}
2025-01-15 15:42:36,716 [INFO] Step[850/2713]: training loss : 1.0058275175094604 TRAIN  loss dict:  {'classification_loss': 1.0058275175094604}
2025-01-15 15:42:48,619 [INFO] Step[900/2713]: training loss : 0.9794476568698883 TRAIN  loss dict:  {'classification_loss': 0.9794476568698883}
2025-01-15 15:43:00,562 [INFO] Step[950/2713]: training loss : 1.0157277524471282 TRAIN  loss dict:  {'classification_loss': 1.0157277524471282}
2025-01-15 15:43:12,489 [INFO] Step[1000/2713]: training loss : 0.9690256774425506 TRAIN  loss dict:  {'classification_loss': 0.9690256774425506}
2025-01-15 15:43:24,411 [INFO] Step[1050/2713]: training loss : 1.0038886320590974 TRAIN  loss dict:  {'classification_loss': 1.0038886320590974}
2025-01-15 15:43:36,371 [INFO] Step[1100/2713]: training loss : 1.0268272686004638 TRAIN  loss dict:  {'classification_loss': 1.0268272686004638}
2025-01-15 15:43:48,304 [INFO] Step[1150/2713]: training loss : 1.0079037654399872 TRAIN  loss dict:  {'classification_loss': 1.0079037654399872}
2025-01-15 15:44:00,230 [INFO] Step[1200/2713]: training loss : 1.018142238855362 TRAIN  loss dict:  {'classification_loss': 1.018142238855362}
2025-01-15 15:44:12,168 [INFO] Step[1250/2713]: training loss : 1.009950956106186 TRAIN  loss dict:  {'classification_loss': 1.009950956106186}
2025-01-15 15:44:24,099 [INFO] Step[1300/2713]: training loss : 0.9921546030044556 TRAIN  loss dict:  {'classification_loss': 0.9921546030044556}
2025-01-15 15:44:36,055 [INFO] Step[1350/2713]: training loss : 1.0611569058895112 TRAIN  loss dict:  {'classification_loss': 1.0611569058895112}
2025-01-15 15:44:47,974 [INFO] Step[1400/2713]: training loss : 0.9995265340805054 TRAIN  loss dict:  {'classification_loss': 0.9995265340805054}
2025-01-15 15:44:59,921 [INFO] Step[1450/2713]: training loss : 0.9503727245330811 TRAIN  loss dict:  {'classification_loss': 0.9503727245330811}
2025-01-15 15:45:11,830 [INFO] Step[1500/2713]: training loss : 0.956024912595749 TRAIN  loss dict:  {'classification_loss': 0.956024912595749}
2025-01-15 15:45:23,745 [INFO] Step[1550/2713]: training loss : 0.9805548048019409 TRAIN  loss dict:  {'classification_loss': 0.9805548048019409}
2025-01-15 15:45:35,646 [INFO] Step[1600/2713]: training loss : 1.0590237975120544 TRAIN  loss dict:  {'classification_loss': 1.0590237975120544}
2025-01-15 15:45:47,585 [INFO] Step[1650/2713]: training loss : 1.0228383445739746 TRAIN  loss dict:  {'classification_loss': 1.0228383445739746}
2025-01-15 15:45:59,501 [INFO] Step[1700/2713]: training loss : 0.9858379697799683 TRAIN  loss dict:  {'classification_loss': 0.9858379697799683}
2025-01-15 15:46:11,429 [INFO] Step[1750/2713]: training loss : 1.0538864469528197 TRAIN  loss dict:  {'classification_loss': 1.0538864469528197}
2025-01-15 15:46:23,348 [INFO] Step[1800/2713]: training loss : 0.969590972661972 TRAIN  loss dict:  {'classification_loss': 0.969590972661972}
2025-01-15 15:46:35,248 [INFO] Step[1850/2713]: training loss : 0.9929256987571716 TRAIN  loss dict:  {'classification_loss': 0.9929256987571716}
2025-01-15 15:46:47,143 [INFO] Step[1900/2713]: training loss : 0.9915639209747314 TRAIN  loss dict:  {'classification_loss': 0.9915639209747314}
2025-01-15 15:46:59,100 [INFO] Step[1950/2713]: training loss : 0.9965283811092377 TRAIN  loss dict:  {'classification_loss': 0.9965283811092377}
2025-01-15 15:47:11,016 [INFO] Step[2000/2713]: training loss : 1.0089581418037414 TRAIN  loss dict:  {'classification_loss': 1.0089581418037414}
2025-01-15 15:47:22,927 [INFO] Step[2050/2713]: training loss : 1.0015883433818817 TRAIN  loss dict:  {'classification_loss': 1.0015883433818817}
2025-01-15 15:47:34,866 [INFO] Step[2100/2713]: training loss : 1.0233592545986177 TRAIN  loss dict:  {'classification_loss': 1.0233592545986177}
2025-01-15 15:47:46,823 [INFO] Step[2150/2713]: training loss : 1.0370584774017333 TRAIN  loss dict:  {'classification_loss': 1.0370584774017333}
2025-01-15 15:47:58,719 [INFO] Step[2200/2713]: training loss : 0.964701520204544 TRAIN  loss dict:  {'classification_loss': 0.964701520204544}
2025-01-15 15:48:10,662 [INFO] Step[2250/2713]: training loss : 1.0209532380104065 TRAIN  loss dict:  {'classification_loss': 1.0209532380104065}
2025-01-15 15:48:22,569 [INFO] Step[2300/2713]: training loss : 0.9932454979419708 TRAIN  loss dict:  {'classification_loss': 0.9932454979419708}
2025-01-15 15:48:34,480 [INFO] Step[2350/2713]: training loss : 0.9744416522979736 TRAIN  loss dict:  {'classification_loss': 0.9744416522979736}
2025-01-15 15:48:46,437 [INFO] Step[2400/2713]: training loss : 1.064472805261612 TRAIN  loss dict:  {'classification_loss': 1.064472805261612}
2025-01-15 15:48:58,387 [INFO] Step[2450/2713]: training loss : 0.977524437904358 TRAIN  loss dict:  {'classification_loss': 0.977524437904358}
2025-01-15 15:49:10,307 [INFO] Step[2500/2713]: training loss : 0.9608830797672272 TRAIN  loss dict:  {'classification_loss': 0.9608830797672272}
2025-01-15 15:49:22,218 [INFO] Step[2550/2713]: training loss : 0.9785821998119354 TRAIN  loss dict:  {'classification_loss': 0.9785821998119354}
2025-01-15 15:49:34,147 [INFO] Step[2600/2713]: training loss : 1.0577493727207183 TRAIN  loss dict:  {'classification_loss': 1.0577493727207183}
2025-01-15 15:49:46,059 [INFO] Step[2650/2713]: training loss : 1.0302279639244079 TRAIN  loss dict:  {'classification_loss': 1.0302279639244079}
2025-01-15 15:49:57,951 [INFO] Step[2700/2713]: training loss : 0.9707644283771515 TRAIN  loss dict:  {'classification_loss': 0.9707644283771515}
2025-01-15 15:51:11,759 [INFO] Label accuracies statistics:
2025-01-15 15:51:11,759 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.75, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.5, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 0.75, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.5, 168: 0.5, 169: 0.75, 170: 0.5, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.25, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.25, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.0, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.5, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.5, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.5, 246: 0.75, 247: 0.75, 248: 0.3333333333333333, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.25, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.5, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.25, 300: 0.75, 301: 0.75, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 0.75, 308: 0.75, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.5, 331: 0.75, 332: 0.75, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.25, 342: 0.5, 343: 1.0, 344: 0.5, 345: 0.25, 346: 0.75, 347: 0.75, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.0, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.0, 382: 0.75, 383: 0.75, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.25, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-15 15:51:12,933 [INFO] [5] TRAIN  loss: 1.0038501540102116 acc: 0.9825531392062907
2025-01-15 15:51:12,933 [INFO] [5] TRAIN  loss dict: {'classification_loss': 1.0038501540102116}
2025-01-15 15:51:12,933 [INFO] [5] VALIDATION loss: 2.106501983296602 VALIDATION acc: 0.7460815047021944
2025-01-15 15:51:12,933 [INFO] [5] VALIDATION loss dict: {'classification_loss': 2.106501983296602}
2025-01-15 15:51:12,933 [INFO] 
2025-01-15 15:51:30,233 [INFO] Step[50/2713]: training loss : 0.9628710007667541 TRAIN  loss dict:  {'classification_loss': 0.9628710007667541}
2025-01-15 15:51:41,997 [INFO] Step[100/2713]: training loss : 0.9849775147438049 TRAIN  loss dict:  {'classification_loss': 0.9849775147438049}
2025-01-15 15:51:53,889 [INFO] Step[150/2713]: training loss : 0.9989265358448028 TRAIN  loss dict:  {'classification_loss': 0.9989265358448028}
2025-01-15 15:52:05,788 [INFO] Step[200/2713]: training loss : 0.9968026494979858 TRAIN  loss dict:  {'classification_loss': 0.9968026494979858}
2025-01-15 15:52:17,695 [INFO] Step[250/2713]: training loss : 0.9800834727287292 TRAIN  loss dict:  {'classification_loss': 0.9800834727287292}
2025-01-15 15:52:29,621 [INFO] Step[300/2713]: training loss : 0.9773824322223663 TRAIN  loss dict:  {'classification_loss': 0.9773824322223663}
2025-01-15 15:52:41,558 [INFO] Step[350/2713]: training loss : 0.9680307996273041 TRAIN  loss dict:  {'classification_loss': 0.9680307996273041}
2025-01-15 15:52:53,441 [INFO] Step[400/2713]: training loss : 0.9611925220489502 TRAIN  loss dict:  {'classification_loss': 0.9611925220489502}
2025-01-15 15:53:05,376 [INFO] Step[450/2713]: training loss : 0.9818147647380829 TRAIN  loss dict:  {'classification_loss': 0.9818147647380829}
2025-01-15 15:53:17,300 [INFO] Step[500/2713]: training loss : 1.082335592508316 TRAIN  loss dict:  {'classification_loss': 1.082335592508316}
2025-01-15 15:53:29,228 [INFO] Step[550/2713]: training loss : 0.9990843760967255 TRAIN  loss dict:  {'classification_loss': 0.9990843760967255}
2025-01-15 15:53:41,138 [INFO] Step[600/2713]: training loss : 0.9545513832569122 TRAIN  loss dict:  {'classification_loss': 0.9545513832569122}
2025-01-15 15:53:53,071 [INFO] Step[650/2713]: training loss : 0.9860016787052155 TRAIN  loss dict:  {'classification_loss': 0.9860016787052155}
2025-01-15 15:54:05,007 [INFO] Step[700/2713]: training loss : 1.0090997195243836 TRAIN  loss dict:  {'classification_loss': 1.0090997195243836}
2025-01-15 15:54:16,963 [INFO] Step[750/2713]: training loss : 0.986508868932724 TRAIN  loss dict:  {'classification_loss': 0.986508868932724}
2025-01-15 15:54:28,886 [INFO] Step[800/2713]: training loss : 0.9595605432987213 TRAIN  loss dict:  {'classification_loss': 0.9595605432987213}
2025-01-15 15:54:40,847 [INFO] Step[850/2713]: training loss : 0.9839463174343109 TRAIN  loss dict:  {'classification_loss': 0.9839463174343109}
2025-01-15 15:54:52,751 [INFO] Step[900/2713]: training loss : 0.9848663640022278 TRAIN  loss dict:  {'classification_loss': 0.9848663640022278}
2025-01-15 15:55:04,707 [INFO] Step[950/2713]: training loss : 1.0135964679718017 TRAIN  loss dict:  {'classification_loss': 1.0135964679718017}
2025-01-15 15:55:16,628 [INFO] Step[1000/2713]: training loss : 1.0417659640312196 TRAIN  loss dict:  {'classification_loss': 1.0417659640312196}
2025-01-15 15:55:28,550 [INFO] Step[1050/2713]: training loss : 0.9739794945716858 TRAIN  loss dict:  {'classification_loss': 0.9739794945716858}
2025-01-15 15:55:40,478 [INFO] Step[1100/2713]: training loss : 0.9943478071689605 TRAIN  loss dict:  {'classification_loss': 0.9943478071689605}
2025-01-15 15:55:52,398 [INFO] Step[1150/2713]: training loss : 1.007996265888214 TRAIN  loss dict:  {'classification_loss': 1.007996265888214}
2025-01-15 15:56:04,333 [INFO] Step[1200/2713]: training loss : 1.0610303342342378 TRAIN  loss dict:  {'classification_loss': 1.0610303342342378}
2025-01-15 15:56:16,252 [INFO] Step[1250/2713]: training loss : 1.0131627523899078 TRAIN  loss dict:  {'classification_loss': 1.0131627523899078}
2025-01-15 15:56:28,173 [INFO] Step[1300/2713]: training loss : 1.0110890185832977 TRAIN  loss dict:  {'classification_loss': 1.0110890185832977}
2025-01-15 15:56:40,096 [INFO] Step[1350/2713]: training loss : 1.0049882233142853 TRAIN  loss dict:  {'classification_loss': 1.0049882233142853}
2025-01-15 15:56:51,972 [INFO] Step[1400/2713]: training loss : 0.9995532655715942 TRAIN  loss dict:  {'classification_loss': 0.9995532655715942}
2025-01-15 15:57:03,892 [INFO] Step[1450/2713]: training loss : 1.0037475872039794 TRAIN  loss dict:  {'classification_loss': 1.0037475872039794}
2025-01-15 15:57:15,851 [INFO] Step[1500/2713]: training loss : 1.0212500786781311 TRAIN  loss dict:  {'classification_loss': 1.0212500786781311}
2025-01-15 15:57:27,789 [INFO] Step[1550/2713]: training loss : 0.9947590053081512 TRAIN  loss dict:  {'classification_loss': 0.9947590053081512}
2025-01-15 15:57:39,698 [INFO] Step[1600/2713]: training loss : 0.954123638868332 TRAIN  loss dict:  {'classification_loss': 0.954123638868332}
2025-01-15 15:57:51,637 [INFO] Step[1650/2713]: training loss : 1.07091255068779 TRAIN  loss dict:  {'classification_loss': 1.07091255068779}
2025-01-15 15:58:03,566 [INFO] Step[1700/2713]: training loss : 1.0053638565540313 TRAIN  loss dict:  {'classification_loss': 1.0053638565540313}
2025-01-15 15:58:15,475 [INFO] Step[1750/2713]: training loss : 0.9607645082473755 TRAIN  loss dict:  {'classification_loss': 0.9607645082473755}
2025-01-15 15:58:27,372 [INFO] Step[1800/2713]: training loss : 0.9838491415977478 TRAIN  loss dict:  {'classification_loss': 0.9838491415977478}
2025-01-15 15:58:39,309 [INFO] Step[1850/2713]: training loss : 0.9971601593494416 TRAIN  loss dict:  {'classification_loss': 0.9971601593494416}
2025-01-15 15:58:51,232 [INFO] Step[1900/2713]: training loss : 0.9815926563739776 TRAIN  loss dict:  {'classification_loss': 0.9815926563739776}
2025-01-15 15:59:03,138 [INFO] Step[1950/2713]: training loss : 0.955733151435852 TRAIN  loss dict:  {'classification_loss': 0.955733151435852}
2025-01-15 15:59:15,024 [INFO] Step[2000/2713]: training loss : 0.9537673151493072 TRAIN  loss dict:  {'classification_loss': 0.9537673151493072}
2025-01-15 15:59:26,980 [INFO] Step[2050/2713]: training loss : 0.9939818120002747 TRAIN  loss dict:  {'classification_loss': 0.9939818120002747}
2025-01-15 15:59:38,876 [INFO] Step[2100/2713]: training loss : 0.9990451145172119 TRAIN  loss dict:  {'classification_loss': 0.9990451145172119}
2025-01-15 15:59:50,824 [INFO] Step[2150/2713]: training loss : 0.9650026094913483 TRAIN  loss dict:  {'classification_loss': 0.9650026094913483}
2025-01-15 16:00:02,707 [INFO] Step[2200/2713]: training loss : 1.0043319654464722 TRAIN  loss dict:  {'classification_loss': 1.0043319654464722}
2025-01-15 16:00:14,642 [INFO] Step[2250/2713]: training loss : 1.010041401386261 TRAIN  loss dict:  {'classification_loss': 1.010041401386261}
2025-01-15 16:00:26,546 [INFO] Step[2300/2713]: training loss : 0.9800710308551789 TRAIN  loss dict:  {'classification_loss': 0.9800710308551789}
2025-01-15 16:00:38,498 [INFO] Step[2350/2713]: training loss : 1.02261247754097 TRAIN  loss dict:  {'classification_loss': 1.02261247754097}
2025-01-15 16:00:50,417 [INFO] Step[2400/2713]: training loss : 1.0281403112411498 TRAIN  loss dict:  {'classification_loss': 1.0281403112411498}
2025-01-15 16:01:02,349 [INFO] Step[2450/2713]: training loss : 0.9925096082687378 TRAIN  loss dict:  {'classification_loss': 0.9925096082687378}
2025-01-15 16:01:14,254 [INFO] Step[2500/2713]: training loss : 0.961381939649582 TRAIN  loss dict:  {'classification_loss': 0.961381939649582}
2025-01-15 16:01:26,179 [INFO] Step[2550/2713]: training loss : 1.0274644136428832 TRAIN  loss dict:  {'classification_loss': 1.0274644136428832}
2025-01-15 16:01:38,096 [INFO] Step[2600/2713]: training loss : 0.9546146070957184 TRAIN  loss dict:  {'classification_loss': 0.9546146070957184}
2025-01-15 16:01:50,012 [INFO] Step[2650/2713]: training loss : 0.9537625360488892 TRAIN  loss dict:  {'classification_loss': 0.9537625360488892}
2025-01-15 16:02:01,883 [INFO] Step[2700/2713]: training loss : 0.9811688458919525 TRAIN  loss dict:  {'classification_loss': 0.9811688458919525}
2025-01-15 16:03:33,611 [INFO] Label accuracies statistics:
2025-01-15 16:03:33,612 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.0, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.5, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 1.0, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.5, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.25, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.5, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 0.8, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.25, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.5, 168: 0.5, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 0.75, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.5, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.5, 269: 1.0, 270: 0.75, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.0, 275: 0.75, 276: 0.5, 277: 0.75, 278: 0.5, 279: 0.75, 280: 0.5, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 1.0, 291: 0.5, 292: 0.75, 293: 1.0, 294: 0.75, 295: 0.5, 296: 0.25, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.5, 302: 1.0, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.5, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.25, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.25, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.5, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.5, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.5, 379: 1.0, 380: 0.75, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.25, 390: 1.0, 391: 0.75, 392: 0.5, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 16:03:59,703 [INFO] [6] TRAIN  loss: 0.9932766709431397 acc: 0.9843961174591473
2025-01-15 16:03:59,703 [INFO] [6] TRAIN  loss dict: {'classification_loss': 0.9932766709431397}
2025-01-15 16:03:59,703 [INFO] [6] VALIDATION loss: 2.04976257928332 VALIDATION acc: 0.747962382445141
2025-01-15 16:03:59,703 [INFO] [6] VALIDATION loss dict: {'classification_loss': 2.04976257928332}
2025-01-15 16:03:59,703 [INFO] 
2025-01-15 16:04:16,888 [INFO] Step[50/2713]: training loss : 0.9667614960670471 TRAIN  loss dict:  {'classification_loss': 0.9667614960670471}
2025-01-15 16:04:28,732 [INFO] Step[100/2713]: training loss : 0.9779051327705384 TRAIN  loss dict:  {'classification_loss': 0.9779051327705384}
2025-01-15 16:04:40,639 [INFO] Step[150/2713]: training loss : 0.9998770546913147 TRAIN  loss dict:  {'classification_loss': 0.9998770546913147}
2025-01-15 16:04:52,590 [INFO] Step[200/2713]: training loss : 1.0298692619800567 TRAIN  loss dict:  {'classification_loss': 1.0298692619800567}
2025-01-15 16:05:04,534 [INFO] Step[250/2713]: training loss : 1.04796231508255 TRAIN  loss dict:  {'classification_loss': 1.04796231508255}
2025-01-15 16:05:16,431 [INFO] Step[300/2713]: training loss : 0.9777255952358246 TRAIN  loss dict:  {'classification_loss': 0.9777255952358246}
2025-01-15 16:05:28,376 [INFO] Step[350/2713]: training loss : 1.005529375076294 TRAIN  loss dict:  {'classification_loss': 1.005529375076294}
2025-01-15 16:05:40,314 [INFO] Step[400/2713]: training loss : 0.9644539821147918 TRAIN  loss dict:  {'classification_loss': 0.9644539821147918}
2025-01-15 16:05:52,249 [INFO] Step[450/2713]: training loss : 0.954437358379364 TRAIN  loss dict:  {'classification_loss': 0.954437358379364}
2025-01-15 16:06:04,177 [INFO] Step[500/2713]: training loss : 0.9565435779094696 TRAIN  loss dict:  {'classification_loss': 0.9565435779094696}
2025-01-15 16:06:16,133 [INFO] Step[550/2713]: training loss : 0.9545414578914643 TRAIN  loss dict:  {'classification_loss': 0.9545414578914643}
2025-01-15 16:06:28,076 [INFO] Step[600/2713]: training loss : 1.0225335490703582 TRAIN  loss dict:  {'classification_loss': 1.0225335490703582}
2025-01-15 16:06:40,008 [INFO] Step[650/2713]: training loss : 1.0063116133213044 TRAIN  loss dict:  {'classification_loss': 1.0063116133213044}
2025-01-15 16:06:51,971 [INFO] Step[700/2713]: training loss : 1.0313852286338807 TRAIN  loss dict:  {'classification_loss': 1.0313852286338807}
2025-01-15 16:07:03,922 [INFO] Step[750/2713]: training loss : 0.9576404941082001 TRAIN  loss dict:  {'classification_loss': 0.9576404941082001}
2025-01-15 16:07:15,842 [INFO] Step[800/2713]: training loss : 1.043512464761734 TRAIN  loss dict:  {'classification_loss': 1.043512464761734}
2025-01-15 16:07:27,791 [INFO] Step[850/2713]: training loss : 0.9559211194515228 TRAIN  loss dict:  {'classification_loss': 0.9559211194515228}
2025-01-15 16:07:39,721 [INFO] Step[900/2713]: training loss : 0.9869087553024292 TRAIN  loss dict:  {'classification_loss': 0.9869087553024292}
2025-01-15 16:07:51,668 [INFO] Step[950/2713]: training loss : 0.9763614273071289 TRAIN  loss dict:  {'classification_loss': 0.9763614273071289}
2025-01-15 16:08:03,755 [INFO] Step[1000/2713]: training loss : 0.969685788154602 TRAIN  loss dict:  {'classification_loss': 0.969685788154602}
2025-01-15 16:08:15,762 [INFO] Step[1050/2713]: training loss : 0.9567629039287567 TRAIN  loss dict:  {'classification_loss': 0.9567629039287567}
2025-01-15 16:08:27,776 [INFO] Step[1100/2713]: training loss : 1.0046518051624298 TRAIN  loss dict:  {'classification_loss': 1.0046518051624298}
2025-01-15 16:08:39,648 [INFO] Step[1150/2713]: training loss : 0.9786549115180969 TRAIN  loss dict:  {'classification_loss': 0.9786549115180969}
2025-01-15 16:08:51,561 [INFO] Step[1200/2713]: training loss : 0.9495692229270936 TRAIN  loss dict:  {'classification_loss': 0.9495692229270936}
2025-01-15 16:09:03,467 [INFO] Step[1250/2713]: training loss : 0.9754028105735779 TRAIN  loss dict:  {'classification_loss': 0.9754028105735779}
2025-01-15 16:09:15,463 [INFO] Step[1300/2713]: training loss : 1.0637318062782288 TRAIN  loss dict:  {'classification_loss': 1.0637318062782288}
2025-01-15 16:09:27,396 [INFO] Step[1350/2713]: training loss : 0.9974147891998291 TRAIN  loss dict:  {'classification_loss': 0.9974147891998291}
2025-01-15 16:09:39,317 [INFO] Step[1400/2713]: training loss : 0.9557978951931 TRAIN  loss dict:  {'classification_loss': 0.9557978951931}
2025-01-15 16:09:51,218 [INFO] Step[1450/2713]: training loss : 0.9638151919841766 TRAIN  loss dict:  {'classification_loss': 0.9638151919841766}
2025-01-15 16:10:03,163 [INFO] Step[1500/2713]: training loss : 1.0796029448509217 TRAIN  loss dict:  {'classification_loss': 1.0796029448509217}
2025-01-15 16:10:15,112 [INFO] Step[1550/2713]: training loss : 1.008885850906372 TRAIN  loss dict:  {'classification_loss': 1.008885850906372}
2025-01-15 16:10:27,065 [INFO] Step[1600/2713]: training loss : 1.0996259725093842 TRAIN  loss dict:  {'classification_loss': 1.0996259725093842}
2025-01-15 16:10:39,003 [INFO] Step[1650/2713]: training loss : 1.0376623284816742 TRAIN  loss dict:  {'classification_loss': 1.0376623284816742}
2025-01-15 16:10:50,906 [INFO] Step[1700/2713]: training loss : 1.0648738706111909 TRAIN  loss dict:  {'classification_loss': 1.0648738706111909}
2025-01-15 16:11:02,828 [INFO] Step[1750/2713]: training loss : 0.9787170231342316 TRAIN  loss dict:  {'classification_loss': 0.9787170231342316}
2025-01-15 16:11:14,767 [INFO] Step[1800/2713]: training loss : 0.9973727476596832 TRAIN  loss dict:  {'classification_loss': 0.9973727476596832}
2025-01-15 16:11:26,704 [INFO] Step[1850/2713]: training loss : 0.9742014503479004 TRAIN  loss dict:  {'classification_loss': 0.9742014503479004}
2025-01-15 16:11:38,605 [INFO] Step[1900/2713]: training loss : 0.9782911205291748 TRAIN  loss dict:  {'classification_loss': 0.9782911205291748}
2025-01-15 16:11:50,537 [INFO] Step[1950/2713]: training loss : 0.959281747341156 TRAIN  loss dict:  {'classification_loss': 0.959281747341156}
2025-01-15 16:12:02,497 [INFO] Step[2000/2713]: training loss : 0.9970839071273804 TRAIN  loss dict:  {'classification_loss': 0.9970839071273804}
2025-01-15 16:12:14,442 [INFO] Step[2050/2713]: training loss : 1.0109858953952788 TRAIN  loss dict:  {'classification_loss': 1.0109858953952788}
2025-01-15 16:12:26,378 [INFO] Step[2100/2713]: training loss : 1.0078184258937837 TRAIN  loss dict:  {'classification_loss': 1.0078184258937837}
2025-01-15 16:12:38,332 [INFO] Step[2150/2713]: training loss : 1.0172483015060425 TRAIN  loss dict:  {'classification_loss': 1.0172483015060425}
2025-01-15 16:12:50,295 [INFO] Step[2200/2713]: training loss : 1.0522053110599519 TRAIN  loss dict:  {'classification_loss': 1.0522053110599519}
2025-01-15 16:13:02,298 [INFO] Step[2250/2713]: training loss : 1.0741401267051698 TRAIN  loss dict:  {'classification_loss': 1.0741401267051698}
2025-01-15 16:13:14,254 [INFO] Step[2300/2713]: training loss : 1.1148214495182038 TRAIN  loss dict:  {'classification_loss': 1.1148214495182038}
2025-01-15 16:13:26,242 [INFO] Step[2350/2713]: training loss : 1.0169832599163056 TRAIN  loss dict:  {'classification_loss': 1.0169832599163056}
2025-01-15 16:13:38,213 [INFO] Step[2400/2713]: training loss : 1.0129979920387269 TRAIN  loss dict:  {'classification_loss': 1.0129979920387269}
2025-01-15 16:13:50,194 [INFO] Step[2450/2713]: training loss : 0.9883703172206879 TRAIN  loss dict:  {'classification_loss': 0.9883703172206879}
2025-01-15 16:14:02,003 [INFO] Step[2500/2713]: training loss : 0.9609270489215851 TRAIN  loss dict:  {'classification_loss': 0.9609270489215851}
2025-01-15 16:14:13,917 [INFO] Step[2550/2713]: training loss : 1.0059379827976227 TRAIN  loss dict:  {'classification_loss': 1.0059379827976227}
2025-01-15 16:14:25,832 [INFO] Step[2600/2713]: training loss : 0.9932900202274323 TRAIN  loss dict:  {'classification_loss': 0.9932900202274323}
2025-01-15 16:14:37,705 [INFO] Step[2650/2713]: training loss : 1.0672097420692443 TRAIN  loss dict:  {'classification_loss': 1.0672097420692443}
2025-01-15 16:14:49,530 [INFO] Step[2700/2713]: training loss : 1.0020544087886811 TRAIN  loss dict:  {'classification_loss': 1.0020544087886811}
2025-01-15 16:16:02,816 [INFO] Label accuracies statistics:
2025-01-15 16:16:02,816 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.25, 54: 0.25, 55: 0.75, 56: 1.0, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.25, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.25, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.25, 135: 1.0, 136: 1.0, 137: 0.75, 138: 0.5, 139: 0.5, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 1.0, 183: 0.75, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.25, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.0, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.25, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.5, 244: 0.75, 245: 1.0, 246: 0.75, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.25, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.5, 277: 0.75, 278: 0.25, 279: 0.75, 280: 1.0, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.5, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.25, 326: 1.0, 327: 0.5, 328: 0.5, 329: 0.75, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.5, 335: 0.75, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.5, 356: 0.25, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.5, 377: 0.5, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.75, 382: 0.75, 383: 0.75, 384: 0.25, 385: 1.0, 386: 0.75, 387: 0.25, 388: 0.75, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-15 16:16:03,992 [INFO] [7] TRAIN  loss: 1.0025473712249036 acc: 0.9825531392062907
2025-01-15 16:16:03,993 [INFO] [7] TRAIN  loss dict: {'classification_loss': 1.0025473712249036}
2025-01-15 16:16:03,993 [INFO] [7] VALIDATION loss: 2.0588728310470295 VALIDATION acc: 0.7492163009404389
2025-01-15 16:16:03,993 [INFO] [7] VALIDATION loss dict: {'classification_loss': 2.0588728310470295}
2025-01-15 16:16:03,993 [INFO] 
2025-01-15 16:16:20,632 [INFO] Step[50/2713]: training loss : 1.0241313862800598 TRAIN  loss dict:  {'classification_loss': 1.0241313862800598}
2025-01-15 16:16:32,488 [INFO] Step[100/2713]: training loss : 0.9591878318786621 TRAIN  loss dict:  {'classification_loss': 0.9591878318786621}
2025-01-15 16:16:44,220 [INFO] Step[150/2713]: training loss : 0.9517057907581329 TRAIN  loss dict:  {'classification_loss': 0.9517057907581329}
2025-01-15 16:16:56,080 [INFO] Step[200/2713]: training loss : 1.0088196802139282 TRAIN  loss dict:  {'classification_loss': 1.0088196802139282}
2025-01-15 16:17:07,987 [INFO] Step[250/2713]: training loss : 0.9632472681999207 TRAIN  loss dict:  {'classification_loss': 0.9632472681999207}
2025-01-15 16:17:19,888 [INFO] Step[300/2713]: training loss : 0.9837172198295593 TRAIN  loss dict:  {'classification_loss': 0.9837172198295593}
2025-01-15 16:17:31,834 [INFO] Step[350/2713]: training loss : 0.9930439376831055 TRAIN  loss dict:  {'classification_loss': 0.9930439376831055}
2025-01-15 16:17:43,738 [INFO] Step[400/2713]: training loss : 0.9793714606761932 TRAIN  loss dict:  {'classification_loss': 0.9793714606761932}
2025-01-15 16:17:55,713 [INFO] Step[450/2713]: training loss : 0.9725760316848755 TRAIN  loss dict:  {'classification_loss': 0.9725760316848755}
2025-01-15 16:18:07,632 [INFO] Step[500/2713]: training loss : 1.0108120667934417 TRAIN  loss dict:  {'classification_loss': 1.0108120667934417}
2025-01-15 16:18:19,587 [INFO] Step[550/2713]: training loss : 0.9788635683059692 TRAIN  loss dict:  {'classification_loss': 0.9788635683059692}
2025-01-15 16:18:31,492 [INFO] Step[600/2713]: training loss : 0.9766630065441132 TRAIN  loss dict:  {'classification_loss': 0.9766630065441132}
2025-01-15 16:18:43,426 [INFO] Step[650/2713]: training loss : 0.9991747760772705 TRAIN  loss dict:  {'classification_loss': 0.9991747760772705}
2025-01-15 16:18:55,363 [INFO] Step[700/2713]: training loss : 0.9810596144199372 TRAIN  loss dict:  {'classification_loss': 0.9810596144199372}
2025-01-15 16:19:07,319 [INFO] Step[750/2713]: training loss : 0.9792839241027832 TRAIN  loss dict:  {'classification_loss': 0.9792839241027832}
2025-01-15 16:19:19,255 [INFO] Step[800/2713]: training loss : 0.9773939168453216 TRAIN  loss dict:  {'classification_loss': 0.9773939168453216}
2025-01-15 16:19:31,222 [INFO] Step[850/2713]: training loss : 1.0610258626937865 TRAIN  loss dict:  {'classification_loss': 1.0610258626937865}
2025-01-15 16:19:43,158 [INFO] Step[900/2713]: training loss : 0.9758005869388581 TRAIN  loss dict:  {'classification_loss': 0.9758005869388581}
2025-01-15 16:19:55,076 [INFO] Step[950/2713]: training loss : 1.092123783826828 TRAIN  loss dict:  {'classification_loss': 1.092123783826828}
2025-01-15 16:20:07,036 [INFO] Step[1000/2713]: training loss : 0.9721508169174194 TRAIN  loss dict:  {'classification_loss': 0.9721508169174194}
2025-01-15 16:20:18,950 [INFO] Step[1050/2713]: training loss : 1.0130169081687928 TRAIN  loss dict:  {'classification_loss': 1.0130169081687928}
2025-01-15 16:20:30,953 [INFO] Step[1100/2713]: training loss : 0.9965092360973358 TRAIN  loss dict:  {'classification_loss': 0.9965092360973358}
2025-01-15 16:20:42,950 [INFO] Step[1150/2713]: training loss : 0.9955096817016602 TRAIN  loss dict:  {'classification_loss': 0.9955096817016602}
2025-01-15 16:20:54,900 [INFO] Step[1200/2713]: training loss : 1.0108192443847657 TRAIN  loss dict:  {'classification_loss': 1.0108192443847657}
2025-01-15 16:21:06,848 [INFO] Step[1250/2713]: training loss : 0.990313104391098 TRAIN  loss dict:  {'classification_loss': 0.990313104391098}
2025-01-15 16:21:18,800 [INFO] Step[1300/2713]: training loss : 0.9414033842086792 TRAIN  loss dict:  {'classification_loss': 0.9414033842086792}
2025-01-15 16:21:30,801 [INFO] Step[1350/2713]: training loss : 0.9946551704406739 TRAIN  loss dict:  {'classification_loss': 0.9946551704406739}
2025-01-15 16:21:42,769 [INFO] Step[1400/2713]: training loss : 0.9467016267776489 TRAIN  loss dict:  {'classification_loss': 0.9467016267776489}
2025-01-15 16:21:54,725 [INFO] Step[1450/2713]: training loss : 1.0009325432777405 TRAIN  loss dict:  {'classification_loss': 1.0009325432777405}
2025-01-15 16:22:06,636 [INFO] Step[1500/2713]: training loss : 0.97553049325943 TRAIN  loss dict:  {'classification_loss': 0.97553049325943}
2025-01-15 16:22:18,582 [INFO] Step[1550/2713]: training loss : 0.9641583049297333 TRAIN  loss dict:  {'classification_loss': 0.9641583049297333}
2025-01-15 16:22:30,489 [INFO] Step[1600/2713]: training loss : 0.9608501195907593 TRAIN  loss dict:  {'classification_loss': 0.9608501195907593}
2025-01-15 16:22:42,463 [INFO] Step[1650/2713]: training loss : 0.9605938863754272 TRAIN  loss dict:  {'classification_loss': 0.9605938863754272}
2025-01-15 16:22:54,397 [INFO] Step[1700/2713]: training loss : 1.0453543639183045 TRAIN  loss dict:  {'classification_loss': 1.0453543639183045}
2025-01-15 16:23:06,327 [INFO] Step[1750/2713]: training loss : 0.9548796045780182 TRAIN  loss dict:  {'classification_loss': 0.9548796045780182}
2025-01-15 16:23:18,239 [INFO] Step[1800/2713]: training loss : 0.9701452350616455 TRAIN  loss dict:  {'classification_loss': 0.9701452350616455}
2025-01-15 16:23:30,173 [INFO] Step[1850/2713]: training loss : 0.9777884829044342 TRAIN  loss dict:  {'classification_loss': 0.9777884829044342}
2025-01-15 16:23:42,068 [INFO] Step[1900/2713]: training loss : 0.9962775385379792 TRAIN  loss dict:  {'classification_loss': 0.9962775385379792}
2025-01-15 16:23:53,967 [INFO] Step[1950/2713]: training loss : 1.015621361732483 TRAIN  loss dict:  {'classification_loss': 1.015621361732483}
2025-01-15 16:24:05,903 [INFO] Step[2000/2713]: training loss : 0.9657838249206543 TRAIN  loss dict:  {'classification_loss': 0.9657838249206543}
2025-01-15 16:24:17,843 [INFO] Step[2050/2713]: training loss : 0.9753760802745819 TRAIN  loss dict:  {'classification_loss': 0.9753760802745819}
2025-01-15 16:24:29,781 [INFO] Step[2100/2713]: training loss : 1.0125571119785308 TRAIN  loss dict:  {'classification_loss': 1.0125571119785308}
2025-01-15 16:24:41,766 [INFO] Step[2150/2713]: training loss : 0.9941297698020936 TRAIN  loss dict:  {'classification_loss': 0.9941297698020936}
2025-01-15 16:24:53,661 [INFO] Step[2200/2713]: training loss : 1.0484635353088378 TRAIN  loss dict:  {'classification_loss': 1.0484635353088378}
2025-01-15 16:25:05,587 [INFO] Step[2250/2713]: training loss : 0.9901108157634735 TRAIN  loss dict:  {'classification_loss': 0.9901108157634735}
2025-01-15 16:25:17,557 [INFO] Step[2300/2713]: training loss : 1.000006983280182 TRAIN  loss dict:  {'classification_loss': 1.000006983280182}
2025-01-15 16:25:29,489 [INFO] Step[2350/2713]: training loss : 0.9773829960823059 TRAIN  loss dict:  {'classification_loss': 0.9773829960823059}
2025-01-15 16:25:41,466 [INFO] Step[2400/2713]: training loss : 1.0422614979743958 TRAIN  loss dict:  {'classification_loss': 1.0422614979743958}
2025-01-15 16:25:53,394 [INFO] Step[2450/2713]: training loss : 0.9670340979099273 TRAIN  loss dict:  {'classification_loss': 0.9670340979099273}
2025-01-15 16:26:05,316 [INFO] Step[2500/2713]: training loss : 1.0234296345710754 TRAIN  loss dict:  {'classification_loss': 1.0234296345710754}
2025-01-15 16:26:17,235 [INFO] Step[2550/2713]: training loss : 0.9892234575748443 TRAIN  loss dict:  {'classification_loss': 0.9892234575748443}
2025-01-15 16:26:29,193 [INFO] Step[2600/2713]: training loss : 0.9652030313014984 TRAIN  loss dict:  {'classification_loss': 0.9652030313014984}
2025-01-15 16:26:41,151 [INFO] Step[2650/2713]: training loss : 0.9728188848495484 TRAIN  loss dict:  {'classification_loss': 0.9728188848495484}
2025-01-15 16:26:53,042 [INFO] Step[2700/2713]: training loss : 1.0065033340454101 TRAIN  loss dict:  {'classification_loss': 1.0065033340454101}
2025-01-15 16:28:07,616 [INFO] Label accuracies statistics:
2025-01-15 16:28:07,616 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 1.0, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.25, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.5, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.25, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.0, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.25, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 0.75, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.25, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 0.5, 118: 0.75, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.0, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 0.75, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 0.5, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.5, 185: 0.75, 186: 0.5, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 0.5, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.25, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.5, 356: 0.5, 357: 0.75, 358: 1.0, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 16:28:40,479 [INFO] [8] TRAIN  loss: 0.9906581663971145 acc: 0.9869762870131465
2025-01-15 16:28:40,479 [INFO] [8] TRAIN  loss dict: {'classification_loss': 0.9906581663971145}
2025-01-15 16:28:40,479 [INFO] [8] VALIDATION loss: 2.0246061835073887 VALIDATION acc: 0.7573667711598746
2025-01-15 16:28:40,479 [INFO] [8] VALIDATION loss dict: {'classification_loss': 2.0246061835073887}
2025-01-15 16:28:40,479 [INFO] 
2025-01-15 16:28:57,883 [INFO] Step[50/2713]: training loss : 0.9796172511577607 TRAIN  loss dict:  {'classification_loss': 0.9796172511577607}
2025-01-15 16:29:09,692 [INFO] Step[100/2713]: training loss : 1.044279831647873 TRAIN  loss dict:  {'classification_loss': 1.044279831647873}
2025-01-15 16:29:21,568 [INFO] Step[150/2713]: training loss : 1.0556705343723296 TRAIN  loss dict:  {'classification_loss': 1.0556705343723296}
2025-01-15 16:29:33,452 [INFO] Step[200/2713]: training loss : 0.9741758513450622 TRAIN  loss dict:  {'classification_loss': 0.9741758513450622}
2025-01-15 16:29:45,354 [INFO] Step[250/2713]: training loss : 0.9729880452156067 TRAIN  loss dict:  {'classification_loss': 0.9729880452156067}
2025-01-15 16:29:57,261 [INFO] Step[300/2713]: training loss : 0.9840882444381713 TRAIN  loss dict:  {'classification_loss': 0.9840882444381713}
2025-01-15 16:30:09,181 [INFO] Step[350/2713]: training loss : 0.9706346237659454 TRAIN  loss dict:  {'classification_loss': 0.9706346237659454}
2025-01-15 16:30:21,056 [INFO] Step[400/2713]: training loss : 1.002482739686966 TRAIN  loss dict:  {'classification_loss': 1.002482739686966}
2025-01-15 16:30:32,965 [INFO] Step[450/2713]: training loss : 0.9661699831485748 TRAIN  loss dict:  {'classification_loss': 0.9661699831485748}
2025-01-15 16:30:44,838 [INFO] Step[500/2713]: training loss : 0.9972739088535308 TRAIN  loss dict:  {'classification_loss': 0.9972739088535308}
2025-01-15 16:30:56,739 [INFO] Step[550/2713]: training loss : 1.04096888422966 TRAIN  loss dict:  {'classification_loss': 1.04096888422966}
2025-01-15 16:31:08,626 [INFO] Step[600/2713]: training loss : 1.062139984369278 TRAIN  loss dict:  {'classification_loss': 1.062139984369278}
2025-01-15 16:31:20,512 [INFO] Step[650/2713]: training loss : 0.958288871049881 TRAIN  loss dict:  {'classification_loss': 0.958288871049881}
2025-01-15 16:31:32,414 [INFO] Step[700/2713]: training loss : 0.9896984469890594 TRAIN  loss dict:  {'classification_loss': 0.9896984469890594}
2025-01-15 16:31:44,379 [INFO] Step[750/2713]: training loss : 0.9712281501293183 TRAIN  loss dict:  {'classification_loss': 0.9712281501293183}
2025-01-15 16:31:56,316 [INFO] Step[800/2713]: training loss : 1.0925739789009095 TRAIN  loss dict:  {'classification_loss': 1.0925739789009095}
2025-01-15 16:32:08,229 [INFO] Step[850/2713]: training loss : 0.9459423017501831 TRAIN  loss dict:  {'classification_loss': 0.9459423017501831}
2025-01-15 16:32:20,127 [INFO] Step[900/2713]: training loss : 0.9956339120864868 TRAIN  loss dict:  {'classification_loss': 0.9956339120864868}
2025-01-15 16:32:32,062 [INFO] Step[950/2713]: training loss : 0.9545162761211395 TRAIN  loss dict:  {'classification_loss': 0.9545162761211395}
2025-01-15 16:32:43,963 [INFO] Step[1000/2713]: training loss : 1.0561820340156556 TRAIN  loss dict:  {'classification_loss': 1.0561820340156556}
2025-01-15 16:32:55,880 [INFO] Step[1050/2713]: training loss : 1.009010648727417 TRAIN  loss dict:  {'classification_loss': 1.009010648727417}
2025-01-15 16:33:07,788 [INFO] Step[1100/2713]: training loss : 1.0093591690063477 TRAIN  loss dict:  {'classification_loss': 1.0093591690063477}
2025-01-15 16:33:19,724 [INFO] Step[1150/2713]: training loss : 1.0557303488254548 TRAIN  loss dict:  {'classification_loss': 1.0557303488254548}
2025-01-15 16:33:31,653 [INFO] Step[1200/2713]: training loss : 1.0328287541866303 TRAIN  loss dict:  {'classification_loss': 1.0328287541866303}
2025-01-15 16:33:43,603 [INFO] Step[1250/2713]: training loss : 1.005598613023758 TRAIN  loss dict:  {'classification_loss': 1.005598613023758}
2025-01-15 16:33:55,462 [INFO] Step[1300/2713]: training loss : 0.9528810071945191 TRAIN  loss dict:  {'classification_loss': 0.9528810071945191}
2025-01-15 16:34:07,363 [INFO] Step[1350/2713]: training loss : 0.9456350076198577 TRAIN  loss dict:  {'classification_loss': 0.9456350076198577}
2025-01-15 16:34:19,272 [INFO] Step[1400/2713]: training loss : 0.9599769949913025 TRAIN  loss dict:  {'classification_loss': 0.9599769949913025}
2025-01-15 16:34:31,165 [INFO] Step[1450/2713]: training loss : 0.9814040029048919 TRAIN  loss dict:  {'classification_loss': 0.9814040029048919}
2025-01-15 16:34:43,036 [INFO] Step[1500/2713]: training loss : 0.9613372921943665 TRAIN  loss dict:  {'classification_loss': 0.9613372921943665}
2025-01-15 16:34:54,953 [INFO] Step[1550/2713]: training loss : 0.9968325555324554 TRAIN  loss dict:  {'classification_loss': 0.9968325555324554}
2025-01-15 16:35:06,836 [INFO] Step[1600/2713]: training loss : 0.985093035697937 TRAIN  loss dict:  {'classification_loss': 0.985093035697937}
2025-01-15 16:35:18,727 [INFO] Step[1650/2713]: training loss : 0.9548557412624359 TRAIN  loss dict:  {'classification_loss': 0.9548557412624359}
2025-01-15 16:35:30,610 [INFO] Step[1700/2713]: training loss : 0.9994825983047485 TRAIN  loss dict:  {'classification_loss': 0.9994825983047485}
2025-01-15 16:35:42,509 [INFO] Step[1750/2713]: training loss : 0.9854639959335327 TRAIN  loss dict:  {'classification_loss': 0.9854639959335327}
2025-01-15 16:35:54,401 [INFO] Step[1800/2713]: training loss : 0.9585722970962525 TRAIN  loss dict:  {'classification_loss': 0.9585722970962525}
2025-01-15 16:36:06,305 [INFO] Step[1850/2713]: training loss : 0.9821530818939209 TRAIN  loss dict:  {'classification_loss': 0.9821530818939209}
2025-01-15 16:36:18,203 [INFO] Step[1900/2713]: training loss : 1.0457906329631805 TRAIN  loss dict:  {'classification_loss': 1.0457906329631805}
2025-01-15 16:36:30,112 [INFO] Step[1950/2713]: training loss : 1.0102763962745667 TRAIN  loss dict:  {'classification_loss': 1.0102763962745667}
2025-01-15 16:36:42,001 [INFO] Step[2000/2713]: training loss : 0.9551163017749786 TRAIN  loss dict:  {'classification_loss': 0.9551163017749786}
2025-01-15 16:36:53,935 [INFO] Step[2050/2713]: training loss : 0.9738394439220428 TRAIN  loss dict:  {'classification_loss': 0.9738394439220428}
2025-01-15 16:37:05,832 [INFO] Step[2100/2713]: training loss : 1.0465160167217256 TRAIN  loss dict:  {'classification_loss': 1.0465160167217256}
2025-01-15 16:37:17,748 [INFO] Step[2150/2713]: training loss : 0.9824377000331879 TRAIN  loss dict:  {'classification_loss': 0.9824377000331879}
2025-01-15 16:37:29,604 [INFO] Step[2200/2713]: training loss : 0.9925961196422577 TRAIN  loss dict:  {'classification_loss': 0.9925961196422577}
2025-01-15 16:37:41,530 [INFO] Step[2250/2713]: training loss : 0.9503153038024902 TRAIN  loss dict:  {'classification_loss': 0.9503153038024902}
2025-01-15 16:37:53,424 [INFO] Step[2300/2713]: training loss : 1.0688321244716645 TRAIN  loss dict:  {'classification_loss': 1.0688321244716645}
2025-01-15 16:38:05,327 [INFO] Step[2350/2713]: training loss : 1.0039239811897278 TRAIN  loss dict:  {'classification_loss': 1.0039239811897278}
2025-01-15 16:38:17,238 [INFO] Step[2400/2713]: training loss : 0.9605830597877503 TRAIN  loss dict:  {'classification_loss': 0.9605830597877503}
2025-01-15 16:38:29,156 [INFO] Step[2450/2713]: training loss : 0.9900676715373993 TRAIN  loss dict:  {'classification_loss': 0.9900676715373993}
2025-01-15 16:38:41,046 [INFO] Step[2500/2713]: training loss : 0.9849962770938874 TRAIN  loss dict:  {'classification_loss': 0.9849962770938874}
2025-01-15 16:38:52,952 [INFO] Step[2550/2713]: training loss : 1.0387231314182281 TRAIN  loss dict:  {'classification_loss': 1.0387231314182281}
2025-01-15 16:39:04,862 [INFO] Step[2600/2713]: training loss : 0.956619005203247 TRAIN  loss dict:  {'classification_loss': 0.956619005203247}
2025-01-15 16:39:16,757 [INFO] Step[2650/2713]: training loss : 0.9549343657493591 TRAIN  loss dict:  {'classification_loss': 0.9549343657493591}
2025-01-15 16:39:28,683 [INFO] Step[2700/2713]: training loss : 1.0699285876750946 TRAIN  loss dict:  {'classification_loss': 1.0699285876750946}
2025-01-15 16:40:42,689 [INFO] Label accuracies statistics:
2025-01-15 16:40:42,689 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.25, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.5, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 0.75, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.75, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.75, 144: 0.75, 145: 0.5, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 0.75, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.5, 186: 0.5, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.5, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 0.5, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.5, 226: 0.5, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 0.75, 256: 1.0, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.25, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.0, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.0, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.5, 326: 0.75, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.25, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.25, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.25, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.5, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.5, 380: 1.0, 381: 0.0, 382: 0.75, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.25, 388: 0.5, 389: 0.75, 390: 0.0, 391: 0.5, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 16:40:42,691 [INFO] [9] TRAIN  loss: 0.9956610724577111 acc: 0.9830446000737191
2025-01-15 16:40:42,691 [INFO] [9] TRAIN  loss dict: {'classification_loss': 0.9956610724577111}
2025-01-15 16:40:42,691 [INFO] [9] VALIDATION loss: 2.1094829754945925 VALIDATION acc: 0.7366771159874608
2025-01-15 16:40:42,691 [INFO] [9] VALIDATION loss dict: {'classification_loss': 2.1094829754945925}
2025-01-15 16:40:42,692 [INFO] 
2025-01-15 16:40:59,571 [INFO] Step[50/2713]: training loss : 0.9755679237842559 TRAIN  loss dict:  {'classification_loss': 0.9755679237842559}
2025-01-15 16:41:11,413 [INFO] Step[100/2713]: training loss : 1.0303555619716644 TRAIN  loss dict:  {'classification_loss': 1.0303555619716644}
2025-01-15 16:41:23,302 [INFO] Step[150/2713]: training loss : 0.950229287147522 TRAIN  loss dict:  {'classification_loss': 0.950229287147522}
2025-01-15 16:41:35,204 [INFO] Step[200/2713]: training loss : 0.9682374930381775 TRAIN  loss dict:  {'classification_loss': 0.9682374930381775}
2025-01-15 16:41:47,122 [INFO] Step[250/2713]: training loss : 0.9538540554046631 TRAIN  loss dict:  {'classification_loss': 0.9538540554046631}
2025-01-15 16:41:59,066 [INFO] Step[300/2713]: training loss : 1.067535433769226 TRAIN  loss dict:  {'classification_loss': 1.067535433769226}
2025-01-15 16:42:11,021 [INFO] Step[350/2713]: training loss : 1.048238765001297 TRAIN  loss dict:  {'classification_loss': 1.048238765001297}
2025-01-15 16:42:22,935 [INFO] Step[400/2713]: training loss : 1.0113124537467957 TRAIN  loss dict:  {'classification_loss': 1.0113124537467957}
2025-01-15 16:42:34,875 [INFO] Step[450/2713]: training loss : 1.067205501794815 TRAIN  loss dict:  {'classification_loss': 1.067205501794815}
2025-01-15 16:42:46,817 [INFO] Step[500/2713]: training loss : 0.9503089094161987 TRAIN  loss dict:  {'classification_loss': 0.9503089094161987}
2025-01-15 16:42:58,828 [INFO] Step[550/2713]: training loss : 0.986917575597763 TRAIN  loss dict:  {'classification_loss': 0.986917575597763}
2025-01-15 16:43:10,753 [INFO] Step[600/2713]: training loss : 0.9431301760673523 TRAIN  loss dict:  {'classification_loss': 0.9431301760673523}
2025-01-15 16:43:22,660 [INFO] Step[650/2713]: training loss : 0.960364305973053 TRAIN  loss dict:  {'classification_loss': 0.960364305973053}
2025-01-15 16:43:34,564 [INFO] Step[700/2713]: training loss : 1.0037171459197998 TRAIN  loss dict:  {'classification_loss': 1.0037171459197998}
2025-01-15 16:43:46,477 [INFO] Step[750/2713]: training loss : 0.9952075552940368 TRAIN  loss dict:  {'classification_loss': 0.9952075552940368}
2025-01-15 16:43:58,416 [INFO] Step[800/2713]: training loss : 0.9627604496479034 TRAIN  loss dict:  {'classification_loss': 0.9627604496479034}
2025-01-15 16:44:10,375 [INFO] Step[850/2713]: training loss : 0.9469570350646973 TRAIN  loss dict:  {'classification_loss': 0.9469570350646973}
2025-01-15 16:44:22,335 [INFO] Step[900/2713]: training loss : 0.9934978342056274 TRAIN  loss dict:  {'classification_loss': 0.9934978342056274}
2025-01-15 16:44:34,275 [INFO] Step[950/2713]: training loss : 0.973387302160263 TRAIN  loss dict:  {'classification_loss': 0.973387302160263}
2025-01-15 16:44:46,170 [INFO] Step[1000/2713]: training loss : 0.9446811842918396 TRAIN  loss dict:  {'classification_loss': 0.9446811842918396}
2025-01-15 16:44:58,115 [INFO] Step[1050/2713]: training loss : 1.0485247600078582 TRAIN  loss dict:  {'classification_loss': 1.0485247600078582}
2025-01-15 16:45:10,052 [INFO] Step[1100/2713]: training loss : 1.029216192960739 TRAIN  loss dict:  {'classification_loss': 1.029216192960739}
2025-01-15 16:45:21,990 [INFO] Step[1150/2713]: training loss : 0.9879242396354675 TRAIN  loss dict:  {'classification_loss': 0.9879242396354675}
2025-01-15 16:45:33,910 [INFO] Step[1200/2713]: training loss : 0.9618006789684296 TRAIN  loss dict:  {'classification_loss': 0.9618006789684296}
2025-01-15 16:45:45,814 [INFO] Step[1250/2713]: training loss : 0.986467604637146 TRAIN  loss dict:  {'classification_loss': 0.986467604637146}
2025-01-15 16:45:57,732 [INFO] Step[1300/2713]: training loss : 1.0085195744037627 TRAIN  loss dict:  {'classification_loss': 1.0085195744037627}
2025-01-15 16:46:09,660 [INFO] Step[1350/2713]: training loss : 0.983256653547287 TRAIN  loss dict:  {'classification_loss': 0.983256653547287}
2025-01-15 16:46:21,579 [INFO] Step[1400/2713]: training loss : 0.9740381586551666 TRAIN  loss dict:  {'classification_loss': 0.9740381586551666}
2025-01-15 16:46:33,517 [INFO] Step[1450/2713]: training loss : 1.041764919757843 TRAIN  loss dict:  {'classification_loss': 1.041764919757843}
2025-01-15 16:46:45,418 [INFO] Step[1500/2713]: training loss : 1.0066164803504944 TRAIN  loss dict:  {'classification_loss': 1.0066164803504944}
2025-01-15 16:46:57,340 [INFO] Step[1550/2713]: training loss : 1.0269236147403717 TRAIN  loss dict:  {'classification_loss': 1.0269236147403717}
2025-01-15 16:47:09,228 [INFO] Step[1600/2713]: training loss : 1.0679512691497803 TRAIN  loss dict:  {'classification_loss': 1.0679512691497803}
2025-01-15 16:47:21,156 [INFO] Step[1650/2713]: training loss : 0.957004109621048 TRAIN  loss dict:  {'classification_loss': 0.957004109621048}
2025-01-15 16:47:33,056 [INFO] Step[1700/2713]: training loss : 0.9600748646259308 TRAIN  loss dict:  {'classification_loss': 0.9600748646259308}
2025-01-15 16:47:44,988 [INFO] Step[1750/2713]: training loss : 1.0373516774177551 TRAIN  loss dict:  {'classification_loss': 1.0373516774177551}
2025-01-15 16:47:56,975 [INFO] Step[1800/2713]: training loss : 0.9575424456596374 TRAIN  loss dict:  {'classification_loss': 0.9575424456596374}
2025-01-15 16:48:08,924 [INFO] Step[1850/2713]: training loss : 0.9735762715339661 TRAIN  loss dict:  {'classification_loss': 0.9735762715339661}
2025-01-15 16:48:20,868 [INFO] Step[1900/2713]: training loss : 1.0203117156028747 TRAIN  loss dict:  {'classification_loss': 1.0203117156028747}
2025-01-15 16:48:32,798 [INFO] Step[1950/2713]: training loss : 1.0045523202419282 TRAIN  loss dict:  {'classification_loss': 1.0045523202419282}
2025-01-15 16:48:44,711 [INFO] Step[2000/2713]: training loss : 0.9663511681556701 TRAIN  loss dict:  {'classification_loss': 0.9663511681556701}
2025-01-15 16:48:56,675 [INFO] Step[2050/2713]: training loss : 0.9943334412574768 TRAIN  loss dict:  {'classification_loss': 0.9943334412574768}
2025-01-15 16:49:08,611 [INFO] Step[2100/2713]: training loss : 0.9710451555252075 TRAIN  loss dict:  {'classification_loss': 0.9710451555252075}
2025-01-15 16:49:20,532 [INFO] Step[2150/2713]: training loss : 0.9659854626655578 TRAIN  loss dict:  {'classification_loss': 0.9659854626655578}
2025-01-15 16:49:32,469 [INFO] Step[2200/2713]: training loss : 1.0120750331878663 TRAIN  loss dict:  {'classification_loss': 1.0120750331878663}
2025-01-15 16:49:44,415 [INFO] Step[2250/2713]: training loss : 1.0384949672222137 TRAIN  loss dict:  {'classification_loss': 1.0384949672222137}
2025-01-15 16:49:56,340 [INFO] Step[2300/2713]: training loss : 0.9915971171855926 TRAIN  loss dict:  {'classification_loss': 0.9915971171855926}
2025-01-15 16:50:08,311 [INFO] Step[2350/2713]: training loss : 1.0312974762916565 TRAIN  loss dict:  {'classification_loss': 1.0312974762916565}
2025-01-15 16:50:20,240 [INFO] Step[2400/2713]: training loss : 1.0190242874622344 TRAIN  loss dict:  {'classification_loss': 1.0190242874622344}
2025-01-15 16:50:32,155 [INFO] Step[2450/2713]: training loss : 1.0108778488636017 TRAIN  loss dict:  {'classification_loss': 1.0108778488636017}
2025-01-15 16:50:44,087 [INFO] Step[2500/2713]: training loss : 0.9591921055316925 TRAIN  loss dict:  {'classification_loss': 0.9591921055316925}
2025-01-15 16:50:56,020 [INFO] Step[2550/2713]: training loss : 0.9741043424606324 TRAIN  loss dict:  {'classification_loss': 0.9741043424606324}
2025-01-15 16:51:07,931 [INFO] Step[2600/2713]: training loss : 0.9938873994350433 TRAIN  loss dict:  {'classification_loss': 0.9938873994350433}
2025-01-15 16:51:19,877 [INFO] Step[2650/2713]: training loss : 0.9795138394832611 TRAIN  loss dict:  {'classification_loss': 0.9795138394832611}
2025-01-15 16:51:31,803 [INFO] Step[2700/2713]: training loss : 1.001800493001938 TRAIN  loss dict:  {'classification_loss': 1.001800493001938}
2025-01-15 16:52:51,781 [INFO] Label accuracies statistics:
2025-01-15 16:52:51,782 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 0.25, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 0.75, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.5, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.75, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.75, 115: 0.75, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.25, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.5, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.5, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 0.5, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.25, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.5, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.5, 208: 0.25, 209: 0.75, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.25, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.0, 291: 0.5, 292: 0.75, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.25, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 0.75, 306: 0.75, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.25, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.5, 338: 1.0, 339: 0.75, 340: 0.5, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 0.25, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.5, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 16:52:51,808 [INFO] [10] TRAIN  loss: 0.9937569549766537 acc: 0.9846418478928616
2025-01-15 16:52:51,808 [INFO] [10] TRAIN  loss dict: {'classification_loss': 0.9937569549766537}
2025-01-15 16:52:51,808 [INFO] [10] VALIDATION loss: 2.0719082537002134 VALIDATION acc: 0.7542319749216301
2025-01-15 16:52:51,808 [INFO] [10] VALIDATION loss dict: {'classification_loss': 2.0719082537002134}
2025-01-15 16:52:51,808 [INFO] 
2025-01-15 16:53:09,627 [INFO] Step[50/2713]: training loss : 1.0605791640281677 TRAIN  loss dict:  {'classification_loss': 1.0605791640281677}
2025-01-15 16:53:21,521 [INFO] Step[100/2713]: training loss : 1.0101168131828309 TRAIN  loss dict:  {'classification_loss': 1.0101168131828309}
2025-01-15 16:53:33,431 [INFO] Step[150/2713]: training loss : 0.9793117654323578 TRAIN  loss dict:  {'classification_loss': 0.9793117654323578}
2025-01-15 16:53:45,374 [INFO] Step[200/2713]: training loss : 0.9581618595123291 TRAIN  loss dict:  {'classification_loss': 0.9581618595123291}
2025-01-15 16:53:57,304 [INFO] Step[250/2713]: training loss : 0.951013661623001 TRAIN  loss dict:  {'classification_loss': 0.951013661623001}
2025-01-15 16:54:09,229 [INFO] Step[300/2713]: training loss : 1.0112124967575074 TRAIN  loss dict:  {'classification_loss': 1.0112124967575074}
2025-01-15 16:54:21,172 [INFO] Step[350/2713]: training loss : 1.0237752676010132 TRAIN  loss dict:  {'classification_loss': 1.0237752676010132}
2025-01-15 16:54:33,127 [INFO] Step[400/2713]: training loss : 0.9555217945575714 TRAIN  loss dict:  {'classification_loss': 0.9555217945575714}
2025-01-15 16:54:45,084 [INFO] Step[450/2713]: training loss : 0.9552115309238434 TRAIN  loss dict:  {'classification_loss': 0.9552115309238434}
2025-01-15 16:54:57,022 [INFO] Step[500/2713]: training loss : 0.9903427302837372 TRAIN  loss dict:  {'classification_loss': 0.9903427302837372}
2025-01-15 16:55:08,928 [INFO] Step[550/2713]: training loss : 0.9814928948879242 TRAIN  loss dict:  {'classification_loss': 0.9814928948879242}
2025-01-15 16:55:20,865 [INFO] Step[600/2713]: training loss : 1.0179314708709717 TRAIN  loss dict:  {'classification_loss': 1.0179314708709717}
2025-01-15 16:55:32,840 [INFO] Step[650/2713]: training loss : 1.041626921892166 TRAIN  loss dict:  {'classification_loss': 1.041626921892166}
2025-01-15 16:55:44,753 [INFO] Step[700/2713]: training loss : 1.0240939271450042 TRAIN  loss dict:  {'classification_loss': 1.0240939271450042}
2025-01-15 16:55:56,717 [INFO] Step[750/2713]: training loss : 0.9696865582466125 TRAIN  loss dict:  {'classification_loss': 0.9696865582466125}
2025-01-15 16:56:08,665 [INFO] Step[800/2713]: training loss : 0.9853431487083435 TRAIN  loss dict:  {'classification_loss': 0.9853431487083435}
2025-01-15 16:56:20,611 [INFO] Step[850/2713]: training loss : 0.9372099459171295 TRAIN  loss dict:  {'classification_loss': 0.9372099459171295}
2025-01-15 16:56:32,568 [INFO] Step[900/2713]: training loss : 0.9496333742141724 TRAIN  loss dict:  {'classification_loss': 0.9496333742141724}
2025-01-15 16:56:44,507 [INFO] Step[950/2713]: training loss : 0.9929322791099549 TRAIN  loss dict:  {'classification_loss': 0.9929322791099549}
2025-01-15 16:56:56,434 [INFO] Step[1000/2713]: training loss : 0.9935454869270325 TRAIN  loss dict:  {'classification_loss': 0.9935454869270325}
2025-01-15 16:57:08,362 [INFO] Step[1050/2713]: training loss : 0.9729532885551453 TRAIN  loss dict:  {'classification_loss': 0.9729532885551453}
2025-01-15 16:57:20,291 [INFO] Step[1100/2713]: training loss : 0.9544097650051117 TRAIN  loss dict:  {'classification_loss': 0.9544097650051117}
2025-01-15 16:57:32,212 [INFO] Step[1150/2713]: training loss : 0.9507998192310333 TRAIN  loss dict:  {'classification_loss': 0.9507998192310333}
2025-01-15 16:57:44,136 [INFO] Step[1200/2713]: training loss : 0.9633896505832672 TRAIN  loss dict:  {'classification_loss': 0.9633896505832672}
2025-01-15 16:57:56,089 [INFO] Step[1250/2713]: training loss : 0.9421630811691284 TRAIN  loss dict:  {'classification_loss': 0.9421630811691284}
2025-01-15 16:58:08,057 [INFO] Step[1300/2713]: training loss : 0.9883625185489655 TRAIN  loss dict:  {'classification_loss': 0.9883625185489655}
2025-01-15 16:58:20,017 [INFO] Step[1350/2713]: training loss : 0.979846283197403 TRAIN  loss dict:  {'classification_loss': 0.979846283197403}
2025-01-15 16:58:31,952 [INFO] Step[1400/2713]: training loss : 0.9502312552928924 TRAIN  loss dict:  {'classification_loss': 0.9502312552928924}
2025-01-15 16:58:43,875 [INFO] Step[1450/2713]: training loss : 0.9666993594169617 TRAIN  loss dict:  {'classification_loss': 0.9666993594169617}
2025-01-15 16:58:55,799 [INFO] Step[1500/2713]: training loss : 0.9494919574260712 TRAIN  loss dict:  {'classification_loss': 0.9494919574260712}
2025-01-15 16:59:07,739 [INFO] Step[1550/2713]: training loss : 0.9657293283939361 TRAIN  loss dict:  {'classification_loss': 0.9657293283939361}
2025-01-15 16:59:19,699 [INFO] Step[1600/2713]: training loss : 1.0009942197799682 TRAIN  loss dict:  {'classification_loss': 1.0009942197799682}
2025-01-15 16:59:31,659 [INFO] Step[1650/2713]: training loss : 0.9717291498184204 TRAIN  loss dict:  {'classification_loss': 0.9717291498184204}
2025-01-15 16:59:43,579 [INFO] Step[1700/2713]: training loss : 0.9474454259872437 TRAIN  loss dict:  {'classification_loss': 0.9474454259872437}
2025-01-15 16:59:55,521 [INFO] Step[1750/2713]: training loss : 0.9716313660144806 TRAIN  loss dict:  {'classification_loss': 0.9716313660144806}
2025-01-15 17:00:07,560 [INFO] Step[1800/2713]: training loss : 0.9537039232254029 TRAIN  loss dict:  {'classification_loss': 0.9537039232254029}
2025-01-15 17:00:19,544 [INFO] Step[1850/2713]: training loss : 0.9999693048000335 TRAIN  loss dict:  {'classification_loss': 0.9999693048000335}
2025-01-15 17:00:31,465 [INFO] Step[1900/2713]: training loss : 1.0120411968231202 TRAIN  loss dict:  {'classification_loss': 1.0120411968231202}
2025-01-15 17:00:43,407 [INFO] Step[1950/2713]: training loss : 0.9418535983562469 TRAIN  loss dict:  {'classification_loss': 0.9418535983562469}
2025-01-15 17:00:55,340 [INFO] Step[2000/2713]: training loss : 0.9611461794376374 TRAIN  loss dict:  {'classification_loss': 0.9611461794376374}
2025-01-15 17:01:07,256 [INFO] Step[2050/2713]: training loss : 0.964979110956192 TRAIN  loss dict:  {'classification_loss': 0.964979110956192}
2025-01-15 17:01:19,189 [INFO] Step[2100/2713]: training loss : 0.9717554533481598 TRAIN  loss dict:  {'classification_loss': 0.9717554533481598}
2025-01-15 17:01:31,102 [INFO] Step[2150/2713]: training loss : 0.9479748117923736 TRAIN  loss dict:  {'classification_loss': 0.9479748117923736}
2025-01-15 17:01:43,008 [INFO] Step[2200/2713]: training loss : 1.0389186561107635 TRAIN  loss dict:  {'classification_loss': 1.0389186561107635}
2025-01-15 17:01:54,942 [INFO] Step[2250/2713]: training loss : 0.9435360956192017 TRAIN  loss dict:  {'classification_loss': 0.9435360956192017}
2025-01-15 17:02:06,882 [INFO] Step[2300/2713]: training loss : 0.9369460678100586 TRAIN  loss dict:  {'classification_loss': 0.9369460678100586}
2025-01-15 17:02:18,859 [INFO] Step[2350/2713]: training loss : 0.9478128445148468 TRAIN  loss dict:  {'classification_loss': 0.9478128445148468}
2025-01-15 17:02:30,785 [INFO] Step[2400/2713]: training loss : 0.9455137360095978 TRAIN  loss dict:  {'classification_loss': 0.9455137360095978}
2025-01-15 17:02:42,776 [INFO] Step[2450/2713]: training loss : 1.060840619802475 TRAIN  loss dict:  {'classification_loss': 1.060840619802475}
2025-01-15 17:02:54,708 [INFO] Step[2500/2713]: training loss : 1.01061784863472 TRAIN  loss dict:  {'classification_loss': 1.01061784863472}
2025-01-15 17:03:06,662 [INFO] Step[2550/2713]: training loss : 0.965059505701065 TRAIN  loss dict:  {'classification_loss': 0.965059505701065}
2025-01-15 17:03:18,572 [INFO] Step[2600/2713]: training loss : 0.9768610262870788 TRAIN  loss dict:  {'classification_loss': 0.9768610262870788}
2025-01-15 17:03:30,525 [INFO] Step[2650/2713]: training loss : 0.9857723867893219 TRAIN  loss dict:  {'classification_loss': 0.9857723867893219}
2025-01-15 17:03:42,402 [INFO] Step[2700/2713]: training loss : 0.9414637589454651 TRAIN  loss dict:  {'classification_loss': 0.9414637589454651}
2025-01-15 17:05:45,499 [INFO] Label accuracies statistics:
2025-01-15 17:05:45,499 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.5, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.5, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.25, 108: 1.0, 109: 0.25, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.75, 115: 0.75, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 0.75, 127: 0.25, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.5, 133: 0.75, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.5, 170: 0.75, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.0, 243: 0.5, 244: 0.5, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 0.75, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.5, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.0, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.5, 305: 0.75, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.5, 351: 1.0, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 0.75, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.25, 373: 0.75, 374: 0.75, 375: 0.5, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.5, 395: 0.25, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 17:05:47,866 [INFO] [11] TRAIN  loss: 0.9770502991620087 acc: 0.9880820739648606
2025-01-15 17:05:47,866 [INFO] [11] TRAIN  loss dict: {'classification_loss': 0.9770502991620087}
2025-01-15 17:05:47,866 [INFO] [11] VALIDATION loss: 2.003556601983264 VALIDATION acc: 0.7617554858934169
2025-01-15 17:05:47,866 [INFO] [11] VALIDATION loss dict: {'classification_loss': 2.003556601983264}
2025-01-15 17:05:47,866 [INFO] 
2025-01-15 17:06:05,363 [INFO] Step[50/2713]: training loss : 0.957024233341217 TRAIN  loss dict:  {'classification_loss': 0.957024233341217}
2025-01-15 17:06:17,285 [INFO] Step[100/2713]: training loss : 0.9481844592094422 TRAIN  loss dict:  {'classification_loss': 0.9481844592094422}
2025-01-15 17:06:29,227 [INFO] Step[150/2713]: training loss : 0.939243552684784 TRAIN  loss dict:  {'classification_loss': 0.939243552684784}
2025-01-15 17:06:41,151 [INFO] Step[200/2713]: training loss : 0.9595402777194977 TRAIN  loss dict:  {'classification_loss': 0.9595402777194977}
2025-01-15 17:06:53,094 [INFO] Step[250/2713]: training loss : 0.9858886444568634 TRAIN  loss dict:  {'classification_loss': 0.9858886444568634}
2025-01-15 17:07:05,016 [INFO] Step[300/2713]: training loss : 0.9645603573322297 TRAIN  loss dict:  {'classification_loss': 0.9645603573322297}
2025-01-15 17:07:16,980 [INFO] Step[350/2713]: training loss : 0.9599912750720978 TRAIN  loss dict:  {'classification_loss': 0.9599912750720978}
2025-01-15 17:07:28,893 [INFO] Step[400/2713]: training loss : 0.9694303929805755 TRAIN  loss dict:  {'classification_loss': 0.9694303929805755}
2025-01-15 17:07:40,837 [INFO] Step[450/2713]: training loss : 0.9456980061531067 TRAIN  loss dict:  {'classification_loss': 0.9456980061531067}
2025-01-15 17:07:52,771 [INFO] Step[500/2713]: training loss : 0.9700354647636413 TRAIN  loss dict:  {'classification_loss': 0.9700354647636413}
2025-01-15 17:08:04,778 [INFO] Step[550/2713]: training loss : 1.0053054463863373 TRAIN  loss dict:  {'classification_loss': 1.0053054463863373}
2025-01-15 17:08:16,668 [INFO] Step[600/2713]: training loss : 0.9528188884258271 TRAIN  loss dict:  {'classification_loss': 0.9528188884258271}
2025-01-15 17:08:28,610 [INFO] Step[650/2713]: training loss : 0.9931971108913422 TRAIN  loss dict:  {'classification_loss': 0.9931971108913422}
2025-01-15 17:08:40,529 [INFO] Step[700/2713]: training loss : 0.9586561715602875 TRAIN  loss dict:  {'classification_loss': 0.9586561715602875}
2025-01-15 17:08:52,474 [INFO] Step[750/2713]: training loss : 0.945473530292511 TRAIN  loss dict:  {'classification_loss': 0.945473530292511}
2025-01-15 17:09:04,429 [INFO] Step[800/2713]: training loss : 0.9989138555526733 TRAIN  loss dict:  {'classification_loss': 0.9989138555526733}
2025-01-15 17:09:16,346 [INFO] Step[850/2713]: training loss : 0.9580932581424713 TRAIN  loss dict:  {'classification_loss': 0.9580932581424713}
2025-01-15 17:09:28,293 [INFO] Step[900/2713]: training loss : 0.9636590814590454 TRAIN  loss dict:  {'classification_loss': 0.9636590814590454}
2025-01-15 17:09:40,227 [INFO] Step[950/2713]: training loss : 0.9344473075866699 TRAIN  loss dict:  {'classification_loss': 0.9344473075866699}
2025-01-15 17:09:52,124 [INFO] Step[1000/2713]: training loss : 0.9434425985813141 TRAIN  loss dict:  {'classification_loss': 0.9434425985813141}
2025-01-15 17:10:04,047 [INFO] Step[1050/2713]: training loss : 0.9885504806041717 TRAIN  loss dict:  {'classification_loss': 0.9885504806041717}
2025-01-15 17:10:15,963 [INFO] Step[1100/2713]: training loss : 0.9455994725227356 TRAIN  loss dict:  {'classification_loss': 0.9455994725227356}
2025-01-15 17:10:27,887 [INFO] Step[1150/2713]: training loss : 0.995765334367752 TRAIN  loss dict:  {'classification_loss': 0.995765334367752}
2025-01-15 17:10:39,882 [INFO] Step[1200/2713]: training loss : 0.9498335456848145 TRAIN  loss dict:  {'classification_loss': 0.9498335456848145}
2025-01-15 17:10:51,809 [INFO] Step[1250/2713]: training loss : 0.9564279329776764 TRAIN  loss dict:  {'classification_loss': 0.9564279329776764}
2025-01-15 17:11:03,758 [INFO] Step[1300/2713]: training loss : 0.953358645439148 TRAIN  loss dict:  {'classification_loss': 0.953358645439148}
2025-01-15 17:11:15,683 [INFO] Step[1350/2713]: training loss : 0.9460661590099335 TRAIN  loss dict:  {'classification_loss': 0.9460661590099335}
2025-01-15 17:11:27,626 [INFO] Step[1400/2713]: training loss : 0.9388727593421936 TRAIN  loss dict:  {'classification_loss': 0.9388727593421936}
2025-01-15 17:11:39,546 [INFO] Step[1450/2713]: training loss : 0.9392343580722808 TRAIN  loss dict:  {'classification_loss': 0.9392343580722808}
2025-01-15 17:11:51,448 [INFO] Step[1500/2713]: training loss : 1.0029562318325043 TRAIN  loss dict:  {'classification_loss': 1.0029562318325043}
2025-01-15 17:12:03,373 [INFO] Step[1550/2713]: training loss : 0.9750124049186707 TRAIN  loss dict:  {'classification_loss': 0.9750124049186707}
2025-01-15 17:12:15,273 [INFO] Step[1600/2713]: training loss : 0.9465527963638306 TRAIN  loss dict:  {'classification_loss': 0.9465527963638306}
2025-01-15 17:12:27,205 [INFO] Step[1650/2713]: training loss : 0.9618779802322388 TRAIN  loss dict:  {'classification_loss': 0.9618779802322388}
2025-01-15 17:12:39,125 [INFO] Step[1700/2713]: training loss : 0.9503829431533813 TRAIN  loss dict:  {'classification_loss': 0.9503829431533813}
2025-01-15 17:12:51,087 [INFO] Step[1750/2713]: training loss : 1.0341409504413606 TRAIN  loss dict:  {'classification_loss': 1.0341409504413606}
2025-01-15 17:13:03,013 [INFO] Step[1800/2713]: training loss : 0.9332123577594758 TRAIN  loss dict:  {'classification_loss': 0.9332123577594758}
2025-01-15 17:13:14,941 [INFO] Step[1850/2713]: training loss : 0.9685949361324311 TRAIN  loss dict:  {'classification_loss': 0.9685949361324311}
2025-01-15 17:13:26,900 [INFO] Step[1900/2713]: training loss : 0.9631095540523529 TRAIN  loss dict:  {'classification_loss': 0.9631095540523529}
2025-01-15 17:13:38,823 [INFO] Step[1950/2713]: training loss : 0.971992381811142 TRAIN  loss dict:  {'classification_loss': 0.971992381811142}
2025-01-15 17:13:50,700 [INFO] Step[2000/2713]: training loss : 0.9575100529193878 TRAIN  loss dict:  {'classification_loss': 0.9575100529193878}
2025-01-15 17:14:02,616 [INFO] Step[2050/2713]: training loss : 0.9367352330684662 TRAIN  loss dict:  {'classification_loss': 0.9367352330684662}
2025-01-15 17:14:14,545 [INFO] Step[2100/2713]: training loss : 0.9678489768505096 TRAIN  loss dict:  {'classification_loss': 0.9678489768505096}
2025-01-15 17:14:26,466 [INFO] Step[2150/2713]: training loss : 0.9366329741477967 TRAIN  loss dict:  {'classification_loss': 0.9366329741477967}
2025-01-15 17:14:38,363 [INFO] Step[2200/2713]: training loss : 0.944320353269577 TRAIN  loss dict:  {'classification_loss': 0.944320353269577}
2025-01-15 17:14:50,274 [INFO] Step[2250/2713]: training loss : 0.9613087499141693 TRAIN  loss dict:  {'classification_loss': 0.9613087499141693}
2025-01-15 17:15:02,186 [INFO] Step[2300/2713]: training loss : 0.9749771928787232 TRAIN  loss dict:  {'classification_loss': 0.9749771928787232}
2025-01-15 17:15:14,127 [INFO] Step[2350/2713]: training loss : 0.9544594526290894 TRAIN  loss dict:  {'classification_loss': 0.9544594526290894}
2025-01-15 17:15:26,056 [INFO] Step[2400/2713]: training loss : 0.9653812062740326 TRAIN  loss dict:  {'classification_loss': 0.9653812062740326}
2025-01-15 17:15:37,967 [INFO] Step[2450/2713]: training loss : 0.9462825274467468 TRAIN  loss dict:  {'classification_loss': 0.9462825274467468}
2025-01-15 17:15:49,853 [INFO] Step[2500/2713]: training loss : 0.9691981995105743 TRAIN  loss dict:  {'classification_loss': 0.9691981995105743}
2025-01-15 17:16:01,789 [INFO] Step[2550/2713]: training loss : 0.984133208990097 TRAIN  loss dict:  {'classification_loss': 0.984133208990097}
2025-01-15 17:16:13,710 [INFO] Step[2600/2713]: training loss : 0.9560082793235779 TRAIN  loss dict:  {'classification_loss': 0.9560082793235779}
2025-01-15 17:16:25,652 [INFO] Step[2650/2713]: training loss : 0.9404152083396912 TRAIN  loss dict:  {'classification_loss': 0.9404152083396912}
2025-01-15 17:16:37,557 [INFO] Step[2700/2713]: training loss : 0.9787642478942871 TRAIN  loss dict:  {'classification_loss': 0.9787642478942871}
2025-01-15 17:17:50,668 [INFO] Label accuracies statistics:
2025-01-15 17:17:50,668 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.5, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 1.0, 56: 0.5, 57: 0.5, 58: 0.5, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.5, 63: 0.25, 64: 0.5, 65: 1.0, 66: 0.25, 67: 1.0, 68: 1.0, 69: 0.75, 70: 1.0, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.75, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.5, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.5, 185: 1.0, 186: 0.5, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.5, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.25, 243: 0.75, 244: 0.75, 245: 0.5, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.5, 265: 0.5, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.5, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.5, 288: 0.5, 289: 1.0, 290: 0.0, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.5, 303: 0.75, 304: 0.5, 305: 0.75, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.25, 321: 1.0, 322: 1.0, 323: 0.5, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.5, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 0.75, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 0.75, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.25, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 17:17:50,669 [INFO] [12] TRAIN  loss: 0.9624537624558794 acc: 0.9911537043862882
2025-01-15 17:17:50,670 [INFO] [12] TRAIN  loss dict: {'classification_loss': 0.9624537624558794}
2025-01-15 17:17:50,670 [INFO] [12] VALIDATION loss: 2.0840969631322346 VALIDATION acc: 0.7448275862068966
2025-01-15 17:17:50,670 [INFO] [12] VALIDATION loss dict: {'classification_loss': 2.0840969631322346}
2025-01-15 17:17:50,670 [INFO] 
2025-01-15 17:18:08,120 [INFO] Step[50/2713]: training loss : 1.0076814842224122 TRAIN  loss dict:  {'classification_loss': 1.0076814842224122}
2025-01-15 17:18:19,983 [INFO] Step[100/2713]: training loss : 0.9609486365318298 TRAIN  loss dict:  {'classification_loss': 0.9609486365318298}
2025-01-15 17:18:31,937 [INFO] Step[150/2713]: training loss : 0.9582308089733124 TRAIN  loss dict:  {'classification_loss': 0.9582308089733124}
2025-01-15 17:18:43,826 [INFO] Step[200/2713]: training loss : 0.9981335484981537 TRAIN  loss dict:  {'classification_loss': 0.9981335484981537}
2025-01-15 17:18:55,729 [INFO] Step[250/2713]: training loss : 0.9703288733959198 TRAIN  loss dict:  {'classification_loss': 0.9703288733959198}
2025-01-15 17:19:07,672 [INFO] Step[300/2713]: training loss : 0.9566484308242797 TRAIN  loss dict:  {'classification_loss': 0.9566484308242797}
2025-01-15 17:19:19,629 [INFO] Step[350/2713]: training loss : 0.9433147716522217 TRAIN  loss dict:  {'classification_loss': 0.9433147716522217}
2025-01-15 17:19:31,558 [INFO] Step[400/2713]: training loss : 1.0347616279125214 TRAIN  loss dict:  {'classification_loss': 1.0347616279125214}
2025-01-15 17:19:43,557 [INFO] Step[450/2713]: training loss : 0.9806632435321808 TRAIN  loss dict:  {'classification_loss': 0.9806632435321808}
2025-01-15 17:19:55,532 [INFO] Step[500/2713]: training loss : 0.9409110152721405 TRAIN  loss dict:  {'classification_loss': 0.9409110152721405}
2025-01-15 17:20:07,546 [INFO] Step[550/2713]: training loss : 0.9372162389755249 TRAIN  loss dict:  {'classification_loss': 0.9372162389755249}
2025-01-15 17:20:19,476 [INFO] Step[600/2713]: training loss : 0.9524701631069183 TRAIN  loss dict:  {'classification_loss': 0.9524701631069183}
2025-01-15 17:20:31,388 [INFO] Step[650/2713]: training loss : 0.9896184051036835 TRAIN  loss dict:  {'classification_loss': 0.9896184051036835}
2025-01-15 17:20:43,297 [INFO] Step[700/2713]: training loss : 1.0034529435634614 TRAIN  loss dict:  {'classification_loss': 1.0034529435634614}
2025-01-15 17:20:55,230 [INFO] Step[750/2713]: training loss : 0.9472427272796631 TRAIN  loss dict:  {'classification_loss': 0.9472427272796631}
2025-01-15 17:21:07,168 [INFO] Step[800/2713]: training loss : 0.9409820735454559 TRAIN  loss dict:  {'classification_loss': 0.9409820735454559}
2025-01-15 17:21:19,180 [INFO] Step[850/2713]: training loss : 0.9424171030521393 TRAIN  loss dict:  {'classification_loss': 0.9424171030521393}
2025-01-15 17:21:31,109 [INFO] Step[900/2713]: training loss : 0.9371720445156098 TRAIN  loss dict:  {'classification_loss': 0.9371720445156098}
2025-01-15 17:21:43,012 [INFO] Step[950/2713]: training loss : 0.9433639967441558 TRAIN  loss dict:  {'classification_loss': 0.9433639967441558}
2025-01-15 17:21:54,914 [INFO] Step[1000/2713]: training loss : 0.9419554340839386 TRAIN  loss dict:  {'classification_loss': 0.9419554340839386}
2025-01-15 17:22:06,904 [INFO] Step[1050/2713]: training loss : 0.9499373233318329 TRAIN  loss dict:  {'classification_loss': 0.9499373233318329}
2025-01-15 17:22:18,805 [INFO] Step[1100/2713]: training loss : 0.9465484130382538 TRAIN  loss dict:  {'classification_loss': 0.9465484130382538}
2025-01-15 17:22:30,747 [INFO] Step[1150/2713]: training loss : 0.9468377721309662 TRAIN  loss dict:  {'classification_loss': 0.9468377721309662}
2025-01-15 17:22:42,675 [INFO] Step[1200/2713]: training loss : 0.9380447971820831 TRAIN  loss dict:  {'classification_loss': 0.9380447971820831}
2025-01-15 17:22:54,597 [INFO] Step[1250/2713]: training loss : 0.9473307371139527 TRAIN  loss dict:  {'classification_loss': 0.9473307371139527}
2025-01-15 17:23:06,508 [INFO] Step[1300/2713]: training loss : 0.9834743821620942 TRAIN  loss dict:  {'classification_loss': 0.9834743821620942}
2025-01-15 17:23:18,442 [INFO] Step[1350/2713]: training loss : 1.0261511099338532 TRAIN  loss dict:  {'classification_loss': 1.0261511099338532}
2025-01-15 17:23:30,393 [INFO] Step[1400/2713]: training loss : 0.9504075074195861 TRAIN  loss dict:  {'classification_loss': 0.9504075074195861}
2025-01-15 17:23:42,318 [INFO] Step[1450/2713]: training loss : 0.9646013689041137 TRAIN  loss dict:  {'classification_loss': 0.9646013689041137}
2025-01-15 17:23:54,227 [INFO] Step[1500/2713]: training loss : 0.9484632229804992 TRAIN  loss dict:  {'classification_loss': 0.9484632229804992}
2025-01-15 17:24:06,172 [INFO] Step[1550/2713]: training loss : 1.0143601274490357 TRAIN  loss dict:  {'classification_loss': 1.0143601274490357}
2025-01-15 17:24:18,099 [INFO] Step[1600/2713]: training loss : 0.9385988104343415 TRAIN  loss dict:  {'classification_loss': 0.9385988104343415}
2025-01-15 17:24:30,058 [INFO] Step[1650/2713]: training loss : 0.9727019846439362 TRAIN  loss dict:  {'classification_loss': 0.9727019846439362}
2025-01-15 17:24:42,020 [INFO] Step[1700/2713]: training loss : 1.0170443427562714 TRAIN  loss dict:  {'classification_loss': 1.0170443427562714}
2025-01-15 17:24:53,942 [INFO] Step[1750/2713]: training loss : 0.9595051312446594 TRAIN  loss dict:  {'classification_loss': 0.9595051312446594}
2025-01-15 17:25:05,859 [INFO] Step[1800/2713]: training loss : 1.06047625541687 TRAIN  loss dict:  {'classification_loss': 1.06047625541687}
2025-01-15 17:25:17,829 [INFO] Step[1850/2713]: training loss : 0.9885524988174439 TRAIN  loss dict:  {'classification_loss': 0.9885524988174439}
2025-01-15 17:25:29,757 [INFO] Step[1900/2713]: training loss : 0.9784983253479004 TRAIN  loss dict:  {'classification_loss': 0.9784983253479004}
2025-01-15 17:25:41,667 [INFO] Step[1950/2713]: training loss : 0.9719521141052246 TRAIN  loss dict:  {'classification_loss': 0.9719521141052246}
2025-01-15 17:25:53,554 [INFO] Step[2000/2713]: training loss : 0.9533014523983002 TRAIN  loss dict:  {'classification_loss': 0.9533014523983002}
2025-01-15 17:26:05,459 [INFO] Step[2050/2713]: training loss : 0.9742238020896912 TRAIN  loss dict:  {'classification_loss': 0.9742238020896912}
2025-01-15 17:26:17,380 [INFO] Step[2100/2713]: training loss : 0.9998207879066467 TRAIN  loss dict:  {'classification_loss': 0.9998207879066467}
2025-01-15 17:26:29,356 [INFO] Step[2150/2713]: training loss : 0.9616546857357026 TRAIN  loss dict:  {'classification_loss': 0.9616546857357026}
2025-01-15 17:26:41,285 [INFO] Step[2200/2713]: training loss : 0.9384785175323487 TRAIN  loss dict:  {'classification_loss': 0.9384785175323487}
2025-01-15 17:26:53,213 [INFO] Step[2250/2713]: training loss : 0.9740766441822052 TRAIN  loss dict:  {'classification_loss': 0.9740766441822052}
2025-01-15 17:27:05,123 [INFO] Step[2300/2713]: training loss : 0.965034236907959 TRAIN  loss dict:  {'classification_loss': 0.965034236907959}
2025-01-15 17:27:17,064 [INFO] Step[2350/2713]: training loss : 0.9795750904083252 TRAIN  loss dict:  {'classification_loss': 0.9795750904083252}
2025-01-15 17:27:28,986 [INFO] Step[2400/2713]: training loss : 0.9651878726482391 TRAIN  loss dict:  {'classification_loss': 0.9651878726482391}
2025-01-15 17:27:40,915 [INFO] Step[2450/2713]: training loss : 0.9365690481662751 TRAIN  loss dict:  {'classification_loss': 0.9365690481662751}
2025-01-15 17:27:52,821 [INFO] Step[2500/2713]: training loss : 0.9815944492816925 TRAIN  loss dict:  {'classification_loss': 0.9815944492816925}
2025-01-15 17:28:04,743 [INFO] Step[2550/2713]: training loss : 0.9360723400115967 TRAIN  loss dict:  {'classification_loss': 0.9360723400115967}
2025-01-15 17:28:16,681 [INFO] Step[2600/2713]: training loss : 0.9839919471740722 TRAIN  loss dict:  {'classification_loss': 0.9839919471740722}
2025-01-15 17:28:28,614 [INFO] Step[2650/2713]: training loss : 0.9830340003967285 TRAIN  loss dict:  {'classification_loss': 0.9830340003967285}
2025-01-15 17:28:40,498 [INFO] Step[2700/2713]: training loss : 0.9440035963058472 TRAIN  loss dict:  {'classification_loss': 0.9440035963058472}
2025-01-15 17:29:53,168 [INFO] Label accuracies statistics:
2025-01-15 17:29:53,168 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.5, 59: 1.0, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.0, 114: 0.5, 115: 0.75, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.6666666666666666, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.5, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.5, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.5, 267: 0.25, 268: 0.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.5, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.5, 347: 1.0, 348: 0.75, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.0, 382: 0.75, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 17:29:54,324 [INFO] [13] TRAIN  loss: 0.9680586781969291 acc: 0.9901707826514313
2025-01-15 17:29:54,324 [INFO] [13] TRAIN  loss dict: {'classification_loss': 0.9680586781969291}
2025-01-15 17:29:54,325 [INFO] [13] VALIDATION loss: 2.003964439034462 VALIDATION acc: 0.7711598746081505
2025-01-15 17:29:54,325 [INFO] [13] VALIDATION loss dict: {'classification_loss': 2.003964439034462}
2025-01-15 17:29:54,325 [INFO] 
2025-01-15 17:30:11,795 [INFO] Step[50/2713]: training loss : 0.9387863373756409 TRAIN  loss dict:  {'classification_loss': 0.9387863373756409}
2025-01-15 17:30:23,644 [INFO] Step[100/2713]: training loss : 0.9600036883354187 TRAIN  loss dict:  {'classification_loss': 0.9600036883354187}
2025-01-15 17:30:35,492 [INFO] Step[150/2713]: training loss : 1.0081172096729278 TRAIN  loss dict:  {'classification_loss': 1.0081172096729278}
2025-01-15 17:30:47,364 [INFO] Step[200/2713]: training loss : 1.0018236017227173 TRAIN  loss dict:  {'classification_loss': 1.0018236017227173}
2025-01-15 17:30:59,309 [INFO] Step[250/2713]: training loss : 1.000343257188797 TRAIN  loss dict:  {'classification_loss': 1.000343257188797}
2025-01-15 17:31:11,184 [INFO] Step[300/2713]: training loss : 0.9547855007648468 TRAIN  loss dict:  {'classification_loss': 0.9547855007648468}
2025-01-15 17:31:23,150 [INFO] Step[350/2713]: training loss : 0.9405489206314087 TRAIN  loss dict:  {'classification_loss': 0.9405489206314087}
2025-01-15 17:31:35,063 [INFO] Step[400/2713]: training loss : 0.9423680186271668 TRAIN  loss dict:  {'classification_loss': 0.9423680186271668}
2025-01-15 17:31:47,004 [INFO] Step[450/2713]: training loss : 0.957546193599701 TRAIN  loss dict:  {'classification_loss': 0.957546193599701}
2025-01-15 17:31:58,929 [INFO] Step[500/2713]: training loss : 1.0377291226387024 TRAIN  loss dict:  {'classification_loss': 1.0377291226387024}
2025-01-15 17:32:10,880 [INFO] Step[550/2713]: training loss : 0.9691834580898285 TRAIN  loss dict:  {'classification_loss': 0.9691834580898285}
2025-01-15 17:32:22,834 [INFO] Step[600/2713]: training loss : 0.949680689573288 TRAIN  loss dict:  {'classification_loss': 0.949680689573288}
2025-01-15 17:32:34,762 [INFO] Step[650/2713]: training loss : 0.9793723952770234 TRAIN  loss dict:  {'classification_loss': 0.9793723952770234}
2025-01-15 17:32:46,720 [INFO] Step[700/2713]: training loss : 0.9653297162055969 TRAIN  loss dict:  {'classification_loss': 0.9653297162055969}
2025-01-15 17:32:58,700 [INFO] Step[750/2713]: training loss : 0.9457655918598175 TRAIN  loss dict:  {'classification_loss': 0.9457655918598175}
2025-01-15 17:33:10,629 [INFO] Step[800/2713]: training loss : 0.9550200939178467 TRAIN  loss dict:  {'classification_loss': 0.9550200939178467}
2025-01-15 17:33:22,568 [INFO] Step[850/2713]: training loss : 0.9544578385353089 TRAIN  loss dict:  {'classification_loss': 0.9544578385353089}
2025-01-15 17:33:34,483 [INFO] Step[900/2713]: training loss : 0.9341191005706787 TRAIN  loss dict:  {'classification_loss': 0.9341191005706787}
2025-01-15 17:33:46,386 [INFO] Step[950/2713]: training loss : 0.9896165931224823 TRAIN  loss dict:  {'classification_loss': 0.9896165931224823}
2025-01-15 17:33:58,321 [INFO] Step[1000/2713]: training loss : 0.9433134090900421 TRAIN  loss dict:  {'classification_loss': 0.9433134090900421}
2025-01-15 17:34:10,236 [INFO] Step[1050/2713]: training loss : 0.9958485841751099 TRAIN  loss dict:  {'classification_loss': 0.9958485841751099}
2025-01-15 17:34:22,146 [INFO] Step[1100/2713]: training loss : 0.9727614486217498 TRAIN  loss dict:  {'classification_loss': 0.9727614486217498}
2025-01-15 17:34:34,057 [INFO] Step[1150/2713]: training loss : 0.9369976115226746 TRAIN  loss dict:  {'classification_loss': 0.9369976115226746}
2025-01-15 17:34:45,960 [INFO] Step[1200/2713]: training loss : 0.9720054495334626 TRAIN  loss dict:  {'classification_loss': 0.9720054495334626}
2025-01-15 17:34:57,906 [INFO] Step[1250/2713]: training loss : 0.9369319272041321 TRAIN  loss dict:  {'classification_loss': 0.9369319272041321}
2025-01-15 17:35:09,815 [INFO] Step[1300/2713]: training loss : 0.9361626601219177 TRAIN  loss dict:  {'classification_loss': 0.9361626601219177}
2025-01-15 17:35:21,817 [INFO] Step[1350/2713]: training loss : 0.9384744548797608 TRAIN  loss dict:  {'classification_loss': 0.9384744548797608}
2025-01-15 17:35:33,770 [INFO] Step[1400/2713]: training loss : 0.9469540071487427 TRAIN  loss dict:  {'classification_loss': 0.9469540071487427}
2025-01-15 17:35:45,687 [INFO] Step[1450/2713]: training loss : 0.9613206171989441 TRAIN  loss dict:  {'classification_loss': 0.9613206171989441}
2025-01-15 17:35:57,603 [INFO] Step[1500/2713]: training loss : 0.973185979127884 TRAIN  loss dict:  {'classification_loss': 0.973185979127884}
2025-01-15 17:36:09,554 [INFO] Step[1550/2713]: training loss : 0.9635446560382843 TRAIN  loss dict:  {'classification_loss': 0.9635446560382843}
2025-01-15 17:36:21,484 [INFO] Step[1600/2713]: training loss : 0.944466632604599 TRAIN  loss dict:  {'classification_loss': 0.944466632604599}
2025-01-15 17:36:33,460 [INFO] Step[1650/2713]: training loss : 0.9594093430042266 TRAIN  loss dict:  {'classification_loss': 0.9594093430042266}
2025-01-15 17:36:45,375 [INFO] Step[1700/2713]: training loss : 0.9705325472354889 TRAIN  loss dict:  {'classification_loss': 0.9705325472354889}
2025-01-15 17:36:57,332 [INFO] Step[1750/2713]: training loss : 0.9703731560707092 TRAIN  loss dict:  {'classification_loss': 0.9703731560707092}
2025-01-15 17:37:09,266 [INFO] Step[1800/2713]: training loss : 0.98838130235672 TRAIN  loss dict:  {'classification_loss': 0.98838130235672}
2025-01-15 17:37:21,187 [INFO] Step[1850/2713]: training loss : 0.9576414108276368 TRAIN  loss dict:  {'classification_loss': 0.9576414108276368}
2025-01-15 17:37:33,104 [INFO] Step[1900/2713]: training loss : 0.9860130167007446 TRAIN  loss dict:  {'classification_loss': 0.9860130167007446}
2025-01-15 17:37:45,013 [INFO] Step[1950/2713]: training loss : 0.9460491728782654 TRAIN  loss dict:  {'classification_loss': 0.9460491728782654}
2025-01-15 17:37:56,973 [INFO] Step[2000/2713]: training loss : 0.9623235845565796 TRAIN  loss dict:  {'classification_loss': 0.9623235845565796}
2025-01-15 17:38:08,923 [INFO] Step[2050/2713]: training loss : 0.9680763030052185 TRAIN  loss dict:  {'classification_loss': 0.9680763030052185}
2025-01-15 17:38:20,856 [INFO] Step[2100/2713]: training loss : 0.9707000541687012 TRAIN  loss dict:  {'classification_loss': 0.9707000541687012}
2025-01-15 17:38:32,758 [INFO] Step[2150/2713]: training loss : 0.9604535162448883 TRAIN  loss dict:  {'classification_loss': 0.9604535162448883}
2025-01-15 17:38:44,696 [INFO] Step[2200/2713]: training loss : 0.9514631271362305 TRAIN  loss dict:  {'classification_loss': 0.9514631271362305}
2025-01-15 17:38:56,606 [INFO] Step[2250/2713]: training loss : 0.9706683421134948 TRAIN  loss dict:  {'classification_loss': 0.9706683421134948}
2025-01-15 17:39:08,542 [INFO] Step[2300/2713]: training loss : 1.0034169721603394 TRAIN  loss dict:  {'classification_loss': 1.0034169721603394}
2025-01-15 17:39:20,511 [INFO] Step[2350/2713]: training loss : 0.95358243227005 TRAIN  loss dict:  {'classification_loss': 0.95358243227005}
2025-01-15 17:39:32,464 [INFO] Step[2400/2713]: training loss : 0.9496649396419525 TRAIN  loss dict:  {'classification_loss': 0.9496649396419525}
2025-01-15 17:39:44,386 [INFO] Step[2450/2713]: training loss : 0.9359617757797242 TRAIN  loss dict:  {'classification_loss': 0.9359617757797242}
2025-01-15 17:39:56,308 [INFO] Step[2500/2713]: training loss : 0.990297634601593 TRAIN  loss dict:  {'classification_loss': 0.990297634601593}
2025-01-15 17:40:08,255 [INFO] Step[2550/2713]: training loss : 1.0205699169635774 TRAIN  loss dict:  {'classification_loss': 1.0205699169635774}
2025-01-15 17:40:20,168 [INFO] Step[2600/2713]: training loss : 0.9668551647663116 TRAIN  loss dict:  {'classification_loss': 0.9668551647663116}
2025-01-15 17:40:32,091 [INFO] Step[2650/2713]: training loss : 0.9942729139328003 TRAIN  loss dict:  {'classification_loss': 0.9942729139328003}
2025-01-15 17:40:43,948 [INFO] Step[2700/2713]: training loss : 0.944595786333084 TRAIN  loss dict:  {'classification_loss': 0.944595786333084}
2025-01-15 17:41:56,827 [INFO] Label accuracies statistics:
2025-01-15 17:41:56,827 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.5, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 0.75, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 1.0, 70: 1.0, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 1.0, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.5, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.5, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.5, 186: 0.25, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.5, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.5, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.5, 288: 0.75, 289: 1.0, 290: 0.0, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.5, 305: 1.0, 306: 0.75, 307: 0.75, 308: 0.75, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.25, 356: 0.75, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 0.5, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 17:41:58,041 [INFO] [14] TRAIN  loss: 0.9651999417108844 acc: 0.9913994348200025
2025-01-15 17:41:58,041 [INFO] [14] TRAIN  loss dict: {'classification_loss': 0.9651999417108844}
2025-01-15 17:41:58,041 [INFO] [14] VALIDATION loss: 1.9916000650789505 VALIDATION acc: 0.7648902821316614
2025-01-15 17:41:58,041 [INFO] [14] VALIDATION loss dict: {'classification_loss': 1.9916000650789505}
2025-01-15 17:41:58,041 [INFO] 
2025-01-15 17:42:15,162 [INFO] Step[50/2713]: training loss : 0.9367919588088989 TRAIN  loss dict:  {'classification_loss': 0.9367919588088989}
2025-01-15 17:42:27,016 [INFO] Step[100/2713]: training loss : 0.9467668318748474 TRAIN  loss dict:  {'classification_loss': 0.9467668318748474}
2025-01-15 17:42:38,941 [INFO] Step[150/2713]: training loss : 0.9367305099964142 TRAIN  loss dict:  {'classification_loss': 0.9367305099964142}
2025-01-15 17:42:50,855 [INFO] Step[200/2713]: training loss : 0.9444672453403473 TRAIN  loss dict:  {'classification_loss': 0.9444672453403473}
2025-01-15 17:43:02,770 [INFO] Step[250/2713]: training loss : 0.9490992665290833 TRAIN  loss dict:  {'classification_loss': 0.9490992665290833}
2025-01-15 17:43:14,679 [INFO] Step[300/2713]: training loss : 0.9874144184589386 TRAIN  loss dict:  {'classification_loss': 0.9874144184589386}
2025-01-15 17:43:26,612 [INFO] Step[350/2713]: training loss : 0.9597681236267089 TRAIN  loss dict:  {'classification_loss': 0.9597681236267089}
2025-01-15 17:43:38,542 [INFO] Step[400/2713]: training loss : 0.9451332032680512 TRAIN  loss dict:  {'classification_loss': 0.9451332032680512}
2025-01-15 17:43:50,488 [INFO] Step[450/2713]: training loss : 0.948504935503006 TRAIN  loss dict:  {'classification_loss': 0.948504935503006}
2025-01-15 17:44:02,445 [INFO] Step[500/2713]: training loss : 0.944540034532547 TRAIN  loss dict:  {'classification_loss': 0.944540034532547}
2025-01-15 17:44:14,374 [INFO] Step[550/2713]: training loss : 0.9441069221496582 TRAIN  loss dict:  {'classification_loss': 0.9441069221496582}
2025-01-15 17:44:26,307 [INFO] Step[600/2713]: training loss : 0.9620756924152374 TRAIN  loss dict:  {'classification_loss': 0.9620756924152374}
2025-01-15 17:44:38,241 [INFO] Step[650/2713]: training loss : 0.9852844536304474 TRAIN  loss dict:  {'classification_loss': 0.9852844536304474}
2025-01-15 17:44:50,212 [INFO] Step[700/2713]: training loss : 0.9534943008422851 TRAIN  loss dict:  {'classification_loss': 0.9534943008422851}
2025-01-15 17:45:02,151 [INFO] Step[750/2713]: training loss : 0.9398814797401428 TRAIN  loss dict:  {'classification_loss': 0.9398814797401428}
2025-01-15 17:45:14,106 [INFO] Step[800/2713]: training loss : 0.9615581202507019 TRAIN  loss dict:  {'classification_loss': 0.9615581202507019}
2025-01-15 17:45:26,061 [INFO] Step[850/2713]: training loss : 0.9697590017318726 TRAIN  loss dict:  {'classification_loss': 0.9697590017318726}
2025-01-15 17:45:37,979 [INFO] Step[900/2713]: training loss : 0.9444068312644959 TRAIN  loss dict:  {'classification_loss': 0.9444068312644959}
2025-01-15 17:45:49,926 [INFO] Step[950/2713]: training loss : 0.9556606161594391 TRAIN  loss dict:  {'classification_loss': 0.9556606161594391}
2025-01-15 17:46:01,870 [INFO] Step[1000/2713]: training loss : 0.9764546239376068 TRAIN  loss dict:  {'classification_loss': 0.9764546239376068}
2025-01-15 17:46:13,776 [INFO] Step[1050/2713]: training loss : 0.9523727178573609 TRAIN  loss dict:  {'classification_loss': 0.9523727178573609}
2025-01-15 17:46:25,720 [INFO] Step[1100/2713]: training loss : 0.9421248948574066 TRAIN  loss dict:  {'classification_loss': 0.9421248948574066}
2025-01-15 17:46:37,642 [INFO] Step[1150/2713]: training loss : 0.959346798658371 TRAIN  loss dict:  {'classification_loss': 0.959346798658371}
2025-01-15 17:46:49,553 [INFO] Step[1200/2713]: training loss : 0.9347808647155762 TRAIN  loss dict:  {'classification_loss': 0.9347808647155762}
2025-01-15 17:47:01,499 [INFO] Step[1250/2713]: training loss : 0.9680105936527252 TRAIN  loss dict:  {'classification_loss': 0.9680105936527252}
2025-01-15 17:47:13,434 [INFO] Step[1300/2713]: training loss : 0.9470726013183594 TRAIN  loss dict:  {'classification_loss': 0.9470726013183594}
2025-01-15 17:47:25,355 [INFO] Step[1350/2713]: training loss : 0.9823135137557983 TRAIN  loss dict:  {'classification_loss': 0.9823135137557983}
2025-01-15 17:47:37,289 [INFO] Step[1400/2713]: training loss : 0.9571142435073853 TRAIN  loss dict:  {'classification_loss': 0.9571142435073853}
2025-01-15 17:47:49,228 [INFO] Step[1450/2713]: training loss : 0.9467584300041199 TRAIN  loss dict:  {'classification_loss': 0.9467584300041199}
2025-01-15 17:48:01,150 [INFO] Step[1500/2713]: training loss : 0.9937996566295624 TRAIN  loss dict:  {'classification_loss': 0.9937996566295624}
2025-01-15 17:48:13,053 [INFO] Step[1550/2713]: training loss : 1.004711023569107 TRAIN  loss dict:  {'classification_loss': 1.004711023569107}
2025-01-15 17:48:24,995 [INFO] Step[1600/2713]: training loss : 0.9375264239311218 TRAIN  loss dict:  {'classification_loss': 0.9375264239311218}
2025-01-15 17:48:36,926 [INFO] Step[1650/2713]: training loss : 0.9537946450710296 TRAIN  loss dict:  {'classification_loss': 0.9537946450710296}
2025-01-15 17:48:48,806 [INFO] Step[1700/2713]: training loss : 0.9738327431678772 TRAIN  loss dict:  {'classification_loss': 0.9738327431678772}
2025-01-15 17:49:00,716 [INFO] Step[1750/2713]: training loss : 0.9550717544555664 TRAIN  loss dict:  {'classification_loss': 0.9550717544555664}
2025-01-15 17:49:12,620 [INFO] Step[1800/2713]: training loss : 0.9774569690227508 TRAIN  loss dict:  {'classification_loss': 0.9774569690227508}
2025-01-15 17:49:24,552 [INFO] Step[1850/2713]: training loss : 0.9792928338050843 TRAIN  loss dict:  {'classification_loss': 0.9792928338050843}
2025-01-15 17:49:36,492 [INFO] Step[1900/2713]: training loss : 0.9734445285797119 TRAIN  loss dict:  {'classification_loss': 0.9734445285797119}
2025-01-15 17:49:48,404 [INFO] Step[1950/2713]: training loss : 1.0421577191352844 TRAIN  loss dict:  {'classification_loss': 1.0421577191352844}
2025-01-15 17:50:00,315 [INFO] Step[2000/2713]: training loss : 0.9527881109714508 TRAIN  loss dict:  {'classification_loss': 0.9527881109714508}
2025-01-15 17:50:12,218 [INFO] Step[2050/2713]: training loss : 0.9571978437900543 TRAIN  loss dict:  {'classification_loss': 0.9571978437900543}
2025-01-15 17:50:24,164 [INFO] Step[2100/2713]: training loss : 0.956193425655365 TRAIN  loss dict:  {'classification_loss': 0.956193425655365}
2025-01-15 17:50:36,124 [INFO] Step[2150/2713]: training loss : 0.9817447781562805 TRAIN  loss dict:  {'classification_loss': 0.9817447781562805}
2025-01-15 17:50:48,042 [INFO] Step[2200/2713]: training loss : 0.9920149338245392 TRAIN  loss dict:  {'classification_loss': 0.9920149338245392}
2025-01-15 17:50:59,997 [INFO] Step[2250/2713]: training loss : 0.9454757046699523 TRAIN  loss dict:  {'classification_loss': 0.9454757046699523}
2025-01-15 17:51:11,884 [INFO] Step[2300/2713]: training loss : 0.938907482624054 TRAIN  loss dict:  {'classification_loss': 0.938907482624054}
2025-01-15 17:51:23,813 [INFO] Step[2350/2713]: training loss : 0.9523935627937317 TRAIN  loss dict:  {'classification_loss': 0.9523935627937317}
2025-01-15 17:51:35,721 [INFO] Step[2400/2713]: training loss : 0.9407096600532532 TRAIN  loss dict:  {'classification_loss': 0.9407096600532532}
2025-01-15 17:51:47,661 [INFO] Step[2450/2713]: training loss : 0.9492432069778443 TRAIN  loss dict:  {'classification_loss': 0.9492432069778443}
2025-01-15 17:51:59,597 [INFO] Step[2500/2713]: training loss : 1.0248305690288544 TRAIN  loss dict:  {'classification_loss': 1.0248305690288544}
2025-01-15 17:52:11,530 [INFO] Step[2550/2713]: training loss : 0.9414035475254059 TRAIN  loss dict:  {'classification_loss': 0.9414035475254059}
2025-01-15 17:52:23,417 [INFO] Step[2600/2713]: training loss : 0.9461831796169281 TRAIN  loss dict:  {'classification_loss': 0.9461831796169281}
2025-01-15 17:52:35,361 [INFO] Step[2650/2713]: training loss : 0.9465625953674316 TRAIN  loss dict:  {'classification_loss': 0.9465625953674316}
2025-01-15 17:52:47,264 [INFO] Step[2700/2713]: training loss : 0.9761597514152527 TRAIN  loss dict:  {'classification_loss': 0.9761597514152527}
2025-01-15 17:53:59,980 [INFO] Label accuracies statistics:
2025-01-15 17:53:59,980 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 0.75, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.5, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 0.5, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 0.75, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 1.0, 190: 0.5, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.25, 214: 0.75, 215: 1.0, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.25, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.5, 288: 0.5, 289: 0.5, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 1.0, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.25, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 1.0, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.0, 382: 0.75, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 17:54:02,317 [INFO] [15] TRAIN  loss: 0.9605206275311423 acc: 0.9923823565548593
2025-01-15 17:54:02,317 [INFO] [15] TRAIN  loss dict: {'classification_loss': 0.9605206275311423}
2025-01-15 17:54:02,317 [INFO] [15] VALIDATION loss: 1.9225359690144546 VALIDATION acc: 0.7774294670846394
2025-01-15 17:54:02,317 [INFO] [15] VALIDATION loss dict: {'classification_loss': 1.9225359690144546}
2025-01-15 17:54:02,317 [INFO] 
2025-01-15 17:54:19,406 [INFO] Step[50/2713]: training loss : 0.9412346017360688 TRAIN  loss dict:  {'classification_loss': 0.9412346017360688}
2025-01-15 17:54:31,232 [INFO] Step[100/2713]: training loss : 0.9636852371692658 TRAIN  loss dict:  {'classification_loss': 0.9636852371692658}
2025-01-15 17:54:43,095 [INFO] Step[150/2713]: training loss : 0.964666405916214 TRAIN  loss dict:  {'classification_loss': 0.964666405916214}
2025-01-15 17:54:54,988 [INFO] Step[200/2713]: training loss : 0.9569934189319611 TRAIN  loss dict:  {'classification_loss': 0.9569934189319611}
2025-01-15 17:55:06,884 [INFO] Step[250/2713]: training loss : 0.9519553935527801 TRAIN  loss dict:  {'classification_loss': 0.9519553935527801}
2025-01-15 17:55:18,781 [INFO] Step[300/2713]: training loss : 0.960280305147171 TRAIN  loss dict:  {'classification_loss': 0.960280305147171}
2025-01-15 17:55:30,724 [INFO] Step[350/2713]: training loss : 0.9949525177478791 TRAIN  loss dict:  {'classification_loss': 0.9949525177478791}
2025-01-15 17:55:42,679 [INFO] Step[400/2713]: training loss : 0.9512310647964477 TRAIN  loss dict:  {'classification_loss': 0.9512310647964477}
2025-01-15 17:55:54,612 [INFO] Step[450/2713]: training loss : 0.952925820350647 TRAIN  loss dict:  {'classification_loss': 0.952925820350647}
2025-01-15 17:56:06,548 [INFO] Step[500/2713]: training loss : 0.9442221152782441 TRAIN  loss dict:  {'classification_loss': 0.9442221152782441}
2025-01-15 17:56:18,482 [INFO] Step[550/2713]: training loss : 0.991328194141388 TRAIN  loss dict:  {'classification_loss': 0.991328194141388}
2025-01-15 17:56:30,409 [INFO] Step[600/2713]: training loss : 0.9667377626895904 TRAIN  loss dict:  {'classification_loss': 0.9667377626895904}
2025-01-15 17:56:42,356 [INFO] Step[650/2713]: training loss : 0.9637595844268799 TRAIN  loss dict:  {'classification_loss': 0.9637595844268799}
2025-01-15 17:56:54,295 [INFO] Step[700/2713]: training loss : 1.01442631483078 TRAIN  loss dict:  {'classification_loss': 1.01442631483078}
2025-01-15 17:57:06,240 [INFO] Step[750/2713]: training loss : 0.9978128540515899 TRAIN  loss dict:  {'classification_loss': 0.9978128540515899}
2025-01-15 17:57:18,183 [INFO] Step[800/2713]: training loss : 0.9366078901290894 TRAIN  loss dict:  {'classification_loss': 0.9366078901290894}
2025-01-15 17:57:30,137 [INFO] Step[850/2713]: training loss : 0.9526695287227631 TRAIN  loss dict:  {'classification_loss': 0.9526695287227631}
2025-01-15 17:57:42,067 [INFO] Step[900/2713]: training loss : 1.0132594466209413 TRAIN  loss dict:  {'classification_loss': 1.0132594466209413}
2025-01-15 17:57:53,994 [INFO] Step[950/2713]: training loss : 0.9372142159938812 TRAIN  loss dict:  {'classification_loss': 0.9372142159938812}
2025-01-15 17:58:05,936 [INFO] Step[1000/2713]: training loss : 0.9579429662227631 TRAIN  loss dict:  {'classification_loss': 0.9579429662227631}
2025-01-15 17:58:17,880 [INFO] Step[1050/2713]: training loss : 0.958420273065567 TRAIN  loss dict:  {'classification_loss': 0.958420273065567}
2025-01-15 17:58:29,828 [INFO] Step[1100/2713]: training loss : 0.9355485308170318 TRAIN  loss dict:  {'classification_loss': 0.9355485308170318}
2025-01-15 17:58:41,750 [INFO] Step[1150/2713]: training loss : 0.9731509554386139 TRAIN  loss dict:  {'classification_loss': 0.9731509554386139}
2025-01-15 17:58:53,674 [INFO] Step[1200/2713]: training loss : 0.9558635199069977 TRAIN  loss dict:  {'classification_loss': 0.9558635199069977}
2025-01-15 17:59:05,609 [INFO] Step[1250/2713]: training loss : 0.9433979380130768 TRAIN  loss dict:  {'classification_loss': 0.9433979380130768}
2025-01-15 17:59:17,515 [INFO] Step[1300/2713]: training loss : 0.9754564714431763 TRAIN  loss dict:  {'classification_loss': 0.9754564714431763}
2025-01-15 17:59:29,456 [INFO] Step[1350/2713]: training loss : 0.9423237752914428 TRAIN  loss dict:  {'classification_loss': 0.9423237752914428}
2025-01-15 17:59:41,345 [INFO] Step[1400/2713]: training loss : 0.9570929396152497 TRAIN  loss dict:  {'classification_loss': 0.9570929396152497}
2025-01-15 17:59:53,308 [INFO] Step[1450/2713]: training loss : 0.937837209701538 TRAIN  loss dict:  {'classification_loss': 0.937837209701538}
2025-01-15 18:00:05,217 [INFO] Step[1500/2713]: training loss : 0.9586510598659516 TRAIN  loss dict:  {'classification_loss': 0.9586510598659516}
2025-01-15 18:00:17,203 [INFO] Step[1550/2713]: training loss : 0.9624539029598236 TRAIN  loss dict:  {'classification_loss': 0.9624539029598236}
2025-01-15 18:00:29,114 [INFO] Step[1600/2713]: training loss : 0.9803589975833893 TRAIN  loss dict:  {'classification_loss': 0.9803589975833893}
2025-01-15 18:00:41,038 [INFO] Step[1650/2713]: training loss : 0.9676403903961182 TRAIN  loss dict:  {'classification_loss': 0.9676403903961182}
2025-01-15 18:00:52,975 [INFO] Step[1700/2713]: training loss : 0.9436665809154511 TRAIN  loss dict:  {'classification_loss': 0.9436665809154511}
2025-01-15 18:01:04,916 [INFO] Step[1750/2713]: training loss : 0.9347041201591492 TRAIN  loss dict:  {'classification_loss': 0.9347041201591492}
2025-01-15 18:01:16,815 [INFO] Step[1800/2713]: training loss : 0.9728106415271759 TRAIN  loss dict:  {'classification_loss': 0.9728106415271759}
2025-01-15 18:01:28,719 [INFO] Step[1850/2713]: training loss : 0.9594340562820435 TRAIN  loss dict:  {'classification_loss': 0.9594340562820435}
2025-01-15 18:01:40,677 [INFO] Step[1900/2713]: training loss : 0.9422433495521545 TRAIN  loss dict:  {'classification_loss': 0.9422433495521545}
2025-01-15 18:01:52,607 [INFO] Step[1950/2713]: training loss : 0.9711007022857666 TRAIN  loss dict:  {'classification_loss': 0.9711007022857666}
2025-01-15 18:02:04,550 [INFO] Step[2000/2713]: training loss : 0.9465313446521759 TRAIN  loss dict:  {'classification_loss': 0.9465313446521759}
2025-01-15 18:02:16,452 [INFO] Step[2050/2713]: training loss : 0.9817387676239013 TRAIN  loss dict:  {'classification_loss': 0.9817387676239013}
2025-01-15 18:02:28,341 [INFO] Step[2100/2713]: training loss : 0.9513718295097351 TRAIN  loss dict:  {'classification_loss': 0.9513718295097351}
2025-01-15 18:02:40,262 [INFO] Step[2150/2713]: training loss : 0.939826272726059 TRAIN  loss dict:  {'classification_loss': 0.939826272726059}
2025-01-15 18:02:52,168 [INFO] Step[2200/2713]: training loss : 0.9820104598999023 TRAIN  loss dict:  {'classification_loss': 0.9820104598999023}
2025-01-15 18:03:04,091 [INFO] Step[2250/2713]: training loss : 0.9904858028888702 TRAIN  loss dict:  {'classification_loss': 0.9904858028888702}
2025-01-15 18:03:15,971 [INFO] Step[2300/2713]: training loss : 0.9612843251228332 TRAIN  loss dict:  {'classification_loss': 0.9612843251228332}
2025-01-15 18:03:27,906 [INFO] Step[2350/2713]: training loss : 1.0005760490894318 TRAIN  loss dict:  {'classification_loss': 1.0005760490894318}
2025-01-15 18:03:39,819 [INFO] Step[2400/2713]: training loss : 1.059073997735977 TRAIN  loss dict:  {'classification_loss': 1.059073997735977}
2025-01-15 18:03:51,796 [INFO] Step[2450/2713]: training loss : 0.9704877746105194 TRAIN  loss dict:  {'classification_loss': 0.9704877746105194}
2025-01-15 18:04:03,698 [INFO] Step[2500/2713]: training loss : 0.9609724664688111 TRAIN  loss dict:  {'classification_loss': 0.9609724664688111}
2025-01-15 18:04:15,633 [INFO] Step[2550/2713]: training loss : 0.9369802594184875 TRAIN  loss dict:  {'classification_loss': 0.9369802594184875}
2025-01-15 18:04:27,558 [INFO] Step[2600/2713]: training loss : 1.0208055138587953 TRAIN  loss dict:  {'classification_loss': 1.0208055138587953}
2025-01-15 18:04:39,500 [INFO] Step[2650/2713]: training loss : 0.9624368536472321 TRAIN  loss dict:  {'classification_loss': 0.9624368536472321}
2025-01-15 18:04:51,390 [INFO] Step[2700/2713]: training loss : 0.938938500881195 TRAIN  loss dict:  {'classification_loss': 0.938938500881195}
2025-01-15 18:06:04,644 [INFO] Label accuracies statistics:
2025-01-15 18:06:04,644 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.5, 65: 0.75, 66: 0.25, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.5, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.5, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 0.75, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.75, 261: 1.0, 262: 0.5, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.25, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.25, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.75, 313: 0.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 1.0, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.5, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.25, 356: 0.75, 357: 1.0, 358: 0.5, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 18:06:04,645 [INFO] [16] TRAIN  loss: 0.9645311025469407 acc: 0.9917680304705738
2025-01-15 18:06:04,646 [INFO] [16] TRAIN  loss dict: {'classification_loss': 0.9645311025469407}
2025-01-15 18:06:04,646 [INFO] [16] VALIDATION loss: 1.9855339076734126 VALIDATION acc: 0.7699059561128526
2025-01-15 18:06:04,646 [INFO] [16] VALIDATION loss dict: {'classification_loss': 1.9855339076734126}
2025-01-15 18:06:04,646 [INFO] 
2025-01-15 18:06:21,724 [INFO] Step[50/2713]: training loss : 0.9513461470603943 TRAIN  loss dict:  {'classification_loss': 0.9513461470603943}
2025-01-15 18:06:33,606 [INFO] Step[100/2713]: training loss : 0.937822721004486 TRAIN  loss dict:  {'classification_loss': 0.937822721004486}
2025-01-15 18:06:45,499 [INFO] Step[150/2713]: training loss : 0.9821713864803314 TRAIN  loss dict:  {'classification_loss': 0.9821713864803314}
2025-01-15 18:06:57,426 [INFO] Step[200/2713]: training loss : 0.9434475147724152 TRAIN  loss dict:  {'classification_loss': 0.9434475147724152}
2025-01-15 18:07:09,352 [INFO] Step[250/2713]: training loss : 1.0111548912525177 TRAIN  loss dict:  {'classification_loss': 1.0111548912525177}
2025-01-15 18:07:21,266 [INFO] Step[300/2713]: training loss : 0.9506131660938263 TRAIN  loss dict:  {'classification_loss': 0.9506131660938263}
2025-01-15 18:07:33,198 [INFO] Step[350/2713]: training loss : 0.9621487009525299 TRAIN  loss dict:  {'classification_loss': 0.9621487009525299}
2025-01-15 18:07:45,137 [INFO] Step[400/2713]: training loss : 0.9921299231052398 TRAIN  loss dict:  {'classification_loss': 0.9921299231052398}
2025-01-15 18:07:57,096 [INFO] Step[450/2713]: training loss : 0.9577102935314179 TRAIN  loss dict:  {'classification_loss': 0.9577102935314179}
2025-01-15 18:08:09,014 [INFO] Step[500/2713]: training loss : 1.0136983132362365 TRAIN  loss dict:  {'classification_loss': 1.0136983132362365}
2025-01-15 18:08:20,983 [INFO] Step[550/2713]: training loss : 1.0292486500740052 TRAIN  loss dict:  {'classification_loss': 1.0292486500740052}
2025-01-15 18:08:32,939 [INFO] Step[600/2713]: training loss : 0.9537244880199433 TRAIN  loss dict:  {'classification_loss': 0.9537244880199433}
2025-01-15 18:08:44,902 [INFO] Step[650/2713]: training loss : 0.9873341536521911 TRAIN  loss dict:  {'classification_loss': 0.9873341536521911}
2025-01-15 18:08:56,834 [INFO] Step[700/2713]: training loss : 0.9982145345211029 TRAIN  loss dict:  {'classification_loss': 0.9982145345211029}
2025-01-15 18:09:08,805 [INFO] Step[750/2713]: training loss : 0.9706313216686249 TRAIN  loss dict:  {'classification_loss': 0.9706313216686249}
2025-01-15 18:09:20,730 [INFO] Step[800/2713]: training loss : 1.0253892123699189 TRAIN  loss dict:  {'classification_loss': 1.0253892123699189}
2025-01-15 18:09:32,690 [INFO] Step[850/2713]: training loss : 0.9514634394645691 TRAIN  loss dict:  {'classification_loss': 0.9514634394645691}
2025-01-15 18:09:44,642 [INFO] Step[900/2713]: training loss : 0.9430237972736358 TRAIN  loss dict:  {'classification_loss': 0.9430237972736358}
2025-01-15 18:09:56,572 [INFO] Step[950/2713]: training loss : 0.9994420349597931 TRAIN  loss dict:  {'classification_loss': 0.9994420349597931}
2025-01-15 18:10:08,502 [INFO] Step[1000/2713]: training loss : 0.9723415637016296 TRAIN  loss dict:  {'classification_loss': 0.9723415637016296}
2025-01-15 18:10:20,413 [INFO] Step[1050/2713]: training loss : 0.9798244833946228 TRAIN  loss dict:  {'classification_loss': 0.9798244833946228}
2025-01-15 18:10:32,322 [INFO] Step[1100/2713]: training loss : 0.9622303569316863 TRAIN  loss dict:  {'classification_loss': 0.9622303569316863}
2025-01-15 18:10:44,244 [INFO] Step[1150/2713]: training loss : 0.9790008962154388 TRAIN  loss dict:  {'classification_loss': 0.9790008962154388}
2025-01-15 18:10:56,154 [INFO] Step[1200/2713]: training loss : 0.9436149179935456 TRAIN  loss dict:  {'classification_loss': 0.9436149179935456}
2025-01-15 18:11:08,046 [INFO] Step[1250/2713]: training loss : 0.9759109294414521 TRAIN  loss dict:  {'classification_loss': 0.9759109294414521}
2025-01-15 18:11:19,994 [INFO] Step[1300/2713]: training loss : 0.9394593632221222 TRAIN  loss dict:  {'classification_loss': 0.9394593632221222}
2025-01-15 18:11:31,959 [INFO] Step[1350/2713]: training loss : 0.94174556016922 TRAIN  loss dict:  {'classification_loss': 0.94174556016922}
2025-01-15 18:11:43,885 [INFO] Step[1400/2713]: training loss : 0.992824159860611 TRAIN  loss dict:  {'classification_loss': 0.992824159860611}
2025-01-15 18:11:55,833 [INFO] Step[1450/2713]: training loss : 1.0065146720409393 TRAIN  loss dict:  {'classification_loss': 1.0065146720409393}
2025-01-15 18:12:07,765 [INFO] Step[1500/2713]: training loss : 0.9645654118061066 TRAIN  loss dict:  {'classification_loss': 0.9645654118061066}
2025-01-15 18:12:19,710 [INFO] Step[1550/2713]: training loss : 0.943801075220108 TRAIN  loss dict:  {'classification_loss': 0.943801075220108}
2025-01-15 18:12:31,634 [INFO] Step[1600/2713]: training loss : 0.9416558229923249 TRAIN  loss dict:  {'classification_loss': 0.9416558229923249}
2025-01-15 18:12:43,557 [INFO] Step[1650/2713]: training loss : 0.9872739768028259 TRAIN  loss dict:  {'classification_loss': 0.9872739768028259}
2025-01-15 18:12:55,488 [INFO] Step[1700/2713]: training loss : 0.9393954360485077 TRAIN  loss dict:  {'classification_loss': 0.9393954360485077}
2025-01-15 18:13:07,426 [INFO] Step[1750/2713]: training loss : 0.9883178520202637 TRAIN  loss dict:  {'classification_loss': 0.9883178520202637}
2025-01-15 18:13:19,346 [INFO] Step[1800/2713]: training loss : 0.9748844456672668 TRAIN  loss dict:  {'classification_loss': 0.9748844456672668}
2025-01-15 18:13:31,289 [INFO] Step[1850/2713]: training loss : 1.0453745877742768 TRAIN  loss dict:  {'classification_loss': 1.0453745877742768}
2025-01-15 18:13:43,192 [INFO] Step[1900/2713]: training loss : 1.0080616164207459 TRAIN  loss dict:  {'classification_loss': 1.0080616164207459}
2025-01-15 18:13:55,165 [INFO] Step[1950/2713]: training loss : 0.9562611925601959 TRAIN  loss dict:  {'classification_loss': 0.9562611925601959}
2025-01-15 18:14:07,105 [INFO] Step[2000/2713]: training loss : 0.9455734467506409 TRAIN  loss dict:  {'classification_loss': 0.9455734467506409}
2025-01-15 18:14:19,014 [INFO] Step[2050/2713]: training loss : 0.9851558828353881 TRAIN  loss dict:  {'classification_loss': 0.9851558828353881}
2025-01-15 18:14:30,957 [INFO] Step[2100/2713]: training loss : 0.982170375585556 TRAIN  loss dict:  {'classification_loss': 0.982170375585556}
2025-01-15 18:14:42,872 [INFO] Step[2150/2713]: training loss : 1.0119621384143829 TRAIN  loss dict:  {'classification_loss': 1.0119621384143829}
2025-01-15 18:14:54,787 [INFO] Step[2200/2713]: training loss : 0.9365323185920715 TRAIN  loss dict:  {'classification_loss': 0.9365323185920715}
2025-01-15 18:15:06,779 [INFO] Step[2250/2713]: training loss : 0.9668607294559479 TRAIN  loss dict:  {'classification_loss': 0.9668607294559479}
2025-01-15 18:15:18,654 [INFO] Step[2300/2713]: training loss : 0.9859000301361084 TRAIN  loss dict:  {'classification_loss': 0.9859000301361084}
2025-01-15 18:15:30,592 [INFO] Step[2350/2713]: training loss : 0.9575069856643676 TRAIN  loss dict:  {'classification_loss': 0.9575069856643676}
2025-01-15 18:15:42,502 [INFO] Step[2400/2713]: training loss : 0.9478210473060608 TRAIN  loss dict:  {'classification_loss': 0.9478210473060608}
2025-01-15 18:15:54,465 [INFO] Step[2450/2713]: training loss : 0.9744551014900208 TRAIN  loss dict:  {'classification_loss': 0.9744551014900208}
2025-01-15 18:16:06,381 [INFO] Step[2500/2713]: training loss : 0.977099050283432 TRAIN  loss dict:  {'classification_loss': 0.977099050283432}
2025-01-15 18:16:18,290 [INFO] Step[2550/2713]: training loss : 0.9530094146728516 TRAIN  loss dict:  {'classification_loss': 0.9530094146728516}
2025-01-15 18:16:30,239 [INFO] Step[2600/2713]: training loss : 1.0120552206039428 TRAIN  loss dict:  {'classification_loss': 1.0120552206039428}
2025-01-15 18:16:42,190 [INFO] Step[2650/2713]: training loss : 1.0212264215946198 TRAIN  loss dict:  {'classification_loss': 1.0212264215946198}
2025-01-15 18:16:54,079 [INFO] Step[2700/2713]: training loss : 1.052667692899704 TRAIN  loss dict:  {'classification_loss': 1.052667692899704}
2025-01-15 18:18:07,406 [INFO] Label accuracies statistics:
2025-01-15 18:18:07,406 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.25, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.5, 165: 0.75, 166: 1.0, 167: 0.5, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.5, 187: 0.75, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.25, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 1.0, 200: 0.5, 201: 0.25, 202: 0.75, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 0.75, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.5, 263: 0.75, 264: 1.0, 265: 0.5, 266: 1.0, 267: 0.0, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.75, 292: 0.75, 293: 1.0, 294: 1.0, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.25, 314: 0.75, 315: 0.5, 316: 0.5, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.5, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.5, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.25, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.0, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.5, 377: 1.0, 378: 0.25, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 18:18:07,408 [INFO] [17] TRAIN  loss: 0.9763286921796085 acc: 0.9878363435311464
2025-01-15 18:18:07,408 [INFO] [17] TRAIN  loss dict: {'classification_loss': 0.9763286921796085}
2025-01-15 18:18:07,408 [INFO] [17] VALIDATION loss: 2.0051661745498053 VALIDATION acc: 0.7667711598746082
2025-01-15 18:18:07,408 [INFO] [17] VALIDATION loss dict: {'classification_loss': 2.0051661745498053}
2025-01-15 18:18:07,408 [INFO] 
2025-01-15 18:18:24,217 [INFO] Step[50/2713]: training loss : 1.037871803045273 TRAIN  loss dict:  {'classification_loss': 1.037871803045273}
2025-01-15 18:18:36,108 [INFO] Step[100/2713]: training loss : 0.9457831084728241 TRAIN  loss dict:  {'classification_loss': 0.9457831084728241}
2025-01-15 18:18:48,048 [INFO] Step[150/2713]: training loss : 0.9705169451236725 TRAIN  loss dict:  {'classification_loss': 0.9705169451236725}
2025-01-15 18:18:59,943 [INFO] Step[200/2713]: training loss : 0.9578711378574372 TRAIN  loss dict:  {'classification_loss': 0.9578711378574372}
2025-01-15 18:19:11,842 [INFO] Step[250/2713]: training loss : 0.9373932564258576 TRAIN  loss dict:  {'classification_loss': 0.9373932564258576}
2025-01-15 18:19:23,740 [INFO] Step[300/2713]: training loss : 0.9395484781265259 TRAIN  loss dict:  {'classification_loss': 0.9395484781265259}
2025-01-15 18:19:35,653 [INFO] Step[350/2713]: training loss : 1.0578340756893159 TRAIN  loss dict:  {'classification_loss': 1.0578340756893159}
2025-01-15 18:19:47,572 [INFO] Step[400/2713]: training loss : 0.9419685637950898 TRAIN  loss dict:  {'classification_loss': 0.9419685637950898}
2025-01-15 18:19:59,473 [INFO] Step[450/2713]: training loss : 0.9460866069793701 TRAIN  loss dict:  {'classification_loss': 0.9460866069793701}
2025-01-15 18:20:11,401 [INFO] Step[500/2713]: training loss : 0.9365875554084778 TRAIN  loss dict:  {'classification_loss': 0.9365875554084778}
2025-01-15 18:20:23,339 [INFO] Step[550/2713]: training loss : 0.994922503232956 TRAIN  loss dict:  {'classification_loss': 0.994922503232956}
2025-01-15 18:20:35,226 [INFO] Step[600/2713]: training loss : 0.9663385450839996 TRAIN  loss dict:  {'classification_loss': 0.9663385450839996}
2025-01-15 18:20:47,184 [INFO] Step[650/2713]: training loss : 0.9440121555328369 TRAIN  loss dict:  {'classification_loss': 0.9440121555328369}
2025-01-15 18:20:59,101 [INFO] Step[700/2713]: training loss : 0.9515649259090424 TRAIN  loss dict:  {'classification_loss': 0.9515649259090424}
2025-01-15 18:21:10,991 [INFO] Step[750/2713]: training loss : 0.9522683191299438 TRAIN  loss dict:  {'classification_loss': 0.9522683191299438}
2025-01-15 18:21:22,920 [INFO] Step[800/2713]: training loss : 0.97726700425148 TRAIN  loss dict:  {'classification_loss': 0.97726700425148}
2025-01-15 18:21:34,845 [INFO] Step[850/2713]: training loss : 0.9393620026111603 TRAIN  loss dict:  {'classification_loss': 0.9393620026111603}
2025-01-15 18:21:46,740 [INFO] Step[900/2713]: training loss : 0.951282205581665 TRAIN  loss dict:  {'classification_loss': 0.951282205581665}
2025-01-15 18:21:58,691 [INFO] Step[950/2713]: training loss : 0.9694898998737336 TRAIN  loss dict:  {'classification_loss': 0.9694898998737336}
2025-01-15 18:22:10,621 [INFO] Step[1000/2713]: training loss : 0.9999474203586578 TRAIN  loss dict:  {'classification_loss': 0.9999474203586578}
2025-01-15 18:22:22,532 [INFO] Step[1050/2713]: training loss : 0.9911516273021698 TRAIN  loss dict:  {'classification_loss': 0.9911516273021698}
2025-01-15 18:22:34,440 [INFO] Step[1100/2713]: training loss : 0.9405634295940399 TRAIN  loss dict:  {'classification_loss': 0.9405634295940399}
2025-01-15 18:22:46,341 [INFO] Step[1150/2713]: training loss : 0.9363317656517028 TRAIN  loss dict:  {'classification_loss': 0.9363317656517028}
2025-01-15 18:22:58,253 [INFO] Step[1200/2713]: training loss : 0.9608315229415894 TRAIN  loss dict:  {'classification_loss': 0.9608315229415894}
2025-01-15 18:23:10,131 [INFO] Step[1250/2713]: training loss : 0.958495534658432 TRAIN  loss dict:  {'classification_loss': 0.958495534658432}
2025-01-15 18:23:22,014 [INFO] Step[1300/2713]: training loss : 0.9523416340351105 TRAIN  loss dict:  {'classification_loss': 0.9523416340351105}
2025-01-15 18:23:33,930 [INFO] Step[1350/2713]: training loss : 0.9371450781822205 TRAIN  loss dict:  {'classification_loss': 0.9371450781822205}
2025-01-15 18:23:45,839 [INFO] Step[1400/2713]: training loss : 0.9510528218746185 TRAIN  loss dict:  {'classification_loss': 0.9510528218746185}
2025-01-15 18:23:57,747 [INFO] Step[1450/2713]: training loss : 0.9767955029010773 TRAIN  loss dict:  {'classification_loss': 0.9767955029010773}
2025-01-15 18:24:09,646 [INFO] Step[1500/2713]: training loss : 0.9454113173484803 TRAIN  loss dict:  {'classification_loss': 0.9454113173484803}
2025-01-15 18:24:21,582 [INFO] Step[1550/2713]: training loss : 0.9435764133930207 TRAIN  loss dict:  {'classification_loss': 0.9435764133930207}
2025-01-15 18:24:33,510 [INFO] Step[1600/2713]: training loss : 0.9479573643207551 TRAIN  loss dict:  {'classification_loss': 0.9479573643207551}
2025-01-15 18:24:45,392 [INFO] Step[1650/2713]: training loss : 0.9452345514297485 TRAIN  loss dict:  {'classification_loss': 0.9452345514297485}
2025-01-15 18:24:57,285 [INFO] Step[1700/2713]: training loss : 0.9501660275459289 TRAIN  loss dict:  {'classification_loss': 0.9501660275459289}
2025-01-15 18:25:09,170 [INFO] Step[1750/2713]: training loss : 1.0040585505962372 TRAIN  loss dict:  {'classification_loss': 1.0040585505962372}
2025-01-15 18:25:21,043 [INFO] Step[1800/2713]: training loss : 0.9369027686119079 TRAIN  loss dict:  {'classification_loss': 0.9369027686119079}
2025-01-15 18:25:33,001 [INFO] Step[1850/2713]: training loss : 1.0395964574813843 TRAIN  loss dict:  {'classification_loss': 1.0395964574813843}
2025-01-15 18:25:44,919 [INFO] Step[1900/2713]: training loss : 0.9515115094184875 TRAIN  loss dict:  {'classification_loss': 0.9515115094184875}
2025-01-15 18:25:56,862 [INFO] Step[1950/2713]: training loss : 0.9497740387916564 TRAIN  loss dict:  {'classification_loss': 0.9497740387916564}
2025-01-15 18:26:08,770 [INFO] Step[2000/2713]: training loss : 0.9785170066356659 TRAIN  loss dict:  {'classification_loss': 0.9785170066356659}
2025-01-15 18:26:20,734 [INFO] Step[2050/2713]: training loss : 0.9605310952663422 TRAIN  loss dict:  {'classification_loss': 0.9605310952663422}
2025-01-15 18:26:32,636 [INFO] Step[2100/2713]: training loss : 0.9650994110107421 TRAIN  loss dict:  {'classification_loss': 0.9650994110107421}
2025-01-15 18:26:44,554 [INFO] Step[2150/2713]: training loss : 0.9719509506225585 TRAIN  loss dict:  {'classification_loss': 0.9719509506225585}
2025-01-15 18:26:56,453 [INFO] Step[2200/2713]: training loss : 0.9398611998558044 TRAIN  loss dict:  {'classification_loss': 0.9398611998558044}
2025-01-15 18:27:08,360 [INFO] Step[2250/2713]: training loss : 0.9440225768089294 TRAIN  loss dict:  {'classification_loss': 0.9440225768089294}
2025-01-15 18:27:20,226 [INFO] Step[2300/2713]: training loss : 1.140483078956604 TRAIN  loss dict:  {'classification_loss': 1.140483078956604}
2025-01-15 18:27:32,132 [INFO] Step[2350/2713]: training loss : 0.9847922170162201 TRAIN  loss dict:  {'classification_loss': 0.9847922170162201}
2025-01-15 18:27:44,002 [INFO] Step[2400/2713]: training loss : 0.9678591740131378 TRAIN  loss dict:  {'classification_loss': 0.9678591740131378}
2025-01-15 18:27:55,896 [INFO] Step[2450/2713]: training loss : 0.9472557365894317 TRAIN  loss dict:  {'classification_loss': 0.9472557365894317}
2025-01-15 18:28:07,771 [INFO] Step[2500/2713]: training loss : 0.9580187582969666 TRAIN  loss dict:  {'classification_loss': 0.9580187582969666}
2025-01-15 18:28:19,671 [INFO] Step[2550/2713]: training loss : 0.9445421040058136 TRAIN  loss dict:  {'classification_loss': 0.9445421040058136}
2025-01-15 18:28:31,580 [INFO] Step[2600/2713]: training loss : 0.9849163103103638 TRAIN  loss dict:  {'classification_loss': 0.9849163103103638}
2025-01-15 18:28:43,504 [INFO] Step[2650/2713]: training loss : 1.014522135257721 TRAIN  loss dict:  {'classification_loss': 1.014522135257721}
2025-01-15 18:28:55,359 [INFO] Step[2700/2713]: training loss : 0.9676356744766236 TRAIN  loss dict:  {'classification_loss': 0.9676356744766236}
2025-01-15 18:30:08,016 [INFO] Label accuracies statistics:
2025-01-15 18:30:08,016 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.75, 9: 0.75, 10: 0.75, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 1.0, 57: 0.5, 58: 0.75, 59: 1.0, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 1.0, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.0, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.5, 115: 0.75, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.5, 140: 0.75, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.5, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.25, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.75, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.6666666666666666, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.25, 187: 1.0, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 0.75, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.5, 206: 0.5, 207: 0.5, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.25, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.75, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.5, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 0.5, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.5, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.5, 324: 0.75, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.5, 331: 0.75, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 0.75, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 0.5, 358: 1.0, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.5, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.5, 379: 0.75, 380: 1.0, 381: 1.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 1.0, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.5, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 18:30:36,574 [INFO] [18] TRAIN  loss: 0.9671650479914647 acc: 0.9912765696031454
2025-01-15 18:30:36,574 [INFO] [18] TRAIN  loss dict: {'classification_loss': 0.9671650479914647}
2025-01-15 18:30:36,574 [INFO] [18] VALIDATION loss: 1.9215741690836454 VALIDATION acc: 0.7786833855799373
2025-01-15 18:30:36,574 [INFO] [18] VALIDATION loss dict: {'classification_loss': 1.9215741690836454}
2025-01-15 18:30:36,574 [INFO] 
2025-01-15 18:30:53,745 [INFO] Step[50/2713]: training loss : 0.9872925841808319 TRAIN  loss dict:  {'classification_loss': 0.9872925841808319}
2025-01-15 18:31:05,613 [INFO] Step[100/2713]: training loss : 0.9340369772911071 TRAIN  loss dict:  {'classification_loss': 0.9340369772911071}
2025-01-15 18:31:17,595 [INFO] Step[150/2713]: training loss : 0.9803878343105317 TRAIN  loss dict:  {'classification_loss': 0.9803878343105317}
2025-01-15 18:31:29,497 [INFO] Step[200/2713]: training loss : 0.9767367053031921 TRAIN  loss dict:  {'classification_loss': 0.9767367053031921}
2025-01-15 18:31:41,460 [INFO] Step[250/2713]: training loss : 0.945432894229889 TRAIN  loss dict:  {'classification_loss': 0.945432894229889}
2025-01-15 18:31:53,407 [INFO] Step[300/2713]: training loss : 1.0638325202465058 TRAIN  loss dict:  {'classification_loss': 1.0638325202465058}
2025-01-15 18:32:05,396 [INFO] Step[350/2713]: training loss : 0.9529037296772003 TRAIN  loss dict:  {'classification_loss': 0.9529037296772003}
2025-01-15 18:32:17,343 [INFO] Step[400/2713]: training loss : 0.9645971262454986 TRAIN  loss dict:  {'classification_loss': 0.9645971262454986}
2025-01-15 18:32:29,345 [INFO] Step[450/2713]: training loss : 0.9572586250305176 TRAIN  loss dict:  {'classification_loss': 0.9572586250305176}
2025-01-15 18:32:41,349 [INFO] Step[500/2713]: training loss : 1.0226254630088807 TRAIN  loss dict:  {'classification_loss': 1.0226254630088807}
2025-01-15 18:32:53,343 [INFO] Step[550/2713]: training loss : 0.9511492443084717 TRAIN  loss dict:  {'classification_loss': 0.9511492443084717}
2025-01-15 18:33:05,336 [INFO] Step[600/2713]: training loss : 0.9909966731071472 TRAIN  loss dict:  {'classification_loss': 0.9909966731071472}
2025-01-15 18:33:17,328 [INFO] Step[650/2713]: training loss : 0.9561077797412872 TRAIN  loss dict:  {'classification_loss': 0.9561077797412872}
2025-01-15 18:33:29,337 [INFO] Step[700/2713]: training loss : 0.9756933462619781 TRAIN  loss dict:  {'classification_loss': 0.9756933462619781}
2025-01-15 18:33:41,289 [INFO] Step[750/2713]: training loss : 0.9489290189743042 TRAIN  loss dict:  {'classification_loss': 0.9489290189743042}
2025-01-15 18:33:53,264 [INFO] Step[800/2713]: training loss : 0.9615161204338074 TRAIN  loss dict:  {'classification_loss': 0.9615161204338074}
2025-01-15 18:34:05,222 [INFO] Step[850/2713]: training loss : 0.9540750968456269 TRAIN  loss dict:  {'classification_loss': 0.9540750968456269}
2025-01-15 18:34:17,183 [INFO] Step[900/2713]: training loss : 1.00746844291687 TRAIN  loss dict:  {'classification_loss': 1.00746844291687}
2025-01-15 18:34:29,218 [INFO] Step[950/2713]: training loss : 0.9750417733192444 TRAIN  loss dict:  {'classification_loss': 0.9750417733192444}
2025-01-15 18:34:41,196 [INFO] Step[1000/2713]: training loss : 0.9653660798072815 TRAIN  loss dict:  {'classification_loss': 0.9653660798072815}
2025-01-15 18:34:53,164 [INFO] Step[1050/2713]: training loss : 0.9433633959293366 TRAIN  loss dict:  {'classification_loss': 0.9433633959293366}
2025-01-15 18:35:05,111 [INFO] Step[1100/2713]: training loss : 1.015556627511978 TRAIN  loss dict:  {'classification_loss': 1.015556627511978}
2025-01-15 18:35:17,088 [INFO] Step[1150/2713]: training loss : 0.970245350599289 TRAIN  loss dict:  {'classification_loss': 0.970245350599289}
2025-01-15 18:35:29,033 [INFO] Step[1200/2713]: training loss : 0.9851978826522827 TRAIN  loss dict:  {'classification_loss': 0.9851978826522827}
2025-01-15 18:35:40,991 [INFO] Step[1250/2713]: training loss : 0.9424427962303161 TRAIN  loss dict:  {'classification_loss': 0.9424427962303161}
2025-01-15 18:35:52,920 [INFO] Step[1300/2713]: training loss : 0.9342137205600739 TRAIN  loss dict:  {'classification_loss': 0.9342137205600739}
2025-01-15 18:36:04,914 [INFO] Step[1350/2713]: training loss : 0.9549744617938996 TRAIN  loss dict:  {'classification_loss': 0.9549744617938996}
2025-01-15 18:36:16,889 [INFO] Step[1400/2713]: training loss : 0.9790719759464264 TRAIN  loss dict:  {'classification_loss': 0.9790719759464264}
2025-01-15 18:36:28,876 [INFO] Step[1450/2713]: training loss : 0.9473652529716492 TRAIN  loss dict:  {'classification_loss': 0.9473652529716492}
2025-01-15 18:36:40,834 [INFO] Step[1500/2713]: training loss : 0.9575368988513947 TRAIN  loss dict:  {'classification_loss': 0.9575368988513947}
2025-01-15 18:36:52,811 [INFO] Step[1550/2713]: training loss : 0.9380263006687164 TRAIN  loss dict:  {'classification_loss': 0.9380263006687164}
2025-01-15 18:37:04,754 [INFO] Step[1600/2713]: training loss : 1.0008344805240632 TRAIN  loss dict:  {'classification_loss': 1.0008344805240632}
2025-01-15 18:37:16,737 [INFO] Step[1650/2713]: training loss : 0.966703999042511 TRAIN  loss dict:  {'classification_loss': 0.966703999042511}
2025-01-15 18:37:28,695 [INFO] Step[1700/2713]: training loss : 0.984447340965271 TRAIN  loss dict:  {'classification_loss': 0.984447340965271}
2025-01-15 18:37:40,658 [INFO] Step[1750/2713]: training loss : 0.9756715059280395 TRAIN  loss dict:  {'classification_loss': 0.9756715059280395}
2025-01-15 18:37:52,619 [INFO] Step[1800/2713]: training loss : 0.9702698373794556 TRAIN  loss dict:  {'classification_loss': 0.9702698373794556}
2025-01-15 18:38:04,569 [INFO] Step[1850/2713]: training loss : 0.9748612117767333 TRAIN  loss dict:  {'classification_loss': 0.9748612117767333}
2025-01-15 18:38:16,576 [INFO] Step[1900/2713]: training loss : 0.972661726474762 TRAIN  loss dict:  {'classification_loss': 0.972661726474762}
2025-01-15 18:38:28,544 [INFO] Step[1950/2713]: training loss : 0.9476398348808288 TRAIN  loss dict:  {'classification_loss': 0.9476398348808288}
2025-01-15 18:38:40,495 [INFO] Step[2000/2713]: training loss : 1.036771606206894 TRAIN  loss dict:  {'classification_loss': 1.036771606206894}
2025-01-15 18:38:52,470 [INFO] Step[2050/2713]: training loss : 0.9532317173480988 TRAIN  loss dict:  {'classification_loss': 0.9532317173480988}
2025-01-15 18:39:04,410 [INFO] Step[2100/2713]: training loss : 0.9403725481033325 TRAIN  loss dict:  {'classification_loss': 0.9403725481033325}
2025-01-15 18:39:16,349 [INFO] Step[2150/2713]: training loss : 0.9557331466674804 TRAIN  loss dict:  {'classification_loss': 0.9557331466674804}
2025-01-15 18:39:28,337 [INFO] Step[2200/2713]: training loss : 1.0283360600471496 TRAIN  loss dict:  {'classification_loss': 1.0283360600471496}
2025-01-15 18:39:40,299 [INFO] Step[2250/2713]: training loss : 0.9844182288646698 TRAIN  loss dict:  {'classification_loss': 0.9844182288646698}
2025-01-15 18:39:52,264 [INFO] Step[2300/2713]: training loss : 0.9746117389202118 TRAIN  loss dict:  {'classification_loss': 0.9746117389202118}
2025-01-15 18:40:04,248 [INFO] Step[2350/2713]: training loss : 0.949423770904541 TRAIN  loss dict:  {'classification_loss': 0.949423770904541}
2025-01-15 18:40:16,219 [INFO] Step[2400/2713]: training loss : 0.9736997509002685 TRAIN  loss dict:  {'classification_loss': 0.9736997509002685}
2025-01-15 18:40:28,182 [INFO] Step[2450/2713]: training loss : 0.9508713841438293 TRAIN  loss dict:  {'classification_loss': 0.9508713841438293}
2025-01-15 18:40:40,126 [INFO] Step[2500/2713]: training loss : 0.9552097582817077 TRAIN  loss dict:  {'classification_loss': 0.9552097582817077}
2025-01-15 18:40:52,116 [INFO] Step[2550/2713]: training loss : 0.9383520460128785 TRAIN  loss dict:  {'classification_loss': 0.9383520460128785}
2025-01-15 18:41:04,051 [INFO] Step[2600/2713]: training loss : 0.9750760591030121 TRAIN  loss dict:  {'classification_loss': 0.9750760591030121}
2025-01-15 18:41:16,007 [INFO] Step[2650/2713]: training loss : 0.939655796289444 TRAIN  loss dict:  {'classification_loss': 0.939655796289444}
2025-01-15 18:41:27,969 [INFO] Step[2700/2713]: training loss : 0.963633850812912 TRAIN  loss dict:  {'classification_loss': 0.963633850812912}
2025-01-15 18:42:41,068 [INFO] Label accuracies statistics:
2025-01-15 18:42:41,068 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 1.0, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.0, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 1.0, 19: 0.5, 20: 1.0, 21: 0.5, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 1.0, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 1.0, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 1.0, 89: 0.75, 90: 0.75, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.5, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.25, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.5, 140: 1.0, 141: 1.0, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.0, 161: 0.5, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.25, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 0.5, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.5, 186: 0.5, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.25, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.25, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.0, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 0.75, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.5, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.25, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.0, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.25, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 0.5, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.5, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.25, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 0.5, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 18:42:41,070 [INFO] [19] TRAIN  loss: 0.9694109534892129 acc: 0.9905393783020027
2025-01-15 18:42:41,070 [INFO] [19] TRAIN  loss dict: {'classification_loss': 0.9694109534892129}
2025-01-15 18:42:41,070 [INFO] [19] VALIDATION loss: 1.999950804880687 VALIDATION acc: 0.761128526645768
2025-01-15 18:42:41,070 [INFO] [19] VALIDATION loss dict: {'classification_loss': 1.999950804880687}
2025-01-15 18:42:41,070 [INFO] 
2025-01-15 18:42:57,967 [INFO] Step[50/2713]: training loss : 0.9672770524024963 TRAIN  loss dict:  {'classification_loss': 0.9672770524024963}
2025-01-15 18:43:09,801 [INFO] Step[100/2713]: training loss : 0.9550015842914581 TRAIN  loss dict:  {'classification_loss': 0.9550015842914581}
2025-01-15 18:43:21,664 [INFO] Step[150/2713]: training loss : 1.0568371260166167 TRAIN  loss dict:  {'classification_loss': 1.0568371260166167}
2025-01-15 18:43:33,598 [INFO] Step[200/2713]: training loss : 0.9634049499034881 TRAIN  loss dict:  {'classification_loss': 0.9634049499034881}
2025-01-15 18:43:45,498 [INFO] Step[250/2713]: training loss : 1.0249034655094147 TRAIN  loss dict:  {'classification_loss': 1.0249034655094147}
2025-01-15 18:43:57,411 [INFO] Step[300/2713]: training loss : 0.9451640582084656 TRAIN  loss dict:  {'classification_loss': 0.9451640582084656}
2025-01-15 18:44:09,376 [INFO] Step[350/2713]: training loss : 0.9496404147148132 TRAIN  loss dict:  {'classification_loss': 0.9496404147148132}
2025-01-15 18:44:21,315 [INFO] Step[400/2713]: training loss : 0.9824142408370972 TRAIN  loss dict:  {'classification_loss': 0.9824142408370972}
2025-01-15 18:44:33,299 [INFO] Step[450/2713]: training loss : 0.9482696032524109 TRAIN  loss dict:  {'classification_loss': 0.9482696032524109}
2025-01-15 18:44:45,200 [INFO] Step[500/2713]: training loss : 0.9562375354766846 TRAIN  loss dict:  {'classification_loss': 0.9562375354766846}
2025-01-15 18:44:57,126 [INFO] Step[550/2713]: training loss : 1.0207289731502533 TRAIN  loss dict:  {'classification_loss': 1.0207289731502533}
2025-01-15 18:45:09,039 [INFO] Step[600/2713]: training loss : 0.9772068452835083 TRAIN  loss dict:  {'classification_loss': 0.9772068452835083}
2025-01-15 18:45:20,964 [INFO] Step[650/2713]: training loss : 0.9371338009834289 TRAIN  loss dict:  {'classification_loss': 0.9371338009834289}
2025-01-15 18:45:32,892 [INFO] Step[700/2713]: training loss : 1.007075617313385 TRAIN  loss dict:  {'classification_loss': 1.007075617313385}
2025-01-15 18:45:44,833 [INFO] Step[750/2713]: training loss : 0.9710754013061523 TRAIN  loss dict:  {'classification_loss': 0.9710754013061523}
2025-01-15 18:45:56,750 [INFO] Step[800/2713]: training loss : 0.9702298164367675 TRAIN  loss dict:  {'classification_loss': 0.9702298164367675}
2025-01-15 18:46:08,682 [INFO] Step[850/2713]: training loss : 0.9851086378097534 TRAIN  loss dict:  {'classification_loss': 0.9851086378097534}
2025-01-15 18:46:20,604 [INFO] Step[900/2713]: training loss : 0.9423132407665252 TRAIN  loss dict:  {'classification_loss': 0.9423132407665252}
2025-01-15 18:46:32,648 [INFO] Step[950/2713]: training loss : 0.9722130513191223 TRAIN  loss dict:  {'classification_loss': 0.9722130513191223}
2025-01-15 18:46:44,550 [INFO] Step[1000/2713]: training loss : 0.990733927488327 TRAIN  loss dict:  {'classification_loss': 0.990733927488327}
2025-01-15 18:46:56,482 [INFO] Step[1050/2713]: training loss : 0.9654799389839173 TRAIN  loss dict:  {'classification_loss': 0.9654799389839173}
2025-01-15 18:47:08,411 [INFO] Step[1100/2713]: training loss : 1.0174539577960968 TRAIN  loss dict:  {'classification_loss': 1.0174539577960968}
2025-01-15 18:47:20,344 [INFO] Step[1150/2713]: training loss : 0.9471393728256225 TRAIN  loss dict:  {'classification_loss': 0.9471393728256225}
2025-01-15 18:47:32,254 [INFO] Step[1200/2713]: training loss : 0.9715087831020355 TRAIN  loss dict:  {'classification_loss': 0.9715087831020355}
2025-01-15 18:47:44,196 [INFO] Step[1250/2713]: training loss : 0.9463484728336334 TRAIN  loss dict:  {'classification_loss': 0.9463484728336334}
2025-01-15 18:47:56,118 [INFO] Step[1300/2713]: training loss : 0.9484178721904755 TRAIN  loss dict:  {'classification_loss': 0.9484178721904755}
2025-01-15 18:48:08,042 [INFO] Step[1350/2713]: training loss : 0.9490852558612823 TRAIN  loss dict:  {'classification_loss': 0.9490852558612823}
2025-01-15 18:48:19,954 [INFO] Step[1400/2713]: training loss : 0.9748732948303223 TRAIN  loss dict:  {'classification_loss': 0.9748732948303223}
2025-01-15 18:48:31,891 [INFO] Step[1450/2713]: training loss : 0.9877867364883423 TRAIN  loss dict:  {'classification_loss': 0.9877867364883423}
2025-01-15 18:48:43,834 [INFO] Step[1500/2713]: training loss : 0.9814990341663361 TRAIN  loss dict:  {'classification_loss': 0.9814990341663361}
2025-01-15 18:48:55,814 [INFO] Step[1550/2713]: training loss : 0.9638967180252075 TRAIN  loss dict:  {'classification_loss': 0.9638967180252075}
2025-01-15 18:49:07,751 [INFO] Step[1600/2713]: training loss : 0.968975236415863 TRAIN  loss dict:  {'classification_loss': 0.968975236415863}
2025-01-15 18:49:19,654 [INFO] Step[1650/2713]: training loss : 0.9422800874710083 TRAIN  loss dict:  {'classification_loss': 0.9422800874710083}
2025-01-15 18:49:31,553 [INFO] Step[1700/2713]: training loss : 0.9502979898452759 TRAIN  loss dict:  {'classification_loss': 0.9502979898452759}
2025-01-15 18:49:43,466 [INFO] Step[1750/2713]: training loss : 0.9671017479896545 TRAIN  loss dict:  {'classification_loss': 0.9671017479896545}
2025-01-15 18:49:55,426 [INFO] Step[1800/2713]: training loss : 0.9468611228466034 TRAIN  loss dict:  {'classification_loss': 0.9468611228466034}
2025-01-15 18:50:07,359 [INFO] Step[1850/2713]: training loss : 0.9635885441303254 TRAIN  loss dict:  {'classification_loss': 0.9635885441303254}
2025-01-15 18:50:19,284 [INFO] Step[1900/2713]: training loss : 0.9749852955341339 TRAIN  loss dict:  {'classification_loss': 0.9749852955341339}
2025-01-15 18:50:31,198 [INFO] Step[1950/2713]: training loss : 0.96033212184906 TRAIN  loss dict:  {'classification_loss': 0.96033212184906}
2025-01-15 18:50:43,134 [INFO] Step[2000/2713]: training loss : 0.9510076427459717 TRAIN  loss dict:  {'classification_loss': 0.9510076427459717}
2025-01-15 18:50:55,044 [INFO] Step[2050/2713]: training loss : 0.9778243434429169 TRAIN  loss dict:  {'classification_loss': 0.9778243434429169}
2025-01-15 18:51:06,965 [INFO] Step[2100/2713]: training loss : 0.9575282013416291 TRAIN  loss dict:  {'classification_loss': 0.9575282013416291}
2025-01-15 18:51:18,876 [INFO] Step[2150/2713]: training loss : 1.0170888423919677 TRAIN  loss dict:  {'classification_loss': 1.0170888423919677}
2025-01-15 18:51:30,778 [INFO] Step[2200/2713]: training loss : 0.9446959781646729 TRAIN  loss dict:  {'classification_loss': 0.9446959781646729}
2025-01-15 18:51:42,687 [INFO] Step[2250/2713]: training loss : 1.0229303467273712 TRAIN  loss dict:  {'classification_loss': 1.0229303467273712}
2025-01-15 18:51:54,597 [INFO] Step[2300/2713]: training loss : 0.9640400791168213 TRAIN  loss dict:  {'classification_loss': 0.9640400791168213}
2025-01-15 18:52:06,521 [INFO] Step[2350/2713]: training loss : 0.9520766580104828 TRAIN  loss dict:  {'classification_loss': 0.9520766580104828}
2025-01-15 18:52:18,433 [INFO] Step[2400/2713]: training loss : 0.9735532081127167 TRAIN  loss dict:  {'classification_loss': 0.9735532081127167}
2025-01-15 18:52:30,386 [INFO] Step[2450/2713]: training loss : 0.9771596956253051 TRAIN  loss dict:  {'classification_loss': 0.9771596956253051}
2025-01-15 18:52:42,312 [INFO] Step[2500/2713]: training loss : 1.0634620428085326 TRAIN  loss dict:  {'classification_loss': 1.0634620428085326}
2025-01-15 18:52:54,263 [INFO] Step[2550/2713]: training loss : 0.9450261509418487 TRAIN  loss dict:  {'classification_loss': 0.9450261509418487}
2025-01-15 18:53:06,172 [INFO] Step[2600/2713]: training loss : 0.9588155353069305 TRAIN  loss dict:  {'classification_loss': 0.9588155353069305}
2025-01-15 18:53:18,117 [INFO] Step[2650/2713]: training loss : 1.0002081322669982 TRAIN  loss dict:  {'classification_loss': 1.0002081322669982}
2025-01-15 18:53:30,002 [INFO] Step[2700/2713]: training loss : 1.0401762223243713 TRAIN  loss dict:  {'classification_loss': 1.0401762223243713}
2025-01-15 18:54:43,013 [INFO] Label accuracies statistics:
2025-01-15 18:54:43,013 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.25, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.5, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 1.0, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.0, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.25, 89: 0.75, 90: 0.5, 91: 0.75, 92: 1.0, 93: 0.75, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 0.75, 138: 1.0, 139: 0.5, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.25, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.5, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.5, 245: 1.0, 246: 0.75, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 0.75, 266: 0.25, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 1.0, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 0.5, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.25, 339: 0.75, 340: 0.25, 341: 0.75, 342: 1.0, 343: 0.75, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.75, 377: 1.0, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 18:54:43,015 [INFO] [20] TRAIN  loss: 0.9738261784480763 acc: 0.9893107261334316
2025-01-15 18:54:43,015 [INFO] [20] TRAIN  loss dict: {'classification_loss': 0.9738261784480763}
2025-01-15 18:54:43,015 [INFO] [20] VALIDATION loss: 2.079478712234282 VALIDATION acc: 0.7498432601880878
2025-01-15 18:54:43,015 [INFO] [20] VALIDATION loss dict: {'classification_loss': 2.079478712234282}
2025-01-15 18:54:43,015 [INFO] 
2025-01-15 18:54:59,681 [INFO] Step[50/2713]: training loss : 0.9452548611164093 TRAIN  loss dict:  {'classification_loss': 0.9452548611164093}
2025-01-15 18:55:11,529 [INFO] Step[100/2713]: training loss : 0.984780695438385 TRAIN  loss dict:  {'classification_loss': 0.984780695438385}
2025-01-15 18:55:23,404 [INFO] Step[150/2713]: training loss : 1.0231263899803162 TRAIN  loss dict:  {'classification_loss': 1.0231263899803162}
2025-01-15 18:55:35,282 [INFO] Step[200/2713]: training loss : 0.9398577630519866 TRAIN  loss dict:  {'classification_loss': 0.9398577630519866}
2025-01-15 18:55:47,196 [INFO] Step[250/2713]: training loss : 0.9693578672409058 TRAIN  loss dict:  {'classification_loss': 0.9693578672409058}
2025-01-15 18:55:59,097 [INFO] Step[300/2713]: training loss : 0.994053156375885 TRAIN  loss dict:  {'classification_loss': 0.994053156375885}
2025-01-15 18:56:11,051 [INFO] Step[350/2713]: training loss : 0.9417097544670106 TRAIN  loss dict:  {'classification_loss': 0.9417097544670106}
2025-01-15 18:56:22,991 [INFO] Step[400/2713]: training loss : 0.9353825259208679 TRAIN  loss dict:  {'classification_loss': 0.9353825259208679}
2025-01-15 18:56:34,916 [INFO] Step[450/2713]: training loss : 0.9680351936817169 TRAIN  loss dict:  {'classification_loss': 0.9680351936817169}
2025-01-15 18:56:46,841 [INFO] Step[500/2713]: training loss : 1.0070421981811524 TRAIN  loss dict:  {'classification_loss': 1.0070421981811524}
2025-01-15 18:56:58,812 [INFO] Step[550/2713]: training loss : 0.9434478795528412 TRAIN  loss dict:  {'classification_loss': 0.9434478795528412}
2025-01-15 18:57:10,792 [INFO] Step[600/2713]: training loss : 0.9328891110420227 TRAIN  loss dict:  {'classification_loss': 0.9328891110420227}
2025-01-15 18:57:22,722 [INFO] Step[650/2713]: training loss : 0.936134648323059 TRAIN  loss dict:  {'classification_loss': 0.936134648323059}
2025-01-15 18:57:34,651 [INFO] Step[700/2713]: training loss : 0.9371655249595642 TRAIN  loss dict:  {'classification_loss': 0.9371655249595642}
2025-01-15 18:57:46,551 [INFO] Step[750/2713]: training loss : 0.9624166452884674 TRAIN  loss dict:  {'classification_loss': 0.9624166452884674}
2025-01-15 18:57:58,528 [INFO] Step[800/2713]: training loss : 0.9613172793388367 TRAIN  loss dict:  {'classification_loss': 0.9613172793388367}
2025-01-15 18:58:10,470 [INFO] Step[850/2713]: training loss : 0.967475860118866 TRAIN  loss dict:  {'classification_loss': 0.967475860118866}
2025-01-15 18:58:22,423 [INFO] Step[900/2713]: training loss : 1.008551265001297 TRAIN  loss dict:  {'classification_loss': 1.008551265001297}
2025-01-15 18:58:34,370 [INFO] Step[950/2713]: training loss : 0.9933992230892181 TRAIN  loss dict:  {'classification_loss': 0.9933992230892181}
2025-01-15 18:58:46,317 [INFO] Step[1000/2713]: training loss : 0.9482949817180634 TRAIN  loss dict:  {'classification_loss': 0.9482949817180634}
2025-01-15 18:58:58,209 [INFO] Step[1050/2713]: training loss : 0.9656587219238282 TRAIN  loss dict:  {'classification_loss': 0.9656587219238282}
2025-01-15 18:59:10,101 [INFO] Step[1100/2713]: training loss : 0.9575506973266602 TRAIN  loss dict:  {'classification_loss': 0.9575506973266602}
2025-01-15 18:59:22,067 [INFO] Step[1150/2713]: training loss : 0.9336588990688324 TRAIN  loss dict:  {'classification_loss': 0.9336588990688324}
2025-01-15 18:59:33,964 [INFO] Step[1200/2713]: training loss : 0.9319010841846466 TRAIN  loss dict:  {'classification_loss': 0.9319010841846466}
2025-01-15 18:59:45,895 [INFO] Step[1250/2713]: training loss : 0.9552852916717529 TRAIN  loss dict:  {'classification_loss': 0.9552852916717529}
2025-01-15 18:59:57,774 [INFO] Step[1300/2713]: training loss : 0.9538630843162537 TRAIN  loss dict:  {'classification_loss': 0.9538630843162537}
2025-01-15 19:00:09,713 [INFO] Step[1350/2713]: training loss : 0.990967619419098 TRAIN  loss dict:  {'classification_loss': 0.990967619419098}
2025-01-15 19:00:21,619 [INFO] Step[1400/2713]: training loss : 0.9550467813014984 TRAIN  loss dict:  {'classification_loss': 0.9550467813014984}
2025-01-15 19:00:33,542 [INFO] Step[1450/2713]: training loss : 0.9361414515972137 TRAIN  loss dict:  {'classification_loss': 0.9361414515972137}
2025-01-15 19:00:45,478 [INFO] Step[1500/2713]: training loss : 0.9350666391849518 TRAIN  loss dict:  {'classification_loss': 0.9350666391849518}
2025-01-15 19:00:57,413 [INFO] Step[1550/2713]: training loss : 0.9584456384181976 TRAIN  loss dict:  {'classification_loss': 0.9584456384181976}
2025-01-15 19:01:09,358 [INFO] Step[1600/2713]: training loss : 0.9346640992164612 TRAIN  loss dict:  {'classification_loss': 0.9346640992164612}
2025-01-15 19:01:21,273 [INFO] Step[1650/2713]: training loss : 0.9769386839866638 TRAIN  loss dict:  {'classification_loss': 0.9769386839866638}
2025-01-15 19:01:33,178 [INFO] Step[1700/2713]: training loss : 0.9326311504840851 TRAIN  loss dict:  {'classification_loss': 0.9326311504840851}
2025-01-15 19:01:45,136 [INFO] Step[1750/2713]: training loss : 0.9366102480888366 TRAIN  loss dict:  {'classification_loss': 0.9366102480888366}
2025-01-15 19:01:57,039 [INFO] Step[1800/2713]: training loss : 0.967542736530304 TRAIN  loss dict:  {'classification_loss': 0.967542736530304}
2025-01-15 19:02:08,941 [INFO] Step[1850/2713]: training loss : 0.9732733631134033 TRAIN  loss dict:  {'classification_loss': 0.9732733631134033}
2025-01-15 19:02:20,837 [INFO] Step[1900/2713]: training loss : 0.9643620908260345 TRAIN  loss dict:  {'classification_loss': 0.9643620908260345}
2025-01-15 19:02:32,743 [INFO] Step[1950/2713]: training loss : 0.934119598865509 TRAIN  loss dict:  {'classification_loss': 0.934119598865509}
2025-01-15 19:02:44,633 [INFO] Step[2000/2713]: training loss : 0.9339623856544494 TRAIN  loss dict:  {'classification_loss': 0.9339623856544494}
2025-01-15 19:02:56,575 [INFO] Step[2050/2713]: training loss : 0.9307978773117065 TRAIN  loss dict:  {'classification_loss': 0.9307978773117065}
2025-01-15 19:03:08,483 [INFO] Step[2100/2713]: training loss : 0.9859163224697113 TRAIN  loss dict:  {'classification_loss': 0.9859163224697113}
2025-01-15 19:03:20,390 [INFO] Step[2150/2713]: training loss : 0.9329346323013306 TRAIN  loss dict:  {'classification_loss': 0.9329346323013306}
2025-01-15 19:03:32,340 [INFO] Step[2200/2713]: training loss : 0.9351680946350097 TRAIN  loss dict:  {'classification_loss': 0.9351680946350097}
2025-01-15 19:03:44,261 [INFO] Step[2250/2713]: training loss : 0.9335540604591369 TRAIN  loss dict:  {'classification_loss': 0.9335540604591369}
2025-01-15 19:03:56,210 [INFO] Step[2300/2713]: training loss : 0.9713796126842499 TRAIN  loss dict:  {'classification_loss': 0.9713796126842499}
2025-01-15 19:04:08,161 [INFO] Step[2350/2713]: training loss : 0.9437220585346222 TRAIN  loss dict:  {'classification_loss': 0.9437220585346222}
2025-01-15 19:04:20,111 [INFO] Step[2400/2713]: training loss : 0.9730644750595093 TRAIN  loss dict:  {'classification_loss': 0.9730644750595093}
2025-01-15 19:04:32,016 [INFO] Step[2450/2713]: training loss : 0.9328024351596832 TRAIN  loss dict:  {'classification_loss': 0.9328024351596832}
2025-01-15 19:04:43,956 [INFO] Step[2500/2713]: training loss : 0.9450581908226013 TRAIN  loss dict:  {'classification_loss': 0.9450581908226013}
2025-01-15 19:04:55,855 [INFO] Step[2550/2713]: training loss : 0.9373626327514648 TRAIN  loss dict:  {'classification_loss': 0.9373626327514648}
2025-01-15 19:05:07,755 [INFO] Step[2600/2713]: training loss : 0.9535054445266724 TRAIN  loss dict:  {'classification_loss': 0.9535054445266724}
2025-01-15 19:05:19,704 [INFO] Step[2650/2713]: training loss : 0.9709048080444336 TRAIN  loss dict:  {'classification_loss': 0.9709048080444336}
2025-01-15 19:05:31,596 [INFO] Step[2700/2713]: training loss : 0.9328986597061157 TRAIN  loss dict:  {'classification_loss': 0.9328986597061157}
2025-01-15 19:06:44,767 [INFO] Label accuracies statistics:
2025-01-15 19:06:44,768 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 1.0, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 0.75, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.5, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.5, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.25, 353: 0.5, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.25, 396: 1.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 19:07:35,528 [INFO] [21] TRAIN  loss: 0.9555569322971326 acc: 0.993119547856002
2025-01-15 19:07:35,528 [INFO] [21] TRAIN  loss dict: {'classification_loss': 0.9555569322971326}
2025-01-15 19:07:35,528 [INFO] [21] VALIDATION loss: 1.9022077715262435 VALIDATION acc: 0.7849529780564264
2025-01-15 19:07:35,528 [INFO] [21] VALIDATION loss dict: {'classification_loss': 1.9022077715262435}
2025-01-15 19:07:35,528 [INFO] 
2025-01-15 19:07:52,309 [INFO] Step[50/2713]: training loss : 0.9798265278339386 TRAIN  loss dict:  {'classification_loss': 0.9798265278339386}
2025-01-15 19:08:04,188 [INFO] Step[100/2713]: training loss : 0.9447071027755737 TRAIN  loss dict:  {'classification_loss': 0.9447071027755737}
2025-01-15 19:08:16,051 [INFO] Step[150/2713]: training loss : 0.9329236257076263 TRAIN  loss dict:  {'classification_loss': 0.9329236257076263}
2025-01-15 19:08:27,919 [INFO] Step[200/2713]: training loss : 0.9307134711742401 TRAIN  loss dict:  {'classification_loss': 0.9307134711742401}
2025-01-15 19:08:39,843 [INFO] Step[250/2713]: training loss : 0.9345115613937378 TRAIN  loss dict:  {'classification_loss': 0.9345115613937378}
2025-01-15 19:08:51,783 [INFO] Step[300/2713]: training loss : 0.9783634841442108 TRAIN  loss dict:  {'classification_loss': 0.9783634841442108}
2025-01-15 19:09:03,718 [INFO] Step[350/2713]: training loss : 0.9521145737171173 TRAIN  loss dict:  {'classification_loss': 0.9521145737171173}
2025-01-15 19:09:15,631 [INFO] Step[400/2713]: training loss : 0.9318081331253052 TRAIN  loss dict:  {'classification_loss': 0.9318081331253052}
2025-01-15 19:09:27,555 [INFO] Step[450/2713]: training loss : 0.9515845382213592 TRAIN  loss dict:  {'classification_loss': 0.9515845382213592}
2025-01-15 19:09:39,494 [INFO] Step[500/2713]: training loss : 0.9403661346435547 TRAIN  loss dict:  {'classification_loss': 0.9403661346435547}
2025-01-15 19:09:51,423 [INFO] Step[550/2713]: training loss : 0.9325354087352753 TRAIN  loss dict:  {'classification_loss': 0.9325354087352753}
2025-01-15 19:10:03,375 [INFO] Step[600/2713]: training loss : 1.0042145884037017 TRAIN  loss dict:  {'classification_loss': 1.0042145884037017}
2025-01-15 19:10:15,285 [INFO] Step[650/2713]: training loss : 0.9928313648700714 TRAIN  loss dict:  {'classification_loss': 0.9928313648700714}
2025-01-15 19:10:27,204 [INFO] Step[700/2713]: training loss : 0.9455911207199097 TRAIN  loss dict:  {'classification_loss': 0.9455911207199097}
2025-01-15 19:10:39,136 [INFO] Step[750/2713]: training loss : 0.933928986787796 TRAIN  loss dict:  {'classification_loss': 0.933928986787796}
2025-01-15 19:10:51,050 [INFO] Step[800/2713]: training loss : 0.9303334045410157 TRAIN  loss dict:  {'classification_loss': 0.9303334045410157}
2025-01-15 19:11:02,981 [INFO] Step[850/2713]: training loss : 0.943579397201538 TRAIN  loss dict:  {'classification_loss': 0.943579397201538}
2025-01-15 19:11:14,892 [INFO] Step[900/2713]: training loss : 0.9382834839820862 TRAIN  loss dict:  {'classification_loss': 0.9382834839820862}
2025-01-15 19:11:26,828 [INFO] Step[950/2713]: training loss : 0.9379911196231842 TRAIN  loss dict:  {'classification_loss': 0.9379911196231842}
2025-01-15 19:11:38,764 [INFO] Step[1000/2713]: training loss : 0.9338609480857849 TRAIN  loss dict:  {'classification_loss': 0.9338609480857849}
2025-01-15 19:11:50,680 [INFO] Step[1050/2713]: training loss : 0.936387311220169 TRAIN  loss dict:  {'classification_loss': 0.936387311220169}
2025-01-15 19:12:02,589 [INFO] Step[1100/2713]: training loss : 0.9559914517402649 TRAIN  loss dict:  {'classification_loss': 0.9559914517402649}
2025-01-15 19:12:14,531 [INFO] Step[1150/2713]: training loss : 0.9745834529399872 TRAIN  loss dict:  {'classification_loss': 0.9745834529399872}
2025-01-15 19:12:26,447 [INFO] Step[1200/2713]: training loss : 0.9698264086246491 TRAIN  loss dict:  {'classification_loss': 0.9698264086246491}
2025-01-15 19:12:38,374 [INFO] Step[1250/2713]: training loss : 0.9472617495059967 TRAIN  loss dict:  {'classification_loss': 0.9472617495059967}
2025-01-15 19:12:50,309 [INFO] Step[1300/2713]: training loss : 0.9325590419769287 TRAIN  loss dict:  {'classification_loss': 0.9325590419769287}
2025-01-15 19:13:02,252 [INFO] Step[1350/2713]: training loss : 0.9599012553691864 TRAIN  loss dict:  {'classification_loss': 0.9599012553691864}
2025-01-15 19:13:14,205 [INFO] Step[1400/2713]: training loss : 0.9868730163574219 TRAIN  loss dict:  {'classification_loss': 0.9868730163574219}
2025-01-15 19:13:26,159 [INFO] Step[1450/2713]: training loss : 0.9374151146411895 TRAIN  loss dict:  {'classification_loss': 0.9374151146411895}
2025-01-15 19:13:38,076 [INFO] Step[1500/2713]: training loss : 0.9429042887687683 TRAIN  loss dict:  {'classification_loss': 0.9429042887687683}
2025-01-15 19:13:50,030 [INFO] Step[1550/2713]: training loss : 0.930924619436264 TRAIN  loss dict:  {'classification_loss': 0.930924619436264}
2025-01-15 19:14:01,941 [INFO] Step[1600/2713]: training loss : 0.9914532804489136 TRAIN  loss dict:  {'classification_loss': 0.9914532804489136}
2025-01-15 19:14:13,882 [INFO] Step[1650/2713]: training loss : 0.968148581981659 TRAIN  loss dict:  {'classification_loss': 0.968148581981659}
2025-01-15 19:14:25,801 [INFO] Step[1700/2713]: training loss : 0.936069769859314 TRAIN  loss dict:  {'classification_loss': 0.936069769859314}
2025-01-15 19:14:37,723 [INFO] Step[1750/2713]: training loss : 0.9321195912361145 TRAIN  loss dict:  {'classification_loss': 0.9321195912361145}
2025-01-15 19:14:49,649 [INFO] Step[1800/2713]: training loss : 0.9350614786148072 TRAIN  loss dict:  {'classification_loss': 0.9350614786148072}
2025-01-15 19:15:01,566 [INFO] Step[1850/2713]: training loss : 0.9696204483509063 TRAIN  loss dict:  {'classification_loss': 0.9696204483509063}
2025-01-15 19:15:13,487 [INFO] Step[1900/2713]: training loss : 0.9508493316173553 TRAIN  loss dict:  {'classification_loss': 0.9508493316173553}
2025-01-15 19:15:25,469 [INFO] Step[1950/2713]: training loss : 0.9325088429450988 TRAIN  loss dict:  {'classification_loss': 0.9325088429450988}
2025-01-15 19:15:37,381 [INFO] Step[2000/2713]: training loss : 0.9420211267471313 TRAIN  loss dict:  {'classification_loss': 0.9420211267471313}
2025-01-15 19:15:49,299 [INFO] Step[2050/2713]: training loss : 0.9429334425926208 TRAIN  loss dict:  {'classification_loss': 0.9429334425926208}
2025-01-15 19:16:01,285 [INFO] Step[2100/2713]: training loss : 0.9517481184005737 TRAIN  loss dict:  {'classification_loss': 0.9517481184005737}
2025-01-15 19:16:13,243 [INFO] Step[2150/2713]: training loss : 0.969802223443985 TRAIN  loss dict:  {'classification_loss': 0.969802223443985}
2025-01-15 19:16:25,141 [INFO] Step[2200/2713]: training loss : 0.9324105870723725 TRAIN  loss dict:  {'classification_loss': 0.9324105870723725}
2025-01-15 19:16:37,070 [INFO] Step[2250/2713]: training loss : 0.946316579580307 TRAIN  loss dict:  {'classification_loss': 0.946316579580307}
2025-01-15 19:16:49,000 [INFO] Step[2300/2713]: training loss : 0.9528239119052887 TRAIN  loss dict:  {'classification_loss': 0.9528239119052887}
2025-01-15 19:17:00,928 [INFO] Step[2350/2713]: training loss : 0.9640135955810547 TRAIN  loss dict:  {'classification_loss': 0.9640135955810547}
2025-01-15 19:17:12,816 [INFO] Step[2400/2713]: training loss : 0.9340635454654693 TRAIN  loss dict:  {'classification_loss': 0.9340635454654693}
2025-01-15 19:17:24,765 [INFO] Step[2450/2713]: training loss : 0.931959697008133 TRAIN  loss dict:  {'classification_loss': 0.931959697008133}
2025-01-15 19:17:36,696 [INFO] Step[2500/2713]: training loss : 0.9583743858337402 TRAIN  loss dict:  {'classification_loss': 0.9583743858337402}
2025-01-15 19:17:48,627 [INFO] Step[2550/2713]: training loss : 0.9339377760887146 TRAIN  loss dict:  {'classification_loss': 0.9339377760887146}
2025-01-15 19:18:00,581 [INFO] Step[2600/2713]: training loss : 0.9667490804195404 TRAIN  loss dict:  {'classification_loss': 0.9667490804195404}
2025-01-15 19:18:12,477 [INFO] Step[2650/2713]: training loss : 0.932369065284729 TRAIN  loss dict:  {'classification_loss': 0.932369065284729}
2025-01-15 19:18:24,335 [INFO] Step[2700/2713]: training loss : 0.9327162861824035 TRAIN  loss dict:  {'classification_loss': 0.9327162861824035}
2025-01-15 19:19:36,695 [INFO] Label accuracies statistics:
2025-01-15 19:19:36,695 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.0, 55: 1.0, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.25, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.5, 245: 0.5, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.5, 262: 0.5, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.25, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.5, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.5, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 0.75, 333: 0.75, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.5, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 19:19:36,697 [INFO] [22] TRAIN  loss: 0.949027064450324 acc: 0.9948396608920015
2025-01-15 19:19:36,697 [INFO] [22] TRAIN  loss dict: {'classification_loss': 0.949027064450324}
2025-01-15 19:19:36,697 [INFO] [22] VALIDATION loss: 2.006233342944231 VALIDATION acc: 0.7730407523510971
2025-01-15 19:19:36,697 [INFO] [22] VALIDATION loss dict: {'classification_loss': 2.006233342944231}
2025-01-15 19:19:36,697 [INFO] 
2025-01-15 19:19:53,652 [INFO] Step[50/2713]: training loss : 0.9342986750602722 TRAIN  loss dict:  {'classification_loss': 0.9342986750602722}
2025-01-15 19:20:05,543 [INFO] Step[100/2713]: training loss : 0.9391838836669922 TRAIN  loss dict:  {'classification_loss': 0.9391838836669922}
2025-01-15 19:20:17,455 [INFO] Step[150/2713]: training loss : 0.9557232773303985 TRAIN  loss dict:  {'classification_loss': 0.9557232773303985}
2025-01-15 19:20:29,366 [INFO] Step[200/2713]: training loss : 0.9661752200126648 TRAIN  loss dict:  {'classification_loss': 0.9661752200126648}
2025-01-15 19:20:41,305 [INFO] Step[250/2713]: training loss : 0.9837573695182801 TRAIN  loss dict:  {'classification_loss': 0.9837573695182801}
2025-01-15 19:20:53,227 [INFO] Step[300/2713]: training loss : 0.9481957828998566 TRAIN  loss dict:  {'classification_loss': 0.9481957828998566}
2025-01-15 19:21:05,208 [INFO] Step[350/2713]: training loss : 0.9348298525810241 TRAIN  loss dict:  {'classification_loss': 0.9348298525810241}
2025-01-15 19:21:17,132 [INFO] Step[400/2713]: training loss : 0.9315167152881623 TRAIN  loss dict:  {'classification_loss': 0.9315167152881623}
2025-01-15 19:21:29,048 [INFO] Step[450/2713]: training loss : 0.9297739493846894 TRAIN  loss dict:  {'classification_loss': 0.9297739493846894}
2025-01-15 19:21:40,991 [INFO] Step[500/2713]: training loss : 0.9337525784969329 TRAIN  loss dict:  {'classification_loss': 0.9337525784969329}
2025-01-15 19:21:52,953 [INFO] Step[550/2713]: training loss : 0.9487061023712158 TRAIN  loss dict:  {'classification_loss': 0.9487061023712158}
2025-01-15 19:22:04,856 [INFO] Step[600/2713]: training loss : 0.9312213850021362 TRAIN  loss dict:  {'classification_loss': 0.9312213850021362}
2025-01-15 19:22:16,805 [INFO] Step[650/2713]: training loss : 0.9408424723148346 TRAIN  loss dict:  {'classification_loss': 0.9408424723148346}
2025-01-15 19:22:28,760 [INFO] Step[700/2713]: training loss : 0.937496018409729 TRAIN  loss dict:  {'classification_loss': 0.937496018409729}
2025-01-15 19:22:40,692 [INFO] Step[750/2713]: training loss : 0.9709027671813965 TRAIN  loss dict:  {'classification_loss': 0.9709027671813965}
2025-01-15 19:22:52,613 [INFO] Step[800/2713]: training loss : 0.9318930649757385 TRAIN  loss dict:  {'classification_loss': 0.9318930649757385}
2025-01-15 19:23:04,545 [INFO] Step[850/2713]: training loss : 0.9392542910575866 TRAIN  loss dict:  {'classification_loss': 0.9392542910575866}
2025-01-15 19:23:16,492 [INFO] Step[900/2713]: training loss : 0.9313861763477326 TRAIN  loss dict:  {'classification_loss': 0.9313861763477326}
2025-01-15 19:23:28,395 [INFO] Step[950/2713]: training loss : 0.9384021711349487 TRAIN  loss dict:  {'classification_loss': 0.9384021711349487}
2025-01-15 19:23:40,331 [INFO] Step[1000/2713]: training loss : 0.9310762774944306 TRAIN  loss dict:  {'classification_loss': 0.9310762774944306}
2025-01-15 19:23:52,255 [INFO] Step[1050/2713]: training loss : 0.9421632802486419 TRAIN  loss dict:  {'classification_loss': 0.9421632802486419}
2025-01-15 19:24:04,181 [INFO] Step[1100/2713]: training loss : 0.9314130699634552 TRAIN  loss dict:  {'classification_loss': 0.9314130699634552}
2025-01-15 19:24:16,137 [INFO] Step[1150/2713]: training loss : 0.9438926684856415 TRAIN  loss dict:  {'classification_loss': 0.9438926684856415}
2025-01-15 19:24:28,079 [INFO] Step[1200/2713]: training loss : 0.9323610293865204 TRAIN  loss dict:  {'classification_loss': 0.9323610293865204}
2025-01-15 19:24:40,030 [INFO] Step[1250/2713]: training loss : 0.9322717559337615 TRAIN  loss dict:  {'classification_loss': 0.9322717559337615}
2025-01-15 19:24:51,983 [INFO] Step[1300/2713]: training loss : 0.9456587743759155 TRAIN  loss dict:  {'classification_loss': 0.9456587743759155}
2025-01-15 19:25:03,932 [INFO] Step[1350/2713]: training loss : 0.9393577480316162 TRAIN  loss dict:  {'classification_loss': 0.9393577480316162}
2025-01-15 19:25:15,848 [INFO] Step[1400/2713]: training loss : 0.9712437725067139 TRAIN  loss dict:  {'classification_loss': 0.9712437725067139}
2025-01-15 19:25:27,764 [INFO] Step[1450/2713]: training loss : 0.9388395190238953 TRAIN  loss dict:  {'classification_loss': 0.9388395190238953}
2025-01-15 19:25:39,648 [INFO] Step[1500/2713]: training loss : 0.9692440724372864 TRAIN  loss dict:  {'classification_loss': 0.9692440724372864}
2025-01-15 19:25:51,570 [INFO] Step[1550/2713]: training loss : 0.9319123995304107 TRAIN  loss dict:  {'classification_loss': 0.9319123995304107}
2025-01-15 19:26:03,501 [INFO] Step[1600/2713]: training loss : 0.9383991515636444 TRAIN  loss dict:  {'classification_loss': 0.9383991515636444}
2025-01-15 19:26:15,427 [INFO] Step[1650/2713]: training loss : 0.9333824980258941 TRAIN  loss dict:  {'classification_loss': 0.9333824980258941}
2025-01-15 19:26:27,338 [INFO] Step[1700/2713]: training loss : 0.9352033770084381 TRAIN  loss dict:  {'classification_loss': 0.9352033770084381}
2025-01-15 19:26:39,295 [INFO] Step[1750/2713]: training loss : 0.9357917785644532 TRAIN  loss dict:  {'classification_loss': 0.9357917785644532}
2025-01-15 19:26:51,203 [INFO] Step[1800/2713]: training loss : 0.939380019903183 TRAIN  loss dict:  {'classification_loss': 0.939380019903183}
2025-01-15 19:27:03,116 [INFO] Step[1850/2713]: training loss : 0.932271134853363 TRAIN  loss dict:  {'classification_loss': 0.932271134853363}
2025-01-15 19:27:15,038 [INFO] Step[1900/2713]: training loss : 1.0205074751377106 TRAIN  loss dict:  {'classification_loss': 1.0205074751377106}
2025-01-15 19:27:26,969 [INFO] Step[1950/2713]: training loss : 0.971895968914032 TRAIN  loss dict:  {'classification_loss': 0.971895968914032}
2025-01-15 19:27:38,873 [INFO] Step[2000/2713]: training loss : 0.9612236630916595 TRAIN  loss dict:  {'classification_loss': 0.9612236630916595}
2025-01-15 19:27:50,850 [INFO] Step[2050/2713]: training loss : 0.9367810833454132 TRAIN  loss dict:  {'classification_loss': 0.9367810833454132}
2025-01-15 19:28:02,764 [INFO] Step[2100/2713]: training loss : 0.9386198508739472 TRAIN  loss dict:  {'classification_loss': 0.9386198508739472}
2025-01-15 19:28:14,688 [INFO] Step[2150/2713]: training loss : 0.9385188698768616 TRAIN  loss dict:  {'classification_loss': 0.9385188698768616}
2025-01-15 19:28:26,574 [INFO] Step[2200/2713]: training loss : 0.972804400920868 TRAIN  loss dict:  {'classification_loss': 0.972804400920868}
2025-01-15 19:28:38,487 [INFO] Step[2250/2713]: training loss : 0.9941348993778228 TRAIN  loss dict:  {'classification_loss': 0.9941348993778228}
2025-01-15 19:28:50,389 [INFO] Step[2300/2713]: training loss : 0.9763781356811524 TRAIN  loss dict:  {'classification_loss': 0.9763781356811524}
2025-01-15 19:29:02,347 [INFO] Step[2350/2713]: training loss : 0.932569432258606 TRAIN  loss dict:  {'classification_loss': 0.932569432258606}
2025-01-15 19:29:14,265 [INFO] Step[2400/2713]: training loss : 0.9338512480258941 TRAIN  loss dict:  {'classification_loss': 0.9338512480258941}
2025-01-15 19:29:26,170 [INFO] Step[2450/2713]: training loss : 0.9841528916358948 TRAIN  loss dict:  {'classification_loss': 0.9841528916358948}
2025-01-15 19:29:38,063 [INFO] Step[2500/2713]: training loss : 0.9425847995281219 TRAIN  loss dict:  {'classification_loss': 0.9425847995281219}
2025-01-15 19:29:50,042 [INFO] Step[2550/2713]: training loss : 0.9502277147769927 TRAIN  loss dict:  {'classification_loss': 0.9502277147769927}
2025-01-15 19:30:01,955 [INFO] Step[2600/2713]: training loss : 1.0117631018161775 TRAIN  loss dict:  {'classification_loss': 1.0117631018161775}
2025-01-15 19:30:13,887 [INFO] Step[2650/2713]: training loss : 0.9703589498996734 TRAIN  loss dict:  {'classification_loss': 0.9703589498996734}
2025-01-15 19:30:25,910 [INFO] Step[2700/2713]: training loss : 0.9342419707775116 TRAIN  loss dict:  {'classification_loss': 0.9342419707775116}
2025-01-15 19:31:38,512 [INFO] Label accuracies statistics:
2025-01-15 19:31:38,512 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 0.75, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.0, 55: 1.0, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.5, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.5, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.25, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.5, 167: 1.0, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.5, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.5, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.75, 244: 0.5, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.25, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 1.0, 303: 0.75, 304: 0.0, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.5, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 0.75, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.25, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.25, 395: 0.25, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 19:31:38,514 [INFO] [23] TRAIN  loss: 0.9485293661717599 acc: 0.9955768521931441
2025-01-15 19:31:38,514 [INFO] [23] TRAIN  loss dict: {'classification_loss': 0.9485293661717599}
2025-01-15 19:31:38,514 [INFO] [23] VALIDATION loss: 2.014744267988026 VALIDATION acc: 0.7636363636363637
2025-01-15 19:31:38,514 [INFO] [23] VALIDATION loss dict: {'classification_loss': 2.014744267988026}
2025-01-15 19:31:38,514 [INFO] 
2025-01-15 19:31:54,837 [INFO] Step[50/2713]: training loss : 0.9749698209762573 TRAIN  loss dict:  {'classification_loss': 0.9749698209762573}
2025-01-15 19:32:06,701 [INFO] Step[100/2713]: training loss : 0.9323122835159302 TRAIN  loss dict:  {'classification_loss': 0.9323122835159302}
2025-01-15 19:32:18,582 [INFO] Step[150/2713]: training loss : 0.938747318983078 TRAIN  loss dict:  {'classification_loss': 0.938747318983078}
2025-01-15 19:32:30,444 [INFO] Step[200/2713]: training loss : 0.9369969284534454 TRAIN  loss dict:  {'classification_loss': 0.9369969284534454}
2025-01-15 19:32:42,347 [INFO] Step[250/2713]: training loss : 0.9683151912689208 TRAIN  loss dict:  {'classification_loss': 0.9683151912689208}
2025-01-15 19:32:54,272 [INFO] Step[300/2713]: training loss : 0.9669565212726593 TRAIN  loss dict:  {'classification_loss': 0.9669565212726593}
2025-01-15 19:33:06,199 [INFO] Step[350/2713]: training loss : 0.9319925916194916 TRAIN  loss dict:  {'classification_loss': 0.9319925916194916}
2025-01-15 19:33:18,172 [INFO] Step[400/2713]: training loss : 0.9446886920928955 TRAIN  loss dict:  {'classification_loss': 0.9446886920928955}
2025-01-15 19:33:30,175 [INFO] Step[450/2713]: training loss : 0.9537072598934173 TRAIN  loss dict:  {'classification_loss': 0.9537072598934173}
2025-01-15 19:33:42,104 [INFO] Step[500/2713]: training loss : 0.9358550798892975 TRAIN  loss dict:  {'classification_loss': 0.9358550798892975}
2025-01-15 19:33:54,056 [INFO] Step[550/2713]: training loss : 0.9550151658058167 TRAIN  loss dict:  {'classification_loss': 0.9550151658058167}
2025-01-15 19:34:05,964 [INFO] Step[600/2713]: training loss : 0.9800692546367645 TRAIN  loss dict:  {'classification_loss': 0.9800692546367645}
2025-01-15 19:34:17,965 [INFO] Step[650/2713]: training loss : 0.9399068558216095 TRAIN  loss dict:  {'classification_loss': 0.9399068558216095}
2025-01-15 19:34:29,907 [INFO] Step[700/2713]: training loss : 0.9597430992126464 TRAIN  loss dict:  {'classification_loss': 0.9597430992126464}
2025-01-15 19:34:41,824 [INFO] Step[750/2713]: training loss : 0.9333849179744721 TRAIN  loss dict:  {'classification_loss': 0.9333849179744721}
2025-01-15 19:34:53,750 [INFO] Step[800/2713]: training loss : 0.9421558141708374 TRAIN  loss dict:  {'classification_loss': 0.9421558141708374}
2025-01-15 19:35:05,712 [INFO] Step[850/2713]: training loss : 0.9483879339694977 TRAIN  loss dict:  {'classification_loss': 0.9483879339694977}
2025-01-15 19:35:17,666 [INFO] Step[900/2713]: training loss : 0.9507354021072387 TRAIN  loss dict:  {'classification_loss': 0.9507354021072387}
2025-01-15 19:35:29,582 [INFO] Step[950/2713]: training loss : 0.9681372809410095 TRAIN  loss dict:  {'classification_loss': 0.9681372809410095}
2025-01-15 19:35:41,462 [INFO] Step[1000/2713]: training loss : 0.9323169243335724 TRAIN  loss dict:  {'classification_loss': 0.9323169243335724}
2025-01-15 19:35:53,419 [INFO] Step[1050/2713]: training loss : 0.973139476776123 TRAIN  loss dict:  {'classification_loss': 0.973139476776123}
2025-01-15 19:36:05,329 [INFO] Step[1100/2713]: training loss : 0.9391916871070862 TRAIN  loss dict:  {'classification_loss': 0.9391916871070862}
2025-01-15 19:36:17,268 [INFO] Step[1150/2713]: training loss : 0.9463812923431396 TRAIN  loss dict:  {'classification_loss': 0.9463812923431396}
2025-01-15 19:36:29,194 [INFO] Step[1200/2713]: training loss : 0.9544844436645508 TRAIN  loss dict:  {'classification_loss': 0.9544844436645508}
2025-01-15 19:36:41,181 [INFO] Step[1250/2713]: training loss : 0.9319780826568603 TRAIN  loss dict:  {'classification_loss': 0.9319780826568603}
2025-01-15 19:36:53,069 [INFO] Step[1300/2713]: training loss : 0.9477114129066467 TRAIN  loss dict:  {'classification_loss': 0.9477114129066467}
2025-01-15 19:37:05,006 [INFO] Step[1350/2713]: training loss : 0.9428218901157379 TRAIN  loss dict:  {'classification_loss': 0.9428218901157379}
2025-01-15 19:37:16,942 [INFO] Step[1400/2713]: training loss : 0.9412307667732239 TRAIN  loss dict:  {'classification_loss': 0.9412307667732239}
2025-01-15 19:37:28,881 [INFO] Step[1450/2713]: training loss : 0.9306414818763733 TRAIN  loss dict:  {'classification_loss': 0.9306414818763733}
2025-01-15 19:37:40,796 [INFO] Step[1500/2713]: training loss : 0.9316756546497345 TRAIN  loss dict:  {'classification_loss': 0.9316756546497345}
2025-01-15 19:37:52,713 [INFO] Step[1550/2713]: training loss : 0.9311301541328431 TRAIN  loss dict:  {'classification_loss': 0.9311301541328431}
2025-01-15 19:38:04,634 [INFO] Step[1600/2713]: training loss : 0.930935492515564 TRAIN  loss dict:  {'classification_loss': 0.930935492515564}
2025-01-15 19:38:16,547 [INFO] Step[1650/2713]: training loss : 0.9437342751026153 TRAIN  loss dict:  {'classification_loss': 0.9437342751026153}
2025-01-15 19:38:28,448 [INFO] Step[1700/2713]: training loss : 0.9386052930355072 TRAIN  loss dict:  {'classification_loss': 0.9386052930355072}
2025-01-15 19:38:40,392 [INFO] Step[1750/2713]: training loss : 0.9359289908409119 TRAIN  loss dict:  {'classification_loss': 0.9359289908409119}
2025-01-15 19:38:52,283 [INFO] Step[1800/2713]: training loss : 1.0005449044704438 TRAIN  loss dict:  {'classification_loss': 1.0005449044704438}
2025-01-15 19:39:04,240 [INFO] Step[1850/2713]: training loss : 0.9316460108757019 TRAIN  loss dict:  {'classification_loss': 0.9316460108757019}
2025-01-15 19:39:16,145 [INFO] Step[1900/2713]: training loss : 0.9739923119544983 TRAIN  loss dict:  {'classification_loss': 0.9739923119544983}
2025-01-15 19:39:28,061 [INFO] Step[1950/2713]: training loss : 0.9414175176620483 TRAIN  loss dict:  {'classification_loss': 0.9414175176620483}
2025-01-15 19:39:39,964 [INFO] Step[2000/2713]: training loss : 0.9534632062911987 TRAIN  loss dict:  {'classification_loss': 0.9534632062911987}
2025-01-15 19:39:51,910 [INFO] Step[2050/2713]: training loss : 0.9430985248088837 TRAIN  loss dict:  {'classification_loss': 0.9430985248088837}
2025-01-15 19:40:03,828 [INFO] Step[2100/2713]: training loss : 0.930659852027893 TRAIN  loss dict:  {'classification_loss': 0.930659852027893}
2025-01-15 19:40:15,757 [INFO] Step[2150/2713]: training loss : 0.9716480350494385 TRAIN  loss dict:  {'classification_loss': 0.9716480350494385}
2025-01-15 19:40:27,674 [INFO] Step[2200/2713]: training loss : 0.9404550051689148 TRAIN  loss dict:  {'classification_loss': 0.9404550051689148}
2025-01-15 19:40:39,609 [INFO] Step[2250/2713]: training loss : 0.9573541867733002 TRAIN  loss dict:  {'classification_loss': 0.9573541867733002}
2025-01-15 19:40:51,590 [INFO] Step[2300/2713]: training loss : 0.9665135395526886 TRAIN  loss dict:  {'classification_loss': 0.9665135395526886}
2025-01-15 19:41:03,511 [INFO] Step[2350/2713]: training loss : 0.9491456592082977 TRAIN  loss dict:  {'classification_loss': 0.9491456592082977}
2025-01-15 19:41:15,420 [INFO] Step[2400/2713]: training loss : 0.953343893289566 TRAIN  loss dict:  {'classification_loss': 0.953343893289566}
2025-01-15 19:41:27,330 [INFO] Step[2450/2713]: training loss : 0.9593617844581604 TRAIN  loss dict:  {'classification_loss': 0.9593617844581604}
2025-01-15 19:41:39,258 [INFO] Step[2500/2713]: training loss : 0.9718930721282959 TRAIN  loss dict:  {'classification_loss': 0.9718930721282959}
2025-01-15 19:41:51,173 [INFO] Step[2550/2713]: training loss : 0.9453436470031739 TRAIN  loss dict:  {'classification_loss': 0.9453436470031739}
2025-01-15 19:42:03,049 [INFO] Step[2600/2713]: training loss : 0.93216801404953 TRAIN  loss dict:  {'classification_loss': 0.93216801404953}
2025-01-15 19:42:14,980 [INFO] Step[2650/2713]: training loss : 0.9447660303115845 TRAIN  loss dict:  {'classification_loss': 0.9447660303115845}
2025-01-15 19:42:26,841 [INFO] Step[2700/2713]: training loss : 0.9300809824466705 TRAIN  loss dict:  {'classification_loss': 0.9300809824466705}
2025-01-15 19:43:39,632 [INFO] Label accuracies statistics:
2025-01-15 19:43:39,632 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 0.75, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.0, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.5, 170: 0.75, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 0.25, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.25, 187: 1.0, 188: 1.0, 189: 0.0, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.0, 355: 0.75, 356: 0.75, 357: 0.75, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 19:43:39,634 [INFO] [24] TRAIN  loss: 0.948265886838437 acc: 0.9949625261088586
2025-01-15 19:43:39,634 [INFO] [24] TRAIN  loss dict: {'classification_loss': 0.948265886838437}
2025-01-15 19:43:39,634 [INFO] [24] VALIDATION loss: 1.995076086288108 VALIDATION acc: 0.7705329153605016
2025-01-15 19:43:39,634 [INFO] [24] VALIDATION loss dict: {'classification_loss': 1.995076086288108}
2025-01-15 19:43:39,635 [INFO] 
2025-01-15 19:43:56,373 [INFO] Step[50/2713]: training loss : 0.9616088032722473 TRAIN  loss dict:  {'classification_loss': 0.9616088032722473}
2025-01-15 19:44:08,218 [INFO] Step[100/2713]: training loss : 0.9577171421051025 TRAIN  loss dict:  {'classification_loss': 0.9577171421051025}
2025-01-15 19:44:20,105 [INFO] Step[150/2713]: training loss : 0.931428256034851 TRAIN  loss dict:  {'classification_loss': 0.931428256034851}
2025-01-15 19:44:31,986 [INFO] Step[200/2713]: training loss : 0.9502567231655121 TRAIN  loss dict:  {'classification_loss': 0.9502567231655121}
2025-01-15 19:44:43,912 [INFO] Step[250/2713]: training loss : 0.9347202014923096 TRAIN  loss dict:  {'classification_loss': 0.9347202014923096}
2025-01-15 19:44:55,844 [INFO] Step[300/2713]: training loss : 0.9369438028335572 TRAIN  loss dict:  {'classification_loss': 0.9369438028335572}
2025-01-15 19:45:07,759 [INFO] Step[350/2713]: training loss : 0.9438529944419861 TRAIN  loss dict:  {'classification_loss': 0.9438529944419861}
2025-01-15 19:45:19,720 [INFO] Step[400/2713]: training loss : 0.9326945114135742 TRAIN  loss dict:  {'classification_loss': 0.9326945114135742}
2025-01-15 19:45:31,688 [INFO] Step[450/2713]: training loss : 0.9743628442287445 TRAIN  loss dict:  {'classification_loss': 0.9743628442287445}
2025-01-15 19:45:43,611 [INFO] Step[500/2713]: training loss : 0.9386445808410645 TRAIN  loss dict:  {'classification_loss': 0.9386445808410645}
2025-01-15 19:45:55,529 [INFO] Step[550/2713]: training loss : 1.0081989288330078 TRAIN  loss dict:  {'classification_loss': 1.0081989288330078}
2025-01-15 19:46:07,497 [INFO] Step[600/2713]: training loss : 0.976396211385727 TRAIN  loss dict:  {'classification_loss': 0.976396211385727}
2025-01-15 19:46:19,452 [INFO] Step[650/2713]: training loss : 0.9472863936424255 TRAIN  loss dict:  {'classification_loss': 0.9472863936424255}
2025-01-15 19:46:31,364 [INFO] Step[700/2713]: training loss : 0.967774133682251 TRAIN  loss dict:  {'classification_loss': 0.967774133682251}
2025-01-15 19:46:43,302 [INFO] Step[750/2713]: training loss : 0.9479995739459991 TRAIN  loss dict:  {'classification_loss': 0.9479995739459991}
2025-01-15 19:46:55,209 [INFO] Step[800/2713]: training loss : 0.9312726891040802 TRAIN  loss dict:  {'classification_loss': 0.9312726891040802}
2025-01-15 19:47:07,135 [INFO] Step[850/2713]: training loss : 0.9548397159576416 TRAIN  loss dict:  {'classification_loss': 0.9548397159576416}
2025-01-15 19:47:19,057 [INFO] Step[900/2713]: training loss : 0.9685142683982849 TRAIN  loss dict:  {'classification_loss': 0.9685142683982849}
2025-01-15 19:47:31,004 [INFO] Step[950/2713]: training loss : 0.937589579820633 TRAIN  loss dict:  {'classification_loss': 0.937589579820633}
2025-01-15 19:47:42,925 [INFO] Step[1000/2713]: training loss : 1.0297844564914704 TRAIN  loss dict:  {'classification_loss': 1.0297844564914704}
2025-01-15 19:47:54,823 [INFO] Step[1050/2713]: training loss : 0.9651715409755707 TRAIN  loss dict:  {'classification_loss': 0.9651715409755707}
2025-01-15 19:48:06,756 [INFO] Step[1100/2713]: training loss : 0.9309617221355438 TRAIN  loss dict:  {'classification_loss': 0.9309617221355438}
2025-01-15 19:48:18,685 [INFO] Step[1150/2713]: training loss : 0.9310930109024048 TRAIN  loss dict:  {'classification_loss': 0.9310930109024048}
2025-01-15 19:48:30,599 [INFO] Step[1200/2713]: training loss : 0.9755100309848785 TRAIN  loss dict:  {'classification_loss': 0.9755100309848785}
2025-01-15 19:48:42,513 [INFO] Step[1250/2713]: training loss : 0.9950040555000306 TRAIN  loss dict:  {'classification_loss': 0.9950040555000306}
2025-01-15 19:48:54,431 [INFO] Step[1300/2713]: training loss : 0.9656066608428955 TRAIN  loss dict:  {'classification_loss': 0.9656066608428955}
2025-01-15 19:49:06,389 [INFO] Step[1350/2713]: training loss : 0.9502732527256011 TRAIN  loss dict:  {'classification_loss': 0.9502732527256011}
2025-01-15 19:49:18,290 [INFO] Step[1400/2713]: training loss : 0.9582042622566224 TRAIN  loss dict:  {'classification_loss': 0.9582042622566224}
2025-01-15 19:49:30,197 [INFO] Step[1450/2713]: training loss : 0.9335707437992096 TRAIN  loss dict:  {'classification_loss': 0.9335707437992096}
2025-01-15 19:49:42,104 [INFO] Step[1500/2713]: training loss : 0.9392803120613098 TRAIN  loss dict:  {'classification_loss': 0.9392803120613098}
2025-01-15 19:49:54,062 [INFO] Step[1550/2713]: training loss : 0.9645481789112091 TRAIN  loss dict:  {'classification_loss': 0.9645481789112091}
2025-01-15 19:50:05,977 [INFO] Step[1600/2713]: training loss : 0.935959438085556 TRAIN  loss dict:  {'classification_loss': 0.935959438085556}
2025-01-15 19:50:17,914 [INFO] Step[1650/2713]: training loss : 0.9404487991333008 TRAIN  loss dict:  {'classification_loss': 0.9404487991333008}
2025-01-15 19:50:29,840 [INFO] Step[1700/2713]: training loss : 0.9439830565452576 TRAIN  loss dict:  {'classification_loss': 0.9439830565452576}
2025-01-15 19:50:41,816 [INFO] Step[1750/2713]: training loss : 0.9377926850318908 TRAIN  loss dict:  {'classification_loss': 0.9377926850318908}
2025-01-15 19:50:53,744 [INFO] Step[1800/2713]: training loss : 1.0008306205272675 TRAIN  loss dict:  {'classification_loss': 1.0008306205272675}
2025-01-15 19:51:05,673 [INFO] Step[1850/2713]: training loss : 0.9414115190505982 TRAIN  loss dict:  {'classification_loss': 0.9414115190505982}
2025-01-15 19:51:17,561 [INFO] Step[1900/2713]: training loss : 0.962417311668396 TRAIN  loss dict:  {'classification_loss': 0.962417311668396}
2025-01-15 19:51:29,441 [INFO] Step[1950/2713]: training loss : 0.9368702208995819 TRAIN  loss dict:  {'classification_loss': 0.9368702208995819}
2025-01-15 19:51:41,368 [INFO] Step[2000/2713]: training loss : 0.9311413061618805 TRAIN  loss dict:  {'classification_loss': 0.9311413061618805}
2025-01-15 19:51:53,273 [INFO] Step[2050/2713]: training loss : 0.9456372582912445 TRAIN  loss dict:  {'classification_loss': 0.9456372582912445}
2025-01-15 19:52:05,206 [INFO] Step[2100/2713]: training loss : 0.9380416965484619 TRAIN  loss dict:  {'classification_loss': 0.9380416965484619}
2025-01-15 19:52:17,135 [INFO] Step[2150/2713]: training loss : 0.9980951023101806 TRAIN  loss dict:  {'classification_loss': 0.9980951023101806}
2025-01-15 19:52:29,111 [INFO] Step[2200/2713]: training loss : 0.9945887696743011 TRAIN  loss dict:  {'classification_loss': 0.9945887696743011}
2025-01-15 19:52:41,078 [INFO] Step[2250/2713]: training loss : 0.9344362795352936 TRAIN  loss dict:  {'classification_loss': 0.9344362795352936}
2025-01-15 19:52:53,021 [INFO] Step[2300/2713]: training loss : 0.9316360485553742 TRAIN  loss dict:  {'classification_loss': 0.9316360485553742}
2025-01-15 19:53:04,930 [INFO] Step[2350/2713]: training loss : 0.9809893083572387 TRAIN  loss dict:  {'classification_loss': 0.9809893083572387}
2025-01-15 19:53:16,879 [INFO] Step[2400/2713]: training loss : 0.9414175522327423 TRAIN  loss dict:  {'classification_loss': 0.9414175522327423}
2025-01-15 19:53:28,804 [INFO] Step[2450/2713]: training loss : 0.9644265472888947 TRAIN  loss dict:  {'classification_loss': 0.9644265472888947}
2025-01-15 19:53:40,712 [INFO] Step[2500/2713]: training loss : 0.9328283882141113 TRAIN  loss dict:  {'classification_loss': 0.9328283882141113}
2025-01-15 19:53:52,651 [INFO] Step[2550/2713]: training loss : 0.9382027995586395 TRAIN  loss dict:  {'classification_loss': 0.9382027995586395}
2025-01-15 19:54:04,564 [INFO] Step[2600/2713]: training loss : 0.9472554588317871 TRAIN  loss dict:  {'classification_loss': 0.9472554588317871}
2025-01-15 19:54:16,475 [INFO] Step[2650/2713]: training loss : 0.9313189125061035 TRAIN  loss dict:  {'classification_loss': 0.9313189125061035}
2025-01-15 19:54:28,420 [INFO] Step[2700/2713]: training loss : 0.9317146706581115 TRAIN  loss dict:  {'classification_loss': 0.9317146706581115}
2025-01-15 19:55:40,727 [INFO] Label accuracies statistics:
2025-01-15 19:55:40,727 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 0.75, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 1.0, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 0.75, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.5, 142: 1.0, 143: 0.75, 144: 1.0, 145: 0.5, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.75, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.5, 166: 0.5, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.25, 205: 0.75, 206: 0.75, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.3333333333333333, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.25, 268: 0.75, 269: 0.5, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.25, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.25, 326: 1.0, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 1.0, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.5, 369: 0.75, 370: 0.25, 371: 0.75, 372: 0.5, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.25, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 19:55:40,729 [INFO] [25] TRAIN  loss: 0.9532695416370706 acc: 0.9934881435065733
2025-01-15 19:55:40,729 [INFO] [25] TRAIN  loss dict: {'classification_loss': 0.9532695416370706}
2025-01-15 19:55:40,729 [INFO] [25] VALIDATION loss: 2.0926552269243657 VALIDATION acc: 0.7567398119122257
2025-01-15 19:55:40,729 [INFO] [25] VALIDATION loss dict: {'classification_loss': 2.0926552269243657}
2025-01-15 19:55:40,729 [INFO] 
2025-01-15 19:55:57,251 [INFO] Step[50/2713]: training loss : 0.9320839941501617 TRAIN  loss dict:  {'classification_loss': 0.9320839941501617}
2025-01-15 19:56:09,135 [INFO] Step[100/2713]: training loss : 0.9304920017719269 TRAIN  loss dict:  {'classification_loss': 0.9304920017719269}
2025-01-15 19:56:21,043 [INFO] Step[150/2713]: training loss : 0.9367767179012298 TRAIN  loss dict:  {'classification_loss': 0.9367767179012298}
2025-01-15 19:56:32,929 [INFO] Step[200/2713]: training loss : 0.9316878628730774 TRAIN  loss dict:  {'classification_loss': 0.9316878628730774}
2025-01-15 19:56:44,860 [INFO] Step[250/2713]: training loss : 0.9803551805019378 TRAIN  loss dict:  {'classification_loss': 0.9803551805019378}
2025-01-15 19:56:56,736 [INFO] Step[300/2713]: training loss : 0.9788237822055816 TRAIN  loss dict:  {'classification_loss': 0.9788237822055816}
2025-01-15 19:57:08,638 [INFO] Step[350/2713]: training loss : 0.9614518463611603 TRAIN  loss dict:  {'classification_loss': 0.9614518463611603}
2025-01-15 19:57:20,591 [INFO] Step[400/2713]: training loss : 0.9749859499931336 TRAIN  loss dict:  {'classification_loss': 0.9749859499931336}
2025-01-15 19:57:32,555 [INFO] Step[450/2713]: training loss : 0.9585124659538269 TRAIN  loss dict:  {'classification_loss': 0.9585124659538269}
2025-01-15 19:57:44,480 [INFO] Step[500/2713]: training loss : 0.9561889326572418 TRAIN  loss dict:  {'classification_loss': 0.9561889326572418}
2025-01-15 19:57:56,432 [INFO] Step[550/2713]: training loss : 0.9597662937641144 TRAIN  loss dict:  {'classification_loss': 0.9597662937641144}
2025-01-15 19:58:08,371 [INFO] Step[600/2713]: training loss : 0.9347223520278931 TRAIN  loss dict:  {'classification_loss': 0.9347223520278931}
2025-01-15 19:58:20,291 [INFO] Step[650/2713]: training loss : 0.9758156514167786 TRAIN  loss dict:  {'classification_loss': 0.9758156514167786}
2025-01-15 19:58:32,243 [INFO] Step[700/2713]: training loss : 0.9425080823898315 TRAIN  loss dict:  {'classification_loss': 0.9425080823898315}
2025-01-15 19:58:44,166 [INFO] Step[750/2713]: training loss : 0.9339617741107941 TRAIN  loss dict:  {'classification_loss': 0.9339617741107941}
2025-01-15 19:58:56,098 [INFO] Step[800/2713]: training loss : 0.9807840299606323 TRAIN  loss dict:  {'classification_loss': 0.9807840299606323}
2025-01-15 19:59:08,108 [INFO] Step[850/2713]: training loss : 0.9653861224651337 TRAIN  loss dict:  {'classification_loss': 0.9653861224651337}
2025-01-15 19:59:20,035 [INFO] Step[900/2713]: training loss : 1.0310735785961151 TRAIN  loss dict:  {'classification_loss': 1.0310735785961151}
2025-01-15 19:59:31,994 [INFO] Step[950/2713]: training loss : 0.9717535710334778 TRAIN  loss dict:  {'classification_loss': 0.9717535710334778}
2025-01-15 19:59:43,917 [INFO] Step[1000/2713]: training loss : 0.9403850185871124 TRAIN  loss dict:  {'classification_loss': 0.9403850185871124}
2025-01-15 19:59:55,827 [INFO] Step[1050/2713]: training loss : 0.9682395386695862 TRAIN  loss dict:  {'classification_loss': 0.9682395386695862}
2025-01-15 20:00:07,716 [INFO] Step[1100/2713]: training loss : 0.9335575556755066 TRAIN  loss dict:  {'classification_loss': 0.9335575556755066}
2025-01-15 20:00:19,663 [INFO] Step[1150/2713]: training loss : 0.965001357793808 TRAIN  loss dict:  {'classification_loss': 0.965001357793808}
2025-01-15 20:00:31,577 [INFO] Step[1200/2713]: training loss : 0.9814324927330017 TRAIN  loss dict:  {'classification_loss': 0.9814324927330017}
2025-01-15 20:00:43,508 [INFO] Step[1250/2713]: training loss : 0.9431651484966278 TRAIN  loss dict:  {'classification_loss': 0.9431651484966278}
2025-01-15 20:00:55,435 [INFO] Step[1300/2713]: training loss : 0.9929256403446197 TRAIN  loss dict:  {'classification_loss': 0.9929256403446197}
2025-01-15 20:01:07,369 [INFO] Step[1350/2713]: training loss : 0.9671324729919434 TRAIN  loss dict:  {'classification_loss': 0.9671324729919434}
2025-01-15 20:01:19,307 [INFO] Step[1400/2713]: training loss : 0.9419500696659088 TRAIN  loss dict:  {'classification_loss': 0.9419500696659088}
2025-01-15 20:01:31,223 [INFO] Step[1450/2713]: training loss : 0.9584942722320556 TRAIN  loss dict:  {'classification_loss': 0.9584942722320556}
2025-01-15 20:01:43,157 [INFO] Step[1500/2713]: training loss : 0.9360984802246094 TRAIN  loss dict:  {'classification_loss': 0.9360984802246094}
2025-01-15 20:01:55,133 [INFO] Step[1550/2713]: training loss : 0.943610451221466 TRAIN  loss dict:  {'classification_loss': 0.943610451221466}
2025-01-15 20:02:07,031 [INFO] Step[1600/2713]: training loss : 0.9849050915241242 TRAIN  loss dict:  {'classification_loss': 0.9849050915241242}
2025-01-15 20:02:18,939 [INFO] Step[1650/2713]: training loss : 0.93467041015625 TRAIN  loss dict:  {'classification_loss': 0.93467041015625}
2025-01-15 20:02:30,882 [INFO] Step[1700/2713]: training loss : 0.9706981694698333 TRAIN  loss dict:  {'classification_loss': 0.9706981694698333}
2025-01-15 20:02:42,838 [INFO] Step[1750/2713]: training loss : 0.9587531781196594 TRAIN  loss dict:  {'classification_loss': 0.9587531781196594}
2025-01-15 20:02:54,792 [INFO] Step[1800/2713]: training loss : 0.9654595863819122 TRAIN  loss dict:  {'classification_loss': 0.9654595863819122}
2025-01-15 20:03:06,686 [INFO] Step[1850/2713]: training loss : 0.9347820854187012 TRAIN  loss dict:  {'classification_loss': 0.9347820854187012}
2025-01-15 20:03:18,570 [INFO] Step[1900/2713]: training loss : 0.9726505124568939 TRAIN  loss dict:  {'classification_loss': 0.9726505124568939}
2025-01-15 20:03:30,488 [INFO] Step[1950/2713]: training loss : 0.9490804219245911 TRAIN  loss dict:  {'classification_loss': 0.9490804219245911}
2025-01-15 20:03:42,393 [INFO] Step[2000/2713]: training loss : 0.9318664634227752 TRAIN  loss dict:  {'classification_loss': 0.9318664634227752}
2025-01-15 20:03:54,309 [INFO] Step[2050/2713]: training loss : 0.9679479169845581 TRAIN  loss dict:  {'classification_loss': 0.9679479169845581}
2025-01-15 20:04:06,223 [INFO] Step[2100/2713]: training loss : 0.9428323662281036 TRAIN  loss dict:  {'classification_loss': 0.9428323662281036}
2025-01-15 20:04:18,151 [INFO] Step[2150/2713]: training loss : 0.9915633821487426 TRAIN  loss dict:  {'classification_loss': 0.9915633821487426}
2025-01-15 20:04:30,031 [INFO] Step[2200/2713]: training loss : 0.9957603108882904 TRAIN  loss dict:  {'classification_loss': 0.9957603108882904}
2025-01-15 20:04:41,943 [INFO] Step[2250/2713]: training loss : 0.9695877873897553 TRAIN  loss dict:  {'classification_loss': 0.9695877873897553}
2025-01-15 20:04:53,843 [INFO] Step[2300/2713]: training loss : 0.9715764272212982 TRAIN  loss dict:  {'classification_loss': 0.9715764272212982}
2025-01-15 20:05:05,812 [INFO] Step[2350/2713]: training loss : 1.021961568593979 TRAIN  loss dict:  {'classification_loss': 1.021961568593979}
2025-01-15 20:05:17,734 [INFO] Step[2400/2713]: training loss : 0.9887093389034272 TRAIN  loss dict:  {'classification_loss': 0.9887093389034272}
2025-01-15 20:05:29,663 [INFO] Step[2450/2713]: training loss : 0.9718234288692474 TRAIN  loss dict:  {'classification_loss': 0.9718234288692474}
2025-01-15 20:05:41,556 [INFO] Step[2500/2713]: training loss : 0.978227299451828 TRAIN  loss dict:  {'classification_loss': 0.978227299451828}
2025-01-15 20:05:53,486 [INFO] Step[2550/2713]: training loss : 0.9308700478076934 TRAIN  loss dict:  {'classification_loss': 0.9308700478076934}
2025-01-15 20:06:05,392 [INFO] Step[2600/2713]: training loss : 0.9314099061489105 TRAIN  loss dict:  {'classification_loss': 0.9314099061489105}
2025-01-15 20:06:17,333 [INFO] Step[2650/2713]: training loss : 0.9632986259460449 TRAIN  loss dict:  {'classification_loss': 0.9632986259460449}
2025-01-15 20:06:29,256 [INFO] Step[2700/2713]: training loss : 0.9637574553489685 TRAIN  loss dict:  {'classification_loss': 0.9637574553489685}
2025-01-15 20:07:41,673 [INFO] Label accuracies statistics:
2025-01-15 20:07:41,673 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.75, 151: 1.0, 152: 1.0, 153: 1.0, 154: 0.5, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.25, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.5, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.25, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.75, 209: 0.5, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 1.0, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.5, 260: 1.0, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.5, 267: 0.25, 268: 0.5, 269: 0.5, 270: 1.0, 271: 0.25, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 1.0, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.25, 388: 0.5, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 20:07:41,675 [INFO] [26] TRAIN  loss: 0.9609754669723413 acc: 0.9921366261211451
2025-01-15 20:07:41,675 [INFO] [26] TRAIN  loss dict: {'classification_loss': 0.9609754669723413}
2025-01-15 20:07:41,675 [INFO] [26] VALIDATION loss: 2.0573316855090007 VALIDATION acc: 0.7579937304075235
2025-01-15 20:07:41,675 [INFO] [26] VALIDATION loss dict: {'classification_loss': 2.0573316855090007}
2025-01-15 20:07:41,675 [INFO] 
2025-01-15 20:07:58,216 [INFO] Step[50/2713]: training loss : 0.9655092227458953 TRAIN  loss dict:  {'classification_loss': 0.9655092227458953}
2025-01-15 20:08:10,072 [INFO] Step[100/2713]: training loss : 0.9494818258285522 TRAIN  loss dict:  {'classification_loss': 0.9494818258285522}
2025-01-15 20:08:21,983 [INFO] Step[150/2713]: training loss : 0.9702412939071655 TRAIN  loss dict:  {'classification_loss': 0.9702412939071655}
2025-01-15 20:08:33,855 [INFO] Step[200/2713]: training loss : 0.9420182371139526 TRAIN  loss dict:  {'classification_loss': 0.9420182371139526}
2025-01-15 20:08:45,795 [INFO] Step[250/2713]: training loss : 0.9610986053943634 TRAIN  loss dict:  {'classification_loss': 0.9610986053943634}
2025-01-15 20:08:57,702 [INFO] Step[300/2713]: training loss : 0.9303495490550995 TRAIN  loss dict:  {'classification_loss': 0.9303495490550995}
2025-01-15 20:09:09,628 [INFO] Step[350/2713]: training loss : 0.9471304082870483 TRAIN  loss dict:  {'classification_loss': 0.9471304082870483}
2025-01-15 20:09:21,569 [INFO] Step[400/2713]: training loss : 0.9663930070400238 TRAIN  loss dict:  {'classification_loss': 0.9663930070400238}
2025-01-15 20:09:33,474 [INFO] Step[450/2713]: training loss : 0.9331804656982422 TRAIN  loss dict:  {'classification_loss': 0.9331804656982422}
2025-01-15 20:09:45,361 [INFO] Step[500/2713]: training loss : 0.9565383076667786 TRAIN  loss dict:  {'classification_loss': 0.9565383076667786}
2025-01-15 20:09:57,301 [INFO] Step[550/2713]: training loss : 0.9353617191314697 TRAIN  loss dict:  {'classification_loss': 0.9353617191314697}
2025-01-15 20:10:09,225 [INFO] Step[600/2713]: training loss : 0.9303538048267365 TRAIN  loss dict:  {'classification_loss': 0.9303538048267365}
2025-01-15 20:10:21,124 [INFO] Step[650/2713]: training loss : 0.9941112208366394 TRAIN  loss dict:  {'classification_loss': 0.9941112208366394}
2025-01-15 20:10:33,006 [INFO] Step[700/2713]: training loss : 0.945727698802948 TRAIN  loss dict:  {'classification_loss': 0.945727698802948}
2025-01-15 20:10:44,996 [INFO] Step[750/2713]: training loss : 0.936217166185379 TRAIN  loss dict:  {'classification_loss': 0.936217166185379}
2025-01-15 20:10:56,892 [INFO] Step[800/2713]: training loss : 0.9578923845291137 TRAIN  loss dict:  {'classification_loss': 0.9578923845291137}
2025-01-15 20:11:08,816 [INFO] Step[850/2713]: training loss : 0.9443403339385986 TRAIN  loss dict:  {'classification_loss': 0.9443403339385986}
2025-01-15 20:11:20,698 [INFO] Step[900/2713]: training loss : 0.9565233492851257 TRAIN  loss dict:  {'classification_loss': 0.9565233492851257}
2025-01-15 20:11:32,614 [INFO] Step[950/2713]: training loss : 0.9374224019050598 TRAIN  loss dict:  {'classification_loss': 0.9374224019050598}
2025-01-15 20:11:44,503 [INFO] Step[1000/2713]: training loss : 0.9304174625873566 TRAIN  loss dict:  {'classification_loss': 0.9304174625873566}
2025-01-15 20:11:56,431 [INFO] Step[1050/2713]: training loss : 0.9841990602016449 TRAIN  loss dict:  {'classification_loss': 0.9841990602016449}
2025-01-15 20:12:08,333 [INFO] Step[1100/2713]: training loss : 0.9425407540798187 TRAIN  loss dict:  {'classification_loss': 0.9425407540798187}
2025-01-15 20:12:20,236 [INFO] Step[1150/2713]: training loss : 0.9967608058452606 TRAIN  loss dict:  {'classification_loss': 0.9967608058452606}
2025-01-15 20:12:32,149 [INFO] Step[1200/2713]: training loss : 0.9339041459560394 TRAIN  loss dict:  {'classification_loss': 0.9339041459560394}
2025-01-15 20:12:44,079 [INFO] Step[1250/2713]: training loss : 0.9976228821277618 TRAIN  loss dict:  {'classification_loss': 0.9976228821277618}
2025-01-15 20:12:55,981 [INFO] Step[1300/2713]: training loss : 0.9307521319389344 TRAIN  loss dict:  {'classification_loss': 0.9307521319389344}
2025-01-15 20:13:07,899 [INFO] Step[1350/2713]: training loss : 1.003513765335083 TRAIN  loss dict:  {'classification_loss': 1.003513765335083}
2025-01-15 20:13:19,795 [INFO] Step[1400/2713]: training loss : 0.9666548216342926 TRAIN  loss dict:  {'classification_loss': 0.9666548216342926}
2025-01-15 20:13:31,694 [INFO] Step[1450/2713]: training loss : 0.9562407433986664 TRAIN  loss dict:  {'classification_loss': 0.9562407433986664}
2025-01-15 20:13:43,587 [INFO] Step[1500/2713]: training loss : 0.9443577527999878 TRAIN  loss dict:  {'classification_loss': 0.9443577527999878}
2025-01-15 20:13:55,497 [INFO] Step[1550/2713]: training loss : 0.9607970595359803 TRAIN  loss dict:  {'classification_loss': 0.9607970595359803}
2025-01-15 20:14:07,379 [INFO] Step[1600/2713]: training loss : 0.9446453428268433 TRAIN  loss dict:  {'classification_loss': 0.9446453428268433}
2025-01-15 20:14:19,300 [INFO] Step[1650/2713]: training loss : 0.9545445930957794 TRAIN  loss dict:  {'classification_loss': 0.9545445930957794}
2025-01-15 20:14:31,195 [INFO] Step[1700/2713]: training loss : 0.941930046081543 TRAIN  loss dict:  {'classification_loss': 0.941930046081543}
2025-01-15 20:14:43,089 [INFO] Step[1750/2713]: training loss : 0.9666135156154633 TRAIN  loss dict:  {'classification_loss': 0.9666135156154633}
2025-01-15 20:14:55,011 [INFO] Step[1800/2713]: training loss : 0.981908951997757 TRAIN  loss dict:  {'classification_loss': 0.981908951997757}
2025-01-15 20:15:06,957 [INFO] Step[1850/2713]: training loss : 0.9790542662143707 TRAIN  loss dict:  {'classification_loss': 0.9790542662143707}
2025-01-15 20:15:18,865 [INFO] Step[1900/2713]: training loss : 0.9299979472160339 TRAIN  loss dict:  {'classification_loss': 0.9299979472160339}
2025-01-15 20:15:30,784 [INFO] Step[1950/2713]: training loss : 0.93438108086586 TRAIN  loss dict:  {'classification_loss': 0.93438108086586}
2025-01-15 20:15:42,734 [INFO] Step[2000/2713]: training loss : 0.9344170689582825 TRAIN  loss dict:  {'classification_loss': 0.9344170689582825}
2025-01-15 20:15:54,630 [INFO] Step[2050/2713]: training loss : 0.9306868636608123 TRAIN  loss dict:  {'classification_loss': 0.9306868636608123}
2025-01-15 20:16:06,521 [INFO] Step[2100/2713]: training loss : 0.9364832746982574 TRAIN  loss dict:  {'classification_loss': 0.9364832746982574}
2025-01-15 20:16:18,379 [INFO] Step[2150/2713]: training loss : 0.9428714907169342 TRAIN  loss dict:  {'classification_loss': 0.9428714907169342}
2025-01-15 20:16:30,257 [INFO] Step[2200/2713]: training loss : 0.9608695185184479 TRAIN  loss dict:  {'classification_loss': 0.9608695185184479}
2025-01-15 20:16:42,138 [INFO] Step[2250/2713]: training loss : 0.9500020146369934 TRAIN  loss dict:  {'classification_loss': 0.9500020146369934}
2025-01-15 20:16:54,085 [INFO] Step[2300/2713]: training loss : 0.9315663278102875 TRAIN  loss dict:  {'classification_loss': 0.9315663278102875}
2025-01-15 20:17:06,011 [INFO] Step[2350/2713]: training loss : 0.9444414031505585 TRAIN  loss dict:  {'classification_loss': 0.9444414031505585}
2025-01-15 20:17:17,940 [INFO] Step[2400/2713]: training loss : 0.9394981741905213 TRAIN  loss dict:  {'classification_loss': 0.9394981741905213}
2025-01-15 20:17:29,857 [INFO] Step[2450/2713]: training loss : 0.9325408554077148 TRAIN  loss dict:  {'classification_loss': 0.9325408554077148}
2025-01-15 20:17:41,785 [INFO] Step[2500/2713]: training loss : 0.942025614976883 TRAIN  loss dict:  {'classification_loss': 0.942025614976883}
2025-01-15 20:17:53,698 [INFO] Step[2550/2713]: training loss : 0.9981026327610016 TRAIN  loss dict:  {'classification_loss': 0.9981026327610016}
2025-01-15 20:18:05,544 [INFO] Step[2600/2713]: training loss : 0.958727867603302 TRAIN  loss dict:  {'classification_loss': 0.958727867603302}
2025-01-15 20:18:17,490 [INFO] Step[2650/2713]: training loss : 0.9382467210292816 TRAIN  loss dict:  {'classification_loss': 0.9382467210292816}
2025-01-15 20:18:29,372 [INFO] Step[2700/2713]: training loss : 0.971313693523407 TRAIN  loss dict:  {'classification_loss': 0.971313693523407}
2025-01-15 20:19:41,977 [INFO] Label accuracies statistics:
2025-01-15 20:19:41,977 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.25, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.25, 17: 0.0, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 0.75, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 1.0, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.0, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 1.0, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 0.75, 125: 1.0, 126: 0.75, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 0.75, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.5, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 0.75, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 0.75, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 1.0, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.5, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 1.0, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.25, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.5, 380: 1.0, 381: 0.25, 382: 0.75, 383: 0.5, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-15 20:19:41,979 [INFO] [27] TRAIN  loss: 0.9531069831174983 acc: 0.993119547856002
2025-01-15 20:19:41,979 [INFO] [27] TRAIN  loss dict: {'classification_loss': 0.9531069831174983}
2025-01-15 20:19:41,979 [INFO] [27] VALIDATION loss: 2.062063637420647 VALIDATION acc: 0.7567398119122257
2025-01-15 20:19:41,979 [INFO] [27] VALIDATION loss dict: {'classification_loss': 2.062063637420647}
2025-01-15 20:19:41,979 [INFO] 
2025-01-15 20:19:59,143 [INFO] Step[50/2713]: training loss : 0.9550129461288452 TRAIN  loss dict:  {'classification_loss': 0.9550129461288452}
2025-01-15 20:20:11,027 [INFO] Step[100/2713]: training loss : 0.9871573233604432 TRAIN  loss dict:  {'classification_loss': 0.9871573233604432}
2025-01-15 20:20:22,890 [INFO] Step[150/2713]: training loss : 0.9341192400455475 TRAIN  loss dict:  {'classification_loss': 0.9341192400455475}
2025-01-15 20:20:34,810 [INFO] Step[200/2713]: training loss : 0.9328713583946228 TRAIN  loss dict:  {'classification_loss': 0.9328713583946228}
2025-01-15 20:20:46,739 [INFO] Step[250/2713]: training loss : 0.9350772655010223 TRAIN  loss dict:  {'classification_loss': 0.9350772655010223}
2025-01-15 20:20:58,627 [INFO] Step[300/2713]: training loss : 0.9318498206138611 TRAIN  loss dict:  {'classification_loss': 0.9318498206138611}
2025-01-15 20:21:10,560 [INFO] Step[350/2713]: training loss : 0.9589110863208771 TRAIN  loss dict:  {'classification_loss': 0.9589110863208771}
2025-01-15 20:21:22,496 [INFO] Step[400/2713]: training loss : 0.9351708281040192 TRAIN  loss dict:  {'classification_loss': 0.9351708281040192}
2025-01-15 20:21:34,388 [INFO] Step[450/2713]: training loss : 0.9304217505455017 TRAIN  loss dict:  {'classification_loss': 0.9304217505455017}
2025-01-15 20:21:46,279 [INFO] Step[500/2713]: training loss : 0.9311285889148713 TRAIN  loss dict:  {'classification_loss': 0.9311285889148713}
2025-01-15 20:21:58,246 [INFO] Step[550/2713]: training loss : 0.9601353180408477 TRAIN  loss dict:  {'classification_loss': 0.9601353180408477}
2025-01-15 20:22:10,162 [INFO] Step[600/2713]: training loss : 0.9595453763008117 TRAIN  loss dict:  {'classification_loss': 0.9595453763008117}
2025-01-15 20:22:22,062 [INFO] Step[650/2713]: training loss : 0.9743773198127746 TRAIN  loss dict:  {'classification_loss': 0.9743773198127746}
2025-01-15 20:22:33,992 [INFO] Step[700/2713]: training loss : 0.956096180677414 TRAIN  loss dict:  {'classification_loss': 0.956096180677414}
2025-01-15 20:22:45,905 [INFO] Step[750/2713]: training loss : 0.9510485017299652 TRAIN  loss dict:  {'classification_loss': 0.9510485017299652}
2025-01-15 20:22:57,826 [INFO] Step[800/2713]: training loss : 0.9328774774074554 TRAIN  loss dict:  {'classification_loss': 0.9328774774074554}
2025-01-15 20:23:09,723 [INFO] Step[850/2713]: training loss : 0.9466540169715881 TRAIN  loss dict:  {'classification_loss': 0.9466540169715881}
2025-01-15 20:23:21,626 [INFO] Step[900/2713]: training loss : 1.0043818974494934 TRAIN  loss dict:  {'classification_loss': 1.0043818974494934}
2025-01-15 20:23:33,559 [INFO] Step[950/2713]: training loss : 0.9318399667739868 TRAIN  loss dict:  {'classification_loss': 0.9318399667739868}
2025-01-15 20:23:45,460 [INFO] Step[1000/2713]: training loss : 0.9313258028030396 TRAIN  loss dict:  {'classification_loss': 0.9313258028030396}
2025-01-15 20:23:57,395 [INFO] Step[1050/2713]: training loss : 0.9367013645172119 TRAIN  loss dict:  {'classification_loss': 0.9367013645172119}
2025-01-15 20:24:09,376 [INFO] Step[1100/2713]: training loss : 0.9752697741985321 TRAIN  loss dict:  {'classification_loss': 0.9752697741985321}
2025-01-15 20:24:21,279 [INFO] Step[1150/2713]: training loss : 0.9343573009967804 TRAIN  loss dict:  {'classification_loss': 0.9343573009967804}
2025-01-15 20:24:33,199 [INFO] Step[1200/2713]: training loss : 0.9342107272148132 TRAIN  loss dict:  {'classification_loss': 0.9342107272148132}
2025-01-15 20:24:45,151 [INFO] Step[1250/2713]: training loss : 0.9675934100151062 TRAIN  loss dict:  {'classification_loss': 0.9675934100151062}
2025-01-15 20:24:57,060 [INFO] Step[1300/2713]: training loss : 0.9682804572582245 TRAIN  loss dict:  {'classification_loss': 0.9682804572582245}
2025-01-15 20:25:08,954 [INFO] Step[1350/2713]: training loss : 0.9874643242359161 TRAIN  loss dict:  {'classification_loss': 0.9874643242359161}
2025-01-15 20:25:20,851 [INFO] Step[1400/2713]: training loss : 0.9397413253784179 TRAIN  loss dict:  {'classification_loss': 0.9397413253784179}
2025-01-15 20:25:32,749 [INFO] Step[1450/2713]: training loss : 0.937850923538208 TRAIN  loss dict:  {'classification_loss': 0.937850923538208}
2025-01-15 20:25:44,631 [INFO] Step[1500/2713]: training loss : 0.9716962969303131 TRAIN  loss dict:  {'classification_loss': 0.9716962969303131}
2025-01-15 20:25:56,504 [INFO] Step[1550/2713]: training loss : 0.9851922380924225 TRAIN  loss dict:  {'classification_loss': 0.9851922380924225}
2025-01-15 20:26:08,407 [INFO] Step[1600/2713]: training loss : 0.9519851624965667 TRAIN  loss dict:  {'classification_loss': 0.9519851624965667}
2025-01-15 20:26:20,335 [INFO] Step[1650/2713]: training loss : 0.9347566795349121 TRAIN  loss dict:  {'classification_loss': 0.9347566795349121}
2025-01-15 20:26:32,236 [INFO] Step[1700/2713]: training loss : 0.9436984992027283 TRAIN  loss dict:  {'classification_loss': 0.9436984992027283}
2025-01-15 20:26:44,177 [INFO] Step[1750/2713]: training loss : 0.9535701215267182 TRAIN  loss dict:  {'classification_loss': 0.9535701215267182}
2025-01-15 20:26:56,076 [INFO] Step[1800/2713]: training loss : 0.9344143593311309 TRAIN  loss dict:  {'classification_loss': 0.9344143593311309}
2025-01-15 20:27:08,040 [INFO] Step[1850/2713]: training loss : 0.9351102364063263 TRAIN  loss dict:  {'classification_loss': 0.9351102364063263}
2025-01-15 20:27:19,922 [INFO] Step[1900/2713]: training loss : 0.9558746695518494 TRAIN  loss dict:  {'classification_loss': 0.9558746695518494}
2025-01-15 20:27:31,857 [INFO] Step[1950/2713]: training loss : 0.9337621450424194 TRAIN  loss dict:  {'classification_loss': 0.9337621450424194}
2025-01-15 20:27:43,748 [INFO] Step[2000/2713]: training loss : 0.9311102509498597 TRAIN  loss dict:  {'classification_loss': 0.9311102509498597}
2025-01-15 20:27:55,649 [INFO] Step[2050/2713]: training loss : 0.9501016211509704 TRAIN  loss dict:  {'classification_loss': 0.9501016211509704}
2025-01-15 20:28:07,520 [INFO] Step[2100/2713]: training loss : 0.96548614859581 TRAIN  loss dict:  {'classification_loss': 0.96548614859581}
2025-01-15 20:28:19,424 [INFO] Step[2150/2713]: training loss : 0.9554021680355071 TRAIN  loss dict:  {'classification_loss': 0.9554021680355071}
2025-01-15 20:28:31,320 [INFO] Step[2200/2713]: training loss : 0.9369050908088684 TRAIN  loss dict:  {'classification_loss': 0.9369050908088684}
2025-01-15 20:28:43,217 [INFO] Step[2250/2713]: training loss : 0.9430587565898896 TRAIN  loss dict:  {'classification_loss': 0.9430587565898896}
2025-01-15 20:28:55,141 [INFO] Step[2300/2713]: training loss : 0.982642012834549 TRAIN  loss dict:  {'classification_loss': 0.982642012834549}
2025-01-15 20:29:07,103 [INFO] Step[2350/2713]: training loss : 0.9299537312984466 TRAIN  loss dict:  {'classification_loss': 0.9299537312984466}
2025-01-15 20:29:19,015 [INFO] Step[2400/2713]: training loss : 0.9990305042266846 TRAIN  loss dict:  {'classification_loss': 0.9990305042266846}
2025-01-15 20:29:30,955 [INFO] Step[2450/2713]: training loss : 0.9551495862007141 TRAIN  loss dict:  {'classification_loss': 0.9551495862007141}
2025-01-15 20:29:42,899 [INFO] Step[2500/2713]: training loss : 0.9479565024375916 TRAIN  loss dict:  {'classification_loss': 0.9479565024375916}
2025-01-15 20:29:54,811 [INFO] Step[2550/2713]: training loss : 0.9487057828903198 TRAIN  loss dict:  {'classification_loss': 0.9487057828903198}
2025-01-15 20:30:06,688 [INFO] Step[2600/2713]: training loss : 0.934953179359436 TRAIN  loss dict:  {'classification_loss': 0.934953179359436}
2025-01-15 20:30:18,595 [INFO] Step[2650/2713]: training loss : 0.9616658413410186 TRAIN  loss dict:  {'classification_loss': 0.9616658413410186}
2025-01-15 20:30:30,450 [INFO] Step[2700/2713]: training loss : 0.9513654410839081 TRAIN  loss dict:  {'classification_loss': 0.9513654410839081}
2025-01-15 20:31:43,286 [INFO] Label accuracies statistics:
2025-01-15 20:31:43,286 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.25, 64: 0.5, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.5, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.25, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.5, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.0, 207: 0.5, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.5, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.25, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.5, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.25, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.25, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 0.5, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.25, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.5, 322: 1.0, 323: 0.5, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 1.0, 378: 0.5, 379: 0.5, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 1.0, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.5, 395: 0.5, 396: 1.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 20:31:43,288 [INFO] [28] TRAIN  loss: 0.9509310395217583 acc: 0.9936110087234304
2025-01-15 20:31:43,288 [INFO] [28] TRAIN  loss dict: {'classification_loss': 0.9509310395217583}
2025-01-15 20:31:43,288 [INFO] [28] VALIDATION loss: 2.0405378736051403 VALIDATION acc: 0.7517241379310344
2025-01-15 20:31:43,288 [INFO] [28] VALIDATION loss dict: {'classification_loss': 2.0405378736051403}
2025-01-15 20:31:43,289 [INFO] 
2025-01-15 20:32:00,176 [INFO] Step[50/2713]: training loss : 0.9498979556560516 TRAIN  loss dict:  {'classification_loss': 0.9498979556560516}
2025-01-15 20:32:12,030 [INFO] Step[100/2713]: training loss : 0.9331195759773254 TRAIN  loss dict:  {'classification_loss': 0.9331195759773254}
2025-01-15 20:32:23,917 [INFO] Step[150/2713]: training loss : 0.9748883116245269 TRAIN  loss dict:  {'classification_loss': 0.9748883116245269}
2025-01-15 20:32:35,781 [INFO] Step[200/2713]: training loss : 0.9491118216514587 TRAIN  loss dict:  {'classification_loss': 0.9491118216514587}
2025-01-15 20:32:47,656 [INFO] Step[250/2713]: training loss : 0.9542182922363281 TRAIN  loss dict:  {'classification_loss': 0.9542182922363281}
2025-01-15 20:32:59,550 [INFO] Step[300/2713]: training loss : 0.9747066676616669 TRAIN  loss dict:  {'classification_loss': 0.9747066676616669}
2025-01-15 20:33:11,456 [INFO] Step[350/2713]: training loss : 0.9460255205631256 TRAIN  loss dict:  {'classification_loss': 0.9460255205631256}
2025-01-15 20:33:23,366 [INFO] Step[400/2713]: training loss : 0.9465125358104706 TRAIN  loss dict:  {'classification_loss': 0.9465125358104706}
2025-01-15 20:33:35,322 [INFO] Step[450/2713]: training loss : 0.9715928971767426 TRAIN  loss dict:  {'classification_loss': 0.9715928971767426}
2025-01-15 20:33:47,225 [INFO] Step[500/2713]: training loss : 0.9368949484825134 TRAIN  loss dict:  {'classification_loss': 0.9368949484825134}
2025-01-15 20:33:59,184 [INFO] Step[550/2713]: training loss : 0.9682465195655823 TRAIN  loss dict:  {'classification_loss': 0.9682465195655823}
2025-01-15 20:34:11,139 [INFO] Step[600/2713]: training loss : 0.9366531443595886 TRAIN  loss dict:  {'classification_loss': 0.9366531443595886}
2025-01-15 20:34:23,092 [INFO] Step[650/2713]: training loss : 0.9971034741401672 TRAIN  loss dict:  {'classification_loss': 0.9971034741401672}
2025-01-15 20:34:35,008 [INFO] Step[700/2713]: training loss : 0.9529992794990539 TRAIN  loss dict:  {'classification_loss': 0.9529992794990539}
2025-01-15 20:34:46,902 [INFO] Step[750/2713]: training loss : 0.9479525065422059 TRAIN  loss dict:  {'classification_loss': 0.9479525065422059}
2025-01-15 20:34:58,818 [INFO] Step[800/2713]: training loss : 0.931024626493454 TRAIN  loss dict:  {'classification_loss': 0.931024626493454}
2025-01-15 20:35:10,743 [INFO] Step[850/2713]: training loss : 0.9400046932697296 TRAIN  loss dict:  {'classification_loss': 0.9400046932697296}
2025-01-15 20:35:22,675 [INFO] Step[900/2713]: training loss : 0.9333441495895386 TRAIN  loss dict:  {'classification_loss': 0.9333441495895386}
2025-01-15 20:35:34,564 [INFO] Step[950/2713]: training loss : 0.9491956198215484 TRAIN  loss dict:  {'classification_loss': 0.9491956198215484}
2025-01-15 20:35:46,476 [INFO] Step[1000/2713]: training loss : 0.9323952174186707 TRAIN  loss dict:  {'classification_loss': 0.9323952174186707}
2025-01-15 20:35:58,392 [INFO] Step[1050/2713]: training loss : 0.933804349899292 TRAIN  loss dict:  {'classification_loss': 0.933804349899292}
2025-01-15 20:36:10,283 [INFO] Step[1100/2713]: training loss : 0.9580352652072907 TRAIN  loss dict:  {'classification_loss': 0.9580352652072907}
2025-01-15 20:36:22,202 [INFO] Step[1150/2713]: training loss : 0.9380623650550842 TRAIN  loss dict:  {'classification_loss': 0.9380623650550842}
2025-01-15 20:36:34,115 [INFO] Step[1200/2713]: training loss : 0.9529047954082489 TRAIN  loss dict:  {'classification_loss': 0.9529047954082489}
2025-01-15 20:36:46,104 [INFO] Step[1250/2713]: training loss : 0.9448602283000946 TRAIN  loss dict:  {'classification_loss': 0.9448602283000946}
2025-01-15 20:36:57,964 [INFO] Step[1300/2713]: training loss : 0.931917507648468 TRAIN  loss dict:  {'classification_loss': 0.931917507648468}
2025-01-15 20:37:09,849 [INFO] Step[1350/2713]: training loss : 0.9413809049129486 TRAIN  loss dict:  {'classification_loss': 0.9413809049129486}
2025-01-15 20:37:21,721 [INFO] Step[1400/2713]: training loss : 0.9349641883373261 TRAIN  loss dict:  {'classification_loss': 0.9349641883373261}
2025-01-15 20:37:33,601 [INFO] Step[1450/2713]: training loss : 0.9746571326255798 TRAIN  loss dict:  {'classification_loss': 0.9746571326255798}
2025-01-15 20:37:45,486 [INFO] Step[1500/2713]: training loss : 1.0099125945568084 TRAIN  loss dict:  {'classification_loss': 1.0099125945568084}
2025-01-15 20:37:57,397 [INFO] Step[1550/2713]: training loss : 0.9324236762523651 TRAIN  loss dict:  {'classification_loss': 0.9324236762523651}
2025-01-15 20:38:09,259 [INFO] Step[1600/2713]: training loss : 0.9712244045734405 TRAIN  loss dict:  {'classification_loss': 0.9712244045734405}
2025-01-15 20:38:21,221 [INFO] Step[1650/2713]: training loss : 0.982387752532959 TRAIN  loss dict:  {'classification_loss': 0.982387752532959}
2025-01-15 20:38:33,150 [INFO] Step[1700/2713]: training loss : 0.9606739819049835 TRAIN  loss dict:  {'classification_loss': 0.9606739819049835}
2025-01-15 20:38:45,097 [INFO] Step[1750/2713]: training loss : 0.9505494523048401 TRAIN  loss dict:  {'classification_loss': 0.9505494523048401}
2025-01-15 20:38:56,994 [INFO] Step[1800/2713]: training loss : 0.9430958771705628 TRAIN  loss dict:  {'classification_loss': 0.9430958771705628}
2025-01-15 20:39:08,897 [INFO] Step[1850/2713]: training loss : 0.9308823657035827 TRAIN  loss dict:  {'classification_loss': 0.9308823657035827}
2025-01-15 20:39:20,814 [INFO] Step[1900/2713]: training loss : 0.943961695432663 TRAIN  loss dict:  {'classification_loss': 0.943961695432663}
2025-01-15 20:39:32,698 [INFO] Step[1950/2713]: training loss : 0.9750936353206634 TRAIN  loss dict:  {'classification_loss': 0.9750936353206634}
2025-01-15 20:39:44,581 [INFO] Step[2000/2713]: training loss : 0.9898937344551086 TRAIN  loss dict:  {'classification_loss': 0.9898937344551086}
2025-01-15 20:39:56,477 [INFO] Step[2050/2713]: training loss : 0.953277086019516 TRAIN  loss dict:  {'classification_loss': 0.953277086019516}
2025-01-15 20:40:08,374 [INFO] Step[2100/2713]: training loss : 0.934682686328888 TRAIN  loss dict:  {'classification_loss': 0.934682686328888}
2025-01-15 20:40:20,278 [INFO] Step[2150/2713]: training loss : 0.9397159028053284 TRAIN  loss dict:  {'classification_loss': 0.9397159028053284}
2025-01-15 20:40:32,243 [INFO] Step[2200/2713]: training loss : 0.9324361193180084 TRAIN  loss dict:  {'classification_loss': 0.9324361193180084}
2025-01-15 20:40:44,139 [INFO] Step[2250/2713]: training loss : 0.9449664318561554 TRAIN  loss dict:  {'classification_loss': 0.9449664318561554}
2025-01-15 20:40:56,031 [INFO] Step[2300/2713]: training loss : 0.9354255986213684 TRAIN  loss dict:  {'classification_loss': 0.9354255986213684}
2025-01-15 20:41:07,940 [INFO] Step[2350/2713]: training loss : 0.938762629032135 TRAIN  loss dict:  {'classification_loss': 0.938762629032135}
2025-01-15 20:41:19,835 [INFO] Step[2400/2713]: training loss : 0.9343051493167878 TRAIN  loss dict:  {'classification_loss': 0.9343051493167878}
2025-01-15 20:41:31,747 [INFO] Step[2450/2713]: training loss : 0.963743542432785 TRAIN  loss dict:  {'classification_loss': 0.963743542432785}
2025-01-15 20:41:43,709 [INFO] Step[2500/2713]: training loss : 0.9853903770446777 TRAIN  loss dict:  {'classification_loss': 0.9853903770446777}
2025-01-15 20:41:55,612 [INFO] Step[2550/2713]: training loss : 0.9570192551612854 TRAIN  loss dict:  {'classification_loss': 0.9570192551612854}
2025-01-15 20:42:07,504 [INFO] Step[2600/2713]: training loss : 0.9304481863975524 TRAIN  loss dict:  {'classification_loss': 0.9304481863975524}
2025-01-15 20:42:19,402 [INFO] Step[2650/2713]: training loss : 0.9379340744018555 TRAIN  loss dict:  {'classification_loss': 0.9379340744018555}
2025-01-15 20:42:31,256 [INFO] Step[2700/2713]: training loss : 1.0334477829933166 TRAIN  loss dict:  {'classification_loss': 1.0334477829933166}
2025-01-15 20:43:44,073 [INFO] Label accuracies statistics:
2025-01-15 20:43:44,074 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.25, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.5, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.3333333333333333, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.5, 168: 0.5, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.5, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.25, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.75, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 0.75, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.75, 274: 0.25, 275: 0.75, 276: 0.75, 277: 0.5, 278: 0.25, 279: 0.75, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.0, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 0.75, 344: 0.5, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.5, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-15 20:43:44,075 [INFO] [29] TRAIN  loss: 0.9527108320836603 acc: 0.9933652782897162
2025-01-15 20:43:44,075 [INFO] [29] TRAIN  loss dict: {'classification_loss': 0.9527108320836603}
2025-01-15 20:43:44,076 [INFO] [29] VALIDATION loss: 1.986973468074225 VALIDATION acc: 0.7711598746081505
2025-01-15 20:43:44,076 [INFO] [29] VALIDATION loss dict: {'classification_loss': 1.986973468074225}
2025-01-15 20:43:44,076 [INFO] 
2025-01-15 20:44:00,750 [INFO] Step[50/2713]: training loss : 0.9303317201137543 TRAIN  loss dict:  {'classification_loss': 0.9303317201137543}
2025-01-15 20:44:12,659 [INFO] Step[100/2713]: training loss : 0.9293530988693237 TRAIN  loss dict:  {'classification_loss': 0.9293530988693237}
2025-01-15 20:44:24,594 [INFO] Step[150/2713]: training loss : 0.939277195930481 TRAIN  loss dict:  {'classification_loss': 0.939277195930481}
2025-01-15 20:44:36,514 [INFO] Step[200/2713]: training loss : 0.9311261045932769 TRAIN  loss dict:  {'classification_loss': 0.9311261045932769}
2025-01-15 20:44:48,417 [INFO] Step[250/2713]: training loss : 0.9315764391422272 TRAIN  loss dict:  {'classification_loss': 0.9315764391422272}
2025-01-15 20:45:00,313 [INFO] Step[300/2713]: training loss : 0.9367569565773011 TRAIN  loss dict:  {'classification_loss': 0.9367569565773011}
2025-01-15 20:45:12,230 [INFO] Step[350/2713]: training loss : 0.9486276841163636 TRAIN  loss dict:  {'classification_loss': 0.9486276841163636}
2025-01-15 20:45:24,149 [INFO] Step[400/2713]: training loss : 0.931183990240097 TRAIN  loss dict:  {'classification_loss': 0.931183990240097}
2025-01-15 20:45:36,053 [INFO] Step[450/2713]: training loss : 0.9703543198108673 TRAIN  loss dict:  {'classification_loss': 0.9703543198108673}
2025-01-15 20:45:47,970 [INFO] Step[500/2713]: training loss : 0.9747740447521209 TRAIN  loss dict:  {'classification_loss': 0.9747740447521209}
2025-01-15 20:45:59,895 [INFO] Step[550/2713]: training loss : 0.9554660248756409 TRAIN  loss dict:  {'classification_loss': 0.9554660248756409}
2025-01-15 20:46:11,781 [INFO] Step[600/2713]: training loss : 0.9375056624412537 TRAIN  loss dict:  {'classification_loss': 0.9375056624412537}
2025-01-15 20:46:23,688 [INFO] Step[650/2713]: training loss : 0.943888703584671 TRAIN  loss dict:  {'classification_loss': 0.943888703584671}
2025-01-15 20:46:35,594 [INFO] Step[700/2713]: training loss : 0.9644098496437072 TRAIN  loss dict:  {'classification_loss': 0.9644098496437072}
2025-01-15 20:46:47,510 [INFO] Step[750/2713]: training loss : 0.9375476133823395 TRAIN  loss dict:  {'classification_loss': 0.9375476133823395}
2025-01-15 20:46:59,437 [INFO] Step[800/2713]: training loss : 0.9369035995006562 TRAIN  loss dict:  {'classification_loss': 0.9369035995006562}
2025-01-15 20:47:11,336 [INFO] Step[850/2713]: training loss : 0.9580237233638763 TRAIN  loss dict:  {'classification_loss': 0.9580237233638763}
2025-01-15 20:47:23,214 [INFO] Step[900/2713]: training loss : 0.9323737943172454 TRAIN  loss dict:  {'classification_loss': 0.9323737943172454}
2025-01-15 20:47:35,133 [INFO] Step[950/2713]: training loss : 0.9371184229850769 TRAIN  loss dict:  {'classification_loss': 0.9371184229850769}
2025-01-15 20:47:47,045 [INFO] Step[1000/2713]: training loss : 0.9816453015804291 TRAIN  loss dict:  {'classification_loss': 0.9816453015804291}
2025-01-15 20:47:58,948 [INFO] Step[1050/2713]: training loss : 0.9294852888584137 TRAIN  loss dict:  {'classification_loss': 0.9294852888584137}
2025-01-15 20:48:10,821 [INFO] Step[1100/2713]: training loss : 0.9575855112075806 TRAIN  loss dict:  {'classification_loss': 0.9575855112075806}
2025-01-15 20:48:22,736 [INFO] Step[1150/2713]: training loss : 0.9341483128070831 TRAIN  loss dict:  {'classification_loss': 0.9341483128070831}
2025-01-15 20:48:34,704 [INFO] Step[1200/2713]: training loss : 0.957988772392273 TRAIN  loss dict:  {'classification_loss': 0.957988772392273}
2025-01-15 20:48:46,594 [INFO] Step[1250/2713]: training loss : 0.9293818306922913 TRAIN  loss dict:  {'classification_loss': 0.9293818306922913}
2025-01-15 20:48:58,482 [INFO] Step[1300/2713]: training loss : 0.9488812446594238 TRAIN  loss dict:  {'classification_loss': 0.9488812446594238}
2025-01-15 20:49:10,382 [INFO] Step[1350/2713]: training loss : 0.9305769920349121 TRAIN  loss dict:  {'classification_loss': 0.9305769920349121}
2025-01-15 20:49:22,284 [INFO] Step[1400/2713]: training loss : 0.975224279165268 TRAIN  loss dict:  {'classification_loss': 0.975224279165268}
2025-01-15 20:49:34,190 [INFO] Step[1450/2713]: training loss : 0.9950062382221222 TRAIN  loss dict:  {'classification_loss': 0.9950062382221222}
2025-01-15 20:49:46,094 [INFO] Step[1500/2713]: training loss : 0.9476547944545746 TRAIN  loss dict:  {'classification_loss': 0.9476547944545746}
2025-01-15 20:49:58,020 [INFO] Step[1550/2713]: training loss : 0.9627638304233551 TRAIN  loss dict:  {'classification_loss': 0.9627638304233551}
2025-01-15 20:50:09,932 [INFO] Step[1600/2713]: training loss : 0.9419262564182281 TRAIN  loss dict:  {'classification_loss': 0.9419262564182281}
2025-01-15 20:50:21,843 [INFO] Step[1650/2713]: training loss : 0.9345111787319184 TRAIN  loss dict:  {'classification_loss': 0.9345111787319184}
2025-01-15 20:50:33,738 [INFO] Step[1700/2713]: training loss : 0.9461701834201812 TRAIN  loss dict:  {'classification_loss': 0.9461701834201812}
2025-01-15 20:50:45,644 [INFO] Step[1750/2713]: training loss : 0.9305957782268525 TRAIN  loss dict:  {'classification_loss': 0.9305957782268525}
2025-01-15 20:50:57,575 [INFO] Step[1800/2713]: training loss : 0.940796457529068 TRAIN  loss dict:  {'classification_loss': 0.940796457529068}
2025-01-15 20:51:09,467 [INFO] Step[1850/2713]: training loss : 0.9572598528862 TRAIN  loss dict:  {'classification_loss': 0.9572598528862}
2025-01-15 20:51:21,384 [INFO] Step[1900/2713]: training loss : 0.929930819272995 TRAIN  loss dict:  {'classification_loss': 0.929930819272995}
2025-01-15 20:51:33,305 [INFO] Step[1950/2713]: training loss : 0.9347627770900726 TRAIN  loss dict:  {'classification_loss': 0.9347627770900726}
2025-01-15 20:51:45,234 [INFO] Step[2000/2713]: training loss : 0.9505682396888733 TRAIN  loss dict:  {'classification_loss': 0.9505682396888733}
2025-01-15 20:51:57,156 [INFO] Step[2050/2713]: training loss : 0.9563761210441589 TRAIN  loss dict:  {'classification_loss': 0.9563761210441589}
2025-01-15 20:52:09,060 [INFO] Step[2100/2713]: training loss : 0.9636149680614472 TRAIN  loss dict:  {'classification_loss': 0.9636149680614472}
2025-01-15 20:52:20,965 [INFO] Step[2150/2713]: training loss : 0.9410108852386475 TRAIN  loss dict:  {'classification_loss': 0.9410108852386475}
2025-01-15 20:52:32,840 [INFO] Step[2200/2713]: training loss : 0.9539894676208496 TRAIN  loss dict:  {'classification_loss': 0.9539894676208496}
2025-01-15 20:52:44,709 [INFO] Step[2250/2713]: training loss : 0.9337284910678864 TRAIN  loss dict:  {'classification_loss': 0.9337284910678864}
2025-01-15 20:52:56,622 [INFO] Step[2300/2713]: training loss : 0.9313726711273194 TRAIN  loss dict:  {'classification_loss': 0.9313726711273194}
2025-01-15 20:53:08,530 [INFO] Step[2350/2713]: training loss : 0.9388418757915497 TRAIN  loss dict:  {'classification_loss': 0.9388418757915497}
2025-01-15 20:53:20,408 [INFO] Step[2400/2713]: training loss : 1.0180290782451629 TRAIN  loss dict:  {'classification_loss': 1.0180290782451629}
2025-01-15 20:53:32,323 [INFO] Step[2450/2713]: training loss : 0.9308523333072662 TRAIN  loss dict:  {'classification_loss': 0.9308523333072662}
2025-01-15 20:53:44,195 [INFO] Step[2500/2713]: training loss : 0.9785844349861145 TRAIN  loss dict:  {'classification_loss': 0.9785844349861145}
2025-01-15 20:53:56,082 [INFO] Step[2550/2713]: training loss : 0.9401237976551056 TRAIN  loss dict:  {'classification_loss': 0.9401237976551056}
2025-01-15 20:54:07,981 [INFO] Step[2600/2713]: training loss : 0.9307583808898926 TRAIN  loss dict:  {'classification_loss': 0.9307583808898926}
2025-01-15 20:54:19,876 [INFO] Step[2650/2713]: training loss : 0.9471812546253204 TRAIN  loss dict:  {'classification_loss': 0.9471812546253204}
2025-01-15 20:54:31,757 [INFO] Step[2700/2713]: training loss : 0.933174387216568 TRAIN  loss dict:  {'classification_loss': 0.933174387216568}
2025-01-15 20:55:44,360 [INFO] Label accuracies statistics:
2025-01-15 20:55:44,360 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 1.0, 4: 0.5, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 0.5, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 1.0, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.75, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.5, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.25, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.5, 184: 0.75, 185: 0.75, 186: 0.75, 187: 0.75, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.0, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.25, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 0.5, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.5, 325: 0.75, 326: 0.5, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 0.75, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.5, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.5, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.25, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.5, 384: 0.75, 385: 0.5, 386: 1.0, 387: 0.5, 388: 0.25, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 20:55:44,362 [INFO] [30] TRAIN  loss: 0.9469727324551864 acc: 0.9952082565425728
2025-01-15 20:55:44,362 [INFO] [30] TRAIN  loss dict: {'classification_loss': 0.9469727324551864}
2025-01-15 20:55:44,362 [INFO] [30] VALIDATION loss: 2.046826790149947 VALIDATION acc: 0.7617554858934169
2025-01-15 20:55:44,362 [INFO] [30] VALIDATION loss dict: {'classification_loss': 2.046826790149947}
2025-01-15 20:55:44,362 [INFO] 
2025-01-15 20:56:01,327 [INFO] Step[50/2713]: training loss : 0.9374613630771637 TRAIN  loss dict:  {'classification_loss': 0.9374613630771637}
2025-01-15 20:56:13,170 [INFO] Step[100/2713]: training loss : 0.9676350212097168 TRAIN  loss dict:  {'classification_loss': 0.9676350212097168}
2025-01-15 20:56:25,078 [INFO] Step[150/2713]: training loss : 0.9340378332138062 TRAIN  loss dict:  {'classification_loss': 0.9340378332138062}
2025-01-15 20:56:37,007 [INFO] Step[200/2713]: training loss : 0.98374156832695 TRAIN  loss dict:  {'classification_loss': 0.98374156832695}
2025-01-15 20:56:48,931 [INFO] Step[250/2713]: training loss : 0.9445950508117675 TRAIN  loss dict:  {'classification_loss': 0.9445950508117675}
2025-01-15 20:57:00,825 [INFO] Step[300/2713]: training loss : 0.957580736875534 TRAIN  loss dict:  {'classification_loss': 0.957580736875534}
2025-01-15 20:57:12,752 [INFO] Step[350/2713]: training loss : 0.9392284786701203 TRAIN  loss dict:  {'classification_loss': 0.9392284786701203}
2025-01-15 20:57:24,697 [INFO] Step[400/2713]: training loss : 0.9406333494186402 TRAIN  loss dict:  {'classification_loss': 0.9406333494186402}
2025-01-15 20:57:36,589 [INFO] Step[450/2713]: training loss : 0.9331163740158082 TRAIN  loss dict:  {'classification_loss': 0.9331163740158082}
2025-01-15 20:57:48,467 [INFO] Step[500/2713]: training loss : 0.9388244521617889 TRAIN  loss dict:  {'classification_loss': 0.9388244521617889}
2025-01-15 20:58:00,383 [INFO] Step[550/2713]: training loss : 0.938498488664627 TRAIN  loss dict:  {'classification_loss': 0.938498488664627}
2025-01-15 20:58:12,291 [INFO] Step[600/2713]: training loss : 0.9303162109851837 TRAIN  loss dict:  {'classification_loss': 0.9303162109851837}
2025-01-15 20:58:24,286 [INFO] Step[650/2713]: training loss : 0.9304712998867035 TRAIN  loss dict:  {'classification_loss': 0.9304712998867035}
2025-01-15 20:58:36,201 [INFO] Step[700/2713]: training loss : 0.9448277270793914 TRAIN  loss dict:  {'classification_loss': 0.9448277270793914}
2025-01-15 20:58:48,134 [INFO] Step[750/2713]: training loss : 0.929308363199234 TRAIN  loss dict:  {'classification_loss': 0.929308363199234}
2025-01-15 20:59:00,083 [INFO] Step[800/2713]: training loss : 0.9545512282848359 TRAIN  loss dict:  {'classification_loss': 0.9545512282848359}
2025-01-15 20:59:12,033 [INFO] Step[850/2713]: training loss : 0.958094391822815 TRAIN  loss dict:  {'classification_loss': 0.958094391822815}
2025-01-15 20:59:23,926 [INFO] Step[900/2713]: training loss : 0.953318110704422 TRAIN  loss dict:  {'classification_loss': 0.953318110704422}
2025-01-15 20:59:35,887 [INFO] Step[950/2713]: training loss : 0.9318776392936706 TRAIN  loss dict:  {'classification_loss': 0.9318776392936706}
2025-01-15 20:59:47,755 [INFO] Step[1000/2713]: training loss : 0.9319152879714966 TRAIN  loss dict:  {'classification_loss': 0.9319152879714966}
2025-01-15 20:59:59,673 [INFO] Step[1050/2713]: training loss : 0.932245626449585 TRAIN  loss dict:  {'classification_loss': 0.932245626449585}
2025-01-15 21:00:11,553 [INFO] Step[1100/2713]: training loss : 0.948554949760437 TRAIN  loss dict:  {'classification_loss': 0.948554949760437}
2025-01-15 21:00:23,498 [INFO] Step[1150/2713]: training loss : 0.9449679172039032 TRAIN  loss dict:  {'classification_loss': 0.9449679172039032}
2025-01-15 21:00:35,400 [INFO] Step[1200/2713]: training loss : 0.9345406496524811 TRAIN  loss dict:  {'classification_loss': 0.9345406496524811}
2025-01-15 21:00:47,324 [INFO] Step[1250/2713]: training loss : 0.9291005420684815 TRAIN  loss dict:  {'classification_loss': 0.9291005420684815}
2025-01-15 21:00:59,224 [INFO] Step[1300/2713]: training loss : 0.9326328659057617 TRAIN  loss dict:  {'classification_loss': 0.9326328659057617}
2025-01-15 21:01:11,118 [INFO] Step[1350/2713]: training loss : 0.9436900925636291 TRAIN  loss dict:  {'classification_loss': 0.9436900925636291}
2025-01-15 21:01:23,044 [INFO] Step[1400/2713]: training loss : 0.9474567890167236 TRAIN  loss dict:  {'classification_loss': 0.9474567890167236}
2025-01-15 21:01:34,935 [INFO] Step[1450/2713]: training loss : 0.9421093189716339 TRAIN  loss dict:  {'classification_loss': 0.9421093189716339}
2025-01-15 21:01:46,831 [INFO] Step[1500/2713]: training loss : 0.9736880660057068 TRAIN  loss dict:  {'classification_loss': 0.9736880660057068}
2025-01-15 21:01:58,745 [INFO] Step[1550/2713]: training loss : 0.9292470407485962 TRAIN  loss dict:  {'classification_loss': 0.9292470407485962}
2025-01-15 21:02:10,624 [INFO] Step[1600/2713]: training loss : 0.9503196489810943 TRAIN  loss dict:  {'classification_loss': 0.9503196489810943}
2025-01-15 21:02:22,559 [INFO] Step[1650/2713]: training loss : 0.9292310357093811 TRAIN  loss dict:  {'classification_loss': 0.9292310357093811}
2025-01-15 21:02:34,448 [INFO] Step[1700/2713]: training loss : 0.9287139809131623 TRAIN  loss dict:  {'classification_loss': 0.9287139809131623}
2025-01-15 21:02:46,388 [INFO] Step[1750/2713]: training loss : 0.9325580441951752 TRAIN  loss dict:  {'classification_loss': 0.9325580441951752}
2025-01-15 21:02:58,294 [INFO] Step[1800/2713]: training loss : 0.9735089921951294 TRAIN  loss dict:  {'classification_loss': 0.9735089921951294}
2025-01-15 21:03:10,175 [INFO] Step[1850/2713]: training loss : 0.9287302696704864 TRAIN  loss dict:  {'classification_loss': 0.9287302696704864}
2025-01-15 21:03:22,108 [INFO] Step[1900/2713]: training loss : 0.9511624348163604 TRAIN  loss dict:  {'classification_loss': 0.9511624348163604}
2025-01-15 21:03:34,003 [INFO] Step[1950/2713]: training loss : 0.9291775560379029 TRAIN  loss dict:  {'classification_loss': 0.9291775560379029}
2025-01-15 21:03:45,913 [INFO] Step[2000/2713]: training loss : 0.9358060777187347 TRAIN  loss dict:  {'classification_loss': 0.9358060777187347}
2025-01-15 21:03:57,800 [INFO] Step[2050/2713]: training loss : 0.9548857855796814 TRAIN  loss dict:  {'classification_loss': 0.9548857855796814}
2025-01-15 21:04:09,696 [INFO] Step[2100/2713]: training loss : 0.9310781705379486 TRAIN  loss dict:  {'classification_loss': 0.9310781705379486}
2025-01-15 21:04:21,619 [INFO] Step[2150/2713]: training loss : 0.9305963158607483 TRAIN  loss dict:  {'classification_loss': 0.9305963158607483}
2025-01-15 21:04:33,503 [INFO] Step[2200/2713]: training loss : 0.9287788891792297 TRAIN  loss dict:  {'classification_loss': 0.9287788891792297}
2025-01-15 21:04:45,412 [INFO] Step[2250/2713]: training loss : 0.9475486373901367 TRAIN  loss dict:  {'classification_loss': 0.9475486373901367}
2025-01-15 21:04:57,324 [INFO] Step[2300/2713]: training loss : 0.9867170310020447 TRAIN  loss dict:  {'classification_loss': 0.9867170310020447}
2025-01-15 21:05:09,206 [INFO] Step[2350/2713]: training loss : 0.9523668670654297 TRAIN  loss dict:  {'classification_loss': 0.9523668670654297}
2025-01-15 21:05:21,079 [INFO] Step[2400/2713]: training loss : 0.9299399280548095 TRAIN  loss dict:  {'classification_loss': 0.9299399280548095}
2025-01-15 21:05:32,993 [INFO] Step[2450/2713]: training loss : 0.9316267132759094 TRAIN  loss dict:  {'classification_loss': 0.9316267132759094}
2025-01-15 21:05:44,879 [INFO] Step[2500/2713]: training loss : 0.9673216164112091 TRAIN  loss dict:  {'classification_loss': 0.9673216164112091}
2025-01-15 21:05:56,828 [INFO] Step[2550/2713]: training loss : 0.9313475215435028 TRAIN  loss dict:  {'classification_loss': 0.9313475215435028}
2025-01-15 21:06:08,726 [INFO] Step[2600/2713]: training loss : 0.9802826404571533 TRAIN  loss dict:  {'classification_loss': 0.9802826404571533}
2025-01-15 21:06:20,648 [INFO] Step[2650/2713]: training loss : 0.9362802958488464 TRAIN  loss dict:  {'classification_loss': 0.9362802958488464}
2025-01-15 21:06:32,557 [INFO] Step[2700/2713]: training loss : 0.9413616144657135 TRAIN  loss dict:  {'classification_loss': 0.9413616144657135}
2025-01-15 21:08:03,145 [INFO] Label accuracies statistics:
2025-01-15 21:08:03,145 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.5, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.25, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 0.75, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.25, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.25, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.5, 245: 0.75, 246: 0.75, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 0.75, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.25, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.5, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.5, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.25, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.25, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.75, 378: 0.25, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.25, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 21:08:03,147 [INFO] [31] TRAIN  loss: 0.943402056649212 acc: 0.9952082565425728
2025-01-15 21:08:03,147 [INFO] [31] TRAIN  loss dict: {'classification_loss': 0.943402056649212}
2025-01-15 21:08:03,147 [INFO] [31] VALIDATION loss: 2.012038722858393 VALIDATION acc: 0.768025078369906
2025-01-15 21:08:03,147 [INFO] [31] VALIDATION loss dict: {'classification_loss': 2.012038722858393}
2025-01-15 21:08:03,147 [INFO] 
2025-01-15 21:08:19,517 [INFO] Step[50/2713]: training loss : 0.930243706703186 TRAIN  loss dict:  {'classification_loss': 0.930243706703186}
2025-01-15 21:08:31,385 [INFO] Step[100/2713]: training loss : 0.9334013092517853 TRAIN  loss dict:  {'classification_loss': 0.9334013092517853}
2025-01-15 21:08:43,280 [INFO] Step[150/2713]: training loss : 0.9293465912342072 TRAIN  loss dict:  {'classification_loss': 0.9293465912342072}
2025-01-15 21:08:55,179 [INFO] Step[200/2713]: training loss : 0.9711029160022736 TRAIN  loss dict:  {'classification_loss': 0.9711029160022736}
2025-01-15 21:09:07,137 [INFO] Step[250/2713]: training loss : 0.9367329120635987 TRAIN  loss dict:  {'classification_loss': 0.9367329120635987}
2025-01-15 21:09:19,054 [INFO] Step[300/2713]: training loss : 0.9286557137966156 TRAIN  loss dict:  {'classification_loss': 0.9286557137966156}
2025-01-15 21:09:30,984 [INFO] Step[350/2713]: training loss : 0.9543758845329284 TRAIN  loss dict:  {'classification_loss': 0.9543758845329284}
2025-01-15 21:09:42,885 [INFO] Step[400/2713]: training loss : 0.9313800466060639 TRAIN  loss dict:  {'classification_loss': 0.9313800466060639}
2025-01-15 21:09:54,879 [INFO] Step[450/2713]: training loss : 0.9349750423431397 TRAIN  loss dict:  {'classification_loss': 0.9349750423431397}
2025-01-15 21:10:06,789 [INFO] Step[500/2713]: training loss : 0.9305498266220092 TRAIN  loss dict:  {'classification_loss': 0.9305498266220092}
2025-01-15 21:10:18,697 [INFO] Step[550/2713]: training loss : 0.9517131626605988 TRAIN  loss dict:  {'classification_loss': 0.9517131626605988}
2025-01-15 21:10:30,624 [INFO] Step[600/2713]: training loss : 0.9293743014335633 TRAIN  loss dict:  {'classification_loss': 0.9293743014335633}
2025-01-15 21:10:42,542 [INFO] Step[650/2713]: training loss : 0.9883825385570526 TRAIN  loss dict:  {'classification_loss': 0.9883825385570526}
2025-01-15 21:10:54,511 [INFO] Step[700/2713]: training loss : 0.9300918650627136 TRAIN  loss dict:  {'classification_loss': 0.9300918650627136}
2025-01-15 21:11:06,453 [INFO] Step[750/2713]: training loss : 0.9646078205108642 TRAIN  loss dict:  {'classification_loss': 0.9646078205108642}
2025-01-15 21:11:18,395 [INFO] Step[800/2713]: training loss : 0.936451563835144 TRAIN  loss dict:  {'classification_loss': 0.936451563835144}
2025-01-15 21:11:30,320 [INFO] Step[850/2713]: training loss : 0.9287949943542481 TRAIN  loss dict:  {'classification_loss': 0.9287949943542481}
2025-01-15 21:11:42,209 [INFO] Step[900/2713]: training loss : 0.9338554060459137 TRAIN  loss dict:  {'classification_loss': 0.9338554060459137}
2025-01-15 21:11:54,148 [INFO] Step[950/2713]: training loss : 0.9486640906333923 TRAIN  loss dict:  {'classification_loss': 0.9486640906333923}
2025-01-15 21:12:06,093 [INFO] Step[1000/2713]: training loss : 0.929782794713974 TRAIN  loss dict:  {'classification_loss': 0.929782794713974}
2025-01-15 21:12:18,011 [INFO] Step[1050/2713]: training loss : 0.927754944562912 TRAIN  loss dict:  {'classification_loss': 0.927754944562912}
2025-01-15 21:12:29,948 [INFO] Step[1100/2713]: training loss : 0.9289442825317383 TRAIN  loss dict:  {'classification_loss': 0.9289442825317383}
2025-01-15 21:12:41,855 [INFO] Step[1150/2713]: training loss : 0.9408385360240936 TRAIN  loss dict:  {'classification_loss': 0.9408385360240936}
2025-01-15 21:12:53,763 [INFO] Step[1200/2713]: training loss : 0.9348640394210815 TRAIN  loss dict:  {'classification_loss': 0.9348640394210815}
2025-01-15 21:13:05,680 [INFO] Step[1250/2713]: training loss : 0.933620023727417 TRAIN  loss dict:  {'classification_loss': 0.933620023727417}
2025-01-15 21:13:17,609 [INFO] Step[1300/2713]: training loss : 0.9317201888561248 TRAIN  loss dict:  {'classification_loss': 0.9317201888561248}
2025-01-15 21:13:29,538 [INFO] Step[1350/2713]: training loss : 0.9369263064861297 TRAIN  loss dict:  {'classification_loss': 0.9369263064861297}
2025-01-15 21:13:41,474 [INFO] Step[1400/2713]: training loss : 0.929276112318039 TRAIN  loss dict:  {'classification_loss': 0.929276112318039}
2025-01-15 21:13:53,383 [INFO] Step[1450/2713]: training loss : 0.9317906785011292 TRAIN  loss dict:  {'classification_loss': 0.9317906785011292}
2025-01-15 21:14:05,321 [INFO] Step[1500/2713]: training loss : 0.9291880655288697 TRAIN  loss dict:  {'classification_loss': 0.9291880655288697}
2025-01-15 21:14:17,266 [INFO] Step[1550/2713]: training loss : 0.9344388508796692 TRAIN  loss dict:  {'classification_loss': 0.9344388508796692}
2025-01-15 21:14:29,193 [INFO] Step[1600/2713]: training loss : 0.9345623302459717 TRAIN  loss dict:  {'classification_loss': 0.9345623302459717}
2025-01-15 21:14:41,102 [INFO] Step[1650/2713]: training loss : 0.9405970096588134 TRAIN  loss dict:  {'classification_loss': 0.9405970096588134}
2025-01-15 21:14:53,036 [INFO] Step[1700/2713]: training loss : 0.9375456261634827 TRAIN  loss dict:  {'classification_loss': 0.9375456261634827}
2025-01-15 21:15:04,970 [INFO] Step[1750/2713]: training loss : 0.9292848706245422 TRAIN  loss dict:  {'classification_loss': 0.9292848706245422}
2025-01-15 21:15:16,888 [INFO] Step[1800/2713]: training loss : 0.9289493381977081 TRAIN  loss dict:  {'classification_loss': 0.9289493381977081}
2025-01-15 21:15:28,810 [INFO] Step[1850/2713]: training loss : 0.9312344527244568 TRAIN  loss dict:  {'classification_loss': 0.9312344527244568}
2025-01-15 21:15:40,728 [INFO] Step[1900/2713]: training loss : 0.9296099030971527 TRAIN  loss dict:  {'classification_loss': 0.9296099030971527}
2025-01-15 21:15:52,672 [INFO] Step[1950/2713]: training loss : 0.9319359481334686 TRAIN  loss dict:  {'classification_loss': 0.9319359481334686}
2025-01-15 21:16:04,636 [INFO] Step[2000/2713]: training loss : 0.9323570489883423 TRAIN  loss dict:  {'classification_loss': 0.9323570489883423}
2025-01-15 21:16:16,601 [INFO] Step[2050/2713]: training loss : 0.931059068441391 TRAIN  loss dict:  {'classification_loss': 0.931059068441391}
2025-01-15 21:16:28,543 [INFO] Step[2100/2713]: training loss : 0.938835529088974 TRAIN  loss dict:  {'classification_loss': 0.938835529088974}
2025-01-15 21:16:40,432 [INFO] Step[2150/2713]: training loss : 0.9313973903656005 TRAIN  loss dict:  {'classification_loss': 0.9313973903656005}
2025-01-15 21:16:52,339 [INFO] Step[2200/2713]: training loss : 0.9356398272514344 TRAIN  loss dict:  {'classification_loss': 0.9356398272514344}
2025-01-15 21:17:04,261 [INFO] Step[2250/2713]: training loss : 0.9568150067329406 TRAIN  loss dict:  {'classification_loss': 0.9568150067329406}
2025-01-15 21:17:16,142 [INFO] Step[2300/2713]: training loss : 0.9316055274009705 TRAIN  loss dict:  {'classification_loss': 0.9316055274009705}
2025-01-15 21:17:28,073 [INFO] Step[2350/2713]: training loss : 0.9288374614715577 TRAIN  loss dict:  {'classification_loss': 0.9288374614715577}
2025-01-15 21:17:39,989 [INFO] Step[2400/2713]: training loss : 0.9303886675834656 TRAIN  loss dict:  {'classification_loss': 0.9303886675834656}
2025-01-15 21:17:51,928 [INFO] Step[2450/2713]: training loss : 0.9291871070861817 TRAIN  loss dict:  {'classification_loss': 0.9291871070861817}
2025-01-15 21:18:03,846 [INFO] Step[2500/2713]: training loss : 0.930846756696701 TRAIN  loss dict:  {'classification_loss': 0.930846756696701}
2025-01-15 21:18:15,783 [INFO] Step[2550/2713]: training loss : 0.9355487370491028 TRAIN  loss dict:  {'classification_loss': 0.9355487370491028}
2025-01-15 21:18:27,671 [INFO] Step[2600/2713]: training loss : 0.9334981298446655 TRAIN  loss dict:  {'classification_loss': 0.9334981298446655}
2025-01-15 21:18:39,607 [INFO] Step[2650/2713]: training loss : 0.9285334813594818 TRAIN  loss dict:  {'classification_loss': 0.9285334813594818}
2025-01-15 21:18:51,511 [INFO] Step[2700/2713]: training loss : 0.9290076959133148 TRAIN  loss dict:  {'classification_loss': 0.9290076959133148}
2025-01-15 21:20:04,543 [INFO] Label accuracies statistics:
2025-01-15 21:20:04,543 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.5, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.25, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.25, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 0.75, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.5, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 0.75, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.5, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.25, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 0.75, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.5, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.5, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.5, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 21:20:04,545 [INFO] [32] TRAIN  loss: 0.9360610833697067 acc: 0.9982798869640005
2025-01-15 21:20:04,545 [INFO] [32] TRAIN  loss dict: {'classification_loss': 0.9360610833697067}
2025-01-15 21:20:04,545 [INFO] [32] VALIDATION loss: 2.037491520432601 VALIDATION acc: 0.7605015673981191
2025-01-15 21:20:04,545 [INFO] [32] VALIDATION loss dict: {'classification_loss': 2.037491520432601}
2025-01-15 21:20:04,545 [INFO] 
2025-01-15 21:20:21,408 [INFO] Step[50/2713]: training loss : 0.9284749698638916 TRAIN  loss dict:  {'classification_loss': 0.9284749698638916}
2025-01-15 21:20:33,258 [INFO] Step[100/2713]: training loss : 0.9384601187705993 TRAIN  loss dict:  {'classification_loss': 0.9384601187705993}
2025-01-15 21:20:45,247 [INFO] Step[150/2713]: training loss : 0.9321367597579956 TRAIN  loss dict:  {'classification_loss': 0.9321367597579956}
2025-01-15 21:20:57,144 [INFO] Step[200/2713]: training loss : 0.9390580916404724 TRAIN  loss dict:  {'classification_loss': 0.9390580916404724}
2025-01-15 21:21:09,063 [INFO] Step[250/2713]: training loss : 0.9300403416156768 TRAIN  loss dict:  {'classification_loss': 0.9300403416156768}
2025-01-15 21:21:20,964 [INFO] Step[300/2713]: training loss : 0.9322909939289094 TRAIN  loss dict:  {'classification_loss': 0.9322909939289094}
2025-01-15 21:21:32,883 [INFO] Step[350/2713]: training loss : 0.931721202135086 TRAIN  loss dict:  {'classification_loss': 0.931721202135086}
2025-01-15 21:21:44,847 [INFO] Step[400/2713]: training loss : 0.9888875448703766 TRAIN  loss dict:  {'classification_loss': 0.9888875448703766}
2025-01-15 21:21:56,792 [INFO] Step[450/2713]: training loss : 0.975311735868454 TRAIN  loss dict:  {'classification_loss': 0.975311735868454}
2025-01-15 21:22:08,709 [INFO] Step[500/2713]: training loss : 0.9306053578853607 TRAIN  loss dict:  {'classification_loss': 0.9306053578853607}
2025-01-15 21:22:20,638 [INFO] Step[550/2713]: training loss : 0.9314474213123322 TRAIN  loss dict:  {'classification_loss': 0.9314474213123322}
2025-01-15 21:22:32,571 [INFO] Step[600/2713]: training loss : 0.9467677819728851 TRAIN  loss dict:  {'classification_loss': 0.9467677819728851}
2025-01-15 21:22:44,512 [INFO] Step[650/2713]: training loss : 0.928983005285263 TRAIN  loss dict:  {'classification_loss': 0.928983005285263}
2025-01-15 21:22:56,444 [INFO] Step[700/2713]: training loss : 0.9287021458148956 TRAIN  loss dict:  {'classification_loss': 0.9287021458148956}
2025-01-15 21:23:08,370 [INFO] Step[750/2713]: training loss : 0.9295195257663726 TRAIN  loss dict:  {'classification_loss': 0.9295195257663726}
2025-01-15 21:23:20,293 [INFO] Step[800/2713]: training loss : 0.9286922407150269 TRAIN  loss dict:  {'classification_loss': 0.9286922407150269}
2025-01-15 21:23:32,232 [INFO] Step[850/2713]: training loss : 0.933033983707428 TRAIN  loss dict:  {'classification_loss': 0.933033983707428}
2025-01-15 21:23:44,131 [INFO] Step[900/2713]: training loss : 0.9288246154785156 TRAIN  loss dict:  {'classification_loss': 0.9288246154785156}
2025-01-15 21:23:56,108 [INFO] Step[950/2713]: training loss : 0.939253317117691 TRAIN  loss dict:  {'classification_loss': 0.939253317117691}
2025-01-15 21:24:08,001 [INFO] Step[1000/2713]: training loss : 0.967003562450409 TRAIN  loss dict:  {'classification_loss': 0.967003562450409}
2025-01-15 21:24:19,920 [INFO] Step[1050/2713]: training loss : 0.9320000898838043 TRAIN  loss dict:  {'classification_loss': 0.9320000898838043}
2025-01-15 21:24:31,871 [INFO] Step[1100/2713]: training loss : 0.9328739261627197 TRAIN  loss dict:  {'classification_loss': 0.9328739261627197}
2025-01-15 21:24:43,805 [INFO] Step[1150/2713]: training loss : 0.9282725763320923 TRAIN  loss dict:  {'classification_loss': 0.9282725763320923}
2025-01-15 21:24:55,710 [INFO] Step[1200/2713]: training loss : 0.9377697730064392 TRAIN  loss dict:  {'classification_loss': 0.9377697730064392}
2025-01-15 21:25:07,637 [INFO] Step[1250/2713]: training loss : 0.9508071398735046 TRAIN  loss dict:  {'classification_loss': 0.9508071398735046}
2025-01-15 21:25:19,532 [INFO] Step[1300/2713]: training loss : 0.9298748624324799 TRAIN  loss dict:  {'classification_loss': 0.9298748624324799}
2025-01-15 21:25:31,487 [INFO] Step[1350/2713]: training loss : 0.9293204736709595 TRAIN  loss dict:  {'classification_loss': 0.9293204736709595}
2025-01-15 21:25:43,405 [INFO] Step[1400/2713]: training loss : 0.9392057240009308 TRAIN  loss dict:  {'classification_loss': 0.9392057240009308}
2025-01-15 21:25:55,294 [INFO] Step[1450/2713]: training loss : 0.9552039790153504 TRAIN  loss dict:  {'classification_loss': 0.9552039790153504}
2025-01-15 21:26:07,235 [INFO] Step[1500/2713]: training loss : 0.9282499480247498 TRAIN  loss dict:  {'classification_loss': 0.9282499480247498}
2025-01-15 21:26:19,175 [INFO] Step[1550/2713]: training loss : 0.9477004539966584 TRAIN  loss dict:  {'classification_loss': 0.9477004539966584}
2025-01-15 21:26:31,090 [INFO] Step[1600/2713]: training loss : 0.9390289914608002 TRAIN  loss dict:  {'classification_loss': 0.9390289914608002}
2025-01-15 21:26:43,018 [INFO] Step[1650/2713]: training loss : 0.9762911522388458 TRAIN  loss dict:  {'classification_loss': 0.9762911522388458}
2025-01-15 21:26:54,975 [INFO] Step[1700/2713]: training loss : 0.9310704946517945 TRAIN  loss dict:  {'classification_loss': 0.9310704946517945}
2025-01-15 21:27:06,917 [INFO] Step[1750/2713]: training loss : 0.9291974174976348 TRAIN  loss dict:  {'classification_loss': 0.9291974174976348}
2025-01-15 21:27:18,880 [INFO] Step[1800/2713]: training loss : 0.9284220600128174 TRAIN  loss dict:  {'classification_loss': 0.9284220600128174}
2025-01-15 21:27:30,775 [INFO] Step[1850/2713]: training loss : 0.9921256792545319 TRAIN  loss dict:  {'classification_loss': 0.9921256792545319}
2025-01-15 21:27:42,714 [INFO] Step[1900/2713]: training loss : 0.9298019099235535 TRAIN  loss dict:  {'classification_loss': 0.9298019099235535}
2025-01-15 21:27:54,636 [INFO] Step[1950/2713]: training loss : 0.9313776135444641 TRAIN  loss dict:  {'classification_loss': 0.9313776135444641}
2025-01-15 21:28:06,533 [INFO] Step[2000/2713]: training loss : 0.9304003202915192 TRAIN  loss dict:  {'classification_loss': 0.9304003202915192}
2025-01-15 21:28:18,427 [INFO] Step[2050/2713]: training loss : 0.929744610786438 TRAIN  loss dict:  {'classification_loss': 0.929744610786438}
2025-01-15 21:28:30,370 [INFO] Step[2100/2713]: training loss : 0.9297806787490844 TRAIN  loss dict:  {'classification_loss': 0.9297806787490844}
2025-01-15 21:28:42,292 [INFO] Step[2150/2713]: training loss : 0.9314789366722107 TRAIN  loss dict:  {'classification_loss': 0.9314789366722107}
2025-01-15 21:28:54,168 [INFO] Step[2200/2713]: training loss : 0.9570044994354248 TRAIN  loss dict:  {'classification_loss': 0.9570044994354248}
2025-01-15 21:29:06,051 [INFO] Step[2250/2713]: training loss : 0.934615478515625 TRAIN  loss dict:  {'classification_loss': 0.934615478515625}
2025-01-15 21:29:17,975 [INFO] Step[2300/2713]: training loss : 0.9280641865730286 TRAIN  loss dict:  {'classification_loss': 0.9280641865730286}
2025-01-15 21:29:29,890 [INFO] Step[2350/2713]: training loss : 0.9667048883438111 TRAIN  loss dict:  {'classification_loss': 0.9667048883438111}
2025-01-15 21:29:41,809 [INFO] Step[2400/2713]: training loss : 1.0060402655601501 TRAIN  loss dict:  {'classification_loss': 1.0060402655601501}
2025-01-15 21:29:53,752 [INFO] Step[2450/2713]: training loss : 0.9446345365047455 TRAIN  loss dict:  {'classification_loss': 0.9446345365047455}
2025-01-15 21:30:05,653 [INFO] Step[2500/2713]: training loss : 0.9293762910366058 TRAIN  loss dict:  {'classification_loss': 0.9293762910366058}
2025-01-15 21:30:17,579 [INFO] Step[2550/2713]: training loss : 0.9309619235992431 TRAIN  loss dict:  {'classification_loss': 0.9309619235992431}
2025-01-15 21:30:29,552 [INFO] Step[2600/2713]: training loss : 0.9490849983692169 TRAIN  loss dict:  {'classification_loss': 0.9490849983692169}
2025-01-15 21:30:41,467 [INFO] Step[2650/2713]: training loss : 0.945254716873169 TRAIN  loss dict:  {'classification_loss': 0.945254716873169}
2025-01-15 21:30:53,364 [INFO] Step[2700/2713]: training loss : 0.9282735693454742 TRAIN  loss dict:  {'classification_loss': 0.9282735693454742}
2025-01-15 21:32:06,198 [INFO] Label accuracies statistics:
2025-01-15 21:32:06,198 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.25, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.25, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 0.5, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 0.5, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 1.0, 152: 1.0, 153: 0.25, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 0.75, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.5, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 1.0, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 1.0, 261: 0.75, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 1.0, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.5, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 1.0, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.5, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.25, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.5, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 21:32:06,200 [INFO] [33] TRAIN  loss: 0.9406633101790934 acc: 0.9970512347954295
2025-01-15 21:32:06,200 [INFO] [33] TRAIN  loss dict: {'classification_loss': 0.9406633101790934}
2025-01-15 21:32:06,200 [INFO] [33] VALIDATION loss: 2.001630500519186 VALIDATION acc: 0.7692789968652037
2025-01-15 21:32:06,200 [INFO] [33] VALIDATION loss dict: {'classification_loss': 2.001630500519186}
2025-01-15 21:32:06,200 [INFO] 
2025-01-15 21:32:23,347 [INFO] Step[50/2713]: training loss : 0.9516364562511445 TRAIN  loss dict:  {'classification_loss': 0.9516364562511445}
2025-01-15 21:32:35,191 [INFO] Step[100/2713]: training loss : 0.9476288843154907 TRAIN  loss dict:  {'classification_loss': 0.9476288843154907}
2025-01-15 21:32:47,076 [INFO] Step[150/2713]: training loss : 0.996204423904419 TRAIN  loss dict:  {'classification_loss': 0.996204423904419}
2025-01-15 21:32:58,947 [INFO] Step[200/2713]: training loss : 0.9580073678493499 TRAIN  loss dict:  {'classification_loss': 0.9580073678493499}
2025-01-15 21:33:10,840 [INFO] Step[250/2713]: training loss : 0.9296101760864258 TRAIN  loss dict:  {'classification_loss': 0.9296101760864258}
2025-01-15 21:33:22,752 [INFO] Step[300/2713]: training loss : 0.9293628358840942 TRAIN  loss dict:  {'classification_loss': 0.9293628358840942}
2025-01-15 21:33:34,701 [INFO] Step[350/2713]: training loss : 0.983970913887024 TRAIN  loss dict:  {'classification_loss': 0.983970913887024}
2025-01-15 21:33:46,606 [INFO] Step[400/2713]: training loss : 0.942300934791565 TRAIN  loss dict:  {'classification_loss': 0.942300934791565}
2025-01-15 21:33:58,526 [INFO] Step[450/2713]: training loss : 0.9343128037452698 TRAIN  loss dict:  {'classification_loss': 0.9343128037452698}
2025-01-15 21:34:10,441 [INFO] Step[500/2713]: training loss : 1.0163066673278809 TRAIN  loss dict:  {'classification_loss': 1.0163066673278809}
2025-01-15 21:34:22,412 [INFO] Step[550/2713]: training loss : 0.979665778875351 TRAIN  loss dict:  {'classification_loss': 0.979665778875351}
2025-01-15 21:34:34,323 [INFO] Step[600/2713]: training loss : 0.9570380902290344 TRAIN  loss dict:  {'classification_loss': 0.9570380902290344}
2025-01-15 21:34:46,256 [INFO] Step[650/2713]: training loss : 0.9292456662654877 TRAIN  loss dict:  {'classification_loss': 0.9292456662654877}
2025-01-15 21:34:58,161 [INFO] Step[700/2713]: training loss : 0.928629823923111 TRAIN  loss dict:  {'classification_loss': 0.928629823923111}
2025-01-15 21:35:10,075 [INFO] Step[750/2713]: training loss : 0.9641731774806976 TRAIN  loss dict:  {'classification_loss': 0.9641731774806976}
2025-01-15 21:35:21,973 [INFO] Step[800/2713]: training loss : 0.9301346337795258 TRAIN  loss dict:  {'classification_loss': 0.9301346337795258}
2025-01-15 21:35:33,930 [INFO] Step[850/2713]: training loss : 0.9428679776191712 TRAIN  loss dict:  {'classification_loss': 0.9428679776191712}
2025-01-15 21:35:45,852 [INFO] Step[900/2713]: training loss : 0.9466692888736725 TRAIN  loss dict:  {'classification_loss': 0.9466692888736725}
2025-01-15 21:35:57,776 [INFO] Step[950/2713]: training loss : 0.9299723172187805 TRAIN  loss dict:  {'classification_loss': 0.9299723172187805}
2025-01-15 21:36:09,718 [INFO] Step[1000/2713]: training loss : 0.9306040930747986 TRAIN  loss dict:  {'classification_loss': 0.9306040930747986}
2025-01-15 21:36:21,636 [INFO] Step[1050/2713]: training loss : 0.930646402835846 TRAIN  loss dict:  {'classification_loss': 0.930646402835846}
2025-01-15 21:36:33,552 [INFO] Step[1100/2713]: training loss : 0.9296470081806183 TRAIN  loss dict:  {'classification_loss': 0.9296470081806183}
2025-01-15 21:36:45,476 [INFO] Step[1150/2713]: training loss : 0.9325882482528687 TRAIN  loss dict:  {'classification_loss': 0.9325882482528687}
2025-01-15 21:36:57,383 [INFO] Step[1200/2713]: training loss : 0.93199786901474 TRAIN  loss dict:  {'classification_loss': 0.93199786901474}
2025-01-15 21:37:09,303 [INFO] Step[1250/2713]: training loss : 0.9768210291862488 TRAIN  loss dict:  {'classification_loss': 0.9768210291862488}
2025-01-15 21:37:21,191 [INFO] Step[1300/2713]: training loss : 0.9327633142471313 TRAIN  loss dict:  {'classification_loss': 0.9327633142471313}
2025-01-15 21:37:33,159 [INFO] Step[1350/2713]: training loss : 0.9567875075340271 TRAIN  loss dict:  {'classification_loss': 0.9567875075340271}
2025-01-15 21:37:45,110 [INFO] Step[1400/2713]: training loss : 0.9428969728946686 TRAIN  loss dict:  {'classification_loss': 0.9428969728946686}
2025-01-15 21:37:57,085 [INFO] Step[1450/2713]: training loss : 0.9310096740722656 TRAIN  loss dict:  {'classification_loss': 0.9310096740722656}
2025-01-15 21:38:09,014 [INFO] Step[1500/2713]: training loss : 1.034190101623535 TRAIN  loss dict:  {'classification_loss': 1.034190101623535}
2025-01-15 21:38:20,922 [INFO] Step[1550/2713]: training loss : 0.9293952119350434 TRAIN  loss dict:  {'classification_loss': 0.9293952119350434}
2025-01-15 21:38:32,831 [INFO] Step[1600/2713]: training loss : 0.9542127227783204 TRAIN  loss dict:  {'classification_loss': 0.9542127227783204}
2025-01-15 21:38:44,786 [INFO] Step[1650/2713]: training loss : 0.9467371392250061 TRAIN  loss dict:  {'classification_loss': 0.9467371392250061}
2025-01-15 21:38:56,677 [INFO] Step[1700/2713]: training loss : 1.0016583871841431 TRAIN  loss dict:  {'classification_loss': 1.0016583871841431}
2025-01-15 21:39:08,650 [INFO] Step[1750/2713]: training loss : 0.9310858952999115 TRAIN  loss dict:  {'classification_loss': 0.9310858952999115}
2025-01-15 21:39:20,576 [INFO] Step[1800/2713]: training loss : 1.0412035942077638 TRAIN  loss dict:  {'classification_loss': 1.0412035942077638}
2025-01-15 21:39:32,495 [INFO] Step[1850/2713]: training loss : 0.9339136052131652 TRAIN  loss dict:  {'classification_loss': 0.9339136052131652}
2025-01-15 21:39:44,463 [INFO] Step[1900/2713]: training loss : 0.9709531736373901 TRAIN  loss dict:  {'classification_loss': 0.9709531736373901}
2025-01-15 21:39:56,421 [INFO] Step[1950/2713]: training loss : 0.9377603328227997 TRAIN  loss dict:  {'classification_loss': 0.9377603328227997}
2025-01-15 21:40:08,347 [INFO] Step[2000/2713]: training loss : 0.9688429546356201 TRAIN  loss dict:  {'classification_loss': 0.9688429546356201}
2025-01-15 21:40:20,276 [INFO] Step[2050/2713]: training loss : 0.9341294741630555 TRAIN  loss dict:  {'classification_loss': 0.9341294741630555}
2025-01-15 21:40:32,199 [INFO] Step[2100/2713]: training loss : 0.9325265252590179 TRAIN  loss dict:  {'classification_loss': 0.9325265252590179}
2025-01-15 21:40:44,128 [INFO] Step[2150/2713]: training loss : 0.9368709707260132 TRAIN  loss dict:  {'classification_loss': 0.9368709707260132}
2025-01-15 21:40:56,058 [INFO] Step[2200/2713]: training loss : 0.9292172741889954 TRAIN  loss dict:  {'classification_loss': 0.9292172741889954}
2025-01-15 21:41:07,995 [INFO] Step[2250/2713]: training loss : 0.9410955011844635 TRAIN  loss dict:  {'classification_loss': 0.9410955011844635}
2025-01-15 21:41:19,911 [INFO] Step[2300/2713]: training loss : 0.9309479677677155 TRAIN  loss dict:  {'classification_loss': 0.9309479677677155}
2025-01-15 21:41:31,834 [INFO] Step[2350/2713]: training loss : 0.9737112092971801 TRAIN  loss dict:  {'classification_loss': 0.9737112092971801}
2025-01-15 21:41:43,764 [INFO] Step[2400/2713]: training loss : 0.9507738780975342 TRAIN  loss dict:  {'classification_loss': 0.9507738780975342}
2025-01-15 21:41:55,697 [INFO] Step[2450/2713]: training loss : 0.982850090265274 TRAIN  loss dict:  {'classification_loss': 0.982850090265274}
2025-01-15 21:42:07,614 [INFO] Step[2500/2713]: training loss : 0.9540774500370026 TRAIN  loss dict:  {'classification_loss': 0.9540774500370026}
2025-01-15 21:42:19,546 [INFO] Step[2550/2713]: training loss : 0.9586706447601319 TRAIN  loss dict:  {'classification_loss': 0.9586706447601319}
2025-01-15 21:42:31,463 [INFO] Step[2600/2713]: training loss : 0.9485792875289917 TRAIN  loss dict:  {'classification_loss': 0.9485792875289917}
2025-01-15 21:42:43,384 [INFO] Step[2650/2713]: training loss : 0.9377659237384797 TRAIN  loss dict:  {'classification_loss': 0.9377659237384797}
2025-01-15 21:42:55,236 [INFO] Step[2700/2713]: training loss : 0.936164835691452 TRAIN  loss dict:  {'classification_loss': 0.936164835691452}
2025-01-15 21:44:07,914 [INFO] Label accuracies statistics:
2025-01-15 21:44:07,914 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.5, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.25, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.25, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.25, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.5, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.25, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.5, 300: 1.0, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 1.0, 355: 0.5, 356: 0.5, 357: 0.75, 358: 0.5, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.5, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 21:44:07,916 [INFO] [34] TRAIN  loss: 0.951567984774154 acc: 0.9937338739402876
2025-01-15 21:44:07,916 [INFO] [34] TRAIN  loss dict: {'classification_loss': 0.951567984774154}
2025-01-15 21:44:07,916 [INFO] [34] VALIDATION loss: 2.074673810063448 VALIDATION acc: 0.7554858934169278
2025-01-15 21:44:07,916 [INFO] [34] VALIDATION loss dict: {'classification_loss': 2.074673810063448}
2025-01-15 21:44:07,916 [INFO] 
2025-01-15 21:44:24,767 [INFO] Step[50/2713]: training loss : 0.9330073535442353 TRAIN  loss dict:  {'classification_loss': 0.9330073535442353}
2025-01-15 21:44:36,640 [INFO] Step[100/2713]: training loss : 0.9279763495922089 TRAIN  loss dict:  {'classification_loss': 0.9279763495922089}
2025-01-15 21:44:48,575 [INFO] Step[150/2713]: training loss : 0.9435889005661011 TRAIN  loss dict:  {'classification_loss': 0.9435889005661011}
2025-01-15 21:45:00,464 [INFO] Step[200/2713]: training loss : 0.9714745688438415 TRAIN  loss dict:  {'classification_loss': 0.9714745688438415}
2025-01-15 21:45:12,412 [INFO] Step[250/2713]: training loss : 0.9294508123397827 TRAIN  loss dict:  {'classification_loss': 0.9294508123397827}
2025-01-15 21:45:24,316 [INFO] Step[300/2713]: training loss : 0.9297134101390838 TRAIN  loss dict:  {'classification_loss': 0.9297134101390838}
2025-01-15 21:45:36,267 [INFO] Step[350/2713]: training loss : 0.9400138616561889 TRAIN  loss dict:  {'classification_loss': 0.9400138616561889}
2025-01-15 21:45:48,202 [INFO] Step[400/2713]: training loss : 0.9296233808994293 TRAIN  loss dict:  {'classification_loss': 0.9296233808994293}
2025-01-15 21:46:00,125 [INFO] Step[450/2713]: training loss : 0.9406971848011016 TRAIN  loss dict:  {'classification_loss': 0.9406971848011016}
2025-01-15 21:46:12,067 [INFO] Step[500/2713]: training loss : 0.9332249474525451 TRAIN  loss dict:  {'classification_loss': 0.9332249474525451}
2025-01-15 21:46:24,035 [INFO] Step[550/2713]: training loss : 0.9293239820003509 TRAIN  loss dict:  {'classification_loss': 0.9293239820003509}
2025-01-15 21:46:35,993 [INFO] Step[600/2713]: training loss : 0.9298763525485992 TRAIN  loss dict:  {'classification_loss': 0.9298763525485992}
2025-01-15 21:46:47,937 [INFO] Step[650/2713]: training loss : 0.9287763786315918 TRAIN  loss dict:  {'classification_loss': 0.9287763786315918}
2025-01-15 21:46:59,843 [INFO] Step[700/2713]: training loss : 0.9295847404003144 TRAIN  loss dict:  {'classification_loss': 0.9295847404003144}
2025-01-15 21:47:11,797 [INFO] Step[750/2713]: training loss : 0.940148354768753 TRAIN  loss dict:  {'classification_loss': 0.940148354768753}
2025-01-15 21:47:23,721 [INFO] Step[800/2713]: training loss : 0.9296406888961792 TRAIN  loss dict:  {'classification_loss': 0.9296406888961792}
2025-01-15 21:47:35,635 [INFO] Step[850/2713]: training loss : 0.9372994947433472 TRAIN  loss dict:  {'classification_loss': 0.9372994947433472}
2025-01-15 21:47:47,552 [INFO] Step[900/2713]: training loss : 0.9687175357341766 TRAIN  loss dict:  {'classification_loss': 0.9687175357341766}
2025-01-15 21:47:59,462 [INFO] Step[950/2713]: training loss : 0.9455735754966735 TRAIN  loss dict:  {'classification_loss': 0.9455735754966735}
2025-01-15 21:48:11,405 [INFO] Step[1000/2713]: training loss : 0.9363267087936401 TRAIN  loss dict:  {'classification_loss': 0.9363267087936401}
2025-01-15 21:48:23,338 [INFO] Step[1050/2713]: training loss : 0.9289670872688294 TRAIN  loss dict:  {'classification_loss': 0.9289670872688294}
2025-01-15 21:48:35,238 [INFO] Step[1100/2713]: training loss : 0.9393686521053314 TRAIN  loss dict:  {'classification_loss': 0.9393686521053314}
2025-01-15 21:48:47,188 [INFO] Step[1150/2713]: training loss : 0.9286692333221436 TRAIN  loss dict:  {'classification_loss': 0.9286692333221436}
2025-01-15 21:48:59,108 [INFO] Step[1200/2713]: training loss : 0.9282004225254059 TRAIN  loss dict:  {'classification_loss': 0.9282004225254059}
2025-01-15 21:49:11,042 [INFO] Step[1250/2713]: training loss : 0.9296334636211395 TRAIN  loss dict:  {'classification_loss': 0.9296334636211395}
2025-01-15 21:49:22,948 [INFO] Step[1300/2713]: training loss : 0.9295740830898285 TRAIN  loss dict:  {'classification_loss': 0.9295740830898285}
2025-01-15 21:49:34,877 [INFO] Step[1350/2713]: training loss : 0.9285024416446686 TRAIN  loss dict:  {'classification_loss': 0.9285024416446686}
2025-01-15 21:49:46,778 [INFO] Step[1400/2713]: training loss : 0.9305373680591583 TRAIN  loss dict:  {'classification_loss': 0.9305373680591583}
2025-01-15 21:49:58,697 [INFO] Step[1450/2713]: training loss : 0.9781442654132843 TRAIN  loss dict:  {'classification_loss': 0.9781442654132843}
2025-01-15 21:50:10,611 [INFO] Step[1500/2713]: training loss : 0.9292539978027343 TRAIN  loss dict:  {'classification_loss': 0.9292539978027343}
2025-01-15 21:50:22,563 [INFO] Step[1550/2713]: training loss : 0.9772875726222991 TRAIN  loss dict:  {'classification_loss': 0.9772875726222991}
2025-01-15 21:50:34,459 [INFO] Step[1600/2713]: training loss : 0.93159437417984 TRAIN  loss dict:  {'classification_loss': 0.93159437417984}
2025-01-15 21:50:46,396 [INFO] Step[1650/2713]: training loss : 0.9418375897407532 TRAIN  loss dict:  {'classification_loss': 0.9418375897407532}
2025-01-15 21:50:58,291 [INFO] Step[1700/2713]: training loss : 0.9294027185440064 TRAIN  loss dict:  {'classification_loss': 0.9294027185440064}
2025-01-15 21:51:10,222 [INFO] Step[1750/2713]: training loss : 0.9751729059219361 TRAIN  loss dict:  {'classification_loss': 0.9751729059219361}
2025-01-15 21:51:22,127 [INFO] Step[1800/2713]: training loss : 0.9640388119220734 TRAIN  loss dict:  {'classification_loss': 0.9640388119220734}
2025-01-15 21:51:34,059 [INFO] Step[1850/2713]: training loss : 0.9773320615291595 TRAIN  loss dict:  {'classification_loss': 0.9773320615291595}
2025-01-15 21:51:45,992 [INFO] Step[1900/2713]: training loss : 0.9415271031856537 TRAIN  loss dict:  {'classification_loss': 0.9415271031856537}
2025-01-15 21:51:57,905 [INFO] Step[1950/2713]: training loss : 0.9335004162788391 TRAIN  loss dict:  {'classification_loss': 0.9335004162788391}
2025-01-15 21:52:09,805 [INFO] Step[2000/2713]: training loss : 0.9518018901348114 TRAIN  loss dict:  {'classification_loss': 0.9518018901348114}
2025-01-15 21:52:21,725 [INFO] Step[2050/2713]: training loss : 0.9281078207492829 TRAIN  loss dict:  {'classification_loss': 0.9281078207492829}
2025-01-15 21:52:33,644 [INFO] Step[2100/2713]: training loss : 0.9291711664199829 TRAIN  loss dict:  {'classification_loss': 0.9291711664199829}
2025-01-15 21:52:45,559 [INFO] Step[2150/2713]: training loss : 0.9315530943870545 TRAIN  loss dict:  {'classification_loss': 0.9315530943870545}
2025-01-15 21:52:57,485 [INFO] Step[2200/2713]: training loss : 0.9335715568065643 TRAIN  loss dict:  {'classification_loss': 0.9335715568065643}
2025-01-15 21:53:09,389 [INFO] Step[2250/2713]: training loss : 0.9293634748458862 TRAIN  loss dict:  {'classification_loss': 0.9293634748458862}
2025-01-15 21:53:21,286 [INFO] Step[2300/2713]: training loss : 0.9278201746940613 TRAIN  loss dict:  {'classification_loss': 0.9278201746940613}
2025-01-15 21:53:33,179 [INFO] Step[2350/2713]: training loss : 0.9580864095687867 TRAIN  loss dict:  {'classification_loss': 0.9580864095687867}
2025-01-15 21:53:45,114 [INFO] Step[2400/2713]: training loss : 0.9879340946674346 TRAIN  loss dict:  {'classification_loss': 0.9879340946674346}
2025-01-15 21:53:57,023 [INFO] Step[2450/2713]: training loss : 0.9580389618873596 TRAIN  loss dict:  {'classification_loss': 0.9580389618873596}
2025-01-15 21:54:08,928 [INFO] Step[2500/2713]: training loss : 0.9319641137123108 TRAIN  loss dict:  {'classification_loss': 0.9319641137123108}
2025-01-15 21:54:20,906 [INFO] Step[2550/2713]: training loss : 0.9295172679424286 TRAIN  loss dict:  {'classification_loss': 0.9295172679424286}
2025-01-15 21:54:32,860 [INFO] Step[2600/2713]: training loss : 0.9274831712245941 TRAIN  loss dict:  {'classification_loss': 0.9274831712245941}
2025-01-15 21:54:44,763 [INFO] Step[2650/2713]: training loss : 0.9281663656234741 TRAIN  loss dict:  {'classification_loss': 0.9281663656234741}
2025-01-15 21:54:56,677 [INFO] Step[2700/2713]: training loss : 0.9374407124519348 TRAIN  loss dict:  {'classification_loss': 0.9374407124519348}
2025-01-15 21:56:09,139 [INFO] Label accuracies statistics:
2025-01-15 21:56:09,139 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.5, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 1.0, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 0.75, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 0.75, 178: 0.75, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 0.75, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.5, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 1.0, 236: 1.0, 237: 0.25, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.25, 244: 0.5, 245: 1.0, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.75, 260: 0.5, 261: 1.0, 262: 0.5, 263: 0.75, 264: 1.0, 265: 0.75, 266: 0.75, 267: 1.0, 268: 0.25, 269: 0.75, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 1.0, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.75, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.25, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.25, 351: 1.0, 352: 1.0, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.5, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.5, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.0, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 21:56:09,141 [INFO] [35] TRAIN  loss: 0.9400728321409384 acc: 0.9970512347954295
2025-01-15 21:56:09,141 [INFO] [35] TRAIN  loss dict: {'classification_loss': 0.9400728321409384}
2025-01-15 21:56:09,141 [INFO] [35] VALIDATION loss: 2.069755355554416 VALIDATION acc: 0.7623824451410658
2025-01-15 21:56:09,141 [INFO] [35] VALIDATION loss dict: {'classification_loss': 2.069755355554416}
2025-01-15 21:56:09,141 [INFO] 
2025-01-15 21:56:26,309 [INFO] Step[50/2713]: training loss : 0.9594969856739044 TRAIN  loss dict:  {'classification_loss': 0.9594969856739044}
2025-01-15 21:56:38,169 [INFO] Step[100/2713]: training loss : 0.9567773628234864 TRAIN  loss dict:  {'classification_loss': 0.9567773628234864}
2025-01-15 21:56:50,084 [INFO] Step[150/2713]: training loss : 0.9296957468986511 TRAIN  loss dict:  {'classification_loss': 0.9296957468986511}
2025-01-15 21:57:01,948 [INFO] Step[200/2713]: training loss : 0.9756378245353698 TRAIN  loss dict:  {'classification_loss': 0.9756378245353698}
2025-01-15 21:57:13,837 [INFO] Step[250/2713]: training loss : 0.9306638336181641 TRAIN  loss dict:  {'classification_loss': 0.9306638336181641}
2025-01-15 21:57:25,753 [INFO] Step[300/2713]: training loss : 0.93902960896492 TRAIN  loss dict:  {'classification_loss': 0.93902960896492}
2025-01-15 21:57:37,646 [INFO] Step[350/2713]: training loss : 0.939222891330719 TRAIN  loss dict:  {'classification_loss': 0.939222891330719}
2025-01-15 21:57:49,551 [INFO] Step[400/2713]: training loss : 0.9294057786464691 TRAIN  loss dict:  {'classification_loss': 0.9294057786464691}
2025-01-15 21:58:01,438 [INFO] Step[450/2713]: training loss : 0.9297515034675599 TRAIN  loss dict:  {'classification_loss': 0.9297515034675599}
2025-01-15 21:58:13,395 [INFO] Step[500/2713]: training loss : 0.9281621193885803 TRAIN  loss dict:  {'classification_loss': 0.9281621193885803}
2025-01-15 21:58:25,293 [INFO] Step[550/2713]: training loss : 0.9301731956005096 TRAIN  loss dict:  {'classification_loss': 0.9301731956005096}
2025-01-15 21:58:37,215 [INFO] Step[600/2713]: training loss : 0.928222838640213 TRAIN  loss dict:  {'classification_loss': 0.928222838640213}
2025-01-15 21:58:49,171 [INFO] Step[650/2713]: training loss : 0.9431118881702423 TRAIN  loss dict:  {'classification_loss': 0.9431118881702423}
2025-01-15 21:59:01,081 [INFO] Step[700/2713]: training loss : 0.9891650295257568 TRAIN  loss dict:  {'classification_loss': 0.9891650295257568}
2025-01-15 21:59:13,012 [INFO] Step[750/2713]: training loss : 0.9290106308460235 TRAIN  loss dict:  {'classification_loss': 0.9290106308460235}
2025-01-15 21:59:24,921 [INFO] Step[800/2713]: training loss : 0.9426781821250916 TRAIN  loss dict:  {'classification_loss': 0.9426781821250916}
2025-01-15 21:59:36,865 [INFO] Step[850/2713]: training loss : 0.9373918104171753 TRAIN  loss dict:  {'classification_loss': 0.9373918104171753}
2025-01-15 21:59:48,768 [INFO] Step[900/2713]: training loss : 0.9305106317996978 TRAIN  loss dict:  {'classification_loss': 0.9305106317996978}
2025-01-15 22:00:00,698 [INFO] Step[950/2713]: training loss : 0.9719553458690643 TRAIN  loss dict:  {'classification_loss': 0.9719553458690643}
2025-01-15 22:00:12,676 [INFO] Step[1000/2713]: training loss : 0.9383529531955719 TRAIN  loss dict:  {'classification_loss': 0.9383529531955719}
2025-01-15 22:00:24,601 [INFO] Step[1050/2713]: training loss : 0.9549258649349213 TRAIN  loss dict:  {'classification_loss': 0.9549258649349213}
2025-01-15 22:00:36,515 [INFO] Step[1100/2713]: training loss : 0.9282020044326782 TRAIN  loss dict:  {'classification_loss': 0.9282020044326782}
2025-01-15 22:00:48,413 [INFO] Step[1150/2713]: training loss : 0.9327358794212341 TRAIN  loss dict:  {'classification_loss': 0.9327358794212341}
2025-01-15 22:01:00,293 [INFO] Step[1200/2713]: training loss : 0.92935999751091 TRAIN  loss dict:  {'classification_loss': 0.92935999751091}
2025-01-15 22:01:12,217 [INFO] Step[1250/2713]: training loss : 0.9301305305957794 TRAIN  loss dict:  {'classification_loss': 0.9301305305957794}
2025-01-15 22:01:24,106 [INFO] Step[1300/2713]: training loss : 0.9288528084754943 TRAIN  loss dict:  {'classification_loss': 0.9288528084754943}
2025-01-15 22:01:36,002 [INFO] Step[1350/2713]: training loss : 0.9304548990726471 TRAIN  loss dict:  {'classification_loss': 0.9304548990726471}
2025-01-15 22:01:47,946 [INFO] Step[1400/2713]: training loss : 0.9290219163894653 TRAIN  loss dict:  {'classification_loss': 0.9290219163894653}
2025-01-15 22:01:59,878 [INFO] Step[1450/2713]: training loss : 0.9516859364509582 TRAIN  loss dict:  {'classification_loss': 0.9516859364509582}
2025-01-15 22:02:11,789 [INFO] Step[1500/2713]: training loss : 0.9419573247432709 TRAIN  loss dict:  {'classification_loss': 0.9419573247432709}
2025-01-15 22:02:23,723 [INFO] Step[1550/2713]: training loss : 0.9285624742507934 TRAIN  loss dict:  {'classification_loss': 0.9285624742507934}
2025-01-15 22:02:35,618 [INFO] Step[1600/2713]: training loss : 0.9306811356544494 TRAIN  loss dict:  {'classification_loss': 0.9306811356544494}
2025-01-15 22:02:47,565 [INFO] Step[1650/2713]: training loss : 0.928223329782486 TRAIN  loss dict:  {'classification_loss': 0.928223329782486}
2025-01-15 22:02:59,444 [INFO] Step[1700/2713]: training loss : 0.933710458278656 TRAIN  loss dict:  {'classification_loss': 0.933710458278656}
2025-01-15 22:03:11,352 [INFO] Step[1750/2713]: training loss : 0.9277061915397644 TRAIN  loss dict:  {'classification_loss': 0.9277061915397644}
2025-01-15 22:03:23,268 [INFO] Step[1800/2713]: training loss : 0.9598663306236267 TRAIN  loss dict:  {'classification_loss': 0.9598663306236267}
2025-01-15 22:03:35,192 [INFO] Step[1850/2713]: training loss : 0.930862500667572 TRAIN  loss dict:  {'classification_loss': 0.930862500667572}
2025-01-15 22:03:47,084 [INFO] Step[1900/2713]: training loss : 0.9518352484703064 TRAIN  loss dict:  {'classification_loss': 0.9518352484703064}
2025-01-15 22:03:59,085 [INFO] Step[1950/2713]: training loss : 0.9367476844787598 TRAIN  loss dict:  {'classification_loss': 0.9367476844787598}
2025-01-15 22:04:10,974 [INFO] Step[2000/2713]: training loss : 0.9581640064716339 TRAIN  loss dict:  {'classification_loss': 0.9581640064716339}
2025-01-15 22:04:22,891 [INFO] Step[2050/2713]: training loss : 0.9476257169246673 TRAIN  loss dict:  {'classification_loss': 0.9476257169246673}
2025-01-15 22:04:34,796 [INFO] Step[2100/2713]: training loss : 0.9371731877326965 TRAIN  loss dict:  {'classification_loss': 0.9371731877326965}
2025-01-15 22:04:46,722 [INFO] Step[2150/2713]: training loss : 0.9494010603427887 TRAIN  loss dict:  {'classification_loss': 0.9494010603427887}
2025-01-15 22:04:58,653 [INFO] Step[2200/2713]: training loss : 0.9300169599056244 TRAIN  loss dict:  {'classification_loss': 0.9300169599056244}
2025-01-15 22:05:10,568 [INFO] Step[2250/2713]: training loss : 0.932885195016861 TRAIN  loss dict:  {'classification_loss': 0.932885195016861}
2025-01-15 22:05:22,455 [INFO] Step[2300/2713]: training loss : 0.9272007930278778 TRAIN  loss dict:  {'classification_loss': 0.9272007930278778}
2025-01-15 22:05:34,356 [INFO] Step[2350/2713]: training loss : 0.9345413398742676 TRAIN  loss dict:  {'classification_loss': 0.9345413398742676}
2025-01-15 22:05:46,284 [INFO] Step[2400/2713]: training loss : 0.9350796365737915 TRAIN  loss dict:  {'classification_loss': 0.9350796365737915}
2025-01-15 22:05:58,215 [INFO] Step[2450/2713]: training loss : 0.9283833456039429 TRAIN  loss dict:  {'classification_loss': 0.9283833456039429}
2025-01-15 22:06:10,145 [INFO] Step[2500/2713]: training loss : 0.9432695353031159 TRAIN  loss dict:  {'classification_loss': 0.9432695353031159}
2025-01-15 22:06:22,080 [INFO] Step[2550/2713]: training loss : 0.9350974297523499 TRAIN  loss dict:  {'classification_loss': 0.9350974297523499}
2025-01-15 22:06:33,967 [INFO] Step[2600/2713]: training loss : 0.9287281394004822 TRAIN  loss dict:  {'classification_loss': 0.9287281394004822}
2025-01-15 22:06:45,894 [INFO] Step[2650/2713]: training loss : 0.9664406371116638 TRAIN  loss dict:  {'classification_loss': 0.9664406371116638}
2025-01-15 22:06:57,757 [INFO] Step[2700/2713]: training loss : 0.9692066669464111 TRAIN  loss dict:  {'classification_loss': 0.9692066669464111}
2025-01-15 22:08:10,219 [INFO] Label accuracies statistics:
2025-01-15 22:08:10,219 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.5, 10: 1.0, 11: 1.0, 12: 0.25, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.25, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.5, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.25, 60: 0.25, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 1.0, 96: 0.5, 97: 0.5, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.5, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 0.75, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.25, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 0.75, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 0.75, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.5, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.5, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 1.0, 378: 0.25, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 22:08:10,221 [INFO] [36] TRAIN  loss: 0.9402090926650174 acc: 0.9966826391448581
2025-01-15 22:08:10,221 [INFO] [36] TRAIN  loss dict: {'classification_loss': 0.9402090926650174}
2025-01-15 22:08:10,221 [INFO] [36] VALIDATION loss: 2.070594262807889 VALIDATION acc: 0.7623824451410658
2025-01-15 22:08:10,221 [INFO] [36] VALIDATION loss dict: {'classification_loss': 2.070594262807889}
2025-01-15 22:08:10,221 [INFO] 
2025-01-15 22:08:27,056 [INFO] Step[50/2713]: training loss : 1.0126240360736847 TRAIN  loss dict:  {'classification_loss': 1.0126240360736847}
2025-01-15 22:08:38,932 [INFO] Step[100/2713]: training loss : 0.930514200925827 TRAIN  loss dict:  {'classification_loss': 0.930514200925827}
2025-01-15 22:08:50,842 [INFO] Step[150/2713]: training loss : 0.9284742450714112 TRAIN  loss dict:  {'classification_loss': 0.9284742450714112}
2025-01-15 22:09:02,711 [INFO] Step[200/2713]: training loss : 0.9300974547863007 TRAIN  loss dict:  {'classification_loss': 0.9300974547863007}
2025-01-15 22:09:14,611 [INFO] Step[250/2713]: training loss : 1.0208082163333894 TRAIN  loss dict:  {'classification_loss': 1.0208082163333894}
2025-01-15 22:09:26,554 [INFO] Step[300/2713]: training loss : 0.9370842230319977 TRAIN  loss dict:  {'classification_loss': 0.9370842230319977}
2025-01-15 22:09:38,472 [INFO] Step[350/2713]: training loss : 1.0132722413539887 TRAIN  loss dict:  {'classification_loss': 1.0132722413539887}
2025-01-15 22:09:50,413 [INFO] Step[400/2713]: training loss : 0.9820600998401642 TRAIN  loss dict:  {'classification_loss': 0.9820600998401642}
2025-01-15 22:10:02,350 [INFO] Step[450/2713]: training loss : 0.9546541845798493 TRAIN  loss dict:  {'classification_loss': 0.9546541845798493}
2025-01-15 22:10:14,262 [INFO] Step[500/2713]: training loss : 0.9616926574707031 TRAIN  loss dict:  {'classification_loss': 0.9616926574707031}
2025-01-15 22:10:26,227 [INFO] Step[550/2713]: training loss : 0.9767557942867279 TRAIN  loss dict:  {'classification_loss': 0.9767557942867279}
2025-01-15 22:10:38,138 [INFO] Step[600/2713]: training loss : 0.9389668798446655 TRAIN  loss dict:  {'classification_loss': 0.9389668798446655}
2025-01-15 22:10:50,073 [INFO] Step[650/2713]: training loss : 0.9282983756065368 TRAIN  loss dict:  {'classification_loss': 0.9282983756065368}
2025-01-15 22:11:01,993 [INFO] Step[700/2713]: training loss : 0.944140511751175 TRAIN  loss dict:  {'classification_loss': 0.944140511751175}
2025-01-15 22:11:13,951 [INFO] Step[750/2713]: training loss : 0.9731713521480561 TRAIN  loss dict:  {'classification_loss': 0.9731713521480561}
2025-01-15 22:11:25,851 [INFO] Step[800/2713]: training loss : 0.9483311903476715 TRAIN  loss dict:  {'classification_loss': 0.9483311903476715}
2025-01-15 22:11:37,772 [INFO] Step[850/2713]: training loss : 0.9351278030872345 TRAIN  loss dict:  {'classification_loss': 0.9351278030872345}
2025-01-15 22:11:49,685 [INFO] Step[900/2713]: training loss : 0.9375702691078186 TRAIN  loss dict:  {'classification_loss': 0.9375702691078186}
2025-01-15 22:12:01,631 [INFO] Step[950/2713]: training loss : 0.9275622463226318 TRAIN  loss dict:  {'classification_loss': 0.9275622463226318}
2025-01-15 22:12:13,508 [INFO] Step[1000/2713]: training loss : 0.9653242468833924 TRAIN  loss dict:  {'classification_loss': 0.9653242468833924}
2025-01-15 22:12:25,447 [INFO] Step[1050/2713]: training loss : 0.9655245745182037 TRAIN  loss dict:  {'classification_loss': 0.9655245745182037}
2025-01-15 22:12:37,405 [INFO] Step[1100/2713]: training loss : 0.9871399199962616 TRAIN  loss dict:  {'classification_loss': 0.9871399199962616}
2025-01-15 22:12:49,371 [INFO] Step[1150/2713]: training loss : 0.9369217681884766 TRAIN  loss dict:  {'classification_loss': 0.9369217681884766}
2025-01-15 22:13:01,323 [INFO] Step[1200/2713]: training loss : 0.9746928012371063 TRAIN  loss dict:  {'classification_loss': 0.9746928012371063}
2025-01-15 22:13:13,240 [INFO] Step[1250/2713]: training loss : 0.9284924495220185 TRAIN  loss dict:  {'classification_loss': 0.9284924495220185}
2025-01-15 22:13:25,151 [INFO] Step[1300/2713]: training loss : 0.9517259931564331 TRAIN  loss dict:  {'classification_loss': 0.9517259931564331}
2025-01-15 22:13:37,076 [INFO] Step[1350/2713]: training loss : 0.9292805445194244 TRAIN  loss dict:  {'classification_loss': 0.9292805445194244}
2025-01-15 22:13:48,994 [INFO] Step[1400/2713]: training loss : 0.9306042623519898 TRAIN  loss dict:  {'classification_loss': 0.9306042623519898}
2025-01-15 22:14:00,989 [INFO] Step[1450/2713]: training loss : 0.9682448256015778 TRAIN  loss dict:  {'classification_loss': 0.9682448256015778}
2025-01-15 22:14:12,902 [INFO] Step[1500/2713]: training loss : 0.9316363894939422 TRAIN  loss dict:  {'classification_loss': 0.9316363894939422}
2025-01-15 22:14:24,818 [INFO] Step[1550/2713]: training loss : 0.9700589680671692 TRAIN  loss dict:  {'classification_loss': 0.9700589680671692}
2025-01-15 22:14:36,690 [INFO] Step[1600/2713]: training loss : 0.9289356875419617 TRAIN  loss dict:  {'classification_loss': 0.9289356875419617}
2025-01-15 22:14:48,643 [INFO] Step[1650/2713]: training loss : 0.9308137607574463 TRAIN  loss dict:  {'classification_loss': 0.9308137607574463}
2025-01-15 22:15:00,524 [INFO] Step[1700/2713]: training loss : 0.9284929835796356 TRAIN  loss dict:  {'classification_loss': 0.9284929835796356}
2025-01-15 22:15:12,454 [INFO] Step[1750/2713]: training loss : 0.932863656282425 TRAIN  loss dict:  {'classification_loss': 0.932863656282425}
2025-01-15 22:15:24,371 [INFO] Step[1800/2713]: training loss : 0.9409532201290131 TRAIN  loss dict:  {'classification_loss': 0.9409532201290131}
2025-01-15 22:15:36,322 [INFO] Step[1850/2713]: training loss : 0.9493656063079834 TRAIN  loss dict:  {'classification_loss': 0.9493656063079834}
2025-01-15 22:15:48,200 [INFO] Step[1900/2713]: training loss : 0.929981359243393 TRAIN  loss dict:  {'classification_loss': 0.929981359243393}
2025-01-15 22:16:00,116 [INFO] Step[1950/2713]: training loss : 0.9382578766345978 TRAIN  loss dict:  {'classification_loss': 0.9382578766345978}
2025-01-15 22:16:12,042 [INFO] Step[2000/2713]: training loss : 0.9270731997489929 TRAIN  loss dict:  {'classification_loss': 0.9270731997489929}
2025-01-15 22:16:23,966 [INFO] Step[2050/2713]: training loss : 0.9302464067935944 TRAIN  loss dict:  {'classification_loss': 0.9302464067935944}
2025-01-15 22:16:35,908 [INFO] Step[2100/2713]: training loss : 0.9719942617416382 TRAIN  loss dict:  {'classification_loss': 0.9719942617416382}
2025-01-15 22:16:47,809 [INFO] Step[2150/2713]: training loss : 0.9330672824382782 TRAIN  loss dict:  {'classification_loss': 0.9330672824382782}
2025-01-15 22:16:59,683 [INFO] Step[2200/2713]: training loss : 0.936063973903656 TRAIN  loss dict:  {'classification_loss': 0.936063973903656}
2025-01-15 22:17:11,589 [INFO] Step[2250/2713]: training loss : 0.9292533338069916 TRAIN  loss dict:  {'classification_loss': 0.9292533338069916}
2025-01-15 22:17:23,536 [INFO] Step[2300/2713]: training loss : 0.9693460774421692 TRAIN  loss dict:  {'classification_loss': 0.9693460774421692}
2025-01-15 22:17:35,465 [INFO] Step[2350/2713]: training loss : 0.9287429583072663 TRAIN  loss dict:  {'classification_loss': 0.9287429583072663}
2025-01-15 22:17:47,355 [INFO] Step[2400/2713]: training loss : 0.9296294987201691 TRAIN  loss dict:  {'classification_loss': 0.9296294987201691}
2025-01-15 22:17:59,272 [INFO] Step[2450/2713]: training loss : 0.928544237613678 TRAIN  loss dict:  {'classification_loss': 0.928544237613678}
2025-01-15 22:18:11,186 [INFO] Step[2500/2713]: training loss : 0.9526732194423676 TRAIN  loss dict:  {'classification_loss': 0.9526732194423676}
2025-01-15 22:18:23,112 [INFO] Step[2550/2713]: training loss : 0.9365796494483948 TRAIN  loss dict:  {'classification_loss': 0.9365796494483948}
2025-01-15 22:18:35,027 [INFO] Step[2600/2713]: training loss : 0.9289776146411896 TRAIN  loss dict:  {'classification_loss': 0.9289776146411896}
2025-01-15 22:18:46,961 [INFO] Step[2650/2713]: training loss : 0.9311525344848632 TRAIN  loss dict:  {'classification_loss': 0.9311525344848632}
2025-01-15 22:18:58,847 [INFO] Step[2700/2713]: training loss : 0.9283849239349365 TRAIN  loss dict:  {'classification_loss': 0.9283849239349365}
2025-01-15 22:20:11,547 [INFO] Label accuracies statistics:
2025-01-15 22:20:11,547 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.5, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.75, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.25, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.5, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.5, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 0.75, 247: 0.75, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.5, 254: 0.75, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.25, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.5, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.5, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 22:20:11,549 [INFO] [37] TRAIN  loss: 0.9473487167748585 acc: 0.9949625261088586
2025-01-15 22:20:11,549 [INFO] [37] TRAIN  loss dict: {'classification_loss': 0.9473487167748585}
2025-01-15 22:20:11,549 [INFO] [37] VALIDATION loss: 2.103139277127452 VALIDATION acc: 0.7573667711598746
2025-01-15 22:20:11,549 [INFO] [37] VALIDATION loss dict: {'classification_loss': 2.103139277127452}
2025-01-15 22:20:11,549 [INFO] 
2025-01-15 22:20:28,301 [INFO] Step[50/2713]: training loss : 0.9287532067298889 TRAIN  loss dict:  {'classification_loss': 0.9287532067298889}
2025-01-15 22:20:40,165 [INFO] Step[100/2713]: training loss : 0.9705942118167877 TRAIN  loss dict:  {'classification_loss': 0.9705942118167877}
2025-01-15 22:20:52,039 [INFO] Step[150/2713]: training loss : 0.9328463566303253 TRAIN  loss dict:  {'classification_loss': 0.9328463566303253}
2025-01-15 22:21:03,912 [INFO] Step[200/2713]: training loss : 0.9571333396434784 TRAIN  loss dict:  {'classification_loss': 0.9571333396434784}
2025-01-15 22:21:15,849 [INFO] Step[250/2713]: training loss : 0.9288360869884491 TRAIN  loss dict:  {'classification_loss': 0.9288360869884491}
2025-01-15 22:21:27,711 [INFO] Step[300/2713]: training loss : 0.9509729850292206 TRAIN  loss dict:  {'classification_loss': 0.9509729850292206}
2025-01-15 22:21:39,633 [INFO] Step[350/2713]: training loss : 0.9290192353725434 TRAIN  loss dict:  {'classification_loss': 0.9290192353725434}
2025-01-15 22:21:51,546 [INFO] Step[400/2713]: training loss : 0.9280853497982026 TRAIN  loss dict:  {'classification_loss': 0.9280853497982026}
2025-01-15 22:22:03,484 [INFO] Step[450/2713]: training loss : 0.9404251658916474 TRAIN  loss dict:  {'classification_loss': 0.9404251658916474}
2025-01-15 22:22:15,414 [INFO] Step[500/2713]: training loss : 0.9433950388431549 TRAIN  loss dict:  {'classification_loss': 0.9433950388431549}
2025-01-15 22:22:27,395 [INFO] Step[550/2713]: training loss : 0.9366264891624451 TRAIN  loss dict:  {'classification_loss': 0.9366264891624451}
2025-01-15 22:22:39,297 [INFO] Step[600/2713]: training loss : 0.9285040891170502 TRAIN  loss dict:  {'classification_loss': 0.9285040891170502}
2025-01-15 22:22:51,224 [INFO] Step[650/2713]: training loss : 0.9439745044708252 TRAIN  loss dict:  {'classification_loss': 0.9439745044708252}
2025-01-15 22:23:03,166 [INFO] Step[700/2713]: training loss : 0.9358413708209992 TRAIN  loss dict:  {'classification_loss': 0.9358413708209992}
2025-01-15 22:23:15,121 [INFO] Step[750/2713]: training loss : 0.93274200797081 TRAIN  loss dict:  {'classification_loss': 0.93274200797081}
2025-01-15 22:23:27,048 [INFO] Step[800/2713]: training loss : 0.947353173494339 TRAIN  loss dict:  {'classification_loss': 0.947353173494339}
2025-01-15 22:23:38,997 [INFO] Step[850/2713]: training loss : 0.9326711750030517 TRAIN  loss dict:  {'classification_loss': 0.9326711750030517}
2025-01-15 22:23:50,900 [INFO] Step[900/2713]: training loss : 0.9361205267906189 TRAIN  loss dict:  {'classification_loss': 0.9361205267906189}
2025-01-15 22:24:02,812 [INFO] Step[950/2713]: training loss : 0.9280478942394257 TRAIN  loss dict:  {'classification_loss': 0.9280478942394257}
2025-01-15 22:24:14,726 [INFO] Step[1000/2713]: training loss : 0.9633142900466919 TRAIN  loss dict:  {'classification_loss': 0.9633142900466919}
2025-01-15 22:24:26,653 [INFO] Step[1050/2713]: training loss : 0.9451270473003387 TRAIN  loss dict:  {'classification_loss': 0.9451270473003387}
2025-01-15 22:24:38,547 [INFO] Step[1100/2713]: training loss : 0.9317320799827575 TRAIN  loss dict:  {'classification_loss': 0.9317320799827575}
2025-01-15 22:24:50,471 [INFO] Step[1150/2713]: training loss : 0.9326675355434417 TRAIN  loss dict:  {'classification_loss': 0.9326675355434417}
2025-01-15 22:25:02,369 [INFO] Step[1200/2713]: training loss : 0.9295215213298798 TRAIN  loss dict:  {'classification_loss': 0.9295215213298798}
2025-01-15 22:25:14,285 [INFO] Step[1250/2713]: training loss : 0.9297314286231995 TRAIN  loss dict:  {'classification_loss': 0.9297314286231995}
2025-01-15 22:25:26,241 [INFO] Step[1300/2713]: training loss : 0.9354523205757141 TRAIN  loss dict:  {'classification_loss': 0.9354523205757141}
2025-01-15 22:25:38,224 [INFO] Step[1350/2713]: training loss : 0.9624503874778747 TRAIN  loss dict:  {'classification_loss': 0.9624503874778747}
2025-01-15 22:25:50,140 [INFO] Step[1400/2713]: training loss : 0.9546965956687927 TRAIN  loss dict:  {'classification_loss': 0.9546965956687927}
2025-01-15 22:26:02,094 [INFO] Step[1450/2713]: training loss : 0.9468188154697418 TRAIN  loss dict:  {'classification_loss': 0.9468188154697418}
2025-01-15 22:26:14,022 [INFO] Step[1500/2713]: training loss : 0.9292269337177277 TRAIN  loss dict:  {'classification_loss': 0.9292269337177277}
2025-01-15 22:26:25,946 [INFO] Step[1550/2713]: training loss : 0.9304642772674561 TRAIN  loss dict:  {'classification_loss': 0.9304642772674561}
2025-01-15 22:26:37,849 [INFO] Step[1600/2713]: training loss : 0.9280719888210297 TRAIN  loss dict:  {'classification_loss': 0.9280719888210297}
2025-01-15 22:26:49,772 [INFO] Step[1650/2713]: training loss : 0.9316719627380371 TRAIN  loss dict:  {'classification_loss': 0.9316719627380371}
2025-01-15 22:27:01,755 [INFO] Step[1700/2713]: training loss : 0.9283998250961304 TRAIN  loss dict:  {'classification_loss': 0.9283998250961304}
2025-01-15 22:27:13,676 [INFO] Step[1750/2713]: training loss : 0.9319081509113312 TRAIN  loss dict:  {'classification_loss': 0.9319081509113312}
2025-01-15 22:27:25,578 [INFO] Step[1800/2713]: training loss : 0.9742688536643982 TRAIN  loss dict:  {'classification_loss': 0.9742688536643982}
2025-01-15 22:27:37,554 [INFO] Step[1850/2713]: training loss : 0.9290252792835235 TRAIN  loss dict:  {'classification_loss': 0.9290252792835235}
2025-01-15 22:27:49,485 [INFO] Step[1900/2713]: training loss : 0.9321910071372986 TRAIN  loss dict:  {'classification_loss': 0.9321910071372986}
2025-01-15 22:28:01,382 [INFO] Step[1950/2713]: training loss : 0.9306663393974304 TRAIN  loss dict:  {'classification_loss': 0.9306663393974304}
2025-01-15 22:28:13,303 [INFO] Step[2000/2713]: training loss : 0.9295948100090027 TRAIN  loss dict:  {'classification_loss': 0.9295948100090027}
2025-01-15 22:28:25,217 [INFO] Step[2050/2713]: training loss : 0.9572703588008881 TRAIN  loss dict:  {'classification_loss': 0.9572703588008881}
2025-01-15 22:28:37,163 [INFO] Step[2100/2713]: training loss : 0.9288871479034424 TRAIN  loss dict:  {'classification_loss': 0.9288871479034424}
2025-01-15 22:28:49,107 [INFO] Step[2150/2713]: training loss : 0.9483193266391754 TRAIN  loss dict:  {'classification_loss': 0.9483193266391754}
2025-01-15 22:29:01,018 [INFO] Step[2200/2713]: training loss : 0.9300617003440856 TRAIN  loss dict:  {'classification_loss': 0.9300617003440856}
2025-01-15 22:29:12,951 [INFO] Step[2250/2713]: training loss : 0.9305570745468139 TRAIN  loss dict:  {'classification_loss': 0.9305570745468139}
2025-01-15 22:29:24,867 [INFO] Step[2300/2713]: training loss : 0.9481306397914886 TRAIN  loss dict:  {'classification_loss': 0.9481306397914886}
2025-01-15 22:29:36,770 [INFO] Step[2350/2713]: training loss : 0.9284042203426361 TRAIN  loss dict:  {'classification_loss': 0.9284042203426361}
2025-01-15 22:29:48,718 [INFO] Step[2400/2713]: training loss : 0.946492612361908 TRAIN  loss dict:  {'classification_loss': 0.946492612361908}
2025-01-15 22:30:00,671 [INFO] Step[2450/2713]: training loss : 0.941179107427597 TRAIN  loss dict:  {'classification_loss': 0.941179107427597}
2025-01-15 22:30:12,603 [INFO] Step[2500/2713]: training loss : 0.9457596552371978 TRAIN  loss dict:  {'classification_loss': 0.9457596552371978}
2025-01-15 22:30:24,539 [INFO] Step[2550/2713]: training loss : 0.9298912298679352 TRAIN  loss dict:  {'classification_loss': 0.9298912298679352}
2025-01-15 22:30:36,462 [INFO] Step[2600/2713]: training loss : 0.9286504936218262 TRAIN  loss dict:  {'classification_loss': 0.9286504936218262}
2025-01-15 22:30:48,398 [INFO] Step[2650/2713]: training loss : 0.928509830236435 TRAIN  loss dict:  {'classification_loss': 0.928509830236435}
2025-01-15 22:31:00,262 [INFO] Step[2700/2713]: training loss : 0.978739676475525 TRAIN  loss dict:  {'classification_loss': 0.978739676475525}
2025-01-15 22:32:12,850 [INFO] Label accuracies statistics:
2025-01-15 22:32:12,850 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.0, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.5, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.5, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.25, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.5, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.5, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 0.75, 197: 0.75, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 0.5, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 0.75, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.25, 351: 1.0, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.75, 357: 0.75, 358: 0.5, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.25, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.75, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 22:32:12,851 [INFO] [38] TRAIN  loss: 0.939313772548137 acc: 0.9964369087111439
2025-01-15 22:32:12,852 [INFO] [38] TRAIN  loss dict: {'classification_loss': 0.939313772548137}
2025-01-15 22:32:12,852 [INFO] [38] VALIDATION loss: 2.0867352343367456 VALIDATION acc: 0.7567398119122257
2025-01-15 22:32:12,852 [INFO] [38] VALIDATION loss dict: {'classification_loss': 2.0867352343367456}
2025-01-15 22:32:12,852 [INFO] 
2025-01-15 22:32:29,892 [INFO] Step[50/2713]: training loss : 0.9313860940933227 TRAIN  loss dict:  {'classification_loss': 0.9313860940933227}
2025-01-15 22:32:41,727 [INFO] Step[100/2713]: training loss : 0.9281220149993896 TRAIN  loss dict:  {'classification_loss': 0.9281220149993896}
2025-01-15 22:32:53,586 [INFO] Step[150/2713]: training loss : 0.9408082461357117 TRAIN  loss dict:  {'classification_loss': 0.9408082461357117}
2025-01-15 22:33:05,457 [INFO] Step[200/2713]: training loss : 0.9508781850337982 TRAIN  loss dict:  {'classification_loss': 0.9508781850337982}
2025-01-15 22:33:17,394 [INFO] Step[250/2713]: training loss : 0.9474874460697174 TRAIN  loss dict:  {'classification_loss': 0.9474874460697174}
2025-01-15 22:33:29,278 [INFO] Step[300/2713]: training loss : 0.9281731355190277 TRAIN  loss dict:  {'classification_loss': 0.9281731355190277}
2025-01-15 22:33:41,197 [INFO] Step[350/2713]: training loss : 0.9336357057094574 TRAIN  loss dict:  {'classification_loss': 0.9336357057094574}
2025-01-15 22:33:53,120 [INFO] Step[400/2713]: training loss : 0.9490812337398529 TRAIN  loss dict:  {'classification_loss': 0.9490812337398529}
2025-01-15 22:34:05,089 [INFO] Step[450/2713]: training loss : 0.9292319965362549 TRAIN  loss dict:  {'classification_loss': 0.9292319965362549}
2025-01-15 22:34:17,025 [INFO] Step[500/2713]: training loss : 0.9379306018352509 TRAIN  loss dict:  {'classification_loss': 0.9379306018352509}
2025-01-15 22:34:28,939 [INFO] Step[550/2713]: training loss : 0.9499275600910186 TRAIN  loss dict:  {'classification_loss': 0.9499275600910186}
2025-01-15 22:34:40,862 [INFO] Step[600/2713]: training loss : 0.9588306498527527 TRAIN  loss dict:  {'classification_loss': 0.9588306498527527}
2025-01-15 22:34:52,767 [INFO] Step[650/2713]: training loss : 0.9313980233669281 TRAIN  loss dict:  {'classification_loss': 0.9313980233669281}
2025-01-15 22:35:04,679 [INFO] Step[700/2713]: training loss : 0.9360824728012085 TRAIN  loss dict:  {'classification_loss': 0.9360824728012085}
2025-01-15 22:35:16,633 [INFO] Step[750/2713]: training loss : 0.9338971650600434 TRAIN  loss dict:  {'classification_loss': 0.9338971650600434}
2025-01-15 22:35:28,551 [INFO] Step[800/2713]: training loss : 0.928162282705307 TRAIN  loss dict:  {'classification_loss': 0.928162282705307}
2025-01-15 22:35:40,483 [INFO] Step[850/2713]: training loss : 0.9283822631835937 TRAIN  loss dict:  {'classification_loss': 0.9283822631835937}
2025-01-15 22:35:52,403 [INFO] Step[900/2713]: training loss : 0.9522618341445923 TRAIN  loss dict:  {'classification_loss': 0.9522618341445923}
2025-01-15 22:36:04,331 [INFO] Step[950/2713]: training loss : 0.9713966655731201 TRAIN  loss dict:  {'classification_loss': 0.9713966655731201}
2025-01-15 22:36:16,275 [INFO] Step[1000/2713]: training loss : 0.9469574904441833 TRAIN  loss dict:  {'classification_loss': 0.9469574904441833}
2025-01-15 22:36:28,187 [INFO] Step[1050/2713]: training loss : 0.9419949388504029 TRAIN  loss dict:  {'classification_loss': 0.9419949388504029}
2025-01-15 22:36:40,073 [INFO] Step[1100/2713]: training loss : 0.9281215274333954 TRAIN  loss dict:  {'classification_loss': 0.9281215274333954}
2025-01-15 22:36:51,999 [INFO] Step[1150/2713]: training loss : 0.9615040981769561 TRAIN  loss dict:  {'classification_loss': 0.9615040981769561}
2025-01-15 22:37:03,960 [INFO] Step[1200/2713]: training loss : 0.9417389488220215 TRAIN  loss dict:  {'classification_loss': 0.9417389488220215}
2025-01-15 22:37:15,860 [INFO] Step[1250/2713]: training loss : 0.9293468618392944 TRAIN  loss dict:  {'classification_loss': 0.9293468618392944}
2025-01-15 22:37:27,774 [INFO] Step[1300/2713]: training loss : 0.9830155086517334 TRAIN  loss dict:  {'classification_loss': 0.9830155086517334}
2025-01-15 22:37:39,675 [INFO] Step[1350/2713]: training loss : 0.9321549689769745 TRAIN  loss dict:  {'classification_loss': 0.9321549689769745}
2025-01-15 22:37:51,631 [INFO] Step[1400/2713]: training loss : 0.9286093294620514 TRAIN  loss dict:  {'classification_loss': 0.9286093294620514}
2025-01-15 22:38:03,559 [INFO] Step[1450/2713]: training loss : 0.9437256956100464 TRAIN  loss dict:  {'classification_loss': 0.9437256956100464}
2025-01-15 22:38:15,446 [INFO] Step[1500/2713]: training loss : 0.9296097600460053 TRAIN  loss dict:  {'classification_loss': 0.9296097600460053}
2025-01-15 22:38:27,361 [INFO] Step[1550/2713]: training loss : 0.948786449432373 TRAIN  loss dict:  {'classification_loss': 0.948786449432373}
2025-01-15 22:38:39,294 [INFO] Step[1600/2713]: training loss : 0.9372870540618896 TRAIN  loss dict:  {'classification_loss': 0.9372870540618896}
2025-01-15 22:38:51,228 [INFO] Step[1650/2713]: training loss : 0.9426284635066986 TRAIN  loss dict:  {'classification_loss': 0.9426284635066986}
2025-01-15 22:39:03,123 [INFO] Step[1700/2713]: training loss : 0.9298748505115509 TRAIN  loss dict:  {'classification_loss': 0.9298748505115509}
2025-01-15 22:39:15,053 [INFO] Step[1750/2713]: training loss : 0.9284355998039245 TRAIN  loss dict:  {'classification_loss': 0.9284355998039245}
2025-01-15 22:39:26,966 [INFO] Step[1800/2713]: training loss : 0.9347743237018585 TRAIN  loss dict:  {'classification_loss': 0.9347743237018585}
2025-01-15 22:39:38,881 [INFO] Step[1850/2713]: training loss : 0.9281201684474945 TRAIN  loss dict:  {'classification_loss': 0.9281201684474945}
2025-01-15 22:39:50,812 [INFO] Step[1900/2713]: training loss : 0.9355900037288666 TRAIN  loss dict:  {'classification_loss': 0.9355900037288666}
2025-01-15 22:40:02,728 [INFO] Step[1950/2713]: training loss : 0.9378308176994323 TRAIN  loss dict:  {'classification_loss': 0.9378308176994323}
2025-01-15 22:40:14,685 [INFO] Step[2000/2713]: training loss : 0.9400995433330536 TRAIN  loss dict:  {'classification_loss': 0.9400995433330536}
2025-01-15 22:40:26,655 [INFO] Step[2050/2713]: training loss : 0.9385100686550141 TRAIN  loss dict:  {'classification_loss': 0.9385100686550141}
2025-01-15 22:40:38,556 [INFO] Step[2100/2713]: training loss : 0.9608902156352996 TRAIN  loss dict:  {'classification_loss': 0.9608902156352996}
2025-01-15 22:40:50,491 [INFO] Step[2150/2713]: training loss : 0.9398858654499054 TRAIN  loss dict:  {'classification_loss': 0.9398858654499054}
2025-01-15 22:41:02,382 [INFO] Step[2200/2713]: training loss : 0.9281289315223694 TRAIN  loss dict:  {'classification_loss': 0.9281289315223694}
2025-01-15 22:41:14,326 [INFO] Step[2250/2713]: training loss : 0.932078274488449 TRAIN  loss dict:  {'classification_loss': 0.932078274488449}
2025-01-15 22:41:26,225 [INFO] Step[2300/2713]: training loss : 0.92780925989151 TRAIN  loss dict:  {'classification_loss': 0.92780925989151}
2025-01-15 22:41:38,162 [INFO] Step[2350/2713]: training loss : 0.9428039622306824 TRAIN  loss dict:  {'classification_loss': 0.9428039622306824}
2025-01-15 22:41:50,152 [INFO] Step[2400/2713]: training loss : 0.9344717669486999 TRAIN  loss dict:  {'classification_loss': 0.9344717669486999}
2025-01-15 22:42:02,066 [INFO] Step[2450/2713]: training loss : 0.9281606376171112 TRAIN  loss dict:  {'classification_loss': 0.9281606376171112}
2025-01-15 22:42:13,986 [INFO] Step[2500/2713]: training loss : 0.940012412071228 TRAIN  loss dict:  {'classification_loss': 0.940012412071228}
2025-01-15 22:42:25,911 [INFO] Step[2550/2713]: training loss : 0.930386152267456 TRAIN  loss dict:  {'classification_loss': 0.930386152267456}
2025-01-15 22:42:37,864 [INFO] Step[2600/2713]: training loss : 0.927912974357605 TRAIN  loss dict:  {'classification_loss': 0.927912974357605}
2025-01-15 22:42:49,771 [INFO] Step[2650/2713]: training loss : 0.9611779141426087 TRAIN  loss dict:  {'classification_loss': 0.9611779141426087}
2025-01-15 22:43:01,672 [INFO] Step[2700/2713]: training loss : 0.9281661677360534 TRAIN  loss dict:  {'classification_loss': 0.9281661677360534}
2025-01-15 22:44:14,526 [INFO] Label accuracies statistics:
2025-01-15 22:44:14,526 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.5, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.5, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.25, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.0, 55: 0.5, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.0, 67: 1.0, 68: 0.5, 69: 0.5, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.0, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.75, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 0.5, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.25, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.5, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.5, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.5, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.5, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 0.75, 247: 0.5, 248: 1.0, 249: 1.0, 250: 0.5, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.25, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.25, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.5, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.25, 379: 0.75, 380: 1.0, 381: 0.25, 382: 0.75, 383: 1.0, 384: 1.0, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.5}

2025-01-15 22:44:14,527 [INFO] [39] TRAIN  loss: 0.9391726394267351 acc: 0.996559773928001
2025-01-15 22:44:14,528 [INFO] [39] TRAIN  loss dict: {'classification_loss': 0.9391726394267351}
2025-01-15 22:44:14,528 [INFO] [39] VALIDATION loss: 2.143015280365944 VALIDATION acc: 0.7498432601880878
2025-01-15 22:44:14,528 [INFO] [39] VALIDATION loss dict: {'classification_loss': 2.143015280365944}
2025-01-15 22:44:14,528 [INFO] 
2025-01-15 22:44:31,214 [INFO] Step[50/2713]: training loss : 0.9279992055892944 TRAIN  loss dict:  {'classification_loss': 0.9279992055892944}
2025-01-15 22:44:43,118 [INFO] Step[100/2713]: training loss : 1.0327179801464081 TRAIN  loss dict:  {'classification_loss': 1.0327179801464081}
2025-01-15 22:44:55,037 [INFO] Step[150/2713]: training loss : 0.942958972454071 TRAIN  loss dict:  {'classification_loss': 0.942958972454071}
2025-01-15 22:45:06,907 [INFO] Step[200/2713]: training loss : 0.933795393705368 TRAIN  loss dict:  {'classification_loss': 0.933795393705368}
2025-01-15 22:45:18,823 [INFO] Step[250/2713]: training loss : 0.9294432842731476 TRAIN  loss dict:  {'classification_loss': 0.9294432842731476}
2025-01-15 22:45:30,735 [INFO] Step[300/2713]: training loss : 0.9567739534378051 TRAIN  loss dict:  {'classification_loss': 0.9567739534378051}
2025-01-15 22:45:42,653 [INFO] Step[350/2713]: training loss : 0.967941974401474 TRAIN  loss dict:  {'classification_loss': 0.967941974401474}
2025-01-15 22:45:54,579 [INFO] Step[400/2713]: training loss : 0.9292336332798005 TRAIN  loss dict:  {'classification_loss': 0.9292336332798005}
2025-01-15 22:46:06,526 [INFO] Step[450/2713]: training loss : 0.9290361070632934 TRAIN  loss dict:  {'classification_loss': 0.9290361070632934}
2025-01-15 22:46:18,429 [INFO] Step[500/2713]: training loss : 0.9687581706047058 TRAIN  loss dict:  {'classification_loss': 0.9687581706047058}
2025-01-15 22:46:30,391 [INFO] Step[550/2713]: training loss : 0.9286666941642762 TRAIN  loss dict:  {'classification_loss': 0.9286666941642762}
2025-01-15 22:46:42,293 [INFO] Step[600/2713]: training loss : 0.9276723313331604 TRAIN  loss dict:  {'classification_loss': 0.9276723313331604}
2025-01-15 22:46:54,209 [INFO] Step[650/2713]: training loss : 0.9486107778549194 TRAIN  loss dict:  {'classification_loss': 0.9486107778549194}
2025-01-15 22:47:06,121 [INFO] Step[700/2713]: training loss : 0.9411295747756958 TRAIN  loss dict:  {'classification_loss': 0.9411295747756958}
2025-01-15 22:47:18,040 [INFO] Step[750/2713]: training loss : 0.9305518960952759 TRAIN  loss dict:  {'classification_loss': 0.9305518960952759}
2025-01-15 22:47:29,937 [INFO] Step[800/2713]: training loss : 0.9451154541969299 TRAIN  loss dict:  {'classification_loss': 0.9451154541969299}
2025-01-15 22:47:41,869 [INFO] Step[850/2713]: training loss : 0.9506563472747803 TRAIN  loss dict:  {'classification_loss': 0.9506563472747803}
2025-01-15 22:47:53,791 [INFO] Step[900/2713]: training loss : 0.9291871058940887 TRAIN  loss dict:  {'classification_loss': 0.9291871058940887}
2025-01-15 22:48:05,733 [INFO] Step[950/2713]: training loss : 0.9422320711612702 TRAIN  loss dict:  {'classification_loss': 0.9422320711612702}
2025-01-15 22:48:17,616 [INFO] Step[1000/2713]: training loss : 0.9290774142742158 TRAIN  loss dict:  {'classification_loss': 0.9290774142742158}
2025-01-15 22:48:29,534 [INFO] Step[1050/2713]: training loss : 0.9361819553375245 TRAIN  loss dict:  {'classification_loss': 0.9361819553375245}
2025-01-15 22:48:41,434 [INFO] Step[1100/2713]: training loss : 0.9285005557537079 TRAIN  loss dict:  {'classification_loss': 0.9285005557537079}
2025-01-15 22:48:53,366 [INFO] Step[1150/2713]: training loss : 0.9319558131694794 TRAIN  loss dict:  {'classification_loss': 0.9319558131694794}
2025-01-15 22:49:05,257 [INFO] Step[1200/2713]: training loss : 0.9328952503204345 TRAIN  loss dict:  {'classification_loss': 0.9328952503204345}
2025-01-15 22:49:17,200 [INFO] Step[1250/2713]: training loss : 0.9305478703975677 TRAIN  loss dict:  {'classification_loss': 0.9305478703975677}
2025-01-15 22:49:29,080 [INFO] Step[1300/2713]: training loss : 0.9459667575359344 TRAIN  loss dict:  {'classification_loss': 0.9459667575359344}
2025-01-15 22:49:40,996 [INFO] Step[1350/2713]: training loss : 0.9565466821193696 TRAIN  loss dict:  {'classification_loss': 0.9565466821193696}
2025-01-15 22:49:52,868 [INFO] Step[1400/2713]: training loss : 0.9279186153411865 TRAIN  loss dict:  {'classification_loss': 0.9279186153411865}
2025-01-15 22:50:04,767 [INFO] Step[1450/2713]: training loss : 0.9435801815986633 TRAIN  loss dict:  {'classification_loss': 0.9435801815986633}
2025-01-15 22:50:16,654 [INFO] Step[1500/2713]: training loss : 0.9599261689186096 TRAIN  loss dict:  {'classification_loss': 0.9599261689186096}
2025-01-15 22:50:28,554 [INFO] Step[1550/2713]: training loss : 0.9680196022987366 TRAIN  loss dict:  {'classification_loss': 0.9680196022987366}
2025-01-15 22:50:40,477 [INFO] Step[1600/2713]: training loss : 0.9294448590278626 TRAIN  loss dict:  {'classification_loss': 0.9294448590278626}
2025-01-15 22:50:52,373 [INFO] Step[1650/2713]: training loss : 0.9282193410396576 TRAIN  loss dict:  {'classification_loss': 0.9282193410396576}
2025-01-15 22:51:04,247 [INFO] Step[1700/2713]: training loss : 0.9507286834716797 TRAIN  loss dict:  {'classification_loss': 0.9507286834716797}
2025-01-15 22:51:16,148 [INFO] Step[1750/2713]: training loss : 0.9346285653114319 TRAIN  loss dict:  {'classification_loss': 0.9346285653114319}
2025-01-15 22:51:28,041 [INFO] Step[1800/2713]: training loss : 0.9451646673679351 TRAIN  loss dict:  {'classification_loss': 0.9451646673679351}
2025-01-15 22:51:39,955 [INFO] Step[1850/2713]: training loss : 0.9453133380413056 TRAIN  loss dict:  {'classification_loss': 0.9453133380413056}
2025-01-15 22:51:51,877 [INFO] Step[1900/2713]: training loss : 0.9273558986186982 TRAIN  loss dict:  {'classification_loss': 0.9273558986186982}
2025-01-15 22:52:03,839 [INFO] Step[1950/2713]: training loss : 0.9275031733512878 TRAIN  loss dict:  {'classification_loss': 0.9275031733512878}
2025-01-15 22:52:15,727 [INFO] Step[2000/2713]: training loss : 0.9304023647308349 TRAIN  loss dict:  {'classification_loss': 0.9304023647308349}
2025-01-15 22:52:27,621 [INFO] Step[2050/2713]: training loss : 0.9277000558376313 TRAIN  loss dict:  {'classification_loss': 0.9277000558376313}
2025-01-15 22:52:39,514 [INFO] Step[2100/2713]: training loss : 0.9381488049030304 TRAIN  loss dict:  {'classification_loss': 0.9381488049030304}
2025-01-15 22:52:51,436 [INFO] Step[2150/2713]: training loss : 0.9342188894748688 TRAIN  loss dict:  {'classification_loss': 0.9342188894748688}
2025-01-15 22:53:03,326 [INFO] Step[2200/2713]: training loss : 0.92931281208992 TRAIN  loss dict:  {'classification_loss': 0.92931281208992}
2025-01-15 22:53:15,225 [INFO] Step[2250/2713]: training loss : 0.9407141637802124 TRAIN  loss dict:  {'classification_loss': 0.9407141637802124}
2025-01-15 22:53:27,111 [INFO] Step[2300/2713]: training loss : 0.9358125412464142 TRAIN  loss dict:  {'classification_loss': 0.9358125412464142}
2025-01-15 22:53:39,001 [INFO] Step[2350/2713]: training loss : 0.9274573314189911 TRAIN  loss dict:  {'classification_loss': 0.9274573314189911}
2025-01-15 22:53:50,869 [INFO] Step[2400/2713]: training loss : 0.9276739704608917 TRAIN  loss dict:  {'classification_loss': 0.9276739704608917}
2025-01-15 22:54:02,761 [INFO] Step[2450/2713]: training loss : 0.9765047359466553 TRAIN  loss dict:  {'classification_loss': 0.9765047359466553}
2025-01-15 22:54:14,665 [INFO] Step[2500/2713]: training loss : 0.9281748080253601 TRAIN  loss dict:  {'classification_loss': 0.9281748080253601}
2025-01-15 22:54:26,594 [INFO] Step[2550/2713]: training loss : 0.9289979255199432 TRAIN  loss dict:  {'classification_loss': 0.9289979255199432}
2025-01-15 22:54:38,516 [INFO] Step[2600/2713]: training loss : 0.9290130865573883 TRAIN  loss dict:  {'classification_loss': 0.9290130865573883}
2025-01-15 22:54:50,404 [INFO] Step[2650/2713]: training loss : 0.9834940528869629 TRAIN  loss dict:  {'classification_loss': 0.9834940528869629}
2025-01-15 22:55:02,272 [INFO] Step[2700/2713]: training loss : 0.9275058400630951 TRAIN  loss dict:  {'classification_loss': 0.9275058400630951}
2025-01-15 22:56:14,901 [INFO] Label accuracies statistics:
2025-01-15 22:56:14,902 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.25, 20: 1.0, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.5, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.5, 56: 1.0, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.25, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 0.5, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.5, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 0.5, 314: 0.75, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.75, 357: 0.75, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.25, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.25, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.5, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 22:56:14,904 [INFO] [40] TRAIN  loss: 0.9409714418498081 acc: 0.9960683130605725
2025-01-15 22:56:14,904 [INFO] [40] TRAIN  loss dict: {'classification_loss': 0.9409714418498081}
2025-01-15 22:56:14,904 [INFO] [40] VALIDATION loss: 2.0935626778387486 VALIDATION acc: 0.7554858934169278
2025-01-15 22:56:14,904 [INFO] [40] VALIDATION loss dict: {'classification_loss': 2.0935626778387486}
2025-01-15 22:56:14,904 [INFO] 
2025-01-15 22:56:31,406 [INFO] Step[50/2713]: training loss : 0.9506172442436218 TRAIN  loss dict:  {'classification_loss': 0.9506172442436218}
2025-01-15 22:56:43,251 [INFO] Step[100/2713]: training loss : 0.932275949716568 TRAIN  loss dict:  {'classification_loss': 0.932275949716568}
2025-01-15 22:56:55,123 [INFO] Step[150/2713]: training loss : 0.9282210898399353 TRAIN  loss dict:  {'classification_loss': 0.9282210898399353}
2025-01-15 22:57:07,009 [INFO] Step[200/2713]: training loss : 0.9286981105804444 TRAIN  loss dict:  {'classification_loss': 0.9286981105804444}
2025-01-15 22:57:18,983 [INFO] Step[250/2713]: training loss : 0.9277655053138733 TRAIN  loss dict:  {'classification_loss': 0.9277655053138733}
2025-01-15 22:57:30,877 [INFO] Step[300/2713]: training loss : 0.9614132356643676 TRAIN  loss dict:  {'classification_loss': 0.9614132356643676}
2025-01-15 22:57:42,794 [INFO] Step[350/2713]: training loss : 0.9324299049377441 TRAIN  loss dict:  {'classification_loss': 0.9324299049377441}
2025-01-15 22:57:54,723 [INFO] Step[400/2713]: training loss : 0.9314276480674744 TRAIN  loss dict:  {'classification_loss': 0.9314276480674744}
2025-01-15 22:58:06,725 [INFO] Step[450/2713]: training loss : 0.9276276385784149 TRAIN  loss dict:  {'classification_loss': 0.9276276385784149}
2025-01-15 22:58:18,644 [INFO] Step[500/2713]: training loss : 0.927797544002533 TRAIN  loss dict:  {'classification_loss': 0.927797544002533}
2025-01-15 22:58:30,566 [INFO] Step[550/2713]: training loss : 0.9297630631923676 TRAIN  loss dict:  {'classification_loss': 0.9297630631923676}
2025-01-15 22:58:42,472 [INFO] Step[600/2713]: training loss : 0.9284496426582336 TRAIN  loss dict:  {'classification_loss': 0.9284496426582336}
2025-01-15 22:58:54,416 [INFO] Step[650/2713]: training loss : 0.9273590326309205 TRAIN  loss dict:  {'classification_loss': 0.9273590326309205}
2025-01-15 22:59:06,347 [INFO] Step[700/2713]: training loss : 0.9536225569248199 TRAIN  loss dict:  {'classification_loss': 0.9536225569248199}
2025-01-15 22:59:18,259 [INFO] Step[750/2713]: training loss : 0.938103506565094 TRAIN  loss dict:  {'classification_loss': 0.938103506565094}
2025-01-15 22:59:30,201 [INFO] Step[800/2713]: training loss : 0.9608448791503906 TRAIN  loss dict:  {'classification_loss': 0.9608448791503906}
2025-01-15 22:59:42,171 [INFO] Step[850/2713]: training loss : 0.9266523694992066 TRAIN  loss dict:  {'classification_loss': 0.9266523694992066}
2025-01-15 22:59:54,111 [INFO] Step[900/2713]: training loss : 0.9273288989067078 TRAIN  loss dict:  {'classification_loss': 0.9273288989067078}
2025-01-15 23:00:06,067 [INFO] Step[950/2713]: training loss : 0.9635987436771393 TRAIN  loss dict:  {'classification_loss': 0.9635987436771393}
2025-01-15 23:00:18,030 [INFO] Step[1000/2713]: training loss : 0.9581726610660553 TRAIN  loss dict:  {'classification_loss': 0.9581726610660553}
2025-01-15 23:00:29,971 [INFO] Step[1050/2713]: training loss : 0.9282026207447052 TRAIN  loss dict:  {'classification_loss': 0.9282026207447052}
2025-01-15 23:00:41,863 [INFO] Step[1100/2713]: training loss : 0.9274579727649689 TRAIN  loss dict:  {'classification_loss': 0.9274579727649689}
2025-01-15 23:00:53,811 [INFO] Step[1150/2713]: training loss : 0.9315021622180939 TRAIN  loss dict:  {'classification_loss': 0.9315021622180939}
2025-01-15 23:01:05,720 [INFO] Step[1200/2713]: training loss : 0.9278477096557617 TRAIN  loss dict:  {'classification_loss': 0.9278477096557617}
2025-01-15 23:01:17,637 [INFO] Step[1250/2713]: training loss : 0.9469545996189117 TRAIN  loss dict:  {'classification_loss': 0.9469545996189117}
2025-01-15 23:01:29,543 [INFO] Step[1300/2713]: training loss : 0.9610510396957398 TRAIN  loss dict:  {'classification_loss': 0.9610510396957398}
2025-01-15 23:01:41,512 [INFO] Step[1350/2713]: training loss : 0.9281311750411987 TRAIN  loss dict:  {'classification_loss': 0.9281311750411987}
2025-01-15 23:01:53,435 [INFO] Step[1400/2713]: training loss : 0.9873761081695557 TRAIN  loss dict:  {'classification_loss': 0.9873761081695557}
2025-01-15 23:02:05,357 [INFO] Step[1450/2713]: training loss : 0.9285980486869811 TRAIN  loss dict:  {'classification_loss': 0.9285980486869811}
2025-01-15 23:02:17,270 [INFO] Step[1500/2713]: training loss : 0.9291286516189575 TRAIN  loss dict:  {'classification_loss': 0.9291286516189575}
2025-01-15 23:02:29,193 [INFO] Step[1550/2713]: training loss : 0.9271488106250763 TRAIN  loss dict:  {'classification_loss': 0.9271488106250763}
2025-01-15 23:02:41,116 [INFO] Step[1600/2713]: training loss : 0.9347495818138123 TRAIN  loss dict:  {'classification_loss': 0.9347495818138123}
2025-01-15 23:02:53,045 [INFO] Step[1650/2713]: training loss : 0.9315490984916687 TRAIN  loss dict:  {'classification_loss': 0.9315490984916687}
2025-01-15 23:03:04,972 [INFO] Step[1700/2713]: training loss : 0.9275647532939911 TRAIN  loss dict:  {'classification_loss': 0.9275647532939911}
2025-01-15 23:03:16,872 [INFO] Step[1750/2713]: training loss : 0.9278744208812714 TRAIN  loss dict:  {'classification_loss': 0.9278744208812714}
2025-01-15 23:03:28,776 [INFO] Step[1800/2713]: training loss : 0.9279890024662018 TRAIN  loss dict:  {'classification_loss': 0.9279890024662018}
2025-01-15 23:03:40,735 [INFO] Step[1850/2713]: training loss : 0.9435806834697723 TRAIN  loss dict:  {'classification_loss': 0.9435806834697723}
2025-01-15 23:03:52,624 [INFO] Step[1900/2713]: training loss : 0.9281820094585419 TRAIN  loss dict:  {'classification_loss': 0.9281820094585419}
2025-01-15 23:04:04,534 [INFO] Step[1950/2713]: training loss : 0.9287639558315277 TRAIN  loss dict:  {'classification_loss': 0.9287639558315277}
2025-01-15 23:04:16,465 [INFO] Step[2000/2713]: training loss : 0.9453265440464019 TRAIN  loss dict:  {'classification_loss': 0.9453265440464019}
2025-01-15 23:04:28,389 [INFO] Step[2050/2713]: training loss : 0.9313395726680755 TRAIN  loss dict:  {'classification_loss': 0.9313395726680755}
2025-01-15 23:04:40,318 [INFO] Step[2100/2713]: training loss : 0.9295229971408844 TRAIN  loss dict:  {'classification_loss': 0.9295229971408844}
2025-01-15 23:04:52,217 [INFO] Step[2150/2713]: training loss : 0.9282372152805328 TRAIN  loss dict:  {'classification_loss': 0.9282372152805328}
2025-01-15 23:05:04,168 [INFO] Step[2200/2713]: training loss : 0.9268929517269134 TRAIN  loss dict:  {'classification_loss': 0.9268929517269134}
2025-01-15 23:05:16,106 [INFO] Step[2250/2713]: training loss : 0.9273134124279022 TRAIN  loss dict:  {'classification_loss': 0.9273134124279022}
2025-01-15 23:05:28,019 [INFO] Step[2300/2713]: training loss : 0.9528900265693665 TRAIN  loss dict:  {'classification_loss': 0.9528900265693665}
2025-01-15 23:05:39,938 [INFO] Step[2350/2713]: training loss : 0.9495862877368927 TRAIN  loss dict:  {'classification_loss': 0.9495862877368927}
2025-01-15 23:05:51,845 [INFO] Step[2400/2713]: training loss : 0.9653155899047852 TRAIN  loss dict:  {'classification_loss': 0.9653155899047852}
2025-01-15 23:06:03,815 [INFO] Step[2450/2713]: training loss : 0.9281128644943237 TRAIN  loss dict:  {'classification_loss': 0.9281128644943237}
2025-01-15 23:06:15,761 [INFO] Step[2500/2713]: training loss : 0.9281236279010773 TRAIN  loss dict:  {'classification_loss': 0.9281236279010773}
2025-01-15 23:06:27,687 [INFO] Step[2550/2713]: training loss : 0.9280902671813965 TRAIN  loss dict:  {'classification_loss': 0.9280902671813965}
2025-01-15 23:06:39,598 [INFO] Step[2600/2713]: training loss : 0.9277006757259368 TRAIN  loss dict:  {'classification_loss': 0.9277006757259368}
2025-01-15 23:06:51,504 [INFO] Step[2650/2713]: training loss : 0.9286733508110047 TRAIN  loss dict:  {'classification_loss': 0.9286733508110047}
2025-01-15 23:07:03,398 [INFO] Step[2700/2713]: training loss : 0.9504521095752716 TRAIN  loss dict:  {'classification_loss': 0.9504521095752716}
2025-01-15 23:08:16,363 [INFO] Label accuracies statistics:
2025-01-15 23:08:16,363 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.25, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.25, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.25, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 0.75, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.5, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 0.75, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.5, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.25, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.5, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.5, 290: 0.75, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.5, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.5, 382: 0.75, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 23:08:16,365 [INFO] [41] TRAIN  loss: 0.936688828898935 acc: 0.9972969652291437
2025-01-15 23:08:16,365 [INFO] [41] TRAIN  loss dict: {'classification_loss': 0.936688828898935}
2025-01-15 23:08:16,365 [INFO] [41] VALIDATION loss: 2.1033231477092085 VALIDATION acc: 0.7567398119122257
2025-01-15 23:08:16,365 [INFO] [41] VALIDATION loss dict: {'classification_loss': 2.1033231477092085}
2025-01-15 23:08:16,365 [INFO] 
2025-01-15 23:08:32,873 [INFO] Step[50/2713]: training loss : 0.9283916449546814 TRAIN  loss dict:  {'classification_loss': 0.9283916449546814}
2025-01-15 23:08:44,736 [INFO] Step[100/2713]: training loss : 0.9305718326568604 TRAIN  loss dict:  {'classification_loss': 0.9305718326568604}
2025-01-15 23:08:56,618 [INFO] Step[150/2713]: training loss : 0.9680127179622651 TRAIN  loss dict:  {'classification_loss': 0.9680127179622651}
2025-01-15 23:09:08,498 [INFO] Step[200/2713]: training loss : 0.9478479826450348 TRAIN  loss dict:  {'classification_loss': 0.9478479826450348}
2025-01-15 23:09:20,400 [INFO] Step[250/2713]: training loss : 0.9497244644165039 TRAIN  loss dict:  {'classification_loss': 0.9497244644165039}
2025-01-15 23:09:32,306 [INFO] Step[300/2713]: training loss : 0.9281641256809234 TRAIN  loss dict:  {'classification_loss': 0.9281641256809234}
2025-01-15 23:09:44,231 [INFO] Step[350/2713]: training loss : 0.9308438026905059 TRAIN  loss dict:  {'classification_loss': 0.9308438026905059}
2025-01-15 23:09:56,128 [INFO] Step[400/2713]: training loss : 0.9597356522083282 TRAIN  loss dict:  {'classification_loss': 0.9597356522083282}
2025-01-15 23:10:08,082 [INFO] Step[450/2713]: training loss : 0.9277135193347931 TRAIN  loss dict:  {'classification_loss': 0.9277135193347931}
2025-01-15 23:10:20,044 [INFO] Step[500/2713]: training loss : 0.9288392353057862 TRAIN  loss dict:  {'classification_loss': 0.9288392353057862}
2025-01-15 23:10:31,944 [INFO] Step[550/2713]: training loss : 0.92757004737854 TRAIN  loss dict:  {'classification_loss': 0.92757004737854}
2025-01-15 23:10:43,841 [INFO] Step[600/2713]: training loss : 0.9275799775123597 TRAIN  loss dict:  {'classification_loss': 0.9275799775123597}
2025-01-15 23:10:55,730 [INFO] Step[650/2713]: training loss : 0.9283530449867249 TRAIN  loss dict:  {'classification_loss': 0.9283530449867249}
2025-01-15 23:11:07,642 [INFO] Step[700/2713]: training loss : 0.9537058079242706 TRAIN  loss dict:  {'classification_loss': 0.9537058079242706}
2025-01-15 23:11:19,567 [INFO] Step[750/2713]: training loss : 0.9273554039001465 TRAIN  loss dict:  {'classification_loss': 0.9273554039001465}
2025-01-15 23:11:31,471 [INFO] Step[800/2713]: training loss : 0.9570626187324524 TRAIN  loss dict:  {'classification_loss': 0.9570626187324524}
2025-01-15 23:11:43,375 [INFO] Step[850/2713]: training loss : 0.9319589471817017 TRAIN  loss dict:  {'classification_loss': 0.9319589471817017}
2025-01-15 23:11:55,306 [INFO] Step[900/2713]: training loss : 0.9275875210762023 TRAIN  loss dict:  {'classification_loss': 0.9275875210762023}
2025-01-15 23:12:07,215 [INFO] Step[950/2713]: training loss : 0.9360022318363189 TRAIN  loss dict:  {'classification_loss': 0.9360022318363189}
2025-01-15 23:12:19,103 [INFO] Step[1000/2713]: training loss : 0.9273344695568084 TRAIN  loss dict:  {'classification_loss': 0.9273344695568084}
2025-01-15 23:12:31,031 [INFO] Step[1050/2713]: training loss : 0.9774565255641937 TRAIN  loss dict:  {'classification_loss': 0.9774565255641937}
2025-01-15 23:12:42,967 [INFO] Step[1100/2713]: training loss : 0.9296836733818055 TRAIN  loss dict:  {'classification_loss': 0.9296836733818055}
2025-01-15 23:12:54,887 [INFO] Step[1150/2713]: training loss : 0.9586371076107025 TRAIN  loss dict:  {'classification_loss': 0.9586371076107025}
2025-01-15 23:13:06,793 [INFO] Step[1200/2713]: training loss : 0.9287022316455841 TRAIN  loss dict:  {'classification_loss': 0.9287022316455841}
2025-01-15 23:13:18,694 [INFO] Step[1250/2713]: training loss : 0.9279172813892365 TRAIN  loss dict:  {'classification_loss': 0.9279172813892365}
2025-01-15 23:13:30,632 [INFO] Step[1300/2713]: training loss : 0.9392954587936402 TRAIN  loss dict:  {'classification_loss': 0.9392954587936402}
2025-01-15 23:13:42,521 [INFO] Step[1350/2713]: training loss : 0.959989401102066 TRAIN  loss dict:  {'classification_loss': 0.959989401102066}
2025-01-15 23:13:54,429 [INFO] Step[1400/2713]: training loss : 0.9409538400173187 TRAIN  loss dict:  {'classification_loss': 0.9409538400173187}
2025-01-15 23:14:06,368 [INFO] Step[1450/2713]: training loss : 0.9464454984664917 TRAIN  loss dict:  {'classification_loss': 0.9464454984664917}
2025-01-15 23:14:18,239 [INFO] Step[1500/2713]: training loss : 0.9403723180294037 TRAIN  loss dict:  {'classification_loss': 0.9403723180294037}
2025-01-15 23:14:30,140 [INFO] Step[1550/2713]: training loss : 0.9505142080783844 TRAIN  loss dict:  {'classification_loss': 0.9505142080783844}
2025-01-15 23:14:42,085 [INFO] Step[1600/2713]: training loss : 0.927062566280365 TRAIN  loss dict:  {'classification_loss': 0.927062566280365}
2025-01-15 23:14:54,043 [INFO] Step[1650/2713]: training loss : 0.9281951177120209 TRAIN  loss dict:  {'classification_loss': 0.9281951177120209}
2025-01-15 23:15:05,949 [INFO] Step[1700/2713]: training loss : 0.9278225409984588 TRAIN  loss dict:  {'classification_loss': 0.9278225409984588}
2025-01-15 23:15:17,863 [INFO] Step[1750/2713]: training loss : 0.9265757071971893 TRAIN  loss dict:  {'classification_loss': 0.9265757071971893}
2025-01-15 23:15:29,775 [INFO] Step[1800/2713]: training loss : 0.9276559209823608 TRAIN  loss dict:  {'classification_loss': 0.9276559209823608}
2025-01-15 23:15:41,693 [INFO] Step[1850/2713]: training loss : 0.9282652127742768 TRAIN  loss dict:  {'classification_loss': 0.9282652127742768}
2025-01-15 23:15:53,598 [INFO] Step[1900/2713]: training loss : 0.9351776242256165 TRAIN  loss dict:  {'classification_loss': 0.9351776242256165}
2025-01-15 23:16:05,497 [INFO] Step[1950/2713]: training loss : 0.9277217864990235 TRAIN  loss dict:  {'classification_loss': 0.9277217864990235}
2025-01-15 23:16:17,399 [INFO] Step[2000/2713]: training loss : 0.9361653232574463 TRAIN  loss dict:  {'classification_loss': 0.9361653232574463}
2025-01-15 23:16:29,338 [INFO] Step[2050/2713]: training loss : 0.98446413397789 TRAIN  loss dict:  {'classification_loss': 0.98446413397789}
2025-01-15 23:16:41,299 [INFO] Step[2100/2713]: training loss : 0.9627391147613525 TRAIN  loss dict:  {'classification_loss': 0.9627391147613525}
2025-01-15 23:16:53,201 [INFO] Step[2150/2713]: training loss : 0.9603330838680267 TRAIN  loss dict:  {'classification_loss': 0.9603330838680267}
2025-01-15 23:17:05,094 [INFO] Step[2200/2713]: training loss : 0.9278506326675415 TRAIN  loss dict:  {'classification_loss': 0.9278506326675415}
2025-01-15 23:17:16,995 [INFO] Step[2250/2713]: training loss : 0.9533256983757019 TRAIN  loss dict:  {'classification_loss': 0.9533256983757019}
2025-01-15 23:17:28,864 [INFO] Step[2300/2713]: training loss : 0.9834343588352203 TRAIN  loss dict:  {'classification_loss': 0.9834343588352203}
2025-01-15 23:17:40,754 [INFO] Step[2350/2713]: training loss : 0.9589436721801757 TRAIN  loss dict:  {'classification_loss': 0.9589436721801757}
2025-01-15 23:17:52,619 [INFO] Step[2400/2713]: training loss : 0.9316371417045594 TRAIN  loss dict:  {'classification_loss': 0.9316371417045594}
2025-01-15 23:18:04,483 [INFO] Step[2450/2713]: training loss : 0.9275998628139496 TRAIN  loss dict:  {'classification_loss': 0.9275998628139496}
2025-01-15 23:18:16,368 [INFO] Step[2500/2713]: training loss : 0.9278806018829345 TRAIN  loss dict:  {'classification_loss': 0.9278806018829345}
2025-01-15 23:18:28,262 [INFO] Step[2550/2713]: training loss : 0.9471144473552704 TRAIN  loss dict:  {'classification_loss': 0.9471144473552704}
2025-01-15 23:18:40,145 [INFO] Step[2600/2713]: training loss : 0.9270071840286255 TRAIN  loss dict:  {'classification_loss': 0.9270071840286255}
2025-01-15 23:18:52,048 [INFO] Step[2650/2713]: training loss : 0.9540877163410186 TRAIN  loss dict:  {'classification_loss': 0.9540877163410186}
2025-01-15 23:19:03,934 [INFO] Step[2700/2713]: training loss : 0.9281520271301269 TRAIN  loss dict:  {'classification_loss': 0.9281520271301269}
2025-01-15 23:20:16,360 [INFO] Label accuracies statistics:
2025-01-15 23:20:16,360 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 0.75, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.5, 218: 0.5, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 0.75, 247: 0.75, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.25, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.5, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.0, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.5, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.5, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.25, 379: 0.75, 380: 1.0, 381: 0.75, 382: 1.0, 383: 0.5, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 23:20:16,361 [INFO] [42] TRAIN  loss: 0.9403753953800841 acc: 0.9964369087111439
2025-01-15 23:20:16,361 [INFO] [42] TRAIN  loss dict: {'classification_loss': 0.9403753953800841}
2025-01-15 23:20:16,362 [INFO] [42] VALIDATION loss: 2.0119701035712896 VALIDATION acc: 0.7673981191222571
2025-01-15 23:20:16,362 [INFO] [42] VALIDATION loss dict: {'classification_loss': 2.0119701035712896}
2025-01-15 23:20:16,362 [INFO] 
2025-01-15 23:20:33,133 [INFO] Step[50/2713]: training loss : 0.9300040864944458 TRAIN  loss dict:  {'classification_loss': 0.9300040864944458}
2025-01-15 23:20:45,035 [INFO] Step[100/2713]: training loss : 0.9283383345603943 TRAIN  loss dict:  {'classification_loss': 0.9283383345603943}
2025-01-15 23:20:56,935 [INFO] Step[150/2713]: training loss : 0.9276155793666839 TRAIN  loss dict:  {'classification_loss': 0.9276155793666839}
2025-01-15 23:21:08,819 [INFO] Step[200/2713]: training loss : 0.9277789795398712 TRAIN  loss dict:  {'classification_loss': 0.9277789795398712}
2025-01-15 23:21:20,711 [INFO] Step[250/2713]: training loss : 0.9281899893283844 TRAIN  loss dict:  {'classification_loss': 0.9281899893283844}
2025-01-15 23:21:32,599 [INFO] Step[300/2713]: training loss : 0.92730184674263 TRAIN  loss dict:  {'classification_loss': 0.92730184674263}
2025-01-15 23:21:44,524 [INFO] Step[350/2713]: training loss : 0.9272096276283264 TRAIN  loss dict:  {'classification_loss': 0.9272096276283264}
2025-01-15 23:21:56,458 [INFO] Step[400/2713]: training loss : 0.9286205697059632 TRAIN  loss dict:  {'classification_loss': 0.9286205697059632}
2025-01-15 23:22:08,377 [INFO] Step[450/2713]: training loss : 0.9675147497653961 TRAIN  loss dict:  {'classification_loss': 0.9675147497653961}
2025-01-15 23:22:20,289 [INFO] Step[500/2713]: training loss : 0.9276941049098969 TRAIN  loss dict:  {'classification_loss': 0.9276941049098969}
2025-01-15 23:22:32,234 [INFO] Step[550/2713]: training loss : 0.9276934766769409 TRAIN  loss dict:  {'classification_loss': 0.9276934766769409}
2025-01-15 23:22:44,135 [INFO] Step[600/2713]: training loss : 0.9353225588798523 TRAIN  loss dict:  {'classification_loss': 0.9353225588798523}
2025-01-15 23:22:56,075 [INFO] Step[650/2713]: training loss : 0.9366330814361572 TRAIN  loss dict:  {'classification_loss': 0.9366330814361572}
2025-01-15 23:23:07,980 [INFO] Step[700/2713]: training loss : 0.9280624151229858 TRAIN  loss dict:  {'classification_loss': 0.9280624151229858}
2025-01-15 23:23:19,927 [INFO] Step[750/2713]: training loss : 0.9540137755870819 TRAIN  loss dict:  {'classification_loss': 0.9540137755870819}
2025-01-15 23:23:31,814 [INFO] Step[800/2713]: training loss : 0.9273047220706939 TRAIN  loss dict:  {'classification_loss': 0.9273047220706939}
2025-01-15 23:23:43,765 [INFO] Step[850/2713]: training loss : 0.9467480647563934 TRAIN  loss dict:  {'classification_loss': 0.9467480647563934}
2025-01-15 23:23:55,667 [INFO] Step[900/2713]: training loss : 0.9327974116802216 TRAIN  loss dict:  {'classification_loss': 0.9327974116802216}
2025-01-15 23:24:07,577 [INFO] Step[950/2713]: training loss : 0.9305271816253662 TRAIN  loss dict:  {'classification_loss': 0.9305271816253662}
2025-01-15 23:24:19,507 [INFO] Step[1000/2713]: training loss : 0.9315372431278228 TRAIN  loss dict:  {'classification_loss': 0.9315372431278228}
2025-01-15 23:24:31,434 [INFO] Step[1050/2713]: training loss : 0.9476806831359863 TRAIN  loss dict:  {'classification_loss': 0.9476806831359863}
2025-01-15 23:24:43,338 [INFO] Step[1100/2713]: training loss : 0.9274094045162201 TRAIN  loss dict:  {'classification_loss': 0.9274094045162201}
2025-01-15 23:24:55,217 [INFO] Step[1150/2713]: training loss : 0.9399177527427673 TRAIN  loss dict:  {'classification_loss': 0.9399177527427673}
2025-01-15 23:25:07,098 [INFO] Step[1200/2713]: training loss : 0.9546224403381348 TRAIN  loss dict:  {'classification_loss': 0.9546224403381348}
2025-01-15 23:25:18,993 [INFO] Step[1250/2713]: training loss : 0.957566990852356 TRAIN  loss dict:  {'classification_loss': 0.957566990852356}
2025-01-15 23:25:30,911 [INFO] Step[1300/2713]: training loss : 0.9336385035514831 TRAIN  loss dict:  {'classification_loss': 0.9336385035514831}
2025-01-15 23:25:42,875 [INFO] Step[1350/2713]: training loss : 0.9296572148799896 TRAIN  loss dict:  {'classification_loss': 0.9296572148799896}
2025-01-15 23:25:54,757 [INFO] Step[1400/2713]: training loss : 1.0067062282562256 TRAIN  loss dict:  {'classification_loss': 1.0067062282562256}
2025-01-15 23:26:06,662 [INFO] Step[1450/2713]: training loss : 0.9337091422080994 TRAIN  loss dict:  {'classification_loss': 0.9337091422080994}
2025-01-15 23:26:18,575 [INFO] Step[1500/2713]: training loss : 0.9918915367126465 TRAIN  loss dict:  {'classification_loss': 0.9918915367126465}
2025-01-15 23:26:30,512 [INFO] Step[1550/2713]: training loss : 0.939211003780365 TRAIN  loss dict:  {'classification_loss': 0.939211003780365}
2025-01-15 23:26:42,407 [INFO] Step[1600/2713]: training loss : 0.9279476118087768 TRAIN  loss dict:  {'classification_loss': 0.9279476118087768}
2025-01-15 23:26:54,317 [INFO] Step[1650/2713]: training loss : 0.9268948078155518 TRAIN  loss dict:  {'classification_loss': 0.9268948078155518}
2025-01-15 23:27:06,192 [INFO] Step[1700/2713]: training loss : 0.9534447991847992 TRAIN  loss dict:  {'classification_loss': 0.9534447991847992}
2025-01-15 23:27:18,105 [INFO] Step[1750/2713]: training loss : 0.9453442227840424 TRAIN  loss dict:  {'classification_loss': 0.9453442227840424}
2025-01-15 23:27:30,007 [INFO] Step[1800/2713]: training loss : 0.9551319456100464 TRAIN  loss dict:  {'classification_loss': 0.9551319456100464}
2025-01-15 23:27:41,908 [INFO] Step[1850/2713]: training loss : 0.9660482954978943 TRAIN  loss dict:  {'classification_loss': 0.9660482954978943}
2025-01-15 23:27:53,835 [INFO] Step[1900/2713]: training loss : 0.9404804039001465 TRAIN  loss dict:  {'classification_loss': 0.9404804039001465}
2025-01-15 23:28:05,757 [INFO] Step[1950/2713]: training loss : 0.9319021666049957 TRAIN  loss dict:  {'classification_loss': 0.9319021666049957}
2025-01-15 23:28:17,648 [INFO] Step[2000/2713]: training loss : 0.9277995467185974 TRAIN  loss dict:  {'classification_loss': 0.9277995467185974}
2025-01-15 23:28:29,544 [INFO] Step[2050/2713]: training loss : 0.9368559372425079 TRAIN  loss dict:  {'classification_loss': 0.9368559372425079}
2025-01-15 23:28:41,441 [INFO] Step[2100/2713]: training loss : 0.9317905950546265 TRAIN  loss dict:  {'classification_loss': 0.9317905950546265}
2025-01-15 23:28:53,347 [INFO] Step[2150/2713]: training loss : 0.9492947340011597 TRAIN  loss dict:  {'classification_loss': 0.9492947340011597}
2025-01-15 23:29:05,241 [INFO] Step[2200/2713]: training loss : 0.9351439702510834 TRAIN  loss dict:  {'classification_loss': 0.9351439702510834}
2025-01-15 23:29:17,142 [INFO] Step[2250/2713]: training loss : 0.9296474707126617 TRAIN  loss dict:  {'classification_loss': 0.9296474707126617}
2025-01-15 23:29:29,080 [INFO] Step[2300/2713]: training loss : 0.92850630402565 TRAIN  loss dict:  {'classification_loss': 0.92850630402565}
2025-01-15 23:29:41,009 [INFO] Step[2350/2713]: training loss : 0.932314156293869 TRAIN  loss dict:  {'classification_loss': 0.932314156293869}
2025-01-15 23:29:52,890 [INFO] Step[2400/2713]: training loss : 0.9771505224704743 TRAIN  loss dict:  {'classification_loss': 0.9771505224704743}
2025-01-15 23:30:04,797 [INFO] Step[2450/2713]: training loss : 0.9301635134220123 TRAIN  loss dict:  {'classification_loss': 0.9301635134220123}
2025-01-15 23:30:16,675 [INFO] Step[2500/2713]: training loss : 0.9284733247756958 TRAIN  loss dict:  {'classification_loss': 0.9284733247756958}
2025-01-15 23:30:28,583 [INFO] Step[2550/2713]: training loss : 0.9471765553951264 TRAIN  loss dict:  {'classification_loss': 0.9471765553951264}
2025-01-15 23:30:40,477 [INFO] Step[2600/2713]: training loss : 0.9503488004207611 TRAIN  loss dict:  {'classification_loss': 0.9503488004207611}
2025-01-15 23:30:52,385 [INFO] Step[2650/2713]: training loss : 0.9466884827613831 TRAIN  loss dict:  {'classification_loss': 0.9466884827613831}
2025-01-15 23:31:04,240 [INFO] Step[2700/2713]: training loss : 0.9718406772613526 TRAIN  loss dict:  {'classification_loss': 0.9718406772613526}
2025-01-15 23:32:16,908 [INFO] Label accuracies statistics:
2025-01-15 23:32:16,909 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.25, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.25, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.5, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.5, 65: 1.0, 66: 0.0, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.25, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 0.75, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.5, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.5, 191: 0.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.5, 203: 0.25, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.25, 244: 0.75, 245: 0.75, 246: 0.75, 247: 0.5, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.25, 259: 0.75, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.25, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.75, 274: 1.0, 275: 0.5, 276: 0.75, 277: 0.5, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.25, 298: 0.75, 299: 0.25, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.75, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.25, 379: 0.75, 380: 1.0, 381: 0.0, 382: 0.75, 383: 0.25, 384: 1.0, 385: 0.75, 386: 1.0, 387: 0.25, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 1.0, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-15 23:32:16,910 [INFO] [43] TRAIN  loss: 0.9407386982348674 acc: 0.996559773928001
2025-01-15 23:32:16,910 [INFO] [43] TRAIN  loss dict: {'classification_loss': 0.9407386982348674}
2025-01-15 23:32:16,910 [INFO] [43] VALIDATION loss: 2.1316937793018225 VALIDATION acc: 0.7492163009404389
2025-01-15 23:32:16,910 [INFO] [43] VALIDATION loss dict: {'classification_loss': 2.1316937793018225}
2025-01-15 23:32:16,910 [INFO] 
2025-01-15 23:32:33,846 [INFO] Step[50/2713]: training loss : 0.9283874869346619 TRAIN  loss dict:  {'classification_loss': 0.9283874869346619}
2025-01-15 23:32:45,702 [INFO] Step[100/2713]: training loss : 0.9317680203914642 TRAIN  loss dict:  {'classification_loss': 0.9317680203914642}
2025-01-15 23:32:57,609 [INFO] Step[150/2713]: training loss : 0.9272396528720855 TRAIN  loss dict:  {'classification_loss': 0.9272396528720855}
2025-01-15 23:33:09,479 [INFO] Step[200/2713]: training loss : 0.9321058177947998 TRAIN  loss dict:  {'classification_loss': 0.9321058177947998}
2025-01-15 23:33:21,393 [INFO] Step[250/2713]: training loss : 0.9291177809238433 TRAIN  loss dict:  {'classification_loss': 0.9291177809238433}
2025-01-15 23:33:33,301 [INFO] Step[300/2713]: training loss : 0.92824138879776 TRAIN  loss dict:  {'classification_loss': 0.92824138879776}
2025-01-15 23:33:45,253 [INFO] Step[350/2713]: training loss : 0.9293722355365753 TRAIN  loss dict:  {'classification_loss': 0.9293722355365753}
2025-01-15 23:33:57,159 [INFO] Step[400/2713]: training loss : 0.9290097236633301 TRAIN  loss dict:  {'classification_loss': 0.9290097236633301}
2025-01-15 23:34:09,081 [INFO] Step[450/2713]: training loss : 0.9508764088153839 TRAIN  loss dict:  {'classification_loss': 0.9508764088153839}
2025-01-15 23:34:20,997 [INFO] Step[500/2713]: training loss : 0.9335358119010926 TRAIN  loss dict:  {'classification_loss': 0.9335358119010926}
2025-01-15 23:34:32,908 [INFO] Step[550/2713]: training loss : 0.9271004486083985 TRAIN  loss dict:  {'classification_loss': 0.9271004486083985}
2025-01-15 23:34:44,812 [INFO] Step[600/2713]: training loss : 0.9443855965137482 TRAIN  loss dict:  {'classification_loss': 0.9443855965137482}
2025-01-15 23:34:56,713 [INFO] Step[650/2713]: training loss : 0.9263567495346069 TRAIN  loss dict:  {'classification_loss': 0.9263567495346069}
2025-01-15 23:35:08,619 [INFO] Step[700/2713]: training loss : 0.9284017884731293 TRAIN  loss dict:  {'classification_loss': 0.9284017884731293}
2025-01-15 23:35:20,541 [INFO] Step[750/2713]: training loss : 0.927615864276886 TRAIN  loss dict:  {'classification_loss': 0.927615864276886}
2025-01-15 23:35:32,457 [INFO] Step[800/2713]: training loss : 0.9273714411258698 TRAIN  loss dict:  {'classification_loss': 0.9273714411258698}
2025-01-15 23:35:44,383 [INFO] Step[850/2713]: training loss : 0.9281732714176179 TRAIN  loss dict:  {'classification_loss': 0.9281732714176179}
2025-01-15 23:35:56,280 [INFO] Step[900/2713]: training loss : 0.9280003023147583 TRAIN  loss dict:  {'classification_loss': 0.9280003023147583}
2025-01-15 23:36:08,172 [INFO] Step[950/2713]: training loss : 0.9333917081356049 TRAIN  loss dict:  {'classification_loss': 0.9333917081356049}
2025-01-15 23:36:20,095 [INFO] Step[1000/2713]: training loss : 0.9273918437957763 TRAIN  loss dict:  {'classification_loss': 0.9273918437957763}
2025-01-15 23:36:32,025 [INFO] Step[1050/2713]: training loss : 0.9268992662429809 TRAIN  loss dict:  {'classification_loss': 0.9268992662429809}
2025-01-15 23:36:43,915 [INFO] Step[1100/2713]: training loss : 0.9611198115348816 TRAIN  loss dict:  {'classification_loss': 0.9611198115348816}
2025-01-15 23:36:55,858 [INFO] Step[1150/2713]: training loss : 0.9271702861785889 TRAIN  loss dict:  {'classification_loss': 0.9271702861785889}
2025-01-15 23:37:07,770 [INFO] Step[1200/2713]: training loss : 0.9276201343536377 TRAIN  loss dict:  {'classification_loss': 0.9276201343536377}
2025-01-15 23:37:19,672 [INFO] Step[1250/2713]: training loss : 0.92757639169693 TRAIN  loss dict:  {'classification_loss': 0.92757639169693}
2025-01-15 23:37:31,581 [INFO] Step[1300/2713]: training loss : 0.9283432948589325 TRAIN  loss dict:  {'classification_loss': 0.9283432948589325}
2025-01-15 23:37:43,479 [INFO] Step[1350/2713]: training loss : 0.9277040493488312 TRAIN  loss dict:  {'classification_loss': 0.9277040493488312}
2025-01-15 23:37:55,374 [INFO] Step[1400/2713]: training loss : 0.967262237071991 TRAIN  loss dict:  {'classification_loss': 0.967262237071991}
2025-01-15 23:38:07,277 [INFO] Step[1450/2713]: training loss : 0.9287720274925232 TRAIN  loss dict:  {'classification_loss': 0.9287720274925232}
2025-01-15 23:38:19,172 [INFO] Step[1500/2713]: training loss : 0.926977367401123 TRAIN  loss dict:  {'classification_loss': 0.926977367401123}
2025-01-15 23:38:31,073 [INFO] Step[1550/2713]: training loss : 0.9273379039764404 TRAIN  loss dict:  {'classification_loss': 0.9273379039764404}
2025-01-15 23:38:42,986 [INFO] Step[1600/2713]: training loss : 0.9276011717319489 TRAIN  loss dict:  {'classification_loss': 0.9276011717319489}
2025-01-15 23:38:54,909 [INFO] Step[1650/2713]: training loss : 0.9291762447357178 TRAIN  loss dict:  {'classification_loss': 0.9291762447357178}
2025-01-15 23:39:06,789 [INFO] Step[1700/2713]: training loss : 0.9281131255626679 TRAIN  loss dict:  {'classification_loss': 0.9281131255626679}
2025-01-15 23:39:18,692 [INFO] Step[1750/2713]: training loss : 0.9338024628162384 TRAIN  loss dict:  {'classification_loss': 0.9338024628162384}
2025-01-15 23:39:30,602 [INFO] Step[1800/2713]: training loss : 0.933206536769867 TRAIN  loss dict:  {'classification_loss': 0.933206536769867}
2025-01-15 23:39:42,498 [INFO] Step[1850/2713]: training loss : 0.936070340871811 TRAIN  loss dict:  {'classification_loss': 0.936070340871811}
2025-01-15 23:39:54,398 [INFO] Step[1900/2713]: training loss : 0.9609597301483155 TRAIN  loss dict:  {'classification_loss': 0.9609597301483155}
2025-01-15 23:40:06,315 [INFO] Step[1950/2713]: training loss : 0.9272318875789642 TRAIN  loss dict:  {'classification_loss': 0.9272318875789642}
2025-01-15 23:40:18,242 [INFO] Step[2000/2713]: training loss : 0.9277440869808197 TRAIN  loss dict:  {'classification_loss': 0.9277440869808197}
2025-01-15 23:40:30,143 [INFO] Step[2050/2713]: training loss : 0.9302113556861877 TRAIN  loss dict:  {'classification_loss': 0.9302113556861877}
2025-01-15 23:40:42,071 [INFO] Step[2100/2713]: training loss : 0.9530933964252472 TRAIN  loss dict:  {'classification_loss': 0.9530933964252472}
2025-01-15 23:40:53,976 [INFO] Step[2150/2713]: training loss : 0.9315545809268951 TRAIN  loss dict:  {'classification_loss': 0.9315545809268951}
2025-01-15 23:41:05,833 [INFO] Step[2200/2713]: training loss : 0.9269562685489654 TRAIN  loss dict:  {'classification_loss': 0.9269562685489654}
2025-01-15 23:41:17,738 [INFO] Step[2250/2713]: training loss : 0.9274652516841888 TRAIN  loss dict:  {'classification_loss': 0.9274652516841888}
2025-01-15 23:41:29,650 [INFO] Step[2300/2713]: training loss : 0.9274722123146057 TRAIN  loss dict:  {'classification_loss': 0.9274722123146057}
2025-01-15 23:41:41,558 [INFO] Step[2350/2713]: training loss : 0.9283200120925903 TRAIN  loss dict:  {'classification_loss': 0.9283200120925903}
2025-01-15 23:41:53,475 [INFO] Step[2400/2713]: training loss : 0.9443194031715393 TRAIN  loss dict:  {'classification_loss': 0.9443194031715393}
2025-01-15 23:42:05,390 [INFO] Step[2450/2713]: training loss : 0.9360120379924775 TRAIN  loss dict:  {'classification_loss': 0.9360120379924775}
2025-01-15 23:42:17,254 [INFO] Step[2500/2713]: training loss : 0.9270368921756744 TRAIN  loss dict:  {'classification_loss': 0.9270368921756744}
2025-01-15 23:42:29,171 [INFO] Step[2550/2713]: training loss : 0.9274591886997223 TRAIN  loss dict:  {'classification_loss': 0.9274591886997223}
2025-01-15 23:42:41,056 [INFO] Step[2600/2713]: training loss : 0.9263140475749969 TRAIN  loss dict:  {'classification_loss': 0.9263140475749969}
2025-01-15 23:42:52,993 [INFO] Step[2650/2713]: training loss : 0.93007599234581 TRAIN  loss dict:  {'classification_loss': 0.93007599234581}
2025-01-15 23:43:04,864 [INFO] Step[2700/2713]: training loss : 0.9386828327178955 TRAIN  loss dict:  {'classification_loss': 0.9386828327178955}
2025-01-15 23:44:19,072 [INFO] Label accuracies statistics:
2025-01-15 23:44:19,072 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.5, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.25, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.5, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.75, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.5, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.25, 233: 0.75, 234: 0.5, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 0.75, 247: 0.5, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 0.75, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.75, 260: 0.5, 261: 0.25, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.75, 269: 1.0, 270: 0.75, 271: 0.75, 272: 0.75, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 0.5, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 0.75, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.5, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.25, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.5, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.25, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.5, 348: 0.5, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.5, 356: 0.75, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.25, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.25, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.5, 395: 0.25, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-15 23:44:19,074 [INFO] [44] TRAIN  loss: 0.9324482863256073 acc: 0.9986484826145718
2025-01-15 23:44:19,074 [INFO] [44] TRAIN  loss dict: {'classification_loss': 0.9324482863256073}
2025-01-15 23:44:19,074 [INFO] [44] VALIDATION loss: 2.1113159473901404 VALIDATION acc: 0.7492163009404389
2025-01-15 23:44:19,074 [INFO] [44] VALIDATION loss dict: {'classification_loss': 2.1113159473901404}
2025-01-15 23:44:19,074 [INFO] 
2025-01-15 23:44:36,056 [INFO] Step[50/2713]: training loss : 0.9302451360225678 TRAIN  loss dict:  {'classification_loss': 0.9302451360225678}
2025-01-15 23:44:47,987 [INFO] Step[100/2713]: training loss : 0.9404794454574585 TRAIN  loss dict:  {'classification_loss': 0.9404794454574585}
2025-01-15 23:44:59,888 [INFO] Step[150/2713]: training loss : 0.9277377748489379 TRAIN  loss dict:  {'classification_loss': 0.9277377748489379}
2025-01-15 23:45:11,770 [INFO] Step[200/2713]: training loss : 0.9280604481697082 TRAIN  loss dict:  {'classification_loss': 0.9280604481697082}
2025-01-15 23:45:23,675 [INFO] Step[250/2713]: training loss : 0.9269022977352143 TRAIN  loss dict:  {'classification_loss': 0.9269022977352143}
2025-01-15 23:45:35,572 [INFO] Step[300/2713]: training loss : 0.9265897250175477 TRAIN  loss dict:  {'classification_loss': 0.9265897250175477}
2025-01-15 23:45:47,464 [INFO] Step[350/2713]: training loss : 0.9268660175800324 TRAIN  loss dict:  {'classification_loss': 0.9268660175800324}
2025-01-15 23:45:59,409 [INFO] Step[400/2713]: training loss : 0.9275063359737397 TRAIN  loss dict:  {'classification_loss': 0.9275063359737397}
2025-01-15 23:46:11,334 [INFO] Step[450/2713]: training loss : 0.9314908719062805 TRAIN  loss dict:  {'classification_loss': 0.9314908719062805}
2025-01-15 23:46:23,265 [INFO] Step[500/2713]: training loss : 0.9279164791107177 TRAIN  loss dict:  {'classification_loss': 0.9279164791107177}
2025-01-15 23:46:35,175 [INFO] Step[550/2713]: training loss : 0.9273849999904633 TRAIN  loss dict:  {'classification_loss': 0.9273849999904633}
2025-01-15 23:46:47,070 [INFO] Step[600/2713]: training loss : 0.9271337878704071 TRAIN  loss dict:  {'classification_loss': 0.9271337878704071}
2025-01-15 23:46:58,984 [INFO] Step[650/2713]: training loss : 0.9333406245708465 TRAIN  loss dict:  {'classification_loss': 0.9333406245708465}
2025-01-15 23:47:10,909 [INFO] Step[700/2713]: training loss : 0.9263505375385285 TRAIN  loss dict:  {'classification_loss': 0.9263505375385285}
2025-01-15 23:47:22,829 [INFO] Step[750/2713]: training loss : 0.9281228113174439 TRAIN  loss dict:  {'classification_loss': 0.9281228113174439}
2025-01-15 23:47:34,750 [INFO] Step[800/2713]: training loss : 0.935142011642456 TRAIN  loss dict:  {'classification_loss': 0.935142011642456}
2025-01-15 23:47:46,675 [INFO] Step[850/2713]: training loss : 0.9289660811424255 TRAIN  loss dict:  {'classification_loss': 0.9289660811424255}
2025-01-15 23:47:58,555 [INFO] Step[900/2713]: training loss : 0.9400796866416932 TRAIN  loss dict:  {'classification_loss': 0.9400796866416932}
2025-01-15 23:48:10,458 [INFO] Step[950/2713]: training loss : 0.9458659982681275 TRAIN  loss dict:  {'classification_loss': 0.9458659982681275}
2025-01-15 23:48:22,376 [INFO] Step[1000/2713]: training loss : 0.9283179473876954 TRAIN  loss dict:  {'classification_loss': 0.9283179473876954}
2025-01-15 23:48:34,288 [INFO] Step[1050/2713]: training loss : 0.9273904955387116 TRAIN  loss dict:  {'classification_loss': 0.9273904955387116}
2025-01-15 23:48:46,210 [INFO] Step[1100/2713]: training loss : 0.9350914001464844 TRAIN  loss dict:  {'classification_loss': 0.9350914001464844}
2025-01-15 23:48:58,119 [INFO] Step[1150/2713]: training loss : 1.0080426466464996 TRAIN  loss dict:  {'classification_loss': 1.0080426466464996}
2025-01-15 23:49:10,020 [INFO] Step[1200/2713]: training loss : 0.9272939395904541 TRAIN  loss dict:  {'classification_loss': 0.9272939395904541}
2025-01-15 23:49:21,990 [INFO] Step[1250/2713]: training loss : 0.9289910912513732 TRAIN  loss dict:  {'classification_loss': 0.9289910912513732}
2025-01-15 23:49:33,902 [INFO] Step[1300/2713]: training loss : 0.9475170505046845 TRAIN  loss dict:  {'classification_loss': 0.9475170505046845}
2025-01-15 23:49:45,833 [INFO] Step[1350/2713]: training loss : 0.9272859036922455 TRAIN  loss dict:  {'classification_loss': 0.9272859036922455}
2025-01-15 23:49:57,749 [INFO] Step[1400/2713]: training loss : 0.9273846375942231 TRAIN  loss dict:  {'classification_loss': 0.9273846375942231}
