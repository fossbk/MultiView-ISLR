2025-01-16 10:19:06,553 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 4a 01dr)...


2025-01-16 10:19:36,981 [INFO] Step[50/2713]: training loss : 6.086325941085815 TRAIN  loss dict:  {'classification_loss': 6.086325941085815}
2025-01-16 10:19:53,202 [INFO] Step[100/2713]: training loss : 6.1276975345611575 TRAIN  loss dict:  {'classification_loss': 6.1276975345611575}
2025-01-16 10:20:09,582 [INFO] Step[150/2713]: training loss : 6.038376302719116 TRAIN  loss dict:  {'classification_loss': 6.038376302719116}
2025-01-16 10:20:25,867 [INFO] Step[200/2713]: training loss : 5.959481172561645 TRAIN  loss dict:  {'classification_loss': 5.959481172561645}
2025-01-16 10:20:42,123 [INFO] Step[250/2713]: training loss : 5.698312749862671 TRAIN  loss dict:  {'classification_loss': 5.698312749862671}
2025-01-16 10:20:58,490 [INFO] Step[300/2713]: training loss : 5.41514666557312 TRAIN  loss dict:  {'classification_loss': 5.41514666557312}
2025-01-16 10:21:14,915 [INFO] Step[350/2713]: training loss : 5.349530792236328 TRAIN  loss dict:  {'classification_loss': 5.349530792236328}
2025-01-16 10:21:31,198 [INFO] Step[400/2713]: training loss : 5.199599504470825 TRAIN  loss dict:  {'classification_loss': 5.199599504470825}
2025-01-16 10:21:47,523 [INFO] Step[450/2713]: training loss : 4.969663786888122 TRAIN  loss dict:  {'classification_loss': 4.969663786888122}
2025-01-16 10:22:03,744 [INFO] Step[500/2713]: training loss : 4.786101598739624 TRAIN  loss dict:  {'classification_loss': 4.786101598739624}
2025-01-16 10:22:20,034 [INFO] Step[550/2713]: training loss : 4.177864928245544 TRAIN  loss dict:  {'classification_loss': 4.177864928245544}
2025-01-16 10:22:36,400 [INFO] Step[600/2713]: training loss : 4.0394127368927 TRAIN  loss dict:  {'classification_loss': 4.0394127368927}
2025-01-16 10:22:52,663 [INFO] Step[650/2713]: training loss : 3.8797456073760985 TRAIN  loss dict:  {'classification_loss': 3.8797456073760985}
2025-01-16 10:23:08,948 [INFO] Step[700/2713]: training loss : 3.7000834250450136 TRAIN  loss dict:  {'classification_loss': 3.7000834250450136}
2025-01-16 10:23:25,279 [INFO] Step[750/2713]: training loss : 3.6475056290626524 TRAIN  loss dict:  {'classification_loss': 3.6475056290626524}
2025-01-16 10:23:41,550 [INFO] Step[800/2713]: training loss : 3.116553683280945 TRAIN  loss dict:  {'classification_loss': 3.116553683280945}
2025-01-16 10:23:57,854 [INFO] Step[850/2713]: training loss : 2.8317381978034972 TRAIN  loss dict:  {'classification_loss': 2.8317381978034972}
2025-01-16 10:24:14,143 [INFO] Step[900/2713]: training loss : 2.7896794986724855 TRAIN  loss dict:  {'classification_loss': 2.7896794986724855}
2025-01-16 10:24:30,547 [INFO] Step[950/2713]: training loss : 2.5787866258621217 TRAIN  loss dict:  {'classification_loss': 2.5787866258621217}
2025-01-16 10:24:46,905 [INFO] Step[1000/2713]: training loss : 2.4337710762023925 TRAIN  loss dict:  {'classification_loss': 2.4337710762023925}
2025-01-16 10:25:03,192 [INFO] Step[1050/2713]: training loss : 2.3534311485290527 TRAIN  loss dict:  {'classification_loss': 2.3534311485290527}
2025-01-16 10:25:19,470 [INFO] Step[1100/2713]: training loss : 2.5269128322601317 TRAIN  loss dict:  {'classification_loss': 2.5269128322601317}
2025-01-16 10:25:35,737 [INFO] Step[1150/2713]: training loss : 2.6304346036911013 TRAIN  loss dict:  {'classification_loss': 2.6304346036911013}
2025-01-16 10:25:52,057 [INFO] Step[1200/2713]: training loss : 2.4240811538696287 TRAIN  loss dict:  {'classification_loss': 2.4240811538696287}
2025-01-16 10:26:08,306 [INFO] Step[1250/2713]: training loss : 2.2843887639045715 TRAIN  loss dict:  {'classification_loss': 2.2843887639045715}
2025-01-16 10:26:24,518 [INFO] Step[1300/2713]: training loss : 2.2479583621025085 TRAIN  loss dict:  {'classification_loss': 2.2479583621025085}
2025-01-16 10:26:40,729 [INFO] Step[1350/2713]: training loss : 2.1186277794837953 TRAIN  loss dict:  {'classification_loss': 2.1186277794837953}
2025-01-16 10:26:56,942 [INFO] Step[1400/2713]: training loss : 2.2746464681625365 TRAIN  loss dict:  {'classification_loss': 2.2746464681625365}
2025-01-16 10:27:13,182 [INFO] Step[1450/2713]: training loss : 2.3541749215126035 TRAIN  loss dict:  {'classification_loss': 2.3541749215126035}
2025-01-16 10:27:29,333 [INFO] Step[1500/2713]: training loss : 2.447474237680435 TRAIN  loss dict:  {'classification_loss': 2.447474237680435}
2025-01-16 10:27:45,573 [INFO] Step[1550/2713]: training loss : 2.30379695892334 TRAIN  loss dict:  {'classification_loss': 2.30379695892334}
2025-01-16 10:28:01,812 [INFO] Step[1600/2713]: training loss : 2.1133442234992983 TRAIN  loss dict:  {'classification_loss': 2.1133442234992983}
2025-01-16 10:28:17,997 [INFO] Step[1650/2713]: training loss : 2.1898775231838226 TRAIN  loss dict:  {'classification_loss': 2.1898775231838226}
2025-01-16 10:28:34,215 [INFO] Step[1700/2713]: training loss : 2.2560721015930176 TRAIN  loss dict:  {'classification_loss': 2.2560721015930176}
2025-01-16 10:28:50,378 [INFO] Step[1750/2713]: training loss : 2.230938103199005 TRAIN  loss dict:  {'classification_loss': 2.230938103199005}
2025-01-16 10:29:06,592 [INFO] Step[1800/2713]: training loss : 2.1767966079711916 TRAIN  loss dict:  {'classification_loss': 2.1767966079711916}
2025-01-16 10:29:22,829 [INFO] Step[1850/2713]: training loss : 1.9766716694831847 TRAIN  loss dict:  {'classification_loss': 1.9766716694831847}
2025-01-16 10:29:38,995 [INFO] Step[1900/2713]: training loss : 1.8850593626499177 TRAIN  loss dict:  {'classification_loss': 1.8850593626499177}
2025-01-16 10:29:55,287 [INFO] Step[1950/2713]: training loss : 2.2567371773719787 TRAIN  loss dict:  {'classification_loss': 2.2567371773719787}
2025-01-16 10:30:11,481 [INFO] Step[2000/2713]: training loss : 2.0786901545524596 TRAIN  loss dict:  {'classification_loss': 2.0786901545524596}
2025-01-16 10:30:27,729 [INFO] Step[2050/2713]: training loss : 2.17143461227417 TRAIN  loss dict:  {'classification_loss': 2.17143461227417}
2025-01-16 10:30:43,867 [INFO] Step[2100/2713]: training loss : 1.8167408466339112 TRAIN  loss dict:  {'classification_loss': 1.8167408466339112}
2025-01-16 10:31:00,133 [INFO] Step[2150/2713]: training loss : 2.0886502075195312 TRAIN  loss dict:  {'classification_loss': 2.0886502075195312}
2025-01-16 10:31:16,320 [INFO] Step[2200/2713]: training loss : 2.0182917737960815 TRAIN  loss dict:  {'classification_loss': 2.0182917737960815}
2025-01-16 10:31:32,520 [INFO] Step[2250/2713]: training loss : 2.066050088405609 TRAIN  loss dict:  {'classification_loss': 2.066050088405609}
2025-01-16 10:31:48,789 [INFO] Step[2300/2713]: training loss : 1.7278301107883454 TRAIN  loss dict:  {'classification_loss': 1.7278301107883454}
2025-01-16 10:32:05,001 [INFO] Step[2350/2713]: training loss : 1.9423940944671632 TRAIN  loss dict:  {'classification_loss': 1.9423940944671632}
2025-01-16 10:32:21,213 [INFO] Step[2400/2713]: training loss : 1.8735510849952697 TRAIN  loss dict:  {'classification_loss': 1.8735510849952697}
2025-01-16 10:32:37,425 [INFO] Step[2450/2713]: training loss : 1.8019977116584778 TRAIN  loss dict:  {'classification_loss': 1.8019977116584778}
2025-01-16 10:32:53,660 [INFO] Step[2500/2713]: training loss : 1.9222710263729095 TRAIN  loss dict:  {'classification_loss': 1.9222710263729095}
2025-01-16 10:33:09,915 [INFO] Step[2550/2713]: training loss : 2.00593581199646 TRAIN  loss dict:  {'classification_loss': 2.00593581199646}
2025-01-16 10:33:26,078 [INFO] Step[2600/2713]: training loss : 1.984309469461441 TRAIN  loss dict:  {'classification_loss': 1.984309469461441}
2025-01-16 10:33:42,293 [INFO] Step[2650/2713]: training loss : 1.947489379644394 TRAIN  loss dict:  {'classification_loss': 1.947489379644394}
2025-01-16 10:33:58,518 [INFO] Step[2700/2713]: training loss : 1.6749598908424377 TRAIN  loss dict:  {'classification_loss': 1.6749598908424377}
2025-01-16 10:35:20,406 [INFO] Label accuracies statistics:
2025-01-16 10:35:20,406 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.5, 6: 0.75, 7: 0.25, 8: 0.5, 9: 0.5, 10: 1.0, 11: 0.5, 12: 0.25, 13: 0.5, 14: 0.0, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.5, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.25, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 0.75, 81: 1.0, 82: 0.5, 83: 0.5, 84: 0.5, 85: 0.75, 86: 0.0, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.25, 110: 0.5, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.5, 117: 0.25, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.5, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 0.0, 132: 0.0, 133: 0.75, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.25, 139: 0.5, 140: 0.75, 141: 1.0, 142: 0.5, 143: 0.75, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.5, 165: 0.75, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 0.25, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.25, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.25, 191: 0.5, 192: 1.0, 193: 1.0, 194: 0.75, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.25, 200: 0.25, 201: 0.0, 202: 0.5, 203: 0.0, 204: 0.25, 205: 0.75, 206: 0.25, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.25, 213: 0.0, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.25, 218: 0.75, 219: 1.0, 220: 0.5, 221: 0.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.25, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.0, 231: 0.0, 232: 0.25, 233: 0.75, 234: 0.5, 235: 0.25, 236: 0.25, 237: 0.0, 238: 0.5, 239: 0.0, 240: 0.5, 241: 0.0, 242: 0.0, 243: 0.0, 244: 0.5, 245: 1.0, 246: 0.75, 247: 0.5, 248: 0.0, 249: 0.0, 250: 0.5, 251: 0.75, 252: 0.5, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 0.5, 258: 0.5, 259: 0.75, 260: 0.0, 261: 0.0, 262: 0.5, 263: 0.75, 264: 0.5, 265: 0.25, 266: 1.0, 267: 0.25, 268: 0.0, 269: 1.0, 270: 0.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.25, 275: 0.25, 276: 1.0, 277: 0.5, 278: 0.75, 279: 1.0, 280: 0.75, 281: 1.0, 282: 0.25, 283: 0.25, 284: 0.75, 285: 0.75, 286: 0.5, 287: 0.5, 288: 0.5, 289: 0.25, 290: 0.0, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.0, 296: 0.25, 297: 0.25, 298: 0.5, 299: 1.0, 300: 0.0, 301: 0.5, 302: 0.0, 303: 0.5, 304: 0.0, 305: 1.0, 306: 0.75, 307: 0.5, 308: 0.0, 309: 0.5, 310: 0.5, 311: 0.75, 312: 0.0, 313: 1.0, 314: 0.75, 315: 0.25, 316: 0.25, 317: 0.75, 318: 0.0, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.25, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.25, 327: 0.5, 328: 1.0, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.0, 334: 0.75, 335: 0.5, 336: 0.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 0.5, 343: 0.75, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.0, 354: 0.0, 355: 0.75, 356: 0.25, 357: 0.25, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.25, 366: 0.5, 367: 0.5, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.25, 373: 0.75, 374: 1.0, 375: 0.0, 376: 0.0, 377: 0.25, 378: 0.25, 379: 1.0, 380: 0.75, 381: 0.0, 382: 0.75, 383: 0.75, 384: 0.5, 385: 0.75, 386: 0.0, 387: 0.0, 388: 1.0, 389: 0.25, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.75, 394: 0.75, 395: 0.5, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-16 10:35:22,313 [INFO] [1] TRAIN  loss: 2.9747758073161723 acc: 0.5418356063398452
2025-01-16 10:35:22,314 [INFO] [1] TRAIN  loss dict: {'classification_loss': 2.9747758073161723}
2025-01-16 10:35:22,314 [INFO] [1] VALIDATION loss: 2.3722528957558753 VALIDATION acc: 0.6213166144200627
2025-01-16 10:35:22,314 [INFO] [1] VALIDATION loss dict: {'classification_loss': 2.3722528957558753}
2025-01-16 10:35:22,314 [INFO] 
2025-01-16 10:35:44,178 [INFO] Step[50/2713]: training loss : 1.6780457448959352 TRAIN  loss dict:  {'classification_loss': 1.6780457448959352}
2025-01-16 10:35:59,595 [INFO] Step[100/2713]: training loss : 1.6450168681144715 TRAIN  loss dict:  {'classification_loss': 1.6450168681144715}
2025-01-16 10:36:14,430 [INFO] Step[150/2713]: training loss : 1.6076696157455443 TRAIN  loss dict:  {'classification_loss': 1.6076696157455443}
2025-01-16 10:36:30,546 [INFO] Step[200/2713]: training loss : 1.6574293780326843 TRAIN  loss dict:  {'classification_loss': 1.6574293780326843}
2025-01-16 10:36:46,617 [INFO] Step[250/2713]: training loss : 1.7369754195213318 TRAIN  loss dict:  {'classification_loss': 1.7369754195213318}
2025-01-16 10:37:02,753 [INFO] Step[300/2713]: training loss : 1.6194056296348571 TRAIN  loss dict:  {'classification_loss': 1.6194056296348571}
2025-01-16 10:37:18,860 [INFO] Step[350/2713]: training loss : 1.5981751120090484 TRAIN  loss dict:  {'classification_loss': 1.5981751120090484}
2025-01-16 10:37:35,048 [INFO] Step[400/2713]: training loss : 1.7311594438552858 TRAIN  loss dict:  {'classification_loss': 1.7311594438552858}
2025-01-16 10:37:51,172 [INFO] Step[450/2713]: training loss : 1.5430920267105102 TRAIN  loss dict:  {'classification_loss': 1.5430920267105102}
2025-01-16 10:38:07,282 [INFO] Step[500/2713]: training loss : 1.5854466104507445 TRAIN  loss dict:  {'classification_loss': 1.5854466104507445}
2025-01-16 10:38:23,407 [INFO] Step[550/2713]: training loss : 1.6726336073875427 TRAIN  loss dict:  {'classification_loss': 1.6726336073875427}
2025-01-16 10:38:39,483 [INFO] Step[600/2713]: training loss : 1.651731517314911 TRAIN  loss dict:  {'classification_loss': 1.651731517314911}
2025-01-16 10:38:55,661 [INFO] Step[650/2713]: training loss : 1.737479293346405 TRAIN  loss dict:  {'classification_loss': 1.737479293346405}
2025-01-16 10:39:11,836 [INFO] Step[700/2713]: training loss : 1.4944842183589935 TRAIN  loss dict:  {'classification_loss': 1.4944842183589935}
2025-01-16 10:39:27,963 [INFO] Step[750/2713]: training loss : 1.5732127118110657 TRAIN  loss dict:  {'classification_loss': 1.5732127118110657}
2025-01-16 10:39:44,056 [INFO] Step[800/2713]: training loss : 1.6618220102787018 TRAIN  loss dict:  {'classification_loss': 1.6618220102787018}
2025-01-16 10:40:00,183 [INFO] Step[850/2713]: training loss : 1.666485342979431 TRAIN  loss dict:  {'classification_loss': 1.666485342979431}
2025-01-16 10:40:16,275 [INFO] Step[900/2713]: training loss : 1.6560230481624603 TRAIN  loss dict:  {'classification_loss': 1.6560230481624603}
2025-01-16 10:40:32,337 [INFO] Step[950/2713]: training loss : 1.7063208508491516 TRAIN  loss dict:  {'classification_loss': 1.7063208508491516}
2025-01-16 10:40:48,407 [INFO] Step[1000/2713]: training loss : 1.6453188967704773 TRAIN  loss dict:  {'classification_loss': 1.6453188967704773}
2025-01-16 10:41:04,529 [INFO] Step[1050/2713]: training loss : 1.8154693269729614 TRAIN  loss dict:  {'classification_loss': 1.8154693269729614}
2025-01-16 10:41:20,735 [INFO] Step[1100/2713]: training loss : 1.7428567981719971 TRAIN  loss dict:  {'classification_loss': 1.7428567981719971}
2025-01-16 10:41:36,843 [INFO] Step[1150/2713]: training loss : 1.642708101272583 TRAIN  loss dict:  {'classification_loss': 1.642708101272583}
2025-01-16 10:41:53,016 [INFO] Step[1200/2713]: training loss : 1.7278146648406982 TRAIN  loss dict:  {'classification_loss': 1.7278146648406982}
2025-01-16 10:42:09,124 [INFO] Step[1250/2713]: training loss : 1.7840856313705444 TRAIN  loss dict:  {'classification_loss': 1.7840856313705444}
2025-01-16 10:42:25,158 [INFO] Step[1300/2713]: training loss : 1.742355716228485 TRAIN  loss dict:  {'classification_loss': 1.742355716228485}
2025-01-16 10:42:41,366 [INFO] Step[1350/2713]: training loss : 1.7625433945655822 TRAIN  loss dict:  {'classification_loss': 1.7625433945655822}
2025-01-16 10:42:57,469 [INFO] Step[1400/2713]: training loss : 1.7244529938697815 TRAIN  loss dict:  {'classification_loss': 1.7244529938697815}
2025-01-16 10:43:13,566 [INFO] Step[1450/2713]: training loss : 1.5059575319290162 TRAIN  loss dict:  {'classification_loss': 1.5059575319290162}
2025-01-16 10:43:29,722 [INFO] Step[1500/2713]: training loss : 1.552720878124237 TRAIN  loss dict:  {'classification_loss': 1.552720878124237}
2025-01-16 10:43:45,854 [INFO] Step[1550/2713]: training loss : 1.9262094950675965 TRAIN  loss dict:  {'classification_loss': 1.9262094950675965}
2025-01-16 10:44:01,952 [INFO] Step[1600/2713]: training loss : 1.577479636669159 TRAIN  loss dict:  {'classification_loss': 1.577479636669159}
2025-01-16 10:44:18,052 [INFO] Step[1650/2713]: training loss : 1.652154769897461 TRAIN  loss dict:  {'classification_loss': 1.652154769897461}
2025-01-16 10:44:34,137 [INFO] Step[1700/2713]: training loss : 1.7172077655792237 TRAIN  loss dict:  {'classification_loss': 1.7172077655792237}
2025-01-16 10:44:50,262 [INFO] Step[1750/2713]: training loss : 1.6623095726966859 TRAIN  loss dict:  {'classification_loss': 1.6623095726966859}
2025-01-16 10:45:06,331 [INFO] Step[1800/2713]: training loss : 1.5689004290103912 TRAIN  loss dict:  {'classification_loss': 1.5689004290103912}
2025-01-16 10:45:22,453 [INFO] Step[1850/2713]: training loss : 1.628066656589508 TRAIN  loss dict:  {'classification_loss': 1.628066656589508}
2025-01-16 10:45:38,605 [INFO] Step[1900/2713]: training loss : 1.775349531173706 TRAIN  loss dict:  {'classification_loss': 1.775349531173706}
2025-01-16 10:45:54,714 [INFO] Step[1950/2713]: training loss : 1.5728656327724457 TRAIN  loss dict:  {'classification_loss': 1.5728656327724457}
2025-01-16 10:46:10,819 [INFO] Step[2000/2713]: training loss : 1.5213787961006164 TRAIN  loss dict:  {'classification_loss': 1.5213787961006164}
2025-01-16 10:46:26,937 [INFO] Step[2050/2713]: training loss : 1.4370832908153535 TRAIN  loss dict:  {'classification_loss': 1.4370832908153535}
2025-01-16 10:46:43,081 [INFO] Step[2100/2713]: training loss : 1.6426026225090027 TRAIN  loss dict:  {'classification_loss': 1.6426026225090027}
2025-01-16 10:46:59,184 [INFO] Step[2150/2713]: training loss : 1.8590069794654847 TRAIN  loss dict:  {'classification_loss': 1.8590069794654847}
2025-01-16 10:47:15,303 [INFO] Step[2200/2713]: training loss : 1.5711791622638702 TRAIN  loss dict:  {'classification_loss': 1.5711791622638702}
2025-01-16 10:47:31,382 [INFO] Step[2250/2713]: training loss : 1.5185998702049255 TRAIN  loss dict:  {'classification_loss': 1.5185998702049255}
2025-01-16 10:47:47,461 [INFO] Step[2300/2713]: training loss : 1.7390745186805725 TRAIN  loss dict:  {'classification_loss': 1.7390745186805725}
2025-01-16 10:48:03,571 [INFO] Step[2350/2713]: training loss : 1.6863529348373414 TRAIN  loss dict:  {'classification_loss': 1.6863529348373414}
2025-01-16 10:48:19,650 [INFO] Step[2400/2713]: training loss : 1.71686225771904 TRAIN  loss dict:  {'classification_loss': 1.71686225771904}
2025-01-16 10:48:35,806 [INFO] Step[2450/2713]: training loss : 1.637139891386032 TRAIN  loss dict:  {'classification_loss': 1.637139891386032}
2025-01-16 10:48:51,914 [INFO] Step[2500/2713]: training loss : 1.4935652601718903 TRAIN  loss dict:  {'classification_loss': 1.4935652601718903}
2025-01-16 10:49:08,081 [INFO] Step[2550/2713]: training loss : 1.5827659904956817 TRAIN  loss dict:  {'classification_loss': 1.5827659904956817}
2025-01-16 10:49:24,145 [INFO] Step[2600/2713]: training loss : 1.6146117615699769 TRAIN  loss dict:  {'classification_loss': 1.6146117615699769}
2025-01-16 10:49:40,354 [INFO] Step[2650/2713]: training loss : 1.7035779500007628 TRAIN  loss dict:  {'classification_loss': 1.7035779500007628}
2025-01-16 10:49:56,490 [INFO] Step[2700/2713]: training loss : 1.4749231147766113 TRAIN  loss dict:  {'classification_loss': 1.4749231147766113}
2025-01-16 10:51:15,122 [INFO] Label accuracies statistics:
2025-01-16 10:51:15,122 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.5, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.0, 17: 0.25, 18: 1.0, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.25, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.5, 33: 0.75, 34: 0.75, 35: 0.5, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.5, 63: 0.25, 64: 0.5, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.75, 86: 0.0, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.0, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.25, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.0, 114: 0.5, 115: 0.5, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.25, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.5, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.5, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.5, 181: 0.75, 182: 0.5, 183: 0.5, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 0.75, 195: 0.75, 196: 0.75, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.25, 203: 0.5, 204: 0.5, 205: 0.75, 206: 0.75, 207: 0.5, 208: 1.0, 209: 0.5, 210: 1.0, 211: 0.0, 212: 0.25, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.25, 218: 0.5, 219: 0.75, 220: 0.25, 221: 0.5, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.5, 237: 0.0, 238: 0.75, 239: 0.75, 240: 0.25, 241: 0.5, 242: 0.0, 243: 0.25, 244: 0.5, 245: 0.75, 246: 1.0, 247: 0.5, 248: 1.0, 249: 0.5, 250: 0.5, 251: 1.0, 252: 0.75, 253: 0.5, 254: 0.75, 255: 0.5, 256: 1.0, 257: 0.75, 258: 0.25, 259: 0.25, 260: 0.0, 261: 0.5, 262: 0.75, 263: 0.5, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.25, 268: 0.25, 269: 0.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.0, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.25, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.5, 288: 0.5, 289: 0.25, 290: 0.25, 291: 0.25, 292: 0.75, 293: 0.25, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.25, 298: 0.75, 299: 0.75, 300: 0.0, 301: 0.75, 302: 0.5, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.5, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.25, 328: 0.5, 329: 1.0, 330: 0.5, 331: 0.75, 332: 0.5, 333: 0.0, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 0.25, 344: 0.25, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.25, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.0, 357: 0.75, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.25, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.0, 370: 0.5, 371: 0.25, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.0, 376: 0.5, 377: 0.25, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 0.75, 386: 0.75, 387: 1.0, 388: 0.5, 389: 0.25, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.0, 394: 0.0, 395: 1.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 10:51:17,877 [INFO] [2] TRAIN  loss: 1.6502973385240334 acc: 0.7897776139574887
2025-01-16 10:51:17,877 [INFO] [2] TRAIN  loss dict: {'classification_loss': 1.6502973385240334}
2025-01-16 10:51:17,877 [INFO] [2] VALIDATION loss: 2.1951724063409004 VALIDATION acc: 0.677115987460815
2025-01-16 10:51:17,878 [INFO] [2] VALIDATION loss dict: {'classification_loss': 2.1951724063409004}
2025-01-16 10:51:17,878 [INFO] 
2025-01-16 10:51:38,506 [INFO] Step[50/2713]: training loss : 1.4429659497737886 TRAIN  loss dict:  {'classification_loss': 1.4429659497737886}
2025-01-16 10:51:54,607 [INFO] Step[100/2713]: training loss : 1.4865247130393981 TRAIN  loss dict:  {'classification_loss': 1.4865247130393981}
2025-01-16 10:52:10,760 [INFO] Step[150/2713]: training loss : 1.4377020168304444 TRAIN  loss dict:  {'classification_loss': 1.4377020168304444}
2025-01-16 10:52:26,955 [INFO] Step[200/2713]: training loss : 1.4832733058929444 TRAIN  loss dict:  {'classification_loss': 1.4832733058929444}
2025-01-16 10:52:43,184 [INFO] Step[250/2713]: training loss : 1.3543628883361816 TRAIN  loss dict:  {'classification_loss': 1.3543628883361816}
2025-01-16 10:52:59,316 [INFO] Step[300/2713]: training loss : 1.6144455778598785 TRAIN  loss dict:  {'classification_loss': 1.6144455778598785}
2025-01-16 10:53:15,521 [INFO] Step[350/2713]: training loss : 1.3837342286109924 TRAIN  loss dict:  {'classification_loss': 1.3837342286109924}
2025-01-16 10:53:31,625 [INFO] Step[400/2713]: training loss : 1.5811295485496522 TRAIN  loss dict:  {'classification_loss': 1.5811295485496522}
2025-01-16 10:53:47,695 [INFO] Step[450/2713]: training loss : 1.4043000960350036 TRAIN  loss dict:  {'classification_loss': 1.4043000960350036}
2025-01-16 10:54:03,822 [INFO] Step[500/2713]: training loss : 1.4541353750228883 TRAIN  loss dict:  {'classification_loss': 1.4541353750228883}
2025-01-16 10:54:19,938 [INFO] Step[550/2713]: training loss : 1.4866859102249146 TRAIN  loss dict:  {'classification_loss': 1.4866859102249146}
2025-01-16 10:54:36,031 [INFO] Step[600/2713]: training loss : 1.5268206238746642 TRAIN  loss dict:  {'classification_loss': 1.5268206238746642}
2025-01-16 10:54:52,211 [INFO] Step[650/2713]: training loss : 1.3692284405231476 TRAIN  loss dict:  {'classification_loss': 1.3692284405231476}
2025-01-16 10:55:08,317 [INFO] Step[700/2713]: training loss : 1.5062861812114716 TRAIN  loss dict:  {'classification_loss': 1.5062861812114716}
2025-01-16 10:55:24,421 [INFO] Step[750/2713]: training loss : 1.4676174306869507 TRAIN  loss dict:  {'classification_loss': 1.4676174306869507}
2025-01-16 10:55:40,519 [INFO] Step[800/2713]: training loss : 1.5336454713344574 TRAIN  loss dict:  {'classification_loss': 1.5336454713344574}
2025-01-16 10:55:56,559 [INFO] Step[850/2713]: training loss : 1.6278071308135986 TRAIN  loss dict:  {'classification_loss': 1.6278071308135986}
2025-01-16 10:56:12,657 [INFO] Step[900/2713]: training loss : 1.3990910518169404 TRAIN  loss dict:  {'classification_loss': 1.3990910518169404}
2025-01-16 10:56:28,769 [INFO] Step[950/2713]: training loss : 1.429110244512558 TRAIN  loss dict:  {'classification_loss': 1.429110244512558}
2025-01-16 10:56:44,868 [INFO] Step[1000/2713]: training loss : 1.4583037734031676 TRAIN  loss dict:  {'classification_loss': 1.4583037734031676}
2025-01-16 10:57:00,897 [INFO] Step[1050/2713]: training loss : 1.5083855772018433 TRAIN  loss dict:  {'classification_loss': 1.5083855772018433}
2025-01-16 10:57:16,998 [INFO] Step[1100/2713]: training loss : 1.5169635844230651 TRAIN  loss dict:  {'classification_loss': 1.5169635844230651}
2025-01-16 10:57:33,129 [INFO] Step[1150/2713]: training loss : 1.4858008337020874 TRAIN  loss dict:  {'classification_loss': 1.4858008337020874}
2025-01-16 10:57:49,182 [INFO] Step[1200/2713]: training loss : 1.4219631934165955 TRAIN  loss dict:  {'classification_loss': 1.4219631934165955}
2025-01-16 10:58:05,194 [INFO] Step[1250/2713]: training loss : 1.5231092607975005 TRAIN  loss dict:  {'classification_loss': 1.5231092607975005}
2025-01-16 10:58:21,197 [INFO] Step[1300/2713]: training loss : 1.5962388110160828 TRAIN  loss dict:  {'classification_loss': 1.5962388110160828}
2025-01-16 10:58:37,214 [INFO] Step[1350/2713]: training loss : 1.4883088970184326 TRAIN  loss dict:  {'classification_loss': 1.4883088970184326}
2025-01-16 10:58:53,257 [INFO] Step[1400/2713]: training loss : 1.5662822937965393 TRAIN  loss dict:  {'classification_loss': 1.5662822937965393}
2025-01-16 10:59:09,334 [INFO] Step[1450/2713]: training loss : 1.4546138215065003 TRAIN  loss dict:  {'classification_loss': 1.4546138215065003}
2025-01-16 10:59:25,306 [INFO] Step[1500/2713]: training loss : 1.5565361762046814 TRAIN  loss dict:  {'classification_loss': 1.5565361762046814}
2025-01-16 10:59:41,385 [INFO] Step[1550/2713]: training loss : 1.5220949268341064 TRAIN  loss dict:  {'classification_loss': 1.5220949268341064}
2025-01-16 10:59:57,429 [INFO] Step[1600/2713]: training loss : 1.5533892369270326 TRAIN  loss dict:  {'classification_loss': 1.5533892369270326}
2025-01-16 11:00:13,559 [INFO] Step[1650/2713]: training loss : 1.4159212744235992 TRAIN  loss dict:  {'classification_loss': 1.4159212744235992}
2025-01-16 11:00:29,606 [INFO] Step[1700/2713]: training loss : 1.3987014830112456 TRAIN  loss dict:  {'classification_loss': 1.3987014830112456}
2025-01-16 11:00:45,695 [INFO] Step[1750/2713]: training loss : 1.4665939581394196 TRAIN  loss dict:  {'classification_loss': 1.4665939581394196}
2025-01-16 11:01:01,675 [INFO] Step[1800/2713]: training loss : 1.3925116634368897 TRAIN  loss dict:  {'classification_loss': 1.3925116634368897}
2025-01-16 11:01:17,784 [INFO] Step[1850/2713]: training loss : 1.57067929148674 TRAIN  loss dict:  {'classification_loss': 1.57067929148674}
2025-01-16 11:01:33,863 [INFO] Step[1900/2713]: training loss : 1.492753735780716 TRAIN  loss dict:  {'classification_loss': 1.492753735780716}
2025-01-16 11:01:50,019 [INFO] Step[1950/2713]: training loss : 1.4635664570331572 TRAIN  loss dict:  {'classification_loss': 1.4635664570331572}
2025-01-16 11:02:06,135 [INFO] Step[2000/2713]: training loss : 1.4597561502456664 TRAIN  loss dict:  {'classification_loss': 1.4597561502456664}
2025-01-16 11:02:22,193 [INFO] Step[2050/2713]: training loss : 1.4565554904937743 TRAIN  loss dict:  {'classification_loss': 1.4565554904937743}
2025-01-16 11:02:38,335 [INFO] Step[2100/2713]: training loss : 1.5064261698722838 TRAIN  loss dict:  {'classification_loss': 1.5064261698722838}
2025-01-16 11:02:54,419 [INFO] Step[2150/2713]: training loss : 1.5163420784473418 TRAIN  loss dict:  {'classification_loss': 1.5163420784473418}
2025-01-16 11:03:10,583 [INFO] Step[2200/2713]: training loss : 1.4779699528217316 TRAIN  loss dict:  {'classification_loss': 1.4779699528217316}
2025-01-16 11:03:26,784 [INFO] Step[2250/2713]: training loss : 1.4506899189949036 TRAIN  loss dict:  {'classification_loss': 1.4506899189949036}
2025-01-16 11:03:42,953 [INFO] Step[2300/2713]: training loss : 1.6103457927703857 TRAIN  loss dict:  {'classification_loss': 1.6103457927703857}
2025-01-16 11:03:59,104 [INFO] Step[2350/2713]: training loss : 1.4683815681934356 TRAIN  loss dict:  {'classification_loss': 1.4683815681934356}
2025-01-16 11:04:15,299 [INFO] Step[2400/2713]: training loss : 1.5629657697677612 TRAIN  loss dict:  {'classification_loss': 1.5629657697677612}
2025-01-16 11:04:31,424 [INFO] Step[2450/2713]: training loss : 1.497646083831787 TRAIN  loss dict:  {'classification_loss': 1.497646083831787}
2025-01-16 11:04:47,572 [INFO] Step[2500/2713]: training loss : 1.4684188175201416 TRAIN  loss dict:  {'classification_loss': 1.4684188175201416}
2025-01-16 11:05:03,726 [INFO] Step[2550/2713]: training loss : 1.425122948884964 TRAIN  loss dict:  {'classification_loss': 1.425122948884964}
2025-01-16 11:05:19,893 [INFO] Step[2600/2713]: training loss : 1.406755347251892 TRAIN  loss dict:  {'classification_loss': 1.406755347251892}
2025-01-16 11:05:36,067 [INFO] Step[2650/2713]: training loss : 1.4390865921974183 TRAIN  loss dict:  {'classification_loss': 1.4390865921974183}
2025-01-16 11:05:52,279 [INFO] Step[2700/2713]: training loss : 1.6210203862190247 TRAIN  loss dict:  {'classification_loss': 1.6210203862190247}
2025-01-16 11:07:11,283 [INFO] Label accuracies statistics:
2025-01-16 11:07:11,283 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.0, 13: 0.5, 14: 0.5, 15: 0.3333333333333333, 16: 0.25, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.5, 43: 0.25, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.5, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 1.0, 61: 0.5, 62: 0.5, 63: 0.0, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.0, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.25, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.0, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.25, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.25, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.0, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 0.0, 156: 0.25, 157: 0.5, 158: 0.3333333333333333, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 0.75, 197: 1.0, 198: 0.75, 199: 1.0, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.0, 204: 0.5, 205: 0.5, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.25, 214: 0.5, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.0, 230: 1.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 0.5, 242: 0.5, 243: 0.25, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.0, 249: 0.5, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 0.5, 256: 1.0, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.75, 272: 0.75, 273: 0.25, 274: 1.0, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.25, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.25, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.5, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 1.0, 326: 0.5, 327: 0.75, 328: 0.75, 329: 0.5, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.0, 334: 0.5, 335: 0.5, 336: 0.5, 337: 0.75, 338: 0.75, 339: 1.0, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.25, 350: 0.5, 351: 0.75, 352: 0.25, 353: 0.0, 354: 0.0, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.0, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.0, 376: 1.0, 377: 0.25, 378: 0.75, 379: 1.0, 380: 0.75, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 0.75, 386: 0.25, 387: 0.75, 388: 0.5, 389: 0.25, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.25, 394: 0.0, 395: 1.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 11:07:14,204 [INFO] [3] TRAIN  loss: 1.4856897625938976 acc: 0.8429782528566163
2025-01-16 11:07:14,204 [INFO] [3] TRAIN  loss dict: {'classification_loss': 1.4856897625938976}
2025-01-16 11:07:14,204 [INFO] [3] VALIDATION loss: 2.098376401497009 VALIDATION acc: 0.7021943573667712
2025-01-16 11:07:14,204 [INFO] [3] VALIDATION loss dict: {'classification_loss': 2.098376401497009}
2025-01-16 11:07:14,204 [INFO] 
2025-01-16 11:07:34,637 [INFO] Step[50/2713]: training loss : 1.3902723681926727 TRAIN  loss dict:  {'classification_loss': 1.3902723681926727}
2025-01-16 11:07:50,787 [INFO] Step[100/2713]: training loss : 1.422543501853943 TRAIN  loss dict:  {'classification_loss': 1.422543501853943}
2025-01-16 11:08:07,005 [INFO] Step[150/2713]: training loss : 1.385355304479599 TRAIN  loss dict:  {'classification_loss': 1.385355304479599}
2025-01-16 11:08:23,255 [INFO] Step[200/2713]: training loss : 1.2845822942256928 TRAIN  loss dict:  {'classification_loss': 1.2845822942256928}
2025-01-16 11:08:39,522 [INFO] Step[250/2713]: training loss : 1.4372943890094758 TRAIN  loss dict:  {'classification_loss': 1.4372943890094758}
2025-01-16 11:08:55,720 [INFO] Step[300/2713]: training loss : 1.341345293521881 TRAIN  loss dict:  {'classification_loss': 1.341345293521881}
2025-01-16 11:09:11,924 [INFO] Step[350/2713]: training loss : 1.3560494267940522 TRAIN  loss dict:  {'classification_loss': 1.3560494267940522}
2025-01-16 11:09:28,128 [INFO] Step[400/2713]: training loss : 1.338018193244934 TRAIN  loss dict:  {'classification_loss': 1.338018193244934}
2025-01-16 11:09:44,334 [INFO] Step[450/2713]: training loss : 1.281052713394165 TRAIN  loss dict:  {'classification_loss': 1.281052713394165}
2025-01-16 11:10:00,535 [INFO] Step[500/2713]: training loss : 1.451217702627182 TRAIN  loss dict:  {'classification_loss': 1.451217702627182}
2025-01-16 11:10:16,737 [INFO] Step[550/2713]: training loss : 1.3929622411727904 TRAIN  loss dict:  {'classification_loss': 1.3929622411727904}
2025-01-16 11:10:32,966 [INFO] Step[600/2713]: training loss : 1.5738317108154296 TRAIN  loss dict:  {'classification_loss': 1.5738317108154296}
2025-01-16 11:10:49,148 [INFO] Step[650/2713]: training loss : 1.4886796593666076 TRAIN  loss dict:  {'classification_loss': 1.4886796593666076}
2025-01-16 11:11:05,332 [INFO] Step[700/2713]: training loss : 1.3402272009849547 TRAIN  loss dict:  {'classification_loss': 1.3402272009849547}
2025-01-16 11:11:21,556 [INFO] Step[750/2713]: training loss : 1.3246499347686767 TRAIN  loss dict:  {'classification_loss': 1.3246499347686767}
2025-01-16 11:11:37,794 [INFO] Step[800/2713]: training loss : 1.338202120065689 TRAIN  loss dict:  {'classification_loss': 1.338202120065689}
2025-01-16 11:11:54,040 [INFO] Step[850/2713]: training loss : 1.3353835582733153 TRAIN  loss dict:  {'classification_loss': 1.3353835582733153}
2025-01-16 11:12:10,313 [INFO] Step[900/2713]: training loss : 1.368795530796051 TRAIN  loss dict:  {'classification_loss': 1.368795530796051}
2025-01-16 11:12:26,543 [INFO] Step[950/2713]: training loss : 1.3280828893184662 TRAIN  loss dict:  {'classification_loss': 1.3280828893184662}
2025-01-16 11:12:42,659 [INFO] Step[1000/2713]: training loss : 1.2795673835277557 TRAIN  loss dict:  {'classification_loss': 1.2795673835277557}
2025-01-16 11:12:58,877 [INFO] Step[1050/2713]: training loss : 1.2861747300624848 TRAIN  loss dict:  {'classification_loss': 1.2861747300624848}
2025-01-16 11:13:15,069 [INFO] Step[1100/2713]: training loss : 1.4012201476097106 TRAIN  loss dict:  {'classification_loss': 1.4012201476097106}
2025-01-16 11:13:31,245 [INFO] Step[1150/2713]: training loss : 1.4853171622753143 TRAIN  loss dict:  {'classification_loss': 1.4853171622753143}
2025-01-16 11:13:47,459 [INFO] Step[1200/2713]: training loss : 1.4914909136295318 TRAIN  loss dict:  {'classification_loss': 1.4914909136295318}
2025-01-16 11:14:03,621 [INFO] Step[1250/2713]: training loss : 1.3289132988452912 TRAIN  loss dict:  {'classification_loss': 1.3289132988452912}
2025-01-16 11:14:19,819 [INFO] Step[1300/2713]: training loss : 1.4296768379211426 TRAIN  loss dict:  {'classification_loss': 1.4296768379211426}
2025-01-16 11:14:36,016 [INFO] Step[1350/2713]: training loss : 1.336561815738678 TRAIN  loss dict:  {'classification_loss': 1.336561815738678}
2025-01-16 11:14:52,241 [INFO] Step[1400/2713]: training loss : 1.3222079980373382 TRAIN  loss dict:  {'classification_loss': 1.3222079980373382}
2025-01-16 11:15:08,468 [INFO] Step[1450/2713]: training loss : 1.4577982652187347 TRAIN  loss dict:  {'classification_loss': 1.4577982652187347}
2025-01-16 11:15:24,673 [INFO] Step[1500/2713]: training loss : 1.4273890209198 TRAIN  loss dict:  {'classification_loss': 1.4273890209198}
2025-01-16 11:15:40,927 [INFO] Step[1550/2713]: training loss : 1.4749007093906403 TRAIN  loss dict:  {'classification_loss': 1.4749007093906403}
2025-01-16 11:15:57,140 [INFO] Step[1600/2713]: training loss : 1.3814394342899323 TRAIN  loss dict:  {'classification_loss': 1.3814394342899323}
2025-01-16 11:16:13,313 [INFO] Step[1650/2713]: training loss : 1.476274003982544 TRAIN  loss dict:  {'classification_loss': 1.476274003982544}
2025-01-16 11:16:29,469 [INFO] Step[1700/2713]: training loss : 1.2992922389507293 TRAIN  loss dict:  {'classification_loss': 1.2992922389507293}
2025-01-16 11:16:45,699 [INFO] Step[1750/2713]: training loss : 1.5123640382289887 TRAIN  loss dict:  {'classification_loss': 1.5123640382289887}
2025-01-16 11:17:01,869 [INFO] Step[1800/2713]: training loss : 1.4775711166858674 TRAIN  loss dict:  {'classification_loss': 1.4775711166858674}
2025-01-16 11:17:18,042 [INFO] Step[1850/2713]: training loss : 1.2794677901268006 TRAIN  loss dict:  {'classification_loss': 1.2794677901268006}
2025-01-16 11:17:34,313 [INFO] Step[1900/2713]: training loss : 1.3021283948421478 TRAIN  loss dict:  {'classification_loss': 1.3021283948421478}
2025-01-16 11:17:50,539 [INFO] Step[1950/2713]: training loss : 1.3989542531967163 TRAIN  loss dict:  {'classification_loss': 1.3989542531967163}
2025-01-16 11:18:06,709 [INFO] Step[2000/2713]: training loss : 1.3980663895606995 TRAIN  loss dict:  {'classification_loss': 1.3980663895606995}
2025-01-16 11:18:22,952 [INFO] Step[2050/2713]: training loss : 1.418754242658615 TRAIN  loss dict:  {'classification_loss': 1.418754242658615}
2025-01-16 11:18:39,077 [INFO] Step[2100/2713]: training loss : 1.4864894843101502 TRAIN  loss dict:  {'classification_loss': 1.4864894843101502}
2025-01-16 11:18:55,334 [INFO] Step[2150/2713]: training loss : 1.307083786725998 TRAIN  loss dict:  {'classification_loss': 1.307083786725998}
2025-01-16 11:19:11,501 [INFO] Step[2200/2713]: training loss : 1.264724315404892 TRAIN  loss dict:  {'classification_loss': 1.264724315404892}
2025-01-16 11:19:27,692 [INFO] Step[2250/2713]: training loss : 1.4602021121978759 TRAIN  loss dict:  {'classification_loss': 1.4602021121978759}
2025-01-16 11:19:43,830 [INFO] Step[2300/2713]: training loss : 1.4134805476665497 TRAIN  loss dict:  {'classification_loss': 1.4134805476665497}
2025-01-16 11:19:59,974 [INFO] Step[2350/2713]: training loss : 1.4391356825828552 TRAIN  loss dict:  {'classification_loss': 1.4391356825828552}
2025-01-16 11:20:16,180 [INFO] Step[2400/2713]: training loss : 1.2976309335231782 TRAIN  loss dict:  {'classification_loss': 1.2976309335231782}
2025-01-16 11:20:32,387 [INFO] Step[2450/2713]: training loss : 1.353620743751526 TRAIN  loss dict:  {'classification_loss': 1.353620743751526}
2025-01-16 11:20:48,597 [INFO] Step[2500/2713]: training loss : 1.3565170359611511 TRAIN  loss dict:  {'classification_loss': 1.3565170359611511}
2025-01-16 11:21:04,856 [INFO] Step[2550/2713]: training loss : 1.2923495531082154 TRAIN  loss dict:  {'classification_loss': 1.2923495531082154}
2025-01-16 11:21:21,054 [INFO] Step[2600/2713]: training loss : 1.445957453250885 TRAIN  loss dict:  {'classification_loss': 1.445957453250885}
2025-01-16 11:21:37,279 [INFO] Step[2650/2713]: training loss : 1.341100105047226 TRAIN  loss dict:  {'classification_loss': 1.341100105047226}
2025-01-16 11:21:53,516 [INFO] Step[2700/2713]: training loss : 1.3258750903606416 TRAIN  loss dict:  {'classification_loss': 1.3258750903606416}
2025-01-16 11:23:12,063 [INFO] Label accuracies statistics:
2025-01-16 11:23:12,064 [INFO] {0: 1.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.0, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.25, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.5, 26: 0.25, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.5, 33: 0.5, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 0.75, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.0, 67: 0.75, 68: 0.25, 69: 0.75, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 0.75, 93: 0.75, 94: 0.0, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 0.75, 103: 1.0, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 0.5, 132: 0.5, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.25, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 0.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.25, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.0, 190: 0.75, 191: 0.25, 192: 0.75, 193: 0.25, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.25, 204: 0.25, 205: 1.0, 206: 0.25, 207: 0.75, 208: 1.0, 209: 0.25, 210: 1.0, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.5, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.5, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.0, 230: 1.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.0, 240: 1.0, 241: 0.75, 242: 1.0, 243: 0.75, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.5, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 1.0, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.0, 269: 0.75, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.75, 285: 1.0, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.5, 300: 0.75, 301: 1.0, 302: 0.0, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.5, 308: 1.0, 309: 0.75, 310: 0.25, 311: 0.75, 312: 0.75, 313: 0.0, 314: 0.75, 315: 1.0, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.5, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.5, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.25, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.0, 335: 1.0, 336: 0.25, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.25, 350: 0.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.5, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.5, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 0.75, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.75, 388: 0.75, 389: 0.75, 390: 1.0, 391: 1.0, 392: 0.5, 393: 0.0, 394: 0.5, 395: 0.25, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 11:23:14,877 [INFO] [4] TRAIN  loss: 1.3813744024557246 acc: 0.8771347831428923
2025-01-16 11:23:14,877 [INFO] [4] TRAIN  loss dict: {'classification_loss': 1.3813744024557246}
2025-01-16 11:23:14,877 [INFO] [4] VALIDATION loss: 2.0900429973476813 VALIDATION acc: 0.715987460815047
2025-01-16 11:23:14,877 [INFO] [4] VALIDATION loss dict: {'classification_loss': 2.0900429973476813}
2025-01-16 11:23:14,878 [INFO] 
2025-01-16 11:23:35,050 [INFO] Step[50/2713]: training loss : 1.410283408164978 TRAIN  loss dict:  {'classification_loss': 1.410283408164978}
2025-01-16 11:23:51,227 [INFO] Step[100/2713]: training loss : 1.2917444229125976 TRAIN  loss dict:  {'classification_loss': 1.2917444229125976}
2025-01-16 11:24:07,564 [INFO] Step[150/2713]: training loss : 1.3865508663654327 TRAIN  loss dict:  {'classification_loss': 1.3865508663654327}
2025-01-16 11:24:24,019 [INFO] Step[200/2713]: training loss : 1.423534744977951 TRAIN  loss dict:  {'classification_loss': 1.423534744977951}
2025-01-16 11:24:40,304 [INFO] Step[250/2713]: training loss : 1.347874982357025 TRAIN  loss dict:  {'classification_loss': 1.347874982357025}
2025-01-16 11:24:56,573 [INFO] Step[300/2713]: training loss : 1.3034909296035766 TRAIN  loss dict:  {'classification_loss': 1.3034909296035766}
2025-01-16 11:25:12,868 [INFO] Step[350/2713]: training loss : 1.1834800517559052 TRAIN  loss dict:  {'classification_loss': 1.1834800517559052}
2025-01-16 11:25:29,058 [INFO] Step[400/2713]: training loss : 1.2585593116283418 TRAIN  loss dict:  {'classification_loss': 1.2585593116283418}
2025-01-16 11:25:45,320 [INFO] Step[450/2713]: training loss : 1.2283911907672882 TRAIN  loss dict:  {'classification_loss': 1.2283911907672882}
2025-01-16 11:26:01,559 [INFO] Step[500/2713]: training loss : 1.4442089152336122 TRAIN  loss dict:  {'classification_loss': 1.4442089152336122}
2025-01-16 11:26:17,742 [INFO] Step[550/2713]: training loss : 1.2441722691059112 TRAIN  loss dict:  {'classification_loss': 1.2441722691059112}
2025-01-16 11:26:33,981 [INFO] Step[600/2713]: training loss : 1.3554330122470857 TRAIN  loss dict:  {'classification_loss': 1.3554330122470857}
2025-01-16 11:26:50,229 [INFO] Step[650/2713]: training loss : 1.29584463596344 TRAIN  loss dict:  {'classification_loss': 1.29584463596344}
2025-01-16 11:27:06,509 [INFO] Step[700/2713]: training loss : 1.2556535601615906 TRAIN  loss dict:  {'classification_loss': 1.2556535601615906}
2025-01-16 11:27:22,747 [INFO] Step[750/2713]: training loss : 1.2554199051856996 TRAIN  loss dict:  {'classification_loss': 1.2554199051856996}
2025-01-16 11:27:38,962 [INFO] Step[800/2713]: training loss : 1.3332253408432007 TRAIN  loss dict:  {'classification_loss': 1.3332253408432007}
2025-01-16 11:27:55,188 [INFO] Step[850/2713]: training loss : 1.3180941867828369 TRAIN  loss dict:  {'classification_loss': 1.3180941867828369}
2025-01-16 11:28:11,434 [INFO] Step[900/2713]: training loss : 1.3859020423889161 TRAIN  loss dict:  {'classification_loss': 1.3859020423889161}
2025-01-16 11:28:27,688 [INFO] Step[950/2713]: training loss : 1.291540367603302 TRAIN  loss dict:  {'classification_loss': 1.291540367603302}
2025-01-16 11:28:43,973 [INFO] Step[1000/2713]: training loss : 1.25314843416214 TRAIN  loss dict:  {'classification_loss': 1.25314843416214}
2025-01-16 11:29:00,288 [INFO] Step[1050/2713]: training loss : 1.356281168460846 TRAIN  loss dict:  {'classification_loss': 1.356281168460846}
2025-01-16 11:29:16,452 [INFO] Step[1100/2713]: training loss : 1.285492081642151 TRAIN  loss dict:  {'classification_loss': 1.285492081642151}
2025-01-16 11:29:32,692 [INFO] Step[1150/2713]: training loss : 1.4087888264656068 TRAIN  loss dict:  {'classification_loss': 1.4087888264656068}
2025-01-16 11:29:48,971 [INFO] Step[1200/2713]: training loss : 1.3794787645339965 TRAIN  loss dict:  {'classification_loss': 1.3794787645339965}
2025-01-16 11:30:05,190 [INFO] Step[1250/2713]: training loss : 1.2550710248947143 TRAIN  loss dict:  {'classification_loss': 1.2550710248947143}
2025-01-16 11:30:21,431 [INFO] Step[1300/2713]: training loss : 1.3049492228031159 TRAIN  loss dict:  {'classification_loss': 1.3049492228031159}
2025-01-16 11:30:37,635 [INFO] Step[1350/2713]: training loss : 1.2949284386634827 TRAIN  loss dict:  {'classification_loss': 1.2949284386634827}
2025-01-16 11:30:53,882 [INFO] Step[1400/2713]: training loss : 1.2700333523750305 TRAIN  loss dict:  {'classification_loss': 1.2700333523750305}
2025-01-16 11:31:10,163 [INFO] Step[1450/2713]: training loss : 1.3585841059684753 TRAIN  loss dict:  {'classification_loss': 1.3585841059684753}
2025-01-16 11:31:26,507 [INFO] Step[1500/2713]: training loss : 1.2970562362670899 TRAIN  loss dict:  {'classification_loss': 1.2970562362670899}
2025-01-16 11:31:42,623 [INFO] Step[1550/2713]: training loss : 1.4094953536987305 TRAIN  loss dict:  {'classification_loss': 1.4094953536987305}
2025-01-16 11:31:58,778 [INFO] Step[1600/2713]: training loss : 1.3837178456783295 TRAIN  loss dict:  {'classification_loss': 1.3837178456783295}
2025-01-16 11:32:14,973 [INFO] Step[1650/2713]: training loss : 1.2530207443237305 TRAIN  loss dict:  {'classification_loss': 1.2530207443237305}
2025-01-16 11:32:31,127 [INFO] Step[1700/2713]: training loss : 1.2434328997135162 TRAIN  loss dict:  {'classification_loss': 1.2434328997135162}
2025-01-16 11:32:47,265 [INFO] Step[1750/2713]: training loss : 1.3379488563537598 TRAIN  loss dict:  {'classification_loss': 1.3379488563537598}
2025-01-16 11:33:03,434 [INFO] Step[1800/2713]: training loss : 1.3692488169670105 TRAIN  loss dict:  {'classification_loss': 1.3692488169670105}
2025-01-16 11:33:19,567 [INFO] Step[1850/2713]: training loss : 1.3067331206798554 TRAIN  loss dict:  {'classification_loss': 1.3067331206798554}
2025-01-16 11:33:35,753 [INFO] Step[1900/2713]: training loss : 1.3383838403224946 TRAIN  loss dict:  {'classification_loss': 1.3383838403224946}
2025-01-16 11:33:52,015 [INFO] Step[1950/2713]: training loss : 1.3899123084545135 TRAIN  loss dict:  {'classification_loss': 1.3899123084545135}
2025-01-16 11:34:08,121 [INFO] Step[2000/2713]: training loss : 1.3106268775463104 TRAIN  loss dict:  {'classification_loss': 1.3106268775463104}
2025-01-16 11:34:24,535 [INFO] Step[2050/2713]: training loss : 1.2680375063419342 TRAIN  loss dict:  {'classification_loss': 1.2680375063419342}
2025-01-16 11:34:40,665 [INFO] Step[2100/2713]: training loss : 1.2626018023490906 TRAIN  loss dict:  {'classification_loss': 1.2626018023490906}
2025-01-16 11:34:56,870 [INFO] Step[2150/2713]: training loss : 1.2461604750156403 TRAIN  loss dict:  {'classification_loss': 1.2461604750156403}
2025-01-16 11:35:12,978 [INFO] Step[2200/2713]: training loss : 1.325957751274109 TRAIN  loss dict:  {'classification_loss': 1.325957751274109}
2025-01-16 11:35:29,100 [INFO] Step[2250/2713]: training loss : 1.353335520029068 TRAIN  loss dict:  {'classification_loss': 1.353335520029068}
2025-01-16 11:35:45,189 [INFO] Step[2300/2713]: training loss : 1.2896853792667389 TRAIN  loss dict:  {'classification_loss': 1.2896853792667389}
2025-01-16 11:36:01,353 [INFO] Step[2350/2713]: training loss : 1.343670699596405 TRAIN  loss dict:  {'classification_loss': 1.343670699596405}
2025-01-16 11:36:17,459 [INFO] Step[2400/2713]: training loss : 1.2752569210529328 TRAIN  loss dict:  {'classification_loss': 1.2752569210529328}
2025-01-16 11:36:33,641 [INFO] Step[2450/2713]: training loss : 1.3107884013652802 TRAIN  loss dict:  {'classification_loss': 1.3107884013652802}
2025-01-16 11:36:49,741 [INFO] Step[2500/2713]: training loss : 1.3096021294593811 TRAIN  loss dict:  {'classification_loss': 1.3096021294593811}
2025-01-16 11:37:05,994 [INFO] Step[2550/2713]: training loss : 1.446750569343567 TRAIN  loss dict:  {'classification_loss': 1.446750569343567}
2025-01-16 11:37:22,152 [INFO] Step[2600/2713]: training loss : 1.4652425849437714 TRAIN  loss dict:  {'classification_loss': 1.4652425849437714}
2025-01-16 11:37:38,378 [INFO] Step[2650/2713]: training loss : 1.3594613564014435 TRAIN  loss dict:  {'classification_loss': 1.3594613564014435}
2025-01-16 11:37:54,625 [INFO] Step[2700/2713]: training loss : 1.287993485927582 TRAIN  loss dict:  {'classification_loss': 1.287993485927582}
2025-01-16 11:39:12,842 [INFO] Label accuracies statistics:
2025-01-16 11:39:12,842 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.5, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.5, 69: 1.0, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.25, 77: 0.75, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.75, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 0.75, 112: 0.75, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.5, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 0.75, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.5, 162: 1.0, 163: 0.5, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 1.0, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.5, 205: 1.0, 206: 0.25, 207: 0.25, 208: 0.5, 209: 0.5, 210: 0.75, 211: 0.0, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.25, 229: 0.5, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.5, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 0.75, 242: 0.0, 243: 0.0, 244: 0.75, 245: 0.5, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.25, 262: 0.75, 263: 0.75, 264: 0.5, 265: 0.75, 266: 0.75, 267: 0.5, 268: 0.25, 269: 0.75, 270: 0.75, 271: 0.75, 272: 0.75, 273: 0.25, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.0, 279: 0.75, 280: 0.75, 281: 0.25, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.5, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.25, 291: 0.5, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.75, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.5, 310: 1.0, 311: 0.75, 312: 0.75, 313: 0.5, 314: 0.5, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.5, 325: 1.0, 326: 0.5, 327: 0.5, 328: 0.0, 329: 0.75, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.0, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.5, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.5, 368: 1.0, 369: 1.0, 370: 0.75, 371: 1.0, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.75, 380: 0.75, 381: 0.25, 382: 0.75, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.25, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-16 11:39:15,692 [INFO] [5] TRAIN  loss: 1.3212924985001546 acc: 0.8988819265266004
2025-01-16 11:39:15,692 [INFO] [5] TRAIN  loss dict: {'classification_loss': 1.3212924985001546}
2025-01-16 11:39:15,692 [INFO] [5] VALIDATION loss: 1.9848731080616326 VALIDATION acc: 0.7373040752351098
2025-01-16 11:39:15,692 [INFO] [5] VALIDATION loss dict: {'classification_loss': 1.9848731080616326}
2025-01-16 11:39:15,692 [INFO] 
2025-01-16 11:39:37,094 [INFO] Step[50/2713]: training loss : 1.1940127396583557 TRAIN  loss dict:  {'classification_loss': 1.1940127396583557}
2025-01-16 11:39:53,172 [INFO] Step[100/2713]: training loss : 1.3093373930454255 TRAIN  loss dict:  {'classification_loss': 1.3093373930454255}
2025-01-16 11:40:09,308 [INFO] Step[150/2713]: training loss : 1.29291889667511 TRAIN  loss dict:  {'classification_loss': 1.29291889667511}
2025-01-16 11:40:25,416 [INFO] Step[200/2713]: training loss : 1.2035193014144898 TRAIN  loss dict:  {'classification_loss': 1.2035193014144898}
2025-01-16 11:40:41,593 [INFO] Step[250/2713]: training loss : 1.326441127061844 TRAIN  loss dict:  {'classification_loss': 1.326441127061844}
2025-01-16 11:40:57,644 [INFO] Step[300/2713]: training loss : 1.210692229270935 TRAIN  loss dict:  {'classification_loss': 1.210692229270935}
2025-01-16 11:41:13,810 [INFO] Step[350/2713]: training loss : 1.3634802794456482 TRAIN  loss dict:  {'classification_loss': 1.3634802794456482}
2025-01-16 11:41:29,875 [INFO] Step[400/2713]: training loss : 1.2626239609718324 TRAIN  loss dict:  {'classification_loss': 1.2626239609718324}
2025-01-16 11:41:46,000 [INFO] Step[450/2713]: training loss : 1.2752709448337556 TRAIN  loss dict:  {'classification_loss': 1.2752709448337556}
2025-01-16 11:42:02,054 [INFO] Step[500/2713]: training loss : 1.2345970034599305 TRAIN  loss dict:  {'classification_loss': 1.2345970034599305}
2025-01-16 11:42:18,175 [INFO] Step[550/2713]: training loss : 1.2153195893764497 TRAIN  loss dict:  {'classification_loss': 1.2153195893764497}
2025-01-16 11:42:34,245 [INFO] Step[600/2713]: training loss : 1.22703874707222 TRAIN  loss dict:  {'classification_loss': 1.22703874707222}
2025-01-16 11:42:50,431 [INFO] Step[650/2713]: training loss : 1.248838436603546 TRAIN  loss dict:  {'classification_loss': 1.248838436603546}
2025-01-16 11:43:06,528 [INFO] Step[700/2713]: training loss : 1.3632376420497894 TRAIN  loss dict:  {'classification_loss': 1.3632376420497894}
2025-01-16 11:43:22,623 [INFO] Step[750/2713]: training loss : 1.2828051769733428 TRAIN  loss dict:  {'classification_loss': 1.2828051769733428}
2025-01-16 11:43:38,730 [INFO] Step[800/2713]: training loss : 1.4168936204910278 TRAIN  loss dict:  {'classification_loss': 1.4168936204910278}
2025-01-16 11:43:54,803 [INFO] Step[850/2713]: training loss : 1.3163097321987152 TRAIN  loss dict:  {'classification_loss': 1.3163097321987152}
2025-01-16 11:44:10,895 [INFO] Step[900/2713]: training loss : 1.2629302883148192 TRAIN  loss dict:  {'classification_loss': 1.2629302883148192}
2025-01-16 11:44:27,046 [INFO] Step[950/2713]: training loss : 1.3408964967727661 TRAIN  loss dict:  {'classification_loss': 1.3408964967727661}
2025-01-16 11:44:43,187 [INFO] Step[1000/2713]: training loss : 1.3114960825443267 TRAIN  loss dict:  {'classification_loss': 1.3114960825443267}
2025-01-16 11:44:59,357 [INFO] Step[1050/2713]: training loss : 1.257230988740921 TRAIN  loss dict:  {'classification_loss': 1.257230988740921}
2025-01-16 11:45:15,456 [INFO] Step[1100/2713]: training loss : 1.2403278172016143 TRAIN  loss dict:  {'classification_loss': 1.2403278172016143}
2025-01-16 11:45:31,629 [INFO] Step[1150/2713]: training loss : 1.220977213382721 TRAIN  loss dict:  {'classification_loss': 1.220977213382721}
2025-01-16 11:45:47,709 [INFO] Step[1200/2713]: training loss : 1.2817579090595246 TRAIN  loss dict:  {'classification_loss': 1.2817579090595246}
2025-01-16 11:46:03,769 [INFO] Step[1250/2713]: training loss : 1.2913559579849243 TRAIN  loss dict:  {'classification_loss': 1.2913559579849243}
2025-01-16 11:46:19,860 [INFO] Step[1300/2713]: training loss : 1.321767077445984 TRAIN  loss dict:  {'classification_loss': 1.321767077445984}
2025-01-16 11:46:35,984 [INFO] Step[1350/2713]: training loss : 1.2974475157260894 TRAIN  loss dict:  {'classification_loss': 1.2974475157260894}
2025-01-16 11:46:52,083 [INFO] Step[1400/2713]: training loss : 1.2835440278053283 TRAIN  loss dict:  {'classification_loss': 1.2835440278053283}
2025-01-16 11:47:08,226 [INFO] Step[1450/2713]: training loss : 1.2561527848243714 TRAIN  loss dict:  {'classification_loss': 1.2561527848243714}
2025-01-16 11:47:24,332 [INFO] Step[1500/2713]: training loss : 1.2854725515842438 TRAIN  loss dict:  {'classification_loss': 1.2854725515842438}
2025-01-16 11:47:40,454 [INFO] Step[1550/2713]: training loss : 1.2577446842193603 TRAIN  loss dict:  {'classification_loss': 1.2577446842193603}
2025-01-16 11:47:56,507 [INFO] Step[1600/2713]: training loss : 1.2293610239028931 TRAIN  loss dict:  {'classification_loss': 1.2293610239028931}
2025-01-16 11:48:12,676 [INFO] Step[1650/2713]: training loss : 1.2668433272838593 TRAIN  loss dict:  {'classification_loss': 1.2668433272838593}
2025-01-16 11:48:28,735 [INFO] Step[1700/2713]: training loss : 1.1955174124240875 TRAIN  loss dict:  {'classification_loss': 1.1955174124240875}
2025-01-16 11:48:44,888 [INFO] Step[1750/2713]: training loss : 1.2187872731685638 TRAIN  loss dict:  {'classification_loss': 1.2187872731685638}
2025-01-16 11:49:01,007 [INFO] Step[1800/2713]: training loss : 1.372507530450821 TRAIN  loss dict:  {'classification_loss': 1.372507530450821}
2025-01-16 11:49:17,160 [INFO] Step[1850/2713]: training loss : 1.2671558165550232 TRAIN  loss dict:  {'classification_loss': 1.2671558165550232}
2025-01-16 11:49:33,256 [INFO] Step[1900/2713]: training loss : 1.233748309612274 TRAIN  loss dict:  {'classification_loss': 1.233748309612274}
2025-01-16 11:49:49,392 [INFO] Step[1950/2713]: training loss : 1.1990031492710114 TRAIN  loss dict:  {'classification_loss': 1.1990031492710114}
2025-01-16 11:50:05,475 [INFO] Step[2000/2713]: training loss : 1.282584890127182 TRAIN  loss dict:  {'classification_loss': 1.282584890127182}
2025-01-16 11:50:21,519 [INFO] Step[2050/2713]: training loss : 1.193873393535614 TRAIN  loss dict:  {'classification_loss': 1.193873393535614}
2025-01-16 11:50:37,662 [INFO] Step[2100/2713]: training loss : 1.2817100965976715 TRAIN  loss dict:  {'classification_loss': 1.2817100965976715}
2025-01-16 11:50:53,779 [INFO] Step[2150/2713]: training loss : 1.3264058494567872 TRAIN  loss dict:  {'classification_loss': 1.3264058494567872}
2025-01-16 11:51:09,848 [INFO] Step[2200/2713]: training loss : 1.2233864164352417 TRAIN  loss dict:  {'classification_loss': 1.2233864164352417}
2025-01-16 11:51:25,948 [INFO] Step[2250/2713]: training loss : 1.3576857888698577 TRAIN  loss dict:  {'classification_loss': 1.3576857888698577}
2025-01-16 11:51:42,026 [INFO] Step[2300/2713]: training loss : 1.460086942911148 TRAIN  loss dict:  {'classification_loss': 1.460086942911148}
2025-01-16 11:51:58,095 [INFO] Step[2350/2713]: training loss : 1.4128002631664276 TRAIN  loss dict:  {'classification_loss': 1.4128002631664276}
2025-01-16 11:52:14,169 [INFO] Step[2400/2713]: training loss : 1.1963023769855499 TRAIN  loss dict:  {'classification_loss': 1.1963023769855499}
2025-01-16 11:52:30,182 [INFO] Step[2450/2713]: training loss : 1.2457521259784698 TRAIN  loss dict:  {'classification_loss': 1.2457521259784698}
2025-01-16 11:52:46,205 [INFO] Step[2500/2713]: training loss : 1.3397581148147584 TRAIN  loss dict:  {'classification_loss': 1.3397581148147584}
2025-01-16 11:53:02,275 [INFO] Step[2550/2713]: training loss : 1.3522856509685517 TRAIN  loss dict:  {'classification_loss': 1.3522856509685517}
2025-01-16 11:53:18,307 [INFO] Step[2600/2713]: training loss : 1.324635113477707 TRAIN  loss dict:  {'classification_loss': 1.324635113477707}
2025-01-16 11:53:34,363 [INFO] Step[2650/2713]: training loss : 1.2448962652683258 TRAIN  loss dict:  {'classification_loss': 1.2448962652683258}
2025-01-16 11:53:50,427 [INFO] Step[2700/2713]: training loss : 1.2580448913574218 TRAIN  loss dict:  {'classification_loss': 1.2580448913574218}
2025-01-16 11:55:08,623 [INFO] Label accuracies statistics:
2025-01-16 11:55:08,623 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.5, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.25, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 0.75, 36: 0.25, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 1.0, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.0, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 1.0, 69: 0.5, 70: 0.5, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.25, 84: 0.5, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.5, 89: 1.0, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 0.75, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.0, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 0.75, 132: 0.0, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.5, 142: 0.0, 143: 1.0, 144: 0.5, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 0.75, 156: 0.25, 157: 0.5, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.5, 169: 0.25, 170: 1.0, 171: 0.25, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.5, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.0, 190: 0.5, 191: 0.5, 192: 0.75, 193: 1.0, 194: 0.75, 195: 0.0, 196: 0.75, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.5, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.75, 209: 0.5, 210: 0.75, 211: 0.25, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.5, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.0, 232: 0.75, 233: 0.5, 234: 1.0, 235: 0.25, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 0.75, 242: 0.0, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 1.0, 252: 0.5, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.25, 262: 0.75, 263: 1.0, 264: 0.5, 265: 0.75, 266: 1.0, 267: 0.25, 268: 0.0, 269: 1.0, 270: 1.0, 271: 0.25, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.25, 279: 1.0, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.25, 284: 0.5, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.25, 297: 0.75, 298: 0.75, 299: 0.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.5, 315: 0.75, 316: 0.0, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.25, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.25, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.25, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.25, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 0.75, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.25, 388: 0.75, 389: 0.5, 390: 0.5, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 11:55:08,625 [INFO] [6] TRAIN  loss: 1.280537073259814 acc: 0.9093254699594545
2025-01-16 11:55:08,625 [INFO] [6] TRAIN  loss dict: {'classification_loss': 1.280537073259814}
2025-01-16 11:55:08,625 [INFO] [6] VALIDATION loss: 2.0715278078962984 VALIDATION acc: 0.7078369905956113
2025-01-16 11:55:08,625 [INFO] [6] VALIDATION loss dict: {'classification_loss': 2.0715278078962984}
2025-01-16 11:55:08,625 [INFO] 
2025-01-16 11:55:29,964 [INFO] Step[50/2713]: training loss : 1.2767953598499298 TRAIN  loss dict:  {'classification_loss': 1.2767953598499298}
2025-01-16 11:55:46,000 [INFO] Step[100/2713]: training loss : 1.321509827375412 TRAIN  loss dict:  {'classification_loss': 1.321509827375412}
2025-01-16 11:56:02,198 [INFO] Step[150/2713]: training loss : 1.1472648966312409 TRAIN  loss dict:  {'classification_loss': 1.1472648966312409}
2025-01-16 11:56:18,288 [INFO] Step[200/2713]: training loss : 1.2648807847499848 TRAIN  loss dict:  {'classification_loss': 1.2648807847499848}
2025-01-16 11:56:34,510 [INFO] Step[250/2713]: training loss : 1.1816429555416108 TRAIN  loss dict:  {'classification_loss': 1.1816429555416108}
2025-01-16 11:56:50,690 [INFO] Step[300/2713]: training loss : 1.1802411937713624 TRAIN  loss dict:  {'classification_loss': 1.1802411937713624}
2025-01-16 11:57:06,838 [INFO] Step[350/2713]: training loss : 1.2179101753234862 TRAIN  loss dict:  {'classification_loss': 1.2179101753234862}
2025-01-16 11:57:22,908 [INFO] Step[400/2713]: training loss : 1.1545640289783479 TRAIN  loss dict:  {'classification_loss': 1.1545640289783479}
2025-01-16 11:57:39,051 [INFO] Step[450/2713]: training loss : 1.1659937620162963 TRAIN  loss dict:  {'classification_loss': 1.1659937620162963}
2025-01-16 11:57:55,109 [INFO] Step[500/2713]: training loss : 1.1338518393039703 TRAIN  loss dict:  {'classification_loss': 1.1338518393039703}
2025-01-16 11:58:11,271 [INFO] Step[550/2713]: training loss : 1.2699300312995911 TRAIN  loss dict:  {'classification_loss': 1.2699300312995911}
2025-01-16 11:58:27,379 [INFO] Step[600/2713]: training loss : 1.162320820093155 TRAIN  loss dict:  {'classification_loss': 1.162320820093155}
2025-01-16 11:58:43,487 [INFO] Step[650/2713]: training loss : 1.1743953585624696 TRAIN  loss dict:  {'classification_loss': 1.1743953585624696}
2025-01-16 11:58:59,642 [INFO] Step[700/2713]: training loss : 1.2646978902816772 TRAIN  loss dict:  {'classification_loss': 1.2646978902816772}
2025-01-16 11:59:15,743 [INFO] Step[750/2713]: training loss : 1.3211455142498016 TRAIN  loss dict:  {'classification_loss': 1.3211455142498016}
2025-01-16 11:59:31,857 [INFO] Step[800/2713]: training loss : 1.1773907768726348 TRAIN  loss dict:  {'classification_loss': 1.1773907768726348}
2025-01-16 11:59:47,996 [INFO] Step[850/2713]: training loss : 1.2097941100597382 TRAIN  loss dict:  {'classification_loss': 1.2097941100597382}
2025-01-16 12:00:04,144 [INFO] Step[900/2713]: training loss : 1.180023717880249 TRAIN  loss dict:  {'classification_loss': 1.180023717880249}
2025-01-16 12:00:20,342 [INFO] Step[950/2713]: training loss : 1.3057858860492706 TRAIN  loss dict:  {'classification_loss': 1.3057858860492706}
2025-01-16 12:00:36,464 [INFO] Step[1000/2713]: training loss : 1.1521031725406647 TRAIN  loss dict:  {'classification_loss': 1.1521031725406647}
2025-01-16 12:00:52,547 [INFO] Step[1050/2713]: training loss : 1.2639486479759217 TRAIN  loss dict:  {'classification_loss': 1.2639486479759217}
2025-01-16 12:01:08,626 [INFO] Step[1100/2713]: training loss : 1.2929444515705109 TRAIN  loss dict:  {'classification_loss': 1.2929444515705109}
2025-01-16 12:01:24,735 [INFO] Step[1150/2713]: training loss : 1.1914142978191375 TRAIN  loss dict:  {'classification_loss': 1.1914142978191375}
2025-01-16 12:01:40,824 [INFO] Step[1200/2713]: training loss : 1.2314469349384307 TRAIN  loss dict:  {'classification_loss': 1.2314469349384307}
2025-01-16 12:01:57,003 [INFO] Step[1250/2713]: training loss : 1.2359458887577057 TRAIN  loss dict:  {'classification_loss': 1.2359458887577057}
2025-01-16 12:02:13,106 [INFO] Step[1300/2713]: training loss : 1.2413812577724457 TRAIN  loss dict:  {'classification_loss': 1.2413812577724457}
2025-01-16 12:02:29,298 [INFO] Step[1350/2713]: training loss : 1.1990423345565795 TRAIN  loss dict:  {'classification_loss': 1.1990423345565795}
2025-01-16 12:02:45,438 [INFO] Step[1400/2713]: training loss : 1.2489220261573792 TRAIN  loss dict:  {'classification_loss': 1.2489220261573792}
2025-01-16 12:03:01,567 [INFO] Step[1450/2713]: training loss : 1.171169537305832 TRAIN  loss dict:  {'classification_loss': 1.171169537305832}
2025-01-16 12:03:17,639 [INFO] Step[1500/2713]: training loss : 1.1886654913425445 TRAIN  loss dict:  {'classification_loss': 1.1886654913425445}
2025-01-16 12:03:33,753 [INFO] Step[1550/2713]: training loss : 1.2796324789524078 TRAIN  loss dict:  {'classification_loss': 1.2796324789524078}
2025-01-16 12:03:49,875 [INFO] Step[1600/2713]: training loss : 1.254002457857132 TRAIN  loss dict:  {'classification_loss': 1.254002457857132}
2025-01-16 12:04:05,980 [INFO] Step[1650/2713]: training loss : 1.243854089975357 TRAIN  loss dict:  {'classification_loss': 1.243854089975357}
2025-01-16 12:04:22,125 [INFO] Step[1700/2713]: training loss : 1.2277672374248505 TRAIN  loss dict:  {'classification_loss': 1.2277672374248505}
2025-01-16 12:04:38,286 [INFO] Step[1750/2713]: training loss : 1.1732312262058258 TRAIN  loss dict:  {'classification_loss': 1.1732312262058258}
2025-01-16 12:04:54,399 [INFO] Step[1800/2713]: training loss : 1.2110246074199678 TRAIN  loss dict:  {'classification_loss': 1.2110246074199678}
2025-01-16 12:05:10,583 [INFO] Step[1850/2713]: training loss : 1.283825455904007 TRAIN  loss dict:  {'classification_loss': 1.283825455904007}
2025-01-16 12:05:26,774 [INFO] Step[1900/2713]: training loss : 1.2136283385753632 TRAIN  loss dict:  {'classification_loss': 1.2136283385753632}
2025-01-16 12:05:42,870 [INFO] Step[1950/2713]: training loss : 1.2486294889450074 TRAIN  loss dict:  {'classification_loss': 1.2486294889450074}
2025-01-16 12:05:58,965 [INFO] Step[2000/2713]: training loss : 1.337708066701889 TRAIN  loss dict:  {'classification_loss': 1.337708066701889}
2025-01-16 12:06:15,065 [INFO] Step[2050/2713]: training loss : 1.1448851132392883 TRAIN  loss dict:  {'classification_loss': 1.1448851132392883}
2025-01-16 12:06:31,164 [INFO] Step[2100/2713]: training loss : 1.2431506872177125 TRAIN  loss dict:  {'classification_loss': 1.2431506872177125}
2025-01-16 12:06:47,386 [INFO] Step[2150/2713]: training loss : 1.2660899150371552 TRAIN  loss dict:  {'classification_loss': 1.2660899150371552}
2025-01-16 12:07:03,463 [INFO] Step[2200/2713]: training loss : 1.3264520251750946 TRAIN  loss dict:  {'classification_loss': 1.3264520251750946}
2025-01-16 12:07:19,617 [INFO] Step[2250/2713]: training loss : 1.1524163091182709 TRAIN  loss dict:  {'classification_loss': 1.1524163091182709}
2025-01-16 12:07:35,719 [INFO] Step[2300/2713]: training loss : 1.212881120443344 TRAIN  loss dict:  {'classification_loss': 1.212881120443344}
2025-01-16 12:07:51,835 [INFO] Step[2350/2713]: training loss : 1.2188830363750458 TRAIN  loss dict:  {'classification_loss': 1.2188830363750458}
2025-01-16 12:08:07,924 [INFO] Step[2400/2713]: training loss : 1.276202231645584 TRAIN  loss dict:  {'classification_loss': 1.276202231645584}
2025-01-16 12:08:24,058 [INFO] Step[2450/2713]: training loss : 1.0958842039108276 TRAIN  loss dict:  {'classification_loss': 1.0958842039108276}
2025-01-16 12:08:40,167 [INFO] Step[2500/2713]: training loss : 1.258072589635849 TRAIN  loss dict:  {'classification_loss': 1.258072589635849}
2025-01-16 12:08:56,356 [INFO] Step[2550/2713]: training loss : 1.3257618188858031 TRAIN  loss dict:  {'classification_loss': 1.3257618188858031}
2025-01-16 12:09:12,501 [INFO] Step[2600/2713]: training loss : 1.2476988053321838 TRAIN  loss dict:  {'classification_loss': 1.2476988053321838}
2025-01-16 12:09:28,677 [INFO] Step[2650/2713]: training loss : 1.3130453312397004 TRAIN  loss dict:  {'classification_loss': 1.3130453312397004}
2025-01-16 12:09:44,813 [INFO] Step[2700/2713]: training loss : 1.2014450240135193 TRAIN  loss dict:  {'classification_loss': 1.2014450240135193}
2025-01-16 12:11:03,057 [INFO] Label accuracies statistics:
2025-01-16 12:11:03,057 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 1.0, 7: 0.25, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.5, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.25, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.5, 59: 1.0, 60: 0.25, 61: 0.75, 62: 0.5, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 0.75, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.0, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.5, 116: 0.5, 117: 1.0, 118: 0.5, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.25, 154: 0.75, 155: 1.0, 156: 0.25, 157: 1.0, 158: 0.6666666666666666, 159: 0.5, 160: 0.5, 161: 0.75, 162: 1.0, 163: 0.5, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 0.5, 194: 1.0, 195: 0.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.25, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.75, 218: 0.5, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.25, 228: 0.75, 229: 0.0, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.5, 246: 0.75, 247: 1.0, 248: 0.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 0.75, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.0, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.25, 268: 0.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.25, 290: 0.0, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.25, 300: 1.0, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.75, 307: 0.75, 308: 0.5, 309: 0.5, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 0.5, 331: 0.75, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 0.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 0.25, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.5, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.0, 354: 0.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.5, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.25, 388: 1.0, 389: 0.25, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 12:11:03,059 [INFO] [7] TRAIN  loss: 1.2267360405819996 acc: 0.9265266003194496
2025-01-16 12:11:03,059 [INFO] [7] TRAIN  loss dict: {'classification_loss': 1.2267360405819996}
2025-01-16 12:11:03,059 [INFO] [7] VALIDATION loss: 1.9966813470412017 VALIDATION acc: 0.7278996865203762
2025-01-16 12:11:03,059 [INFO] [7] VALIDATION loss dict: {'classification_loss': 1.9966813470412017}
2025-01-16 12:11:03,059 [INFO] 
2025-01-16 12:11:24,132 [INFO] Step[50/2713]: training loss : 1.1561396038532257 TRAIN  loss dict:  {'classification_loss': 1.1561396038532257}
2025-01-16 12:11:40,241 [INFO] Step[100/2713]: training loss : 1.2458068907260895 TRAIN  loss dict:  {'classification_loss': 1.2458068907260895}
2025-01-16 12:11:56,377 [INFO] Step[150/2713]: training loss : 1.1980519425868987 TRAIN  loss dict:  {'classification_loss': 1.1980519425868987}
2025-01-16 12:12:12,530 [INFO] Step[200/2713]: training loss : 1.2035680794715882 TRAIN  loss dict:  {'classification_loss': 1.2035680794715882}
2025-01-16 12:12:28,657 [INFO] Step[250/2713]: training loss : 1.1384183740615845 TRAIN  loss dict:  {'classification_loss': 1.1384183740615845}
2025-01-16 12:12:44,764 [INFO] Step[300/2713]: training loss : 1.333641039133072 TRAIN  loss dict:  {'classification_loss': 1.333641039133072}
2025-01-16 12:13:00,902 [INFO] Step[350/2713]: training loss : 1.2285093104839324 TRAIN  loss dict:  {'classification_loss': 1.2285093104839324}
2025-01-16 12:13:17,023 [INFO] Step[400/2713]: training loss : 1.172464109659195 TRAIN  loss dict:  {'classification_loss': 1.172464109659195}
2025-01-16 12:13:33,098 [INFO] Step[450/2713]: training loss : 1.1848494458198546 TRAIN  loss dict:  {'classification_loss': 1.1848494458198546}
2025-01-16 12:13:49,224 [INFO] Step[500/2713]: training loss : 1.189818527698517 TRAIN  loss dict:  {'classification_loss': 1.189818527698517}
2025-01-16 12:14:05,409 [INFO] Step[550/2713]: training loss : 1.1726866173744201 TRAIN  loss dict:  {'classification_loss': 1.1726866173744201}
2025-01-16 12:14:21,494 [INFO] Step[600/2713]: training loss : 1.183472603559494 TRAIN  loss dict:  {'classification_loss': 1.183472603559494}
2025-01-16 12:14:37,562 [INFO] Step[650/2713]: training loss : 1.227006161212921 TRAIN  loss dict:  {'classification_loss': 1.227006161212921}
2025-01-16 12:14:53,639 [INFO] Step[700/2713]: training loss : 1.1369906902313232 TRAIN  loss dict:  {'classification_loss': 1.1369906902313232}
2025-01-16 12:15:09,795 [INFO] Step[750/2713]: training loss : 1.2273563539981842 TRAIN  loss dict:  {'classification_loss': 1.2273563539981842}
2025-01-16 12:15:25,831 [INFO] Step[800/2713]: training loss : 1.1937861263751983 TRAIN  loss dict:  {'classification_loss': 1.1937861263751983}
2025-01-16 12:15:41,943 [INFO] Step[850/2713]: training loss : 1.1659801077842713 TRAIN  loss dict:  {'classification_loss': 1.1659801077842713}
2025-01-16 12:15:58,072 [INFO] Step[900/2713]: training loss : 1.193260861635208 TRAIN  loss dict:  {'classification_loss': 1.193260861635208}
2025-01-16 12:16:14,189 [INFO] Step[950/2713]: training loss : 1.1950741600990296 TRAIN  loss dict:  {'classification_loss': 1.1950741600990296}
2025-01-16 12:16:30,281 [INFO] Step[1000/2713]: training loss : 1.2081389248371124 TRAIN  loss dict:  {'classification_loss': 1.2081389248371124}
2025-01-16 12:16:46,412 [INFO] Step[1050/2713]: training loss : 1.2463333261013032 TRAIN  loss dict:  {'classification_loss': 1.2463333261013032}
2025-01-16 12:17:02,521 [INFO] Step[1100/2713]: training loss : 1.2553051042556762 TRAIN  loss dict:  {'classification_loss': 1.2553051042556762}
2025-01-16 12:17:18,731 [INFO] Step[1150/2713]: training loss : 1.1618218588829041 TRAIN  loss dict:  {'classification_loss': 1.1618218588829041}
2025-01-16 12:17:34,852 [INFO] Step[1200/2713]: training loss : 1.173534791469574 TRAIN  loss dict:  {'classification_loss': 1.173534791469574}
2025-01-16 12:17:50,917 [INFO] Step[1250/2713]: training loss : 1.2429467236995697 TRAIN  loss dict:  {'classification_loss': 1.2429467236995697}
2025-01-16 12:18:07,050 [INFO] Step[1300/2713]: training loss : 1.301890538930893 TRAIN  loss dict:  {'classification_loss': 1.301890538930893}
2025-01-16 12:18:23,144 [INFO] Step[1350/2713]: training loss : 1.308428440093994 TRAIN  loss dict:  {'classification_loss': 1.308428440093994}
2025-01-16 12:18:39,209 [INFO] Step[1400/2713]: training loss : 1.2117816662788392 TRAIN  loss dict:  {'classification_loss': 1.2117816662788392}
2025-01-16 12:18:55,369 [INFO] Step[1450/2713]: training loss : 1.2097574543952943 TRAIN  loss dict:  {'classification_loss': 1.2097574543952943}
2025-01-16 12:19:11,473 [INFO] Step[1500/2713]: training loss : 1.2693710803985596 TRAIN  loss dict:  {'classification_loss': 1.2693710803985596}
2025-01-16 12:19:27,544 [INFO] Step[1550/2713]: training loss : 1.2059697461128236 TRAIN  loss dict:  {'classification_loss': 1.2059697461128236}
2025-01-16 12:19:43,568 [INFO] Step[1600/2713]: training loss : 1.212316493988037 TRAIN  loss dict:  {'classification_loss': 1.212316493988037}
2025-01-16 12:19:59,637 [INFO] Step[1650/2713]: training loss : 1.1420686614513398 TRAIN  loss dict:  {'classification_loss': 1.1420686614513398}
2025-01-16 12:20:15,727 [INFO] Step[1700/2713]: training loss : 1.228477269411087 TRAIN  loss dict:  {'classification_loss': 1.228477269411087}
2025-01-16 12:20:31,877 [INFO] Step[1750/2713]: training loss : 1.2411198759078979 TRAIN  loss dict:  {'classification_loss': 1.2411198759078979}
2025-01-16 12:20:47,918 [INFO] Step[1800/2713]: training loss : 1.1816177678108215 TRAIN  loss dict:  {'classification_loss': 1.1816177678108215}
2025-01-16 12:21:04,037 [INFO] Step[1850/2713]: training loss : 1.2106113171577453 TRAIN  loss dict:  {'classification_loss': 1.2106113171577453}
2025-01-16 12:21:20,196 [INFO] Step[1900/2713]: training loss : 1.2877277112007142 TRAIN  loss dict:  {'classification_loss': 1.2877277112007142}
2025-01-16 12:21:36,380 [INFO] Step[1950/2713]: training loss : 1.2076701080799104 TRAIN  loss dict:  {'classification_loss': 1.2076701080799104}
2025-01-16 12:21:52,528 [INFO] Step[2000/2713]: training loss : 1.2849030470848084 TRAIN  loss dict:  {'classification_loss': 1.2849030470848084}
2025-01-16 12:22:08,653 [INFO] Step[2050/2713]: training loss : 1.278415094614029 TRAIN  loss dict:  {'classification_loss': 1.278415094614029}
2025-01-16 12:22:24,793 [INFO] Step[2100/2713]: training loss : 1.2552706742286681 TRAIN  loss dict:  {'classification_loss': 1.2552706742286681}
2025-01-16 12:22:40,959 [INFO] Step[2150/2713]: training loss : 1.2435571563243866 TRAIN  loss dict:  {'classification_loss': 1.2435571563243866}
2025-01-16 12:22:56,977 [INFO] Step[2200/2713]: training loss : 1.1662048161029817 TRAIN  loss dict:  {'classification_loss': 1.1662048161029817}
2025-01-16 12:23:13,132 [INFO] Step[2250/2713]: training loss : 1.1967466473579407 TRAIN  loss dict:  {'classification_loss': 1.1967466473579407}
2025-01-16 12:23:29,250 [INFO] Step[2300/2713]: training loss : 1.1830349349975586 TRAIN  loss dict:  {'classification_loss': 1.1830349349975586}
2025-01-16 12:23:45,482 [INFO] Step[2350/2713]: training loss : 1.2143274915218354 TRAIN  loss dict:  {'classification_loss': 1.2143274915218354}
2025-01-16 12:24:01,584 [INFO] Step[2400/2713]: training loss : 1.2467082965373992 TRAIN  loss dict:  {'classification_loss': 1.2467082965373992}
2025-01-16 12:24:17,736 [INFO] Step[2450/2713]: training loss : 1.174860053062439 TRAIN  loss dict:  {'classification_loss': 1.174860053062439}
2025-01-16 12:24:33,838 [INFO] Step[2500/2713]: training loss : 1.145343049764633 TRAIN  loss dict:  {'classification_loss': 1.145343049764633}
2025-01-16 12:24:50,029 [INFO] Step[2550/2713]: training loss : 1.19823793053627 TRAIN  loss dict:  {'classification_loss': 1.19823793053627}
2025-01-16 12:25:06,121 [INFO] Step[2600/2713]: training loss : 1.310812109708786 TRAIN  loss dict:  {'classification_loss': 1.310812109708786}
2025-01-16 12:25:22,257 [INFO] Step[2650/2713]: training loss : 1.1066530227661133 TRAIN  loss dict:  {'classification_loss': 1.1066530227661133}
2025-01-16 12:25:38,299 [INFO] Step[2700/2713]: training loss : 1.2405372750759125 TRAIN  loss dict:  {'classification_loss': 1.2405372750759125}
2025-01-16 12:26:56,599 [INFO] Label accuracies statistics:
2025-01-16 12:26:56,599 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.25, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.0, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.5, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.25, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 0.75, 156: 0.25, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.25, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.3333333333333333, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.25, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 0.25, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.5, 206: 0.75, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.25, 218: 1.0, 219: 0.75, 220: 0.25, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.5, 228: 0.5, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.0, 240: 0.75, 241: 0.5, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.5, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.5, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.75, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.5, 265: 0.75, 266: 1.0, 267: 0.25, 268: 0.5, 269: 0.75, 270: 0.75, 271: 0.75, 272: 0.75, 273: 0.25, 274: 0.0, 275: 0.75, 276: 0.5, 277: 0.75, 278: 1.0, 279: 1.0, 280: 0.75, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.25, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 0.25, 317: 0.5, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.5, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.0, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.25, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.25, 390: 0.5, 391: 1.0, 392: 0.5, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 12:26:59,410 [INFO] [8] TRAIN  loss: 1.212466801482727 acc: 0.9320555350780194
2025-01-16 12:26:59,410 [INFO] [8] TRAIN  loss dict: {'classification_loss': 1.212466801482727}
2025-01-16 12:26:59,410 [INFO] [8] VALIDATION loss: 1.9708895663121588 VALIDATION acc: 0.7467084639498432
2025-01-16 12:26:59,410 [INFO] [8] VALIDATION loss dict: {'classification_loss': 1.9708895663121588}
2025-01-16 12:26:59,410 [INFO] 
2025-01-16 12:27:20,471 [INFO] Step[50/2713]: training loss : 1.2058456552028656 TRAIN  loss dict:  {'classification_loss': 1.2058456552028656}
2025-01-16 12:27:36,514 [INFO] Step[100/2713]: training loss : 1.1059165751934052 TRAIN  loss dict:  {'classification_loss': 1.1059165751934052}
2025-01-16 12:27:52,568 [INFO] Step[150/2713]: training loss : 1.1122377109527588 TRAIN  loss dict:  {'classification_loss': 1.1122377109527588}
2025-01-16 12:28:08,666 [INFO] Step[200/2713]: training loss : 1.17683647274971 TRAIN  loss dict:  {'classification_loss': 1.17683647274971}
2025-01-16 12:28:24,832 [INFO] Step[250/2713]: training loss : 1.2123170852661134 TRAIN  loss dict:  {'classification_loss': 1.2123170852661134}
2025-01-16 12:28:40,997 [INFO] Step[300/2713]: training loss : 1.1449121356010437 TRAIN  loss dict:  {'classification_loss': 1.1449121356010437}
2025-01-16 12:28:57,140 [INFO] Step[350/2713]: training loss : 1.2370576107501983 TRAIN  loss dict:  {'classification_loss': 1.2370576107501983}
2025-01-16 12:29:13,275 [INFO] Step[400/2713]: training loss : 1.2312296104431153 TRAIN  loss dict:  {'classification_loss': 1.2312296104431153}
2025-01-16 12:29:29,421 [INFO] Step[450/2713]: training loss : 1.103086712360382 TRAIN  loss dict:  {'classification_loss': 1.103086712360382}
2025-01-16 12:29:45,531 [INFO] Step[500/2713]: training loss : 1.1592394089698792 TRAIN  loss dict:  {'classification_loss': 1.1592394089698792}
2025-01-16 12:30:01,757 [INFO] Step[550/2713]: training loss : 1.1047504770755767 TRAIN  loss dict:  {'classification_loss': 1.1047504770755767}
2025-01-16 12:30:17,938 [INFO] Step[600/2713]: training loss : 1.1661517596244813 TRAIN  loss dict:  {'classification_loss': 1.1661517596244813}
2025-01-16 12:30:34,133 [INFO] Step[650/2713]: training loss : 1.1492457222938537 TRAIN  loss dict:  {'classification_loss': 1.1492457222938537}
2025-01-16 12:30:50,339 [INFO] Step[700/2713]: training loss : 1.1275457537174225 TRAIN  loss dict:  {'classification_loss': 1.1275457537174225}
2025-01-16 12:31:06,457 [INFO] Step[750/2713]: training loss : 1.146613187789917 TRAIN  loss dict:  {'classification_loss': 1.146613187789917}
2025-01-16 12:31:22,593 [INFO] Step[800/2713]: training loss : 1.2538696444034576 TRAIN  loss dict:  {'classification_loss': 1.2538696444034576}
2025-01-16 12:31:38,755 [INFO] Step[850/2713]: training loss : 1.2327369058132172 TRAIN  loss dict:  {'classification_loss': 1.2327369058132172}
2025-01-16 12:31:54,866 [INFO] Step[900/2713]: training loss : 1.2552887606620788 TRAIN  loss dict:  {'classification_loss': 1.2552887606620788}
2025-01-16 12:32:11,018 [INFO] Step[950/2713]: training loss : 1.145184544324875 TRAIN  loss dict:  {'classification_loss': 1.145184544324875}
2025-01-16 12:32:27,190 [INFO] Step[1000/2713]: training loss : 1.1487773478031158 TRAIN  loss dict:  {'classification_loss': 1.1487773478031158}
2025-01-16 12:32:43,327 [INFO] Step[1050/2713]: training loss : 1.1538262474536896 TRAIN  loss dict:  {'classification_loss': 1.1538262474536896}
2025-01-16 12:32:59,416 [INFO] Step[1100/2713]: training loss : 1.154838809967041 TRAIN  loss dict:  {'classification_loss': 1.154838809967041}
2025-01-16 12:33:15,536 [INFO] Step[1150/2713]: training loss : 1.2458728659152984 TRAIN  loss dict:  {'classification_loss': 1.2458728659152984}
2025-01-16 12:33:31,655 [INFO] Step[1200/2713]: training loss : 1.2575764513015748 TRAIN  loss dict:  {'classification_loss': 1.2575764513015748}
2025-01-16 12:33:47,861 [INFO] Step[1250/2713]: training loss : 1.182341673374176 TRAIN  loss dict:  {'classification_loss': 1.182341673374176}
2025-01-16 12:34:03,904 [INFO] Step[1300/2713]: training loss : 1.1744477558135986 TRAIN  loss dict:  {'classification_loss': 1.1744477558135986}
2025-01-16 12:34:20,073 [INFO] Step[1350/2713]: training loss : 1.241363710165024 TRAIN  loss dict:  {'classification_loss': 1.241363710165024}
2025-01-16 12:34:36,152 [INFO] Step[1400/2713]: training loss : 1.1845517539978028 TRAIN  loss dict:  {'classification_loss': 1.1845517539978028}
2025-01-16 12:34:52,372 [INFO] Step[1450/2713]: training loss : 1.1359669256210327 TRAIN  loss dict:  {'classification_loss': 1.1359669256210327}
2025-01-16 12:35:08,471 [INFO] Step[1500/2713]: training loss : 1.213823972940445 TRAIN  loss dict:  {'classification_loss': 1.213823972940445}
2025-01-16 12:35:24,668 [INFO] Step[1550/2713]: training loss : 1.214588133096695 TRAIN  loss dict:  {'classification_loss': 1.214588133096695}
2025-01-16 12:35:40,810 [INFO] Step[1600/2713]: training loss : 1.1561552369594574 TRAIN  loss dict:  {'classification_loss': 1.1561552369594574}
2025-01-16 12:35:57,035 [INFO] Step[1650/2713]: training loss : 1.3037893712520598 TRAIN  loss dict:  {'classification_loss': 1.3037893712520598}
2025-01-16 12:36:13,156 [INFO] Step[1700/2713]: training loss : 1.1558004903793335 TRAIN  loss dict:  {'classification_loss': 1.1558004903793335}
2025-01-16 12:36:29,310 [INFO] Step[1750/2713]: training loss : 1.3880687153339386 TRAIN  loss dict:  {'classification_loss': 1.3880687153339386}
2025-01-16 12:36:45,448 [INFO] Step[1800/2713]: training loss : 1.2010205054283143 TRAIN  loss dict:  {'classification_loss': 1.2010205054283143}
2025-01-16 12:37:01,588 [INFO] Step[1850/2713]: training loss : 1.2221158385276794 TRAIN  loss dict:  {'classification_loss': 1.2221158385276794}
2025-01-16 12:37:17,762 [INFO] Step[1900/2713]: training loss : 1.1996898221969605 TRAIN  loss dict:  {'classification_loss': 1.1996898221969605}
2025-01-16 12:37:33,920 [INFO] Step[1950/2713]: training loss : 1.1865333366394042 TRAIN  loss dict:  {'classification_loss': 1.1865333366394042}
2025-01-16 12:37:50,093 [INFO] Step[2000/2713]: training loss : 1.2583225786685943 TRAIN  loss dict:  {'classification_loss': 1.2583225786685943}
2025-01-16 12:38:06,348 [INFO] Step[2050/2713]: training loss : 1.163250697851181 TRAIN  loss dict:  {'classification_loss': 1.163250697851181}
2025-01-16 12:38:22,477 [INFO] Step[2100/2713]: training loss : 1.2384163916110993 TRAIN  loss dict:  {'classification_loss': 1.2384163916110993}
2025-01-16 12:38:38,588 [INFO] Step[2150/2713]: training loss : 1.1652182340621948 TRAIN  loss dict:  {'classification_loss': 1.1652182340621948}
2025-01-16 12:38:54,674 [INFO] Step[2200/2713]: training loss : 1.170038126707077 TRAIN  loss dict:  {'classification_loss': 1.170038126707077}
2025-01-16 12:39:10,873 [INFO] Step[2250/2713]: training loss : 1.216915168762207 TRAIN  loss dict:  {'classification_loss': 1.216915168762207}
2025-01-16 12:39:27,041 [INFO] Step[2300/2713]: training loss : 1.1274882304668425 TRAIN  loss dict:  {'classification_loss': 1.1274882304668425}
2025-01-16 12:39:43,219 [INFO] Step[2350/2713]: training loss : 1.1288857519626618 TRAIN  loss dict:  {'classification_loss': 1.1288857519626618}
2025-01-16 12:39:59,391 [INFO] Step[2400/2713]: training loss : 1.2363452923297882 TRAIN  loss dict:  {'classification_loss': 1.2363452923297882}
2025-01-16 12:40:15,527 [INFO] Step[2450/2713]: training loss : 1.21315580368042 TRAIN  loss dict:  {'classification_loss': 1.21315580368042}
2025-01-16 12:40:31,607 [INFO] Step[2500/2713]: training loss : 1.2301950562000274 TRAIN  loss dict:  {'classification_loss': 1.2301950562000274}
2025-01-16 12:40:47,726 [INFO] Step[2550/2713]: training loss : 1.2092186737060546 TRAIN  loss dict:  {'classification_loss': 1.2092186737060546}
2025-01-16 12:41:03,848 [INFO] Step[2600/2713]: training loss : 1.2128546452522277 TRAIN  loss dict:  {'classification_loss': 1.2128546452522277}
2025-01-16 12:41:20,042 [INFO] Step[2650/2713]: training loss : 1.1790692293643952 TRAIN  loss dict:  {'classification_loss': 1.1790692293643952}
2025-01-16 12:41:36,290 [INFO] Step[2700/2713]: training loss : 1.201219574213028 TRAIN  loss dict:  {'classification_loss': 1.201219574213028}
2025-01-16 12:42:54,287 [INFO] Label accuracies statistics:
2025-01-16 12:42:54,287 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.25, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.25, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.25, 61: 0.75, 62: 0.75, 63: 0.0, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.5, 68: 0.5, 69: 0.75, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.75, 91: 1.0, 92: 0.75, 93: 0.75, 94: 0.25, 95: 0.5, 96: 0.75, 97: 0.25, 98: 0.75, 99: 0.8, 100: 0.75, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 0.75, 109: 0.5, 110: 0.5, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.75, 115: 0.5, 116: 0.5, 117: 1.0, 118: 1.0, 119: 0.5, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 0.75, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.5, 143: 0.75, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.3333333333333333, 159: 0.75, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.5, 165: 0.75, 166: 0.75, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.0, 181: 0.75, 182: 0.25, 183: 0.5, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.5, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.5, 223: 0.75, 224: 0.5, 225: 1.0, 226: 0.25, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.5, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.5, 267: 0.25, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.25, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.5, 316: 0.5, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.0, 337: 1.0, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 0.75, 344: 0.5, 345: 0.5, 346: 1.0, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.75, 357: 0.5, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.25, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.0, 371: 0.5, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 0.5, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 0.5, 392: 0.5, 393: 0.25, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 12:42:54,289 [INFO] [9] TRAIN  loss: 1.1908916695681262 acc: 0.938444526354589
2025-01-16 12:42:54,289 [INFO] [9] TRAIN  loss dict: {'classification_loss': 1.1908916695681262}
2025-01-16 12:42:54,289 [INFO] [9] VALIDATION loss: 2.0407853165739462 VALIDATION acc: 0.7210031347962382
2025-01-16 12:42:54,289 [INFO] [9] VALIDATION loss dict: {'classification_loss': 2.0407853165739462}
2025-01-16 12:42:54,289 [INFO] 
2025-01-16 12:43:14,665 [INFO] Step[50/2713]: training loss : 1.1645340633392334 TRAIN  loss dict:  {'classification_loss': 1.1645340633392334}
2025-01-16 12:43:30,622 [INFO] Step[100/2713]: training loss : 1.1620610415935517 TRAIN  loss dict:  {'classification_loss': 1.1620610415935517}
2025-01-16 12:43:46,705 [INFO] Step[150/2713]: training loss : 1.1072725367546081 TRAIN  loss dict:  {'classification_loss': 1.1072725367546081}
2025-01-16 12:44:02,726 [INFO] Step[200/2713]: training loss : 1.1208824551105498 TRAIN  loss dict:  {'classification_loss': 1.1208824551105498}
2025-01-16 12:44:18,777 [INFO] Step[250/2713]: training loss : 1.1694843196868896 TRAIN  loss dict:  {'classification_loss': 1.1694843196868896}
2025-01-16 12:44:34,834 [INFO] Step[300/2713]: training loss : 1.1052925050258637 TRAIN  loss dict:  {'classification_loss': 1.1052925050258637}
2025-01-16 12:44:50,884 [INFO] Step[350/2713]: training loss : 1.2378818309307098 TRAIN  loss dict:  {'classification_loss': 1.2378818309307098}
2025-01-16 12:45:06,989 [INFO] Step[400/2713]: training loss : 1.1382345497608184 TRAIN  loss dict:  {'classification_loss': 1.1382345497608184}
2025-01-16 12:45:23,093 [INFO] Step[450/2713]: training loss : 1.1613875484466554 TRAIN  loss dict:  {'classification_loss': 1.1613875484466554}
2025-01-16 12:45:39,116 [INFO] Step[500/2713]: training loss : 1.1263514494895934 TRAIN  loss dict:  {'classification_loss': 1.1263514494895934}
2025-01-16 12:45:55,138 [INFO] Step[550/2713]: training loss : 1.1586392366886138 TRAIN  loss dict:  {'classification_loss': 1.1586392366886138}
2025-01-16 12:46:11,123 [INFO] Step[600/2713]: training loss : 1.237417962551117 TRAIN  loss dict:  {'classification_loss': 1.237417962551117}
2025-01-16 12:46:27,194 [INFO] Step[650/2713]: training loss : 1.1979019463062286 TRAIN  loss dict:  {'classification_loss': 1.1979019463062286}
2025-01-16 12:46:43,103 [INFO] Step[700/2713]: training loss : 1.1542726409435273 TRAIN  loss dict:  {'classification_loss': 1.1542726409435273}
2025-01-16 12:46:59,183 [INFO] Step[750/2713]: training loss : 1.19198890209198 TRAIN  loss dict:  {'classification_loss': 1.19198890209198}
2025-01-16 12:47:15,200 [INFO] Step[800/2713]: training loss : 1.0985340106487274 TRAIN  loss dict:  {'classification_loss': 1.0985340106487274}
2025-01-16 12:47:31,238 [INFO] Step[850/2713]: training loss : 1.172079448699951 TRAIN  loss dict:  {'classification_loss': 1.172079448699951}
2025-01-16 12:47:47,257 [INFO] Step[900/2713]: training loss : 1.1987244927883147 TRAIN  loss dict:  {'classification_loss': 1.1987244927883147}
2025-01-16 12:48:03,291 [INFO] Step[950/2713]: training loss : 1.1766824281215669 TRAIN  loss dict:  {'classification_loss': 1.1766824281215669}
2025-01-16 12:48:19,352 [INFO] Step[1000/2713]: training loss : 1.132720823287964 TRAIN  loss dict:  {'classification_loss': 1.132720823287964}
2025-01-16 12:48:35,462 [INFO] Step[1050/2713]: training loss : 1.1674944138526917 TRAIN  loss dict:  {'classification_loss': 1.1674944138526917}
2025-01-16 12:48:51,538 [INFO] Step[1100/2713]: training loss : 1.257040342092514 TRAIN  loss dict:  {'classification_loss': 1.257040342092514}
2025-01-16 12:49:07,599 [INFO] Step[1150/2713]: training loss : 1.0779318833351135 TRAIN  loss dict:  {'classification_loss': 1.0779318833351135}
2025-01-16 12:49:23,596 [INFO] Step[1200/2713]: training loss : 1.1061964011192322 TRAIN  loss dict:  {'classification_loss': 1.1061964011192322}
2025-01-16 12:49:39,602 [INFO] Step[1250/2713]: training loss : 1.128696801662445 TRAIN  loss dict:  {'classification_loss': 1.128696801662445}
2025-01-16 12:49:55,679 [INFO] Step[1300/2713]: training loss : 1.1040530622005462 TRAIN  loss dict:  {'classification_loss': 1.1040530622005462}
2025-01-16 12:50:11,745 [INFO] Step[1350/2713]: training loss : 1.1191271269321441 TRAIN  loss dict:  {'classification_loss': 1.1191271269321441}
2025-01-16 12:50:27,776 [INFO] Step[1400/2713]: training loss : 1.1138373494148255 TRAIN  loss dict:  {'classification_loss': 1.1138373494148255}
2025-01-16 12:50:43,895 [INFO] Step[1450/2713]: training loss : 1.2070908296108245 TRAIN  loss dict:  {'classification_loss': 1.2070908296108245}
2025-01-16 12:50:59,951 [INFO] Step[1500/2713]: training loss : 1.1331456696987152 TRAIN  loss dict:  {'classification_loss': 1.1331456696987152}
2025-01-16 12:51:16,001 [INFO] Step[1550/2713]: training loss : 1.1313389766216277 TRAIN  loss dict:  {'classification_loss': 1.1313389766216277}
2025-01-16 12:51:32,128 [INFO] Step[1600/2713]: training loss : 1.2480809819698333 TRAIN  loss dict:  {'classification_loss': 1.2480809819698333}
2025-01-16 12:51:48,184 [INFO] Step[1650/2713]: training loss : 1.2407185304164887 TRAIN  loss dict:  {'classification_loss': 1.2407185304164887}
2025-01-16 12:52:04,285 [INFO] Step[1700/2713]: training loss : 1.171158561706543 TRAIN  loss dict:  {'classification_loss': 1.171158561706543}
2025-01-16 12:52:20,468 [INFO] Step[1750/2713]: training loss : 1.1581948041915893 TRAIN  loss dict:  {'classification_loss': 1.1581948041915893}
2025-01-16 12:52:36,485 [INFO] Step[1800/2713]: training loss : 1.1425637185573578 TRAIN  loss dict:  {'classification_loss': 1.1425637185573578}
2025-01-16 12:52:52,607 [INFO] Step[1850/2713]: training loss : 1.1744787538051604 TRAIN  loss dict:  {'classification_loss': 1.1744787538051604}
2025-01-16 12:53:08,656 [INFO] Step[1900/2713]: training loss : 1.2099225175380708 TRAIN  loss dict:  {'classification_loss': 1.2099225175380708}
2025-01-16 12:53:24,726 [INFO] Step[1950/2713]: training loss : 1.1257479202747345 TRAIN  loss dict:  {'classification_loss': 1.1257479202747345}
2025-01-16 12:53:40,737 [INFO] Step[2000/2713]: training loss : 1.0757969868183137 TRAIN  loss dict:  {'classification_loss': 1.0757969868183137}
2025-01-16 12:53:56,853 [INFO] Step[2050/2713]: training loss : 1.1674211180210115 TRAIN  loss dict:  {'classification_loss': 1.1674211180210115}
2025-01-16 12:54:12,956 [INFO] Step[2100/2713]: training loss : 1.235926022529602 TRAIN  loss dict:  {'classification_loss': 1.235926022529602}
2025-01-16 12:54:29,009 [INFO] Step[2150/2713]: training loss : 1.1586231637001037 TRAIN  loss dict:  {'classification_loss': 1.1586231637001037}
2025-01-16 12:54:45,063 [INFO] Step[2200/2713]: training loss : 1.1660652577877044 TRAIN  loss dict:  {'classification_loss': 1.1660652577877044}
2025-01-16 12:55:01,091 [INFO] Step[2250/2713]: training loss : 1.120261356830597 TRAIN  loss dict:  {'classification_loss': 1.120261356830597}
2025-01-16 12:55:17,116 [INFO] Step[2300/2713]: training loss : 1.2827158057689667 TRAIN  loss dict:  {'classification_loss': 1.2827158057689667}
2025-01-16 12:55:33,162 [INFO] Step[2350/2713]: training loss : 1.1360812520980834 TRAIN  loss dict:  {'classification_loss': 1.1360812520980834}
2025-01-16 12:55:49,177 [INFO] Step[2400/2713]: training loss : 1.1443967759609222 TRAIN  loss dict:  {'classification_loss': 1.1443967759609222}
2025-01-16 12:56:05,340 [INFO] Step[2450/2713]: training loss : 1.1679835510253906 TRAIN  loss dict:  {'classification_loss': 1.1679835510253906}
2025-01-16 12:56:21,355 [INFO] Step[2500/2713]: training loss : 1.1150870883464814 TRAIN  loss dict:  {'classification_loss': 1.1150870883464814}
2025-01-16 12:56:37,398 [INFO] Step[2550/2713]: training loss : 1.3041408002376556 TRAIN  loss dict:  {'classification_loss': 1.3041408002376556}
2025-01-16 12:56:53,450 [INFO] Step[2600/2713]: training loss : 1.2282108962535858 TRAIN  loss dict:  {'classification_loss': 1.2282108962535858}
2025-01-16 12:57:09,511 [INFO] Step[2650/2713]: training loss : 1.169679868221283 TRAIN  loss dict:  {'classification_loss': 1.169679868221283}
2025-01-16 12:57:25,474 [INFO] Step[2700/2713]: training loss : 1.195431159734726 TRAIN  loss dict:  {'classification_loss': 1.195431159734726}
2025-01-16 12:58:44,983 [INFO] Label accuracies statistics:
2025-01-16 12:58:44,984 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.75, 18: 0.5, 19: 0.5, 20: 1.0, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 1.0, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 0.5, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.25, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.25, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.25, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 0.8, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 0.25, 106: 0.75, 107: 0.0, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.25, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.5, 140: 0.75, 141: 1.0, 142: 0.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 0.75, 156: 0.25, 157: 0.75, 158: 0.6666666666666666, 159: 0.25, 160: 0.5, 161: 0.75, 162: 0.5, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.3333333333333333, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.0, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.25, 218: 0.75, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 0.75, 242: 0.75, 243: 0.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.3333333333333333, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.5, 253: 0.5, 254: 0.75, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.25, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.5, 265: 0.5, 266: 1.0, 267: 0.0, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.25, 276: 1.0, 277: 1.0, 278: 1.0, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.5, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 1.0, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.25, 326: 0.75, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.25, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 0.75, 381: 0.25, 382: 0.75, 383: 0.75, 384: 0.75, 385: 0.5, 386: 1.0, 387: 0.25, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-16 12:58:44,985 [INFO] [10] TRAIN  loss: 1.1646028703268805 acc: 0.9437277306794446
2025-01-16 12:58:44,985 [INFO] [10] TRAIN  loss dict: {'classification_loss': 1.1646028703268805}
2025-01-16 12:58:44,986 [INFO] [10] VALIDATION loss: 2.018630550544065 VALIDATION acc: 0.7235109717868339
2025-01-16 12:58:44,986 [INFO] [10] VALIDATION loss dict: {'classification_loss': 2.018630550544065}
2025-01-16 12:58:44,986 [INFO] 
2025-01-16 12:59:06,279 [INFO] Step[50/2713]: training loss : 1.1237063217163086 TRAIN  loss dict:  {'classification_loss': 1.1237063217163086}
2025-01-16 12:59:22,452 [INFO] Step[100/2713]: training loss : 1.2371928870677948 TRAIN  loss dict:  {'classification_loss': 1.2371928870677948}
2025-01-16 12:59:38,536 [INFO] Step[150/2713]: training loss : 1.09321195602417 TRAIN  loss dict:  {'classification_loss': 1.09321195602417}
2025-01-16 12:59:54,645 [INFO] Step[200/2713]: training loss : 1.1581041932106018 TRAIN  loss dict:  {'classification_loss': 1.1581041932106018}
2025-01-16 13:00:10,760 [INFO] Step[250/2713]: training loss : 1.0999110114574433 TRAIN  loss dict:  {'classification_loss': 1.0999110114574433}
2025-01-16 13:00:26,868 [INFO] Step[300/2713]: training loss : 1.0949077677726746 TRAIN  loss dict:  {'classification_loss': 1.0949077677726746}
2025-01-16 13:00:43,054 [INFO] Step[350/2713]: training loss : 1.1851151490211487 TRAIN  loss dict:  {'classification_loss': 1.1851151490211487}
2025-01-16 13:00:59,107 [INFO] Step[400/2713]: training loss : 1.1268750846385955 TRAIN  loss dict:  {'classification_loss': 1.1268750846385955}
2025-01-16 13:01:15,275 [INFO] Step[450/2713]: training loss : 1.143533262014389 TRAIN  loss dict:  {'classification_loss': 1.143533262014389}
2025-01-16 13:01:31,430 [INFO] Step[500/2713]: training loss : 1.0866375064849854 TRAIN  loss dict:  {'classification_loss': 1.0866375064849854}
2025-01-16 13:01:47,545 [INFO] Step[550/2713]: training loss : 1.1523219168186187 TRAIN  loss dict:  {'classification_loss': 1.1523219168186187}
2025-01-16 13:02:03,633 [INFO] Step[600/2713]: training loss : 1.1220143139362335 TRAIN  loss dict:  {'classification_loss': 1.1220143139362335}
2025-01-16 13:02:19,776 [INFO] Step[650/2713]: training loss : 1.088185909986496 TRAIN  loss dict:  {'classification_loss': 1.088185909986496}
2025-01-16 13:02:35,892 [INFO] Step[700/2713]: training loss : 1.0544107925891877 TRAIN  loss dict:  {'classification_loss': 1.0544107925891877}
2025-01-16 13:02:52,018 [INFO] Step[750/2713]: training loss : 1.1020020210742951 TRAIN  loss dict:  {'classification_loss': 1.1020020210742951}
2025-01-16 13:03:08,120 [INFO] Step[800/2713]: training loss : 1.095977519750595 TRAIN  loss dict:  {'classification_loss': 1.095977519750595}
2025-01-16 13:03:24,290 [INFO] Step[850/2713]: training loss : 1.0915073788166045 TRAIN  loss dict:  {'classification_loss': 1.0915073788166045}
2025-01-16 13:03:40,424 [INFO] Step[900/2713]: training loss : 1.0956353724002839 TRAIN  loss dict:  {'classification_loss': 1.0956353724002839}
2025-01-16 13:03:56,591 [INFO] Step[950/2713]: training loss : 1.0943184435367583 TRAIN  loss dict:  {'classification_loss': 1.0943184435367583}
2025-01-16 13:04:12,700 [INFO] Step[1000/2713]: training loss : 1.1657897067070007 TRAIN  loss dict:  {'classification_loss': 1.1657897067070007}
2025-01-16 13:04:28,852 [INFO] Step[1050/2713]: training loss : 1.0813334739208222 TRAIN  loss dict:  {'classification_loss': 1.0813334739208222}
2025-01-16 13:04:44,945 [INFO] Step[1100/2713]: training loss : 1.176563811302185 TRAIN  loss dict:  {'classification_loss': 1.176563811302185}
2025-01-16 13:05:01,107 [INFO] Step[1150/2713]: training loss : 1.0581174218654632 TRAIN  loss dict:  {'classification_loss': 1.0581174218654632}
2025-01-16 13:05:17,225 [INFO] Step[1200/2713]: training loss : 1.064663393497467 TRAIN  loss dict:  {'classification_loss': 1.064663393497467}
2025-01-16 13:05:33,359 [INFO] Step[1250/2713]: training loss : 1.165368525981903 TRAIN  loss dict:  {'classification_loss': 1.165368525981903}
2025-01-16 13:05:49,532 [INFO] Step[1300/2713]: training loss : 1.0761714482307434 TRAIN  loss dict:  {'classification_loss': 1.0761714482307434}
2025-01-16 13:06:05,726 [INFO] Step[1350/2713]: training loss : 1.0868042695522309 TRAIN  loss dict:  {'classification_loss': 1.0868042695522309}
2025-01-16 13:06:21,793 [INFO] Step[1400/2713]: training loss : 1.1011513090133667 TRAIN  loss dict:  {'classification_loss': 1.1011513090133667}
2025-01-16 13:06:37,898 [INFO] Step[1450/2713]: training loss : 1.0762626600265504 TRAIN  loss dict:  {'classification_loss': 1.0762626600265504}
2025-01-16 13:06:54,019 [INFO] Step[1500/2713]: training loss : 1.121635091304779 TRAIN  loss dict:  {'classification_loss': 1.121635091304779}
2025-01-16 13:07:10,192 [INFO] Step[1550/2713]: training loss : 1.1061411333084106 TRAIN  loss dict:  {'classification_loss': 1.1061411333084106}
2025-01-16 13:07:26,313 [INFO] Step[1600/2713]: training loss : 1.0798889267444611 TRAIN  loss dict:  {'classification_loss': 1.0798889267444611}
2025-01-16 13:07:42,443 [INFO] Step[1650/2713]: training loss : 1.1298824751377106 TRAIN  loss dict:  {'classification_loss': 1.1298824751377106}
2025-01-16 13:07:58,635 [INFO] Step[1700/2713]: training loss : 1.1147981929779052 TRAIN  loss dict:  {'classification_loss': 1.1147981929779052}
2025-01-16 13:08:14,790 [INFO] Step[1750/2713]: training loss : 1.1256184482574463 TRAIN  loss dict:  {'classification_loss': 1.1256184482574463}
2025-01-16 13:08:30,900 [INFO] Step[1800/2713]: training loss : 1.1175761306285859 TRAIN  loss dict:  {'classification_loss': 1.1175761306285859}
2025-01-16 13:08:47,034 [INFO] Step[1850/2713]: training loss : 1.0845919609069825 TRAIN  loss dict:  {'classification_loss': 1.0845919609069825}
2025-01-16 13:09:03,193 [INFO] Step[1900/2713]: training loss : 1.1536856043338775 TRAIN  loss dict:  {'classification_loss': 1.1536856043338775}
2025-01-16 13:09:19,295 [INFO] Step[1950/2713]: training loss : 1.1319957494735717 TRAIN  loss dict:  {'classification_loss': 1.1319957494735717}
2025-01-16 13:09:35,387 [INFO] Step[2000/2713]: training loss : 1.1148896825313568 TRAIN  loss dict:  {'classification_loss': 1.1148896825313568}
2025-01-16 13:09:51,509 [INFO] Step[2050/2713]: training loss : 1.0826958346366882 TRAIN  loss dict:  {'classification_loss': 1.0826958346366882}
2025-01-16 13:10:07,669 [INFO] Step[2100/2713]: training loss : 1.1216954159736634 TRAIN  loss dict:  {'classification_loss': 1.1216954159736634}
2025-01-16 13:10:23,828 [INFO] Step[2150/2713]: training loss : 1.0810888636112213 TRAIN  loss dict:  {'classification_loss': 1.0810888636112213}
2025-01-16 13:10:39,997 [INFO] Step[2200/2713]: training loss : 1.098251966238022 TRAIN  loss dict:  {'classification_loss': 1.098251966238022}
2025-01-16 13:10:56,143 [INFO] Step[2250/2713]: training loss : 1.124660484790802 TRAIN  loss dict:  {'classification_loss': 1.124660484790802}
2025-01-16 13:11:12,272 [INFO] Step[2300/2713]: training loss : 1.0849504852294922 TRAIN  loss dict:  {'classification_loss': 1.0849504852294922}
2025-01-16 13:11:28,483 [INFO] Step[2350/2713]: training loss : 1.0952261936664582 TRAIN  loss dict:  {'classification_loss': 1.0952261936664582}
2025-01-16 13:11:44,675 [INFO] Step[2400/2713]: training loss : 1.0980677592754364 TRAIN  loss dict:  {'classification_loss': 1.0980677592754364}
2025-01-16 13:12:00,876 [INFO] Step[2450/2713]: training loss : 1.0800623905658722 TRAIN  loss dict:  {'classification_loss': 1.0800623905658722}
2025-01-16 13:12:17,046 [INFO] Step[2500/2713]: training loss : 1.0653974986076356 TRAIN  loss dict:  {'classification_loss': 1.0653974986076356}
2025-01-16 13:12:33,178 [INFO] Step[2550/2713]: training loss : 1.0702424252033234 TRAIN  loss dict:  {'classification_loss': 1.0702424252033234}
2025-01-16 13:12:49,341 [INFO] Step[2600/2713]: training loss : 1.0664336347579957 TRAIN  loss dict:  {'classification_loss': 1.0664336347579957}
2025-01-16 13:13:05,525 [INFO] Step[2650/2713]: training loss : 1.0660453617572785 TRAIN  loss dict:  {'classification_loss': 1.0660453617572785}
2025-01-16 13:13:21,664 [INFO] Step[2700/2713]: training loss : 1.092453589439392 TRAIN  loss dict:  {'classification_loss': 1.092453589439392}
2025-01-16 13:14:40,935 [INFO] Label accuracies statistics:
2025-01-16 13:14:40,936 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.0, 69: 0.75, 70: 0.5, 71: 0.75, 72: 0.75, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 0.8, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.75, 144: 0.75, 145: 0.5, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 0.5, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.25, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.75, 231: 0.5, 232: 1.0, 233: 1.0, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.25, 240: 0.75, 241: 0.75, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.5, 265: 1.0, 266: 0.75, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 0.5, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.5, 319: 0.75, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 13:15:13,999 [INFO] [11] TRAIN  loss: 1.1072839969996373 acc: 0.9621575132080108
2025-01-16 13:15:13,999 [INFO] [11] TRAIN  loss dict: {'classification_loss': 1.1072839969996373}
2025-01-16 13:15:13,999 [INFO] [11] VALIDATION loss: 1.8633801913575123 VALIDATION acc: 0.7686520376175549
2025-01-16 13:15:13,999 [INFO] [11] VALIDATION loss dict: {'classification_loss': 1.8633801913575123}
2025-01-16 13:15:13,999 [INFO] 
2025-01-16 13:15:35,396 [INFO] Step[50/2713]: training loss : 1.0791859245300293 TRAIN  loss dict:  {'classification_loss': 1.0791859245300293}
2025-01-16 13:15:51,477 [INFO] Step[100/2713]: training loss : 1.0736636006832123 TRAIN  loss dict:  {'classification_loss': 1.0736636006832123}
2025-01-16 13:16:07,590 [INFO] Step[150/2713]: training loss : 1.0791533744335176 TRAIN  loss dict:  {'classification_loss': 1.0791533744335176}
2025-01-16 13:16:23,684 [INFO] Step[200/2713]: training loss : 1.0450215244293213 TRAIN  loss dict:  {'classification_loss': 1.0450215244293213}
2025-01-16 13:16:39,828 [INFO] Step[250/2713]: training loss : 1.1123223972320557 TRAIN  loss dict:  {'classification_loss': 1.1123223972320557}
2025-01-16 13:16:55,929 [INFO] Step[300/2713]: training loss : 1.0673495757579803 TRAIN  loss dict:  {'classification_loss': 1.0673495757579803}
2025-01-16 13:17:12,114 [INFO] Step[350/2713]: training loss : 1.057382160425186 TRAIN  loss dict:  {'classification_loss': 1.057382160425186}
2025-01-16 13:17:28,248 [INFO] Step[400/2713]: training loss : 1.0880772125720979 TRAIN  loss dict:  {'classification_loss': 1.0880772125720979}
2025-01-16 13:17:44,411 [INFO] Step[450/2713]: training loss : 1.0979712879657746 TRAIN  loss dict:  {'classification_loss': 1.0979712879657746}
2025-01-16 13:18:00,474 [INFO] Step[500/2713]: training loss : 1.0645502078533173 TRAIN  loss dict:  {'classification_loss': 1.0645502078533173}
2025-01-16 13:18:16,654 [INFO] Step[550/2713]: training loss : 1.1213148021697998 TRAIN  loss dict:  {'classification_loss': 1.1213148021697998}
2025-01-16 13:18:32,793 [INFO] Step[600/2713]: training loss : 1.0948580753803254 TRAIN  loss dict:  {'classification_loss': 1.0948580753803254}
2025-01-16 13:18:49,012 [INFO] Step[650/2713]: training loss : 1.0385184931755065 TRAIN  loss dict:  {'classification_loss': 1.0385184931755065}
2025-01-16 13:19:05,160 [INFO] Step[700/2713]: training loss : 1.0454401862621308 TRAIN  loss dict:  {'classification_loss': 1.0454401862621308}
2025-01-16 13:19:21,362 [INFO] Step[750/2713]: training loss : 1.0805551755428313 TRAIN  loss dict:  {'classification_loss': 1.0805551755428313}
2025-01-16 13:19:37,464 [INFO] Step[800/2713]: training loss : 1.0706555247306824 TRAIN  loss dict:  {'classification_loss': 1.0706555247306824}
2025-01-16 13:19:53,596 [INFO] Step[850/2713]: training loss : 1.062410479784012 TRAIN  loss dict:  {'classification_loss': 1.062410479784012}
2025-01-16 13:20:09,704 [INFO] Step[900/2713]: training loss : 1.0682776141166688 TRAIN  loss dict:  {'classification_loss': 1.0682776141166688}
2025-01-16 13:20:25,846 [INFO] Step[950/2713]: training loss : 1.1199015533924104 TRAIN  loss dict:  {'classification_loss': 1.1199015533924104}
2025-01-16 13:20:41,944 [INFO] Step[1000/2713]: training loss : 1.0722703218460083 TRAIN  loss dict:  {'classification_loss': 1.0722703218460083}
2025-01-16 13:20:58,100 [INFO] Step[1050/2713]: training loss : 1.1209992420673371 TRAIN  loss dict:  {'classification_loss': 1.1209992420673371}
2025-01-16 13:21:14,198 [INFO] Step[1100/2713]: training loss : 1.0627222013473512 TRAIN  loss dict:  {'classification_loss': 1.0627222013473512}
2025-01-16 13:21:30,390 [INFO] Step[1150/2713]: training loss : 1.0189978539943696 TRAIN  loss dict:  {'classification_loss': 1.0189978539943696}
2025-01-16 13:21:46,473 [INFO] Step[1200/2713]: training loss : 1.1149538588523864 TRAIN  loss dict:  {'classification_loss': 1.1149538588523864}
2025-01-16 13:22:02,599 [INFO] Step[1250/2713]: training loss : 1.055533368587494 TRAIN  loss dict:  {'classification_loss': 1.055533368587494}
2025-01-16 13:22:18,713 [INFO] Step[1300/2713]: training loss : 1.0554896247386933 TRAIN  loss dict:  {'classification_loss': 1.0554896247386933}
2025-01-16 13:22:34,899 [INFO] Step[1350/2713]: training loss : 1.0692176818847656 TRAIN  loss dict:  {'classification_loss': 1.0692176818847656}
2025-01-16 13:22:50,993 [INFO] Step[1400/2713]: training loss : 1.045413761138916 TRAIN  loss dict:  {'classification_loss': 1.045413761138916}
2025-01-16 13:23:07,167 [INFO] Step[1450/2713]: training loss : 1.1295173859596253 TRAIN  loss dict:  {'classification_loss': 1.1295173859596253}
2025-01-16 13:23:23,254 [INFO] Step[1500/2713]: training loss : 1.1018044447898865 TRAIN  loss dict:  {'classification_loss': 1.1018044447898865}
2025-01-16 13:23:39,385 [INFO] Step[1550/2713]: training loss : 1.1403089725971223 TRAIN  loss dict:  {'classification_loss': 1.1403089725971223}
2025-01-16 13:23:55,479 [INFO] Step[1600/2713]: training loss : 1.1020702278614045 TRAIN  loss dict:  {'classification_loss': 1.1020702278614045}
2025-01-16 13:24:11,663 [INFO] Step[1650/2713]: training loss : 1.1182228195667268 TRAIN  loss dict:  {'classification_loss': 1.1182228195667268}
2025-01-16 13:24:27,733 [INFO] Step[1700/2713]: training loss : 1.0483350241184235 TRAIN  loss dict:  {'classification_loss': 1.0483350241184235}
2025-01-16 13:24:43,864 [INFO] Step[1750/2713]: training loss : 1.075668624639511 TRAIN  loss dict:  {'classification_loss': 1.075668624639511}
2025-01-16 13:24:59,959 [INFO] Step[1800/2713]: training loss : 1.0791277360916138 TRAIN  loss dict:  {'classification_loss': 1.0791277360916138}
2025-01-16 13:25:16,165 [INFO] Step[1850/2713]: training loss : 1.0757914590835571 TRAIN  loss dict:  {'classification_loss': 1.0757914590835571}
2025-01-16 13:25:32,275 [INFO] Step[1900/2713]: training loss : 1.1074187386035919 TRAIN  loss dict:  {'classification_loss': 1.1074187386035919}
2025-01-16 13:25:48,407 [INFO] Step[1950/2713]: training loss : 1.0376458168029785 TRAIN  loss dict:  {'classification_loss': 1.0376458168029785}
2025-01-16 13:26:04,593 [INFO] Step[2000/2713]: training loss : 1.1054031562805176 TRAIN  loss dict:  {'classification_loss': 1.1054031562805176}
2025-01-16 13:26:20,702 [INFO] Step[2050/2713]: training loss : 1.0547017335891724 TRAIN  loss dict:  {'classification_loss': 1.0547017335891724}
2025-01-16 13:26:36,778 [INFO] Step[2100/2713]: training loss : 1.0690464544296265 TRAIN  loss dict:  {'classification_loss': 1.0690464544296265}
2025-01-16 13:26:52,895 [INFO] Step[2150/2713]: training loss : 1.1479339981079102 TRAIN  loss dict:  {'classification_loss': 1.1479339981079102}
2025-01-16 13:27:09,020 [INFO] Step[2200/2713]: training loss : 1.0516626405715943 TRAIN  loss dict:  {'classification_loss': 1.0516626405715943}
2025-01-16 13:27:25,159 [INFO] Step[2250/2713]: training loss : 1.0757750427722932 TRAIN  loss dict:  {'classification_loss': 1.0757750427722932}
2025-01-16 13:27:41,256 [INFO] Step[2300/2713]: training loss : 1.0685550320148467 TRAIN  loss dict:  {'classification_loss': 1.0685550320148467}
2025-01-16 13:27:57,398 [INFO] Step[2350/2713]: training loss : 1.0782974946498871 TRAIN  loss dict:  {'classification_loss': 1.0782974946498871}
2025-01-16 13:28:13,516 [INFO] Step[2400/2713]: training loss : 1.0883356595039368 TRAIN  loss dict:  {'classification_loss': 1.0883356595039368}
2025-01-16 13:28:29,678 [INFO] Step[2450/2713]: training loss : 1.108999320268631 TRAIN  loss dict:  {'classification_loss': 1.108999320268631}
2025-01-16 13:28:45,778 [INFO] Step[2500/2713]: training loss : 1.1103077042102814 TRAIN  loss dict:  {'classification_loss': 1.1103077042102814}
2025-01-16 13:29:01,927 [INFO] Step[2550/2713]: training loss : 1.0559601950645447 TRAIN  loss dict:  {'classification_loss': 1.0559601950645447}
2025-01-16 13:29:18,106 [INFO] Step[2600/2713]: training loss : 1.0761276650428773 TRAIN  loss dict:  {'classification_loss': 1.0761276650428773}
2025-01-16 13:29:34,243 [INFO] Step[2650/2713]: training loss : 1.138997083902359 TRAIN  loss dict:  {'classification_loss': 1.138997083902359}
2025-01-16 13:29:50,348 [INFO] Step[2700/2713]: training loss : 1.1321970403194428 TRAIN  loss dict:  {'classification_loss': 1.1321970403194428}
2025-01-16 13:31:09,874 [INFO] Label accuracies statistics:
2025-01-16 13:31:09,875 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.25, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 0.75, 30: 0.75, 31: 0.75, 32: 0.5, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.5, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.5, 64: 0.5, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 0.5, 142: 0.5, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.5, 170: 0.75, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.5, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.0, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.5, 218: 1.0, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.5, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.75, 233: 0.25, 234: 0.5, 235: 0.5, 236: 1.0, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.0, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.75, 261: 0.25, 262: 1.0, 263: 1.0, 264: 0.5, 265: 0.75, 266: 1.0, 267: 0.25, 268: 0.0, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.5, 285: 1.0, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.5, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.25, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.75, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 0.75, 375: 0.5, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 0.75, 383: 0.25, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 0.5, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.75, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 13:31:09,877 [INFO] [12] TRAIN  loss: 1.0827649501562557 acc: 0.968055043617152
2025-01-16 13:31:09,877 [INFO] [12] TRAIN  loss dict: {'classification_loss': 1.0827649501562557}
2025-01-16 13:31:09,877 [INFO] [12] VALIDATION loss: 1.892538837472299 VALIDATION acc: 0.7542319749216301
2025-01-16 13:31:09,877 [INFO] [12] VALIDATION loss dict: {'classification_loss': 1.892538837472299}
2025-01-16 13:31:09,877 [INFO] 
2025-01-16 13:31:30,748 [INFO] Step[50/2713]: training loss : 1.065081776380539 TRAIN  loss dict:  {'classification_loss': 1.065081776380539}
2025-01-16 13:31:47,095 [INFO] Step[100/2713]: training loss : 1.0813640117645265 TRAIN  loss dict:  {'classification_loss': 1.0813640117645265}
2025-01-16 13:32:03,247 [INFO] Step[150/2713]: training loss : 1.0375200450420379 TRAIN  loss dict:  {'classification_loss': 1.0375200450420379}
2025-01-16 13:32:19,451 [INFO] Step[200/2713]: training loss : 1.076623682975769 TRAIN  loss dict:  {'classification_loss': 1.076623682975769}
2025-01-16 13:32:35,526 [INFO] Step[250/2713]: training loss : 1.0443703305721284 TRAIN  loss dict:  {'classification_loss': 1.0443703305721284}
2025-01-16 13:32:51,698 [INFO] Step[300/2713]: training loss : 1.0733631753921509 TRAIN  loss dict:  {'classification_loss': 1.0733631753921509}
2025-01-16 13:33:07,854 [INFO] Step[350/2713]: training loss : 1.0468803536891937 TRAIN  loss dict:  {'classification_loss': 1.0468803536891937}
2025-01-16 13:33:23,973 [INFO] Step[400/2713]: training loss : 1.1239430129528045 TRAIN  loss dict:  {'classification_loss': 1.1239430129528045}
2025-01-16 13:33:40,085 [INFO] Step[450/2713]: training loss : 1.0450423657894135 TRAIN  loss dict:  {'classification_loss': 1.0450423657894135}
2025-01-16 13:33:56,298 [INFO] Step[500/2713]: training loss : 1.0608189105987549 TRAIN  loss dict:  {'classification_loss': 1.0608189105987549}
2025-01-16 13:34:12,386 [INFO] Step[550/2713]: training loss : 1.1256881439685822 TRAIN  loss dict:  {'classification_loss': 1.1256881439685822}
2025-01-16 13:34:28,519 [INFO] Step[600/2713]: training loss : 1.0801139962673187 TRAIN  loss dict:  {'classification_loss': 1.0801139962673187}
2025-01-16 13:34:44,748 [INFO] Step[650/2713]: training loss : 1.054745650291443 TRAIN  loss dict:  {'classification_loss': 1.054745650291443}
2025-01-16 13:35:00,827 [INFO] Step[700/2713]: training loss : 1.063557904958725 TRAIN  loss dict:  {'classification_loss': 1.063557904958725}
2025-01-16 13:35:16,986 [INFO] Step[750/2713]: training loss : 1.070393123626709 TRAIN  loss dict:  {'classification_loss': 1.070393123626709}
2025-01-16 13:35:33,067 [INFO] Step[800/2713]: training loss : 1.0889193987846375 TRAIN  loss dict:  {'classification_loss': 1.0889193987846375}
2025-01-16 13:35:49,194 [INFO] Step[850/2713]: training loss : 1.084503129720688 TRAIN  loss dict:  {'classification_loss': 1.084503129720688}
2025-01-16 13:36:05,322 [INFO] Step[900/2713]: training loss : 1.0509716963768005 TRAIN  loss dict:  {'classification_loss': 1.0509716963768005}
2025-01-16 13:36:21,430 [INFO] Step[950/2713]: training loss : 1.1546756982803346 TRAIN  loss dict:  {'classification_loss': 1.1546756982803346}
2025-01-16 13:36:37,594 [INFO] Step[1000/2713]: training loss : 1.0456456899642945 TRAIN  loss dict:  {'classification_loss': 1.0456456899642945}
2025-01-16 13:36:53,822 [INFO] Step[1050/2713]: training loss : 1.0394291973114014 TRAIN  loss dict:  {'classification_loss': 1.0394291973114014}
2025-01-16 13:37:09,926 [INFO] Step[1100/2713]: training loss : 1.0654935657978057 TRAIN  loss dict:  {'classification_loss': 1.0654935657978057}
2025-01-16 13:37:26,087 [INFO] Step[1150/2713]: training loss : 1.1074353206157683 TRAIN  loss dict:  {'classification_loss': 1.1074353206157683}
2025-01-16 13:37:42,169 [INFO] Step[1200/2713]: training loss : 1.0507947826385498 TRAIN  loss dict:  {'classification_loss': 1.0507947826385498}
2025-01-16 13:37:58,295 [INFO] Step[1250/2713]: training loss : 1.0383541703224182 TRAIN  loss dict:  {'classification_loss': 1.0383541703224182}
2025-01-16 13:38:14,308 [INFO] Step[1300/2713]: training loss : 1.03860689163208 TRAIN  loss dict:  {'classification_loss': 1.03860689163208}
2025-01-16 13:38:30,456 [INFO] Step[1350/2713]: training loss : 1.0653052425384522 TRAIN  loss dict:  {'classification_loss': 1.0653052425384522}
2025-01-16 13:38:46,511 [INFO] Step[1400/2713]: training loss : 1.0097039139270783 TRAIN  loss dict:  {'classification_loss': 1.0097039139270783}
2025-01-16 13:39:02,585 [INFO] Step[1450/2713]: training loss : 1.0930130767822266 TRAIN  loss dict:  {'classification_loss': 1.0930130767822266}
2025-01-16 13:39:18,684 [INFO] Step[1500/2713]: training loss : 1.045852689743042 TRAIN  loss dict:  {'classification_loss': 1.045852689743042}
2025-01-16 13:39:34,835 [INFO] Step[1550/2713]: training loss : 1.0984073460102082 TRAIN  loss dict:  {'classification_loss': 1.0984073460102082}
2025-01-16 13:39:50,907 [INFO] Step[1600/2713]: training loss : 1.0966037046909332 TRAIN  loss dict:  {'classification_loss': 1.0966037046909332}
2025-01-16 13:40:06,994 [INFO] Step[1650/2713]: training loss : 1.0679994165897368 TRAIN  loss dict:  {'classification_loss': 1.0679994165897368}
2025-01-16 13:40:23,110 [INFO] Step[1700/2713]: training loss : 1.0575434648990631 TRAIN  loss dict:  {'classification_loss': 1.0575434648990631}
2025-01-16 13:40:39,176 [INFO] Step[1750/2713]: training loss : 1.1439138555526733 TRAIN  loss dict:  {'classification_loss': 1.1439138555526733}
2025-01-16 13:40:55,231 [INFO] Step[1800/2713]: training loss : 1.0879651057720183 TRAIN  loss dict:  {'classification_loss': 1.0879651057720183}
2025-01-16 13:41:11,388 [INFO] Step[1850/2713]: training loss : 1.0533323109149932 TRAIN  loss dict:  {'classification_loss': 1.0533323109149932}
2025-01-16 13:41:27,561 [INFO] Step[1900/2713]: training loss : 1.0233304917812347 TRAIN  loss dict:  {'classification_loss': 1.0233304917812347}
2025-01-16 13:41:43,697 [INFO] Step[1950/2713]: training loss : 1.1725862956047057 TRAIN  loss dict:  {'classification_loss': 1.1725862956047057}
2025-01-16 13:41:59,843 [INFO] Step[2000/2713]: training loss : 1.086684684753418 TRAIN  loss dict:  {'classification_loss': 1.086684684753418}
2025-01-16 13:42:15,999 [INFO] Step[2050/2713]: training loss : 1.122043342590332 TRAIN  loss dict:  {'classification_loss': 1.122043342590332}
2025-01-16 13:42:32,170 [INFO] Step[2100/2713]: training loss : 1.0236794018745423 TRAIN  loss dict:  {'classification_loss': 1.0236794018745423}
2025-01-16 13:42:48,274 [INFO] Step[2150/2713]: training loss : 1.1121612095832825 TRAIN  loss dict:  {'classification_loss': 1.1121612095832825}
2025-01-16 13:43:04,395 [INFO] Step[2200/2713]: training loss : 1.043687298297882 TRAIN  loss dict:  {'classification_loss': 1.043687298297882}
2025-01-16 13:43:20,513 [INFO] Step[2250/2713]: training loss : 1.1013241136074066 TRAIN  loss dict:  {'classification_loss': 1.1013241136074066}
2025-01-16 13:43:36,628 [INFO] Step[2300/2713]: training loss : 1.0648511600494386 TRAIN  loss dict:  {'classification_loss': 1.0648511600494386}
2025-01-16 13:43:52,803 [INFO] Step[2350/2713]: training loss : 1.0574518358707428 TRAIN  loss dict:  {'classification_loss': 1.0574518358707428}
2025-01-16 13:44:08,889 [INFO] Step[2400/2713]: training loss : 1.1126215887069701 TRAIN  loss dict:  {'classification_loss': 1.1126215887069701}
2025-01-16 13:44:25,025 [INFO] Step[2450/2713]: training loss : 1.07152730345726 TRAIN  loss dict:  {'classification_loss': 1.07152730345726}
2025-01-16 13:44:41,105 [INFO] Step[2500/2713]: training loss : 1.0741293239593506 TRAIN  loss dict:  {'classification_loss': 1.0741293239593506}
2025-01-16 13:44:57,218 [INFO] Step[2550/2713]: training loss : 1.0863013911247252 TRAIN  loss dict:  {'classification_loss': 1.0863013911247252}
2025-01-16 13:45:13,363 [INFO] Step[2600/2713]: training loss : 1.1122938549518586 TRAIN  loss dict:  {'classification_loss': 1.1122938549518586}
2025-01-16 13:45:29,538 [INFO] Step[2650/2713]: training loss : 1.0339494633674622 TRAIN  loss dict:  {'classification_loss': 1.0339494633674622}
2025-01-16 13:45:45,732 [INFO] Step[2700/2713]: training loss : 1.149181762933731 TRAIN  loss dict:  {'classification_loss': 1.149181762933731}
2025-01-16 13:47:04,379 [INFO] Label accuracies statistics:
2025-01-16 13:47:04,379 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 1.0, 25: 0.75, 26: 0.25, 27: 1.0, 28: 1.0, 29: 0.75, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.25, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 1.0, 52: 0.75, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.75, 86: 0.5, 87: 0.5, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.5, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 0.75, 142: 0.5, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 0.75, 196: 0.75, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 1.0, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.25, 230: 1.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.25, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.5, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.5, 260: 0.5, 261: 1.0, 262: 0.5, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.5, 267: 0.25, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.5, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.5, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.5, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 1.0, 326: 0.5, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.5, 357: 0.75, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 1.0, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.25, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.5, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.5, 388: 0.75, 389: 0.75, 390: 0.5, 391: 1.0, 392: 1.0, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 13:47:07,141 [INFO] [13] TRAIN  loss: 1.0760052092744115 acc: 0.9687922349182946
2025-01-16 13:47:07,141 [INFO] [13] TRAIN  loss dict: {'classification_loss': 1.0760052092744115}
2025-01-16 13:47:07,141 [INFO] [13] VALIDATION loss: 1.863132065400145 VALIDATION acc: 0.7774294670846394
2025-01-16 13:47:07,141 [INFO] [13] VALIDATION loss dict: {'classification_loss': 1.863132065400145}
2025-01-16 13:47:07,141 [INFO] 
2025-01-16 13:47:28,013 [INFO] Step[50/2713]: training loss : 1.0285934937000274 TRAIN  loss dict:  {'classification_loss': 1.0285934937000274}
2025-01-16 13:47:44,115 [INFO] Step[100/2713]: training loss : 1.0285162425041199 TRAIN  loss dict:  {'classification_loss': 1.0285162425041199}
2025-01-16 13:48:00,221 [INFO] Step[150/2713]: training loss : 1.0232611203193664 TRAIN  loss dict:  {'classification_loss': 1.0232611203193664}
2025-01-16 13:48:16,331 [INFO] Step[200/2713]: training loss : 1.0182343113422394 TRAIN  loss dict:  {'classification_loss': 1.0182343113422394}
2025-01-16 13:48:32,481 [INFO] Step[250/2713]: training loss : 1.0216134321689605 TRAIN  loss dict:  {'classification_loss': 1.0216134321689605}
2025-01-16 13:48:48,535 [INFO] Step[300/2713]: training loss : 1.0400281596183776 TRAIN  loss dict:  {'classification_loss': 1.0400281596183776}
2025-01-16 13:49:04,710 [INFO] Step[350/2713]: training loss : 1.0318063879013062 TRAIN  loss dict:  {'classification_loss': 1.0318063879013062}
2025-01-16 13:49:20,803 [INFO] Step[400/2713]: training loss : 1.0616225910186767 TRAIN  loss dict:  {'classification_loss': 1.0616225910186767}
2025-01-16 13:49:36,880 [INFO] Step[450/2713]: training loss : 1.0316948676109314 TRAIN  loss dict:  {'classification_loss': 1.0316948676109314}
2025-01-16 13:49:52,935 [INFO] Step[500/2713]: training loss : 1.127726595401764 TRAIN  loss dict:  {'classification_loss': 1.127726595401764}
2025-01-16 13:50:09,008 [INFO] Step[550/2713]: training loss : 1.0861991035938263 TRAIN  loss dict:  {'classification_loss': 1.0861991035938263}
2025-01-16 13:50:25,092 [INFO] Step[600/2713]: training loss : 1.0767438042163848 TRAIN  loss dict:  {'classification_loss': 1.0767438042163848}
2025-01-16 13:50:41,222 [INFO] Step[650/2713]: training loss : 1.0260925149917604 TRAIN  loss dict:  {'classification_loss': 1.0260925149917604}
2025-01-16 13:50:57,286 [INFO] Step[700/2713]: training loss : 1.0851769149303436 TRAIN  loss dict:  {'classification_loss': 1.0851769149303436}
2025-01-16 13:51:13,372 [INFO] Step[750/2713]: training loss : 1.0723361015319823 TRAIN  loss dict:  {'classification_loss': 1.0723361015319823}
2025-01-16 13:51:29,503 [INFO] Step[800/2713]: training loss : 1.0702798819541932 TRAIN  loss dict:  {'classification_loss': 1.0702798819541932}
2025-01-16 13:51:45,617 [INFO] Step[850/2713]: training loss : 1.063516627550125 TRAIN  loss dict:  {'classification_loss': 1.063516627550125}
2025-01-16 13:52:01,668 [INFO] Step[900/2713]: training loss : 1.0640693843364715 TRAIN  loss dict:  {'classification_loss': 1.0640693843364715}
2025-01-16 13:52:17,783 [INFO] Step[950/2713]: training loss : 1.0976432538032532 TRAIN  loss dict:  {'classification_loss': 1.0976432538032532}
2025-01-16 13:52:33,916 [INFO] Step[1000/2713]: training loss : 1.0596814680099487 TRAIN  loss dict:  {'classification_loss': 1.0596814680099487}
2025-01-16 13:52:50,051 [INFO] Step[1050/2713]: training loss : 1.0540722370147706 TRAIN  loss dict:  {'classification_loss': 1.0540722370147706}
2025-01-16 13:53:06,163 [INFO] Step[1100/2713]: training loss : 1.0030682051181794 TRAIN  loss dict:  {'classification_loss': 1.0030682051181794}
2025-01-16 13:53:22,275 [INFO] Step[1150/2713]: training loss : 1.040669286251068 TRAIN  loss dict:  {'classification_loss': 1.040669286251068}
2025-01-16 13:53:38,409 [INFO] Step[1200/2713]: training loss : 1.0208087396621703 TRAIN  loss dict:  {'classification_loss': 1.0208087396621703}
2025-01-16 13:53:54,503 [INFO] Step[1250/2713]: training loss : 1.0598957967758178 TRAIN  loss dict:  {'classification_loss': 1.0598957967758178}
2025-01-16 13:54:10,627 [INFO] Step[1300/2713]: training loss : 1.0554126000404358 TRAIN  loss dict:  {'classification_loss': 1.0554126000404358}
2025-01-16 13:54:26,709 [INFO] Step[1350/2713]: training loss : 1.0599582815170288 TRAIN  loss dict:  {'classification_loss': 1.0599582815170288}
2025-01-16 13:54:42,789 [INFO] Step[1400/2713]: training loss : 1.0697842288017272 TRAIN  loss dict:  {'classification_loss': 1.0697842288017272}
2025-01-16 13:54:58,944 [INFO] Step[1450/2713]: training loss : 1.05340594291687 TRAIN  loss dict:  {'classification_loss': 1.05340594291687}
2025-01-16 13:55:15,048 [INFO] Step[1500/2713]: training loss : 1.0364544892311096 TRAIN  loss dict:  {'classification_loss': 1.0364544892311096}
2025-01-16 13:55:31,099 [INFO] Step[1550/2713]: training loss : 1.0505326282978058 TRAIN  loss dict:  {'classification_loss': 1.0505326282978058}
2025-01-16 13:55:47,193 [INFO] Step[1600/2713]: training loss : 1.0657444941997527 TRAIN  loss dict:  {'classification_loss': 1.0657444941997527}
2025-01-16 13:56:03,309 [INFO] Step[1650/2713]: training loss : 1.072681280374527 TRAIN  loss dict:  {'classification_loss': 1.072681280374527}
2025-01-16 13:56:19,407 [INFO] Step[1700/2713]: training loss : 1.0624948167800903 TRAIN  loss dict:  {'classification_loss': 1.0624948167800903}
2025-01-16 13:56:35,510 [INFO] Step[1750/2713]: training loss : 1.0628029751777648 TRAIN  loss dict:  {'classification_loss': 1.0628029751777648}
2025-01-16 13:56:51,588 [INFO] Step[1800/2713]: training loss : 1.0400876653194429 TRAIN  loss dict:  {'classification_loss': 1.0400876653194429}
2025-01-16 13:57:07,679 [INFO] Step[1850/2713]: training loss : 1.0295582354068755 TRAIN  loss dict:  {'classification_loss': 1.0295582354068755}
2025-01-16 13:57:23,801 [INFO] Step[1900/2713]: training loss : 1.022535307407379 TRAIN  loss dict:  {'classification_loss': 1.022535307407379}
2025-01-16 13:57:39,969 [INFO] Step[1950/2713]: training loss : 1.107162870168686 TRAIN  loss dict:  {'classification_loss': 1.107162870168686}
2025-01-16 13:57:56,020 [INFO] Step[2000/2713]: training loss : 1.022449073791504 TRAIN  loss dict:  {'classification_loss': 1.022449073791504}
2025-01-16 13:58:12,189 [INFO] Step[2050/2713]: training loss : 1.0864233243465424 TRAIN  loss dict:  {'classification_loss': 1.0864233243465424}
2025-01-16 13:58:28,310 [INFO] Step[2100/2713]: training loss : 1.0649947392940522 TRAIN  loss dict:  {'classification_loss': 1.0649947392940522}
2025-01-16 13:58:44,525 [INFO] Step[2150/2713]: training loss : 1.0553663444519044 TRAIN  loss dict:  {'classification_loss': 1.0553663444519044}
2025-01-16 13:59:00,618 [INFO] Step[2200/2713]: training loss : 1.028279265165329 TRAIN  loss dict:  {'classification_loss': 1.028279265165329}
2025-01-16 13:59:16,778 [INFO] Step[2250/2713]: training loss : 1.0049328422546386 TRAIN  loss dict:  {'classification_loss': 1.0049328422546386}
2025-01-16 13:59:32,896 [INFO] Step[2300/2713]: training loss : 1.00983407497406 TRAIN  loss dict:  {'classification_loss': 1.00983407497406}
2025-01-16 13:59:49,015 [INFO] Step[2350/2713]: training loss : 1.0750856256484986 TRAIN  loss dict:  {'classification_loss': 1.0750856256484986}
2025-01-16 14:00:05,115 [INFO] Step[2400/2713]: training loss : 1.1199656510353089 TRAIN  loss dict:  {'classification_loss': 1.1199656510353089}
2025-01-16 14:00:21,281 [INFO] Step[2450/2713]: training loss : 1.098001263141632 TRAIN  loss dict:  {'classification_loss': 1.098001263141632}
2025-01-16 14:00:37,334 [INFO] Step[2500/2713]: training loss : 1.0342965483665467 TRAIN  loss dict:  {'classification_loss': 1.0342965483665467}
2025-01-16 14:00:53,562 [INFO] Step[2550/2713]: training loss : 1.0476462471485137 TRAIN  loss dict:  {'classification_loss': 1.0476462471485137}
2025-01-16 14:01:09,709 [INFO] Step[2600/2713]: training loss : 1.0353666031360627 TRAIN  loss dict:  {'classification_loss': 1.0353666031360627}
2025-01-16 14:01:25,805 [INFO] Step[2650/2713]: training loss : 0.9994758021831512 TRAIN  loss dict:  {'classification_loss': 0.9994758021831512}
2025-01-16 14:01:41,981 [INFO] Step[2700/2713]: training loss : 1.0499159383773804 TRAIN  loss dict:  {'classification_loss': 1.0499159383773804}
2025-01-16 14:03:00,607 [INFO] Label accuracies statistics:
2025-01-16 14:03:00,607 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 0.75, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.25, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 0.75, 132: 0.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 0.75, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.5, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.5, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 1.0, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.25, 207: 0.25, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 1.0, 218: 1.0, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.5, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.5, 250: 0.5, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.5, 261: 0.25, 262: 1.0, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 0.5, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.5, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.75, 332: 0.75, 333: 0.25, 334: 1.0, 335: 1.0, 336: 0.25, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 14:03:00,609 [INFO] [14] TRAIN  loss: 1.0518749795010993 acc: 0.9764098783634353
2025-01-16 14:03:00,609 [INFO] [14] TRAIN  loss dict: {'classification_loss': 1.0518749795010993}
2025-01-16 14:03:00,609 [INFO] [14] VALIDATION loss: 1.9152342282739796 VALIDATION acc: 0.7592476489028214
2025-01-16 14:03:00,609 [INFO] [14] VALIDATION loss dict: {'classification_loss': 1.9152342282739796}
2025-01-16 14:03:00,609 [INFO] 
2025-01-16 14:03:21,791 [INFO] Step[50/2713]: training loss : 1.0636597537994386 TRAIN  loss dict:  {'classification_loss': 1.0636597537994386}
2025-01-16 14:03:37,818 [INFO] Step[100/2713]: training loss : 1.0412826478481292 TRAIN  loss dict:  {'classification_loss': 1.0412826478481292}
2025-01-16 14:03:53,877 [INFO] Step[150/2713]: training loss : 1.0437014877796174 TRAIN  loss dict:  {'classification_loss': 1.0437014877796174}
2025-01-16 14:04:09,894 [INFO] Step[200/2713]: training loss : 1.1174514019489288 TRAIN  loss dict:  {'classification_loss': 1.1174514019489288}
2025-01-16 14:04:25,937 [INFO] Step[250/2713]: training loss : 1.0484879100322724 TRAIN  loss dict:  {'classification_loss': 1.0484879100322724}
2025-01-16 14:04:41,882 [INFO] Step[300/2713]: training loss : 1.0167350089550018 TRAIN  loss dict:  {'classification_loss': 1.0167350089550018}
2025-01-16 14:04:57,979 [INFO] Step[350/2713]: training loss : 1.009334509372711 TRAIN  loss dict:  {'classification_loss': 1.009334509372711}
2025-01-16 14:05:13,991 [INFO] Step[400/2713]: training loss : 1.0346743261814118 TRAIN  loss dict:  {'classification_loss': 1.0346743261814118}
2025-01-16 14:05:30,039 [INFO] Step[450/2713]: training loss : 1.0340555036067962 TRAIN  loss dict:  {'classification_loss': 1.0340555036067962}
2025-01-16 14:05:46,081 [INFO] Step[500/2713]: training loss : 1.05054722905159 TRAIN  loss dict:  {'classification_loss': 1.05054722905159}
2025-01-16 14:06:02,118 [INFO] Step[550/2713]: training loss : 1.0220369446277617 TRAIN  loss dict:  {'classification_loss': 1.0220369446277617}
2025-01-16 14:06:18,162 [INFO] Step[600/2713]: training loss : 1.0362464463710785 TRAIN  loss dict:  {'classification_loss': 1.0362464463710785}
2025-01-16 14:06:34,167 [INFO] Step[650/2713]: training loss : 1.0162908780574798 TRAIN  loss dict:  {'classification_loss': 1.0162908780574798}
2025-01-16 14:06:50,149 [INFO] Step[700/2713]: training loss : 1.085248303413391 TRAIN  loss dict:  {'classification_loss': 1.085248303413391}
2025-01-16 14:07:06,204 [INFO] Step[750/2713]: training loss : 1.0228617870807648 TRAIN  loss dict:  {'classification_loss': 1.0228617870807648}
2025-01-16 14:07:22,244 [INFO] Step[800/2713]: training loss : 1.078914134502411 TRAIN  loss dict:  {'classification_loss': 1.078914134502411}
2025-01-16 14:07:38,290 [INFO] Step[850/2713]: training loss : 1.026720825433731 TRAIN  loss dict:  {'classification_loss': 1.026720825433731}
2025-01-16 14:07:54,348 [INFO] Step[900/2713]: training loss : 1.0026646995544433 TRAIN  loss dict:  {'classification_loss': 1.0026646995544433}
2025-01-16 14:08:10,467 [INFO] Step[950/2713]: training loss : 1.0338503563404082 TRAIN  loss dict:  {'classification_loss': 1.0338503563404082}
2025-01-16 14:08:26,475 [INFO] Step[1000/2713]: training loss : 1.012335580587387 TRAIN  loss dict:  {'classification_loss': 1.012335580587387}
2025-01-16 14:08:42,550 [INFO] Step[1050/2713]: training loss : 1.097400690317154 TRAIN  loss dict:  {'classification_loss': 1.097400690317154}
2025-01-16 14:08:58,584 [INFO] Step[1100/2713]: training loss : 1.052225914001465 TRAIN  loss dict:  {'classification_loss': 1.052225914001465}
2025-01-16 14:09:14,651 [INFO] Step[1150/2713]: training loss : 1.025493609905243 TRAIN  loss dict:  {'classification_loss': 1.025493609905243}
2025-01-16 14:09:30,742 [INFO] Step[1200/2713]: training loss : 1.0309085392951964 TRAIN  loss dict:  {'classification_loss': 1.0309085392951964}
2025-01-16 14:09:46,869 [INFO] Step[1250/2713]: training loss : 1.046307829618454 TRAIN  loss dict:  {'classification_loss': 1.046307829618454}
2025-01-16 14:10:02,912 [INFO] Step[1300/2713]: training loss : 1.0270691001415253 TRAIN  loss dict:  {'classification_loss': 1.0270691001415253}
2025-01-16 14:10:19,047 [INFO] Step[1350/2713]: training loss : 1.0197376608848572 TRAIN  loss dict:  {'classification_loss': 1.0197376608848572}
2025-01-16 14:10:35,188 [INFO] Step[1400/2713]: training loss : 1.021992839574814 TRAIN  loss dict:  {'classification_loss': 1.021992839574814}
2025-01-16 14:10:51,272 [INFO] Step[1450/2713]: training loss : 1.0770992457866668 TRAIN  loss dict:  {'classification_loss': 1.0770992457866668}
2025-01-16 14:11:07,406 [INFO] Step[1500/2713]: training loss : 1.0985108709335327 TRAIN  loss dict:  {'classification_loss': 1.0985108709335327}
2025-01-16 14:11:23,519 [INFO] Step[1550/2713]: training loss : 1.0311689853668213 TRAIN  loss dict:  {'classification_loss': 1.0311689853668213}
2025-01-16 14:11:39,538 [INFO] Step[1600/2713]: training loss : 1.015313971042633 TRAIN  loss dict:  {'classification_loss': 1.015313971042633}
2025-01-16 14:11:55,647 [INFO] Step[1650/2713]: training loss : 1.0713530373573303 TRAIN  loss dict:  {'classification_loss': 1.0713530373573303}
2025-01-16 14:12:11,716 [INFO] Step[1700/2713]: training loss : 1.0505747509002685 TRAIN  loss dict:  {'classification_loss': 1.0505747509002685}
2025-01-16 14:12:27,793 [INFO] Step[1750/2713]: training loss : 1.1318605875968932 TRAIN  loss dict:  {'classification_loss': 1.1318605875968932}
2025-01-16 14:12:43,919 [INFO] Step[1800/2713]: training loss : 1.0740338921546937 TRAIN  loss dict:  {'classification_loss': 1.0740338921546937}
2025-01-16 14:13:00,021 [INFO] Step[1850/2713]: training loss : 1.0173160338401794 TRAIN  loss dict:  {'classification_loss': 1.0173160338401794}
2025-01-16 14:13:16,057 [INFO] Step[1900/2713]: training loss : 1.140303417444229 TRAIN  loss dict:  {'classification_loss': 1.140303417444229}
2025-01-16 14:13:32,127 [INFO] Step[1950/2713]: training loss : 1.0739309227466582 TRAIN  loss dict:  {'classification_loss': 1.0739309227466582}
2025-01-16 14:13:48,151 [INFO] Step[2000/2713]: training loss : 1.0223346638679505 TRAIN  loss dict:  {'classification_loss': 1.0223346638679505}
2025-01-16 14:14:04,219 [INFO] Step[2050/2713]: training loss : 1.1462238430976868 TRAIN  loss dict:  {'classification_loss': 1.1462238430976868}
2025-01-16 14:14:20,244 [INFO] Step[2100/2713]: training loss : 1.0301104974746704 TRAIN  loss dict:  {'classification_loss': 1.0301104974746704}
2025-01-16 14:14:36,365 [INFO] Step[2150/2713]: training loss : 1.0684109795093537 TRAIN  loss dict:  {'classification_loss': 1.0684109795093537}
2025-01-16 14:14:52,460 [INFO] Step[2200/2713]: training loss : 1.065083807706833 TRAIN  loss dict:  {'classification_loss': 1.065083807706833}
2025-01-16 14:15:08,470 [INFO] Step[2250/2713]: training loss : 1.101683669090271 TRAIN  loss dict:  {'classification_loss': 1.101683669090271}
2025-01-16 14:15:24,520 [INFO] Step[2300/2713]: training loss : 1.0357636260986327 TRAIN  loss dict:  {'classification_loss': 1.0357636260986327}
2025-01-16 14:15:40,606 [INFO] Step[2350/2713]: training loss : 1.0464469480514527 TRAIN  loss dict:  {'classification_loss': 1.0464469480514527}
2025-01-16 14:15:56,655 [INFO] Step[2400/2713]: training loss : 1.0550889945030213 TRAIN  loss dict:  {'classification_loss': 1.0550889945030213}
2025-01-16 14:16:12,722 [INFO] Step[2450/2713]: training loss : 1.0488866031169892 TRAIN  loss dict:  {'classification_loss': 1.0488866031169892}
2025-01-16 14:16:28,752 [INFO] Step[2500/2713]: training loss : 1.0969934964179993 TRAIN  loss dict:  {'classification_loss': 1.0969934964179993}
2025-01-16 14:16:44,953 [INFO] Step[2550/2713]: training loss : 1.0667987096309661 TRAIN  loss dict:  {'classification_loss': 1.0667987096309661}
2025-01-16 14:17:00,962 [INFO] Step[2600/2713]: training loss : 1.0460731720924377 TRAIN  loss dict:  {'classification_loss': 1.0460731720924377}
2025-01-16 14:17:17,072 [INFO] Step[2650/2713]: training loss : 1.0888960039615632 TRAIN  loss dict:  {'classification_loss': 1.0888960039615632}
2025-01-16 14:17:33,031 [INFO] Step[2700/2713]: training loss : 1.093115609884262 TRAIN  loss dict:  {'classification_loss': 1.093115609884262}
2025-01-16 14:18:51,566 [INFO] Label accuracies statistics:
2025-01-16 14:18:51,566 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 0.75, 142: 1.0, 143: 0.25, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.25, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.5, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 1.0, 220: 0.5, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 1.0, 229: 0.0, 230: 1.0, 231: 0.5, 232: 0.75, 233: 0.5, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.5, 260: 0.75, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.5, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 0.75, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.0, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 0.75, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.5, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 1.0, 378: 0.5, 379: 1.0, 380: 0.75, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.0, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 14:18:51,567 [INFO] [15] TRAIN  loss: 1.0542608236985325 acc: 0.9740754392431502
2025-01-16 14:18:51,567 [INFO] [15] TRAIN  loss dict: {'classification_loss': 1.0542608236985325}
2025-01-16 14:18:51,568 [INFO] [15] VALIDATION loss: 1.8808045740190305 VALIDATION acc: 0.768025078369906
2025-01-16 14:18:51,568 [INFO] [15] VALIDATION loss dict: {'classification_loss': 1.8808045740190305}
2025-01-16 14:18:51,568 [INFO] 
2025-01-16 14:19:12,721 [INFO] Step[50/2713]: training loss : 0.9983959484100342 TRAIN  loss dict:  {'classification_loss': 0.9983959484100342}
2025-01-16 14:19:28,841 [INFO] Step[100/2713]: training loss : 1.038029043674469 TRAIN  loss dict:  {'classification_loss': 1.038029043674469}
2025-01-16 14:19:45,034 [INFO] Step[150/2713]: training loss : 1.041086996793747 TRAIN  loss dict:  {'classification_loss': 1.041086996793747}
2025-01-16 14:20:01,195 [INFO] Step[200/2713]: training loss : 1.018798404932022 TRAIN  loss dict:  {'classification_loss': 1.018798404932022}
2025-01-16 14:20:17,380 [INFO] Step[250/2713]: training loss : 1.0120636188983918 TRAIN  loss dict:  {'classification_loss': 1.0120636188983918}
2025-01-16 14:20:33,592 [INFO] Step[300/2713]: training loss : 1.0708578753471374 TRAIN  loss dict:  {'classification_loss': 1.0708578753471374}
2025-01-16 14:20:49,845 [INFO] Step[350/2713]: training loss : 1.090560680627823 TRAIN  loss dict:  {'classification_loss': 1.090560680627823}
2025-01-16 14:21:06,003 [INFO] Step[400/2713]: training loss : 1.0555903017520905 TRAIN  loss dict:  {'classification_loss': 1.0555903017520905}
2025-01-16 14:21:22,191 [INFO] Step[450/2713]: training loss : 1.0932291996479035 TRAIN  loss dict:  {'classification_loss': 1.0932291996479035}
2025-01-16 14:21:38,410 [INFO] Step[500/2713]: training loss : 1.0720797669887543 TRAIN  loss dict:  {'classification_loss': 1.0720797669887543}
2025-01-16 14:21:54,533 [INFO] Step[550/2713]: training loss : 1.1007580184936523 TRAIN  loss dict:  {'classification_loss': 1.1007580184936523}
2025-01-16 14:22:10,694 [INFO] Step[600/2713]: training loss : 1.0140149354934693 TRAIN  loss dict:  {'classification_loss': 1.0140149354934693}
2025-01-16 14:22:27,007 [INFO] Step[650/2713]: training loss : 1.0377381682395934 TRAIN  loss dict:  {'classification_loss': 1.0377381682395934}
2025-01-16 14:22:43,162 [INFO] Step[700/2713]: training loss : 1.0814841055870057 TRAIN  loss dict:  {'classification_loss': 1.0814841055870057}
2025-01-16 14:22:59,377 [INFO] Step[750/2713]: training loss : 1.0258058607578278 TRAIN  loss dict:  {'classification_loss': 1.0258058607578278}
2025-01-16 14:23:15,497 [INFO] Step[800/2713]: training loss : 1.0188322162628174 TRAIN  loss dict:  {'classification_loss': 1.0188322162628174}
2025-01-16 14:23:31,721 [INFO] Step[850/2713]: training loss : 0.9772943878173828 TRAIN  loss dict:  {'classification_loss': 0.9772943878173828}
2025-01-16 14:23:47,859 [INFO] Step[900/2713]: training loss : 1.0868819510936738 TRAIN  loss dict:  {'classification_loss': 1.0868819510936738}
2025-01-16 14:24:04,073 [INFO] Step[950/2713]: training loss : 1.0435613155364991 TRAIN  loss dict:  {'classification_loss': 1.0435613155364991}
2025-01-16 14:24:20,197 [INFO] Step[1000/2713]: training loss : 1.0023954951763152 TRAIN  loss dict:  {'classification_loss': 1.0023954951763152}
2025-01-16 14:24:36,276 [INFO] Step[1050/2713]: training loss : 1.013457088470459 TRAIN  loss dict:  {'classification_loss': 1.013457088470459}
2025-01-16 14:24:52,414 [INFO] Step[1100/2713]: training loss : 1.0674890768527985 TRAIN  loss dict:  {'classification_loss': 1.0674890768527985}
2025-01-16 14:25:08,647 [INFO] Step[1150/2713]: training loss : 1.0739610052108766 TRAIN  loss dict:  {'classification_loss': 1.0739610052108766}
2025-01-16 14:25:24,882 [INFO] Step[1200/2713]: training loss : 1.0476265335083008 TRAIN  loss dict:  {'classification_loss': 1.0476265335083008}
2025-01-16 14:25:41,089 [INFO] Step[1250/2713]: training loss : 1.0595131778717042 TRAIN  loss dict:  {'classification_loss': 1.0595131778717042}
2025-01-16 14:25:57,255 [INFO] Step[1300/2713]: training loss : 1.0315349340438842 TRAIN  loss dict:  {'classification_loss': 1.0315349340438842}
2025-01-16 14:26:13,424 [INFO] Step[1350/2713]: training loss : 0.9895678746700287 TRAIN  loss dict:  {'classification_loss': 0.9895678746700287}
2025-01-16 14:26:29,642 [INFO] Step[1400/2713]: training loss : 1.0599059760570526 TRAIN  loss dict:  {'classification_loss': 1.0599059760570526}
2025-01-16 14:26:45,819 [INFO] Step[1450/2713]: training loss : 1.0456559407711028 TRAIN  loss dict:  {'classification_loss': 1.0456559407711028}
2025-01-16 14:27:01,942 [INFO] Step[1500/2713]: training loss : 0.9968103241920471 TRAIN  loss dict:  {'classification_loss': 0.9968103241920471}
2025-01-16 14:27:18,157 [INFO] Step[1550/2713]: training loss : 1.0797631847858429 TRAIN  loss dict:  {'classification_loss': 1.0797631847858429}
2025-01-16 14:27:34,293 [INFO] Step[1600/2713]: training loss : 1.055926694869995 TRAIN  loss dict:  {'classification_loss': 1.055926694869995}
2025-01-16 14:27:50,454 [INFO] Step[1650/2713]: training loss : 1.09106672167778 TRAIN  loss dict:  {'classification_loss': 1.09106672167778}
2025-01-16 14:28:06,524 [INFO] Step[1700/2713]: training loss : 1.0784749007225036 TRAIN  loss dict:  {'classification_loss': 1.0784749007225036}
2025-01-16 14:28:22,687 [INFO] Step[1750/2713]: training loss : 1.0368169796466828 TRAIN  loss dict:  {'classification_loss': 1.0368169796466828}
2025-01-16 14:28:38,888 [INFO] Step[1800/2713]: training loss : 1.0461674082279204 TRAIN  loss dict:  {'classification_loss': 1.0461674082279204}
2025-01-16 14:28:55,008 [INFO] Step[1850/2713]: training loss : 1.0325469374656677 TRAIN  loss dict:  {'classification_loss': 1.0325469374656677}
2025-01-16 14:29:11,169 [INFO] Step[1900/2713]: training loss : 1.0242870318889619 TRAIN  loss dict:  {'classification_loss': 1.0242870318889619}
2025-01-16 14:29:27,395 [INFO] Step[1950/2713]: training loss : 1.0827237749099732 TRAIN  loss dict:  {'classification_loss': 1.0827237749099732}
2025-01-16 14:29:43,555 [INFO] Step[2000/2713]: training loss : 1.0340554463863372 TRAIN  loss dict:  {'classification_loss': 1.0340554463863372}
2025-01-16 14:29:59,711 [INFO] Step[2050/2713]: training loss : 1.0644073295593262 TRAIN  loss dict:  {'classification_loss': 1.0644073295593262}
2025-01-16 14:30:15,892 [INFO] Step[2100/2713]: training loss : 1.0909929084777832 TRAIN  loss dict:  {'classification_loss': 1.0909929084777832}
2025-01-16 14:30:32,046 [INFO] Step[2150/2713]: training loss : 1.044397293329239 TRAIN  loss dict:  {'classification_loss': 1.044397293329239}
2025-01-16 14:30:48,235 [INFO] Step[2200/2713]: training loss : 1.023232445716858 TRAIN  loss dict:  {'classification_loss': 1.023232445716858}
2025-01-16 14:31:04,432 [INFO] Step[2250/2713]: training loss : 1.070742552280426 TRAIN  loss dict:  {'classification_loss': 1.070742552280426}
2025-01-16 14:31:20,667 [INFO] Step[2300/2713]: training loss : 1.0808357405662536 TRAIN  loss dict:  {'classification_loss': 1.0808357405662536}
2025-01-16 14:31:36,844 [INFO] Step[2350/2713]: training loss : 1.0202066922187805 TRAIN  loss dict:  {'classification_loss': 1.0202066922187805}
2025-01-16 14:31:52,957 [INFO] Step[2400/2713]: training loss : 1.0563238203525542 TRAIN  loss dict:  {'classification_loss': 1.0563238203525542}
2025-01-16 14:32:09,148 [INFO] Step[2450/2713]: training loss : 1.029856058359146 TRAIN  loss dict:  {'classification_loss': 1.029856058359146}
2025-01-16 14:32:25,307 [INFO] Step[2500/2713]: training loss : 1.0291077136993407 TRAIN  loss dict:  {'classification_loss': 1.0291077136993407}
2025-01-16 14:32:41,576 [INFO] Step[2550/2713]: training loss : 1.0583130717277527 TRAIN  loss dict:  {'classification_loss': 1.0583130717277527}
2025-01-16 14:32:57,668 [INFO] Step[2600/2713]: training loss : 1.0110955083370208 TRAIN  loss dict:  {'classification_loss': 1.0110955083370208}
2025-01-16 14:33:13,804 [INFO] Step[2650/2713]: training loss : 1.0290789413452148 TRAIN  loss dict:  {'classification_loss': 1.0290789413452148}
2025-01-16 14:33:30,007 [INFO] Step[2700/2713]: training loss : 1.084188107252121 TRAIN  loss dict:  {'classification_loss': 1.084188107252121}
2025-01-16 14:34:48,750 [INFO] Label accuracies statistics:
2025-01-16 14:34:48,750 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 1.0, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 0.75, 138: 0.75, 139: 1.0, 140: 0.5, 141: 0.75, 142: 0.75, 143: 0.75, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.5, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 0.75, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.5, 229: 0.0, 230: 0.75, 231: 0.75, 232: 0.75, 233: 0.5, 234: 1.0, 235: 0.75, 236: 1.0, 237: 0.75, 238: 1.0, 239: 0.5, 240: 0.75, 241: 0.75, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.5, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.25, 257: 1.0, 258: 0.25, 259: 1.0, 260: 1.0, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.75, 278: 1.0, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.75, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.0, 334: 0.75, 335: 0.25, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.25, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 1.0, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 0.75, 387: 1.0, 388: 0.75, 389: 0.5, 390: 0.5, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.5, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 14:35:30,318 [INFO] [16] TRAIN  loss: 1.0466963049454132 acc: 0.977147069664578
2025-01-16 14:35:30,318 [INFO] [16] TRAIN  loss dict: {'classification_loss': 1.0466963049454132}
2025-01-16 14:35:30,318 [INFO] [16] VALIDATION loss: 1.8194042801632917 VALIDATION acc: 0.7811912225705329
2025-01-16 14:35:30,318 [INFO] [16] VALIDATION loss dict: {'classification_loss': 1.8194042801632917}
2025-01-16 14:35:30,319 [INFO] 
2025-01-16 14:35:51,133 [INFO] Step[50/2713]: training loss : 1.044145770072937 TRAIN  loss dict:  {'classification_loss': 1.044145770072937}
2025-01-16 14:36:07,244 [INFO] Step[100/2713]: training loss : 1.0594327127933503 TRAIN  loss dict:  {'classification_loss': 1.0594327127933503}
2025-01-16 14:36:23,481 [INFO] Step[150/2713]: training loss : 1.0360304605960846 TRAIN  loss dict:  {'classification_loss': 1.0360304605960846}
2025-01-16 14:36:39,664 [INFO] Step[200/2713]: training loss : 1.0484684205055237 TRAIN  loss dict:  {'classification_loss': 1.0484684205055237}
2025-01-16 14:36:55,911 [INFO] Step[250/2713]: training loss : 1.086216825246811 TRAIN  loss dict:  {'classification_loss': 1.086216825246811}
2025-01-16 14:37:12,073 [INFO] Step[300/2713]: training loss : 1.0239512097835541 TRAIN  loss dict:  {'classification_loss': 1.0239512097835541}
2025-01-16 14:37:28,233 [INFO] Step[350/2713]: training loss : 1.0379225254058837 TRAIN  loss dict:  {'classification_loss': 1.0379225254058837}
2025-01-16 14:37:44,408 [INFO] Step[400/2713]: training loss : 1.042023321390152 TRAIN  loss dict:  {'classification_loss': 1.042023321390152}
2025-01-16 14:38:00,628 [INFO] Step[450/2713]: training loss : 1.0749045407772064 TRAIN  loss dict:  {'classification_loss': 1.0749045407772064}
2025-01-16 14:38:16,774 [INFO] Step[500/2713]: training loss : 1.018607920408249 TRAIN  loss dict:  {'classification_loss': 1.018607920408249}
2025-01-16 14:38:32,980 [INFO] Step[550/2713]: training loss : 1.003471964597702 TRAIN  loss dict:  {'classification_loss': 1.003471964597702}
2025-01-16 14:38:49,126 [INFO] Step[600/2713]: training loss : 1.1048390114307403 TRAIN  loss dict:  {'classification_loss': 1.1048390114307403}
2025-01-16 14:39:05,300 [INFO] Step[650/2713]: training loss : 1.0106950831413268 TRAIN  loss dict:  {'classification_loss': 1.0106950831413268}
2025-01-16 14:39:21,418 [INFO] Step[700/2713]: training loss : 1.0098019886016845 TRAIN  loss dict:  {'classification_loss': 1.0098019886016845}
2025-01-16 14:39:37,638 [INFO] Step[750/2713]: training loss : 1.0607485938072205 TRAIN  loss dict:  {'classification_loss': 1.0607485938072205}
2025-01-16 14:39:53,880 [INFO] Step[800/2713]: training loss : 1.0717030775547027 TRAIN  loss dict:  {'classification_loss': 1.0717030775547027}
2025-01-16 14:40:10,035 [INFO] Step[850/2713]: training loss : 1.0312612390518188 TRAIN  loss dict:  {'classification_loss': 1.0312612390518188}
2025-01-16 14:40:26,174 [INFO] Step[900/2713]: training loss : 1.0256351518630982 TRAIN  loss dict:  {'classification_loss': 1.0256351518630982}
2025-01-16 14:40:42,352 [INFO] Step[950/2713]: training loss : 0.9972772407531738 TRAIN  loss dict:  {'classification_loss': 0.9972772407531738}
2025-01-16 14:40:58,542 [INFO] Step[1000/2713]: training loss : 0.9924170637130737 TRAIN  loss dict:  {'classification_loss': 0.9924170637130737}
2025-01-16 14:41:14,754 [INFO] Step[1050/2713]: training loss : 1.0316762232780456 TRAIN  loss dict:  {'classification_loss': 1.0316762232780456}
2025-01-16 14:41:30,889 [INFO] Step[1100/2713]: training loss : 1.0593902087211609 TRAIN  loss dict:  {'classification_loss': 1.0593902087211609}
2025-01-16 14:41:47,063 [INFO] Step[1150/2713]: training loss : 1.0481536531448363 TRAIN  loss dict:  {'classification_loss': 1.0481536531448363}
2025-01-16 14:42:03,300 [INFO] Step[1200/2713]: training loss : 1.06437558054924 TRAIN  loss dict:  {'classification_loss': 1.06437558054924}
2025-01-16 14:42:19,526 [INFO] Step[1250/2713]: training loss : 1.0048890042304992 TRAIN  loss dict:  {'classification_loss': 1.0048890042304992}
2025-01-16 14:42:35,695 [INFO] Step[1300/2713]: training loss : 1.0467526698112488 TRAIN  loss dict:  {'classification_loss': 1.0467526698112488}
2025-01-16 14:42:51,938 [INFO] Step[1350/2713]: training loss : 0.9998839664459228 TRAIN  loss dict:  {'classification_loss': 0.9998839664459228}
2025-01-16 14:43:08,082 [INFO] Step[1400/2713]: training loss : 1.1148362421989442 TRAIN  loss dict:  {'classification_loss': 1.1148362421989442}
2025-01-16 14:43:24,215 [INFO] Step[1450/2713]: training loss : 1.0278562784194947 TRAIN  loss dict:  {'classification_loss': 1.0278562784194947}
2025-01-16 14:43:40,374 [INFO] Step[1500/2713]: training loss : 1.0028606009483338 TRAIN  loss dict:  {'classification_loss': 1.0028606009483338}
2025-01-16 14:43:56,646 [INFO] Step[1550/2713]: training loss : 1.0211332106590272 TRAIN  loss dict:  {'classification_loss': 1.0211332106590272}
2025-01-16 14:44:12,815 [INFO] Step[1600/2713]: training loss : 1.0451819610595703 TRAIN  loss dict:  {'classification_loss': 1.0451819610595703}
2025-01-16 14:44:29,002 [INFO] Step[1650/2713]: training loss : 1.025659133195877 TRAIN  loss dict:  {'classification_loss': 1.025659133195877}
2025-01-16 14:44:45,218 [INFO] Step[1700/2713]: training loss : 1.038590681552887 TRAIN  loss dict:  {'classification_loss': 1.038590681552887}
2025-01-16 14:45:01,407 [INFO] Step[1750/2713]: training loss : 0.9998566317558288 TRAIN  loss dict:  {'classification_loss': 0.9998566317558288}
2025-01-16 14:45:17,661 [INFO] Step[1800/2713]: training loss : 1.030109771490097 TRAIN  loss dict:  {'classification_loss': 1.030109771490097}
2025-01-16 14:45:33,848 [INFO] Step[1850/2713]: training loss : 1.0613839268684386 TRAIN  loss dict:  {'classification_loss': 1.0613839268684386}
2025-01-16 14:45:50,021 [INFO] Step[1900/2713]: training loss : 1.0334574472904205 TRAIN  loss dict:  {'classification_loss': 1.0334574472904205}
2025-01-16 14:46:06,208 [INFO] Step[1950/2713]: training loss : 1.029359475374222 TRAIN  loss dict:  {'classification_loss': 1.029359475374222}
2025-01-16 14:46:22,422 [INFO] Step[2000/2713]: training loss : 1.0266306853294374 TRAIN  loss dict:  {'classification_loss': 1.0266306853294374}
2025-01-16 14:46:38,679 [INFO] Step[2050/2713]: training loss : 1.0688248133659364 TRAIN  loss dict:  {'classification_loss': 1.0688248133659364}
2025-01-16 14:46:54,854 [INFO] Step[2100/2713]: training loss : 1.0894972479343414 TRAIN  loss dict:  {'classification_loss': 1.0894972479343414}
2025-01-16 14:47:11,014 [INFO] Step[2150/2713]: training loss : 1.079830777645111 TRAIN  loss dict:  {'classification_loss': 1.079830777645111}
2025-01-16 14:47:27,189 [INFO] Step[2200/2713]: training loss : 1.00986790060997 TRAIN  loss dict:  {'classification_loss': 1.00986790060997}
2025-01-16 14:47:43,294 [INFO] Step[2250/2713]: training loss : 1.1143944227695466 TRAIN  loss dict:  {'classification_loss': 1.1143944227695466}
2025-01-16 14:47:59,417 [INFO] Step[2300/2713]: training loss : 1.058302572965622 TRAIN  loss dict:  {'classification_loss': 1.058302572965622}
2025-01-16 14:48:15,662 [INFO] Step[2350/2713]: training loss : 1.0489396274089813 TRAIN  loss dict:  {'classification_loss': 1.0489396274089813}
2025-01-16 14:48:31,834 [INFO] Step[2400/2713]: training loss : 1.0682010805606843 TRAIN  loss dict:  {'classification_loss': 1.0682010805606843}
2025-01-16 14:48:48,002 [INFO] Step[2450/2713]: training loss : 1.0957764792442322 TRAIN  loss dict:  {'classification_loss': 1.0957764792442322}
2025-01-16 14:49:04,220 [INFO] Step[2500/2713]: training loss : 1.082085553407669 TRAIN  loss dict:  {'classification_loss': 1.082085553407669}
2025-01-16 14:49:20,422 [INFO] Step[2550/2713]: training loss : 1.010815178155899 TRAIN  loss dict:  {'classification_loss': 1.010815178155899}
2025-01-16 14:49:36,571 [INFO] Step[2600/2713]: training loss : 1.119897516965866 TRAIN  loss dict:  {'classification_loss': 1.119897516965866}
2025-01-16 14:49:52,725 [INFO] Step[2650/2713]: training loss : 1.013880317211151 TRAIN  loss dict:  {'classification_loss': 1.013880317211151}
2025-01-16 14:50:08,824 [INFO] Step[2700/2713]: training loss : 1.0107853507995606 TRAIN  loss dict:  {'classification_loss': 1.0107853507995606}
2025-01-16 14:51:27,134 [INFO] Label accuracies statistics:
2025-01-16 14:51:27,135 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 1.0, 61: 0.5, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 1.0, 84: 0.75, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.5, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 0.75, 130: 0.5, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 1.0, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.5, 254: 0.75, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.75, 261: 0.5, 262: 0.75, 263: 1.0, 264: 0.75, 265: 0.5, 266: 0.5, 267: 1.0, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.5, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.5, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.25, 351: 0.75, 352: 0.25, 353: 0.5, 354: 0.75, 355: 0.75, 356: 1.0, 357: 0.75, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.5, 391: 1.0, 392: 1.0, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 14:51:28,564 [INFO] [17] TRAIN  loss: 1.0435293351334045 acc: 0.9769013392308638
2025-01-16 14:51:28,564 [INFO] [17] TRAIN  loss dict: {'classification_loss': 1.0435293351334045}
2025-01-16 14:51:28,564 [INFO] [17] VALIDATION loss: 1.8433133258407277 VALIDATION acc: 0.7855799373040753
2025-01-16 14:51:28,564 [INFO] [17] VALIDATION loss dict: {'classification_loss': 1.8433133258407277}
2025-01-16 14:51:28,565 [INFO] 
2025-01-16 14:51:49,031 [INFO] Step[50/2713]: training loss : 0.9971460103988647 TRAIN  loss dict:  {'classification_loss': 0.9971460103988647}
2025-01-16 14:52:05,273 [INFO] Step[100/2713]: training loss : 1.0534070122241974 TRAIN  loss dict:  {'classification_loss': 1.0534070122241974}
2025-01-16 14:52:21,506 [INFO] Step[150/2713]: training loss : 1.0255091536045073 TRAIN  loss dict:  {'classification_loss': 1.0255091536045073}
2025-01-16 14:52:37,683 [INFO] Step[200/2713]: training loss : 1.0452965247631072 TRAIN  loss dict:  {'classification_loss': 1.0452965247631072}
2025-01-16 14:52:53,904 [INFO] Step[250/2713]: training loss : 1.0210389912128448 TRAIN  loss dict:  {'classification_loss': 1.0210389912128448}
2025-01-16 14:53:10,167 [INFO] Step[300/2713]: training loss : 0.9928423726558685 TRAIN  loss dict:  {'classification_loss': 0.9928423726558685}
2025-01-16 14:53:26,410 [INFO] Step[350/2713]: training loss : 1.0085284149646758 TRAIN  loss dict:  {'classification_loss': 1.0085284149646758}
2025-01-16 14:53:42,639 [INFO] Step[400/2713]: training loss : 0.9986791086196899 TRAIN  loss dict:  {'classification_loss': 0.9986791086196899}
2025-01-16 14:53:58,899 [INFO] Step[450/2713]: training loss : 1.0569167375564574 TRAIN  loss dict:  {'classification_loss': 1.0569167375564574}
2025-01-16 14:54:15,069 [INFO] Step[500/2713]: training loss : 1.0121815156936647 TRAIN  loss dict:  {'classification_loss': 1.0121815156936647}
2025-01-16 14:54:31,259 [INFO] Step[550/2713]: training loss : 0.9980068480968476 TRAIN  loss dict:  {'classification_loss': 0.9980068480968476}
2025-01-16 14:54:47,462 [INFO] Step[600/2713]: training loss : 1.029354875087738 TRAIN  loss dict:  {'classification_loss': 1.029354875087738}
2025-01-16 14:55:03,704 [INFO] Step[650/2713]: training loss : 1.045590569972992 TRAIN  loss dict:  {'classification_loss': 1.045590569972992}
2025-01-16 14:55:19,890 [INFO] Step[700/2713]: training loss : 1.0507671821117401 TRAIN  loss dict:  {'classification_loss': 1.0507671821117401}
2025-01-16 14:55:36,144 [INFO] Step[750/2713]: training loss : 1.0255707669258118 TRAIN  loss dict:  {'classification_loss': 1.0255707669258118}
2025-01-16 14:55:52,284 [INFO] Step[800/2713]: training loss : 0.9971527481079101 TRAIN  loss dict:  {'classification_loss': 0.9971527481079101}
2025-01-16 14:56:08,473 [INFO] Step[850/2713]: training loss : 1.0029430735111236 TRAIN  loss dict:  {'classification_loss': 1.0029430735111236}
2025-01-16 14:56:24,742 [INFO] Step[900/2713]: training loss : 1.020938982963562 TRAIN  loss dict:  {'classification_loss': 1.020938982963562}
2025-01-16 14:56:40,903 [INFO] Step[950/2713]: training loss : 1.0845373952388764 TRAIN  loss dict:  {'classification_loss': 1.0845373952388764}
2025-01-16 14:56:56,961 [INFO] Step[1000/2713]: training loss : 1.024251035451889 TRAIN  loss dict:  {'classification_loss': 1.024251035451889}
2025-01-16 14:57:13,191 [INFO] Step[1050/2713]: training loss : 1.0276181983947754 TRAIN  loss dict:  {'classification_loss': 1.0276181983947754}
2025-01-16 14:57:29,327 [INFO] Step[1100/2713]: training loss : 1.0999080979824065 TRAIN  loss dict:  {'classification_loss': 1.0999080979824065}
2025-01-16 14:57:45,480 [INFO] Step[1150/2713]: training loss : 1.0483990263938905 TRAIN  loss dict:  {'classification_loss': 1.0483990263938905}
2025-01-16 14:58:01,666 [INFO] Step[1200/2713]: training loss : 1.0024320030212401 TRAIN  loss dict:  {'classification_loss': 1.0024320030212401}
2025-01-16 14:58:17,945 [INFO] Step[1250/2713]: training loss : 1.0187106800079346 TRAIN  loss dict:  {'classification_loss': 1.0187106800079346}
2025-01-16 14:58:34,127 [INFO] Step[1300/2713]: training loss : 1.0747626638412475 TRAIN  loss dict:  {'classification_loss': 1.0747626638412475}
2025-01-16 14:58:50,349 [INFO] Step[1350/2713]: training loss : 1.046053981781006 TRAIN  loss dict:  {'classification_loss': 1.046053981781006}
2025-01-16 14:59:06,522 [INFO] Step[1400/2713]: training loss : 0.9870575404167176 TRAIN  loss dict:  {'classification_loss': 0.9870575404167176}
2025-01-16 14:59:22,761 [INFO] Step[1450/2713]: training loss : 0.9986390459537506 TRAIN  loss dict:  {'classification_loss': 0.9986390459537506}
2025-01-16 14:59:38,823 [INFO] Step[1500/2713]: training loss : 1.0088357424736023 TRAIN  loss dict:  {'classification_loss': 1.0088357424736023}
2025-01-16 14:59:55,015 [INFO] Step[1550/2713]: training loss : 1.1257649445533753 TRAIN  loss dict:  {'classification_loss': 1.1257649445533753}
2025-01-16 15:00:11,180 [INFO] Step[1600/2713]: training loss : 0.9900454533100128 TRAIN  loss dict:  {'classification_loss': 0.9900454533100128}
2025-01-16 15:00:27,372 [INFO] Step[1650/2713]: training loss : 1.0239601504802704 TRAIN  loss dict:  {'classification_loss': 1.0239601504802704}
2025-01-16 15:00:43,496 [INFO] Step[1700/2713]: training loss : 1.001181720495224 TRAIN  loss dict:  {'classification_loss': 1.001181720495224}
2025-01-16 15:00:59,659 [INFO] Step[1750/2713]: training loss : 1.0171223282814026 TRAIN  loss dict:  {'classification_loss': 1.0171223282814026}
2025-01-16 15:01:15,767 [INFO] Step[1800/2713]: training loss : 1.0333138585090638 TRAIN  loss dict:  {'classification_loss': 1.0333138585090638}
2025-01-16 15:01:31,993 [INFO] Step[1850/2713]: training loss : 1.0424452304840088 TRAIN  loss dict:  {'classification_loss': 1.0424452304840088}
2025-01-16 15:01:48,105 [INFO] Step[1900/2713]: training loss : 1.0433025395870208 TRAIN  loss dict:  {'classification_loss': 1.0433025395870208}
2025-01-16 15:02:04,239 [INFO] Step[1950/2713]: training loss : 1.00044078707695 TRAIN  loss dict:  {'classification_loss': 1.00044078707695}
2025-01-16 15:02:20,325 [INFO] Step[2000/2713]: training loss : 1.0048996019363403 TRAIN  loss dict:  {'classification_loss': 1.0048996019363403}
2025-01-16 15:02:36,456 [INFO] Step[2050/2713]: training loss : 1.055248841047287 TRAIN  loss dict:  {'classification_loss': 1.055248841047287}
2025-01-16 15:02:52,632 [INFO] Step[2100/2713]: training loss : 0.9994725620746613 TRAIN  loss dict:  {'classification_loss': 0.9994725620746613}
2025-01-16 15:03:08,769 [INFO] Step[2150/2713]: training loss : 1.0041570127010346 TRAIN  loss dict:  {'classification_loss': 1.0041570127010346}
2025-01-16 15:03:24,911 [INFO] Step[2200/2713]: training loss : 0.9967579889297485 TRAIN  loss dict:  {'classification_loss': 0.9967579889297485}
2025-01-16 15:03:41,103 [INFO] Step[2250/2713]: training loss : 1.0345635676383973 TRAIN  loss dict:  {'classification_loss': 1.0345635676383973}
2025-01-16 15:03:57,257 [INFO] Step[2300/2713]: training loss : 1.0486919975280762 TRAIN  loss dict:  {'classification_loss': 1.0486919975280762}
2025-01-16 15:04:13,456 [INFO] Step[2350/2713]: training loss : 1.010770001411438 TRAIN  loss dict:  {'classification_loss': 1.010770001411438}
2025-01-16 15:04:29,600 [INFO] Step[2400/2713]: training loss : 1.006960175037384 TRAIN  loss dict:  {'classification_loss': 1.006960175037384}
2025-01-16 15:04:45,765 [INFO] Step[2450/2713]: training loss : 1.0295322024822235 TRAIN  loss dict:  {'classification_loss': 1.0295322024822235}
2025-01-16 15:05:01,906 [INFO] Step[2500/2713]: training loss : 1.058576114177704 TRAIN  loss dict:  {'classification_loss': 1.058576114177704}
2025-01-16 15:05:18,096 [INFO] Step[2550/2713]: training loss : 1.0171194684505462 TRAIN  loss dict:  {'classification_loss': 1.0171194684505462}
2025-01-16 15:05:34,270 [INFO] Step[2600/2713]: training loss : 1.0342170095443726 TRAIN  loss dict:  {'classification_loss': 1.0342170095443726}
2025-01-16 15:05:50,441 [INFO] Step[2650/2713]: training loss : 1.0494438636302947 TRAIN  loss dict:  {'classification_loss': 1.0494438636302947}
2025-01-16 15:06:06,644 [INFO] Step[2700/2713]: training loss : 0.9980707931518554 TRAIN  loss dict:  {'classification_loss': 0.9980707931518554}
2025-01-16 15:07:25,362 [INFO] Label accuracies statistics:
2025-01-16 15:07:25,363 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 0.5, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.5, 143: 0.75, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 0.75, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.0, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.75, 262: 1.0, 263: 0.75, 264: 0.75, 265: 0.5, 266: 1.0, 267: 0.75, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.5, 276: 0.5, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.5, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.5, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.0, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.5, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.5, 376: 1.0, 377: 1.0, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 15:07:25,365 [INFO] [18] TRAIN  loss: 1.0262453153350457 acc: 0.9814473522545767
2025-01-16 15:07:25,365 [INFO] [18] TRAIN  loss dict: {'classification_loss': 1.0262453153350457}
2025-01-16 15:07:25,365 [INFO] [18] VALIDATION loss: 1.826244026758617 VALIDATION acc: 0.7818181818181819
2025-01-16 15:07:25,365 [INFO] [18] VALIDATION loss dict: {'classification_loss': 1.826244026758617}
2025-01-16 15:07:25,365 [INFO] 
2025-01-16 15:07:46,423 [INFO] Step[50/2713]: training loss : 1.01868501663208 TRAIN  loss dict:  {'classification_loss': 1.01868501663208}
2025-01-16 15:08:02,549 [INFO] Step[100/2713]: training loss : 1.0105069172382355 TRAIN  loss dict:  {'classification_loss': 1.0105069172382355}
2025-01-16 15:08:18,709 [INFO] Step[150/2713]: training loss : 0.9758996951580048 TRAIN  loss dict:  {'classification_loss': 0.9758996951580048}
2025-01-16 15:08:34,793 [INFO] Step[200/2713]: training loss : 1.0001948010921478 TRAIN  loss dict:  {'classification_loss': 1.0001948010921478}
2025-01-16 15:08:50,934 [INFO] Step[250/2713]: training loss : 1.0784831190109252 TRAIN  loss dict:  {'classification_loss': 1.0784831190109252}
2025-01-16 15:09:07,061 [INFO] Step[300/2713]: training loss : 1.006878651380539 TRAIN  loss dict:  {'classification_loss': 1.006878651380539}
2025-01-16 15:09:23,177 [INFO] Step[350/2713]: training loss : 1.017949129343033 TRAIN  loss dict:  {'classification_loss': 1.017949129343033}
2025-01-16 15:09:39,279 [INFO] Step[400/2713]: training loss : 1.0644916105270386 TRAIN  loss dict:  {'classification_loss': 1.0644916105270386}
2025-01-16 15:09:55,441 [INFO] Step[450/2713]: training loss : 1.0239272809028626 TRAIN  loss dict:  {'classification_loss': 1.0239272809028626}
2025-01-16 15:10:11,600 [INFO] Step[500/2713]: training loss : 1.0158069372177123 TRAIN  loss dict:  {'classification_loss': 1.0158069372177123}
2025-01-16 15:10:27,819 [INFO] Step[550/2713]: training loss : 1.0220254933834076 TRAIN  loss dict:  {'classification_loss': 1.0220254933834076}
2025-01-16 15:10:43,890 [INFO] Step[600/2713]: training loss : 0.9943621325492858 TRAIN  loss dict:  {'classification_loss': 0.9943621325492858}
2025-01-16 15:11:00,053 [INFO] Step[650/2713]: training loss : 1.0151886010169984 TRAIN  loss dict:  {'classification_loss': 1.0151886010169984}
2025-01-16 15:11:16,180 [INFO] Step[700/2713]: training loss : 1.0624171853065492 TRAIN  loss dict:  {'classification_loss': 1.0624171853065492}
2025-01-16 15:11:32,360 [INFO] Step[750/2713]: training loss : 1.0314882576465607 TRAIN  loss dict:  {'classification_loss': 1.0314882576465607}
2025-01-16 15:11:48,402 [INFO] Step[800/2713]: training loss : 1.1068338942527771 TRAIN  loss dict:  {'classification_loss': 1.1068338942527771}
2025-01-16 15:12:04,526 [INFO] Step[850/2713]: training loss : 1.0038325572013855 TRAIN  loss dict:  {'classification_loss': 1.0038325572013855}
2025-01-16 15:12:20,677 [INFO] Step[900/2713]: training loss : 1.049060778617859 TRAIN  loss dict:  {'classification_loss': 1.049060778617859}
2025-01-16 15:12:36,869 [INFO] Step[950/2713]: training loss : 1.0428771698474884 TRAIN  loss dict:  {'classification_loss': 1.0428771698474884}
2025-01-16 15:12:53,014 [INFO] Step[1000/2713]: training loss : 1.0521296977996826 TRAIN  loss dict:  {'classification_loss': 1.0521296977996826}
2025-01-16 15:13:09,183 [INFO] Step[1050/2713]: training loss : 1.0200950396060944 TRAIN  loss dict:  {'classification_loss': 1.0200950396060944}
2025-01-16 15:13:25,368 [INFO] Step[1100/2713]: training loss : 1.02662548661232 TRAIN  loss dict:  {'classification_loss': 1.02662548661232}
2025-01-16 15:13:41,503 [INFO] Step[1150/2713]: training loss : 0.9871806645393372 TRAIN  loss dict:  {'classification_loss': 0.9871806645393372}
2025-01-16 15:13:57,640 [INFO] Step[1200/2713]: training loss : 1.047701634168625 TRAIN  loss dict:  {'classification_loss': 1.047701634168625}
2025-01-16 15:14:13,785 [INFO] Step[1250/2713]: training loss : 1.0338153612613679 TRAIN  loss dict:  {'classification_loss': 1.0338153612613679}
2025-01-16 15:14:29,909 [INFO] Step[1300/2713]: training loss : 1.0127848887443542 TRAIN  loss dict:  {'classification_loss': 1.0127848887443542}
2025-01-16 15:14:46,016 [INFO] Step[1350/2713]: training loss : 1.017658976316452 TRAIN  loss dict:  {'classification_loss': 1.017658976316452}
2025-01-16 15:15:02,108 [INFO] Step[1400/2713]: training loss : 1.0160525119304658 TRAIN  loss dict:  {'classification_loss': 1.0160525119304658}
2025-01-16 15:15:18,238 [INFO] Step[1450/2713]: training loss : 1.0239104187488557 TRAIN  loss dict:  {'classification_loss': 1.0239104187488557}
2025-01-16 15:15:34,317 [INFO] Step[1500/2713]: training loss : 1.0414612448215486 TRAIN  loss dict:  {'classification_loss': 1.0414612448215486}
2025-01-16 15:15:50,450 [INFO] Step[1550/2713]: training loss : 0.9948457860946656 TRAIN  loss dict:  {'classification_loss': 0.9948457860946656}
2025-01-16 15:16:06,578 [INFO] Step[1600/2713]: training loss : 1.0189752388000488 TRAIN  loss dict:  {'classification_loss': 1.0189752388000488}
2025-01-16 15:16:22,687 [INFO] Step[1650/2713]: training loss : 1.0006048607826232 TRAIN  loss dict:  {'classification_loss': 1.0006048607826232}
2025-01-16 15:16:38,836 [INFO] Step[1700/2713]: training loss : 1.043424414396286 TRAIN  loss dict:  {'classification_loss': 1.043424414396286}
2025-01-16 15:16:54,972 [INFO] Step[1750/2713]: training loss : 1.0117819607257843 TRAIN  loss dict:  {'classification_loss': 1.0117819607257843}
2025-01-16 15:17:11,044 [INFO] Step[1800/2713]: training loss : 1.0094402503967286 TRAIN  loss dict:  {'classification_loss': 1.0094402503967286}
2025-01-16 15:17:27,168 [INFO] Step[1850/2713]: training loss : 1.000917899608612 TRAIN  loss dict:  {'classification_loss': 1.000917899608612}
2025-01-16 15:17:43,319 [INFO] Step[1900/2713]: training loss : 0.9986823976039887 TRAIN  loss dict:  {'classification_loss': 0.9986823976039887}
2025-01-16 15:17:59,479 [INFO] Step[1950/2713]: training loss : 0.9965087461471558 TRAIN  loss dict:  {'classification_loss': 0.9965087461471558}
2025-01-16 15:18:15,585 [INFO] Step[2000/2713]: training loss : 1.0343765258789062 TRAIN  loss dict:  {'classification_loss': 1.0343765258789062}
2025-01-16 15:18:31,650 [INFO] Step[2050/2713]: training loss : 1.0401854264736174 TRAIN  loss dict:  {'classification_loss': 1.0401854264736174}
2025-01-16 15:18:47,781 [INFO] Step[2100/2713]: training loss : 1.0403441071510315 TRAIN  loss dict:  {'classification_loss': 1.0403441071510315}
2025-01-16 15:19:03,981 [INFO] Step[2150/2713]: training loss : 0.9982799065113067 TRAIN  loss dict:  {'classification_loss': 0.9982799065113067}
2025-01-16 15:19:20,097 [INFO] Step[2200/2713]: training loss : 1.0229792261123658 TRAIN  loss dict:  {'classification_loss': 1.0229792261123658}
2025-01-16 15:19:36,246 [INFO] Step[2250/2713]: training loss : 1.009157476425171 TRAIN  loss dict:  {'classification_loss': 1.009157476425171}
2025-01-16 15:19:52,359 [INFO] Step[2300/2713]: training loss : 1.0043413162231445 TRAIN  loss dict:  {'classification_loss': 1.0043413162231445}
2025-01-16 15:20:08,490 [INFO] Step[2350/2713]: training loss : 1.0199548757076264 TRAIN  loss dict:  {'classification_loss': 1.0199548757076264}
2025-01-16 15:20:24,599 [INFO] Step[2400/2713]: training loss : 1.125196280479431 TRAIN  loss dict:  {'classification_loss': 1.125196280479431}
2025-01-16 15:20:40,712 [INFO] Step[2450/2713]: training loss : 1.0395396876335143 TRAIN  loss dict:  {'classification_loss': 1.0395396876335143}
2025-01-16 15:20:56,858 [INFO] Step[2500/2713]: training loss : 1.0236626172065735 TRAIN  loss dict:  {'classification_loss': 1.0236626172065735}
2025-01-16 15:21:12,929 [INFO] Step[2550/2713]: training loss : 0.9968049883842468 TRAIN  loss dict:  {'classification_loss': 0.9968049883842468}
2025-01-16 15:21:29,038 [INFO] Step[2600/2713]: training loss : 0.9922682905197143 TRAIN  loss dict:  {'classification_loss': 0.9922682905197143}
2025-01-16 15:21:45,171 [INFO] Step[2650/2713]: training loss : 1.024719762802124 TRAIN  loss dict:  {'classification_loss': 1.024719762802124}
2025-01-16 15:22:01,307 [INFO] Step[2700/2713]: training loss : 1.0741322374343871 TRAIN  loss dict:  {'classification_loss': 1.0741322374343871}
2025-01-16 15:23:20,057 [INFO] Label accuracies statistics:
2025-01-16 15:23:20,057 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 1.0, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.25, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.5, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 0.75, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 0.75, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.25, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.25, 203: 0.75, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 1.0, 209: 0.5, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.0, 230: 1.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.75, 261: 0.25, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.25, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 1.0, 283: 1.0, 284: 0.5, 285: 0.75, 286: 0.75, 287: 0.5, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 0.75, 328: 0.75, 329: 0.5, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.25, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.5, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.25, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.0, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-16 15:23:20,059 [INFO] [19] TRAIN  loss: 1.0251046142980615 acc: 0.9824302739894336
2025-01-16 15:23:20,059 [INFO] [19] TRAIN  loss dict: {'classification_loss': 1.0251046142980615}
2025-01-16 15:23:20,059 [INFO] [19] VALIDATION loss: 1.8731655822436613 VALIDATION acc: 0.7636363636363637
2025-01-16 15:23:20,059 [INFO] [19] VALIDATION loss dict: {'classification_loss': 1.8731655822436613}
2025-01-16 15:23:20,059 [INFO] 
2025-01-16 15:23:41,712 [INFO] Step[50/2713]: training loss : 0.9773790001869201 TRAIN  loss dict:  {'classification_loss': 0.9773790001869201}
2025-01-16 15:23:57,806 [INFO] Step[100/2713]: training loss : 1.032122210264206 TRAIN  loss dict:  {'classification_loss': 1.032122210264206}
2025-01-16 15:24:13,966 [INFO] Step[150/2713]: training loss : 0.9708046233654022 TRAIN  loss dict:  {'classification_loss': 0.9708046233654022}
2025-01-16 15:24:30,034 [INFO] Step[200/2713]: training loss : 1.0210746395587922 TRAIN  loss dict:  {'classification_loss': 1.0210746395587922}
2025-01-16 15:24:46,187 [INFO] Step[250/2713]: training loss : 0.9955563044548035 TRAIN  loss dict:  {'classification_loss': 0.9955563044548035}
2025-01-16 15:25:02,357 [INFO] Step[300/2713]: training loss : 1.0241964256763458 TRAIN  loss dict:  {'classification_loss': 1.0241964256763458}
2025-01-16 15:25:18,418 [INFO] Step[350/2713]: training loss : 1.1011438143253327 TRAIN  loss dict:  {'classification_loss': 1.1011438143253327}
2025-01-16 15:25:34,494 [INFO] Step[400/2713]: training loss : 1.0511377942562103 TRAIN  loss dict:  {'classification_loss': 1.0511377942562103}
2025-01-16 15:25:50,617 [INFO] Step[450/2713]: training loss : 1.056592983007431 TRAIN  loss dict:  {'classification_loss': 1.056592983007431}
2025-01-16 15:26:06,736 [INFO] Step[500/2713]: training loss : 1.0798996543884278 TRAIN  loss dict:  {'classification_loss': 1.0798996543884278}
2025-01-16 15:26:22,850 [INFO] Step[550/2713]: training loss : 1.0189821088314057 TRAIN  loss dict:  {'classification_loss': 1.0189821088314057}
2025-01-16 15:26:39,014 [INFO] Step[600/2713]: training loss : 0.9732457304000854 TRAIN  loss dict:  {'classification_loss': 0.9732457304000854}
2025-01-16 15:26:55,145 [INFO] Step[650/2713]: training loss : 1.0433362889289857 TRAIN  loss dict:  {'classification_loss': 1.0433362889289857}
2025-01-16 15:27:11,211 [INFO] Step[700/2713]: training loss : 0.9897423195838928 TRAIN  loss dict:  {'classification_loss': 0.9897423195838928}
2025-01-16 15:27:27,553 [INFO] Step[750/2713]: training loss : 1.020519469976425 TRAIN  loss dict:  {'classification_loss': 1.020519469976425}
2025-01-16 15:27:43,950 [INFO] Step[800/2713]: training loss : 1.0082997488975525 TRAIN  loss dict:  {'classification_loss': 1.0082997488975525}
2025-01-16 15:28:00,287 [INFO] Step[850/2713]: training loss : 1.0533596563339234 TRAIN  loss dict:  {'classification_loss': 1.0533596563339234}
2025-01-16 15:28:16,548 [INFO] Step[900/2713]: training loss : 0.9823688447475434 TRAIN  loss dict:  {'classification_loss': 0.9823688447475434}
2025-01-16 15:28:32,802 [INFO] Step[950/2713]: training loss : 1.0470989501476289 TRAIN  loss dict:  {'classification_loss': 1.0470989501476289}
2025-01-16 15:28:49,175 [INFO] Step[1000/2713]: training loss : 0.990270791053772 TRAIN  loss dict:  {'classification_loss': 0.990270791053772}
2025-01-16 15:29:05,534 [INFO] Step[1050/2713]: training loss : 0.9982867324352265 TRAIN  loss dict:  {'classification_loss': 0.9982867324352265}
2025-01-16 15:29:21,878 [INFO] Step[1100/2713]: training loss : 1.035263888835907 TRAIN  loss dict:  {'classification_loss': 1.035263888835907}
2025-01-16 15:29:38,196 [INFO] Step[1150/2713]: training loss : 1.074011754989624 TRAIN  loss dict:  {'classification_loss': 1.074011754989624}
2025-01-16 15:29:54,626 [INFO] Step[1200/2713]: training loss : 1.0862266755104064 TRAIN  loss dict:  {'classification_loss': 1.0862266755104064}
2025-01-16 15:30:10,925 [INFO] Step[1250/2713]: training loss : 1.023268562555313 TRAIN  loss dict:  {'classification_loss': 1.023268562555313}
2025-01-16 15:30:27,266 [INFO] Step[1300/2713]: training loss : 1.050573605298996 TRAIN  loss dict:  {'classification_loss': 1.050573605298996}
2025-01-16 15:30:43,558 [INFO] Step[1350/2713]: training loss : 0.9980257451534271 TRAIN  loss dict:  {'classification_loss': 0.9980257451534271}
2025-01-16 15:30:59,775 [INFO] Step[1400/2713]: training loss : 1.0250304520130158 TRAIN  loss dict:  {'classification_loss': 1.0250304520130158}
2025-01-16 15:31:16,100 [INFO] Step[1450/2713]: training loss : 1.0673407304286957 TRAIN  loss dict:  {'classification_loss': 1.0673407304286957}
2025-01-16 15:31:32,426 [INFO] Step[1500/2713]: training loss : 1.04324160695076 TRAIN  loss dict:  {'classification_loss': 1.04324160695076}
2025-01-16 15:31:48,702 [INFO] Step[1550/2713]: training loss : 1.0019313514232635 TRAIN  loss dict:  {'classification_loss': 1.0019313514232635}
2025-01-16 15:32:05,036 [INFO] Step[1600/2713]: training loss : 1.0326210749149323 TRAIN  loss dict:  {'classification_loss': 1.0326210749149323}
2025-01-16 15:32:21,454 [INFO] Step[1650/2713]: training loss : 1.0077092349529266 TRAIN  loss dict:  {'classification_loss': 1.0077092349529266}
2025-01-16 15:32:37,790 [INFO] Step[1700/2713]: training loss : 1.043369175195694 TRAIN  loss dict:  {'classification_loss': 1.043369175195694}
2025-01-16 15:32:54,095 [INFO] Step[1750/2713]: training loss : 1.0765662717819213 TRAIN  loss dict:  {'classification_loss': 1.0765662717819213}
2025-01-16 15:33:10,487 [INFO] Step[1800/2713]: training loss : 1.0609492778778076 TRAIN  loss dict:  {'classification_loss': 1.0609492778778076}
2025-01-16 15:33:26,866 [INFO] Step[1850/2713]: training loss : 1.008159795999527 TRAIN  loss dict:  {'classification_loss': 1.008159795999527}
2025-01-16 15:33:43,183 [INFO] Step[1900/2713]: training loss : 0.9936190617084503 TRAIN  loss dict:  {'classification_loss': 0.9936190617084503}
2025-01-16 15:33:59,512 [INFO] Step[1950/2713]: training loss : 1.081501877307892 TRAIN  loss dict:  {'classification_loss': 1.081501877307892}
2025-01-16 15:34:15,781 [INFO] Step[2000/2713]: training loss : 1.0272281241416932 TRAIN  loss dict:  {'classification_loss': 1.0272281241416932}
2025-01-16 15:34:32,135 [INFO] Step[2050/2713]: training loss : 0.9740618550777436 TRAIN  loss dict:  {'classification_loss': 0.9740618550777436}
2025-01-16 15:34:48,371 [INFO] Step[2100/2713]: training loss : 0.9924825739860534 TRAIN  loss dict:  {'classification_loss': 0.9924825739860534}
2025-01-16 15:35:04,696 [INFO] Step[2150/2713]: training loss : 1.0222007417678833 TRAIN  loss dict:  {'classification_loss': 1.0222007417678833}
2025-01-16 15:35:20,957 [INFO] Step[2200/2713]: training loss : 1.0481468009948731 TRAIN  loss dict:  {'classification_loss': 1.0481468009948731}
2025-01-16 15:35:37,316 [INFO] Step[2250/2713]: training loss : 1.054182060956955 TRAIN  loss dict:  {'classification_loss': 1.054182060956955}
2025-01-16 15:35:53,622 [INFO] Step[2300/2713]: training loss : 1.017711753845215 TRAIN  loss dict:  {'classification_loss': 1.017711753845215}
2025-01-16 15:36:09,983 [INFO] Step[2350/2713]: training loss : 1.040364340543747 TRAIN  loss dict:  {'classification_loss': 1.040364340543747}
2025-01-16 15:36:26,333 [INFO] Step[2400/2713]: training loss : 1.026910012960434 TRAIN  loss dict:  {'classification_loss': 1.026910012960434}
2025-01-16 15:36:42,579 [INFO] Step[2450/2713]: training loss : 1.0163562667369843 TRAIN  loss dict:  {'classification_loss': 1.0163562667369843}
2025-01-16 15:36:58,955 [INFO] Step[2500/2713]: training loss : 1.058116194009781 TRAIN  loss dict:  {'classification_loss': 1.058116194009781}
2025-01-16 15:37:15,336 [INFO] Step[2550/2713]: training loss : 0.9908822989463806 TRAIN  loss dict:  {'classification_loss': 0.9908822989463806}
2025-01-16 15:37:31,708 [INFO] Step[2600/2713]: training loss : 1.0202250897884368 TRAIN  loss dict:  {'classification_loss': 1.0202250897884368}
2025-01-16 15:37:48,021 [INFO] Step[2650/2713]: training loss : 1.0326956689357758 TRAIN  loss dict:  {'classification_loss': 1.0326956689357758}
2025-01-16 15:38:04,351 [INFO] Step[2700/2713]: training loss : 1.0011682832241058 TRAIN  loss dict:  {'classification_loss': 1.0011682832241058}
2025-01-16 15:39:23,249 [INFO] Label accuracies statistics:
2025-01-16 15:39:23,249 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 1.0, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.0, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.25, 143: 0.75, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 0.75, 186: 0.5, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.5, 234: 0.25, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.25, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.5, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.25, 260: 0.75, 261: 0.25, 262: 0.75, 263: 0.75, 264: 0.5, 265: 0.75, 266: 1.0, 267: 0.25, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 1.0, 282: 1.0, 283: 1.0, 284: 0.5, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.5, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.25, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 1.0, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 1.0, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.5, 391: 1.0, 392: 0.5, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-16 15:39:23,251 [INFO] [20] TRAIN  loss: 1.0271545125688524 acc: 0.9791129131342917
2025-01-16 15:39:23,251 [INFO] [20] TRAIN  loss dict: {'classification_loss': 1.0271545125688524}
2025-01-16 15:39:23,251 [INFO] [20] VALIDATION loss: 1.9137347404445921 VALIDATION acc: 0.7692789968652037
2025-01-16 15:39:23,251 [INFO] [20] VALIDATION loss dict: {'classification_loss': 1.9137347404445921}
2025-01-16 15:39:23,251 [INFO] 
2025-01-16 15:39:44,481 [INFO] Step[50/2713]: training loss : 1.0280690944194795 TRAIN  loss dict:  {'classification_loss': 1.0280690944194795}
2025-01-16 15:40:00,685 [INFO] Step[100/2713]: training loss : 0.9877752089500427 TRAIN  loss dict:  {'classification_loss': 0.9877752089500427}
2025-01-16 15:40:16,908 [INFO] Step[150/2713]: training loss : 1.0089742231369019 TRAIN  loss dict:  {'classification_loss': 1.0089742231369019}
2025-01-16 15:40:33,144 [INFO] Step[200/2713]: training loss : 1.0188972866535186 TRAIN  loss dict:  {'classification_loss': 1.0188972866535186}
2025-01-16 15:40:49,415 [INFO] Step[250/2713]: training loss : 0.9833135950565338 TRAIN  loss dict:  {'classification_loss': 0.9833135950565338}
2025-01-16 15:41:05,643 [INFO] Step[300/2713]: training loss : 1.0393665874004363 TRAIN  loss dict:  {'classification_loss': 1.0393665874004363}
2025-01-16 15:41:22,266 [INFO] Step[350/2713]: training loss : 1.0071062803268434 TRAIN  loss dict:  {'classification_loss': 1.0071062803268434}
2025-01-16 15:41:38,460 [INFO] Step[400/2713]: training loss : 0.9863760685920715 TRAIN  loss dict:  {'classification_loss': 0.9863760685920715}
2025-01-16 15:41:54,705 [INFO] Step[450/2713]: training loss : 0.9874464344978332 TRAIN  loss dict:  {'classification_loss': 0.9874464344978332}
2025-01-16 15:42:10,920 [INFO] Step[500/2713]: training loss : 1.010586793422699 TRAIN  loss dict:  {'classification_loss': 1.010586793422699}
2025-01-16 15:42:27,270 [INFO] Step[550/2713]: training loss : 1.0679557311534882 TRAIN  loss dict:  {'classification_loss': 1.0679557311534882}
2025-01-16 15:42:43,498 [INFO] Step[600/2713]: training loss : 1.0036875379085541 TRAIN  loss dict:  {'classification_loss': 1.0036875379085541}
2025-01-16 15:42:59,669 [INFO] Step[650/2713]: training loss : 1.000326782464981 TRAIN  loss dict:  {'classification_loss': 1.000326782464981}
2025-01-16 15:43:15,845 [INFO] Step[700/2713]: training loss : 1.0156622326374054 TRAIN  loss dict:  {'classification_loss': 1.0156622326374054}
2025-01-16 15:43:32,211 [INFO] Step[750/2713]: training loss : 1.0154093837738036 TRAIN  loss dict:  {'classification_loss': 1.0154093837738036}
2025-01-16 15:43:48,469 [INFO] Step[800/2713]: training loss : 1.0042159140110016 TRAIN  loss dict:  {'classification_loss': 1.0042159140110016}
2025-01-16 15:44:04,672 [INFO] Step[850/2713]: training loss : 0.988612744808197 TRAIN  loss dict:  {'classification_loss': 0.988612744808197}
2025-01-16 15:44:20,842 [INFO] Step[900/2713]: training loss : 1.0097456097602844 TRAIN  loss dict:  {'classification_loss': 1.0097456097602844}
2025-01-16 15:44:37,094 [INFO] Step[950/2713]: training loss : 0.9888603031635285 TRAIN  loss dict:  {'classification_loss': 0.9888603031635285}
2025-01-16 15:44:53,280 [INFO] Step[1000/2713]: training loss : 1.0364101707935334 TRAIN  loss dict:  {'classification_loss': 1.0364101707935334}
2025-01-16 15:45:09,482 [INFO] Step[1050/2713]: training loss : 0.9941644537448883 TRAIN  loss dict:  {'classification_loss': 0.9941644537448883}
2025-01-16 15:45:25,716 [INFO] Step[1100/2713]: training loss : 0.993163058757782 TRAIN  loss dict:  {'classification_loss': 0.993163058757782}
2025-01-16 15:45:41,896 [INFO] Step[1150/2713]: training loss : 1.0190558528900147 TRAIN  loss dict:  {'classification_loss': 1.0190558528900147}
2025-01-16 15:45:58,074 [INFO] Step[1200/2713]: training loss : 0.9794342660903931 TRAIN  loss dict:  {'classification_loss': 0.9794342660903931}
2025-01-16 15:46:14,288 [INFO] Step[1250/2713]: training loss : 1.0032592105865479 TRAIN  loss dict:  {'classification_loss': 1.0032592105865479}
2025-01-16 15:46:30,493 [INFO] Step[1300/2713]: training loss : 0.9986520171165466 TRAIN  loss dict:  {'classification_loss': 0.9986520171165466}
2025-01-16 15:46:46,726 [INFO] Step[1350/2713]: training loss : 0.9832231938838959 TRAIN  loss dict:  {'classification_loss': 0.9832231938838959}
2025-01-16 15:47:02,977 [INFO] Step[1400/2713]: training loss : 0.988582820892334 TRAIN  loss dict:  {'classification_loss': 0.988582820892334}
2025-01-16 15:47:19,202 [INFO] Step[1450/2713]: training loss : 0.9929315662384033 TRAIN  loss dict:  {'classification_loss': 0.9929315662384033}
2025-01-16 15:47:35,379 [INFO] Step[1500/2713]: training loss : 1.0165949618816377 TRAIN  loss dict:  {'classification_loss': 1.0165949618816377}
2025-01-16 15:47:51,612 [INFO] Step[1550/2713]: training loss : 0.9847996842861175 TRAIN  loss dict:  {'classification_loss': 0.9847996842861175}
2025-01-16 15:48:07,803 [INFO] Step[1600/2713]: training loss : 1.0032846736907959 TRAIN  loss dict:  {'classification_loss': 1.0032846736907959}
2025-01-16 15:48:24,002 [INFO] Step[1650/2713]: training loss : 0.9991027474403381 TRAIN  loss dict:  {'classification_loss': 0.9991027474403381}
2025-01-16 15:48:40,246 [INFO] Step[1700/2713]: training loss : 1.0319730484485625 TRAIN  loss dict:  {'classification_loss': 1.0319730484485625}
2025-01-16 15:48:56,456 [INFO] Step[1750/2713]: training loss : 0.9994628572463989 TRAIN  loss dict:  {'classification_loss': 0.9994628572463989}
2025-01-16 15:49:12,647 [INFO] Step[1800/2713]: training loss : 1.0071213114261628 TRAIN  loss dict:  {'classification_loss': 1.0071213114261628}
2025-01-16 15:49:28,930 [INFO] Step[1850/2713]: training loss : 1.0354434502124787 TRAIN  loss dict:  {'classification_loss': 1.0354434502124787}
2025-01-16 15:49:45,117 [INFO] Step[1900/2713]: training loss : 0.9889171302318573 TRAIN  loss dict:  {'classification_loss': 0.9889171302318573}
2025-01-16 15:50:01,377 [INFO] Step[1950/2713]: training loss : 0.9986407101154328 TRAIN  loss dict:  {'classification_loss': 0.9986407101154328}
2025-01-16 15:50:17,625 [INFO] Step[2000/2713]: training loss : 0.9863466811180115 TRAIN  loss dict:  {'classification_loss': 0.9863466811180115}
2025-01-16 15:50:33,871 [INFO] Step[2050/2713]: training loss : 0.9942919182777404 TRAIN  loss dict:  {'classification_loss': 0.9942919182777404}
2025-01-16 15:50:50,154 [INFO] Step[2100/2713]: training loss : 1.0076432347297668 TRAIN  loss dict:  {'classification_loss': 1.0076432347297668}
2025-01-16 15:51:06,429 [INFO] Step[2150/2713]: training loss : 0.9938600552082062 TRAIN  loss dict:  {'classification_loss': 0.9938600552082062}
2025-01-16 15:51:22,648 [INFO] Step[2200/2713]: training loss : 0.9951881992816926 TRAIN  loss dict:  {'classification_loss': 0.9951881992816926}
2025-01-16 15:51:38,922 [INFO] Step[2250/2713]: training loss : 1.0001404345035554 TRAIN  loss dict:  {'classification_loss': 1.0001404345035554}
2025-01-16 15:51:55,199 [INFO] Step[2300/2713]: training loss : 0.9824345409870148 TRAIN  loss dict:  {'classification_loss': 0.9824345409870148}
2025-01-16 15:52:11,433 [INFO] Step[2350/2713]: training loss : 0.9931756281852722 TRAIN  loss dict:  {'classification_loss': 0.9931756281852722}
2025-01-16 15:52:27,581 [INFO] Step[2400/2713]: training loss : 1.0058259272575378 TRAIN  loss dict:  {'classification_loss': 1.0058259272575378}
2025-01-16 15:52:43,820 [INFO] Step[2450/2713]: training loss : 0.9894217276573181 TRAIN  loss dict:  {'classification_loss': 0.9894217276573181}
2025-01-16 15:53:00,016 [INFO] Step[2500/2713]: training loss : 0.9748198962211609 TRAIN  loss dict:  {'classification_loss': 0.9748198962211609}
2025-01-16 15:53:16,364 [INFO] Step[2550/2713]: training loss : 0.9751291942596435 TRAIN  loss dict:  {'classification_loss': 0.9751291942596435}
2025-01-16 15:53:32,574 [INFO] Step[2600/2713]: training loss : 0.9913754272460937 TRAIN  loss dict:  {'classification_loss': 0.9913754272460937}
2025-01-16 15:53:48,874 [INFO] Step[2650/2713]: training loss : 0.9840765726566315 TRAIN  loss dict:  {'classification_loss': 0.9840765726566315}
2025-01-16 15:54:05,202 [INFO] Step[2700/2713]: training loss : 0.9797892820835113 TRAIN  loss dict:  {'classification_loss': 0.9797892820835113}
2025-01-16 15:55:23,639 [INFO] Label accuracies statistics:
2025-01-16 15:55:23,639 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.5, 122: 1.0, 123: 0.5, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.5, 143: 0.5, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.25, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 1.0, 214: 0.75, 215: 1.0, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.5, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.5, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.25, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.25, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 0.75, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 15:55:49,759 [INFO] [21] TRAIN  loss: 1.0011074542340168 acc: 0.9857476348445755
2025-01-16 15:55:49,759 [INFO] [21] TRAIN  loss dict: {'classification_loss': 1.0011074542340168}
2025-01-16 15:55:49,760 [INFO] [21] VALIDATION loss: 1.8181438375460475 VALIDATION acc: 0.7786833855799373
2025-01-16 15:55:49,760 [INFO] [21] VALIDATION loss dict: {'classification_loss': 1.8181438375460475}
2025-01-16 15:55:49,760 [INFO] 
2025-01-16 15:56:10,511 [INFO] Step[50/2713]: training loss : 0.9713983142375946 TRAIN  loss dict:  {'classification_loss': 0.9713983142375946}
2025-01-16 15:56:26,562 [INFO] Step[100/2713]: training loss : 0.9720309603214264 TRAIN  loss dict:  {'classification_loss': 0.9720309603214264}
2025-01-16 15:56:42,731 [INFO] Step[150/2713]: training loss : 0.9969745075702667 TRAIN  loss dict:  {'classification_loss': 0.9969745075702667}
2025-01-16 15:56:58,881 [INFO] Step[200/2713]: training loss : 0.9701361072063446 TRAIN  loss dict:  {'classification_loss': 0.9701361072063446}
2025-01-16 15:57:15,022 [INFO] Step[250/2713]: training loss : 1.0109404504299164 TRAIN  loss dict:  {'classification_loss': 1.0109404504299164}
2025-01-16 15:57:31,177 [INFO] Step[300/2713]: training loss : 0.9854140985012054 TRAIN  loss dict:  {'classification_loss': 0.9854140985012054}
2025-01-16 15:57:47,328 [INFO] Step[350/2713]: training loss : 0.9755924880504608 TRAIN  loss dict:  {'classification_loss': 0.9755924880504608}
2025-01-16 15:58:03,554 [INFO] Step[400/2713]: training loss : 0.9864120745658874 TRAIN  loss dict:  {'classification_loss': 0.9864120745658874}
2025-01-16 15:58:19,773 [INFO] Step[450/2713]: training loss : 0.9713486683368683 TRAIN  loss dict:  {'classification_loss': 0.9713486683368683}
2025-01-16 15:58:36,004 [INFO] Step[500/2713]: training loss : 0.9835475707054138 TRAIN  loss dict:  {'classification_loss': 0.9835475707054138}
2025-01-16 15:58:52,123 [INFO] Step[550/2713]: training loss : 0.9843421828746796 TRAIN  loss dict:  {'classification_loss': 0.9843421828746796}
2025-01-16 15:59:08,284 [INFO] Step[600/2713]: training loss : 0.9670214474201202 TRAIN  loss dict:  {'classification_loss': 0.9670214474201202}
2025-01-16 15:59:24,405 [INFO] Step[650/2713]: training loss : 0.9928059589862823 TRAIN  loss dict:  {'classification_loss': 0.9928059589862823}
2025-01-16 15:59:40,752 [INFO] Step[700/2713]: training loss : 1.0195394563674927 TRAIN  loss dict:  {'classification_loss': 1.0195394563674927}
2025-01-16 15:59:56,912 [INFO] Step[750/2713]: training loss : 0.9873006153106689 TRAIN  loss dict:  {'classification_loss': 0.9873006153106689}
2025-01-16 16:00:13,129 [INFO] Step[800/2713]: training loss : 0.996788614988327 TRAIN  loss dict:  {'classification_loss': 0.996788614988327}
2025-01-16 16:00:29,379 [INFO] Step[850/2713]: training loss : 0.9706671679019928 TRAIN  loss dict:  {'classification_loss': 0.9706671679019928}
2025-01-16 16:00:45,540 [INFO] Step[900/2713]: training loss : 0.9873893570899963 TRAIN  loss dict:  {'classification_loss': 0.9873893570899963}
2025-01-16 16:01:01,858 [INFO] Step[950/2713]: training loss : 0.9792414581775666 TRAIN  loss dict:  {'classification_loss': 0.9792414581775666}
2025-01-16 16:01:18,096 [INFO] Step[1000/2713]: training loss : 0.9985553574562073 TRAIN  loss dict:  {'classification_loss': 0.9985553574562073}
2025-01-16 16:01:34,419 [INFO] Step[1050/2713]: training loss : 0.9798377192020417 TRAIN  loss dict:  {'classification_loss': 0.9798377192020417}
2025-01-16 16:01:50,702 [INFO] Step[1100/2713]: training loss : 0.9650852119922638 TRAIN  loss dict:  {'classification_loss': 0.9650852119922638}
2025-01-16 16:02:06,948 [INFO] Step[1150/2713]: training loss : 0.9556366658210754 TRAIN  loss dict:  {'classification_loss': 0.9556366658210754}
2025-01-16 16:02:23,227 [INFO] Step[1200/2713]: training loss : 0.9876311361789704 TRAIN  loss dict:  {'classification_loss': 0.9876311361789704}
2025-01-16 16:02:39,664 [INFO] Step[1250/2713]: training loss : 0.9828009510040283 TRAIN  loss dict:  {'classification_loss': 0.9828009510040283}
2025-01-16 16:02:56,086 [INFO] Step[1300/2713]: training loss : 1.0070502543449402 TRAIN  loss dict:  {'classification_loss': 1.0070502543449402}
2025-01-16 16:03:12,441 [INFO] Step[1350/2713]: training loss : 1.0174065911769867 TRAIN  loss dict:  {'classification_loss': 1.0174065911769867}
2025-01-16 16:03:28,852 [INFO] Step[1400/2713]: training loss : 0.9910235726833343 TRAIN  loss dict:  {'classification_loss': 0.9910235726833343}
2025-01-16 16:03:45,203 [INFO] Step[1450/2713]: training loss : 0.9816718125343322 TRAIN  loss dict:  {'classification_loss': 0.9816718125343322}
2025-01-16 16:04:01,422 [INFO] Step[1500/2713]: training loss : 0.9735944557189942 TRAIN  loss dict:  {'classification_loss': 0.9735944557189942}
2025-01-16 16:04:17,761 [INFO] Step[1550/2713]: training loss : 0.9604704391956329 TRAIN  loss dict:  {'classification_loss': 0.9604704391956329}
2025-01-16 16:04:33,996 [INFO] Step[1600/2713]: training loss : 0.9578899800777435 TRAIN  loss dict:  {'classification_loss': 0.9578899800777435}
2025-01-16 16:04:50,199 [INFO] Step[1650/2713]: training loss : 0.9688360071182252 TRAIN  loss dict:  {'classification_loss': 0.9688360071182252}
2025-01-16 16:05:06,495 [INFO] Step[1700/2713]: training loss : 0.96681272149086 TRAIN  loss dict:  {'classification_loss': 0.96681272149086}
2025-01-16 16:05:22,784 [INFO] Step[1750/2713]: training loss : 0.9827630257606507 TRAIN  loss dict:  {'classification_loss': 0.9827630257606507}
2025-01-16 16:05:39,155 [INFO] Step[1800/2713]: training loss : 0.9941967546939849 TRAIN  loss dict:  {'classification_loss': 0.9941967546939849}
2025-01-16 16:05:55,383 [INFO] Step[1850/2713]: training loss : 0.9751407194137574 TRAIN  loss dict:  {'classification_loss': 0.9751407194137574}
2025-01-16 16:06:11,786 [INFO] Step[1900/2713]: training loss : 0.9819595348834992 TRAIN  loss dict:  {'classification_loss': 0.9819595348834992}
2025-01-16 16:06:28,054 [INFO] Step[1950/2713]: training loss : 0.9585051822662354 TRAIN  loss dict:  {'classification_loss': 0.9585051822662354}
2025-01-16 16:06:44,341 [INFO] Step[2000/2713]: training loss : 1.023331388235092 TRAIN  loss dict:  {'classification_loss': 1.023331388235092}
2025-01-16 16:07:00,715 [INFO] Step[2050/2713]: training loss : 1.018226704597473 TRAIN  loss dict:  {'classification_loss': 1.018226704597473}
2025-01-16 16:07:17,024 [INFO] Step[2100/2713]: training loss : 0.9886171460151673 TRAIN  loss dict:  {'classification_loss': 0.9886171460151673}
2025-01-16 16:07:33,325 [INFO] Step[2150/2713]: training loss : 1.0094953286647796 TRAIN  loss dict:  {'classification_loss': 1.0094953286647796}
2025-01-16 16:07:49,688 [INFO] Step[2200/2713]: training loss : 1.0010085272789002 TRAIN  loss dict:  {'classification_loss': 1.0010085272789002}
2025-01-16 16:08:06,033 [INFO] Step[2250/2713]: training loss : 0.9708193480968476 TRAIN  loss dict:  {'classification_loss': 0.9708193480968476}
2025-01-16 16:08:22,325 [INFO] Step[2300/2713]: training loss : 0.9984938037395478 TRAIN  loss dict:  {'classification_loss': 0.9984938037395478}
2025-01-16 16:08:38,702 [INFO] Step[2350/2713]: training loss : 0.9881690776348114 TRAIN  loss dict:  {'classification_loss': 0.9881690776348114}
2025-01-16 16:08:54,990 [INFO] Step[2400/2713]: training loss : 0.9905412495136261 TRAIN  loss dict:  {'classification_loss': 0.9905412495136261}
2025-01-16 16:09:11,384 [INFO] Step[2450/2713]: training loss : 1.0492460608482361 TRAIN  loss dict:  {'classification_loss': 1.0492460608482361}
2025-01-16 16:09:27,678 [INFO] Step[2500/2713]: training loss : 0.988269737958908 TRAIN  loss dict:  {'classification_loss': 0.988269737958908}
2025-01-16 16:09:44,034 [INFO] Step[2550/2713]: training loss : 0.9959967947006225 TRAIN  loss dict:  {'classification_loss': 0.9959967947006225}
2025-01-16 16:10:00,332 [INFO] Step[2600/2713]: training loss : 1.0100679731369018 TRAIN  loss dict:  {'classification_loss': 1.0100679731369018}
2025-01-16 16:10:16,686 [INFO] Step[2650/2713]: training loss : 0.9806026899814606 TRAIN  loss dict:  {'classification_loss': 0.9806026899814606}
2025-01-16 16:10:32,979 [INFO] Step[2700/2713]: training loss : 1.0025821828842163 TRAIN  loss dict:  {'classification_loss': 1.0025821828842163}
2025-01-16 16:11:51,998 [INFO] Label accuracies statistics:
2025-01-16 16:11:51,999 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.5, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 1.0, 77: 0.75, 78: 1.0, 79: 0.5, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 0.75, 111: 0.75, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 0.75, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.5, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.25, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 1.0, 311: 0.5, 312: 1.0, 313: 0.25, 314: 0.75, 315: 0.5, 316: 1.0, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.5, 328: 1.0, 329: 0.5, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 1.0, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.0, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-16 16:11:52,000 [INFO] [22] TRAIN  loss: 0.987091918076003 acc: 0.9904165130851456
2025-01-16 16:11:52,000 [INFO] [22] TRAIN  loss dict: {'classification_loss': 0.987091918076003}
2025-01-16 16:11:52,000 [INFO] [22] VALIDATION loss: 1.8916132526290148 VALIDATION acc: 0.7667711598746082
2025-01-16 16:11:52,000 [INFO] [22] VALIDATION loss dict: {'classification_loss': 1.8916132526290148}
2025-01-16 16:11:52,001 [INFO] 
2025-01-16 16:12:12,492 [INFO] Step[50/2713]: training loss : 0.9919484841823578 TRAIN  loss dict:  {'classification_loss': 0.9919484841823578}
2025-01-16 16:12:28,800 [INFO] Step[100/2713]: training loss : 0.9854584646224975 TRAIN  loss dict:  {'classification_loss': 0.9854584646224975}
2025-01-16 16:12:45,090 [INFO] Step[150/2713]: training loss : 0.9909192025661469 TRAIN  loss dict:  {'classification_loss': 0.9909192025661469}
2025-01-16 16:13:01,501 [INFO] Step[200/2713]: training loss : 0.9848657059669494 TRAIN  loss dict:  {'classification_loss': 0.9848657059669494}
2025-01-16 16:13:17,885 [INFO] Step[250/2713]: training loss : 0.9644648516178131 TRAIN  loss dict:  {'classification_loss': 0.9644648516178131}
2025-01-16 16:13:34,163 [INFO] Step[300/2713]: training loss : 0.9898507535457611 TRAIN  loss dict:  {'classification_loss': 0.9898507535457611}
2025-01-16 16:13:50,290 [INFO] Step[350/2713]: training loss : 1.0122330033779143 TRAIN  loss dict:  {'classification_loss': 1.0122330033779143}
2025-01-16 16:14:06,383 [INFO] Step[400/2713]: training loss : 0.9570236837863922 TRAIN  loss dict:  {'classification_loss': 0.9570236837863922}
2025-01-16 16:14:22,560 [INFO] Step[450/2713]: training loss : 0.9653562521934509 TRAIN  loss dict:  {'classification_loss': 0.9653562521934509}
2025-01-16 16:14:38,724 [INFO] Step[500/2713]: training loss : 1.0181477546691895 TRAIN  loss dict:  {'classification_loss': 1.0181477546691895}
2025-01-16 16:14:54,848 [INFO] Step[550/2713]: training loss : 0.9691802191734314 TRAIN  loss dict:  {'classification_loss': 0.9691802191734314}
2025-01-16 16:15:10,937 [INFO] Step[600/2713]: training loss : 0.9583757984638214 TRAIN  loss dict:  {'classification_loss': 0.9583757984638214}
2025-01-16 16:15:27,067 [INFO] Step[650/2713]: training loss : 0.9731828236579895 TRAIN  loss dict:  {'classification_loss': 0.9731828236579895}
2025-01-16 16:15:43,206 [INFO] Step[700/2713]: training loss : 0.9645914435386658 TRAIN  loss dict:  {'classification_loss': 0.9645914435386658}
2025-01-16 16:15:59,306 [INFO] Step[750/2713]: training loss : 0.9671322762966156 TRAIN  loss dict:  {'classification_loss': 0.9671322762966156}
2025-01-16 16:16:15,377 [INFO] Step[800/2713]: training loss : 0.9565618526935578 TRAIN  loss dict:  {'classification_loss': 0.9565618526935578}
2025-01-16 16:16:31,555 [INFO] Step[850/2713]: training loss : 0.9664539361000061 TRAIN  loss dict:  {'classification_loss': 0.9664539361000061}
2025-01-16 16:16:47,687 [INFO] Step[900/2713]: training loss : 0.9738251268863678 TRAIN  loss dict:  {'classification_loss': 0.9738251268863678}
2025-01-16 16:17:03,782 [INFO] Step[950/2713]: training loss : 0.9574731588363647 TRAIN  loss dict:  {'classification_loss': 0.9574731588363647}
2025-01-16 16:17:19,919 [INFO] Step[1000/2713]: training loss : 0.9948605453968048 TRAIN  loss dict:  {'classification_loss': 0.9948605453968048}
2025-01-16 16:17:36,130 [INFO] Step[1050/2713]: training loss : 0.9965786695480346 TRAIN  loss dict:  {'classification_loss': 0.9965786695480346}
2025-01-16 16:17:52,326 [INFO] Step[1100/2713]: training loss : 0.9685714650154114 TRAIN  loss dict:  {'classification_loss': 0.9685714650154114}
2025-01-16 16:18:08,550 [INFO] Step[1150/2713]: training loss : 1.0006388378143312 TRAIN  loss dict:  {'classification_loss': 1.0006388378143312}
2025-01-16 16:18:24,614 [INFO] Step[1200/2713]: training loss : 0.9618098366260529 TRAIN  loss dict:  {'classification_loss': 0.9618098366260529}
2025-01-16 16:18:40,702 [INFO] Step[1250/2713]: training loss : 0.9851076471805572 TRAIN  loss dict:  {'classification_loss': 0.9851076471805572}
2025-01-16 16:18:56,804 [INFO] Step[1300/2713]: training loss : 0.9817237818241119 TRAIN  loss dict:  {'classification_loss': 0.9817237818241119}
2025-01-16 16:19:12,995 [INFO] Step[1350/2713]: training loss : 0.9654631352424622 TRAIN  loss dict:  {'classification_loss': 0.9654631352424622}
2025-01-16 16:19:29,108 [INFO] Step[1400/2713]: training loss : 0.9687957191467285 TRAIN  loss dict:  {'classification_loss': 0.9687957191467285}
2025-01-16 16:19:45,240 [INFO] Step[1450/2713]: training loss : 0.9777033138275146 TRAIN  loss dict:  {'classification_loss': 0.9777033138275146}
2025-01-16 16:20:01,407 [INFO] Step[1500/2713]: training loss : 0.963161803483963 TRAIN  loss dict:  {'classification_loss': 0.963161803483963}
2025-01-16 16:20:17,606 [INFO] Step[1550/2713]: training loss : 0.9764835631847382 TRAIN  loss dict:  {'classification_loss': 0.9764835631847382}
2025-01-16 16:20:33,750 [INFO] Step[1600/2713]: training loss : 0.9718271446228027 TRAIN  loss dict:  {'classification_loss': 0.9718271446228027}
2025-01-16 16:20:49,866 [INFO] Step[1650/2713]: training loss : 0.9860934710502625 TRAIN  loss dict:  {'classification_loss': 0.9860934710502625}
2025-01-16 16:21:05,981 [INFO] Step[1700/2713]: training loss : 1.0167215931415559 TRAIN  loss dict:  {'classification_loss': 1.0167215931415559}
2025-01-16 16:21:22,147 [INFO] Step[1750/2713]: training loss : 0.9810143280029296 TRAIN  loss dict:  {'classification_loss': 0.9810143280029296}
2025-01-16 16:21:38,266 [INFO] Step[1800/2713]: training loss : 1.0041068422794341 TRAIN  loss dict:  {'classification_loss': 1.0041068422794341}
2025-01-16 16:21:54,399 [INFO] Step[1850/2713]: training loss : 1.0121710193157196 TRAIN  loss dict:  {'classification_loss': 1.0121710193157196}
2025-01-16 16:22:10,562 [INFO] Step[1900/2713]: training loss : 0.9954534780979156 TRAIN  loss dict:  {'classification_loss': 0.9954534780979156}
2025-01-16 16:22:26,673 [INFO] Step[1950/2713]: training loss : 0.9901749873161316 TRAIN  loss dict:  {'classification_loss': 0.9901749873161316}
2025-01-16 16:22:42,788 [INFO] Step[2000/2713]: training loss : 0.9603048586845397 TRAIN  loss dict:  {'classification_loss': 0.9603048586845397}
2025-01-16 16:22:58,965 [INFO] Step[2050/2713]: training loss : 1.0106512796878815 TRAIN  loss dict:  {'classification_loss': 1.0106512796878815}
2025-01-16 16:23:15,080 [INFO] Step[2100/2713]: training loss : 1.014383498430252 TRAIN  loss dict:  {'classification_loss': 1.014383498430252}
2025-01-16 16:23:31,186 [INFO] Step[2150/2713]: training loss : 0.9783601558208466 TRAIN  loss dict:  {'classification_loss': 0.9783601558208466}
2025-01-16 16:23:47,335 [INFO] Step[2200/2713]: training loss : 1.0011507022380828 TRAIN  loss dict:  {'classification_loss': 1.0011507022380828}
2025-01-16 16:24:03,517 [INFO] Step[2250/2713]: training loss : 1.0331772065162659 TRAIN  loss dict:  {'classification_loss': 1.0331772065162659}
2025-01-16 16:24:19,645 [INFO] Step[2300/2713]: training loss : 0.9651259648799896 TRAIN  loss dict:  {'classification_loss': 0.9651259648799896}
2025-01-16 16:24:35,798 [INFO] Step[2350/2713]: training loss : 0.9979264903068542 TRAIN  loss dict:  {'classification_loss': 0.9979264903068542}
2025-01-16 16:24:51,896 [INFO] Step[2400/2713]: training loss : 0.971266301870346 TRAIN  loss dict:  {'classification_loss': 0.971266301870346}
2025-01-16 16:25:08,062 [INFO] Step[2450/2713]: training loss : 0.9918979918956756 TRAIN  loss dict:  {'classification_loss': 0.9918979918956756}
2025-01-16 16:25:24,164 [INFO] Step[2500/2713]: training loss : 0.9904993462562561 TRAIN  loss dict:  {'classification_loss': 0.9904993462562561}
2025-01-16 16:25:40,249 [INFO] Step[2550/2713]: training loss : 0.9691523039340972 TRAIN  loss dict:  {'classification_loss': 0.9691523039340972}
2025-01-16 16:25:56,380 [INFO] Step[2600/2713]: training loss : 0.9799753785133362 TRAIN  loss dict:  {'classification_loss': 0.9799753785133362}
2025-01-16 16:26:12,552 [INFO] Step[2650/2713]: training loss : 0.976159372329712 TRAIN  loss dict:  {'classification_loss': 0.976159372329712}
2025-01-16 16:26:28,651 [INFO] Step[2700/2713]: training loss : 0.9644069874286652 TRAIN  loss dict:  {'classification_loss': 0.9644069874286652}
2025-01-16 16:27:47,452 [INFO] Label accuracies statistics:
2025-01-16 16:27:47,452 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.75, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.5, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.25, 204: 0.25, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 1.0, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.25, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.25, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.5, 302: 0.5, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.5, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.0, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.25, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 16:27:47,454 [INFO] [23] TRAIN  loss: 0.9822466809548358 acc: 0.9904165130851456
2025-01-16 16:27:47,454 [INFO] [23] TRAIN  loss dict: {'classification_loss': 0.9822466809548358}
2025-01-16 16:27:47,454 [INFO] [23] VALIDATION loss: 1.8202232726310428 VALIDATION acc: 0.7843260188087774
2025-01-16 16:27:47,454 [INFO] [23] VALIDATION loss dict: {'classification_loss': 1.8202232726310428}
2025-01-16 16:27:47,455 [INFO] 
2025-01-16 16:28:08,133 [INFO] Step[50/2713]: training loss : 0.958934885263443 TRAIN  loss dict:  {'classification_loss': 0.958934885263443}
2025-01-16 16:28:24,298 [INFO] Step[100/2713]: training loss : 0.964603146314621 TRAIN  loss dict:  {'classification_loss': 0.964603146314621}
2025-01-16 16:28:40,498 [INFO] Step[150/2713]: training loss : 1.0089500987529754 TRAIN  loss dict:  {'classification_loss': 1.0089500987529754}
2025-01-16 16:28:56,655 [INFO] Step[200/2713]: training loss : 0.9578979468345642 TRAIN  loss dict:  {'classification_loss': 0.9578979468345642}
2025-01-16 16:29:12,840 [INFO] Step[250/2713]: training loss : 0.9538200080394745 TRAIN  loss dict:  {'classification_loss': 0.9538200080394745}
2025-01-16 16:29:29,028 [INFO] Step[300/2713]: training loss : 0.9609788107872009 TRAIN  loss dict:  {'classification_loss': 0.9609788107872009}
2025-01-16 16:29:45,155 [INFO] Step[350/2713]: training loss : 0.9774785006046295 TRAIN  loss dict:  {'classification_loss': 0.9774785006046295}
2025-01-16 16:30:01,352 [INFO] Step[400/2713]: training loss : 0.9679331576824188 TRAIN  loss dict:  {'classification_loss': 0.9679331576824188}
2025-01-16 16:30:17,559 [INFO] Step[450/2713]: training loss : 0.9581396508216858 TRAIN  loss dict:  {'classification_loss': 0.9581396508216858}
2025-01-16 16:30:33,679 [INFO] Step[500/2713]: training loss : 0.9960236430168152 TRAIN  loss dict:  {'classification_loss': 0.9960236430168152}
2025-01-16 16:30:49,866 [INFO] Step[550/2713]: training loss : 1.0020405375957488 TRAIN  loss dict:  {'classification_loss': 1.0020405375957488}
2025-01-16 16:31:06,016 [INFO] Step[600/2713]: training loss : 0.9543912482261657 TRAIN  loss dict:  {'classification_loss': 0.9543912482261657}
2025-01-16 16:31:22,160 [INFO] Step[650/2713]: training loss : 0.9680827295780182 TRAIN  loss dict:  {'classification_loss': 0.9680827295780182}
2025-01-16 16:31:38,305 [INFO] Step[700/2713]: training loss : 1.0129620230197907 TRAIN  loss dict:  {'classification_loss': 1.0129620230197907}
2025-01-16 16:31:54,496 [INFO] Step[750/2713]: training loss : 0.9684973740577698 TRAIN  loss dict:  {'classification_loss': 0.9684973740577698}
2025-01-16 16:32:10,622 [INFO] Step[800/2713]: training loss : 0.9863983309268951 TRAIN  loss dict:  {'classification_loss': 0.9863983309268951}
2025-01-16 16:32:26,790 [INFO] Step[850/2713]: training loss : 1.043081693649292 TRAIN  loss dict:  {'classification_loss': 1.043081693649292}
2025-01-16 16:32:42,982 [INFO] Step[900/2713]: training loss : 0.9594940769672394 TRAIN  loss dict:  {'classification_loss': 0.9594940769672394}
2025-01-16 16:32:59,286 [INFO] Step[950/2713]: training loss : 1.0071075248718262 TRAIN  loss dict:  {'classification_loss': 1.0071075248718262}
2025-01-16 16:33:15,461 [INFO] Step[1000/2713]: training loss : 0.9779770159721375 TRAIN  loss dict:  {'classification_loss': 0.9779770159721375}
2025-01-16 16:33:31,680 [INFO] Step[1050/2713]: training loss : 0.9876539266109466 TRAIN  loss dict:  {'classification_loss': 0.9876539266109466}
2025-01-16 16:33:47,853 [INFO] Step[1100/2713]: training loss : 0.996583606004715 TRAIN  loss dict:  {'classification_loss': 0.996583606004715}
2025-01-16 16:34:04,098 [INFO] Step[1150/2713]: training loss : 0.9997642874717713 TRAIN  loss dict:  {'classification_loss': 0.9997642874717713}
2025-01-16 16:34:20,263 [INFO] Step[1200/2713]: training loss : 0.983851273059845 TRAIN  loss dict:  {'classification_loss': 0.983851273059845}
2025-01-16 16:34:36,400 [INFO] Step[1250/2713]: training loss : 0.9828617763519287 TRAIN  loss dict:  {'classification_loss': 0.9828617763519287}
2025-01-16 16:34:52,596 [INFO] Step[1300/2713]: training loss : 0.9744656252861023 TRAIN  loss dict:  {'classification_loss': 0.9744656252861023}
2025-01-16 16:35:08,742 [INFO] Step[1350/2713]: training loss : 0.9667609524726868 TRAIN  loss dict:  {'classification_loss': 0.9667609524726868}
2025-01-16 16:35:24,844 [INFO] Step[1400/2713]: training loss : 0.9696441256999969 TRAIN  loss dict:  {'classification_loss': 0.9696441256999969}
2025-01-16 16:35:41,027 [INFO] Step[1450/2713]: training loss : 0.9638212668895721 TRAIN  loss dict:  {'classification_loss': 0.9638212668895721}
2025-01-16 16:35:57,127 [INFO] Step[1500/2713]: training loss : 0.9764536941051483 TRAIN  loss dict:  {'classification_loss': 0.9764536941051483}
2025-01-16 16:36:13,287 [INFO] Step[1550/2713]: training loss : 0.9836662817001343 TRAIN  loss dict:  {'classification_loss': 0.9836662817001343}
2025-01-16 16:36:29,539 [INFO] Step[1600/2713]: training loss : 0.9905991291999817 TRAIN  loss dict:  {'classification_loss': 0.9905991291999817}
2025-01-16 16:36:45,639 [INFO] Step[1650/2713]: training loss : 1.001567108631134 TRAIN  loss dict:  {'classification_loss': 1.001567108631134}
2025-01-16 16:37:01,887 [INFO] Step[1700/2713]: training loss : 0.9636858141422272 TRAIN  loss dict:  {'classification_loss': 0.9636858141422272}
2025-01-16 16:37:18,108 [INFO] Step[1750/2713]: training loss : 1.0016351163387298 TRAIN  loss dict:  {'classification_loss': 1.0016351163387298}
2025-01-16 16:37:34,240 [INFO] Step[1800/2713]: training loss : 0.9933064961433411 TRAIN  loss dict:  {'classification_loss': 0.9933064961433411}
2025-01-16 16:37:50,439 [INFO] Step[1850/2713]: training loss : 1.0192436385154724 TRAIN  loss dict:  {'classification_loss': 1.0192436385154724}
2025-01-16 16:38:06,596 [INFO] Step[1900/2713]: training loss : 0.9838256669044495 TRAIN  loss dict:  {'classification_loss': 0.9838256669044495}
2025-01-16 16:38:22,716 [INFO] Step[1950/2713]: training loss : 0.9683088457584381 TRAIN  loss dict:  {'classification_loss': 0.9683088457584381}
2025-01-16 16:38:38,936 [INFO] Step[2000/2713]: training loss : 0.9780266082286835 TRAIN  loss dict:  {'classification_loss': 0.9780266082286835}
2025-01-16 16:38:55,125 [INFO] Step[2050/2713]: training loss : 0.989881020784378 TRAIN  loss dict:  {'classification_loss': 0.989881020784378}
2025-01-16 16:39:11,267 [INFO] Step[2100/2713]: training loss : 0.961716845035553 TRAIN  loss dict:  {'classification_loss': 0.961716845035553}
2025-01-16 16:39:27,404 [INFO] Step[2150/2713]: training loss : 1.020895869731903 TRAIN  loss dict:  {'classification_loss': 1.020895869731903}
2025-01-16 16:39:43,633 [INFO] Step[2200/2713]: training loss : 0.9822572720050812 TRAIN  loss dict:  {'classification_loss': 0.9822572720050812}
2025-01-16 16:39:59,886 [INFO] Step[2250/2713]: training loss : 0.9584751915931702 TRAIN  loss dict:  {'classification_loss': 0.9584751915931702}
2025-01-16 16:40:16,124 [INFO] Step[2300/2713]: training loss : 0.9948162639141083 TRAIN  loss dict:  {'classification_loss': 0.9948162639141083}
2025-01-16 16:40:32,428 [INFO] Step[2350/2713]: training loss : 0.9815354478359223 TRAIN  loss dict:  {'classification_loss': 0.9815354478359223}
2025-01-16 16:40:48,767 [INFO] Step[2400/2713]: training loss : 0.9904047882556916 TRAIN  loss dict:  {'classification_loss': 0.9904047882556916}
2025-01-16 16:41:05,117 [INFO] Step[2450/2713]: training loss : 0.9696545040607453 TRAIN  loss dict:  {'classification_loss': 0.9696545040607453}
2025-01-16 16:41:21,393 [INFO] Step[2500/2713]: training loss : 0.9734973657131195 TRAIN  loss dict:  {'classification_loss': 0.9734973657131195}
2025-01-16 16:41:37,691 [INFO] Step[2550/2713]: training loss : 0.9996682059764862 TRAIN  loss dict:  {'classification_loss': 0.9996682059764862}
2025-01-16 16:41:54,019 [INFO] Step[2600/2713]: training loss : 1.0207234954833984 TRAIN  loss dict:  {'classification_loss': 1.0207234954833984}
2025-01-16 16:42:10,304 [INFO] Step[2650/2713]: training loss : 0.9603831398487092 TRAIN  loss dict:  {'classification_loss': 0.9603831398487092}
2025-01-16 16:42:26,518 [INFO] Step[2700/2713]: training loss : 0.9687312507629394 TRAIN  loss dict:  {'classification_loss': 0.9687312507629394}
2025-01-16 16:43:46,190 [INFO] Label accuracies statistics:
2025-01-16 16:43:46,190 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.5, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 1.0, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.25, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.5, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 0.75, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 0.75, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 1.0, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.5, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.75, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 1.0, 372: 1.0, 373: 1.0, 374: 0.75, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.0, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 16:43:46,192 [INFO] [24] TRAIN  loss: 0.9823035635809331 acc: 0.9904165130851456
2025-01-16 16:43:46,192 [INFO] [24] TRAIN  loss dict: {'classification_loss': 0.9823035635809331}
2025-01-16 16:43:46,192 [INFO] [24] VALIDATION loss: 1.8435511779516263 VALIDATION acc: 0.7836990595611285
2025-01-16 16:43:46,192 [INFO] [24] VALIDATION loss dict: {'classification_loss': 1.8435511779516263}
2025-01-16 16:43:46,192 [INFO] 
2025-01-16 16:44:07,859 [INFO] Step[50/2713]: training loss : 0.9573770999908447 TRAIN  loss dict:  {'classification_loss': 0.9573770999908447}
2025-01-16 16:44:24,200 [INFO] Step[100/2713]: training loss : 0.9589964306354523 TRAIN  loss dict:  {'classification_loss': 0.9589964306354523}
2025-01-16 16:44:40,463 [INFO] Step[150/2713]: training loss : 0.9791914677619934 TRAIN  loss dict:  {'classification_loss': 0.9791914677619934}
2025-01-16 16:44:56,751 [INFO] Step[200/2713]: training loss : 0.9998085653781891 TRAIN  loss dict:  {'classification_loss': 0.9998085653781891}
2025-01-16 16:45:13,104 [INFO] Step[250/2713]: training loss : 0.9592040586471557 TRAIN  loss dict:  {'classification_loss': 0.9592040586471557}
2025-01-16 16:45:29,439 [INFO] Step[300/2713]: training loss : 0.9643044316768646 TRAIN  loss dict:  {'classification_loss': 0.9643044316768646}
2025-01-16 16:45:45,796 [INFO] Step[350/2713]: training loss : 0.9767333352565766 TRAIN  loss dict:  {'classification_loss': 0.9767333352565766}
2025-01-16 16:46:02,205 [INFO] Step[400/2713]: training loss : 0.9669511151313782 TRAIN  loss dict:  {'classification_loss': 0.9669511151313782}
2025-01-16 16:46:18,540 [INFO] Step[450/2713]: training loss : 0.9498455238342285 TRAIN  loss dict:  {'classification_loss': 0.9498455238342285}
2025-01-16 16:46:34,844 [INFO] Step[500/2713]: training loss : 0.9845332455635071 TRAIN  loss dict:  {'classification_loss': 0.9845332455635071}
2025-01-16 16:46:51,299 [INFO] Step[550/2713]: training loss : 0.9574929261207581 TRAIN  loss dict:  {'classification_loss': 0.9574929261207581}
2025-01-16 16:47:07,574 [INFO] Step[600/2713]: training loss : 0.9907858097553253 TRAIN  loss dict:  {'classification_loss': 0.9907858097553253}
2025-01-16 16:47:23,863 [INFO] Step[650/2713]: training loss : 0.9790234959125519 TRAIN  loss dict:  {'classification_loss': 0.9790234959125519}
2025-01-16 16:47:40,170 [INFO] Step[700/2713]: training loss : 0.9563005268573761 TRAIN  loss dict:  {'classification_loss': 0.9563005268573761}
2025-01-16 16:47:56,564 [INFO] Step[750/2713]: training loss : 0.9871812736988068 TRAIN  loss dict:  {'classification_loss': 0.9871812736988068}
2025-01-16 16:48:12,903 [INFO] Step[800/2713]: training loss : 0.9907668006420135 TRAIN  loss dict:  {'classification_loss': 0.9907668006420135}
2025-01-16 16:48:29,213 [INFO] Step[850/2713]: training loss : 0.9818005204200745 TRAIN  loss dict:  {'classification_loss': 0.9818005204200745}
2025-01-16 16:48:45,505 [INFO] Step[900/2713]: training loss : 0.9956740808486938 TRAIN  loss dict:  {'classification_loss': 0.9956740808486938}
2025-01-16 16:49:01,873 [INFO] Step[950/2713]: training loss : 0.9616056931018829 TRAIN  loss dict:  {'classification_loss': 0.9616056931018829}
2025-01-16 16:49:18,187 [INFO] Step[1000/2713]: training loss : 0.9934767687320709 TRAIN  loss dict:  {'classification_loss': 0.9934767687320709}
2025-01-16 16:49:34,543 [INFO] Step[1050/2713]: training loss : 0.9775441837310791 TRAIN  loss dict:  {'classification_loss': 0.9775441837310791}
2025-01-16 16:49:50,759 [INFO] Step[1100/2713]: training loss : 0.9669188010692596 TRAIN  loss dict:  {'classification_loss': 0.9669188010692596}
2025-01-16 16:50:07,135 [INFO] Step[1150/2713]: training loss : 1.018246225118637 TRAIN  loss dict:  {'classification_loss': 1.018246225118637}
2025-01-16 16:50:23,432 [INFO] Step[1200/2713]: training loss : 0.9719371402263641 TRAIN  loss dict:  {'classification_loss': 0.9719371402263641}
2025-01-16 16:50:39,819 [INFO] Step[1250/2713]: training loss : 1.0183397912979126 TRAIN  loss dict:  {'classification_loss': 1.0183397912979126}
2025-01-16 16:50:56,185 [INFO] Step[1300/2713]: training loss : 0.9765087306499481 TRAIN  loss dict:  {'classification_loss': 0.9765087306499481}
2025-01-16 16:51:12,518 [INFO] Step[1350/2713]: training loss : 0.9709357964992523 TRAIN  loss dict:  {'classification_loss': 0.9709357964992523}
2025-01-16 16:51:28,878 [INFO] Step[1400/2713]: training loss : 1.0196102905273436 TRAIN  loss dict:  {'classification_loss': 1.0196102905273436}
2025-01-16 16:51:45,171 [INFO] Step[1450/2713]: training loss : 0.9781432318687439 TRAIN  loss dict:  {'classification_loss': 0.9781432318687439}
2025-01-16 16:52:01,491 [INFO] Step[1500/2713]: training loss : 1.0004430031776428 TRAIN  loss dict:  {'classification_loss': 1.0004430031776428}
2025-01-16 16:52:17,779 [INFO] Step[1550/2713]: training loss : 1.0048376023769379 TRAIN  loss dict:  {'classification_loss': 1.0048376023769379}
2025-01-16 16:52:34,098 [INFO] Step[1600/2713]: training loss : 0.9595158743858337 TRAIN  loss dict:  {'classification_loss': 0.9595158743858337}
2025-01-16 16:52:50,369 [INFO] Step[1650/2713]: training loss : 0.9840087902545929 TRAIN  loss dict:  {'classification_loss': 0.9840087902545929}
2025-01-16 16:53:06,668 [INFO] Step[1700/2713]: training loss : 0.9690886056423187 TRAIN  loss dict:  {'classification_loss': 0.9690886056423187}
2025-01-16 16:53:23,048 [INFO] Step[1750/2713]: training loss : 0.9834213519096374 TRAIN  loss dict:  {'classification_loss': 0.9834213519096374}
2025-01-16 16:53:39,385 [INFO] Step[1800/2713]: training loss : 1.0041609072685242 TRAIN  loss dict:  {'classification_loss': 1.0041609072685242}
2025-01-16 16:53:55,597 [INFO] Step[1850/2713]: training loss : 1.0357922637462615 TRAIN  loss dict:  {'classification_loss': 1.0357922637462615}
2025-01-16 16:54:11,885 [INFO] Step[1900/2713]: training loss : 1.0099176347255707 TRAIN  loss dict:  {'classification_loss': 1.0099176347255707}
2025-01-16 16:54:28,199 [INFO] Step[1950/2713]: training loss : 0.9883649063110351 TRAIN  loss dict:  {'classification_loss': 0.9883649063110351}
2025-01-16 16:54:44,547 [INFO] Step[2000/2713]: training loss : 1.0388869631290436 TRAIN  loss dict:  {'classification_loss': 1.0388869631290436}
2025-01-16 16:55:00,890 [INFO] Step[2050/2713]: training loss : 0.9770104241371155 TRAIN  loss dict:  {'classification_loss': 0.9770104241371155}
2025-01-16 16:55:17,119 [INFO] Step[2100/2713]: training loss : 0.9741725051403045 TRAIN  loss dict:  {'classification_loss': 0.9741725051403045}
2025-01-16 16:55:33,422 [INFO] Step[2150/2713]: training loss : 1.0085019755363465 TRAIN  loss dict:  {'classification_loss': 1.0085019755363465}
2025-01-16 16:55:49,714 [INFO] Step[2200/2713]: training loss : 0.9867377769947052 TRAIN  loss dict:  {'classification_loss': 0.9867377769947052}
2025-01-16 16:56:06,003 [INFO] Step[2250/2713]: training loss : 0.9842258250713348 TRAIN  loss dict:  {'classification_loss': 0.9842258250713348}
2025-01-16 16:56:22,337 [INFO] Step[2300/2713]: training loss : 0.9466915583610535 TRAIN  loss dict:  {'classification_loss': 0.9466915583610535}
2025-01-16 16:56:38,650 [INFO] Step[2350/2713]: training loss : 0.9555916273593903 TRAIN  loss dict:  {'classification_loss': 0.9555916273593903}
2025-01-16 16:56:54,845 [INFO] Step[2400/2713]: training loss : 0.9729546904563904 TRAIN  loss dict:  {'classification_loss': 0.9729546904563904}
2025-01-16 16:57:11,088 [INFO] Step[2450/2713]: training loss : 0.9628343939781189 TRAIN  loss dict:  {'classification_loss': 0.9628343939781189}
2025-01-16 16:57:27,224 [INFO] Step[2500/2713]: training loss : 1.014375057220459 TRAIN  loss dict:  {'classification_loss': 1.014375057220459}
2025-01-16 16:57:43,304 [INFO] Step[2550/2713]: training loss : 0.9656780779361724 TRAIN  loss dict:  {'classification_loss': 0.9656780779361724}
2025-01-16 16:57:59,462 [INFO] Step[2600/2713]: training loss : 0.9721530187129974 TRAIN  loss dict:  {'classification_loss': 0.9721530187129974}
2025-01-16 16:58:15,644 [INFO] Step[2650/2713]: training loss : 0.9940747892856598 TRAIN  loss dict:  {'classification_loss': 0.9940747892856598}
2025-01-16 16:58:31,726 [INFO] Step[2700/2713]: training loss : 1.0011801183223725 TRAIN  loss dict:  {'classification_loss': 1.0011801183223725}
2025-01-16 16:59:50,887 [INFO] Label accuracies statistics:
2025-01-16 16:59:50,888 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 1.0, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.5, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.5, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.25, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.5, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 0.75, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 0.5, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 1.0, 211: 0.5, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 0.5, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 0.75, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 0.75, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.25, 365: 0.75, 366: 1.0, 367: 0.5, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.5, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 16:59:52,340 [INFO] [25] TRAIN  loss: 0.9830732248693298 acc: 0.9898021870008601
2025-01-16 16:59:52,340 [INFO] [25] TRAIN  loss dict: {'classification_loss': 0.9830732248693298}
2025-01-16 16:59:52,340 [INFO] [25] VALIDATION loss: 1.8351581231095737 VALIDATION acc: 0.7931034482758621
2025-01-16 16:59:52,341 [INFO] [25] VALIDATION loss dict: {'classification_loss': 1.8351581231095737}
2025-01-16 16:59:52,341 [INFO] 
2025-01-16 17:00:13,385 [INFO] Step[50/2713]: training loss : 0.9683318698406219 TRAIN  loss dict:  {'classification_loss': 0.9683318698406219}
2025-01-16 17:00:29,412 [INFO] Step[100/2713]: training loss : 0.9987752592563629 TRAIN  loss dict:  {'classification_loss': 0.9987752592563629}
2025-01-16 17:00:45,583 [INFO] Step[150/2713]: training loss : 0.9630292975902557 TRAIN  loss dict:  {'classification_loss': 0.9630292975902557}
2025-01-16 17:01:01,686 [INFO] Step[200/2713]: training loss : 1.005745508670807 TRAIN  loss dict:  {'classification_loss': 1.005745508670807}
2025-01-16 17:01:17,776 [INFO] Step[250/2713]: training loss : 0.9711679971218109 TRAIN  loss dict:  {'classification_loss': 0.9711679971218109}
2025-01-16 17:01:33,925 [INFO] Step[300/2713]: training loss : 1.0030634570121766 TRAIN  loss dict:  {'classification_loss': 1.0030634570121766}
2025-01-16 17:01:50,022 [INFO] Step[350/2713]: training loss : 1.0073810303211213 TRAIN  loss dict:  {'classification_loss': 1.0073810303211213}
2025-01-16 17:02:06,116 [INFO] Step[400/2713]: training loss : 1.0045806980133056 TRAIN  loss dict:  {'classification_loss': 1.0045806980133056}
2025-01-16 17:02:22,277 [INFO] Step[450/2713]: training loss : 0.9573007488250732 TRAIN  loss dict:  {'classification_loss': 0.9573007488250732}
2025-01-16 17:02:38,381 [INFO] Step[500/2713]: training loss : 0.9602417886257172 TRAIN  loss dict:  {'classification_loss': 0.9602417886257172}
2025-01-16 17:02:54,549 [INFO] Step[550/2713]: training loss : 0.9559444916248322 TRAIN  loss dict:  {'classification_loss': 0.9559444916248322}
2025-01-16 17:03:10,663 [INFO] Step[600/2713]: training loss : 0.9576783037185669 TRAIN  loss dict:  {'classification_loss': 0.9576783037185669}
2025-01-16 17:03:26,786 [INFO] Step[650/2713]: training loss : 1.0300294876098632 TRAIN  loss dict:  {'classification_loss': 1.0300294876098632}
2025-01-16 17:03:42,941 [INFO] Step[700/2713]: training loss : 0.961164984703064 TRAIN  loss dict:  {'classification_loss': 0.961164984703064}
2025-01-16 17:03:59,084 [INFO] Step[750/2713]: training loss : 0.9567431497573853 TRAIN  loss dict:  {'classification_loss': 0.9567431497573853}
2025-01-16 17:04:15,239 [INFO] Step[800/2713]: training loss : 0.9568163943290711 TRAIN  loss dict:  {'classification_loss': 0.9568163943290711}
2025-01-16 17:04:31,381 [INFO] Step[850/2713]: training loss : 0.9622045397758484 TRAIN  loss dict:  {'classification_loss': 0.9622045397758484}
2025-01-16 17:04:47,532 [INFO] Step[900/2713]: training loss : 0.9685012578964234 TRAIN  loss dict:  {'classification_loss': 0.9685012578964234}
2025-01-16 17:05:03,738 [INFO] Step[950/2713]: training loss : 0.9638493084907531 TRAIN  loss dict:  {'classification_loss': 0.9638493084907531}
2025-01-16 17:05:19,867 [INFO] Step[1000/2713]: training loss : 1.0754197788238526 TRAIN  loss dict:  {'classification_loss': 1.0754197788238526}
2025-01-16 17:05:35,991 [INFO] Step[1050/2713]: training loss : 0.9644273018836975 TRAIN  loss dict:  {'classification_loss': 0.9644273018836975}
2025-01-16 17:05:52,161 [INFO] Step[1100/2713]: training loss : 0.986162736415863 TRAIN  loss dict:  {'classification_loss': 0.986162736415863}
2025-01-16 17:06:08,362 [INFO] Step[1150/2713]: training loss : 0.9629268026351929 TRAIN  loss dict:  {'classification_loss': 0.9629268026351929}
2025-01-16 17:06:24,572 [INFO] Step[1200/2713]: training loss : 0.9661040878295899 TRAIN  loss dict:  {'classification_loss': 0.9661040878295899}
2025-01-16 17:06:40,639 [INFO] Step[1250/2713]: training loss : 1.0142538058757782 TRAIN  loss dict:  {'classification_loss': 1.0142538058757782}
2025-01-16 17:06:56,806 [INFO] Step[1300/2713]: training loss : 0.9638224875926972 TRAIN  loss dict:  {'classification_loss': 0.9638224875926972}
2025-01-16 17:07:12,933 [INFO] Step[1350/2713]: training loss : 0.9905284082889557 TRAIN  loss dict:  {'classification_loss': 0.9905284082889557}
2025-01-16 17:07:29,113 [INFO] Step[1400/2713]: training loss : 1.0072883212566375 TRAIN  loss dict:  {'classification_loss': 1.0072883212566375}
2025-01-16 17:07:45,277 [INFO] Step[1450/2713]: training loss : 0.9690363228321075 TRAIN  loss dict:  {'classification_loss': 0.9690363228321075}
2025-01-16 17:08:01,444 [INFO] Step[1500/2713]: training loss : 0.9762976396083832 TRAIN  loss dict:  {'classification_loss': 0.9762976396083832}
2025-01-16 17:08:17,615 [INFO] Step[1550/2713]: training loss : 1.0005766904354096 TRAIN  loss dict:  {'classification_loss': 1.0005766904354096}
2025-01-16 17:08:33,737 [INFO] Step[1600/2713]: training loss : 0.9680153095722198 TRAIN  loss dict:  {'classification_loss': 0.9680153095722198}
2025-01-16 17:08:49,858 [INFO] Step[1650/2713]: training loss : 0.9869446349143982 TRAIN  loss dict:  {'classification_loss': 0.9869446349143982}
2025-01-16 17:09:06,025 [INFO] Step[1700/2713]: training loss : 0.9792852163314819 TRAIN  loss dict:  {'classification_loss': 0.9792852163314819}
2025-01-16 17:09:22,179 [INFO] Step[1750/2713]: training loss : 0.9621461665630341 TRAIN  loss dict:  {'classification_loss': 0.9621461665630341}
2025-01-16 17:09:38,340 [INFO] Step[1800/2713]: training loss : 0.9698383784294129 TRAIN  loss dict:  {'classification_loss': 0.9698383784294129}
2025-01-16 17:09:54,470 [INFO] Step[1850/2713]: training loss : 1.0265034234523773 TRAIN  loss dict:  {'classification_loss': 1.0265034234523773}
2025-01-16 17:10:10,592 [INFO] Step[1900/2713]: training loss : 1.0151582658290863 TRAIN  loss dict:  {'classification_loss': 1.0151582658290863}
2025-01-16 17:10:26,758 [INFO] Step[1950/2713]: training loss : 1.0093545961380004 TRAIN  loss dict:  {'classification_loss': 1.0093545961380004}
2025-01-16 17:10:42,869 [INFO] Step[2000/2713]: training loss : 0.9982735979557037 TRAIN  loss dict:  {'classification_loss': 0.9982735979557037}
2025-01-16 17:10:59,011 [INFO] Step[2050/2713]: training loss : 1.005798876285553 TRAIN  loss dict:  {'classification_loss': 1.005798876285553}
2025-01-16 17:11:15,189 [INFO] Step[2100/2713]: training loss : 0.982335547208786 TRAIN  loss dict:  {'classification_loss': 0.982335547208786}
2025-01-16 17:11:31,385 [INFO] Step[2150/2713]: training loss : 0.973770956993103 TRAIN  loss dict:  {'classification_loss': 0.973770956993103}
2025-01-16 17:11:47,571 [INFO] Step[2200/2713]: training loss : 0.9557205390930176 TRAIN  loss dict:  {'classification_loss': 0.9557205390930176}
2025-01-16 17:12:03,720 [INFO] Step[2250/2713]: training loss : 0.9626720452308655 TRAIN  loss dict:  {'classification_loss': 0.9626720452308655}
2025-01-16 17:12:19,868 [INFO] Step[2300/2713]: training loss : 1.0034455966949463 TRAIN  loss dict:  {'classification_loss': 1.0034455966949463}
2025-01-16 17:12:36,048 [INFO] Step[2350/2713]: training loss : 0.9882339286804199 TRAIN  loss dict:  {'classification_loss': 0.9882339286804199}
2025-01-16 17:12:52,100 [INFO] Step[2400/2713]: training loss : 0.98834343791008 TRAIN  loss dict:  {'classification_loss': 0.98834343791008}
2025-01-16 17:13:08,335 [INFO] Step[2450/2713]: training loss : 0.9740279233455658 TRAIN  loss dict:  {'classification_loss': 0.9740279233455658}
2025-01-16 17:13:24,433 [INFO] Step[2500/2713]: training loss : 0.9996441638469696 TRAIN  loss dict:  {'classification_loss': 0.9996441638469696}
2025-01-16 17:13:40,602 [INFO] Step[2550/2713]: training loss : 1.012479019165039 TRAIN  loss dict:  {'classification_loss': 1.012479019165039}
2025-01-16 17:13:56,711 [INFO] Step[2600/2713]: training loss : 0.974330507516861 TRAIN  loss dict:  {'classification_loss': 0.974330507516861}
2025-01-16 17:14:12,864 [INFO] Step[2650/2713]: training loss : 0.9556135821342469 TRAIN  loss dict:  {'classification_loss': 0.9556135821342469}
2025-01-16 17:14:29,095 [INFO] Step[2700/2713]: training loss : 0.9880983412265778 TRAIN  loss dict:  {'classification_loss': 0.9880983412265778}
2025-01-16 17:15:48,119 [INFO] Label accuracies statistics:
2025-01-16 17:15:48,119 [INFO] {0: 0.6666666666666666, 1: 0.6666666666666666, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.5, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.25, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.5, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.75, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 1.0, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 0.5, 235: 1.0, 236: 1.0, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.0, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.5, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 1.0, 261: 0.5, 262: 1.0, 263: 0.75, 264: 1.0, 265: 0.75, 266: 0.5, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.5, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 0.75, 327: 1.0, 328: 0.25, 329: 0.5, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.25, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 17:15:48,121 [INFO] [26] TRAIN  loss: 0.9835452418946007 acc: 0.9893107261334316
2025-01-16 17:15:48,121 [INFO] [26] TRAIN  loss dict: {'classification_loss': 0.9835452418946007}
2025-01-16 17:15:48,121 [INFO] [26] VALIDATION loss: 1.827803795499013 VALIDATION acc: 0.7862068965517242
2025-01-16 17:15:48,121 [INFO] [26] VALIDATION loss dict: {'classification_loss': 1.827803795499013}
2025-01-16 17:15:48,121 [INFO] 
2025-01-16 17:16:09,255 [INFO] Step[50/2713]: training loss : 0.9625657653808594 TRAIN  loss dict:  {'classification_loss': 0.9625657653808594}
2025-01-16 17:16:25,419 [INFO] Step[100/2713]: training loss : 0.9673368632793427 TRAIN  loss dict:  {'classification_loss': 0.9673368632793427}
2025-01-16 17:16:41,558 [INFO] Step[150/2713]: training loss : 0.9676743805408478 TRAIN  loss dict:  {'classification_loss': 0.9676743805408478}
2025-01-16 17:16:57,763 [INFO] Step[200/2713]: training loss : 0.9655345928668976 TRAIN  loss dict:  {'classification_loss': 0.9655345928668976}
2025-01-16 17:17:13,865 [INFO] Step[250/2713]: training loss : 0.9871197199821472 TRAIN  loss dict:  {'classification_loss': 0.9871197199821472}
2025-01-16 17:17:30,004 [INFO] Step[300/2713]: training loss : 1.0184297609329223 TRAIN  loss dict:  {'classification_loss': 1.0184297609329223}
2025-01-16 17:17:46,176 [INFO] Step[350/2713]: training loss : 0.956148374080658 TRAIN  loss dict:  {'classification_loss': 0.956148374080658}
2025-01-16 17:18:02,347 [INFO] Step[400/2713]: training loss : 0.9913848161697387 TRAIN  loss dict:  {'classification_loss': 0.9913848161697387}
2025-01-16 17:18:18,543 [INFO] Step[450/2713]: training loss : 0.9805210459232331 TRAIN  loss dict:  {'classification_loss': 0.9805210459232331}
2025-01-16 17:18:34,700 [INFO] Step[500/2713]: training loss : 1.0137693309783935 TRAIN  loss dict:  {'classification_loss': 1.0137693309783935}
2025-01-16 17:18:50,898 [INFO] Step[550/2713]: training loss : 1.0113887226581573 TRAIN  loss dict:  {'classification_loss': 1.0113887226581573}
2025-01-16 17:19:07,078 [INFO] Step[600/2713]: training loss : 0.9608009338378907 TRAIN  loss dict:  {'classification_loss': 0.9608009338378907}
2025-01-16 17:19:23,345 [INFO] Step[650/2713]: training loss : 1.0150369882583619 TRAIN  loss dict:  {'classification_loss': 1.0150369882583619}
2025-01-16 17:19:39,566 [INFO] Step[700/2713]: training loss : 0.9673829472064972 TRAIN  loss dict:  {'classification_loss': 0.9673829472064972}
2025-01-16 17:19:55,807 [INFO] Step[750/2713]: training loss : 0.9646560204029083 TRAIN  loss dict:  {'classification_loss': 0.9646560204029083}
2025-01-16 17:20:11,944 [INFO] Step[800/2713]: training loss : 0.964087245464325 TRAIN  loss dict:  {'classification_loss': 0.964087245464325}
2025-01-16 17:20:28,171 [INFO] Step[850/2713]: training loss : 0.9824410808086396 TRAIN  loss dict:  {'classification_loss': 0.9824410808086396}
2025-01-16 17:20:44,340 [INFO] Step[900/2713]: training loss : 0.9835846173763275 TRAIN  loss dict:  {'classification_loss': 0.9835846173763275}
2025-01-16 17:21:00,494 [INFO] Step[950/2713]: training loss : 1.0048777556419373 TRAIN  loss dict:  {'classification_loss': 1.0048777556419373}
2025-01-16 17:21:16,583 [INFO] Step[1000/2713]: training loss : 0.9790124189853668 TRAIN  loss dict:  {'classification_loss': 0.9790124189853668}
2025-01-16 17:21:32,783 [INFO] Step[1050/2713]: training loss : 0.9911560106277466 TRAIN  loss dict:  {'classification_loss': 0.9911560106277466}
2025-01-16 17:21:48,958 [INFO] Step[1100/2713]: training loss : 0.9858606052398682 TRAIN  loss dict:  {'classification_loss': 0.9858606052398682}
2025-01-16 17:22:05,188 [INFO] Step[1150/2713]: training loss : 0.9599803519248963 TRAIN  loss dict:  {'classification_loss': 0.9599803519248963}
2025-01-16 17:22:21,334 [INFO] Step[1200/2713]: training loss : 0.9874054360389709 TRAIN  loss dict:  {'classification_loss': 0.9874054360389709}
2025-01-16 17:22:37,501 [INFO] Step[1250/2713]: training loss : 0.9980334174633027 TRAIN  loss dict:  {'classification_loss': 0.9980334174633027}
2025-01-16 17:22:53,602 [INFO] Step[1300/2713]: training loss : 0.9899943172931671 TRAIN  loss dict:  {'classification_loss': 0.9899943172931671}
2025-01-16 17:23:09,793 [INFO] Step[1350/2713]: training loss : 0.9713300538063049 TRAIN  loss dict:  {'classification_loss': 0.9713300538063049}
2025-01-16 17:23:25,941 [INFO] Step[1400/2713]: training loss : 0.9555673325061798 TRAIN  loss dict:  {'classification_loss': 0.9555673325061798}
2025-01-16 17:23:42,145 [INFO] Step[1450/2713]: training loss : 0.9667154407501221 TRAIN  loss dict:  {'classification_loss': 0.9667154407501221}
2025-01-16 17:23:58,322 [INFO] Step[1500/2713]: training loss : 0.9730111229419708 TRAIN  loss dict:  {'classification_loss': 0.9730111229419708}
2025-01-16 17:24:14,469 [INFO] Step[1550/2713]: training loss : 0.9657419347763061 TRAIN  loss dict:  {'classification_loss': 0.9657419347763061}
2025-01-16 17:24:30,603 [INFO] Step[1600/2713]: training loss : 0.9694331526756287 TRAIN  loss dict:  {'classification_loss': 0.9694331526756287}
2025-01-16 17:24:46,822 [INFO] Step[1650/2713]: training loss : 0.9700170707702637 TRAIN  loss dict:  {'classification_loss': 0.9700170707702637}
2025-01-16 17:25:03,021 [INFO] Step[1700/2713]: training loss : 1.009879505634308 TRAIN  loss dict:  {'classification_loss': 1.009879505634308}
2025-01-16 17:25:19,200 [INFO] Step[1750/2713]: training loss : 0.9831837701797486 TRAIN  loss dict:  {'classification_loss': 0.9831837701797486}
2025-01-16 17:25:35,383 [INFO] Step[1800/2713]: training loss : 0.9726281702518463 TRAIN  loss dict:  {'classification_loss': 0.9726281702518463}
2025-01-16 17:25:51,608 [INFO] Step[1850/2713]: training loss : 0.9998190069198608 TRAIN  loss dict:  {'classification_loss': 0.9998190069198608}
2025-01-16 17:26:07,787 [INFO] Step[1900/2713]: training loss : 0.9542827582359314 TRAIN  loss dict:  {'classification_loss': 0.9542827582359314}
2025-01-16 17:26:23,985 [INFO] Step[1950/2713]: training loss : 0.9720610225200653 TRAIN  loss dict:  {'classification_loss': 0.9720610225200653}
2025-01-16 17:26:40,172 [INFO] Step[2000/2713]: training loss : 0.9652851521968842 TRAIN  loss dict:  {'classification_loss': 0.9652851521968842}
2025-01-16 17:26:56,403 [INFO] Step[2050/2713]: training loss : 0.976276490688324 TRAIN  loss dict:  {'classification_loss': 0.976276490688324}
2025-01-16 17:27:12,521 [INFO] Step[2100/2713]: training loss : 0.9571589803695679 TRAIN  loss dict:  {'classification_loss': 0.9571589803695679}
2025-01-16 17:27:28,700 [INFO] Step[2150/2713]: training loss : 0.9757537853717804 TRAIN  loss dict:  {'classification_loss': 0.9757537853717804}
2025-01-16 17:27:44,938 [INFO] Step[2200/2713]: training loss : 0.99412815451622 TRAIN  loss dict:  {'classification_loss': 0.99412815451622}
2025-01-16 17:28:01,139 [INFO] Step[2250/2713]: training loss : 0.9505724310874939 TRAIN  loss dict:  {'classification_loss': 0.9505724310874939}
2025-01-16 17:28:17,392 [INFO] Step[2300/2713]: training loss : 0.9624489641189575 TRAIN  loss dict:  {'classification_loss': 0.9624489641189575}
2025-01-16 17:28:33,574 [INFO] Step[2350/2713]: training loss : 0.9637883996963501 TRAIN  loss dict:  {'classification_loss': 0.9637883996963501}
2025-01-16 17:28:49,722 [INFO] Step[2400/2713]: training loss : 0.9631651103496551 TRAIN  loss dict:  {'classification_loss': 0.9631651103496551}
2025-01-16 17:29:06,019 [INFO] Step[2450/2713]: training loss : 0.9607106065750122 TRAIN  loss dict:  {'classification_loss': 0.9607106065750122}
2025-01-16 17:29:22,204 [INFO] Step[2500/2713]: training loss : 1.0348020350933076 TRAIN  loss dict:  {'classification_loss': 1.0348020350933076}
2025-01-16 17:29:38,410 [INFO] Step[2550/2713]: training loss : 0.9502292454242707 TRAIN  loss dict:  {'classification_loss': 0.9502292454242707}
2025-01-16 17:29:54,574 [INFO] Step[2600/2713]: training loss : 0.9649110352993011 TRAIN  loss dict:  {'classification_loss': 0.9649110352993011}
2025-01-16 17:30:10,852 [INFO] Step[2650/2713]: training loss : 0.9889291799068451 TRAIN  loss dict:  {'classification_loss': 0.9889291799068451}
2025-01-16 17:30:27,030 [INFO] Step[2700/2713]: training loss : 0.9749970996379852 TRAIN  loss dict:  {'classification_loss': 0.9749970996379852}
2025-01-16 17:31:46,312 [INFO] Label accuracies statistics:
2025-01-16 17:31:46,312 [INFO] {0: 0.6666666666666666, 1: 0.6666666666666666, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.5, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 1.0, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.5, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.5, 88: 0.75, 89: 1.0, 90: 0.5, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 0.5, 144: 0.75, 145: 0.75, 146: 0.5, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.5, 218: 1.0, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.25, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.5, 251: 0.75, 252: 0.5, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 1.0, 262: 0.75, 263: 1.0, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 0.5, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 1.0, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.25, 353: 0.25, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.0, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 17:31:46,314 [INFO] [27] TRAIN  loss: 0.9777464469200652 acc: 0.9894335913502887
2025-01-16 17:31:46,314 [INFO] [27] TRAIN  loss dict: {'classification_loss': 0.9777464469200652}
2025-01-16 17:31:46,314 [INFO] [27] VALIDATION loss: 1.8544481254385827 VALIDATION acc: 0.7761755485893417
2025-01-16 17:31:46,314 [INFO] [27] VALIDATION loss dict: {'classification_loss': 1.8544481254385827}
2025-01-16 17:31:46,314 [INFO] 
2025-01-16 17:32:08,133 [INFO] Step[50/2713]: training loss : 0.9983899915218353 TRAIN  loss dict:  {'classification_loss': 0.9983899915218353}
2025-01-16 17:32:24,305 [INFO] Step[100/2713]: training loss : 1.0533044707775117 TRAIN  loss dict:  {'classification_loss': 1.0533044707775117}
2025-01-16 17:32:40,499 [INFO] Step[150/2713]: training loss : 0.9617936813831329 TRAIN  loss dict:  {'classification_loss': 0.9617936813831329}
2025-01-16 17:32:56,707 [INFO] Step[200/2713]: training loss : 0.9969019174575806 TRAIN  loss dict:  {'classification_loss': 0.9969019174575806}
2025-01-16 17:33:12,811 [INFO] Step[250/2713]: training loss : 0.9786342501640319 TRAIN  loss dict:  {'classification_loss': 0.9786342501640319}
2025-01-16 17:33:28,986 [INFO] Step[300/2713]: training loss : 0.9519639074802398 TRAIN  loss dict:  {'classification_loss': 0.9519639074802398}
2025-01-16 17:33:45,140 [INFO] Step[350/2713]: training loss : 0.970342481136322 TRAIN  loss dict:  {'classification_loss': 0.970342481136322}
2025-01-16 17:34:01,297 [INFO] Step[400/2713]: training loss : 0.9638878989219666 TRAIN  loss dict:  {'classification_loss': 0.9638878989219666}
2025-01-16 17:34:17,366 [INFO] Step[450/2713]: training loss : 0.9581552708148956 TRAIN  loss dict:  {'classification_loss': 0.9581552708148956}
2025-01-16 17:34:33,530 [INFO] Step[500/2713]: training loss : 0.952895427942276 TRAIN  loss dict:  {'classification_loss': 0.952895427942276}
2025-01-16 17:34:49,655 [INFO] Step[550/2713]: training loss : 0.9516734135150909 TRAIN  loss dict:  {'classification_loss': 0.9516734135150909}
2025-01-16 17:35:05,783 [INFO] Step[600/2713]: training loss : 0.9541818249225616 TRAIN  loss dict:  {'classification_loss': 0.9541818249225616}
2025-01-16 17:35:21,964 [INFO] Step[650/2713]: training loss : 1.0025119507312774 TRAIN  loss dict:  {'classification_loss': 1.0025119507312774}
2025-01-16 17:35:38,062 [INFO] Step[700/2713]: training loss : 0.9555867457389832 TRAIN  loss dict:  {'classification_loss': 0.9555867457389832}
2025-01-16 17:35:54,190 [INFO] Step[750/2713]: training loss : 1.0235461437702178 TRAIN  loss dict:  {'classification_loss': 1.0235461437702178}
2025-01-16 17:36:10,315 [INFO] Step[800/2713]: training loss : 0.9535836613178253 TRAIN  loss dict:  {'classification_loss': 0.9535836613178253}
2025-01-16 17:36:26,438 [INFO] Step[850/2713]: training loss : 0.9521427917480468 TRAIN  loss dict:  {'classification_loss': 0.9521427917480468}
2025-01-16 17:36:42,512 [INFO] Step[900/2713]: training loss : 0.9598719596862793 TRAIN  loss dict:  {'classification_loss': 0.9598719596862793}
2025-01-16 17:36:58,657 [INFO] Step[950/2713]: training loss : 0.9796599173545837 TRAIN  loss dict:  {'classification_loss': 0.9796599173545837}
2025-01-16 17:37:14,729 [INFO] Step[1000/2713]: training loss : 0.9523661887645721 TRAIN  loss dict:  {'classification_loss': 0.9523661887645721}
2025-01-16 17:37:30,860 [INFO] Step[1050/2713]: training loss : 0.9592407166957855 TRAIN  loss dict:  {'classification_loss': 0.9592407166957855}
2025-01-16 17:37:47,065 [INFO] Step[1100/2713]: training loss : 0.9651043915748596 TRAIN  loss dict:  {'classification_loss': 0.9651043915748596}
2025-01-16 17:38:03,182 [INFO] Step[1150/2713]: training loss : 0.9986617171764374 TRAIN  loss dict:  {'classification_loss': 0.9986617171764374}
2025-01-16 17:38:19,299 [INFO] Step[1200/2713]: training loss : 0.9975290703773498 TRAIN  loss dict:  {'classification_loss': 0.9975290703773498}
2025-01-16 17:38:35,437 [INFO] Step[1250/2713]: training loss : 0.9697714591026306 TRAIN  loss dict:  {'classification_loss': 0.9697714591026306}
2025-01-16 17:38:51,562 [INFO] Step[1300/2713]: training loss : 0.9933272099494934 TRAIN  loss dict:  {'classification_loss': 0.9933272099494934}
2025-01-16 17:39:07,735 [INFO] Step[1350/2713]: training loss : 0.9777733719348908 TRAIN  loss dict:  {'classification_loss': 0.9777733719348908}
2025-01-16 17:39:23,843 [INFO] Step[1400/2713]: training loss : 0.953498945236206 TRAIN  loss dict:  {'classification_loss': 0.953498945236206}
2025-01-16 17:39:39,952 [INFO] Step[1450/2713]: training loss : 0.9613008284568787 TRAIN  loss dict:  {'classification_loss': 0.9613008284568787}
2025-01-16 17:39:56,102 [INFO] Step[1500/2713]: training loss : 0.9509277212619781 TRAIN  loss dict:  {'classification_loss': 0.9509277212619781}
2025-01-16 17:40:12,304 [INFO] Step[1550/2713]: training loss : 0.9666524112224579 TRAIN  loss dict:  {'classification_loss': 0.9666524112224579}
2025-01-16 17:40:28,400 [INFO] Step[1600/2713]: training loss : 1.0016837096214295 TRAIN  loss dict:  {'classification_loss': 1.0016837096214295}
2025-01-16 17:40:44,517 [INFO] Step[1650/2713]: training loss : 0.9812571513652801 TRAIN  loss dict:  {'classification_loss': 0.9812571513652801}
2025-01-16 17:41:00,657 [INFO] Step[1700/2713]: training loss : 0.9960694515705109 TRAIN  loss dict:  {'classification_loss': 0.9960694515705109}
2025-01-16 17:41:16,760 [INFO] Step[1750/2713]: training loss : 0.9613889145851136 TRAIN  loss dict:  {'classification_loss': 0.9613889145851136}
2025-01-16 17:41:32,896 [INFO] Step[1800/2713]: training loss : 0.9788493490219117 TRAIN  loss dict:  {'classification_loss': 0.9788493490219117}
2025-01-16 17:41:49,034 [INFO] Step[1850/2713]: training loss : 0.9713495290279388 TRAIN  loss dict:  {'classification_loss': 0.9713495290279388}
2025-01-16 17:42:05,108 [INFO] Step[1900/2713]: training loss : 1.022394633293152 TRAIN  loss dict:  {'classification_loss': 1.022394633293152}
2025-01-16 17:42:21,231 [INFO] Step[1950/2713]: training loss : 0.9693574273586273 TRAIN  loss dict:  {'classification_loss': 0.9693574273586273}
2025-01-16 17:42:37,329 [INFO] Step[2000/2713]: training loss : 0.9971558952331543 TRAIN  loss dict:  {'classification_loss': 0.9971558952331543}
2025-01-16 17:42:53,553 [INFO] Step[2050/2713]: training loss : 1.0392337489128112 TRAIN  loss dict:  {'classification_loss': 1.0392337489128112}
2025-01-16 17:43:09,633 [INFO] Step[2100/2713]: training loss : 0.958031997680664 TRAIN  loss dict:  {'classification_loss': 0.958031997680664}
2025-01-16 17:43:25,783 [INFO] Step[2150/2713]: training loss : 0.9499055922031403 TRAIN  loss dict:  {'classification_loss': 0.9499055922031403}
2025-01-16 17:43:41,882 [INFO] Step[2200/2713]: training loss : 0.9562749373912811 TRAIN  loss dict:  {'classification_loss': 0.9562749373912811}
2025-01-16 17:43:58,035 [INFO] Step[2250/2713]: training loss : 1.000996115207672 TRAIN  loss dict:  {'classification_loss': 1.000996115207672}
2025-01-16 17:44:14,122 [INFO] Step[2300/2713]: training loss : 0.9584927475452423 TRAIN  loss dict:  {'classification_loss': 0.9584927475452423}
2025-01-16 17:44:30,224 [INFO] Step[2350/2713]: training loss : 0.9785697722434997 TRAIN  loss dict:  {'classification_loss': 0.9785697722434997}
2025-01-16 17:44:46,391 [INFO] Step[2400/2713]: training loss : 0.9618994307518005 TRAIN  loss dict:  {'classification_loss': 0.9618994307518005}
2025-01-16 17:45:02,494 [INFO] Step[2450/2713]: training loss : 0.9688393878936767 TRAIN  loss dict:  {'classification_loss': 0.9688393878936767}
2025-01-16 17:45:18,573 [INFO] Step[2500/2713]: training loss : 0.9967130064964295 TRAIN  loss dict:  {'classification_loss': 0.9967130064964295}
2025-01-16 17:45:34,751 [INFO] Step[2550/2713]: training loss : 0.9942810726165772 TRAIN  loss dict:  {'classification_loss': 0.9942810726165772}
2025-01-16 17:45:50,804 [INFO] Step[2600/2713]: training loss : 0.9717653930187226 TRAIN  loss dict:  {'classification_loss': 0.9717653930187226}
2025-01-16 17:46:06,985 [INFO] Step[2650/2713]: training loss : 0.9559462714195252 TRAIN  loss dict:  {'classification_loss': 0.9559462714195252}
2025-01-16 17:46:23,101 [INFO] Step[2700/2713]: training loss : 0.9632199776172637 TRAIN  loss dict:  {'classification_loss': 0.9632199776172637}
2025-01-16 17:47:41,591 [INFO] Label accuracies statistics:
2025-01-16 17:47:41,592 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.75, 61: 0.75, 62: 1.0, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.25, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.75, 115: 0.5, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.5, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.5, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.25, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 0.5, 242: 0.25, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 1.0, 260: 1.0, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 0.75, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.5, 276: 1.0, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.5, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 0.75, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.0, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 1.0, 372: 0.5, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 17:47:41,594 [INFO] [28] TRAIN  loss: 0.9758602052740272 acc: 0.9913994348200025
2025-01-16 17:47:41,594 [INFO] [28] TRAIN  loss dict: {'classification_loss': 0.9758602052740272}
2025-01-16 17:47:41,594 [INFO] [28] VALIDATION loss: 1.9171036184954464 VALIDATION acc: 0.7686520376175549
2025-01-16 17:47:41,594 [INFO] [28] VALIDATION loss dict: {'classification_loss': 1.9171036184954464}
2025-01-16 17:47:41,594 [INFO] 
2025-01-16 17:48:02,776 [INFO] Step[50/2713]: training loss : 0.9516453766822814 TRAIN  loss dict:  {'classification_loss': 0.9516453766822814}
2025-01-16 17:48:18,883 [INFO] Step[100/2713]: training loss : 0.9471387350559235 TRAIN  loss dict:  {'classification_loss': 0.9471387350559235}
2025-01-16 17:48:34,946 [INFO] Step[150/2713]: training loss : 0.9973440456390381 TRAIN  loss dict:  {'classification_loss': 0.9973440456390381}
2025-01-16 17:48:51,036 [INFO] Step[200/2713]: training loss : 0.9512851166725159 TRAIN  loss dict:  {'classification_loss': 0.9512851166725159}
2025-01-16 17:49:07,130 [INFO] Step[250/2713]: training loss : 0.9993869400024414 TRAIN  loss dict:  {'classification_loss': 0.9993869400024414}
2025-01-16 17:49:23,160 [INFO] Step[300/2713]: training loss : 0.9791520094871521 TRAIN  loss dict:  {'classification_loss': 0.9791520094871521}
2025-01-16 17:49:39,228 [INFO] Step[350/2713]: training loss : 0.9674442517757416 TRAIN  loss dict:  {'classification_loss': 0.9674442517757416}
2025-01-16 17:49:55,359 [INFO] Step[400/2713]: training loss : 0.9682140707969665 TRAIN  loss dict:  {'classification_loss': 0.9682140707969665}
2025-01-16 17:50:11,444 [INFO] Step[450/2713]: training loss : 0.9586670434474945 TRAIN  loss dict:  {'classification_loss': 0.9586670434474945}
2025-01-16 17:50:27,477 [INFO] Step[500/2713]: training loss : 0.9638383555412292 TRAIN  loss dict:  {'classification_loss': 0.9638383555412292}
2025-01-16 17:50:43,591 [INFO] Step[550/2713]: training loss : 0.975626963376999 TRAIN  loss dict:  {'classification_loss': 0.975626963376999}
2025-01-16 17:50:59,677 [INFO] Step[600/2713]: training loss : 0.9489465403556824 TRAIN  loss dict:  {'classification_loss': 0.9489465403556824}
2025-01-16 17:51:15,800 [INFO] Step[650/2713]: training loss : 0.9734302711486816 TRAIN  loss dict:  {'classification_loss': 0.9734302711486816}
2025-01-16 17:51:31,918 [INFO] Step[700/2713]: training loss : 0.9995766139030456 TRAIN  loss dict:  {'classification_loss': 0.9995766139030456}
2025-01-16 17:51:47,995 [INFO] Step[750/2713]: training loss : 0.9809700226783753 TRAIN  loss dict:  {'classification_loss': 0.9809700226783753}
2025-01-16 17:52:04,132 [INFO] Step[800/2713]: training loss : 0.9992508435249329 TRAIN  loss dict:  {'classification_loss': 0.9992508435249329}
2025-01-16 17:52:20,257 [INFO] Step[850/2713]: training loss : 0.9808082282543182 TRAIN  loss dict:  {'classification_loss': 0.9808082282543182}
2025-01-16 17:52:36,376 [INFO] Step[900/2713]: training loss : 0.9538101255893707 TRAIN  loss dict:  {'classification_loss': 0.9538101255893707}
2025-01-16 17:52:52,612 [INFO] Step[950/2713]: training loss : 0.9794879531860352 TRAIN  loss dict:  {'classification_loss': 0.9794879531860352}
2025-01-16 17:53:08,651 [INFO] Step[1000/2713]: training loss : 0.9547186267375946 TRAIN  loss dict:  {'classification_loss': 0.9547186267375946}
2025-01-16 17:53:24,722 [INFO] Step[1050/2713]: training loss : 0.9757461524009705 TRAIN  loss dict:  {'classification_loss': 0.9757461524009705}
2025-01-16 17:53:40,699 [INFO] Step[1100/2713]: training loss : 0.971334935426712 TRAIN  loss dict:  {'classification_loss': 0.971334935426712}
2025-01-16 17:53:56,778 [INFO] Step[1150/2713]: training loss : 1.0307814848423005 TRAIN  loss dict:  {'classification_loss': 1.0307814848423005}
2025-01-16 17:54:12,817 [INFO] Step[1200/2713]: training loss : 0.957296508550644 TRAIN  loss dict:  {'classification_loss': 0.957296508550644}
2025-01-16 17:54:28,922 [INFO] Step[1250/2713]: training loss : 1.008560950756073 TRAIN  loss dict:  {'classification_loss': 1.008560950756073}
2025-01-16 17:54:45,000 [INFO] Step[1300/2713]: training loss : 1.067223335504532 TRAIN  loss dict:  {'classification_loss': 1.067223335504532}
2025-01-16 17:55:01,073 [INFO] Step[1350/2713]: training loss : 0.9629722738265991 TRAIN  loss dict:  {'classification_loss': 0.9629722738265991}
2025-01-16 17:55:17,095 [INFO] Step[1400/2713]: training loss : 0.9654649198055267 TRAIN  loss dict:  {'classification_loss': 0.9654649198055267}
2025-01-16 17:55:33,201 [INFO] Step[1450/2713]: training loss : 0.9488446235656738 TRAIN  loss dict:  {'classification_loss': 0.9488446235656738}
2025-01-16 17:55:49,225 [INFO] Step[1500/2713]: training loss : 0.9690723752975464 TRAIN  loss dict:  {'classification_loss': 0.9690723752975464}
2025-01-16 17:56:05,330 [INFO] Step[1550/2713]: training loss : 0.9878748893737793 TRAIN  loss dict:  {'classification_loss': 0.9878748893737793}
2025-01-16 17:56:21,380 [INFO] Step[1600/2713]: training loss : 0.9609940946102142 TRAIN  loss dict:  {'classification_loss': 0.9609940946102142}
2025-01-16 17:56:37,440 [INFO] Step[1650/2713]: training loss : 0.947445021867752 TRAIN  loss dict:  {'classification_loss': 0.947445021867752}
2025-01-16 17:56:53,496 [INFO] Step[1700/2713]: training loss : 0.9486191952228546 TRAIN  loss dict:  {'classification_loss': 0.9486191952228546}
2025-01-16 17:57:09,579 [INFO] Step[1750/2713]: training loss : 0.9591869866847992 TRAIN  loss dict:  {'classification_loss': 0.9591869866847992}
2025-01-16 17:57:25,617 [INFO] Step[1800/2713]: training loss : 0.9805546259880066 TRAIN  loss dict:  {'classification_loss': 0.9805546259880066}
2025-01-16 17:57:41,716 [INFO] Step[1850/2713]: training loss : 0.9471764659881592 TRAIN  loss dict:  {'classification_loss': 0.9471764659881592}
2025-01-16 17:57:57,830 [INFO] Step[1900/2713]: training loss : 0.9557710886001587 TRAIN  loss dict:  {'classification_loss': 0.9557710886001587}
2025-01-16 17:58:13,936 [INFO] Step[1950/2713]: training loss : 0.9591666865348816 TRAIN  loss dict:  {'classification_loss': 0.9591666865348816}
2025-01-16 17:58:30,021 [INFO] Step[2000/2713]: training loss : 0.9687170886993408 TRAIN  loss dict:  {'classification_loss': 0.9687170886993408}
2025-01-16 17:58:46,097 [INFO] Step[2050/2713]: training loss : 0.9815220892429352 TRAIN  loss dict:  {'classification_loss': 0.9815220892429352}
2025-01-16 17:59:02,277 [INFO] Step[2100/2713]: training loss : 0.9639451980590821 TRAIN  loss dict:  {'classification_loss': 0.9639451980590821}
2025-01-16 17:59:18,387 [INFO] Step[2150/2713]: training loss : 0.9525170266628266 TRAIN  loss dict:  {'classification_loss': 0.9525170266628266}
2025-01-16 17:59:34,504 [INFO] Step[2200/2713]: training loss : 0.9871145033836365 TRAIN  loss dict:  {'classification_loss': 0.9871145033836365}
2025-01-16 17:59:50,601 [INFO] Step[2250/2713]: training loss : 0.989761826992035 TRAIN  loss dict:  {'classification_loss': 0.989761826992035}
2025-01-16 18:00:06,748 [INFO] Step[2300/2713]: training loss : 0.9486378502845764 TRAIN  loss dict:  {'classification_loss': 0.9486378502845764}
2025-01-16 18:00:22,915 [INFO] Step[2350/2713]: training loss : 0.9501262927055358 TRAIN  loss dict:  {'classification_loss': 0.9501262927055358}
2025-01-16 18:00:39,038 [INFO] Step[2400/2713]: training loss : 0.9627270221710205 TRAIN  loss dict:  {'classification_loss': 0.9627270221710205}
2025-01-16 18:00:55,102 [INFO] Step[2450/2713]: training loss : 1.0284945249557496 TRAIN  loss dict:  {'classification_loss': 1.0284945249557496}
2025-01-16 18:01:11,245 [INFO] Step[2500/2713]: training loss : 0.9880322825908661 TRAIN  loss dict:  {'classification_loss': 0.9880322825908661}
2025-01-16 18:01:27,364 [INFO] Step[2550/2713]: training loss : 0.9610748136043549 TRAIN  loss dict:  {'classification_loss': 0.9610748136043549}
2025-01-16 18:01:43,538 [INFO] Step[2600/2713]: training loss : 0.990522929430008 TRAIN  loss dict:  {'classification_loss': 0.990522929430008}
2025-01-16 18:01:59,661 [INFO] Step[2650/2713]: training loss : 0.9497397994995117 TRAIN  loss dict:  {'classification_loss': 0.9497397994995117}
2025-01-16 18:02:15,771 [INFO] Step[2700/2713]: training loss : 0.954744565486908 TRAIN  loss dict:  {'classification_loss': 0.954744565486908}
2025-01-16 18:03:34,029 [INFO] Label accuracies statistics:
2025-01-16 18:03:34,029 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.25, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.75, 72: 0.75, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.25, 143: 1.0, 144: 1.0, 145: 0.5, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.5, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.25, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.25, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 0.5, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 0.25, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.25, 353: 0.25, 354: 0.5, 355: 0.5, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.25, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 18:03:34,031 [INFO] [29] TRAIN  loss: 0.9724745776571562 acc: 0.9916451652537167
2025-01-16 18:03:34,031 [INFO] [29] TRAIN  loss dict: {'classification_loss': 0.9724745776571562}
2025-01-16 18:03:34,031 [INFO] [29] VALIDATION loss: 1.829562575857442 VALIDATION acc: 0.7862068965517242
2025-01-16 18:03:34,031 [INFO] [29] VALIDATION loss dict: {'classification_loss': 1.829562575857442}
2025-01-16 18:03:34,031 [INFO] 
2025-01-16 18:03:54,832 [INFO] Step[50/2713]: training loss : 0.9737782144546508 TRAIN  loss dict:  {'classification_loss': 0.9737782144546508}
2025-01-16 18:04:10,893 [INFO] Step[100/2713]: training loss : 0.964702605009079 TRAIN  loss dict:  {'classification_loss': 0.964702605009079}
2025-01-16 18:04:27,041 [INFO] Step[150/2713]: training loss : 0.9501716363430023 TRAIN  loss dict:  {'classification_loss': 0.9501716363430023}
2025-01-16 18:04:43,156 [INFO] Step[200/2713]: training loss : 0.9831518650054931 TRAIN  loss dict:  {'classification_loss': 0.9831518650054931}
2025-01-16 18:04:59,332 [INFO] Step[250/2713]: training loss : 1.0001668882369996 TRAIN  loss dict:  {'classification_loss': 1.0001668882369996}
2025-01-16 18:05:15,483 [INFO] Step[300/2713]: training loss : 0.9541940438747406 TRAIN  loss dict:  {'classification_loss': 0.9541940438747406}
2025-01-16 18:05:31,576 [INFO] Step[350/2713]: training loss : 0.9822793316841125 TRAIN  loss dict:  {'classification_loss': 0.9822793316841125}
2025-01-16 18:05:47,743 [INFO] Step[400/2713]: training loss : 0.9688689494132996 TRAIN  loss dict:  {'classification_loss': 0.9688689494132996}
2025-01-16 18:06:03,938 [INFO] Step[450/2713]: training loss : 0.9576010179519653 TRAIN  loss dict:  {'classification_loss': 0.9576010179519653}
2025-01-16 18:06:20,089 [INFO] Step[500/2713]: training loss : 0.9819672298431397 TRAIN  loss dict:  {'classification_loss': 0.9819672298431397}
2025-01-16 18:06:36,243 [INFO] Step[550/2713]: training loss : 0.9691776525974274 TRAIN  loss dict:  {'classification_loss': 0.9691776525974274}
2025-01-16 18:06:52,402 [INFO] Step[600/2713]: training loss : 0.9474390232563019 TRAIN  loss dict:  {'classification_loss': 0.9474390232563019}
2025-01-16 18:07:08,607 [INFO] Step[650/2713]: training loss : 1.0166362059116363 TRAIN  loss dict:  {'classification_loss': 1.0166362059116363}
2025-01-16 18:07:24,698 [INFO] Step[700/2713]: training loss : 0.9672751796245574 TRAIN  loss dict:  {'classification_loss': 0.9672751796245574}
2025-01-16 18:07:40,873 [INFO] Step[750/2713]: training loss : 0.9633706510066986 TRAIN  loss dict:  {'classification_loss': 0.9633706510066986}
2025-01-16 18:07:56,980 [INFO] Step[800/2713]: training loss : 1.053933082818985 TRAIN  loss dict:  {'classification_loss': 1.053933082818985}
2025-01-16 18:08:13,104 [INFO] Step[850/2713]: training loss : 0.9516030406951904 TRAIN  loss dict:  {'classification_loss': 0.9516030406951904}
2025-01-16 18:08:29,288 [INFO] Step[900/2713]: training loss : 0.9644520378112793 TRAIN  loss dict:  {'classification_loss': 0.9644520378112793}
2025-01-16 18:08:45,397 [INFO] Step[950/2713]: training loss : 0.9615040016174317 TRAIN  loss dict:  {'classification_loss': 0.9615040016174317}
2025-01-16 18:09:01,535 [INFO] Step[1000/2713]: training loss : 0.9998184251785278 TRAIN  loss dict:  {'classification_loss': 0.9998184251785278}
2025-01-16 18:09:17,529 [INFO] Step[1050/2713]: training loss : 0.9936216223239899 TRAIN  loss dict:  {'classification_loss': 0.9936216223239899}
2025-01-16 18:09:33,061 [INFO] Step[1100/2713]: training loss : 0.9747275304794312 TRAIN  loss dict:  {'classification_loss': 0.9747275304794312}
2025-01-16 18:09:49,199 [INFO] Step[1150/2713]: training loss : 0.9672641587257386 TRAIN  loss dict:  {'classification_loss': 0.9672641587257386}
2025-01-16 18:10:04,641 [INFO] Step[1200/2713]: training loss : 0.9555831277370452 TRAIN  loss dict:  {'classification_loss': 0.9555831277370452}
2025-01-16 18:10:20,788 [INFO] Step[1250/2713]: training loss : 0.9766733109951019 TRAIN  loss dict:  {'classification_loss': 0.9766733109951019}
2025-01-16 18:10:36,861 [INFO] Step[1300/2713]: training loss : 0.9492282223701477 TRAIN  loss dict:  {'classification_loss': 0.9492282223701477}
2025-01-16 18:10:52,633 [INFO] Step[1350/2713]: training loss : 0.9707545936107635 TRAIN  loss dict:  {'classification_loss': 0.9707545936107635}
2025-01-16 18:11:08,766 [INFO] Step[1400/2713]: training loss : 0.9499374997615814 TRAIN  loss dict:  {'classification_loss': 0.9499374997615814}
2025-01-16 18:11:24,917 [INFO] Step[1450/2713]: training loss : 0.959627492427826 TRAIN  loss dict:  {'classification_loss': 0.959627492427826}
2025-01-16 18:11:41,060 [INFO] Step[1500/2713]: training loss : 1.0050314056873322 TRAIN  loss dict:  {'classification_loss': 1.0050314056873322}
2025-01-16 18:11:57,270 [INFO] Step[1550/2713]: training loss : 1.0054750895500184 TRAIN  loss dict:  {'classification_loss': 1.0054750895500184}
2025-01-16 18:12:13,402 [INFO] Step[1600/2713]: training loss : 0.9828600907325744 TRAIN  loss dict:  {'classification_loss': 0.9828600907325744}
2025-01-16 18:12:29,546 [INFO] Step[1650/2713]: training loss : 0.9854151272773742 TRAIN  loss dict:  {'classification_loss': 0.9854151272773742}
2025-01-16 18:12:45,808 [INFO] Step[1700/2713]: training loss : 0.9949590802192688 TRAIN  loss dict:  {'classification_loss': 0.9949590802192688}
2025-01-16 18:13:01,949 [INFO] Step[1750/2713]: training loss : 0.9982743191719056 TRAIN  loss dict:  {'classification_loss': 0.9982743191719056}
2025-01-16 18:13:18,050 [INFO] Step[1800/2713]: training loss : 0.9914035630226136 TRAIN  loss dict:  {'classification_loss': 0.9914035630226136}
2025-01-16 18:13:34,244 [INFO] Step[1850/2713]: training loss : 0.9884216630458832 TRAIN  loss dict:  {'classification_loss': 0.9884216630458832}
2025-01-16 18:13:50,417 [INFO] Step[1900/2713]: training loss : 0.9815350234508514 TRAIN  loss dict:  {'classification_loss': 0.9815350234508514}
2025-01-16 18:14:06,626 [INFO] Step[1950/2713]: training loss : 0.9734351849555969 TRAIN  loss dict:  {'classification_loss': 0.9734351849555969}
2025-01-16 18:14:22,752 [INFO] Step[2000/2713]: training loss : 0.952243891954422 TRAIN  loss dict:  {'classification_loss': 0.952243891954422}
2025-01-16 18:14:38,888 [INFO] Step[2050/2713]: training loss : 0.9843886291980743 TRAIN  loss dict:  {'classification_loss': 0.9843886291980743}
2025-01-16 18:14:55,024 [INFO] Step[2100/2713]: training loss : 1.003291199207306 TRAIN  loss dict:  {'classification_loss': 1.003291199207306}
2025-01-16 18:15:11,253 [INFO] Step[2150/2713]: training loss : 0.9770379996299744 TRAIN  loss dict:  {'classification_loss': 0.9770379996299744}
2025-01-16 18:15:27,370 [INFO] Step[2200/2713]: training loss : 0.9738268160820007 TRAIN  loss dict:  {'classification_loss': 0.9738268160820007}
2025-01-16 18:15:43,508 [INFO] Step[2250/2713]: training loss : 0.9942310059070587 TRAIN  loss dict:  {'classification_loss': 0.9942310059070587}
2025-01-16 18:15:59,612 [INFO] Step[2300/2713]: training loss : 1.0208136057853698 TRAIN  loss dict:  {'classification_loss': 1.0208136057853698}
2025-01-16 18:16:15,722 [INFO] Step[2350/2713]: training loss : 0.9925648713111878 TRAIN  loss dict:  {'classification_loss': 0.9925648713111878}
2025-01-16 18:16:31,827 [INFO] Step[2400/2713]: training loss : 1.033447164297104 TRAIN  loss dict:  {'classification_loss': 1.033447164297104}
2025-01-16 18:16:48,004 [INFO] Step[2450/2713]: training loss : 1.0476290905475616 TRAIN  loss dict:  {'classification_loss': 1.0476290905475616}
2025-01-16 18:17:04,102 [INFO] Step[2500/2713]: training loss : 0.9750220739841461 TRAIN  loss dict:  {'classification_loss': 0.9750220739841461}
2025-01-16 18:17:20,203 [INFO] Step[2550/2713]: training loss : 0.9831672930717468 TRAIN  loss dict:  {'classification_loss': 0.9831672930717468}
2025-01-16 18:17:36,324 [INFO] Step[2600/2713]: training loss : 0.95249636054039 TRAIN  loss dict:  {'classification_loss': 0.95249636054039}
2025-01-16 18:17:52,440 [INFO] Step[2650/2713]: training loss : 0.9488987934589386 TRAIN  loss dict:  {'classification_loss': 0.9488987934589386}
2025-01-16 18:18:08,555 [INFO] Step[2700/2713]: training loss : 0.9499216330051422 TRAIN  loss dict:  {'classification_loss': 0.9499216330051422}
2025-01-16 18:19:27,147 [INFO] Label accuracies statistics:
2025-01-16 18:19:27,147 [INFO] {0: 1.0, 1: 0.6666666666666666, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.5, 24: 0.75, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 1.0, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 0.75, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 1.0, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.5, 220: 1.0, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.25, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.75, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.5, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.25, 272: 1.0, 273: 0.25, 274: 0.5, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 1.0, 282: 1.0, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 0.75, 306: 1.0, 307: 1.0, 308: 0.75, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.25, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.25, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.25, 350: 0.25, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.5, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 1.0, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.5, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 18:19:27,149 [INFO] [30] TRAIN  loss: 0.9799801328944141 acc: 0.9879592087480035
2025-01-16 18:19:27,149 [INFO] [30] TRAIN  loss dict: {'classification_loss': 0.9799801328944141}
2025-01-16 18:19:27,149 [INFO] [30] VALIDATION loss: 1.8789647056868202 VALIDATION acc: 0.773667711598746
2025-01-16 18:19:27,149 [INFO] [30] VALIDATION loss dict: {'classification_loss': 1.8789647056868202}
2025-01-16 18:19:27,149 [INFO] 
2025-01-16 18:19:48,406 [INFO] Step[50/2713]: training loss : 0.9580422127246857 TRAIN  loss dict:  {'classification_loss': 0.9580422127246857}
2025-01-16 18:20:04,133 [INFO] Step[100/2713]: training loss : 0.9623669576644898 TRAIN  loss dict:  {'classification_loss': 0.9623669576644898}
2025-01-16 18:20:19,948 [INFO] Step[150/2713]: training loss : 0.9424841475486755 TRAIN  loss dict:  {'classification_loss': 0.9424841475486755}
2025-01-16 18:20:35,993 [INFO] Step[200/2713]: training loss : 0.9562067377567292 TRAIN  loss dict:  {'classification_loss': 0.9562067377567292}
2025-01-16 18:20:52,071 [INFO] Step[250/2713]: training loss : 0.954658043384552 TRAIN  loss dict:  {'classification_loss': 0.954658043384552}
2025-01-16 18:21:08,114 [INFO] Step[300/2713]: training loss : 0.9511093556880951 TRAIN  loss dict:  {'classification_loss': 0.9511093556880951}
2025-01-16 18:21:23,537 [INFO] Step[350/2713]: training loss : 0.962922238111496 TRAIN  loss dict:  {'classification_loss': 0.962922238111496}
2025-01-16 18:21:39,654 [INFO] Step[400/2713]: training loss : 0.9489564764499664 TRAIN  loss dict:  {'classification_loss': 0.9489564764499664}
2025-01-16 18:21:55,775 [INFO] Step[450/2713]: training loss : 0.9593755710124969 TRAIN  loss dict:  {'classification_loss': 0.9593755710124969}
2025-01-16 18:22:11,822 [INFO] Step[500/2713]: training loss : 0.9446888029575348 TRAIN  loss dict:  {'classification_loss': 0.9446888029575348}
2025-01-16 18:22:28,005 [INFO] Step[550/2713]: training loss : 0.9432157158851624 TRAIN  loss dict:  {'classification_loss': 0.9432157158851624}
2025-01-16 18:22:44,027 [INFO] Step[600/2713]: training loss : 0.9570036268234253 TRAIN  loss dict:  {'classification_loss': 0.9570036268234253}
2025-01-16 18:23:00,168 [INFO] Step[650/2713]: training loss : 0.9461099338531495 TRAIN  loss dict:  {'classification_loss': 0.9461099338531495}
2025-01-16 18:23:16,277 [INFO] Step[700/2713]: training loss : 0.9647643506526947 TRAIN  loss dict:  {'classification_loss': 0.9647643506526947}
2025-01-16 18:23:32,350 [INFO] Step[750/2713]: training loss : 0.9997975635528564 TRAIN  loss dict:  {'classification_loss': 0.9997975635528564}
2025-01-16 18:23:48,412 [INFO] Step[800/2713]: training loss : 0.9581691813468933 TRAIN  loss dict:  {'classification_loss': 0.9581691813468933}
2025-01-16 18:24:04,507 [INFO] Step[850/2713]: training loss : 0.9411653411388398 TRAIN  loss dict:  {'classification_loss': 0.9411653411388398}
2025-01-16 18:24:20,636 [INFO] Step[900/2713]: training loss : 0.9776446688175201 TRAIN  loss dict:  {'classification_loss': 0.9776446688175201}
2025-01-16 18:24:36,875 [INFO] Step[950/2713]: training loss : 0.959974216222763 TRAIN  loss dict:  {'classification_loss': 0.959974216222763}
2025-01-16 18:24:53,059 [INFO] Step[1000/2713]: training loss : 0.9955416941642761 TRAIN  loss dict:  {'classification_loss': 0.9955416941642761}
2025-01-16 18:25:09,209 [INFO] Step[1050/2713]: training loss : 1.0000737750530242 TRAIN  loss dict:  {'classification_loss': 1.0000737750530242}
2025-01-16 18:25:25,428 [INFO] Step[1100/2713]: training loss : 0.9461881256103516 TRAIN  loss dict:  {'classification_loss': 0.9461881256103516}
2025-01-16 18:25:41,554 [INFO] Step[1150/2713]: training loss : 0.9983904480934143 TRAIN  loss dict:  {'classification_loss': 0.9983904480934143}
2025-01-16 18:25:57,723 [INFO] Step[1200/2713]: training loss : 0.967878669500351 TRAIN  loss dict:  {'classification_loss': 0.967878669500351}
2025-01-16 18:26:13,855 [INFO] Step[1250/2713]: training loss : 0.9469512808322906 TRAIN  loss dict:  {'classification_loss': 0.9469512808322906}
2025-01-16 18:26:29,961 [INFO] Step[1300/2713]: training loss : 0.9484328281879425 TRAIN  loss dict:  {'classification_loss': 0.9484328281879425}
2025-01-16 18:26:46,126 [INFO] Step[1350/2713]: training loss : 0.9845224511623383 TRAIN  loss dict:  {'classification_loss': 0.9845224511623383}
2025-01-16 18:27:02,193 [INFO] Step[1400/2713]: training loss : 0.9605890583992004 TRAIN  loss dict:  {'classification_loss': 0.9605890583992004}
2025-01-16 18:27:18,165 [INFO] Step[1450/2713]: training loss : 0.9766898071765899 TRAIN  loss dict:  {'classification_loss': 0.9766898071765899}
2025-01-16 18:27:33,907 [INFO] Step[1500/2713]: training loss : 1.0135046935081482 TRAIN  loss dict:  {'classification_loss': 1.0135046935081482}
2025-01-16 18:27:49,830 [INFO] Step[1550/2713]: training loss : 0.9522468864917755 TRAIN  loss dict:  {'classification_loss': 0.9522468864917755}
2025-01-16 18:28:05,848 [INFO] Step[1600/2713]: training loss : 0.9441194796562195 TRAIN  loss dict:  {'classification_loss': 0.9441194796562195}
2025-01-16 18:28:21,822 [INFO] Step[1650/2713]: training loss : 0.9468690609931946 TRAIN  loss dict:  {'classification_loss': 0.9468690609931946}
2025-01-16 18:28:37,899 [INFO] Step[1700/2713]: training loss : 0.9459336185455323 TRAIN  loss dict:  {'classification_loss': 0.9459336185455323}
2025-01-16 18:28:53,999 [INFO] Step[1750/2713]: training loss : 0.9461899769306182 TRAIN  loss dict:  {'classification_loss': 0.9461899769306182}
2025-01-16 18:29:10,042 [INFO] Step[1800/2713]: training loss : 0.9508507192134857 TRAIN  loss dict:  {'classification_loss': 0.9508507192134857}
2025-01-16 18:29:26,102 [INFO] Step[1850/2713]: training loss : 0.9750136542320251 TRAIN  loss dict:  {'classification_loss': 0.9750136542320251}
2025-01-16 18:29:42,190 [INFO] Step[1900/2713]: training loss : 0.9535762274265289 TRAIN  loss dict:  {'classification_loss': 0.9535762274265289}
2025-01-16 18:29:58,278 [INFO] Step[1950/2713]: training loss : 0.9875395977497101 TRAIN  loss dict:  {'classification_loss': 0.9875395977497101}
2025-01-16 18:30:14,353 [INFO] Step[2000/2713]: training loss : 0.9690430450439453 TRAIN  loss dict:  {'classification_loss': 0.9690430450439453}
2025-01-16 18:30:30,515 [INFO] Step[2050/2713]: training loss : 0.9938762998580932 TRAIN  loss dict:  {'classification_loss': 0.9938762998580932}
2025-01-16 18:30:46,650 [INFO] Step[2100/2713]: training loss : 0.9480251455307007 TRAIN  loss dict:  {'classification_loss': 0.9480251455307007}
2025-01-16 18:31:02,823 [INFO] Step[2150/2713]: training loss : 0.9720363473892212 TRAIN  loss dict:  {'classification_loss': 0.9720363473892212}
2025-01-16 18:31:18,936 [INFO] Step[2200/2713]: training loss : 0.9932584357261658 TRAIN  loss dict:  {'classification_loss': 0.9932584357261658}
2025-01-16 18:31:35,017 [INFO] Step[2250/2713]: training loss : 0.9771603870391846 TRAIN  loss dict:  {'classification_loss': 0.9771603870391846}
2025-01-16 18:31:51,103 [INFO] Step[2300/2713]: training loss : 0.9720993363857269 TRAIN  loss dict:  {'classification_loss': 0.9720993363857269}
2025-01-16 18:32:07,245 [INFO] Step[2350/2713]: training loss : 0.9476087820529938 TRAIN  loss dict:  {'classification_loss': 0.9476087820529938}
2025-01-16 18:32:23,280 [INFO] Step[2400/2713]: training loss : 0.9504252076148987 TRAIN  loss dict:  {'classification_loss': 0.9504252076148987}
2025-01-16 18:32:39,409 [INFO] Step[2450/2713]: training loss : 0.9421429514884949 TRAIN  loss dict:  {'classification_loss': 0.9421429514884949}
2025-01-16 18:32:55,569 [INFO] Step[2500/2713]: training loss : 0.9481899070739747 TRAIN  loss dict:  {'classification_loss': 0.9481899070739747}
2025-01-16 18:33:11,674 [INFO] Step[2550/2713]: training loss : 0.966586537361145 TRAIN  loss dict:  {'classification_loss': 0.966586537361145}
2025-01-16 18:33:27,707 [INFO] Step[2600/2713]: training loss : 0.9464768779277801 TRAIN  loss dict:  {'classification_loss': 0.9464768779277801}
2025-01-16 18:33:43,212 [INFO] Step[2650/2713]: training loss : 0.9510068106651306 TRAIN  loss dict:  {'classification_loss': 0.9510068106651306}
2025-01-16 18:33:58,771 [INFO] Step[2700/2713]: training loss : 0.9543279194831848 TRAIN  loss dict:  {'classification_loss': 0.9543279194831848}
2025-01-16 18:35:36,346 [INFO] Label accuracies statistics:
2025-01-16 18:35:36,346 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.0, 212: 0.75, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.0, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.5, 261: 0.25, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.25, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 0.5, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.5, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 18:35:36,348 [INFO] [31] TRAIN  loss: 0.9622217164986959 acc: 0.9933652782897162
2025-01-16 18:35:36,348 [INFO] [31] TRAIN  loss dict: {'classification_loss': 0.9622217164986959}
2025-01-16 18:35:36,348 [INFO] [31] VALIDATION loss: 1.885264316783812 VALIDATION acc: 0.7768025078369906
2025-01-16 18:35:36,348 [INFO] [31] VALIDATION loss dict: {'classification_loss': 1.885264316783812}
2025-01-16 18:35:36,348 [INFO] 
2025-01-16 18:35:57,482 [INFO] Step[50/2713]: training loss : 0.947194492816925 TRAIN  loss dict:  {'classification_loss': 0.947194492816925}
2025-01-16 18:36:13,669 [INFO] Step[100/2713]: training loss : 0.9615257132053375 TRAIN  loss dict:  {'classification_loss': 0.9615257132053375}
2025-01-16 18:36:29,876 [INFO] Step[150/2713]: training loss : 0.9661885738372803 TRAIN  loss dict:  {'classification_loss': 0.9661885738372803}
2025-01-16 18:36:46,035 [INFO] Step[200/2713]: training loss : 0.9620544910430908 TRAIN  loss dict:  {'classification_loss': 0.9620544910430908}
2025-01-16 18:37:02,162 [INFO] Step[250/2713]: training loss : 0.9451094353199005 TRAIN  loss dict:  {'classification_loss': 0.9451094353199005}
2025-01-16 18:37:18,333 [INFO] Step[300/2713]: training loss : 0.9413145017623902 TRAIN  loss dict:  {'classification_loss': 0.9413145017623902}
2025-01-16 18:37:34,462 [INFO] Step[350/2713]: training loss : 0.961647720336914 TRAIN  loss dict:  {'classification_loss': 0.961647720336914}
2025-01-16 18:37:50,589 [INFO] Step[400/2713]: training loss : 0.9863391184806823 TRAIN  loss dict:  {'classification_loss': 0.9863391184806823}
2025-01-16 18:38:06,774 [INFO] Step[450/2713]: training loss : 0.9829729318618774 TRAIN  loss dict:  {'classification_loss': 0.9829729318618774}
2025-01-16 18:38:22,958 [INFO] Step[500/2713]: training loss : 0.9440483641624451 TRAIN  loss dict:  {'classification_loss': 0.9440483641624451}
2025-01-16 18:38:39,175 [INFO] Step[550/2713]: training loss : 0.948753217458725 TRAIN  loss dict:  {'classification_loss': 0.948753217458725}
2025-01-16 18:38:55,279 [INFO] Step[600/2713]: training loss : 0.9604592251777649 TRAIN  loss dict:  {'classification_loss': 0.9604592251777649}
2025-01-16 18:39:11,382 [INFO] Step[650/2713]: training loss : 0.9456470465660095 TRAIN  loss dict:  {'classification_loss': 0.9456470465660095}
2025-01-16 18:39:27,493 [INFO] Step[700/2713]: training loss : 0.9398329019546509 TRAIN  loss dict:  {'classification_loss': 0.9398329019546509}
2025-01-16 18:39:43,663 [INFO] Step[750/2713]: training loss : 0.944160133600235 TRAIN  loss dict:  {'classification_loss': 0.944160133600235}
2025-01-16 18:39:59,873 [INFO] Step[800/2713]: training loss : 0.9400252175331115 TRAIN  loss dict:  {'classification_loss': 0.9400252175331115}
2025-01-16 18:40:16,061 [INFO] Step[850/2713]: training loss : 0.9713973891735077 TRAIN  loss dict:  {'classification_loss': 0.9713973891735077}
2025-01-16 18:40:32,173 [INFO] Step[900/2713]: training loss : 0.9397927570343018 TRAIN  loss dict:  {'classification_loss': 0.9397927570343018}
2025-01-16 18:40:48,370 [INFO] Step[950/2713]: training loss : 0.9429929256439209 TRAIN  loss dict:  {'classification_loss': 0.9429929256439209}
2025-01-16 18:41:04,475 [INFO] Step[1000/2713]: training loss : 0.9557900404930115 TRAIN  loss dict:  {'classification_loss': 0.9557900404930115}
2025-01-16 18:41:20,630 [INFO] Step[1050/2713]: training loss : 0.9596336948871612 TRAIN  loss dict:  {'classification_loss': 0.9596336948871612}
2025-01-16 18:41:36,739 [INFO] Step[1100/2713]: training loss : 0.9415262615680695 TRAIN  loss dict:  {'classification_loss': 0.9415262615680695}
2025-01-16 18:41:52,865 [INFO] Step[1150/2713]: training loss : 0.9373972332477569 TRAIN  loss dict:  {'classification_loss': 0.9373972332477569}
2025-01-16 18:42:09,047 [INFO] Step[1200/2713]: training loss : 0.957282041311264 TRAIN  loss dict:  {'classification_loss': 0.957282041311264}
2025-01-16 18:42:25,264 [INFO] Step[1250/2713]: training loss : 0.9509405076503754 TRAIN  loss dict:  {'classification_loss': 0.9509405076503754}
2025-01-16 18:42:41,388 [INFO] Step[1300/2713]: training loss : 0.9678685092926025 TRAIN  loss dict:  {'classification_loss': 0.9678685092926025}
2025-01-16 18:42:57,556 [INFO] Step[1350/2713]: training loss : 0.9474656593799591 TRAIN  loss dict:  {'classification_loss': 0.9474656593799591}
2025-01-16 18:43:13,694 [INFO] Step[1400/2713]: training loss : 0.9438848924636841 TRAIN  loss dict:  {'classification_loss': 0.9438848924636841}
2025-01-16 18:43:29,879 [INFO] Step[1450/2713]: training loss : 0.9580299818515777 TRAIN  loss dict:  {'classification_loss': 0.9580299818515777}
2025-01-16 18:43:46,088 [INFO] Step[1500/2713]: training loss : 0.9709092831611633 TRAIN  loss dict:  {'classification_loss': 0.9709092831611633}
2025-01-16 18:44:02,276 [INFO] Step[1550/2713]: training loss : 0.9878061628341674 TRAIN  loss dict:  {'classification_loss': 0.9878061628341674}
2025-01-16 18:44:18,405 [INFO] Step[1600/2713]: training loss : 0.9444155514240264 TRAIN  loss dict:  {'classification_loss': 0.9444155514240264}
2025-01-16 18:44:34,635 [INFO] Step[1650/2713]: training loss : 0.9668092322349549 TRAIN  loss dict:  {'classification_loss': 0.9668092322349549}
2025-01-16 18:44:50,764 [INFO] Step[1700/2713]: training loss : 0.9504341566562653 TRAIN  loss dict:  {'classification_loss': 0.9504341566562653}
2025-01-16 18:45:06,950 [INFO] Step[1750/2713]: training loss : 0.954027705192566 TRAIN  loss dict:  {'classification_loss': 0.954027705192566}
2025-01-16 18:45:23,080 [INFO] Step[1800/2713]: training loss : 0.9680880904197693 TRAIN  loss dict:  {'classification_loss': 0.9680880904197693}
2025-01-16 18:45:39,213 [INFO] Step[1850/2713]: training loss : 0.9523312640190125 TRAIN  loss dict:  {'classification_loss': 0.9523312640190125}
2025-01-16 18:45:55,320 [INFO] Step[1900/2713]: training loss : 0.9439586973190308 TRAIN  loss dict:  {'classification_loss': 0.9439586973190308}
2025-01-16 18:46:11,484 [INFO] Step[1950/2713]: training loss : 0.9474034941196442 TRAIN  loss dict:  {'classification_loss': 0.9474034941196442}
2025-01-16 18:46:27,599 [INFO] Step[2000/2713]: training loss : 0.9410359847545624 TRAIN  loss dict:  {'classification_loss': 0.9410359847545624}
2025-01-16 18:46:43,753 [INFO] Step[2050/2713]: training loss : 0.9510224819183349 TRAIN  loss dict:  {'classification_loss': 0.9510224819183349}
2025-01-16 18:46:59,825 [INFO] Step[2100/2713]: training loss : 0.9498541295528412 TRAIN  loss dict:  {'classification_loss': 0.9498541295528412}
2025-01-16 18:47:16,099 [INFO] Step[2150/2713]: training loss : 0.9522320854663849 TRAIN  loss dict:  {'classification_loss': 0.9522320854663849}
2025-01-16 18:47:32,280 [INFO] Step[2200/2713]: training loss : 0.9654546201229095 TRAIN  loss dict:  {'classification_loss': 0.9654546201229095}
2025-01-16 18:47:48,451 [INFO] Step[2250/2713]: training loss : 0.9417460322380066 TRAIN  loss dict:  {'classification_loss': 0.9417460322380066}
2025-01-16 18:48:04,559 [INFO] Step[2300/2713]: training loss : 0.9563703978061676 TRAIN  loss dict:  {'classification_loss': 0.9563703978061676}
2025-01-16 18:48:20,692 [INFO] Step[2350/2713]: training loss : 0.9438524270057678 TRAIN  loss dict:  {'classification_loss': 0.9438524270057678}
2025-01-16 18:48:36,824 [INFO] Step[2400/2713]: training loss : 0.9419049656391144 TRAIN  loss dict:  {'classification_loss': 0.9419049656391144}
2025-01-16 18:48:52,961 [INFO] Step[2450/2713]: training loss : 0.9397380042076111 TRAIN  loss dict:  {'classification_loss': 0.9397380042076111}
2025-01-16 18:49:09,088 [INFO] Step[2500/2713]: training loss : 0.9473106884956359 TRAIN  loss dict:  {'classification_loss': 0.9473106884956359}
2025-01-16 18:49:25,273 [INFO] Step[2550/2713]: training loss : 0.9464376103878022 TRAIN  loss dict:  {'classification_loss': 0.9464376103878022}
2025-01-16 18:49:41,377 [INFO] Step[2600/2713]: training loss : 0.9649624645709991 TRAIN  loss dict:  {'classification_loss': 0.9649624645709991}
2025-01-16 18:49:57,582 [INFO] Step[2650/2713]: training loss : 0.9411917793750763 TRAIN  loss dict:  {'classification_loss': 0.9411917793750763}
2025-01-16 18:50:13,787 [INFO] Step[2700/2713]: training loss : 0.943984887599945 TRAIN  loss dict:  {'classification_loss': 0.943984887599945}
2025-01-16 18:51:32,222 [INFO] Label accuracies statistics:
2025-01-16 18:51:32,222 [INFO] {0: 0.0, 1: 0.6666666666666666, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 1.0, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.75, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 1.0, 214: 0.75, 215: 0.5, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.25, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.25, 244: 1.0, 245: 0.75, 246: 0.75, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.5, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.75, 278: 1.0, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 1.0, 311: 0.5, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 1.0, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 18:51:32,224 [INFO] [32] TRAIN  loss: 0.9530241299589489 acc: 0.9952082565425728
2025-01-16 18:51:32,224 [INFO] [32] TRAIN  loss dict: {'classification_loss': 0.9530241299589489}
2025-01-16 18:51:32,224 [INFO] [32] VALIDATION loss: 1.8646551438964398 VALIDATION acc: 0.7924764890282132
2025-01-16 18:51:32,224 [INFO] [32] VALIDATION loss dict: {'classification_loss': 1.8646551438964398}
2025-01-16 18:51:32,224 [INFO] 
2025-01-16 18:51:53,730 [INFO] Step[50/2713]: training loss : 1.0095919525623323 TRAIN  loss dict:  {'classification_loss': 1.0095919525623323}
2025-01-16 18:52:09,891 [INFO] Step[100/2713]: training loss : 0.9547548890113831 TRAIN  loss dict:  {'classification_loss': 0.9547548890113831}
2025-01-16 18:52:26,030 [INFO] Step[150/2713]: training loss : 0.9447682666778564 TRAIN  loss dict:  {'classification_loss': 0.9447682666778564}
2025-01-16 18:52:42,206 [INFO] Step[200/2713]: training loss : 0.9461992394924164 TRAIN  loss dict:  {'classification_loss': 0.9461992394924164}
2025-01-16 18:52:58,377 [INFO] Step[250/2713]: training loss : 0.9692245984077453 TRAIN  loss dict:  {'classification_loss': 0.9692245984077453}
2025-01-16 18:53:14,520 [INFO] Step[300/2713]: training loss : 0.9506018114089966 TRAIN  loss dict:  {'classification_loss': 0.9506018114089966}
2025-01-16 18:53:30,704 [INFO] Step[350/2713]: training loss : 0.9388520014286041 TRAIN  loss dict:  {'classification_loss': 0.9388520014286041}
2025-01-16 18:53:46,854 [INFO] Step[400/2713]: training loss : 0.950981673002243 TRAIN  loss dict:  {'classification_loss': 0.950981673002243}
2025-01-16 18:54:02,966 [INFO] Step[450/2713]: training loss : 0.9437159287929535 TRAIN  loss dict:  {'classification_loss': 0.9437159287929535}
2025-01-16 18:54:19,089 [INFO] Step[500/2713]: training loss : 0.9702443706989289 TRAIN  loss dict:  {'classification_loss': 0.9702443706989289}
2025-01-16 18:54:35,271 [INFO] Step[550/2713]: training loss : 0.9644019639492035 TRAIN  loss dict:  {'classification_loss': 0.9644019639492035}
2025-01-16 18:54:51,405 [INFO] Step[600/2713]: training loss : 0.9475637769699097 TRAIN  loss dict:  {'classification_loss': 0.9475637769699097}
2025-01-16 18:55:07,554 [INFO] Step[650/2713]: training loss : 0.9557321727275848 TRAIN  loss dict:  {'classification_loss': 0.9557321727275848}
2025-01-16 18:55:23,669 [INFO] Step[700/2713]: training loss : 0.9457517576217651 TRAIN  loss dict:  {'classification_loss': 0.9457517576217651}
2025-01-16 18:55:39,788 [INFO] Step[750/2713]: training loss : 0.9497192430496216 TRAIN  loss dict:  {'classification_loss': 0.9497192430496216}
2025-01-16 18:55:55,901 [INFO] Step[800/2713]: training loss : 0.9683918797969818 TRAIN  loss dict:  {'classification_loss': 0.9683918797969818}
2025-01-16 18:56:12,154 [INFO] Step[850/2713]: training loss : 0.9509237611293793 TRAIN  loss dict:  {'classification_loss': 0.9509237611293793}
2025-01-16 18:56:28,271 [INFO] Step[900/2713]: training loss : 1.0045911991596221 TRAIN  loss dict:  {'classification_loss': 1.0045911991596221}
2025-01-16 18:56:44,463 [INFO] Step[950/2713]: training loss : 0.9419617116451263 TRAIN  loss dict:  {'classification_loss': 0.9419617116451263}
2025-01-16 18:57:00,587 [INFO] Step[1000/2713]: training loss : 0.9511329567432404 TRAIN  loss dict:  {'classification_loss': 0.9511329567432404}
2025-01-16 18:57:16,760 [INFO] Step[1050/2713]: training loss : 0.9567380583286286 TRAIN  loss dict:  {'classification_loss': 0.9567380583286286}
2025-01-16 18:57:32,948 [INFO] Step[1100/2713]: training loss : 0.9658873355388642 TRAIN  loss dict:  {'classification_loss': 0.9658873355388642}
2025-01-16 18:57:49,147 [INFO] Step[1150/2713]: training loss : 0.9614382839202881 TRAIN  loss dict:  {'classification_loss': 0.9614382839202881}
2025-01-16 18:58:05,263 [INFO] Step[1200/2713]: training loss : 0.9965881812572479 TRAIN  loss dict:  {'classification_loss': 0.9965881812572479}
2025-01-16 18:58:21,473 [INFO] Step[1250/2713]: training loss : 0.9508621549606323 TRAIN  loss dict:  {'classification_loss': 0.9508621549606323}
2025-01-16 18:58:37,581 [INFO] Step[1300/2713]: training loss : 0.9585091924667358 TRAIN  loss dict:  {'classification_loss': 0.9585091924667358}
2025-01-16 18:58:53,791 [INFO] Step[1350/2713]: training loss : 0.9525372123718262 TRAIN  loss dict:  {'classification_loss': 0.9525372123718262}
2025-01-16 18:59:09,909 [INFO] Step[1400/2713]: training loss : 0.9917211067676545 TRAIN  loss dict:  {'classification_loss': 0.9917211067676545}
2025-01-16 18:59:26,028 [INFO] Step[1450/2713]: training loss : 0.9584046864509582 TRAIN  loss dict:  {'classification_loss': 0.9584046864509582}
2025-01-16 18:59:42,157 [INFO] Step[1500/2713]: training loss : 0.9520954751968383 TRAIN  loss dict:  {'classification_loss': 0.9520954751968383}
2025-01-16 18:59:58,320 [INFO] Step[1550/2713]: training loss : 0.9490990447998047 TRAIN  loss dict:  {'classification_loss': 0.9490990447998047}
2025-01-16 19:00:14,463 [INFO] Step[1600/2713]: training loss : 1.0034022867679595 TRAIN  loss dict:  {'classification_loss': 1.0034022867679595}
2025-01-16 19:00:30,548 [INFO] Step[1650/2713]: training loss : 0.9555708563327789 TRAIN  loss dict:  {'classification_loss': 0.9555708563327789}
2025-01-16 19:00:46,721 [INFO] Step[1700/2713]: training loss : 0.9406269991397858 TRAIN  loss dict:  {'classification_loss': 0.9406269991397858}
2025-01-16 19:01:02,938 [INFO] Step[1750/2713]: training loss : 0.9943311929702758 TRAIN  loss dict:  {'classification_loss': 0.9943311929702758}
2025-01-16 19:01:19,033 [INFO] Step[1800/2713]: training loss : 0.9518032133579254 TRAIN  loss dict:  {'classification_loss': 0.9518032133579254}
2025-01-16 19:01:35,218 [INFO] Step[1850/2713]: training loss : 0.9424689447879792 TRAIN  loss dict:  {'classification_loss': 0.9424689447879792}
2025-01-16 19:01:51,400 [INFO] Step[1900/2713]: training loss : 0.944451185464859 TRAIN  loss dict:  {'classification_loss': 0.944451185464859}
2025-01-16 19:02:07,557 [INFO] Step[1950/2713]: training loss : 0.9598278510570526 TRAIN  loss dict:  {'classification_loss': 0.9598278510570526}
2025-01-16 19:02:23,643 [INFO] Step[2000/2713]: training loss : 0.943184802532196 TRAIN  loss dict:  {'classification_loss': 0.943184802532196}
2025-01-16 19:02:39,863 [INFO] Step[2050/2713]: training loss : 0.9421787977218627 TRAIN  loss dict:  {'classification_loss': 0.9421787977218627}
2025-01-16 19:02:56,007 [INFO] Step[2100/2713]: training loss : 0.9616823315620422 TRAIN  loss dict:  {'classification_loss': 0.9616823315620422}
2025-01-16 19:03:12,160 [INFO] Step[2150/2713]: training loss : 0.9473601078987122 TRAIN  loss dict:  {'classification_loss': 0.9473601078987122}
2025-01-16 19:03:28,270 [INFO] Step[2200/2713]: training loss : 0.948103529214859 TRAIN  loss dict:  {'classification_loss': 0.948103529214859}
2025-01-16 19:03:44,359 [INFO] Step[2250/2713]: training loss : 0.9907577180862427 TRAIN  loss dict:  {'classification_loss': 0.9907577180862427}
2025-01-16 19:04:00,527 [INFO] Step[2300/2713]: training loss : 0.980570262670517 TRAIN  loss dict:  {'classification_loss': 0.980570262670517}
2025-01-16 19:04:16,692 [INFO] Step[2350/2713]: training loss : 0.9423051619529724 TRAIN  loss dict:  {'classification_loss': 0.9423051619529724}
2025-01-16 19:04:32,849 [INFO] Step[2400/2713]: training loss : 0.9582250618934631 TRAIN  loss dict:  {'classification_loss': 0.9582250618934631}
2025-01-16 19:04:48,952 [INFO] Step[2450/2713]: training loss : 0.9854919159412384 TRAIN  loss dict:  {'classification_loss': 0.9854919159412384}
2025-01-16 19:05:05,142 [INFO] Step[2500/2713]: training loss : 0.947188709974289 TRAIN  loss dict:  {'classification_loss': 0.947188709974289}
2025-01-16 19:05:21,287 [INFO] Step[2550/2713]: training loss : 0.9400365734100342 TRAIN  loss dict:  {'classification_loss': 0.9400365734100342}
2025-01-16 19:05:37,425 [INFO] Step[2600/2713]: training loss : 0.9387346088886261 TRAIN  loss dict:  {'classification_loss': 0.9387346088886261}
2025-01-16 19:05:53,643 [INFO] Step[2650/2713]: training loss : 0.9836857497692109 TRAIN  loss dict:  {'classification_loss': 0.9836857497692109}
2025-01-16 19:06:09,754 [INFO] Step[2700/2713]: training loss : 0.9605338907241822 TRAIN  loss dict:  {'classification_loss': 0.9605338907241822}
2025-01-16 19:07:28,440 [INFO] Label accuracies statistics:
2025-01-16 19:07:28,440 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.5, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.5, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 1.0, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.5, 234: 0.5, 235: 0.5, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.25, 262: 0.75, 263: 0.75, 264: 0.5, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.25, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 1.0, 282: 1.0, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.5, 324: 0.5, 325: 1.0, 326: 0.75, 327: 0.5, 328: 0.25, 329: 0.75, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 1.0, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.5, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.25, 390: 0.0, 391: 1.0, 392: 0.5, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 19:07:28,442 [INFO] [33] TRAIN  loss: 0.9594675013819525 acc: 0.9933652782897162
2025-01-16 19:07:28,442 [INFO] [33] TRAIN  loss dict: {'classification_loss': 0.9594675013819525}
2025-01-16 19:07:28,442 [INFO] [33] VALIDATION loss: 1.9580604521403635 VALIDATION acc: 0.7655172413793103
2025-01-16 19:07:28,442 [INFO] [33] VALIDATION loss dict: {'classification_loss': 1.9580604521403635}
2025-01-16 19:07:28,442 [INFO] 
2025-01-16 19:07:49,344 [INFO] Step[50/2713]: training loss : 0.9870848512649536 TRAIN  loss dict:  {'classification_loss': 0.9870848512649536}
2025-01-16 19:08:05,520 [INFO] Step[100/2713]: training loss : 0.9405129325389862 TRAIN  loss dict:  {'classification_loss': 0.9405129325389862}
2025-01-16 19:08:21,683 [INFO] Step[150/2713]: training loss : 0.9374995625019074 TRAIN  loss dict:  {'classification_loss': 0.9374995625019074}
2025-01-16 19:08:37,797 [INFO] Step[200/2713]: training loss : 0.9624712610244751 TRAIN  loss dict:  {'classification_loss': 0.9624712610244751}
2025-01-16 19:08:54,049 [INFO] Step[250/2713]: training loss : 0.963188705444336 TRAIN  loss dict:  {'classification_loss': 0.963188705444336}
2025-01-16 19:09:10,193 [INFO] Step[300/2713]: training loss : 0.9405707657337189 TRAIN  loss dict:  {'classification_loss': 0.9405707657337189}
2025-01-16 19:09:26,328 [INFO] Step[350/2713]: training loss : 0.9388511753082276 TRAIN  loss dict:  {'classification_loss': 0.9388511753082276}
2025-01-16 19:09:42,570 [INFO] Step[400/2713]: training loss : 0.951077276468277 TRAIN  loss dict:  {'classification_loss': 0.951077276468277}
2025-01-16 19:09:58,850 [INFO] Step[450/2713]: training loss : 0.9488954734802246 TRAIN  loss dict:  {'classification_loss': 0.9488954734802246}
2025-01-16 19:10:14,979 [INFO] Step[500/2713]: training loss : 0.9398365616798401 TRAIN  loss dict:  {'classification_loss': 0.9398365616798401}
2025-01-16 19:10:31,213 [INFO] Step[550/2713]: training loss : 0.9424609017372131 TRAIN  loss dict:  {'classification_loss': 0.9424609017372131}
2025-01-16 19:10:47,410 [INFO] Step[600/2713]: training loss : 0.9509453225135803 TRAIN  loss dict:  {'classification_loss': 0.9509453225135803}
2025-01-16 19:11:03,623 [INFO] Step[650/2713]: training loss : 0.9479670405387879 TRAIN  loss dict:  {'classification_loss': 0.9479670405387879}
2025-01-16 19:11:19,806 [INFO] Step[700/2713]: training loss : 0.9409540200233459 TRAIN  loss dict:  {'classification_loss': 0.9409540200233459}
2025-01-16 19:11:36,003 [INFO] Step[750/2713]: training loss : 0.9426646053791046 TRAIN  loss dict:  {'classification_loss': 0.9426646053791046}
2025-01-16 19:11:52,184 [INFO] Step[800/2713]: training loss : 0.9461951446533203 TRAIN  loss dict:  {'classification_loss': 0.9461951446533203}
2025-01-16 19:12:08,431 [INFO] Step[850/2713]: training loss : 0.9433430218696595 TRAIN  loss dict:  {'classification_loss': 0.9433430218696595}
2025-01-16 19:12:24,593 [INFO] Step[900/2713]: training loss : 0.9438753747940063 TRAIN  loss dict:  {'classification_loss': 0.9438753747940063}
2025-01-16 19:12:40,835 [INFO] Step[950/2713]: training loss : 0.9742670822143554 TRAIN  loss dict:  {'classification_loss': 0.9742670822143554}
2025-01-16 19:12:57,027 [INFO] Step[1000/2713]: training loss : 0.947690144777298 TRAIN  loss dict:  {'classification_loss': 0.947690144777298}
2025-01-16 19:13:13,217 [INFO] Step[1050/2713]: training loss : 0.9403401935100555 TRAIN  loss dict:  {'classification_loss': 0.9403401935100555}
2025-01-16 19:13:29,418 [INFO] Step[1100/2713]: training loss : 0.9372077322006226 TRAIN  loss dict:  {'classification_loss': 0.9372077322006226}
2025-01-16 19:13:45,667 [INFO] Step[1150/2713]: training loss : 0.9451173853874206 TRAIN  loss dict:  {'classification_loss': 0.9451173853874206}
2025-01-16 19:14:01,922 [INFO] Step[1200/2713]: training loss : 0.9398835897445679 TRAIN  loss dict:  {'classification_loss': 0.9398835897445679}
2025-01-16 19:14:18,170 [INFO] Step[1250/2713]: training loss : 0.9611812126636505 TRAIN  loss dict:  {'classification_loss': 0.9611812126636505}
2025-01-16 19:14:34,360 [INFO] Step[1300/2713]: training loss : 0.9460837006568908 TRAIN  loss dict:  {'classification_loss': 0.9460837006568908}
2025-01-16 19:14:50,577 [INFO] Step[1350/2713]: training loss : 0.9495693218708038 TRAIN  loss dict:  {'classification_loss': 0.9495693218708038}
2025-01-16 19:15:06,771 [INFO] Step[1400/2713]: training loss : 1.0208767855167389 TRAIN  loss dict:  {'classification_loss': 1.0208767855167389}
2025-01-16 19:15:22,999 [INFO] Step[1450/2713]: training loss : 0.9484513127803802 TRAIN  loss dict:  {'classification_loss': 0.9484513127803802}
2025-01-16 19:15:39,225 [INFO] Step[1500/2713]: training loss : 0.9475064694881439 TRAIN  loss dict:  {'classification_loss': 0.9475064694881439}
2025-01-16 19:15:55,374 [INFO] Step[1550/2713]: training loss : 0.9391792225837707 TRAIN  loss dict:  {'classification_loss': 0.9391792225837707}
2025-01-16 19:16:11,548 [INFO] Step[1600/2713]: training loss : 0.9791592025756836 TRAIN  loss dict:  {'classification_loss': 0.9791592025756836}
2025-01-16 19:16:27,728 [INFO] Step[1650/2713]: training loss : 0.9435754895210267 TRAIN  loss dict:  {'classification_loss': 0.9435754895210267}
2025-01-16 19:16:43,904 [INFO] Step[1700/2713]: training loss : 0.9504542016983032 TRAIN  loss dict:  {'classification_loss': 0.9504542016983032}
2025-01-16 19:17:00,088 [INFO] Step[1750/2713]: training loss : 0.9420424389839173 TRAIN  loss dict:  {'classification_loss': 0.9420424389839173}
2025-01-16 19:17:16,317 [INFO] Step[1800/2713]: training loss : 0.9477092790603637 TRAIN  loss dict:  {'classification_loss': 0.9477092790603637}
2025-01-16 19:17:32,548 [INFO] Step[1850/2713]: training loss : 0.9778320729732514 TRAIN  loss dict:  {'classification_loss': 0.9778320729732514}
2025-01-16 19:17:48,802 [INFO] Step[1900/2713]: training loss : 0.9435731112957001 TRAIN  loss dict:  {'classification_loss': 0.9435731112957001}
2025-01-16 19:18:05,023 [INFO] Step[1950/2713]: training loss : 0.9572277927398681 TRAIN  loss dict:  {'classification_loss': 0.9572277927398681}
2025-01-16 19:18:21,213 [INFO] Step[2000/2713]: training loss : 0.9486655378341675 TRAIN  loss dict:  {'classification_loss': 0.9486655378341675}
2025-01-16 19:18:37,458 [INFO] Step[2050/2713]: training loss : 0.9736281383037567 TRAIN  loss dict:  {'classification_loss': 0.9736281383037567}
2025-01-16 19:18:53,683 [INFO] Step[2100/2713]: training loss : 0.9383052146434784 TRAIN  loss dict:  {'classification_loss': 0.9383052146434784}
2025-01-16 19:19:09,859 [INFO] Step[2150/2713]: training loss : 0.9571698248386383 TRAIN  loss dict:  {'classification_loss': 0.9571698248386383}
2025-01-16 19:19:26,112 [INFO] Step[2200/2713]: training loss : 1.0221046161651612 TRAIN  loss dict:  {'classification_loss': 1.0221046161651612}
2025-01-16 19:19:42,329 [INFO] Step[2250/2713]: training loss : 0.9467695522308349 TRAIN  loss dict:  {'classification_loss': 0.9467695522308349}
2025-01-16 19:19:58,516 [INFO] Step[2300/2713]: training loss : 0.962403644323349 TRAIN  loss dict:  {'classification_loss': 0.962403644323349}
2025-01-16 19:20:14,741 [INFO] Step[2350/2713]: training loss : 0.9485048604011536 TRAIN  loss dict:  {'classification_loss': 0.9485048604011536}
2025-01-16 19:20:30,917 [INFO] Step[2400/2713]: training loss : 0.9562186765670776 TRAIN  loss dict:  {'classification_loss': 0.9562186765670776}
2025-01-16 19:20:47,151 [INFO] Step[2450/2713]: training loss : 0.9408266258239746 TRAIN  loss dict:  {'classification_loss': 0.9408266258239746}
2025-01-16 19:21:03,333 [INFO] Step[2500/2713]: training loss : 1.0028613710403442 TRAIN  loss dict:  {'classification_loss': 1.0028613710403442}
2025-01-16 19:21:19,529 [INFO] Step[2550/2713]: training loss : 0.9699928605556488 TRAIN  loss dict:  {'classification_loss': 0.9699928605556488}
2025-01-16 19:21:35,708 [INFO] Step[2600/2713]: training loss : 0.948181699514389 TRAIN  loss dict:  {'classification_loss': 0.948181699514389}
2025-01-16 19:21:51,899 [INFO] Step[2650/2713]: training loss : 1.0049815368652344 TRAIN  loss dict:  {'classification_loss': 1.0049815368652344}
2025-01-16 19:22:08,209 [INFO] Step[2700/2713]: training loss : 0.9498212790489197 TRAIN  loss dict:  {'classification_loss': 0.9498212790489197}
2025-01-16 19:23:26,794 [INFO] Label accuracies statistics:
2025-01-16 19:23:26,794 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.25, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.5, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.5, 37: 1.0, 38: 0.75, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 1.0, 185: 0.5, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.5, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.25, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 1.0, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.25, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.75, 274: 0.25, 275: 0.75, 276: 0.75, 277: 1.0, 278: 1.0, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.25, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.5, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 19:23:26,796 [INFO] [34] TRAIN  loss: 0.9550831027458042 acc: 0.9955768521931441
2025-01-16 19:23:26,796 [INFO] [34] TRAIN  loss dict: {'classification_loss': 0.9550831027458042}
2025-01-16 19:23:26,796 [INFO] [34] VALIDATION loss: 1.9512256609093874 VALIDATION acc: 0.7780564263322884
2025-01-16 19:23:26,796 [INFO] [34] VALIDATION loss dict: {'classification_loss': 1.9512256609093874}
2025-01-16 19:23:26,796 [INFO] 
2025-01-16 19:23:48,271 [INFO] Step[50/2713]: training loss : 0.955706102848053 TRAIN  loss dict:  {'classification_loss': 0.955706102848053}
2025-01-16 19:24:04,369 [INFO] Step[100/2713]: training loss : 0.9404586911201477 TRAIN  loss dict:  {'classification_loss': 0.9404586911201477}
2025-01-16 19:24:20,566 [INFO] Step[150/2713]: training loss : 0.948506110906601 TRAIN  loss dict:  {'classification_loss': 0.948506110906601}
2025-01-16 19:24:36,746 [INFO] Step[200/2713]: training loss : 0.945179032087326 TRAIN  loss dict:  {'classification_loss': 0.945179032087326}
2025-01-16 19:24:52,931 [INFO] Step[250/2713]: training loss : 0.9367903649806977 TRAIN  loss dict:  {'classification_loss': 0.9367903649806977}
2025-01-16 19:25:09,063 [INFO] Step[300/2713]: training loss : 0.9761291098594665 TRAIN  loss dict:  {'classification_loss': 0.9761291098594665}
2025-01-16 19:25:25,314 [INFO] Step[350/2713]: training loss : 0.949138412475586 TRAIN  loss dict:  {'classification_loss': 0.949138412475586}
2025-01-16 19:25:41,553 [INFO] Step[400/2713]: training loss : 0.9808551216125488 TRAIN  loss dict:  {'classification_loss': 0.9808551216125488}
2025-01-16 19:25:57,776 [INFO] Step[450/2713]: training loss : 0.9474697375297546 TRAIN  loss dict:  {'classification_loss': 0.9474697375297546}
2025-01-16 19:26:13,950 [INFO] Step[500/2713]: training loss : 0.9583962738513947 TRAIN  loss dict:  {'classification_loss': 0.9583962738513947}
2025-01-16 19:26:30,157 [INFO] Step[550/2713]: training loss : 0.9458683943748474 TRAIN  loss dict:  {'classification_loss': 0.9458683943748474}
2025-01-16 19:26:46,323 [INFO] Step[600/2713]: training loss : 0.9520563077926636 TRAIN  loss dict:  {'classification_loss': 0.9520563077926636}
2025-01-16 19:27:02,560 [INFO] Step[650/2713]: training loss : 0.9711984002590179 TRAIN  loss dict:  {'classification_loss': 0.9711984002590179}
2025-01-16 19:27:18,705 [INFO] Step[700/2713]: training loss : 0.9606308901309967 TRAIN  loss dict:  {'classification_loss': 0.9606308901309967}
2025-01-16 19:27:34,917 [INFO] Step[750/2713]: training loss : 0.9748836445808411 TRAIN  loss dict:  {'classification_loss': 0.9748836445808411}
2025-01-16 19:27:51,126 [INFO] Step[800/2713]: training loss : 0.9433045196533203 TRAIN  loss dict:  {'classification_loss': 0.9433045196533203}
2025-01-16 19:28:07,332 [INFO] Step[850/2713]: training loss : 0.9783601188659667 TRAIN  loss dict:  {'classification_loss': 0.9783601188659667}
2025-01-16 19:28:23,474 [INFO] Step[900/2713]: training loss : 0.9653761553764343 TRAIN  loss dict:  {'classification_loss': 0.9653761553764343}
2025-01-16 19:28:39,684 [INFO] Step[950/2713]: training loss : 0.9400338804721833 TRAIN  loss dict:  {'classification_loss': 0.9400338804721833}
2025-01-16 19:28:55,867 [INFO] Step[1000/2713]: training loss : 0.9438433778285981 TRAIN  loss dict:  {'classification_loss': 0.9438433778285981}
2025-01-16 19:29:12,050 [INFO] Step[1050/2713]: training loss : 0.9378904807567596 TRAIN  loss dict:  {'classification_loss': 0.9378904807567596}
2025-01-16 19:29:28,214 [INFO] Step[1100/2713]: training loss : 0.9679422187805176 TRAIN  loss dict:  {'classification_loss': 0.9679422187805176}
2025-01-16 19:29:44,405 [INFO] Step[1150/2713]: training loss : 0.9707703554630279 TRAIN  loss dict:  {'classification_loss': 0.9707703554630279}
2025-01-16 19:30:00,582 [INFO] Step[1200/2713]: training loss : 0.9431865870952606 TRAIN  loss dict:  {'classification_loss': 0.9431865870952606}
2025-01-16 19:30:16,770 [INFO] Step[1250/2713]: training loss : 0.9647148418426513 TRAIN  loss dict:  {'classification_loss': 0.9647148418426513}
2025-01-16 19:30:32,955 [INFO] Step[1300/2713]: training loss : 0.9465318644046783 TRAIN  loss dict:  {'classification_loss': 0.9465318644046783}
2025-01-16 19:30:49,138 [INFO] Step[1350/2713]: training loss : 0.9727753937244416 TRAIN  loss dict:  {'classification_loss': 0.9727753937244416}
2025-01-16 19:31:05,344 [INFO] Step[1400/2713]: training loss : 0.9502664148807526 TRAIN  loss dict:  {'classification_loss': 0.9502664148807526}
2025-01-16 19:31:21,559 [INFO] Step[1450/2713]: training loss : 0.9575723314285278 TRAIN  loss dict:  {'classification_loss': 0.9575723314285278}
2025-01-16 19:31:37,804 [INFO] Step[1500/2713]: training loss : 0.9421017706394196 TRAIN  loss dict:  {'classification_loss': 0.9421017706394196}
2025-01-16 19:31:54,075 [INFO] Step[1550/2713]: training loss : 0.9732650876045227 TRAIN  loss dict:  {'classification_loss': 0.9732650876045227}
2025-01-16 19:32:10,197 [INFO] Step[1600/2713]: training loss : 0.9528981125354767 TRAIN  loss dict:  {'classification_loss': 0.9528981125354767}
2025-01-16 19:32:26,467 [INFO] Step[1650/2713]: training loss : 0.948314962387085 TRAIN  loss dict:  {'classification_loss': 0.948314962387085}
2025-01-16 19:32:42,722 [INFO] Step[1700/2713]: training loss : 0.978726110458374 TRAIN  loss dict:  {'classification_loss': 0.978726110458374}
2025-01-16 19:32:58,987 [INFO] Step[1750/2713]: training loss : 0.9441960191726685 TRAIN  loss dict:  {'classification_loss': 0.9441960191726685}
2025-01-16 19:33:15,169 [INFO] Step[1800/2713]: training loss : 0.9395820760726928 TRAIN  loss dict:  {'classification_loss': 0.9395820760726928}
2025-01-16 19:33:31,422 [INFO] Step[1850/2713]: training loss : 0.9409545588493348 TRAIN  loss dict:  {'classification_loss': 0.9409545588493348}
2025-01-16 19:33:47,567 [INFO] Step[1900/2713]: training loss : 0.9478476142883301 TRAIN  loss dict:  {'classification_loss': 0.9478476142883301}
2025-01-16 19:34:03,757 [INFO] Step[1950/2713]: training loss : 0.949523503780365 TRAIN  loss dict:  {'classification_loss': 0.949523503780365}
2025-01-16 19:34:19,980 [INFO] Step[2000/2713]: training loss : 0.9990351462364196 TRAIN  loss dict:  {'classification_loss': 0.9990351462364196}
2025-01-16 19:34:36,216 [INFO] Step[2050/2713]: training loss : 0.943937668800354 TRAIN  loss dict:  {'classification_loss': 0.943937668800354}
2025-01-16 19:34:52,440 [INFO] Step[2100/2713]: training loss : 0.9428304076194763 TRAIN  loss dict:  {'classification_loss': 0.9428304076194763}
2025-01-16 19:35:08,663 [INFO] Step[2150/2713]: training loss : 0.9482439172267914 TRAIN  loss dict:  {'classification_loss': 0.9482439172267914}
2025-01-16 19:35:24,830 [INFO] Step[2200/2713]: training loss : 0.9618953335285186 TRAIN  loss dict:  {'classification_loss': 0.9618953335285186}
2025-01-16 19:35:41,087 [INFO] Step[2250/2713]: training loss : 0.9643472051620483 TRAIN  loss dict:  {'classification_loss': 0.9643472051620483}
2025-01-16 19:35:57,288 [INFO] Step[2300/2713]: training loss : 0.9630786907672882 TRAIN  loss dict:  {'classification_loss': 0.9630786907672882}
2025-01-16 19:36:13,527 [INFO] Step[2350/2713]: training loss : 0.953821439743042 TRAIN  loss dict:  {'classification_loss': 0.953821439743042}
2025-01-16 19:36:29,682 [INFO] Step[2400/2713]: training loss : 0.95637260556221 TRAIN  loss dict:  {'classification_loss': 0.95637260556221}
2025-01-16 19:36:45,950 [INFO] Step[2450/2713]: training loss : 0.9735157823562622 TRAIN  loss dict:  {'classification_loss': 0.9735157823562622}
2025-01-16 19:37:02,134 [INFO] Step[2500/2713]: training loss : 0.9437399935722351 TRAIN  loss dict:  {'classification_loss': 0.9437399935722351}
2025-01-16 19:37:18,342 [INFO] Step[2550/2713]: training loss : 0.9703481936454773 TRAIN  loss dict:  {'classification_loss': 0.9703481936454773}
2025-01-16 19:37:34,525 [INFO] Step[2600/2713]: training loss : 0.9420389556884765 TRAIN  loss dict:  {'classification_loss': 0.9420389556884765}
2025-01-16 19:37:50,738 [INFO] Step[2650/2713]: training loss : 0.9533320438861846 TRAIN  loss dict:  {'classification_loss': 0.9533320438861846}
2025-01-16 19:38:07,003 [INFO] Step[2700/2713]: training loss : 0.9537140059471131 TRAIN  loss dict:  {'classification_loss': 0.9537140059471131}
2025-01-16 19:39:25,969 [INFO] Label accuracies statistics:
2025-01-16 19:39:25,969 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.25, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.25, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.5, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.25, 190: 0.75, 191: 0.0, 192: 1.0, 193: 1.0, 194: 0.75, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.5, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.25, 274: 0.25, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.75, 338: 0.25, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.5, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 19:39:25,971 [INFO] [35] TRAIN  loss: 0.9557609881943457 acc: 0.9944710652414301
2025-01-16 19:39:25,971 [INFO] [35] TRAIN  loss dict: {'classification_loss': 0.9557609881943457}
2025-01-16 19:39:25,971 [INFO] [35] VALIDATION loss: 1.9317595538564194 VALIDATION acc: 0.7705329153605016
2025-01-16 19:39:25,971 [INFO] [35] VALIDATION loss dict: {'classification_loss': 1.9317595538564194}
2025-01-16 19:39:25,971 [INFO] 
2025-01-16 19:39:46,757 [INFO] Step[50/2713]: training loss : 0.9478358197212219 TRAIN  loss dict:  {'classification_loss': 0.9478358197212219}
2025-01-16 19:40:02,878 [INFO] Step[100/2713]: training loss : 0.9451309776306153 TRAIN  loss dict:  {'classification_loss': 0.9451309776306153}
2025-01-16 19:40:19,030 [INFO] Step[150/2713]: training loss : 0.9530969858169556 TRAIN  loss dict:  {'classification_loss': 0.9530969858169556}
2025-01-16 19:40:35,228 [INFO] Step[200/2713]: training loss : 0.9722698962688446 TRAIN  loss dict:  {'classification_loss': 0.9722698962688446}
2025-01-16 19:40:51,464 [INFO] Step[250/2713]: training loss : 0.9669946503639221 TRAIN  loss dict:  {'classification_loss': 0.9669946503639221}
2025-01-16 19:41:07,729 [INFO] Step[300/2713]: training loss : 0.9384437906742096 TRAIN  loss dict:  {'classification_loss': 0.9384437906742096}
2025-01-16 19:41:23,947 [INFO] Step[350/2713]: training loss : 0.9549191093444824 TRAIN  loss dict:  {'classification_loss': 0.9549191093444824}
2025-01-16 19:41:40,094 [INFO] Step[400/2713]: training loss : 0.9618584442138672 TRAIN  loss dict:  {'classification_loss': 0.9618584442138672}
2025-01-16 19:41:56,237 [INFO] Step[450/2713]: training loss : 0.9711205768585205 TRAIN  loss dict:  {'classification_loss': 0.9711205768585205}
2025-01-16 19:42:12,397 [INFO] Step[500/2713]: training loss : 0.9439126420021057 TRAIN  loss dict:  {'classification_loss': 0.9439126420021057}
2025-01-16 19:42:28,638 [INFO] Step[550/2713]: training loss : 0.9527235889434814 TRAIN  loss dict:  {'classification_loss': 0.9527235889434814}
2025-01-16 19:42:44,798 [INFO] Step[600/2713]: training loss : 0.9443890941143036 TRAIN  loss dict:  {'classification_loss': 0.9443890941143036}
2025-01-16 19:43:00,965 [INFO] Step[650/2713]: training loss : 0.9454828715324402 TRAIN  loss dict:  {'classification_loss': 0.9454828715324402}
2025-01-16 19:43:17,135 [INFO] Step[700/2713]: training loss : 0.943508986234665 TRAIN  loss dict:  {'classification_loss': 0.943508986234665}
2025-01-16 19:43:33,326 [INFO] Step[750/2713]: training loss : 0.9390455877780914 TRAIN  loss dict:  {'classification_loss': 0.9390455877780914}
2025-01-16 19:43:49,479 [INFO] Step[800/2713]: training loss : 0.9670931971073151 TRAIN  loss dict:  {'classification_loss': 0.9670931971073151}
2025-01-16 19:44:05,686 [INFO] Step[850/2713]: training loss : 0.9531561124324799 TRAIN  loss dict:  {'classification_loss': 0.9531561124324799}
2025-01-16 19:44:21,878 [INFO] Step[900/2713]: training loss : 0.957249128818512 TRAIN  loss dict:  {'classification_loss': 0.957249128818512}
2025-01-16 19:44:38,074 [INFO] Step[950/2713]: training loss : 0.9450761568546295 TRAIN  loss dict:  {'classification_loss': 0.9450761568546295}
2025-01-16 19:44:54,187 [INFO] Step[1000/2713]: training loss : 0.9493899643421173 TRAIN  loss dict:  {'classification_loss': 0.9493899643421173}
2025-01-16 19:45:10,410 [INFO] Step[1050/2713]: training loss : 0.9461899638175965 TRAIN  loss dict:  {'classification_loss': 0.9461899638175965}
2025-01-16 19:45:26,623 [INFO] Step[1100/2713]: training loss : 0.958618801832199 TRAIN  loss dict:  {'classification_loss': 0.958618801832199}
2025-01-16 19:45:42,807 [INFO] Step[1150/2713]: training loss : 0.9384805953502655 TRAIN  loss dict:  {'classification_loss': 0.9384805953502655}
2025-01-16 19:45:58,978 [INFO] Step[1200/2713]: training loss : 0.9439786648750306 TRAIN  loss dict:  {'classification_loss': 0.9439786648750306}
2025-01-16 19:46:15,237 [INFO] Step[1250/2713]: training loss : 0.9659423232078552 TRAIN  loss dict:  {'classification_loss': 0.9659423232078552}
2025-01-16 19:46:31,465 [INFO] Step[1300/2713]: training loss : 0.9519000232219696 TRAIN  loss dict:  {'classification_loss': 0.9519000232219696}
2025-01-16 19:46:47,628 [INFO] Step[1350/2713]: training loss : 0.9887202632427216 TRAIN  loss dict:  {'classification_loss': 0.9887202632427216}
2025-01-16 19:47:03,780 [INFO] Step[1400/2713]: training loss : 0.9514455652236938 TRAIN  loss dict:  {'classification_loss': 0.9514455652236938}
2025-01-16 19:47:19,967 [INFO] Step[1450/2713]: training loss : 0.9782185733318329 TRAIN  loss dict:  {'classification_loss': 0.9782185733318329}
2025-01-16 19:47:36,175 [INFO] Step[1500/2713]: training loss : 0.9729718828201294 TRAIN  loss dict:  {'classification_loss': 0.9729718828201294}
2025-01-16 19:47:52,301 [INFO] Step[1550/2713]: training loss : 0.9406841289997101 TRAIN  loss dict:  {'classification_loss': 0.9406841289997101}
2025-01-16 19:48:08,491 [INFO] Step[1600/2713]: training loss : 0.9610498583316803 TRAIN  loss dict:  {'classification_loss': 0.9610498583316803}
2025-01-16 19:48:24,726 [INFO] Step[1650/2713]: training loss : 0.9619044899940491 TRAIN  loss dict:  {'classification_loss': 0.9619044899940491}
2025-01-16 19:48:40,918 [INFO] Step[1700/2713]: training loss : 0.9438621413707733 TRAIN  loss dict:  {'classification_loss': 0.9438621413707733}
2025-01-16 19:48:57,126 [INFO] Step[1750/2713]: training loss : 0.9823469269275665 TRAIN  loss dict:  {'classification_loss': 0.9823469269275665}
2025-01-16 19:49:13,373 [INFO] Step[1800/2713]: training loss : 0.9562754249572754 TRAIN  loss dict:  {'classification_loss': 0.9562754249572754}
2025-01-16 19:49:29,598 [INFO] Step[1850/2713]: training loss : 0.9402433228492737 TRAIN  loss dict:  {'classification_loss': 0.9402433228492737}
2025-01-16 19:49:45,833 [INFO] Step[1900/2713]: training loss : 0.9558924722671509 TRAIN  loss dict:  {'classification_loss': 0.9558924722671509}
2025-01-16 19:50:02,011 [INFO] Step[1950/2713]: training loss : 0.9389053881168365 TRAIN  loss dict:  {'classification_loss': 0.9389053881168365}
2025-01-16 19:50:18,175 [INFO] Step[2000/2713]: training loss : 1.0357679319381714 TRAIN  loss dict:  {'classification_loss': 1.0357679319381714}
2025-01-16 19:50:34,354 [INFO] Step[2050/2713]: training loss : 0.949236605167389 TRAIN  loss dict:  {'classification_loss': 0.949236605167389}
2025-01-16 19:50:50,586 [INFO] Step[2100/2713]: training loss : 0.9452390503883362 TRAIN  loss dict:  {'classification_loss': 0.9452390503883362}
2025-01-16 19:51:06,821 [INFO] Step[2150/2713]: training loss : 0.9424086940288544 TRAIN  loss dict:  {'classification_loss': 0.9424086940288544}
2025-01-16 19:51:22,972 [INFO] Step[2200/2713]: training loss : 0.946519227027893 TRAIN  loss dict:  {'classification_loss': 0.946519227027893}
2025-01-16 19:51:39,202 [INFO] Step[2250/2713]: training loss : 0.9428964066505432 TRAIN  loss dict:  {'classification_loss': 0.9428964066505432}
2025-01-16 19:51:55,358 [INFO] Step[2300/2713]: training loss : 0.9987173402309417 TRAIN  loss dict:  {'classification_loss': 0.9987173402309417}
2025-01-16 19:52:11,517 [INFO] Step[2350/2713]: training loss : 0.9438916158676147 TRAIN  loss dict:  {'classification_loss': 0.9438916158676147}
2025-01-16 19:52:27,680 [INFO] Step[2400/2713]: training loss : 0.943629322052002 TRAIN  loss dict:  {'classification_loss': 0.943629322052002}
2025-01-16 19:52:43,884 [INFO] Step[2450/2713]: training loss : 0.9563187980651855 TRAIN  loss dict:  {'classification_loss': 0.9563187980651855}
2025-01-16 19:53:00,095 [INFO] Step[2500/2713]: training loss : 0.9472861313819885 TRAIN  loss dict:  {'classification_loss': 0.9472861313819885}
2025-01-16 19:53:16,304 [INFO] Step[2550/2713]: training loss : 1.0113669335842133 TRAIN  loss dict:  {'classification_loss': 1.0113669335842133}
2025-01-16 19:53:32,450 [INFO] Step[2600/2713]: training loss : 0.9917607319355011 TRAIN  loss dict:  {'classification_loss': 0.9917607319355011}
2025-01-16 19:53:48,665 [INFO] Step[2650/2713]: training loss : 0.9429001903533936 TRAIN  loss dict:  {'classification_loss': 0.9429001903533936}
2025-01-16 19:54:04,827 [INFO] Step[2700/2713]: training loss : 0.939392020702362 TRAIN  loss dict:  {'classification_loss': 0.939392020702362}
2025-01-16 19:55:23,080 [INFO] Label accuracies statistics:
2025-01-16 19:55:23,080 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 1.0, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.25, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.5, 78: 1.0, 79: 0.5, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.5, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 0.5, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.5, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 1.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 1.0, 219: 0.75, 220: 1.0, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.25, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.0, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.25, 274: 0.25, 275: 0.75, 276: 1.0, 277: 1.0, 278: 1.0, 279: 1.0, 280: 1.0, 281: 0.5, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 0.5, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.5, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.25, 351: 0.75, 352: 0.0, 353: 0.5, 354: 0.25, 355: 0.5, 356: 0.75, 357: 0.75, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.5, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.5, 391: 0.75, 392: 1.0, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 19:55:23,082 [INFO] [36] TRAIN  loss: 0.957210457988476 acc: 0.9938567391571446
2025-01-16 19:55:23,082 [INFO] [36] TRAIN  loss dict: {'classification_loss': 0.957210457988476}
2025-01-16 19:55:23,082 [INFO] [36] VALIDATION loss: 1.932790853251192 VALIDATION acc: 0.774294670846395
2025-01-16 19:55:23,082 [INFO] [36] VALIDATION loss dict: {'classification_loss': 1.932790853251192}
2025-01-16 19:55:23,082 [INFO] 
2025-01-16 19:55:43,922 [INFO] Step[50/2713]: training loss : 0.9481953299045562 TRAIN  loss dict:  {'classification_loss': 0.9481953299045562}
2025-01-16 19:55:59,991 [INFO] Step[100/2713]: training loss : 1.0005513226985931 TRAIN  loss dict:  {'classification_loss': 1.0005513226985931}
2025-01-16 19:56:16,168 [INFO] Step[150/2713]: training loss : 0.9454782712459564 TRAIN  loss dict:  {'classification_loss': 0.9454782712459564}
2025-01-16 19:56:32,307 [INFO] Step[200/2713]: training loss : 0.9623032784461976 TRAIN  loss dict:  {'classification_loss': 0.9623032784461976}
2025-01-16 19:56:48,447 [INFO] Step[250/2713]: training loss : 0.9464715588092804 TRAIN  loss dict:  {'classification_loss': 0.9464715588092804}
2025-01-16 19:57:04,598 [INFO] Step[300/2713]: training loss : 0.986397716999054 TRAIN  loss dict:  {'classification_loss': 0.986397716999054}
2025-01-16 19:57:20,799 [INFO] Step[350/2713]: training loss : 0.9571826124191284 TRAIN  loss dict:  {'classification_loss': 0.9571826124191284}
2025-01-16 19:57:36,998 [INFO] Step[400/2713]: training loss : 0.9377911353111267 TRAIN  loss dict:  {'classification_loss': 0.9377911353111267}
2025-01-16 19:57:53,145 [INFO] Step[450/2713]: training loss : 0.9393835020065308 TRAIN  loss dict:  {'classification_loss': 0.9393835020065308}
2025-01-16 19:58:09,329 [INFO] Step[500/2713]: training loss : 0.9402092719078063 TRAIN  loss dict:  {'classification_loss': 0.9402092719078063}
2025-01-16 19:58:25,476 [INFO] Step[550/2713]: training loss : 0.9365213000774384 TRAIN  loss dict:  {'classification_loss': 0.9365213000774384}
2025-01-16 19:58:41,601 [INFO] Step[600/2713]: training loss : 0.9374815320968628 TRAIN  loss dict:  {'classification_loss': 0.9374815320968628}
2025-01-16 19:58:57,719 [INFO] Step[650/2713]: training loss : 0.9428133249282837 TRAIN  loss dict:  {'classification_loss': 0.9428133249282837}
2025-01-16 19:59:13,887 [INFO] Step[700/2713]: training loss : 0.9678142547607422 TRAIN  loss dict:  {'classification_loss': 0.9678142547607422}
2025-01-16 19:59:30,174 [INFO] Step[750/2713]: training loss : 0.9417694592475891 TRAIN  loss dict:  {'classification_loss': 0.9417694592475891}
2025-01-16 19:59:46,337 [INFO] Step[800/2713]: training loss : 0.9475787174701691 TRAIN  loss dict:  {'classification_loss': 0.9475787174701691}
2025-01-16 20:00:02,467 [INFO] Step[850/2713]: training loss : 0.9715195524692536 TRAIN  loss dict:  {'classification_loss': 0.9715195524692536}
2025-01-16 20:00:18,590 [INFO] Step[900/2713]: training loss : 0.9394387555122375 TRAIN  loss dict:  {'classification_loss': 0.9394387555122375}
2025-01-16 20:00:34,732 [INFO] Step[950/2713]: training loss : 0.9686981916427613 TRAIN  loss dict:  {'classification_loss': 0.9686981916427613}
2025-01-16 20:00:50,886 [INFO] Step[1000/2713]: training loss : 0.9403237080574036 TRAIN  loss dict:  {'classification_loss': 0.9403237080574036}
2025-01-16 20:01:07,027 [INFO] Step[1050/2713]: training loss : 0.9470814800262451 TRAIN  loss dict:  {'classification_loss': 0.9470814800262451}
2025-01-16 20:01:23,202 [INFO] Step[1100/2713]: training loss : 0.93844806432724 TRAIN  loss dict:  {'classification_loss': 0.93844806432724}
2025-01-16 20:01:39,369 [INFO] Step[1150/2713]: training loss : 0.9496473884582519 TRAIN  loss dict:  {'classification_loss': 0.9496473884582519}
2025-01-16 20:01:55,534 [INFO] Step[1200/2713]: training loss : 0.9450404548645019 TRAIN  loss dict:  {'classification_loss': 0.9450404548645019}
2025-01-16 20:02:11,696 [INFO] Step[1250/2713]: training loss : 0.9386510074138641 TRAIN  loss dict:  {'classification_loss': 0.9386510074138641}
2025-01-16 20:02:27,866 [INFO] Step[1300/2713]: training loss : 0.9368206143379212 TRAIN  loss dict:  {'classification_loss': 0.9368206143379212}
2025-01-16 20:02:44,098 [INFO] Step[1350/2713]: training loss : 0.9389359712600708 TRAIN  loss dict:  {'classification_loss': 0.9389359712600708}
2025-01-16 20:03:00,298 [INFO] Step[1400/2713]: training loss : 0.9580244076251984 TRAIN  loss dict:  {'classification_loss': 0.9580244076251984}
2025-01-16 20:03:16,428 [INFO] Step[1450/2713]: training loss : 0.9502727055549621 TRAIN  loss dict:  {'classification_loss': 0.9502727055549621}
2025-01-16 20:03:32,526 [INFO] Step[1500/2713]: training loss : 0.9610310828685761 TRAIN  loss dict:  {'classification_loss': 0.9610310828685761}
2025-01-16 20:03:48,632 [INFO] Step[1550/2713]: training loss : 0.9496180665493011 TRAIN  loss dict:  {'classification_loss': 0.9496180665493011}
2025-01-16 20:04:04,807 [INFO] Step[1600/2713]: training loss : 0.9371386015415192 TRAIN  loss dict:  {'classification_loss': 0.9371386015415192}
2025-01-16 20:04:20,956 [INFO] Step[1650/2713]: training loss : 0.9385928869247436 TRAIN  loss dict:  {'classification_loss': 0.9385928869247436}
2025-01-16 20:04:37,087 [INFO] Step[1700/2713]: training loss : 0.9465068864822388 TRAIN  loss dict:  {'classification_loss': 0.9465068864822388}
2025-01-16 20:04:53,248 [INFO] Step[1750/2713]: training loss : 0.9368660438060761 TRAIN  loss dict:  {'classification_loss': 0.9368660438060761}
2025-01-16 20:05:09,520 [INFO] Step[1800/2713]: training loss : 0.960890862941742 TRAIN  loss dict:  {'classification_loss': 0.960890862941742}
2025-01-16 20:05:25,646 [INFO] Step[1850/2713]: training loss : 0.9945616054534913 TRAIN  loss dict:  {'classification_loss': 0.9945616054534913}
2025-01-16 20:05:41,775 [INFO] Step[1900/2713]: training loss : 0.9515451312065124 TRAIN  loss dict:  {'classification_loss': 0.9515451312065124}
2025-01-16 20:05:57,925 [INFO] Step[1950/2713]: training loss : 0.944747064113617 TRAIN  loss dict:  {'classification_loss': 0.944747064113617}
2025-01-16 20:06:14,090 [INFO] Step[2000/2713]: training loss : 0.9874224877357483 TRAIN  loss dict:  {'classification_loss': 0.9874224877357483}
2025-01-16 20:06:30,214 [INFO] Step[2050/2713]: training loss : 0.9946056187152863 TRAIN  loss dict:  {'classification_loss': 0.9946056187152863}
2025-01-16 20:06:46,343 [INFO] Step[2100/2713]: training loss : 0.9394837296009064 TRAIN  loss dict:  {'classification_loss': 0.9394837296009064}
2025-01-16 20:07:02,515 [INFO] Step[2150/2713]: training loss : 0.9371447622776031 TRAIN  loss dict:  {'classification_loss': 0.9371447622776031}
2025-01-16 20:07:18,662 [INFO] Step[2200/2713]: training loss : 0.9658848023414612 TRAIN  loss dict:  {'classification_loss': 0.9658848023414612}
2025-01-16 20:07:34,841 [INFO] Step[2250/2713]: training loss : 0.9439627647399902 TRAIN  loss dict:  {'classification_loss': 0.9439627647399902}
2025-01-16 20:07:51,001 [INFO] Step[2300/2713]: training loss : 0.938773558139801 TRAIN  loss dict:  {'classification_loss': 0.938773558139801}
2025-01-16 20:08:07,232 [INFO] Step[2350/2713]: training loss : 0.9386697232723236 TRAIN  loss dict:  {'classification_loss': 0.9386697232723236}
2025-01-16 20:08:23,372 [INFO] Step[2400/2713]: training loss : 0.9377692234516144 TRAIN  loss dict:  {'classification_loss': 0.9377692234516144}
2025-01-16 20:08:39,547 [INFO] Step[2450/2713]: training loss : 0.9450786077976226 TRAIN  loss dict:  {'classification_loss': 0.9450786077976226}
2025-01-16 20:08:55,713 [INFO] Step[2500/2713]: training loss : 0.938419657945633 TRAIN  loss dict:  {'classification_loss': 0.938419657945633}
2025-01-16 20:09:11,956 [INFO] Step[2550/2713]: training loss : 0.9504746508598327 TRAIN  loss dict:  {'classification_loss': 0.9504746508598327}
2025-01-16 20:09:28,144 [INFO] Step[2600/2713]: training loss : 0.9489498662948609 TRAIN  loss dict:  {'classification_loss': 0.9489498662948609}
2025-01-16 20:09:44,351 [INFO] Step[2650/2713]: training loss : 0.9401055705547333 TRAIN  loss dict:  {'classification_loss': 0.9401055705547333}
2025-01-16 20:10:00,507 [INFO] Step[2700/2713]: training loss : 0.9414442658424378 TRAIN  loss dict:  {'classification_loss': 0.9414442658424378}
2025-01-16 20:11:18,887 [INFO] Label accuracies statistics:
2025-01-16 20:11:18,887 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.25, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.5, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.5, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.75, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 0.25, 235: 0.5, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.5, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 0.5, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.25, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.25, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 0.5, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.5, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.5, 329: 0.75, 330: 0.5, 331: 0.5, 332: 0.5, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.25, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.5, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-16 20:11:18,889 [INFO] [37] TRAIN  loss: 0.9504984313168299 acc: 0.99533112175943
2025-01-16 20:11:18,889 [INFO] [37] TRAIN  loss dict: {'classification_loss': 0.9504984313168299}
2025-01-16 20:11:18,889 [INFO] [37] VALIDATION loss: 1.92893767222426 VALIDATION acc: 0.7774294670846394
2025-01-16 20:11:18,889 [INFO] [37] VALIDATION loss dict: {'classification_loss': 1.92893767222426}
2025-01-16 20:11:18,889 [INFO] 
2025-01-16 20:11:39,747 [INFO] Step[50/2713]: training loss : 0.9658830821514129 TRAIN  loss dict:  {'classification_loss': 0.9658830821514129}
2025-01-16 20:11:55,838 [INFO] Step[100/2713]: training loss : 0.9390248930454255 TRAIN  loss dict:  {'classification_loss': 0.9390248930454255}
2025-01-16 20:12:11,996 [INFO] Step[150/2713]: training loss : 0.9386217260360717 TRAIN  loss dict:  {'classification_loss': 0.9386217260360717}
2025-01-16 20:12:28,184 [INFO] Step[200/2713]: training loss : 0.9406343376636506 TRAIN  loss dict:  {'classification_loss': 0.9406343376636506}
2025-01-16 20:12:44,387 [INFO] Step[250/2713]: training loss : 0.9412631595134735 TRAIN  loss dict:  {'classification_loss': 0.9412631595134735}
2025-01-16 20:13:00,514 [INFO] Step[300/2713]: training loss : 0.9373320078849793 TRAIN  loss dict:  {'classification_loss': 0.9373320078849793}
2025-01-16 20:13:16,698 [INFO] Step[350/2713]: training loss : 0.9929553020000458 TRAIN  loss dict:  {'classification_loss': 0.9929553020000458}
2025-01-16 20:13:32,855 [INFO] Step[400/2713]: training loss : 0.9642280972003937 TRAIN  loss dict:  {'classification_loss': 0.9642280972003937}
2025-01-16 20:13:49,033 [INFO] Step[450/2713]: training loss : 0.9560240268707275 TRAIN  loss dict:  {'classification_loss': 0.9560240268707275}
2025-01-16 20:14:05,233 [INFO] Step[500/2713]: training loss : 0.9571224939823151 TRAIN  loss dict:  {'classification_loss': 0.9571224939823151}
2025-01-16 20:14:21,396 [INFO] Step[550/2713]: training loss : 0.969803477525711 TRAIN  loss dict:  {'classification_loss': 0.969803477525711}
2025-01-16 20:14:37,498 [INFO] Step[600/2713]: training loss : 0.9394454038143158 TRAIN  loss dict:  {'classification_loss': 0.9394454038143158}
2025-01-16 20:14:53,654 [INFO] Step[650/2713]: training loss : 0.9589175176620484 TRAIN  loss dict:  {'classification_loss': 0.9589175176620484}
2025-01-16 20:15:09,856 [INFO] Step[700/2713]: training loss : 0.9398103559017181 TRAIN  loss dict:  {'classification_loss': 0.9398103559017181}
2025-01-16 20:15:25,994 [INFO] Step[750/2713]: training loss : 0.9438799321651459 TRAIN  loss dict:  {'classification_loss': 0.9438799321651459}
2025-01-16 20:15:42,139 [INFO] Step[800/2713]: training loss : 0.946016411781311 TRAIN  loss dict:  {'classification_loss': 0.946016411781311}
2025-01-16 20:15:58,317 [INFO] Step[850/2713]: training loss : 0.9453581690788269 TRAIN  loss dict:  {'classification_loss': 0.9453581690788269}
2025-01-16 20:16:14,455 [INFO] Step[900/2713]: training loss : 1.0025426971912383 TRAIN  loss dict:  {'classification_loss': 1.0025426971912383}
2025-01-16 20:16:30,656 [INFO] Step[950/2713]: training loss : 0.9388720881938935 TRAIN  loss dict:  {'classification_loss': 0.9388720881938935}
2025-01-16 20:16:46,812 [INFO] Step[1000/2713]: training loss : 0.9410799682140351 TRAIN  loss dict:  {'classification_loss': 0.9410799682140351}
2025-01-16 20:17:02,931 [INFO] Step[1050/2713]: training loss : 0.9476826882362366 TRAIN  loss dict:  {'classification_loss': 0.9476826882362366}
2025-01-16 20:17:19,086 [INFO] Step[1100/2713]: training loss : 0.9414793193340302 TRAIN  loss dict:  {'classification_loss': 0.9414793193340302}
2025-01-16 20:17:35,296 [INFO] Step[1150/2713]: training loss : 0.9650024616718292 TRAIN  loss dict:  {'classification_loss': 0.9650024616718292}
2025-01-16 20:17:51,423 [INFO] Step[1200/2713]: training loss : 0.9516216039657592 TRAIN  loss dict:  {'classification_loss': 0.9516216039657592}
2025-01-16 20:18:07,554 [INFO] Step[1250/2713]: training loss : 0.9516185152530671 TRAIN  loss dict:  {'classification_loss': 0.9516185152530671}
2025-01-16 20:18:23,701 [INFO] Step[1300/2713]: training loss : 0.9422802770137787 TRAIN  loss dict:  {'classification_loss': 0.9422802770137787}
2025-01-16 20:18:39,833 [INFO] Step[1350/2713]: training loss : 1.0328799438476564 TRAIN  loss dict:  {'classification_loss': 1.0328799438476564}
2025-01-16 20:18:55,990 [INFO] Step[1400/2713]: training loss : 0.9509383249282837 TRAIN  loss dict:  {'classification_loss': 0.9509383249282837}
2025-01-16 20:19:12,121 [INFO] Step[1450/2713]: training loss : 0.9607492339611053 TRAIN  loss dict:  {'classification_loss': 0.9607492339611053}
2025-01-16 20:19:28,364 [INFO] Step[1500/2713]: training loss : 0.9425603294372559 TRAIN  loss dict:  {'classification_loss': 0.9425603294372559}
2025-01-16 20:19:44,551 [INFO] Step[1550/2713]: training loss : 0.9906355226039887 TRAIN  loss dict:  {'classification_loss': 0.9906355226039887}
2025-01-16 20:20:00,693 [INFO] Step[1600/2713]: training loss : 0.9457921385765076 TRAIN  loss dict:  {'classification_loss': 0.9457921385765076}
2025-01-16 20:20:16,831 [INFO] Step[1650/2713]: training loss : 0.9383005464076996 TRAIN  loss dict:  {'classification_loss': 0.9383005464076996}
2025-01-16 20:20:32,995 [INFO] Step[1700/2713]: training loss : 0.942570333480835 TRAIN  loss dict:  {'classification_loss': 0.942570333480835}
2025-01-16 20:20:49,205 [INFO] Step[1750/2713]: training loss : 0.9517668926715851 TRAIN  loss dict:  {'classification_loss': 0.9517668926715851}
2025-01-16 20:21:05,304 [INFO] Step[1800/2713]: training loss : 0.9463653159141541 TRAIN  loss dict:  {'classification_loss': 0.9463653159141541}
2025-01-16 20:21:21,455 [INFO] Step[1850/2713]: training loss : 0.9732778000831604 TRAIN  loss dict:  {'classification_loss': 0.9732778000831604}
2025-01-16 20:21:37,553 [INFO] Step[1900/2713]: training loss : 0.9687922096252441 TRAIN  loss dict:  {'classification_loss': 0.9687922096252441}
2025-01-16 20:21:53,759 [INFO] Step[1950/2713]: training loss : 0.9383077621459961 TRAIN  loss dict:  {'classification_loss': 0.9383077621459961}
2025-01-16 20:22:09,841 [INFO] Step[2000/2713]: training loss : 0.964842255115509 TRAIN  loss dict:  {'classification_loss': 0.964842255115509}
2025-01-16 20:22:26,081 [INFO] Step[2050/2713]: training loss : 0.9362316012382508 TRAIN  loss dict:  {'classification_loss': 0.9362316012382508}
2025-01-16 20:22:42,251 [INFO] Step[2100/2713]: training loss : 0.962710303068161 TRAIN  loss dict:  {'classification_loss': 0.962710303068161}
2025-01-16 20:22:58,400 [INFO] Step[2150/2713]: training loss : 0.9505306220054627 TRAIN  loss dict:  {'classification_loss': 0.9505306220054627}
2025-01-16 20:23:14,496 [INFO] Step[2200/2713]: training loss : 0.9441029608249665 TRAIN  loss dict:  {'classification_loss': 0.9441029608249665}
2025-01-16 20:23:30,655 [INFO] Step[2250/2713]: training loss : 0.9577322840690613 TRAIN  loss dict:  {'classification_loss': 0.9577322840690613}
2025-01-16 20:23:46,811 [INFO] Step[2300/2713]: training loss : 0.9443327987194061 TRAIN  loss dict:  {'classification_loss': 0.9443327987194061}
2025-01-16 20:24:03,011 [INFO] Step[2350/2713]: training loss : 0.9867723786830902 TRAIN  loss dict:  {'classification_loss': 0.9867723786830902}
2025-01-16 20:24:19,181 [INFO] Step[2400/2713]: training loss : 0.9676094305515289 TRAIN  loss dict:  {'classification_loss': 0.9676094305515289}
2025-01-16 20:24:35,363 [INFO] Step[2450/2713]: training loss : 0.9454587018489837 TRAIN  loss dict:  {'classification_loss': 0.9454587018489837}
2025-01-16 20:24:51,482 [INFO] Step[2500/2713]: training loss : 0.956998291015625 TRAIN  loss dict:  {'classification_loss': 0.956998291015625}
2025-01-16 20:25:07,646 [INFO] Step[2550/2713]: training loss : 0.9524319016933441 TRAIN  loss dict:  {'classification_loss': 0.9524319016933441}
2025-01-16 20:25:23,846 [INFO] Step[2600/2713]: training loss : 0.9377846848964692 TRAIN  loss dict:  {'classification_loss': 0.9377846848964692}
2025-01-16 20:25:39,967 [INFO] Step[2650/2713]: training loss : 0.9450475299358367 TRAIN  loss dict:  {'classification_loss': 0.9450475299358367}
2025-01-16 20:25:56,045 [INFO] Step[2700/2713]: training loss : 0.9410980415344238 TRAIN  loss dict:  {'classification_loss': 0.9410980415344238}
2025-01-16 20:27:14,681 [INFO] Label accuracies statistics:
2025-01-16 20:27:14,682 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.5, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.25, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 0.75, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.5, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.25, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 0.75, 197: 0.75, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.25, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.25, 260: 0.75, 261: 0.25, 262: 1.0, 263: 0.75, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.75, 268: 1.0, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 1.0, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.5, 319: 0.75, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.5, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.5, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.25, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-16 20:27:14,683 [INFO] [38] TRAIN  loss: 0.954344294253988 acc: 0.9949625261088586
2025-01-16 20:27:14,683 [INFO] [38] TRAIN  loss dict: {'classification_loss': 0.954344294253988}
2025-01-16 20:27:14,684 [INFO] [38] VALIDATION loss: 1.917336989726339 VALIDATION acc: 0.7836990595611285
2025-01-16 20:27:14,684 [INFO] [38] VALIDATION loss dict: {'classification_loss': 1.917336989726339}
2025-01-16 20:27:14,684 [INFO] 
2025-01-16 20:27:35,586 [INFO] Step[50/2713]: training loss : 1.0057856357097625 TRAIN  loss dict:  {'classification_loss': 1.0057856357097625}
2025-01-16 20:27:51,840 [INFO] Step[100/2713]: training loss : 0.9939611959457397 TRAIN  loss dict:  {'classification_loss': 0.9939611959457397}
2025-01-16 20:28:08,074 [INFO] Step[150/2713]: training loss : 0.9475804996490479 TRAIN  loss dict:  {'classification_loss': 0.9475804996490479}
2025-01-16 20:28:24,325 [INFO] Step[200/2713]: training loss : 0.9385813045501709 TRAIN  loss dict:  {'classification_loss': 0.9385813045501709}
2025-01-16 20:28:40,614 [INFO] Step[250/2713]: training loss : 0.983314973115921 TRAIN  loss dict:  {'classification_loss': 0.983314973115921}
2025-01-16 20:28:56,855 [INFO] Step[300/2713]: training loss : 0.9467278552055359 TRAIN  loss dict:  {'classification_loss': 0.9467278552055359}
2025-01-16 20:29:13,089 [INFO] Step[350/2713]: training loss : 0.940262805223465 TRAIN  loss dict:  {'classification_loss': 0.940262805223465}
2025-01-16 20:29:29,315 [INFO] Step[400/2713]: training loss : 0.9399132418632508 TRAIN  loss dict:  {'classification_loss': 0.9399132418632508}
2025-01-16 20:29:45,595 [INFO] Step[450/2713]: training loss : 0.9633733332157135 TRAIN  loss dict:  {'classification_loss': 0.9633733332157135}
2025-01-16 20:30:01,834 [INFO] Step[500/2713]: training loss : 0.9533467376232148 TRAIN  loss dict:  {'classification_loss': 0.9533467376232148}
2025-01-16 20:30:18,049 [INFO] Step[550/2713]: training loss : 0.9488058435916901 TRAIN  loss dict:  {'classification_loss': 0.9488058435916901}
2025-01-16 20:30:34,255 [INFO] Step[600/2713]: training loss : 0.9442277228832245 TRAIN  loss dict:  {'classification_loss': 0.9442277228832245}
2025-01-16 20:30:50,500 [INFO] Step[650/2713]: training loss : 0.9723266661167145 TRAIN  loss dict:  {'classification_loss': 0.9723266661167145}
2025-01-16 20:31:06,704 [INFO] Step[700/2713]: training loss : 0.9749099624156952 TRAIN  loss dict:  {'classification_loss': 0.9749099624156952}
2025-01-16 20:31:22,983 [INFO] Step[750/2713]: training loss : 0.9694751965999603 TRAIN  loss dict:  {'classification_loss': 0.9694751965999603}
2025-01-16 20:31:39,209 [INFO] Step[800/2713]: training loss : 0.938964147567749 TRAIN  loss dict:  {'classification_loss': 0.938964147567749}
2025-01-16 20:31:55,502 [INFO] Step[850/2713]: training loss : 0.9391692698001861 TRAIN  loss dict:  {'classification_loss': 0.9391692698001861}
2025-01-16 20:32:11,731 [INFO] Step[900/2713]: training loss : 0.9389324831962585 TRAIN  loss dict:  {'classification_loss': 0.9389324831962585}
2025-01-16 20:32:27,984 [INFO] Step[950/2713]: training loss : 0.9653732573986054 TRAIN  loss dict:  {'classification_loss': 0.9653732573986054}
2025-01-16 20:32:44,224 [INFO] Step[1000/2713]: training loss : 0.9813812482357025 TRAIN  loss dict:  {'classification_loss': 0.9813812482357025}
2025-01-16 20:33:00,431 [INFO] Step[1050/2713]: training loss : 0.9420241403579712 TRAIN  loss dict:  {'classification_loss': 0.9420241403579712}
2025-01-16 20:33:16,781 [INFO] Step[1100/2713]: training loss : 0.9395631492137909 TRAIN  loss dict:  {'classification_loss': 0.9395631492137909}
2025-01-16 20:33:33,077 [INFO] Step[1150/2713]: training loss : 0.9440608167648316 TRAIN  loss dict:  {'classification_loss': 0.9440608167648316}
2025-01-16 20:33:49,304 [INFO] Step[1200/2713]: training loss : 0.9598200571537018 TRAIN  loss dict:  {'classification_loss': 0.9598200571537018}
2025-01-16 20:34:05,547 [INFO] Step[1250/2713]: training loss : 0.955970264673233 TRAIN  loss dict:  {'classification_loss': 0.955970264673233}
2025-01-16 20:34:21,789 [INFO] Step[1300/2713]: training loss : 0.9558549022674561 TRAIN  loss dict:  {'classification_loss': 0.9558549022674561}
2025-01-16 20:34:38,102 [INFO] Step[1350/2713]: training loss : 0.9378433072566986 TRAIN  loss dict:  {'classification_loss': 0.9378433072566986}
2025-01-16 20:34:54,354 [INFO] Step[1400/2713]: training loss : 0.9530733168125153 TRAIN  loss dict:  {'classification_loss': 0.9530733168125153}
2025-01-16 20:35:10,678 [INFO] Step[1450/2713]: training loss : 0.9472490477561951 TRAIN  loss dict:  {'classification_loss': 0.9472490477561951}
2025-01-16 20:35:26,977 [INFO] Step[1500/2713]: training loss : 0.942007110118866 TRAIN  loss dict:  {'classification_loss': 0.942007110118866}
2025-01-16 20:35:43,307 [INFO] Step[1550/2713]: training loss : 0.981087327003479 TRAIN  loss dict:  {'classification_loss': 0.981087327003479}
2025-01-16 20:35:59,592 [INFO] Step[1600/2713]: training loss : 0.9395471394062043 TRAIN  loss dict:  {'classification_loss': 0.9395471394062043}
2025-01-16 20:36:15,875 [INFO] Step[1650/2713]: training loss : 0.9390210843086243 TRAIN  loss dict:  {'classification_loss': 0.9390210843086243}
2025-01-16 20:36:32,093 [INFO] Step[1700/2713]: training loss : 0.939775378704071 TRAIN  loss dict:  {'classification_loss': 0.939775378704071}
2025-01-16 20:36:48,386 [INFO] Step[1750/2713]: training loss : 0.9559811544418335 TRAIN  loss dict:  {'classification_loss': 0.9559811544418335}
2025-01-16 20:37:04,654 [INFO] Step[1800/2713]: training loss : 0.9376419222354889 TRAIN  loss dict:  {'classification_loss': 0.9376419222354889}
2025-01-16 20:37:20,954 [INFO] Step[1850/2713]: training loss : 0.940262199640274 TRAIN  loss dict:  {'classification_loss': 0.940262199640274}
2025-01-16 20:37:37,247 [INFO] Step[1900/2713]: training loss : 0.9378697729110718 TRAIN  loss dict:  {'classification_loss': 0.9378697729110718}
2025-01-16 20:37:53,507 [INFO] Step[1950/2713]: training loss : 0.9402348959445953 TRAIN  loss dict:  {'classification_loss': 0.9402348959445953}
2025-01-16 20:38:09,702 [INFO] Step[2000/2713]: training loss : 0.9359612452983856 TRAIN  loss dict:  {'classification_loss': 0.9359612452983856}
2025-01-16 20:38:25,988 [INFO] Step[2050/2713]: training loss : 0.9594141781330109 TRAIN  loss dict:  {'classification_loss': 0.9594141781330109}
2025-01-16 20:38:42,253 [INFO] Step[2100/2713]: training loss : 0.9590590322017669 TRAIN  loss dict:  {'classification_loss': 0.9590590322017669}
2025-01-16 20:38:58,401 [INFO] Step[2150/2713]: training loss : 0.9409294247627258 TRAIN  loss dict:  {'classification_loss': 0.9409294247627258}
2025-01-16 20:39:14,620 [INFO] Step[2200/2713]: training loss : 0.9400421965122223 TRAIN  loss dict:  {'classification_loss': 0.9400421965122223}
2025-01-16 20:39:30,950 [INFO] Step[2250/2713]: training loss : 0.9578100550174713 TRAIN  loss dict:  {'classification_loss': 0.9578100550174713}
2025-01-16 20:39:47,134 [INFO] Step[2300/2713]: training loss : 0.9433063352108002 TRAIN  loss dict:  {'classification_loss': 0.9433063352108002}
2025-01-16 20:40:03,463 [INFO] Step[2350/2713]: training loss : 0.9390637791156768 TRAIN  loss dict:  {'classification_loss': 0.9390637791156768}
2025-01-16 20:40:19,680 [INFO] Step[2400/2713]: training loss : 0.957806488275528 TRAIN  loss dict:  {'classification_loss': 0.957806488275528}
2025-01-16 20:40:36,066 [INFO] Step[2450/2713]: training loss : 0.9637268126010895 TRAIN  loss dict:  {'classification_loss': 0.9637268126010895}
2025-01-16 20:40:52,308 [INFO] Step[2500/2713]: training loss : 0.9380457103252411 TRAIN  loss dict:  {'classification_loss': 0.9380457103252411}
2025-01-16 20:41:08,635 [INFO] Step[2550/2713]: training loss : 0.9429473340511322 TRAIN  loss dict:  {'classification_loss': 0.9429473340511322}
2025-01-16 20:41:24,883 [INFO] Step[2600/2713]: training loss : 0.9372627830505371 TRAIN  loss dict:  {'classification_loss': 0.9372627830505371}
2025-01-16 20:41:41,111 [INFO] Step[2650/2713]: training loss : 0.9811429071426392 TRAIN  loss dict:  {'classification_loss': 0.9811429071426392}
2025-01-16 20:41:57,357 [INFO] Step[2700/2713]: training loss : 0.9537958419322967 TRAIN  loss dict:  {'classification_loss': 0.9537958419322967}
2025-01-16 20:43:16,083 [INFO] Label accuracies statistics:
2025-01-16 20:43:16,083 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 1.0, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.5, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.5, 217: 0.75, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.25, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.5, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.0, 259: 0.25, 260: 0.75, 261: 0.25, 262: 1.0, 263: 0.75, 264: 0.75, 265: 0.75, 266: 0.5, 267: 0.5, 268: 0.75, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 1.0, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.25, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.5, 319: 1.0, 320: 0.5, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.5, 326: 0.75, 327: 0.5, 328: 0.5, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 20:43:16,085 [INFO] [39] TRAIN  loss: 0.9522611608902467 acc: 0.9948396608920015
2025-01-16 20:43:16,085 [INFO] [39] TRAIN  loss dict: {'classification_loss': 0.9522611608902467}
2025-01-16 20:43:16,086 [INFO] [39] VALIDATION loss: 1.8864789364257253 VALIDATION acc: 0.7862068965517242
2025-01-16 20:43:16,086 [INFO] [39] VALIDATION loss dict: {'classification_loss': 1.8864789364257253}
2025-01-16 20:43:16,086 [INFO] 
2025-01-16 20:43:36,984 [INFO] Step[50/2713]: training loss : 0.9543843400478363 TRAIN  loss dict:  {'classification_loss': 0.9543843400478363}
2025-01-16 20:43:53,203 [INFO] Step[100/2713]: training loss : 0.934411780834198 TRAIN  loss dict:  {'classification_loss': 0.934411780834198}
2025-01-16 20:44:09,445 [INFO] Step[150/2713]: training loss : 0.9404971039295197 TRAIN  loss dict:  {'classification_loss': 0.9404971039295197}
2025-01-16 20:44:25,646 [INFO] Step[200/2713]: training loss : 0.9750211536884308 TRAIN  loss dict:  {'classification_loss': 0.9750211536884308}
2025-01-16 20:44:41,893 [INFO] Step[250/2713]: training loss : 0.977411904335022 TRAIN  loss dict:  {'classification_loss': 0.977411904335022}
2025-01-16 20:44:58,130 [INFO] Step[300/2713]: training loss : 0.9421010863780975 TRAIN  loss dict:  {'classification_loss': 0.9421010863780975}
2025-01-16 20:45:14,336 [INFO] Step[350/2713]: training loss : 0.9487644302845001 TRAIN  loss dict:  {'classification_loss': 0.9487644302845001}
2025-01-16 20:45:30,564 [INFO] Step[400/2713]: training loss : 0.985983498096466 TRAIN  loss dict:  {'classification_loss': 0.985983498096466}
2025-01-16 20:45:46,765 [INFO] Step[450/2713]: training loss : 0.9537024796009064 TRAIN  loss dict:  {'classification_loss': 0.9537024796009064}
2025-01-16 20:46:02,953 [INFO] Step[500/2713]: training loss : 0.9463292062282562 TRAIN  loss dict:  {'classification_loss': 0.9463292062282562}
2025-01-16 20:46:19,227 [INFO] Step[550/2713]: training loss : 0.9462458753585815 TRAIN  loss dict:  {'classification_loss': 0.9462458753585815}
2025-01-16 20:46:35,500 [INFO] Step[600/2713]: training loss : 1.005003172159195 TRAIN  loss dict:  {'classification_loss': 1.005003172159195}
2025-01-16 20:46:51,700 [INFO] Step[650/2713]: training loss : 0.9397848069667816 TRAIN  loss dict:  {'classification_loss': 0.9397848069667816}
2025-01-16 20:47:07,907 [INFO] Step[700/2713]: training loss : 0.9551762211322784 TRAIN  loss dict:  {'classification_loss': 0.9551762211322784}
2025-01-16 20:47:24,190 [INFO] Step[750/2713]: training loss : 0.9405425858497619 TRAIN  loss dict:  {'classification_loss': 0.9405425858497619}
2025-01-16 20:47:40,390 [INFO] Step[800/2713]: training loss : 0.9470664680004119 TRAIN  loss dict:  {'classification_loss': 0.9470664680004119}
2025-01-16 20:47:56,624 [INFO] Step[850/2713]: training loss : 0.937873648405075 TRAIN  loss dict:  {'classification_loss': 0.937873648405075}
2025-01-16 20:48:12,836 [INFO] Step[900/2713]: training loss : 0.9657728219032288 TRAIN  loss dict:  {'classification_loss': 0.9657728219032288}
2025-01-16 20:48:29,060 [INFO] Step[950/2713]: training loss : 0.9394609797000885 TRAIN  loss dict:  {'classification_loss': 0.9394609797000885}
2025-01-16 20:48:45,260 [INFO] Step[1000/2713]: training loss : 0.9894833040237426 TRAIN  loss dict:  {'classification_loss': 0.9894833040237426}
2025-01-16 20:49:01,428 [INFO] Step[1050/2713]: training loss : 0.9386985874176026 TRAIN  loss dict:  {'classification_loss': 0.9386985874176026}
2025-01-16 20:49:17,554 [INFO] Step[1100/2713]: training loss : 0.94026127576828 TRAIN  loss dict:  {'classification_loss': 0.94026127576828}
2025-01-16 20:49:33,799 [INFO] Step[1150/2713]: training loss : 0.9730405509471893 TRAIN  loss dict:  {'classification_loss': 0.9730405509471893}
2025-01-16 20:49:50,048 [INFO] Step[1200/2713]: training loss : 0.9371707141399384 TRAIN  loss dict:  {'classification_loss': 0.9371707141399384}
2025-01-16 20:50:06,414 [INFO] Step[1250/2713]: training loss : 0.938482631444931 TRAIN  loss dict:  {'classification_loss': 0.938482631444931}
2025-01-16 20:50:22,640 [INFO] Step[1300/2713]: training loss : 0.9417713046073913 TRAIN  loss dict:  {'classification_loss': 0.9417713046073913}
2025-01-16 20:50:38,948 [INFO] Step[1350/2713]: training loss : 0.9773694968223572 TRAIN  loss dict:  {'classification_loss': 0.9773694968223572}
2025-01-16 20:50:55,074 [INFO] Step[1400/2713]: training loss : 0.9531749522686005 TRAIN  loss dict:  {'classification_loss': 0.9531749522686005}
2025-01-16 20:51:11,289 [INFO] Step[1450/2713]: training loss : 0.9385196900367737 TRAIN  loss dict:  {'classification_loss': 0.9385196900367737}
2025-01-16 20:51:27,475 [INFO] Step[1500/2713]: training loss : 0.9400291395187378 TRAIN  loss dict:  {'classification_loss': 0.9400291395187378}
2025-01-16 20:51:43,746 [INFO] Step[1550/2713]: training loss : 0.9547213125228882 TRAIN  loss dict:  {'classification_loss': 0.9547213125228882}
2025-01-16 20:51:59,859 [INFO] Step[1600/2713]: training loss : 0.9373434209823608 TRAIN  loss dict:  {'classification_loss': 0.9373434209823608}
2025-01-16 20:52:16,027 [INFO] Step[1650/2713]: training loss : 0.9436539125442505 TRAIN  loss dict:  {'classification_loss': 0.9436539125442505}
2025-01-16 20:52:32,271 [INFO] Step[1700/2713]: training loss : 0.9693961942195892 TRAIN  loss dict:  {'classification_loss': 0.9693961942195892}
2025-01-16 20:52:48,465 [INFO] Step[1750/2713]: training loss : 0.9489406037330628 TRAIN  loss dict:  {'classification_loss': 0.9489406037330628}
2025-01-16 20:53:04,649 [INFO] Step[1800/2713]: training loss : 0.9561733567714691 TRAIN  loss dict:  {'classification_loss': 0.9561733567714691}
2025-01-16 20:53:20,915 [INFO] Step[1850/2713]: training loss : 0.9747452843189239 TRAIN  loss dict:  {'classification_loss': 0.9747452843189239}
2025-01-16 20:53:37,137 [INFO] Step[1900/2713]: training loss : 0.9694822883605957 TRAIN  loss dict:  {'classification_loss': 0.9694822883605957}
2025-01-16 20:53:53,367 [INFO] Step[1950/2713]: training loss : 0.9385056436061859 TRAIN  loss dict:  {'classification_loss': 0.9385056436061859}
2025-01-16 20:54:09,651 [INFO] Step[2000/2713]: training loss : 0.9921102523803711 TRAIN  loss dict:  {'classification_loss': 0.9921102523803711}
2025-01-16 20:54:25,846 [INFO] Step[2050/2713]: training loss : 0.9601478588581085 TRAIN  loss dict:  {'classification_loss': 0.9601478588581085}
2025-01-16 20:54:41,949 [INFO] Step[2100/2713]: training loss : 0.9492074835300446 TRAIN  loss dict:  {'classification_loss': 0.9492074835300446}
2025-01-16 20:54:58,233 [INFO] Step[2150/2713]: training loss : 0.9388694703578949 TRAIN  loss dict:  {'classification_loss': 0.9388694703578949}
2025-01-16 20:55:14,385 [INFO] Step[2200/2713]: training loss : 1.0032224082946777 TRAIN  loss dict:  {'classification_loss': 1.0032224082946777}
2025-01-16 20:55:30,658 [INFO] Step[2250/2713]: training loss : 0.9453484547138215 TRAIN  loss dict:  {'classification_loss': 0.9453484547138215}
2025-01-16 20:55:46,928 [INFO] Step[2300/2713]: training loss : 0.9558512675762176 TRAIN  loss dict:  {'classification_loss': 0.9558512675762176}
2025-01-16 20:56:03,158 [INFO] Step[2350/2713]: training loss : 0.9468124961853027 TRAIN  loss dict:  {'classification_loss': 0.9468124961853027}
2025-01-16 20:56:19,381 [INFO] Step[2400/2713]: training loss : 0.9429158520698547 TRAIN  loss dict:  {'classification_loss': 0.9429158520698547}
2025-01-16 20:56:35,542 [INFO] Step[2450/2713]: training loss : 0.9429494142532349 TRAIN  loss dict:  {'classification_loss': 0.9429494142532349}
2025-01-16 20:56:51,761 [INFO] Step[2500/2713]: training loss : 0.9378093314170838 TRAIN  loss dict:  {'classification_loss': 0.9378093314170838}
2025-01-16 20:57:07,951 [INFO] Step[2550/2713]: training loss : 0.9533824455738068 TRAIN  loss dict:  {'classification_loss': 0.9533824455738068}
2025-01-16 20:57:24,233 [INFO] Step[2600/2713]: training loss : 0.9401284539699555 TRAIN  loss dict:  {'classification_loss': 0.9401284539699555}
2025-01-16 20:57:40,580 [INFO] Step[2650/2713]: training loss : 0.9958073258399963 TRAIN  loss dict:  {'classification_loss': 0.9958073258399963}
2025-01-16 20:57:56,867 [INFO] Step[2700/2713]: training loss : 0.9757393789291382 TRAIN  loss dict:  {'classification_loss': 0.9757393789291382}
2025-01-16 20:59:15,685 [INFO] Label accuracies statistics:
2025-01-16 20:59:15,686 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.5, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.5, 140: 0.75, 141: 0.5, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.25, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.5, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.5, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 1.0, 211: 0.5, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.25, 260: 1.0, 261: 0.25, 262: 1.0, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.25, 279: 1.0, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.5, 284: 0.5, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.25, 373: 1.0, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 20:59:17,061 [INFO] [40] TRAIN  loss: 0.9550383580464786 acc: 0.9945939304582873
2025-01-16 20:59:17,061 [INFO] [40] TRAIN  loss dict: {'classification_loss': 0.9550383580464786}
2025-01-16 20:59:17,061 [INFO] [40] VALIDATION loss: 1.8238535753094165 VALIDATION acc: 0.7974921630094044
2025-01-16 20:59:17,061 [INFO] [40] VALIDATION loss dict: {'classification_loss': 1.8238535753094165}
2025-01-16 20:59:17,061 [INFO] 
2025-01-16 20:59:37,812 [INFO] Step[50/2713]: training loss : 0.937934684753418 TRAIN  loss dict:  {'classification_loss': 0.937934684753418}
2025-01-16 20:59:54,064 [INFO] Step[100/2713]: training loss : 0.9961820292472839 TRAIN  loss dict:  {'classification_loss': 0.9961820292472839}
2025-01-16 21:00:10,284 [INFO] Step[150/2713]: training loss : 0.9426067328453064 TRAIN  loss dict:  {'classification_loss': 0.9426067328453064}
2025-01-16 21:00:26,598 [INFO] Step[200/2713]: training loss : 0.9432361090183258 TRAIN  loss dict:  {'classification_loss': 0.9432361090183258}
2025-01-16 21:00:42,841 [INFO] Step[250/2713]: training loss : 0.9669674921035767 TRAIN  loss dict:  {'classification_loss': 0.9669674921035767}
2025-01-16 21:00:59,104 [INFO] Step[300/2713]: training loss : 0.9997653138637542 TRAIN  loss dict:  {'classification_loss': 0.9997653138637542}
2025-01-16 21:01:15,335 [INFO] Step[350/2713]: training loss : 0.945759860277176 TRAIN  loss dict:  {'classification_loss': 0.945759860277176}
2025-01-16 21:01:31,544 [INFO] Step[400/2713]: training loss : 0.9407380330562591 TRAIN  loss dict:  {'classification_loss': 0.9407380330562591}
2025-01-16 21:01:47,746 [INFO] Step[450/2713]: training loss : 0.9354634582996368 TRAIN  loss dict:  {'classification_loss': 0.9354634582996368}
2025-01-16 21:02:03,975 [INFO] Step[500/2713]: training loss : 0.9412014317512513 TRAIN  loss dict:  {'classification_loss': 0.9412014317512513}
2025-01-16 21:02:20,232 [INFO] Step[550/2713]: training loss : 0.9881099569797516 TRAIN  loss dict:  {'classification_loss': 0.9881099569797516}
2025-01-16 21:02:36,455 [INFO] Step[600/2713]: training loss : 0.9395333981513977 TRAIN  loss dict:  {'classification_loss': 0.9395333981513977}
2025-01-16 21:02:52,789 [INFO] Step[650/2713]: training loss : 0.9473705005645752 TRAIN  loss dict:  {'classification_loss': 0.9473705005645752}
2025-01-16 21:03:08,998 [INFO] Step[700/2713]: training loss : 0.9544058525562287 TRAIN  loss dict:  {'classification_loss': 0.9544058525562287}
2025-01-16 21:03:25,245 [INFO] Step[750/2713]: training loss : 0.935196887254715 TRAIN  loss dict:  {'classification_loss': 0.935196887254715}
2025-01-16 21:03:41,426 [INFO] Step[800/2713]: training loss : 0.936223635673523 TRAIN  loss dict:  {'classification_loss': 0.936223635673523}
2025-01-16 21:03:57,655 [INFO] Step[850/2713]: training loss : 0.935713585615158 TRAIN  loss dict:  {'classification_loss': 0.935713585615158}
2025-01-16 21:04:13,886 [INFO] Step[900/2713]: training loss : 0.9654765009880066 TRAIN  loss dict:  {'classification_loss': 0.9654765009880066}
2025-01-16 21:04:30,059 [INFO] Step[950/2713]: training loss : 0.954011207818985 TRAIN  loss dict:  {'classification_loss': 0.954011207818985}
2025-01-16 21:04:46,229 [INFO] Step[1000/2713]: training loss : 0.9566815602779388 TRAIN  loss dict:  {'classification_loss': 0.9566815602779388}
2025-01-16 21:05:02,471 [INFO] Step[1050/2713]: training loss : 0.9374530637264251 TRAIN  loss dict:  {'classification_loss': 0.9374530637264251}
2025-01-16 21:05:18,646 [INFO] Step[1100/2713]: training loss : 0.9367900180816651 TRAIN  loss dict:  {'classification_loss': 0.9367900180816651}
2025-01-16 21:05:35,055 [INFO] Step[1150/2713]: training loss : 0.943956367969513 TRAIN  loss dict:  {'classification_loss': 0.943956367969513}
2025-01-16 21:05:51,298 [INFO] Step[1200/2713]: training loss : 0.9372078013420105 TRAIN  loss dict:  {'classification_loss': 0.9372078013420105}
2025-01-16 21:06:07,546 [INFO] Step[1250/2713]: training loss : 0.9530626344680786 TRAIN  loss dict:  {'classification_loss': 0.9530626344680786}
2025-01-16 21:06:23,702 [INFO] Step[1300/2713]: training loss : 0.9420901310443878 TRAIN  loss dict:  {'classification_loss': 0.9420901310443878}
2025-01-16 21:06:39,916 [INFO] Step[1350/2713]: training loss : 0.9415913462638855 TRAIN  loss dict:  {'classification_loss': 0.9415913462638855}
2025-01-16 21:06:56,192 [INFO] Step[1400/2713]: training loss : 0.9630889546871185 TRAIN  loss dict:  {'classification_loss': 0.9630889546871185}
2025-01-16 21:07:12,417 [INFO] Step[1450/2713]: training loss : 0.9431699419021606 TRAIN  loss dict:  {'classification_loss': 0.9431699419021606}
2025-01-16 21:07:28,672 [INFO] Step[1500/2713]: training loss : 0.9453361177444458 TRAIN  loss dict:  {'classification_loss': 0.9453361177444458}
2025-01-16 21:07:44,877 [INFO] Step[1550/2713]: training loss : 0.9415680599212647 TRAIN  loss dict:  {'classification_loss': 0.9415680599212647}
2025-01-16 21:08:01,012 [INFO] Step[1600/2713]: training loss : 0.9529893624782563 TRAIN  loss dict:  {'classification_loss': 0.9529893624782563}
2025-01-16 21:08:17,214 [INFO] Step[1650/2713]: training loss : 0.9371758484840393 TRAIN  loss dict:  {'classification_loss': 0.9371758484840393}
2025-01-16 21:08:33,523 [INFO] Step[1700/2713]: training loss : 0.9375028359889984 TRAIN  loss dict:  {'classification_loss': 0.9375028359889984}
2025-01-16 21:08:49,808 [INFO] Step[1750/2713]: training loss : 0.938736596107483 TRAIN  loss dict:  {'classification_loss': 0.938736596107483}
2025-01-16 21:09:05,944 [INFO] Step[1800/2713]: training loss : 0.9385003554821014 TRAIN  loss dict:  {'classification_loss': 0.9385003554821014}
2025-01-16 21:09:22,252 [INFO] Step[1850/2713]: training loss : 0.9386786639690399 TRAIN  loss dict:  {'classification_loss': 0.9386786639690399}
2025-01-16 21:09:38,499 [INFO] Step[1900/2713]: training loss : 0.9368662846088409 TRAIN  loss dict:  {'classification_loss': 0.9368662846088409}
2025-01-16 21:09:54,762 [INFO] Step[1950/2713]: training loss : 0.9559056508541107 TRAIN  loss dict:  {'classification_loss': 0.9559056508541107}
2025-01-16 21:10:10,906 [INFO] Step[2000/2713]: training loss : 0.957683265209198 TRAIN  loss dict:  {'classification_loss': 0.957683265209198}
2025-01-16 21:10:27,143 [INFO] Step[2050/2713]: training loss : 0.9397204554080963 TRAIN  loss dict:  {'classification_loss': 0.9397204554080963}
2025-01-16 21:10:43,398 [INFO] Step[2100/2713]: training loss : 0.9401745092868805 TRAIN  loss dict:  {'classification_loss': 0.9401745092868805}
2025-01-16 21:10:59,615 [INFO] Step[2150/2713]: training loss : 0.9350796926021576 TRAIN  loss dict:  {'classification_loss': 0.9350796926021576}
2025-01-16 21:11:15,761 [INFO] Step[2200/2713]: training loss : 0.9348780035972595 TRAIN  loss dict:  {'classification_loss': 0.9348780035972595}
2025-01-16 21:11:31,975 [INFO] Step[2250/2713]: training loss : 0.9330861091613769 TRAIN  loss dict:  {'classification_loss': 0.9330861091613769}
2025-01-16 21:11:48,207 [INFO] Step[2300/2713]: training loss : 0.9364097666740417 TRAIN  loss dict:  {'classification_loss': 0.9364097666740417}
2025-01-16 21:12:04,451 [INFO] Step[2350/2713]: training loss : 0.9367106425762176 TRAIN  loss dict:  {'classification_loss': 0.9367106425762176}
2025-01-16 21:12:20,670 [INFO] Step[2400/2713]: training loss : 0.959232188463211 TRAIN  loss dict:  {'classification_loss': 0.959232188463211}
2025-01-16 21:12:36,883 [INFO] Step[2450/2713]: training loss : 0.9939754784107209 TRAIN  loss dict:  {'classification_loss': 0.9939754784107209}
2025-01-16 21:12:53,070 [INFO] Step[2500/2713]: training loss : 0.9635617315769196 TRAIN  loss dict:  {'classification_loss': 0.9635617315769196}
2025-01-16 21:13:09,342 [INFO] Step[2550/2713]: training loss : 0.9358639562129974 TRAIN  loss dict:  {'classification_loss': 0.9358639562129974}
2025-01-16 21:13:25,499 [INFO] Step[2600/2713]: training loss : 0.935210976600647 TRAIN  loss dict:  {'classification_loss': 0.935210976600647}
2025-01-16 21:13:41,778 [INFO] Step[2650/2713]: training loss : 0.9466687178611756 TRAIN  loss dict:  {'classification_loss': 0.9466687178611756}
2025-01-16 21:13:58,028 [INFO] Step[2700/2713]: training loss : 0.9417086958885192 TRAIN  loss dict:  {'classification_loss': 0.9417086958885192}
2025-01-16 21:15:16,769 [INFO] Label accuracies statistics:
2025-01-16 21:15:16,769 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.25, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.25, 259: 0.25, 260: 0.5, 261: 0.5, 262: 0.75, 263: 1.0, 264: 0.5, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.75, 269: 0.75, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.5, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.5, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.5, 347: 0.75, 348: 0.75, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 0.75, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 21:15:16,771 [INFO] [41] TRAIN  loss: 0.9475962524918494 acc: 0.99533112175943
2025-01-16 21:15:16,771 [INFO] [41] TRAIN  loss dict: {'classification_loss': 0.9475962524918494}
2025-01-16 21:15:16,771 [INFO] [41] VALIDATION loss: 1.8381393519335223 VALIDATION acc: 0.7974921630094044
2025-01-16 21:15:16,771 [INFO] [41] VALIDATION loss dict: {'classification_loss': 1.8381393519335223}
2025-01-16 21:15:16,771 [INFO] 
2025-01-16 21:15:37,987 [INFO] Step[50/2713]: training loss : 0.9327093040943146 TRAIN  loss dict:  {'classification_loss': 0.9327093040943146}
2025-01-16 21:15:54,236 [INFO] Step[100/2713]: training loss : 0.9340655219554901 TRAIN  loss dict:  {'classification_loss': 0.9340655219554901}
2025-01-16 21:16:10,424 [INFO] Step[150/2713]: training loss : 0.9392692732810974 TRAIN  loss dict:  {'classification_loss': 0.9392692732810974}
2025-01-16 21:16:26,673 [INFO] Step[200/2713]: training loss : 0.9556388902664185 TRAIN  loss dict:  {'classification_loss': 0.9556388902664185}
2025-01-16 21:16:42,879 [INFO] Step[250/2713]: training loss : 0.9330629241466523 TRAIN  loss dict:  {'classification_loss': 0.9330629241466523}
2025-01-16 21:16:59,070 [INFO] Step[300/2713]: training loss : 0.9333926594257355 TRAIN  loss dict:  {'classification_loss': 0.9333926594257355}
2025-01-16 21:17:15,367 [INFO] Step[350/2713]: training loss : 0.932826063632965 TRAIN  loss dict:  {'classification_loss': 0.932826063632965}
2025-01-16 21:17:31,427 [INFO] Step[400/2713]: training loss : 0.9732543063163758 TRAIN  loss dict:  {'classification_loss': 0.9732543063163758}
2025-01-16 21:17:47,819 [INFO] Step[450/2713]: training loss : 0.9335388207435608 TRAIN  loss dict:  {'classification_loss': 0.9335388207435608}
2025-01-16 21:18:04,002 [INFO] Step[500/2713]: training loss : 0.93599254488945 TRAIN  loss dict:  {'classification_loss': 0.93599254488945}
2025-01-16 21:18:20,195 [INFO] Step[550/2713]: training loss : 0.9489619290828705 TRAIN  loss dict:  {'classification_loss': 0.9489619290828705}
2025-01-16 21:18:36,367 [INFO] Step[600/2713]: training loss : 0.935218152999878 TRAIN  loss dict:  {'classification_loss': 0.935218152999878}
2025-01-16 21:18:52,494 [INFO] Step[650/2713]: training loss : 0.9359226417541504 TRAIN  loss dict:  {'classification_loss': 0.9359226417541504}
2025-01-16 21:19:08,663 [INFO] Step[700/2713]: training loss : 0.9338958072662353 TRAIN  loss dict:  {'classification_loss': 0.9338958072662353}
2025-01-16 21:19:24,895 [INFO] Step[750/2713]: training loss : 0.9363188076019288 TRAIN  loss dict:  {'classification_loss': 0.9363188076019288}
2025-01-16 21:19:41,069 [INFO] Step[800/2713]: training loss : 0.9362842309474945 TRAIN  loss dict:  {'classification_loss': 0.9362842309474945}
2025-01-16 21:19:57,306 [INFO] Step[850/2713]: training loss : 0.9555099475383758 TRAIN  loss dict:  {'classification_loss': 0.9555099475383758}
2025-01-16 21:20:13,446 [INFO] Step[900/2713]: training loss : 0.9602546334266663 TRAIN  loss dict:  {'classification_loss': 0.9602546334266663}
2025-01-16 21:20:29,697 [INFO] Step[950/2713]: training loss : 0.9455536389350891 TRAIN  loss dict:  {'classification_loss': 0.9455536389350891}
2025-01-16 21:20:45,984 [INFO] Step[1000/2713]: training loss : 0.9344797468185425 TRAIN  loss dict:  {'classification_loss': 0.9344797468185425}
2025-01-16 21:21:02,311 [INFO] Step[1050/2713]: training loss : 0.9601390433311462 TRAIN  loss dict:  {'classification_loss': 0.9601390433311462}
2025-01-16 21:21:18,505 [INFO] Step[1100/2713]: training loss : 0.9403005969524384 TRAIN  loss dict:  {'classification_loss': 0.9403005969524384}
2025-01-16 21:21:34,730 [INFO] Step[1150/2713]: training loss : 0.9402668762207032 TRAIN  loss dict:  {'classification_loss': 0.9402668762207032}
2025-01-16 21:21:50,984 [INFO] Step[1200/2713]: training loss : 0.9348858594894409 TRAIN  loss dict:  {'classification_loss': 0.9348858594894409}
2025-01-16 21:22:07,210 [INFO] Step[1250/2713]: training loss : 0.9353674221038818 TRAIN  loss dict:  {'classification_loss': 0.9353674221038818}
2025-01-16 21:22:23,423 [INFO] Step[1300/2713]: training loss : 0.9413004553318024 TRAIN  loss dict:  {'classification_loss': 0.9413004553318024}
2025-01-16 21:22:39,553 [INFO] Step[1350/2713]: training loss : 0.9606340610980988 TRAIN  loss dict:  {'classification_loss': 0.9606340610980988}
2025-01-16 21:22:55,718 [INFO] Step[1400/2713]: training loss : 0.935088472366333 TRAIN  loss dict:  {'classification_loss': 0.935088472366333}
2025-01-16 21:23:11,914 [INFO] Step[1450/2713]: training loss : 0.9711716520786285 TRAIN  loss dict:  {'classification_loss': 0.9711716520786285}
2025-01-16 21:23:28,086 [INFO] Step[1500/2713]: training loss : 0.9487406027317047 TRAIN  loss dict:  {'classification_loss': 0.9487406027317047}
2025-01-16 21:23:44,304 [INFO] Step[1550/2713]: training loss : 0.9323527526855468 TRAIN  loss dict:  {'classification_loss': 0.9323527526855468}
2025-01-16 21:24:00,447 [INFO] Step[1600/2713]: training loss : 0.9355126094818115 TRAIN  loss dict:  {'classification_loss': 0.9355126094818115}
2025-01-16 21:24:16,579 [INFO] Step[1650/2713]: training loss : 0.9357404673099518 TRAIN  loss dict:  {'classification_loss': 0.9357404673099518}
2025-01-16 21:24:32,671 [INFO] Step[1700/2713]: training loss : 0.9451827144622803 TRAIN  loss dict:  {'classification_loss': 0.9451827144622803}
2025-01-16 21:24:48,871 [INFO] Step[1750/2713]: training loss : 0.9521512770652771 TRAIN  loss dict:  {'classification_loss': 0.9521512770652771}
2025-01-16 21:25:05,020 [INFO] Step[1800/2713]: training loss : 0.9371990585327148 TRAIN  loss dict:  {'classification_loss': 0.9371990585327148}
2025-01-16 21:25:21,145 [INFO] Step[1850/2713]: training loss : 0.9379989922046661 TRAIN  loss dict:  {'classification_loss': 0.9379989922046661}
2025-01-16 21:25:37,265 [INFO] Step[1900/2713]: training loss : 0.9363427543640137 TRAIN  loss dict:  {'classification_loss': 0.9363427543640137}
2025-01-16 21:25:53,428 [INFO] Step[1950/2713]: training loss : 0.9852054977416992 TRAIN  loss dict:  {'classification_loss': 0.9852054977416992}
2025-01-16 21:26:09,563 [INFO] Step[2000/2713]: training loss : 0.9427412378787995 TRAIN  loss dict:  {'classification_loss': 0.9427412378787995}
2025-01-16 21:26:25,679 [INFO] Step[2050/2713]: training loss : 0.9449083375930786 TRAIN  loss dict:  {'classification_loss': 0.9449083375930786}
2025-01-16 21:26:41,833 [INFO] Step[2100/2713]: training loss : 0.938035055398941 TRAIN  loss dict:  {'classification_loss': 0.938035055398941}
2025-01-16 21:26:57,938 [INFO] Step[2150/2713]: training loss : 0.9424566948413848 TRAIN  loss dict:  {'classification_loss': 0.9424566948413848}
2025-01-16 21:27:14,116 [INFO] Step[2200/2713]: training loss : 0.9420568561553955 TRAIN  loss dict:  {'classification_loss': 0.9420568561553955}
2025-01-16 21:27:30,290 [INFO] Step[2250/2713]: training loss : 0.9333377373218537 TRAIN  loss dict:  {'classification_loss': 0.9333377373218537}
2025-01-16 21:27:46,461 [INFO] Step[2300/2713]: training loss : 0.9335956585407257 TRAIN  loss dict:  {'classification_loss': 0.9335956585407257}
2025-01-16 21:28:02,625 [INFO] Step[2350/2713]: training loss : 0.9365843546390533 TRAIN  loss dict:  {'classification_loss': 0.9365843546390533}
2025-01-16 21:28:18,709 [INFO] Step[2400/2713]: training loss : 0.9408513581752778 TRAIN  loss dict:  {'classification_loss': 0.9408513581752778}
2025-01-16 21:28:34,906 [INFO] Step[2450/2713]: training loss : 0.9343963241577149 TRAIN  loss dict:  {'classification_loss': 0.9343963241577149}
2025-01-16 21:28:51,054 [INFO] Step[2500/2713]: training loss : 0.9565778017044068 TRAIN  loss dict:  {'classification_loss': 0.9565778017044068}
2025-01-16 21:29:07,230 [INFO] Step[2550/2713]: training loss : 0.9327877056598664 TRAIN  loss dict:  {'classification_loss': 0.9327877056598664}
2025-01-16 21:29:23,313 [INFO] Step[2600/2713]: training loss : 0.9342587518692017 TRAIN  loss dict:  {'classification_loss': 0.9342587518692017}
2025-01-16 21:29:39,517 [INFO] Step[2650/2713]: training loss : 0.9321314728260041 TRAIN  loss dict:  {'classification_loss': 0.9321314728260041}
2025-01-16 21:29:55,718 [INFO] Step[2700/2713]: training loss : 0.9727062356472015 TRAIN  loss dict:  {'classification_loss': 0.9727062356472015}
2025-01-16 21:31:14,883 [INFO] Label accuracies statistics:
2025-01-16 21:31:14,883 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 0.8, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.25, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.5, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.0, 212: 0.75, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 1.0, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.25, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 1.0, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 1.0, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 1.0, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.25, 353: 0.25, 354: 0.25, 355: 0.75, 356: 0.5, 357: 0.75, 358: 0.5, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 21:31:14,885 [INFO] [42] TRAIN  loss: 0.94272914105924 acc: 0.9966826391448581
2025-01-16 21:31:14,885 [INFO] [42] TRAIN  loss dict: {'classification_loss': 0.94272914105924}
2025-01-16 21:31:14,885 [INFO] [42] VALIDATION loss: 1.8311630658861389 VALIDATION acc: 0.7974921630094044
2025-01-16 21:31:14,885 [INFO] [42] VALIDATION loss dict: {'classification_loss': 1.8311630658861389}
2025-01-16 21:31:14,886 [INFO] 
2025-01-16 21:31:35,665 [INFO] Step[50/2713]: training loss : 0.9386760032176972 TRAIN  loss dict:  {'classification_loss': 0.9386760032176972}
2025-01-16 21:31:51,817 [INFO] Step[100/2713]: training loss : 0.9661830389499664 TRAIN  loss dict:  {'classification_loss': 0.9661830389499664}
2025-01-16 21:32:08,040 [INFO] Step[150/2713]: training loss : 0.9411230027675629 TRAIN  loss dict:  {'classification_loss': 0.9411230027675629}
2025-01-16 21:32:24,269 [INFO] Step[200/2713]: training loss : 0.9353877663612366 TRAIN  loss dict:  {'classification_loss': 0.9353877663612366}
2025-01-16 21:32:40,447 [INFO] Step[250/2713]: training loss : 1.0160069537162781 TRAIN  loss dict:  {'classification_loss': 1.0160069537162781}
2025-01-16 21:32:56,564 [INFO] Step[300/2713]: training loss : 0.9565492868423462 TRAIN  loss dict:  {'classification_loss': 0.9565492868423462}
2025-01-16 21:33:12,753 [INFO] Step[350/2713]: training loss : 0.9316060173511506 TRAIN  loss dict:  {'classification_loss': 0.9316060173511506}
2025-01-16 21:33:28,945 [INFO] Step[400/2713]: training loss : 0.9375814282894135 TRAIN  loss dict:  {'classification_loss': 0.9375814282894135}
2025-01-16 21:33:45,164 [INFO] Step[450/2713]: training loss : 0.9610347354412079 TRAIN  loss dict:  {'classification_loss': 0.9610347354412079}
2025-01-16 21:34:01,347 [INFO] Step[500/2713]: training loss : 0.9364579343795776 TRAIN  loss dict:  {'classification_loss': 0.9364579343795776}
2025-01-16 21:34:17,553 [INFO] Step[550/2713]: training loss : 0.9328746902942657 TRAIN  loss dict:  {'classification_loss': 0.9328746902942657}
2025-01-16 21:34:33,798 [INFO] Step[600/2713]: training loss : 0.9552347779273986 TRAIN  loss dict:  {'classification_loss': 0.9552347779273986}
2025-01-16 21:34:50,021 [INFO] Step[650/2713]: training loss : 0.9381183564662934 TRAIN  loss dict:  {'classification_loss': 0.9381183564662934}
2025-01-16 21:35:06,218 [INFO] Step[700/2713]: training loss : 0.9372536480426789 TRAIN  loss dict:  {'classification_loss': 0.9372536480426789}
2025-01-16 21:35:22,415 [INFO] Step[750/2713]: training loss : 0.9395203197002411 TRAIN  loss dict:  {'classification_loss': 0.9395203197002411}
2025-01-16 21:35:38,582 [INFO] Step[800/2713]: training loss : 0.9337371003627777 TRAIN  loss dict:  {'classification_loss': 0.9337371003627777}
2025-01-16 21:35:54,764 [INFO] Step[850/2713]: training loss : 0.9441250884532928 TRAIN  loss dict:  {'classification_loss': 0.9441250884532928}
2025-01-16 21:36:10,888 [INFO] Step[900/2713]: training loss : 0.9437578654289246 TRAIN  loss dict:  {'classification_loss': 0.9437578654289246}
2025-01-16 21:36:27,079 [INFO] Step[950/2713]: training loss : 0.9528611373901367 TRAIN  loss dict:  {'classification_loss': 0.9528611373901367}
2025-01-16 21:36:43,335 [INFO] Step[1000/2713]: training loss : 0.9434800934791565 TRAIN  loss dict:  {'classification_loss': 0.9434800934791565}
2025-01-16 21:36:59,485 [INFO] Step[1050/2713]: training loss : 0.9544813346862793 TRAIN  loss dict:  {'classification_loss': 0.9544813346862793}
2025-01-16 21:37:15,698 [INFO] Step[1100/2713]: training loss : 0.9538432109355927 TRAIN  loss dict:  {'classification_loss': 0.9538432109355927}
2025-01-16 21:37:31,876 [INFO] Step[1150/2713]: training loss : 0.9668507945537567 TRAIN  loss dict:  {'classification_loss': 0.9668507945537567}
2025-01-16 21:37:48,021 [INFO] Step[1200/2713]: training loss : 0.9603825867176056 TRAIN  loss dict:  {'classification_loss': 0.9603825867176056}
2025-01-16 21:38:04,249 [INFO] Step[1250/2713]: training loss : 0.9362398862838746 TRAIN  loss dict:  {'classification_loss': 0.9362398862838746}
2025-01-16 21:38:20,478 [INFO] Step[1300/2713]: training loss : 0.9430492949485779 TRAIN  loss dict:  {'classification_loss': 0.9430492949485779}
2025-01-16 21:38:36,704 [INFO] Step[1350/2713]: training loss : 0.9339542043209076 TRAIN  loss dict:  {'classification_loss': 0.9339542043209076}
2025-01-16 21:38:52,934 [INFO] Step[1400/2713]: training loss : 0.9509867346286773 TRAIN  loss dict:  {'classification_loss': 0.9509867346286773}
2025-01-16 21:39:09,090 [INFO] Step[1450/2713]: training loss : 0.9358638656139374 TRAIN  loss dict:  {'classification_loss': 0.9358638656139374}
2025-01-16 21:39:25,241 [INFO] Step[1500/2713]: training loss : 0.958769508600235 TRAIN  loss dict:  {'classification_loss': 0.958769508600235}
2025-01-16 21:39:41,427 [INFO] Step[1550/2713]: training loss : 0.9346048963069916 TRAIN  loss dict:  {'classification_loss': 0.9346048963069916}
2025-01-16 21:39:57,641 [INFO] Step[1600/2713]: training loss : 0.944428642988205 TRAIN  loss dict:  {'classification_loss': 0.944428642988205}
2025-01-16 21:40:13,863 [INFO] Step[1650/2713]: training loss : 0.9521767818927764 TRAIN  loss dict:  {'classification_loss': 0.9521767818927764}
2025-01-16 21:40:30,021 [INFO] Step[1700/2713]: training loss : 0.9345458483695984 TRAIN  loss dict:  {'classification_loss': 0.9345458483695984}
2025-01-16 21:40:46,333 [INFO] Step[1750/2713]: training loss : 0.9394998574256896 TRAIN  loss dict:  {'classification_loss': 0.9394998574256896}
2025-01-16 21:41:02,528 [INFO] Step[1800/2713]: training loss : 0.9924154484272003 TRAIN  loss dict:  {'classification_loss': 0.9924154484272003}
2025-01-16 21:41:18,720 [INFO] Step[1850/2713]: training loss : 0.937121775150299 TRAIN  loss dict:  {'classification_loss': 0.937121775150299}
2025-01-16 21:41:34,895 [INFO] Step[1900/2713]: training loss : 0.9451718556880951 TRAIN  loss dict:  {'classification_loss': 0.9451718556880951}
2025-01-16 21:41:51,079 [INFO] Step[1950/2713]: training loss : 0.9379802215099334 TRAIN  loss dict:  {'classification_loss': 0.9379802215099334}
2025-01-16 21:42:07,226 [INFO] Step[2000/2713]: training loss : 0.9355661344528198 TRAIN  loss dict:  {'classification_loss': 0.9355661344528198}
2025-01-16 21:42:23,364 [INFO] Step[2050/2713]: training loss : 0.9358756840229034 TRAIN  loss dict:  {'classification_loss': 0.9358756840229034}
2025-01-16 21:42:39,530 [INFO] Step[2100/2713]: training loss : 0.9479948961734772 TRAIN  loss dict:  {'classification_loss': 0.9479948961734772}
2025-01-16 21:42:55,725 [INFO] Step[2150/2713]: training loss : 0.9346400582790375 TRAIN  loss dict:  {'classification_loss': 0.9346400582790375}
2025-01-16 21:43:11,867 [INFO] Step[2200/2713]: training loss : 0.9327045774459839 TRAIN  loss dict:  {'classification_loss': 0.9327045774459839}
2025-01-16 21:43:28,067 [INFO] Step[2250/2713]: training loss : 0.9611571598052978 TRAIN  loss dict:  {'classification_loss': 0.9611571598052978}
2025-01-16 21:43:44,242 [INFO] Step[2300/2713]: training loss : 0.9337678253650665 TRAIN  loss dict:  {'classification_loss': 0.9337678253650665}
2025-01-16 21:44:00,424 [INFO] Step[2350/2713]: training loss : 0.9329734802246094 TRAIN  loss dict:  {'classification_loss': 0.9329734802246094}
2025-01-16 21:44:16,601 [INFO] Step[2400/2713]: training loss : 0.9337991786003113 TRAIN  loss dict:  {'classification_loss': 0.9337991786003113}
2025-01-16 21:44:32,751 [INFO] Step[2450/2713]: training loss : 0.9332276594638824 TRAIN  loss dict:  {'classification_loss': 0.9332276594638824}
2025-01-16 21:44:48,921 [INFO] Step[2500/2713]: training loss : 0.9445385265350342 TRAIN  loss dict:  {'classification_loss': 0.9445385265350342}
2025-01-16 21:45:05,069 [INFO] Step[2550/2713]: training loss : 0.9517078602313995 TRAIN  loss dict:  {'classification_loss': 0.9517078602313995}
2025-01-16 21:45:21,296 [INFO] Step[2600/2713]: training loss : 0.953975557088852 TRAIN  loss dict:  {'classification_loss': 0.953975557088852}
2025-01-16 21:45:37,516 [INFO] Step[2650/2713]: training loss : 0.9338693869113922 TRAIN  loss dict:  {'classification_loss': 0.9338693869113922}
2025-01-16 21:45:53,754 [INFO] Step[2700/2713]: training loss : 0.9410082459449768 TRAIN  loss dict:  {'classification_loss': 0.9410082459449768}
2025-01-16 21:47:12,398 [INFO] Label accuracies statistics:
2025-01-16 21:47:12,398 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.25, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 1.0, 52: 0.75, 53: 0.5, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 0.75, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.5, 214: 0.5, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.0, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 1.0, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.75, 260: 0.75, 261: 0.5, 262: 0.75, 263: 1.0, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 0.5, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 0.25, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 1.0, 320: 0.5, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 1.0, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.25, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 21:47:12,399 [INFO] [43] TRAIN  loss: 0.9455145181345843 acc: 0.996559773928001
2025-01-16 21:47:12,399 [INFO] [43] TRAIN  loss dict: {'classification_loss': 0.9455145181345843}
2025-01-16 21:47:12,400 [INFO] [43] VALIDATION loss: 1.9031297535376441 VALIDATION acc: 0.7843260188087774
2025-01-16 21:47:12,400 [INFO] [43] VALIDATION loss dict: {'classification_loss': 1.9031297535376441}
2025-01-16 21:47:12,400 [INFO] 
2025-01-16 21:47:33,478 [INFO] Step[50/2713]: training loss : 0.9335153007507324 TRAIN  loss dict:  {'classification_loss': 0.9335153007507324}
2025-01-16 21:47:49,594 [INFO] Step[100/2713]: training loss : 0.9443310070037841 TRAIN  loss dict:  {'classification_loss': 0.9443310070037841}
2025-01-16 21:48:05,728 [INFO] Step[150/2713]: training loss : 0.9337866246700287 TRAIN  loss dict:  {'classification_loss': 0.9337866246700287}
2025-01-16 21:48:21,892 [INFO] Step[200/2713]: training loss : 0.9490666246414184 TRAIN  loss dict:  {'classification_loss': 0.9490666246414184}
2025-01-16 21:48:38,155 [INFO] Step[250/2713]: training loss : 0.942557727098465 TRAIN  loss dict:  {'classification_loss': 0.942557727098465}
2025-01-16 21:48:54,299 [INFO] Step[300/2713]: training loss : 0.9336796128749847 TRAIN  loss dict:  {'classification_loss': 0.9336796128749847}
2025-01-16 21:49:10,507 [INFO] Step[350/2713]: training loss : 0.9331280970573426 TRAIN  loss dict:  {'classification_loss': 0.9331280970573426}
2025-01-16 21:49:26,701 [INFO] Step[400/2713]: training loss : 0.933087854385376 TRAIN  loss dict:  {'classification_loss': 0.933087854385376}
2025-01-16 21:49:42,920 [INFO] Step[450/2713]: training loss : 0.9362195658683777 TRAIN  loss dict:  {'classification_loss': 0.9362195658683777}
2025-01-16 21:49:59,112 [INFO] Step[500/2713]: training loss : 0.9326896154880524 TRAIN  loss dict:  {'classification_loss': 0.9326896154880524}
2025-01-16 21:50:15,268 [INFO] Step[550/2713]: training loss : 0.9372856664657593 TRAIN  loss dict:  {'classification_loss': 0.9372856664657593}
2025-01-16 21:50:31,398 [INFO] Step[600/2713]: training loss : 0.9347996187210083 TRAIN  loss dict:  {'classification_loss': 0.9347996187210083}
2025-01-16 21:50:47,576 [INFO] Step[650/2713]: training loss : 0.933065276145935 TRAIN  loss dict:  {'classification_loss': 0.933065276145935}
2025-01-16 21:51:03,659 [INFO] Step[700/2713]: training loss : 0.9332947850227356 TRAIN  loss dict:  {'classification_loss': 0.9332947850227356}
2025-01-16 21:51:19,820 [INFO] Step[750/2713]: training loss : 0.9342493319511413 TRAIN  loss dict:  {'classification_loss': 0.9342493319511413}
2025-01-16 21:51:35,929 [INFO] Step[800/2713]: training loss : 0.9328594195842743 TRAIN  loss dict:  {'classification_loss': 0.9328594195842743}
2025-01-16 21:51:52,072 [INFO] Step[850/2713]: training loss : 0.932235815525055 TRAIN  loss dict:  {'classification_loss': 0.932235815525055}
2025-01-16 21:52:08,107 [INFO] Step[900/2713]: training loss : 0.9359324657917023 TRAIN  loss dict:  {'classification_loss': 0.9359324657917023}
2025-01-16 21:52:24,238 [INFO] Step[950/2713]: training loss : 0.9376685285568237 TRAIN  loss dict:  {'classification_loss': 0.9376685285568237}
2025-01-16 21:52:40,326 [INFO] Step[1000/2713]: training loss : 0.9327268803119659 TRAIN  loss dict:  {'classification_loss': 0.9327268803119659}
2025-01-16 21:52:56,476 [INFO] Step[1050/2713]: training loss : 0.9332143878936767 TRAIN  loss dict:  {'classification_loss': 0.9332143878936767}
2025-01-16 21:53:12,592 [INFO] Step[1100/2713]: training loss : 0.9316393446922302 TRAIN  loss dict:  {'classification_loss': 0.9316393446922302}
2025-01-16 21:53:28,744 [INFO] Step[1150/2713]: training loss : 0.9380556368827819 TRAIN  loss dict:  {'classification_loss': 0.9380556368827819}
2025-01-16 21:53:44,821 [INFO] Step[1200/2713]: training loss : 0.9337617361545563 TRAIN  loss dict:  {'classification_loss': 0.9337617361545563}
2025-01-16 21:54:00,909 [INFO] Step[1250/2713]: training loss : 0.9668734407424927 TRAIN  loss dict:  {'classification_loss': 0.9668734407424927}
2025-01-16 21:54:17,088 [INFO] Step[1300/2713]: training loss : 0.9417475759983063 TRAIN  loss dict:  {'classification_loss': 0.9417475759983063}
2025-01-16 21:54:33,164 [INFO] Step[1350/2713]: training loss : 0.9493809056282043 TRAIN  loss dict:  {'classification_loss': 0.9493809056282043}
2025-01-16 21:54:49,316 [INFO] Step[1400/2713]: training loss : 0.9572424662113189 TRAIN  loss dict:  {'classification_loss': 0.9572424662113189}
2025-01-16 21:55:05,465 [INFO] Step[1450/2713]: training loss : 0.9411930549144745 TRAIN  loss dict:  {'classification_loss': 0.9411930549144745}
2025-01-16 21:55:21,575 [INFO] Step[1500/2713]: training loss : 0.9323646426200867 TRAIN  loss dict:  {'classification_loss': 0.9323646426200867}
2025-01-16 21:55:37,715 [INFO] Step[1550/2713]: training loss : 0.9344786381721497 TRAIN  loss dict:  {'classification_loss': 0.9344786381721497}
2025-01-16 21:55:53,825 [INFO] Step[1600/2713]: training loss : 0.9332872295379638 TRAIN  loss dict:  {'classification_loss': 0.9332872295379638}
2025-01-16 21:56:09,939 [INFO] Step[1650/2713]: training loss : 0.9464465415477753 TRAIN  loss dict:  {'classification_loss': 0.9464465415477753}
2025-01-16 21:56:26,004 [INFO] Step[1700/2713]: training loss : 0.9615505409240722 TRAIN  loss dict:  {'classification_loss': 0.9615505409240722}
2025-01-16 21:56:42,114 [INFO] Step[1750/2713]: training loss : 0.9334872794151307 TRAIN  loss dict:  {'classification_loss': 0.9334872794151307}
2025-01-16 21:56:58,203 [INFO] Step[1800/2713]: training loss : 0.9355546820163727 TRAIN  loss dict:  {'classification_loss': 0.9355546820163727}
2025-01-16 21:57:14,365 [INFO] Step[1850/2713]: training loss : 0.9466678512096405 TRAIN  loss dict:  {'classification_loss': 0.9466678512096405}
2025-01-16 21:57:30,445 [INFO] Step[1900/2713]: training loss : 0.9317964768409729 TRAIN  loss dict:  {'classification_loss': 0.9317964768409729}
2025-01-16 21:57:46,521 [INFO] Step[1950/2713]: training loss : 0.9350010359287262 TRAIN  loss dict:  {'classification_loss': 0.9350010359287262}
2025-01-16 21:58:02,604 [INFO] Step[2000/2713]: training loss : 0.9742722165584564 TRAIN  loss dict:  {'classification_loss': 0.9742722165584564}
2025-01-16 21:58:18,677 [INFO] Step[2050/2713]: training loss : 0.964891927242279 TRAIN  loss dict:  {'classification_loss': 0.964891927242279}
2025-01-16 21:58:34,810 [INFO] Step[2100/2713]: training loss : 0.9671598553657532 TRAIN  loss dict:  {'classification_loss': 0.9671598553657532}
2025-01-16 21:58:50,879 [INFO] Step[2150/2713]: training loss : 0.9411243093013764 TRAIN  loss dict:  {'classification_loss': 0.9411243093013764}
2025-01-16 21:59:06,993 [INFO] Step[2200/2713]: training loss : 0.937367798089981 TRAIN  loss dict:  {'classification_loss': 0.937367798089981}
2025-01-16 21:59:23,094 [INFO] Step[2250/2713]: training loss : 0.9331200134754181 TRAIN  loss dict:  {'classification_loss': 0.9331200134754181}
2025-01-16 21:59:39,299 [INFO] Step[2300/2713]: training loss : 0.9472637510299683 TRAIN  loss dict:  {'classification_loss': 0.9472637510299683}
2025-01-16 21:59:55,422 [INFO] Step[2350/2713]: training loss : 0.9852211344242096 TRAIN  loss dict:  {'classification_loss': 0.9852211344242096}
2025-01-16 22:00:11,472 [INFO] Step[2400/2713]: training loss : 0.9949081683158875 TRAIN  loss dict:  {'classification_loss': 0.9949081683158875}
2025-01-16 22:00:27,550 [INFO] Step[2450/2713]: training loss : 0.9345003819465637 TRAIN  loss dict:  {'classification_loss': 0.9345003819465637}
2025-01-16 22:00:43,680 [INFO] Step[2500/2713]: training loss : 0.9701744842529297 TRAIN  loss dict:  {'classification_loss': 0.9701744842529297}
2025-01-16 22:00:59,904 [INFO] Step[2550/2713]: training loss : 0.9327591085433959 TRAIN  loss dict:  {'classification_loss': 0.9327591085433959}
2025-01-16 22:01:16,028 [INFO] Step[2600/2713]: training loss : 0.9321487653255462 TRAIN  loss dict:  {'classification_loss': 0.9321487653255462}
2025-01-16 22:01:32,412 [INFO] Step[2650/2713]: training loss : 0.9401863503456116 TRAIN  loss dict:  {'classification_loss': 0.9401863503456116}
2025-01-16 22:01:48,381 [INFO] Step[2700/2713]: training loss : 0.9928002178668975 TRAIN  loss dict:  {'classification_loss': 0.9928002178668975}
2025-01-16 22:03:06,899 [INFO] Label accuracies statistics:
2025-01-16 22:03:06,899 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.25, 18: 0.5, 19: 0.25, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 1.0, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 1.0, 52: 0.75, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.25, 61: 0.5, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.5, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 0.25, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.5, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.5, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.5, 208: 1.0, 209: 1.0, 210: 1.0, 211: 0.5, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.5, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.0, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 0.75, 258: 0.25, 259: 1.0, 260: 1.0, 261: 1.0, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.5, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.5, 339: 1.0, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 1.0, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 0.75, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 22:03:06,901 [INFO] [44] TRAIN  loss: 0.9434321608395757 acc: 0.9964369087111439
2025-01-16 22:03:06,901 [INFO] [44] TRAIN  loss dict: {'classification_loss': 0.9434321608395757}
2025-01-16 22:03:06,901 [INFO] [44] VALIDATION loss: 1.8707005269545363 VALIDATION acc: 0.7968652037617555
2025-01-16 22:03:06,901 [INFO] [44] VALIDATION loss dict: {'classification_loss': 1.8707005269545363}
2025-01-16 22:03:06,901 [INFO] 
2025-01-16 22:03:27,709 [INFO] Step[50/2713]: training loss : 0.9628394484519959 TRAIN  loss dict:  {'classification_loss': 0.9628394484519959}
2025-01-16 22:03:43,735 [INFO] Step[100/2713]: training loss : 0.9333159446716308 TRAIN  loss dict:  {'classification_loss': 0.9333159446716308}
2025-01-16 22:03:59,803 [INFO] Step[150/2713]: training loss : 0.9586356258392335 TRAIN  loss dict:  {'classification_loss': 0.9586356258392335}
2025-01-16 22:04:15,914 [INFO] Step[200/2713]: training loss : 0.9377246165275573 TRAIN  loss dict:  {'classification_loss': 0.9377246165275573}
2025-01-16 22:04:32,017 [INFO] Step[250/2713]: training loss : 0.9401454222202301 TRAIN  loss dict:  {'classification_loss': 0.9401454222202301}
2025-01-16 22:04:48,231 [INFO] Step[300/2713]: training loss : 0.9321506845951081 TRAIN  loss dict:  {'classification_loss': 0.9321506845951081}
2025-01-16 22:05:04,325 [INFO] Step[350/2713]: training loss : 0.9333164584636688 TRAIN  loss dict:  {'classification_loss': 0.9333164584636688}
2025-01-16 22:05:20,494 [INFO] Step[400/2713]: training loss : 0.9366955423355102 TRAIN  loss dict:  {'classification_loss': 0.9366955423355102}
2025-01-16 22:05:36,630 [INFO] Step[450/2713]: training loss : 0.9570604002475739 TRAIN  loss dict:  {'classification_loss': 0.9570604002475739}
2025-01-16 22:05:52,696 [INFO] Step[500/2713]: training loss : 0.9347284376621247 TRAIN  loss dict:  {'classification_loss': 0.9347284376621247}
2025-01-16 22:06:08,753 [INFO] Step[550/2713]: training loss : 0.9670822405815125 TRAIN  loss dict:  {'classification_loss': 0.9670822405815125}
2025-01-16 22:06:24,875 [INFO] Step[600/2713]: training loss : 0.9357161223888397 TRAIN  loss dict:  {'classification_loss': 0.9357161223888397}
2025-01-16 22:06:40,996 [INFO] Step[650/2713]: training loss : 0.9378995060920715 TRAIN  loss dict:  {'classification_loss': 0.9378995060920715}
2025-01-16 22:06:57,069 [INFO] Step[700/2713]: training loss : 0.9343845820426941 TRAIN  loss dict:  {'classification_loss': 0.9343845820426941}
2025-01-16 22:07:13,219 [INFO] Step[750/2713]: training loss : 0.9312059283256531 TRAIN  loss dict:  {'classification_loss': 0.9312059283256531}
2025-01-16 22:07:29,408 [INFO] Step[800/2713]: training loss : 0.9683616805076599 TRAIN  loss dict:  {'classification_loss': 0.9683616805076599}
2025-01-16 22:07:45,589 [INFO] Step[850/2713]: training loss : 0.9324168181419372 TRAIN  loss dict:  {'classification_loss': 0.9324168181419372}
2025-01-16 22:08:01,703 [INFO] Step[900/2713]: training loss : 0.970136661529541 TRAIN  loss dict:  {'classification_loss': 0.970136661529541}
2025-01-16 22:08:17,919 [INFO] Step[950/2713]: training loss : 0.9337092554569244 TRAIN  loss dict:  {'classification_loss': 0.9337092554569244}
2025-01-16 22:08:33,996 [INFO] Step[1000/2713]: training loss : 0.9359613358974457 TRAIN  loss dict:  {'classification_loss': 0.9359613358974457}
2025-01-16 22:08:50,080 [INFO] Step[1050/2713]: training loss : 0.9363358640670776 TRAIN  loss dict:  {'classification_loss': 0.9363358640670776}
2025-01-16 22:09:06,207 [INFO] Step[1100/2713]: training loss : 0.9437881469726562 TRAIN  loss dict:  {'classification_loss': 0.9437881469726562}
2025-01-16 22:09:22,387 [INFO] Step[1150/2713]: training loss : 0.9355793690681458 TRAIN  loss dict:  {'classification_loss': 0.9355793690681458}
2025-01-16 22:09:38,508 [INFO] Step[1200/2713]: training loss : 0.934371794462204 TRAIN  loss dict:  {'classification_loss': 0.934371794462204}
2025-01-16 22:09:54,670 [INFO] Step[1250/2713]: training loss : 0.9464108955860138 TRAIN  loss dict:  {'classification_loss': 0.9464108955860138}
2025-01-16 22:10:10,763 [INFO] Step[1300/2713]: training loss : 0.9340348744392395 TRAIN  loss dict:  {'classification_loss': 0.9340348744392395}
2025-01-16 22:10:26,881 [INFO] Step[1350/2713]: training loss : 0.9547299945354462 TRAIN  loss dict:  {'classification_loss': 0.9547299945354462}
2025-01-16 22:10:42,990 [INFO] Step[1400/2713]: training loss : 0.9345577359199524 TRAIN  loss dict:  {'classification_loss': 0.9345577359199524}
2025-01-16 22:10:59,157 [INFO] Step[1450/2713]: training loss : 0.955507025718689 TRAIN  loss dict:  {'classification_loss': 0.955507025718689}
2025-01-16 22:11:15,248 [INFO] Step[1500/2713]: training loss : 0.9329336678981781 TRAIN  loss dict:  {'classification_loss': 0.9329336678981781}
2025-01-16 22:11:31,331 [INFO] Step[1550/2713]: training loss : 0.9321632397174835 TRAIN  loss dict:  {'classification_loss': 0.9321632397174835}
2025-01-16 22:11:47,382 [INFO] Step[1600/2713]: training loss : 0.9361788856983185 TRAIN  loss dict:  {'classification_loss': 0.9361788856983185}
2025-01-16 22:12:03,465 [INFO] Step[1650/2713]: training loss : 0.9343050765991211 TRAIN  loss dict:  {'classification_loss': 0.9343050765991211}
2025-01-16 22:12:19,504 [INFO] Step[1700/2713]: training loss : 0.9523285353183746 TRAIN  loss dict:  {'classification_loss': 0.9523285353183746}
2025-01-16 22:12:35,635 [INFO] Step[1750/2713]: training loss : 0.9317146110534668 TRAIN  loss dict:  {'classification_loss': 0.9317146110534668}
2025-01-16 22:12:51,703 [INFO] Step[1800/2713]: training loss : 0.9314008045196533 TRAIN  loss dict:  {'classification_loss': 0.9314008045196533}
2025-01-16 22:13:07,871 [INFO] Step[1850/2713]: training loss : 0.9374999034404755 TRAIN  loss dict:  {'classification_loss': 0.9374999034404755}
2025-01-16 22:13:23,987 [INFO] Step[1900/2713]: training loss : 0.9423254299163818 TRAIN  loss dict:  {'classification_loss': 0.9423254299163818}
2025-01-16 22:13:40,194 [INFO] Step[1950/2713]: training loss : 0.9472153460979462 TRAIN  loss dict:  {'classification_loss': 0.9472153460979462}
2025-01-16 22:13:56,307 [INFO] Step[2000/2713]: training loss : 0.9344954371452332 TRAIN  loss dict:  {'classification_loss': 0.9344954371452332}
2025-01-16 22:14:12,404 [INFO] Step[2050/2713]: training loss : 0.9314246928691864 TRAIN  loss dict:  {'classification_loss': 0.9314246928691864}
2025-01-16 22:14:28,435 [INFO] Step[2100/2713]: training loss : 0.9457301735877991 TRAIN  loss dict:  {'classification_loss': 0.9457301735877991}
2025-01-16 22:14:44,648 [INFO] Step[2150/2713]: training loss : 0.9735023987293243 TRAIN  loss dict:  {'classification_loss': 0.9735023987293243}
2025-01-16 22:15:00,782 [INFO] Step[2200/2713]: training loss : 0.9546518492698669 TRAIN  loss dict:  {'classification_loss': 0.9546518492698669}
2025-01-16 22:15:16,972 [INFO] Step[2250/2713]: training loss : 0.9364715719223022 TRAIN  loss dict:  {'classification_loss': 0.9364715719223022}
2025-01-16 22:15:33,135 [INFO] Step[2300/2713]: training loss : 0.9323504710197449 TRAIN  loss dict:  {'classification_loss': 0.9323504710197449}
2025-01-16 22:15:49,400 [INFO] Step[2350/2713]: training loss : 0.9374517941474915 TRAIN  loss dict:  {'classification_loss': 0.9374517941474915}
2025-01-16 22:16:05,599 [INFO] Step[2400/2713]: training loss : 0.9376237618923188 TRAIN  loss dict:  {'classification_loss': 0.9376237618923188}
2025-01-16 22:16:21,849 [INFO] Step[2450/2713]: training loss : 0.9355877590179443 TRAIN  loss dict:  {'classification_loss': 0.9355877590179443}
2025-01-16 22:16:37,991 [INFO] Step[2500/2713]: training loss : 0.9587205696105957 TRAIN  loss dict:  {'classification_loss': 0.9587205696105957}
2025-01-16 22:16:54,234 [INFO] Step[2550/2713]: training loss : 0.9576680779457092 TRAIN  loss dict:  {'classification_loss': 0.9576680779457092}
2025-01-16 22:17:10,427 [INFO] Step[2600/2713]: training loss : 0.934091317653656 TRAIN  loss dict:  {'classification_loss': 0.934091317653656}
2025-01-16 22:17:26,643 [INFO] Step[2650/2713]: training loss : 0.9337875235080719 TRAIN  loss dict:  {'classification_loss': 0.9337875235080719}
2025-01-16 22:17:42,824 [INFO] Step[2700/2713]: training loss : 0.9566306507587433 TRAIN  loss dict:  {'classification_loss': 0.9566306507587433}
2025-01-16 22:19:01,639 [INFO] Label accuracies statistics:
2025-01-16 22:19:01,639 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.5, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.5, 114: 0.25, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 0.75, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.75, 217: 1.0, 218: 1.0, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 1.0, 257: 0.75, 258: 0.25, 259: 0.25, 260: 0.75, 261: 0.5, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 1.0, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.5, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.5, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.5, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.25, 351: 0.75, 352: 1.0, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.5, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 1.0, 379: 1.0, 380: 0.75, 381: 0.25, 382: 0.75, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 22:19:01,642 [INFO] [45] TRAIN  loss: 0.9426984008370051 acc: 0.996559773928001
2025-01-16 22:19:01,642 [INFO] [45] TRAIN  loss dict: {'classification_loss': 0.9426984008370051}
2025-01-16 22:19:01,642 [INFO] [45] VALIDATION loss: 1.959391159894771 VALIDATION acc: 0.7855799373040753
2025-01-16 22:19:01,642 [INFO] [45] VALIDATION loss dict: {'classification_loss': 1.959391159894771}
2025-01-16 22:19:01,642 [INFO] 
2025-01-16 22:19:22,159 [INFO] Step[50/2713]: training loss : 0.9314240157604218 TRAIN  loss dict:  {'classification_loss': 0.9314240157604218}
2025-01-16 22:19:38,267 [INFO] Step[100/2713]: training loss : 0.971156828403473 TRAIN  loss dict:  {'classification_loss': 0.971156828403473}
2025-01-16 22:19:54,412 [INFO] Step[150/2713]: training loss : 0.9406946861743927 TRAIN  loss dict:  {'classification_loss': 0.9406946861743927}
2025-01-16 22:20:10,565 [INFO] Step[200/2713]: training loss : 0.9317237031459809 TRAIN  loss dict:  {'classification_loss': 0.9317237031459809}
2025-01-16 22:20:26,642 [INFO] Step[250/2713]: training loss : 0.9752101504802704 TRAIN  loss dict:  {'classification_loss': 0.9752101504802704}
2025-01-16 22:20:42,680 [INFO] Step[300/2713]: training loss : 0.933608604669571 TRAIN  loss dict:  {'classification_loss': 0.933608604669571}
2025-01-16 22:20:58,730 [INFO] Step[350/2713]: training loss : 0.9335185384750366 TRAIN  loss dict:  {'classification_loss': 0.9335185384750366}
2025-01-16 22:21:14,928 [INFO] Step[400/2713]: training loss : 0.9385313820838929 TRAIN  loss dict:  {'classification_loss': 0.9385313820838929}
2025-01-16 22:21:31,133 [INFO] Step[450/2713]: training loss : 0.9563500368595124 TRAIN  loss dict:  {'classification_loss': 0.9563500368595124}
2025-01-16 22:21:47,181 [INFO] Step[500/2713]: training loss : 0.9333844566345215 TRAIN  loss dict:  {'classification_loss': 0.9333844566345215}
2025-01-16 22:22:03,313 [INFO] Step[550/2713]: training loss : 0.9358772003650665 TRAIN  loss dict:  {'classification_loss': 0.9358772003650665}
2025-01-16 22:22:19,421 [INFO] Step[600/2713]: training loss : 0.9416240549087525 TRAIN  loss dict:  {'classification_loss': 0.9416240549087525}
2025-01-16 22:22:35,542 [INFO] Step[650/2713]: training loss : 0.9327194011211395 TRAIN  loss dict:  {'classification_loss': 0.9327194011211395}
2025-01-16 22:22:51,582 [INFO] Step[700/2713]: training loss : 0.931709851026535 TRAIN  loss dict:  {'classification_loss': 0.931709851026535}
2025-01-16 22:23:07,673 [INFO] Step[750/2713]: training loss : 0.935271327495575 TRAIN  loss dict:  {'classification_loss': 0.935271327495575}
2025-01-16 22:23:23,789 [INFO] Step[800/2713]: training loss : 0.9447271025180817 TRAIN  loss dict:  {'classification_loss': 0.9447271025180817}
2025-01-16 22:23:39,949 [INFO] Step[850/2713]: training loss : 0.9334048211574555 TRAIN  loss dict:  {'classification_loss': 0.9334048211574555}
2025-01-16 22:23:56,077 [INFO] Step[900/2713]: training loss : 0.9335928511619568 TRAIN  loss dict:  {'classification_loss': 0.9335928511619568}
2025-01-16 22:24:12,221 [INFO] Step[950/2713]: training loss : 0.9586343467235565 TRAIN  loss dict:  {'classification_loss': 0.9586343467235565}
2025-01-16 22:24:28,333 [INFO] Step[1000/2713]: training loss : 0.96658207654953 TRAIN  loss dict:  {'classification_loss': 0.96658207654953}
2025-01-16 22:24:44,479 [INFO] Step[1050/2713]: training loss : 0.9325314712524414 TRAIN  loss dict:  {'classification_loss': 0.9325314712524414}
2025-01-16 22:25:00,578 [INFO] Step[1100/2713]: training loss : 0.9395153570175171 TRAIN  loss dict:  {'classification_loss': 0.9395153570175171}
2025-01-16 22:25:16,671 [INFO] Step[1150/2713]: training loss : 0.9376831579208375 TRAIN  loss dict:  {'classification_loss': 0.9376831579208375}
2025-01-16 22:25:32,795 [INFO] Step[1200/2713]: training loss : 0.948013037443161 TRAIN  loss dict:  {'classification_loss': 0.948013037443161}
2025-01-16 22:25:48,886 [INFO] Step[1250/2713]: training loss : 0.9666074109077454 TRAIN  loss dict:  {'classification_loss': 0.9666074109077454}
2025-01-16 22:26:05,036 [INFO] Step[1300/2713]: training loss : 0.9490729737281799 TRAIN  loss dict:  {'classification_loss': 0.9490729737281799}
2025-01-16 22:26:21,221 [INFO] Step[1350/2713]: training loss : 0.9315419626235962 TRAIN  loss dict:  {'classification_loss': 0.9315419626235962}
2025-01-16 22:26:37,326 [INFO] Step[1400/2713]: training loss : 0.9761810290813446 TRAIN  loss dict:  {'classification_loss': 0.9761810290813446}
2025-01-16 22:26:53,476 [INFO] Step[1450/2713]: training loss : 1.020310912132263 TRAIN  loss dict:  {'classification_loss': 1.020310912132263}
2025-01-16 22:27:09,575 [INFO] Step[1500/2713]: training loss : 0.9478711664676667 TRAIN  loss dict:  {'classification_loss': 0.9478711664676667}
2025-01-16 22:27:25,679 [INFO] Step[1550/2713]: training loss : 0.9327846956253052 TRAIN  loss dict:  {'classification_loss': 0.9327846956253052}
2025-01-16 22:27:41,846 [INFO] Step[1600/2713]: training loss : 0.9649333786964417 TRAIN  loss dict:  {'classification_loss': 0.9649333786964417}
2025-01-16 22:27:58,001 [INFO] Step[1650/2713]: training loss : 0.9473653447628021 TRAIN  loss dict:  {'classification_loss': 0.9473653447628021}
2025-01-16 22:28:14,122 [INFO] Step[1700/2713]: training loss : 0.9444840025901794 TRAIN  loss dict:  {'classification_loss': 0.9444840025901794}
2025-01-16 22:28:30,312 [INFO] Step[1750/2713]: training loss : 0.9309464859962463 TRAIN  loss dict:  {'classification_loss': 0.9309464859962463}
2025-01-16 22:28:46,342 [INFO] Step[1800/2713]: training loss : 0.9369044661521911 TRAIN  loss dict:  {'classification_loss': 0.9369044661521911}
2025-01-16 22:29:02,483 [INFO] Step[1850/2713]: training loss : 0.9328358256816864 TRAIN  loss dict:  {'classification_loss': 0.9328358256816864}
2025-01-16 22:29:18,536 [INFO] Step[1900/2713]: training loss : 0.9552302193641663 TRAIN  loss dict:  {'classification_loss': 0.9552302193641663}
2025-01-16 22:29:34,700 [INFO] Step[1950/2713]: training loss : 0.9480439245700836 TRAIN  loss dict:  {'classification_loss': 0.9480439245700836}
2025-01-16 22:29:50,767 [INFO] Step[2000/2713]: training loss : 0.9327019894123078 TRAIN  loss dict:  {'classification_loss': 0.9327019894123078}
2025-01-16 22:30:06,833 [INFO] Step[2050/2713]: training loss : 0.9344516253471374 TRAIN  loss dict:  {'classification_loss': 0.9344516253471374}
2025-01-16 22:30:22,969 [INFO] Step[2100/2713]: training loss : 0.9332606744766235 TRAIN  loss dict:  {'classification_loss': 0.9332606744766235}
2025-01-16 22:30:39,104 [INFO] Step[2150/2713]: training loss : 0.9399413430690765 TRAIN  loss dict:  {'classification_loss': 0.9399413430690765}
2025-01-16 22:30:55,205 [INFO] Step[2200/2713]: training loss : 0.9392616677284241 TRAIN  loss dict:  {'classification_loss': 0.9392616677284241}
2025-01-16 22:31:11,334 [INFO] Step[2250/2713]: training loss : 0.9363017749786376 TRAIN  loss dict:  {'classification_loss': 0.9363017749786376}
2025-01-16 22:31:27,411 [INFO] Step[2300/2713]: training loss : 0.9551621079444885 TRAIN  loss dict:  {'classification_loss': 0.9551621079444885}
2025-01-16 22:31:43,562 [INFO] Step[2350/2713]: training loss : 0.9345084595680236 TRAIN  loss dict:  {'classification_loss': 0.9345084595680236}
2025-01-16 22:31:59,622 [INFO] Step[2400/2713]: training loss : 0.9657711899280548 TRAIN  loss dict:  {'classification_loss': 0.9657711899280548}
2025-01-16 22:32:15,705 [INFO] Step[2450/2713]: training loss : 0.9627110254764557 TRAIN  loss dict:  {'classification_loss': 0.9627110254764557}
2025-01-16 22:32:31,769 [INFO] Step[2500/2713]: training loss : 0.9810382986068725 TRAIN  loss dict:  {'classification_loss': 0.9810382986068725}
2025-01-16 22:32:47,821 [INFO] Step[2550/2713]: training loss : 0.9327887940406799 TRAIN  loss dict:  {'classification_loss': 0.9327887940406799}
2025-01-16 22:33:03,830 [INFO] Step[2600/2713]: training loss : 0.9324163699150085 TRAIN  loss dict:  {'classification_loss': 0.9324163699150085}
2025-01-16 22:33:19,806 [INFO] Step[2650/2713]: training loss : 0.9320145452022552 TRAIN  loss dict:  {'classification_loss': 0.9320145452022552}
2025-01-16 22:33:35,769 [INFO] Step[2700/2713]: training loss : 0.9314470410346984 TRAIN  loss dict:  {'classification_loss': 0.9314470410346984}
2025-01-16 22:34:54,352 [INFO] Label accuracies statistics:
2025-01-16 22:34:54,352 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 0.8, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.25, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.5, 124: 1.0, 125: 1.0, 126: 0.75, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.5, 140: 0.75, 141: 0.75, 142: 0.25, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 0.75, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.25, 165: 0.5, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.5, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 0.75, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.75, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.75, 261: 1.0, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.0, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 0.75, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 1.0, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 1.0, 355: 1.0, 356: 1.0, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 0.75, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.25, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 22:34:54,354 [INFO] [46] TRAIN  loss: 0.9457485649008351 acc: 0.9959454478437154
2025-01-16 22:34:54,354 [INFO] [46] TRAIN  loss dict: {'classification_loss': 0.9457485649008351}
2025-01-16 22:34:54,354 [INFO] [46] VALIDATION loss: 1.8957413759894837 VALIDATION acc: 0.786833855799373
2025-01-16 22:34:54,354 [INFO] [46] VALIDATION loss dict: {'classification_loss': 1.8957413759894837}
2025-01-16 22:34:54,354 [INFO] 
2025-01-16 22:35:15,295 [INFO] Step[50/2713]: training loss : 0.9400509583950043 TRAIN  loss dict:  {'classification_loss': 0.9400509583950043}
2025-01-16 22:35:31,319 [INFO] Step[100/2713]: training loss : 0.9323009681701661 TRAIN  loss dict:  {'classification_loss': 0.9323009681701661}
2025-01-16 22:35:47,328 [INFO] Step[150/2713]: training loss : 0.9473145794868469 TRAIN  loss dict:  {'classification_loss': 0.9473145794868469}
2025-01-16 22:36:03,364 [INFO] Step[200/2713]: training loss : 0.9338810408115387 TRAIN  loss dict:  {'classification_loss': 0.9338810408115387}
2025-01-16 22:36:19,358 [INFO] Step[250/2713]: training loss : 0.9347772741317749 TRAIN  loss dict:  {'classification_loss': 0.9347772741317749}
2025-01-16 22:36:35,500 [INFO] Step[300/2713]: training loss : 0.9319778275489807 TRAIN  loss dict:  {'classification_loss': 0.9319778275489807}
2025-01-16 22:36:51,526 [INFO] Step[350/2713]: training loss : 0.931185610294342 TRAIN  loss dict:  {'classification_loss': 0.931185610294342}
2025-01-16 22:37:07,507 [INFO] Step[400/2713]: training loss : 0.9528182518482208 TRAIN  loss dict:  {'classification_loss': 0.9528182518482208}
2025-01-16 22:37:23,553 [INFO] Step[450/2713]: training loss : 0.9598053312301635 TRAIN  loss dict:  {'classification_loss': 0.9598053312301635}
2025-01-16 22:37:39,598 [INFO] Step[500/2713]: training loss : 0.9326585459709168 TRAIN  loss dict:  {'classification_loss': 0.9326585459709168}
2025-01-16 22:37:55,652 [INFO] Step[550/2713]: training loss : 0.9312246704101562 TRAIN  loss dict:  {'classification_loss': 0.9312246704101562}
2025-01-16 22:38:11,642 [INFO] Step[600/2713]: training loss : 0.9332112169265747 TRAIN  loss dict:  {'classification_loss': 0.9332112169265747}
2025-01-16 22:38:27,780 [INFO] Step[650/2713]: training loss : 0.9375590741634369 TRAIN  loss dict:  {'classification_loss': 0.9375590741634369}
2025-01-16 22:38:43,933 [INFO] Step[700/2713]: training loss : 0.9558614623546601 TRAIN  loss dict:  {'classification_loss': 0.9558614623546601}
2025-01-16 22:39:00,086 [INFO] Step[750/2713]: training loss : 0.9357135331630707 TRAIN  loss dict:  {'classification_loss': 0.9357135331630707}
2025-01-16 22:39:16,213 [INFO] Step[800/2713]: training loss : 0.9324867427349091 TRAIN  loss dict:  {'classification_loss': 0.9324867427349091}
2025-01-16 22:39:32,405 [INFO] Step[850/2713]: training loss : 0.9402789390087127 TRAIN  loss dict:  {'classification_loss': 0.9402789390087127}
2025-01-16 22:39:48,573 [INFO] Step[900/2713]: training loss : 0.942654435634613 TRAIN  loss dict:  {'classification_loss': 0.942654435634613}
2025-01-16 22:40:04,707 [INFO] Step[950/2713]: training loss : 0.9371873772144318 TRAIN  loss dict:  {'classification_loss': 0.9371873772144318}
2025-01-16 22:40:20,834 [INFO] Step[1000/2713]: training loss : 0.9378910028934478 TRAIN  loss dict:  {'classification_loss': 0.9378910028934478}
2025-01-16 22:40:36,966 [INFO] Step[1050/2713]: training loss : 0.9323230874538422 TRAIN  loss dict:  {'classification_loss': 0.9323230874538422}
2025-01-16 22:40:53,030 [INFO] Step[1100/2713]: training loss : 0.9371146929264068 TRAIN  loss dict:  {'classification_loss': 0.9371146929264068}
2025-01-16 22:41:09,188 [INFO] Step[1150/2713]: training loss : 0.9375160753726959 TRAIN  loss dict:  {'classification_loss': 0.9375160753726959}
2025-01-16 22:41:25,343 [INFO] Step[1200/2713]: training loss : 0.9345043826103211 TRAIN  loss dict:  {'classification_loss': 0.9345043826103211}
2025-01-16 22:41:41,403 [INFO] Step[1250/2713]: training loss : 0.9394700038433075 TRAIN  loss dict:  {'classification_loss': 0.9394700038433075}
2025-01-16 22:41:57,559 [INFO] Step[1300/2713]: training loss : 0.9336650860309601 TRAIN  loss dict:  {'classification_loss': 0.9336650860309601}
2025-01-16 22:42:13,572 [INFO] Step[1350/2713]: training loss : 0.9335331273078918 TRAIN  loss dict:  {'classification_loss': 0.9335331273078918}
2025-01-16 22:42:29,577 [INFO] Step[1400/2713]: training loss : 0.9546764874458313 TRAIN  loss dict:  {'classification_loss': 0.9546764874458313}
2025-01-16 22:42:45,655 [INFO] Step[1450/2713]: training loss : 0.967949446439743 TRAIN  loss dict:  {'classification_loss': 0.967949446439743}
2025-01-16 22:43:01,733 [INFO] Step[1500/2713]: training loss : 0.9413620209693909 TRAIN  loss dict:  {'classification_loss': 0.9413620209693909}
2025-01-16 22:43:17,930 [INFO] Step[1550/2713]: training loss : 0.9876117837429047 TRAIN  loss dict:  {'classification_loss': 0.9876117837429047}
2025-01-16 22:43:34,080 [INFO] Step[1600/2713]: training loss : 0.9713673317432403 TRAIN  loss dict:  {'classification_loss': 0.9713673317432403}
2025-01-16 22:43:50,274 [INFO] Step[1650/2713]: training loss : 0.9331442523002624 TRAIN  loss dict:  {'classification_loss': 0.9331442523002624}
2025-01-16 22:44:06,373 [INFO] Step[1700/2713]: training loss : 0.9397169184684754 TRAIN  loss dict:  {'classification_loss': 0.9397169184684754}
2025-01-16 22:44:22,503 [INFO] Step[1750/2713]: training loss : 0.9831582081317901 TRAIN  loss dict:  {'classification_loss': 0.9831582081317901}
2025-01-16 22:44:38,651 [INFO] Step[1800/2713]: training loss : 1.0022394859790802 TRAIN  loss dict:  {'classification_loss': 1.0022394859790802}
2025-01-16 22:44:54,801 [INFO] Step[1850/2713]: training loss : 0.9502570450305938 TRAIN  loss dict:  {'classification_loss': 0.9502570450305938}
2025-01-16 22:45:10,915 [INFO] Step[1900/2713]: training loss : 0.9677105605602264 TRAIN  loss dict:  {'classification_loss': 0.9677105605602264}
2025-01-16 22:45:27,078 [INFO] Step[1950/2713]: training loss : 1.0107675778865814 TRAIN  loss dict:  {'classification_loss': 1.0107675778865814}
2025-01-16 22:45:43,234 [INFO] Step[2000/2713]: training loss : 0.9404693067073822 TRAIN  loss dict:  {'classification_loss': 0.9404693067073822}
2025-01-16 22:45:59,405 [INFO] Step[2050/2713]: training loss : 0.9316245722770691 TRAIN  loss dict:  {'classification_loss': 0.9316245722770691}
2025-01-16 22:46:15,438 [INFO] Step[2100/2713]: training loss : 0.9630651235580444 TRAIN  loss dict:  {'classification_loss': 0.9630651235580444}
2025-01-16 22:46:31,606 [INFO] Step[2150/2713]: training loss : 0.9458142185211181 TRAIN  loss dict:  {'classification_loss': 0.9458142185211181}
2025-01-16 22:46:47,672 [INFO] Step[2200/2713]: training loss : 0.930815200805664 TRAIN  loss dict:  {'classification_loss': 0.930815200805664}
2025-01-16 22:47:03,790 [INFO] Step[2250/2713]: training loss : 0.9452637493610382 TRAIN  loss dict:  {'classification_loss': 0.9452637493610382}
2025-01-16 22:47:19,924 [INFO] Step[2300/2713]: training loss : 0.9457204556465149 TRAIN  loss dict:  {'classification_loss': 0.9457204556465149}
2025-01-16 22:47:36,134 [INFO] Step[2350/2713]: training loss : 0.9384342086315155 TRAIN  loss dict:  {'classification_loss': 0.9384342086315155}
2025-01-16 22:47:52,292 [INFO] Step[2400/2713]: training loss : 0.9323372924327851 TRAIN  loss dict:  {'classification_loss': 0.9323372924327851}
2025-01-16 22:48:08,446 [INFO] Step[2450/2713]: training loss : 0.9322261989116669 TRAIN  loss dict:  {'classification_loss': 0.9322261989116669}
2025-01-16 22:48:24,567 [INFO] Step[2500/2713]: training loss : 0.9367290532588959 TRAIN  loss dict:  {'classification_loss': 0.9367290532588959}
2025-01-16 22:48:40,742 [INFO] Step[2550/2713]: training loss : 0.9703143036365509 TRAIN  loss dict:  {'classification_loss': 0.9703143036365509}
2025-01-16 22:48:56,905 [INFO] Step[2600/2713]: training loss : 0.9360514986515045 TRAIN  loss dict:  {'classification_loss': 0.9360514986515045}
2025-01-16 22:49:13,087 [INFO] Step[2650/2713]: training loss : 0.9632514679431915 TRAIN  loss dict:  {'classification_loss': 0.9632514679431915}
2025-01-16 22:49:29,191 [INFO] Step[2700/2713]: training loss : 0.949022204875946 TRAIN  loss dict:  {'classification_loss': 0.949022204875946}
2025-01-16 22:50:47,507 [INFO] Label accuracies statistics:
2025-01-16 22:50:47,507 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.25, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.5, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.5, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 1.0, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.5, 223: 0.75, 224: 0.75, 225: 0.5, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.5, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.25, 259: 0.25, 260: 0.75, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.75, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.25, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.25, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.25, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 1.0, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 1.0, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 22:50:47,509 [INFO] [47] TRAIN  loss: 0.9462406025616568 acc: 0.9959454478437154
2025-01-16 22:50:47,509 [INFO] [47] TRAIN  loss dict: {'classification_loss': 0.9462406025616568}
2025-01-16 22:50:47,509 [INFO] [47] VALIDATION loss: 1.887005775494683 VALIDATION acc: 0.7855799373040753
2025-01-16 22:50:47,509 [INFO] [47] VALIDATION loss dict: {'classification_loss': 1.887005775494683}
2025-01-16 22:50:47,509 [INFO] 
2025-01-16 22:51:07,891 [INFO] Step[50/2713]: training loss : 0.9436494565010071 TRAIN  loss dict:  {'classification_loss': 0.9436494565010071}
2025-01-16 22:51:24,032 [INFO] Step[100/2713]: training loss : 0.9598923647403717 TRAIN  loss dict:  {'classification_loss': 0.9598923647403717}
2025-01-16 22:51:40,274 [INFO] Step[150/2713]: training loss : 0.9333361566066742 TRAIN  loss dict:  {'classification_loss': 0.9333361566066742}
2025-01-16 22:51:56,472 [INFO] Step[200/2713]: training loss : 0.9568935346603393 TRAIN  loss dict:  {'classification_loss': 0.9568935346603393}
2025-01-16 22:52:12,682 [INFO] Step[250/2713]: training loss : 0.9495004296302796 TRAIN  loss dict:  {'classification_loss': 0.9495004296302796}
2025-01-16 22:52:28,891 [INFO] Step[300/2713]: training loss : 0.9420825242996216 TRAIN  loss dict:  {'classification_loss': 0.9420825242996216}
2025-01-16 22:52:45,039 [INFO] Step[350/2713]: training loss : 0.9337382316589355 TRAIN  loss dict:  {'classification_loss': 0.9337382316589355}
2025-01-16 22:53:01,188 [INFO] Step[400/2713]: training loss : 0.9528983783721924 TRAIN  loss dict:  {'classification_loss': 0.9528983783721924}
2025-01-16 22:53:17,436 [INFO] Step[450/2713]: training loss : 0.9343073451519013 TRAIN  loss dict:  {'classification_loss': 0.9343073451519013}
2025-01-16 22:53:33,581 [INFO] Step[500/2713]: training loss : 0.9309903836250305 TRAIN  loss dict:  {'classification_loss': 0.9309903836250305}
2025-01-16 22:53:49,782 [INFO] Step[550/2713]: training loss : 0.9337605488300323 TRAIN  loss dict:  {'classification_loss': 0.9337605488300323}
2025-01-16 22:54:05,970 [INFO] Step[600/2713]: training loss : 0.9323920369148254 TRAIN  loss dict:  {'classification_loss': 0.9323920369148254}
2025-01-16 22:54:22,183 [INFO] Step[650/2713]: training loss : 0.9361443865299225 TRAIN  loss dict:  {'classification_loss': 0.9361443865299225}
2025-01-16 22:54:38,380 [INFO] Step[700/2713]: training loss : 0.9334919071197509 TRAIN  loss dict:  {'classification_loss': 0.9334919071197509}
2025-01-16 22:54:54,518 [INFO] Step[750/2713]: training loss : 0.9363448321819305 TRAIN  loss dict:  {'classification_loss': 0.9363448321819305}
2025-01-16 22:55:10,656 [INFO] Step[800/2713]: training loss : 0.9323569977283478 TRAIN  loss dict:  {'classification_loss': 0.9323569977283478}
2025-01-16 22:55:26,782 [INFO] Step[850/2713]: training loss : 0.9333089005947113 TRAIN  loss dict:  {'classification_loss': 0.9333089005947113}
2025-01-16 22:55:42,999 [INFO] Step[900/2713]: training loss : 0.977596914768219 TRAIN  loss dict:  {'classification_loss': 0.977596914768219}
2025-01-16 22:55:59,167 [INFO] Step[950/2713]: training loss : 0.9346712100505828 TRAIN  loss dict:  {'classification_loss': 0.9346712100505828}
2025-01-16 22:56:15,280 [INFO] Step[1000/2713]: training loss : 0.9490227115154266 TRAIN  loss dict:  {'classification_loss': 0.9490227115154266}
2025-01-16 22:56:31,463 [INFO] Step[1050/2713]: training loss : 0.9480549323558808 TRAIN  loss dict:  {'classification_loss': 0.9480549323558808}
2025-01-16 22:56:47,620 [INFO] Step[1100/2713]: training loss : 0.9721386933326721 TRAIN  loss dict:  {'classification_loss': 0.9721386933326721}
2025-01-16 22:57:03,795 [INFO] Step[1150/2713]: training loss : 0.9345644783973693 TRAIN  loss dict:  {'classification_loss': 0.9345644783973693}
2025-01-16 22:57:19,918 [INFO] Step[1200/2713]: training loss : 0.9352469217777252 TRAIN  loss dict:  {'classification_loss': 0.9352469217777252}
2025-01-16 22:57:36,102 [INFO] Step[1250/2713]: training loss : 0.9333639097213745 TRAIN  loss dict:  {'classification_loss': 0.9333639097213745}
2025-01-16 22:57:52,282 [INFO] Step[1300/2713]: training loss : 0.9553552460670471 TRAIN  loss dict:  {'classification_loss': 0.9553552460670471}
2025-01-16 22:58:08,452 [INFO] Step[1350/2713]: training loss : 0.9627299416065216 TRAIN  loss dict:  {'classification_loss': 0.9627299416065216}
2025-01-16 22:58:24,694 [INFO] Step[1400/2713]: training loss : 0.9330498254299164 TRAIN  loss dict:  {'classification_loss': 0.9330498254299164}
2025-01-16 22:58:40,883 [INFO] Step[1450/2713]: training loss : 0.9482057666778565 TRAIN  loss dict:  {'classification_loss': 0.9482057666778565}
2025-01-16 22:58:57,081 [INFO] Step[1500/2713]: training loss : 0.9319906508922577 TRAIN  loss dict:  {'classification_loss': 0.9319906508922577}
2025-01-16 22:59:13,309 [INFO] Step[1550/2713]: training loss : 0.9305926287174224 TRAIN  loss dict:  {'classification_loss': 0.9305926287174224}
2025-01-16 22:59:29,893 [INFO] Step[1600/2713]: training loss : 0.9399723410606384 TRAIN  loss dict:  {'classification_loss': 0.9399723410606384}
2025-01-16 22:59:46,191 [INFO] Step[1650/2713]: training loss : 0.9405103945732116 TRAIN  loss dict:  {'classification_loss': 0.9405103945732116}
2025-01-16 23:00:02,334 [INFO] Step[1700/2713]: training loss : 0.9362173771858215 TRAIN  loss dict:  {'classification_loss': 0.9362173771858215}
2025-01-16 23:00:18,558 [INFO] Step[1750/2713]: training loss : 0.9540480244159698 TRAIN  loss dict:  {'classification_loss': 0.9540480244159698}
2025-01-16 23:00:34,703 [INFO] Step[1800/2713]: training loss : 0.9342850661277771 TRAIN  loss dict:  {'classification_loss': 0.9342850661277771}
2025-01-16 23:00:50,917 [INFO] Step[1850/2713]: training loss : 0.9337724685668946 TRAIN  loss dict:  {'classification_loss': 0.9337724685668946}
2025-01-16 23:01:07,017 [INFO] Step[1900/2713]: training loss : 0.9601596438884735 TRAIN  loss dict:  {'classification_loss': 0.9601596438884735}
2025-01-16 23:01:23,175 [INFO] Step[1950/2713]: training loss : 0.950385639667511 TRAIN  loss dict:  {'classification_loss': 0.950385639667511}
2025-01-16 23:01:39,319 [INFO] Step[2000/2713]: training loss : 0.9536832714080811 TRAIN  loss dict:  {'classification_loss': 0.9536832714080811}
2025-01-16 23:01:55,461 [INFO] Step[2050/2713]: training loss : 0.9337019765377045 TRAIN  loss dict:  {'classification_loss': 0.9337019765377045}
2025-01-16 23:02:11,509 [INFO] Step[2100/2713]: training loss : 0.9625270175933838 TRAIN  loss dict:  {'classification_loss': 0.9625270175933838}
2025-01-16 23:02:27,691 [INFO] Step[2150/2713]: training loss : 0.9641214072704315 TRAIN  loss dict:  {'classification_loss': 0.9641214072704315}
2025-01-16 23:02:43,796 [INFO] Step[2200/2713]: training loss : 0.9595496284961701 TRAIN  loss dict:  {'classification_loss': 0.9595496284961701}
2025-01-16 23:02:59,924 [INFO] Step[2250/2713]: training loss : 0.993841587305069 TRAIN  loss dict:  {'classification_loss': 0.993841587305069}
2025-01-16 23:03:16,022 [INFO] Step[2300/2713]: training loss : 0.932154484987259 TRAIN  loss dict:  {'classification_loss': 0.932154484987259}
2025-01-16 23:03:32,146 [INFO] Step[2350/2713]: training loss : 0.9379873597621917 TRAIN  loss dict:  {'classification_loss': 0.9379873597621917}
2025-01-16 23:03:48,266 [INFO] Step[2400/2713]: training loss : 0.969623681306839 TRAIN  loss dict:  {'classification_loss': 0.969623681306839}
2025-01-16 23:04:04,385 [INFO] Step[2450/2713]: training loss : 0.9413904106616974 TRAIN  loss dict:  {'classification_loss': 0.9413904106616974}
2025-01-16 23:04:20,463 [INFO] Step[2500/2713]: training loss : 0.9445939946174622 TRAIN  loss dict:  {'classification_loss': 0.9445939946174622}
2025-01-16 23:04:36,614 [INFO] Step[2550/2713]: training loss : 0.9686071133613586 TRAIN  loss dict:  {'classification_loss': 0.9686071133613586}
2025-01-16 23:04:52,780 [INFO] Step[2600/2713]: training loss : 0.9783716750144958 TRAIN  loss dict:  {'classification_loss': 0.9783716750144958}
2025-01-16 23:05:08,909 [INFO] Step[2650/2713]: training loss : 0.9339931309223175 TRAIN  loss dict:  {'classification_loss': 0.9339931309223175}
2025-01-16 23:05:25,090 [INFO] Step[2700/2713]: training loss : 0.9364540445804596 TRAIN  loss dict:  {'classification_loss': 0.9364540445804596}
2025-01-16 23:06:44,467 [INFO] Label accuracies statistics:
2025-01-16 23:06:44,467 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.5, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.5, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.5, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.0, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.25, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 0.75, 266: 0.75, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.25, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.5, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 23:06:44,469 [INFO] [48] TRAIN  loss: 0.945879725899329 acc: 0.9955768521931441
2025-01-16 23:06:44,469 [INFO] [48] TRAIN  loss dict: {'classification_loss': 0.945879725899329}
2025-01-16 23:06:44,469 [INFO] [48] VALIDATION loss: 1.9365900824392648 VALIDATION acc: 0.7849529780564264
2025-01-16 23:06:44,469 [INFO] [48] VALIDATION loss dict: {'classification_loss': 1.9365900824392648}
2025-01-16 23:06:44,469 [INFO] 
2025-01-16 23:07:04,696 [INFO] Step[50/2713]: training loss : 0.9370299351215362 TRAIN  loss dict:  {'classification_loss': 0.9370299351215362}
2025-01-16 23:07:20,682 [INFO] Step[100/2713]: training loss : 1.0053325343132018 TRAIN  loss dict:  {'classification_loss': 1.0053325343132018}
2025-01-16 23:07:36,772 [INFO] Step[150/2713]: training loss : 0.9514434218406678 TRAIN  loss dict:  {'classification_loss': 0.9514434218406678}
2025-01-16 23:07:52,785 [INFO] Step[200/2713]: training loss : 0.9342713141441346 TRAIN  loss dict:  {'classification_loss': 0.9342713141441346}
2025-01-16 23:08:08,822 [INFO] Step[250/2713]: training loss : 0.9684054744243622 TRAIN  loss dict:  {'classification_loss': 0.9684054744243622}
2025-01-16 23:08:24,892 [INFO] Step[300/2713]: training loss : 0.9610708928108216 TRAIN  loss dict:  {'classification_loss': 0.9610708928108216}
2025-01-16 23:08:40,975 [INFO] Step[350/2713]: training loss : 0.9575998294353485 TRAIN  loss dict:  {'classification_loss': 0.9575998294353485}
2025-01-16 23:08:57,028 [INFO] Step[400/2713]: training loss : 0.9535387480258941 TRAIN  loss dict:  {'classification_loss': 0.9535387480258941}
2025-01-16 23:09:13,063 [INFO] Step[450/2713]: training loss : 0.930978239774704 TRAIN  loss dict:  {'classification_loss': 0.930978239774704}
2025-01-16 23:09:29,044 [INFO] Step[500/2713]: training loss : 0.950353410243988 TRAIN  loss dict:  {'classification_loss': 0.950353410243988}
2025-01-16 23:09:45,063 [INFO] Step[550/2713]: training loss : 0.9323734104633331 TRAIN  loss dict:  {'classification_loss': 0.9323734104633331}
2025-01-16 23:10:01,027 [INFO] Step[600/2713]: training loss : 0.9413961064815521 TRAIN  loss dict:  {'classification_loss': 0.9413961064815521}
2025-01-16 23:10:17,101 [INFO] Step[650/2713]: training loss : 0.9320029151439667 TRAIN  loss dict:  {'classification_loss': 0.9320029151439667}
2025-01-16 23:10:33,113 [INFO] Step[700/2713]: training loss : 0.9394385755062103 TRAIN  loss dict:  {'classification_loss': 0.9394385755062103}
2025-01-16 23:10:49,183 [INFO] Step[750/2713]: training loss : 0.9315447199344635 TRAIN  loss dict:  {'classification_loss': 0.9315447199344635}
2025-01-16 23:11:05,128 [INFO] Step[800/2713]: training loss : 0.9381488347053528 TRAIN  loss dict:  {'classification_loss': 0.9381488347053528}
2025-01-16 23:11:21,175 [INFO] Step[850/2713]: training loss : 0.934861707687378 TRAIN  loss dict:  {'classification_loss': 0.934861707687378}
2025-01-16 23:11:37,175 [INFO] Step[900/2713]: training loss : 0.9325961196422576 TRAIN  loss dict:  {'classification_loss': 0.9325961196422576}
2025-01-16 23:11:53,234 [INFO] Step[950/2713]: training loss : 0.9340443539619446 TRAIN  loss dict:  {'classification_loss': 0.9340443539619446}
2025-01-16 23:12:09,251 [INFO] Step[1000/2713]: training loss : 0.9326927340030671 TRAIN  loss dict:  {'classification_loss': 0.9326927340030671}
2025-01-16 23:12:25,312 [INFO] Step[1050/2713]: training loss : 0.9348323345184326 TRAIN  loss dict:  {'classification_loss': 0.9348323345184326}
2025-01-16 23:12:41,295 [INFO] Step[1100/2713]: training loss : 0.945227073431015 TRAIN  loss dict:  {'classification_loss': 0.945227073431015}
2025-01-16 23:12:57,304 [INFO] Step[1150/2713]: training loss : 0.9315650296211243 TRAIN  loss dict:  {'classification_loss': 0.9315650296211243}
2025-01-16 23:13:13,321 [INFO] Step[1200/2713]: training loss : 0.9323338484764099 TRAIN  loss dict:  {'classification_loss': 0.9323338484764099}
2025-01-16 23:13:29,376 [INFO] Step[1250/2713]: training loss : 0.9781076979637146 TRAIN  loss dict:  {'classification_loss': 0.9781076979637146}
2025-01-16 23:13:45,487 [INFO] Step[1300/2713]: training loss : 0.9429339814186096 TRAIN  loss dict:  {'classification_loss': 0.9429339814186096}
2025-01-16 23:14:01,461 [INFO] Step[1350/2713]: training loss : 0.9309915804862976 TRAIN  loss dict:  {'classification_loss': 0.9309915804862976}
2025-01-16 23:14:17,491 [INFO] Step[1400/2713]: training loss : 0.9322556161880493 TRAIN  loss dict:  {'classification_loss': 0.9322556161880493}
2025-01-16 23:14:33,529 [INFO] Step[1450/2713]: training loss : 0.9353722476959229 TRAIN  loss dict:  {'classification_loss': 0.9353722476959229}
2025-01-16 23:14:49,535 [INFO] Step[1500/2713]: training loss : 0.9341781103610992 TRAIN  loss dict:  {'classification_loss': 0.9341781103610992}
2025-01-16 23:15:05,564 [INFO] Step[1550/2713]: training loss : 0.9325062000751495 TRAIN  loss dict:  {'classification_loss': 0.9325062000751495}
2025-01-16 23:15:21,694 [INFO] Step[1600/2713]: training loss : 0.9326533889770507 TRAIN  loss dict:  {'classification_loss': 0.9326533889770507}
2025-01-16 23:15:37,854 [INFO] Step[1650/2713]: training loss : 0.9327517569065094 TRAIN  loss dict:  {'classification_loss': 0.9327517569065094}
2025-01-16 23:15:53,946 [INFO] Step[1700/2713]: training loss : 0.9340184557437897 TRAIN  loss dict:  {'classification_loss': 0.9340184557437897}
2025-01-16 23:16:10,042 [INFO] Step[1750/2713]: training loss : 0.9367817556858062 TRAIN  loss dict:  {'classification_loss': 0.9367817556858062}
2025-01-16 23:16:26,073 [INFO] Step[1800/2713]: training loss : 0.9800726008415223 TRAIN  loss dict:  {'classification_loss': 0.9800726008415223}
2025-01-16 23:16:42,166 [INFO] Step[1850/2713]: training loss : 0.9304695796966552 TRAIN  loss dict:  {'classification_loss': 0.9304695796966552}
2025-01-16 23:16:58,269 [INFO] Step[1900/2713]: training loss : 0.9771199381351471 TRAIN  loss dict:  {'classification_loss': 0.9771199381351471}
2025-01-16 23:17:14,581 [INFO] Step[1950/2713]: training loss : 0.948720440864563 TRAIN  loss dict:  {'classification_loss': 0.948720440864563}
2025-01-16 23:17:30,672 [INFO] Step[2000/2713]: training loss : 0.9428397417068481 TRAIN  loss dict:  {'classification_loss': 0.9428397417068481}
2025-01-16 23:17:46,629 [INFO] Step[2050/2713]: training loss : 0.9411354744434357 TRAIN  loss dict:  {'classification_loss': 0.9411354744434357}
2025-01-16 23:18:02,598 [INFO] Step[2100/2713]: training loss : 0.960313538312912 TRAIN  loss dict:  {'classification_loss': 0.960313538312912}
2025-01-16 23:18:18,569 [INFO] Step[2150/2713]: training loss : 0.9637201464176178 TRAIN  loss dict:  {'classification_loss': 0.9637201464176178}
2025-01-16 23:18:34,468 [INFO] Step[2200/2713]: training loss : 0.9402298283576965 TRAIN  loss dict:  {'classification_loss': 0.9402298283576965}
2025-01-16 23:18:50,418 [INFO] Step[2250/2713]: training loss : 0.9451509630680084 TRAIN  loss dict:  {'classification_loss': 0.9451509630680084}
2025-01-16 23:19:06,352 [INFO] Step[2300/2713]: training loss : 0.935718046426773 TRAIN  loss dict:  {'classification_loss': 0.935718046426773}
2025-01-16 23:19:22,266 [INFO] Step[2350/2713]: training loss : 0.9335004127025605 TRAIN  loss dict:  {'classification_loss': 0.9335004127025605}
2025-01-16 23:19:38,147 [INFO] Step[2400/2713]: training loss : 0.9436729526519776 TRAIN  loss dict:  {'classification_loss': 0.9436729526519776}
2025-01-16 23:19:54,140 [INFO] Step[2450/2713]: training loss : 0.9338229954242706 TRAIN  loss dict:  {'classification_loss': 0.9338229954242706}
2025-01-16 23:20:10,088 [INFO] Step[2500/2713]: training loss : 0.9333912885189056 TRAIN  loss dict:  {'classification_loss': 0.9333912885189056}
2025-01-16 23:20:26,012 [INFO] Step[2550/2713]: training loss : 0.9331723630428315 TRAIN  loss dict:  {'classification_loss': 0.9331723630428315}
2025-01-16 23:20:41,900 [INFO] Step[2600/2713]: training loss : 0.9332870388031006 TRAIN  loss dict:  {'classification_loss': 0.9332870388031006}
2025-01-16 23:20:57,898 [INFO] Step[2650/2713]: training loss : 0.9660036897659302 TRAIN  loss dict:  {'classification_loss': 0.9660036897659302}
2025-01-16 23:21:13,820 [INFO] Step[2700/2713]: training loss : 0.9311440551280975 TRAIN  loss dict:  {'classification_loss': 0.9311440551280975}
2025-01-16 23:22:31,452 [INFO] Label accuracies statistics:
2025-01-16 23:22:31,452 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.5, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.5, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.5, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.5, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.0, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.5, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 1.0, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.5, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 1.0, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.5, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-16 23:22:31,454 [INFO] [49] TRAIN  loss: 0.9432750650977807 acc: 0.9969283695785723
2025-01-16 23:22:31,454 [INFO] [49] TRAIN  loss dict: {'classification_loss': 0.9432750650977807}
2025-01-16 23:22:31,454 [INFO] [49] VALIDATION loss: 1.9266317015079628 VALIDATION acc: 0.7893416927899687
2025-01-16 23:22:31,454 [INFO] [49] VALIDATION loss dict: {'classification_loss': 1.9266317015079628}
2025-01-16 23:22:31,454 [INFO] 
2025-01-16 23:22:52,448 [INFO] Step[50/2713]: training loss : 0.9317876124382019 TRAIN  loss dict:  {'classification_loss': 0.9317876124382019}
2025-01-16 23:23:08,372 [INFO] Step[100/2713]: training loss : 0.9351248705387115 TRAIN  loss dict:  {'classification_loss': 0.9351248705387115}
2025-01-16 23:23:24,256 [INFO] Step[150/2713]: training loss : 0.9337945580482483 TRAIN  loss dict:  {'classification_loss': 0.9337945580482483}
2025-01-16 23:23:40,246 [INFO] Step[200/2713]: training loss : 0.9329516398906708 TRAIN  loss dict:  {'classification_loss': 0.9329516398906708}
2025-01-16 23:23:56,232 [INFO] Step[250/2713]: training loss : 0.9478782093524933 TRAIN  loss dict:  {'classification_loss': 0.9478782093524933}
2025-01-16 23:24:12,158 [INFO] Step[300/2713]: training loss : 0.9309463620185852 TRAIN  loss dict:  {'classification_loss': 0.9309463620185852}
2025-01-16 23:24:28,180 [INFO] Step[350/2713]: training loss : 0.9429335594177246 TRAIN  loss dict:  {'classification_loss': 0.9429335594177246}
2025-01-16 23:24:44,079 [INFO] Step[400/2713]: training loss : 0.9366408061981201 TRAIN  loss dict:  {'classification_loss': 0.9366408061981201}
2025-01-16 23:25:00,128 [INFO] Step[450/2713]: training loss : 0.9322757840156555 TRAIN  loss dict:  {'classification_loss': 0.9322757840156555}
2025-01-16 23:25:16,131 [INFO] Step[500/2713]: training loss : 0.9426514565944671 TRAIN  loss dict:  {'classification_loss': 0.9426514565944671}
2025-01-16 23:25:32,199 [INFO] Step[550/2713]: training loss : 0.9306481981277466 TRAIN  loss dict:  {'classification_loss': 0.9306481981277466}
2025-01-16 23:25:48,266 [INFO] Step[600/2713]: training loss : 0.9407103812694549 TRAIN  loss dict:  {'classification_loss': 0.9407103812694549}
2025-01-16 23:26:04,307 [INFO] Step[650/2713]: training loss : 0.9525392115116119 TRAIN  loss dict:  {'classification_loss': 0.9525392115116119}
2025-01-16 23:26:20,281 [INFO] Step[700/2713]: training loss : 0.9317232024669647 TRAIN  loss dict:  {'classification_loss': 0.9317232024669647}
2025-01-16 23:26:36,265 [INFO] Step[750/2713]: training loss : 0.9313126182556153 TRAIN  loss dict:  {'classification_loss': 0.9313126182556153}
2025-01-16 23:26:52,202 [INFO] Step[800/2713]: training loss : 0.9306221568584442 TRAIN  loss dict:  {'classification_loss': 0.9306221568584442}
2025-01-16 23:27:08,211 [INFO] Step[850/2713]: training loss : 0.9307133018970489 TRAIN  loss dict:  {'classification_loss': 0.9307133018970489}
2025-01-16 23:27:24,221 [INFO] Step[900/2713]: training loss : 0.9569790601730347 TRAIN  loss dict:  {'classification_loss': 0.9569790601730347}
2025-01-16 23:27:40,208 [INFO] Step[950/2713]: training loss : 0.930952742099762 TRAIN  loss dict:  {'classification_loss': 0.930952742099762}
2025-01-16 23:27:56,177 [INFO] Step[1000/2713]: training loss : 0.9308069920539856 TRAIN  loss dict:  {'classification_loss': 0.9308069920539856}
2025-01-16 23:28:12,190 [INFO] Step[1050/2713]: training loss : 0.972082576751709 TRAIN  loss dict:  {'classification_loss': 0.972082576751709}
2025-01-16 23:28:28,142 [INFO] Step[1100/2713]: training loss : 0.9309021651744842 TRAIN  loss dict:  {'classification_loss': 0.9309021651744842}
2025-01-16 23:28:44,219 [INFO] Step[1150/2713]: training loss : 0.9317390060424805 TRAIN  loss dict:  {'classification_loss': 0.9317390060424805}
2025-01-16 23:29:00,296 [INFO] Step[1200/2713]: training loss : 0.930541626214981 TRAIN  loss dict:  {'classification_loss': 0.930541626214981}
2025-01-16 23:29:16,365 [INFO] Step[1250/2713]: training loss : 0.9316770303249359 TRAIN  loss dict:  {'classification_loss': 0.9316770303249359}
2025-01-16 23:29:32,467 [INFO] Step[1300/2713]: training loss : 0.9556935703754426 TRAIN  loss dict:  {'classification_loss': 0.9556935703754426}
2025-01-16 23:29:48,560 [INFO] Step[1350/2713]: training loss : 0.9351885759830475 TRAIN  loss dict:  {'classification_loss': 0.9351885759830475}
2025-01-16 23:30:04,632 [INFO] Step[1400/2713]: training loss : 0.961771091222763 TRAIN  loss dict:  {'classification_loss': 0.961771091222763}
2025-01-16 23:30:20,709 [INFO] Step[1450/2713]: training loss : 0.9351940619945526 TRAIN  loss dict:  {'classification_loss': 0.9351940619945526}
2025-01-16 23:30:36,779 [INFO] Step[1500/2713]: training loss : 0.9336223649978638 TRAIN  loss dict:  {'classification_loss': 0.9336223649978638}
2025-01-16 23:30:52,783 [INFO] Step[1550/2713]: training loss : 0.9327782189846039 TRAIN  loss dict:  {'classification_loss': 0.9327782189846039}
2025-01-16 23:31:08,871 [INFO] Step[1600/2713]: training loss : 0.9651167142391205 TRAIN  loss dict:  {'classification_loss': 0.9651167142391205}
2025-01-16 23:31:24,969 [INFO] Step[1650/2713]: training loss : 0.9583916544914246 TRAIN  loss dict:  {'classification_loss': 0.9583916544914246}
2025-01-16 23:31:41,017 [INFO] Step[1700/2713]: training loss : 0.9785513234138489 TRAIN  loss dict:  {'classification_loss': 0.9785513234138489}
2025-01-16 23:31:57,035 [INFO] Step[1750/2713]: training loss : 0.9712119698524475 TRAIN  loss dict:  {'classification_loss': 0.9712119698524475}
2025-01-16 23:32:13,140 [INFO] Step[1800/2713]: training loss : 0.9369743371009827 TRAIN  loss dict:  {'classification_loss': 0.9369743371009827}
2025-01-16 23:32:29,191 [INFO] Step[1850/2713]: training loss : 0.9338601601123809 TRAIN  loss dict:  {'classification_loss': 0.9338601601123809}
2025-01-16 23:32:45,272 [INFO] Step[1900/2713]: training loss : 0.9341532266139985 TRAIN  loss dict:  {'classification_loss': 0.9341532266139985}
2025-01-16 23:33:01,404 [INFO] Step[1950/2713]: training loss : 0.9319549608230591 TRAIN  loss dict:  {'classification_loss': 0.9319549608230591}
2025-01-16 23:33:17,448 [INFO] Step[2000/2713]: training loss : 0.93144322514534 TRAIN  loss dict:  {'classification_loss': 0.93144322514534}
2025-01-16 23:33:33,509 [INFO] Step[2050/2713]: training loss : 0.9327628433704376 TRAIN  loss dict:  {'classification_loss': 0.9327628433704376}
2025-01-16 23:33:49,546 [INFO] Step[2100/2713]: training loss : 0.9524371016025543 TRAIN  loss dict:  {'classification_loss': 0.9524371016025543}
2025-01-16 23:34:05,590 [INFO] Step[2150/2713]: training loss : 0.9346063208580017 TRAIN  loss dict:  {'classification_loss': 0.9346063208580017}
2025-01-16 23:34:21,669 [INFO] Step[2200/2713]: training loss : 0.949160851240158 TRAIN  loss dict:  {'classification_loss': 0.949160851240158}
2025-01-16 23:34:37,739 [INFO] Step[2250/2713]: training loss : 0.9403719818592071 TRAIN  loss dict:  {'classification_loss': 0.9403719818592071}
2025-01-16 23:34:53,794 [INFO] Step[2300/2713]: training loss : 0.9310629892349244 TRAIN  loss dict:  {'classification_loss': 0.9310629892349244}
2025-01-16 23:35:09,900 [INFO] Step[2350/2713]: training loss : 0.9443875205516815 TRAIN  loss dict:  {'classification_loss': 0.9443875205516815}
2025-01-16 23:35:26,052 [INFO] Step[2400/2713]: training loss : 0.9343089818954468 TRAIN  loss dict:  {'classification_loss': 0.9343089818954468}
2025-01-16 23:35:42,202 [INFO] Step[2450/2713]: training loss : 0.9378080642223359 TRAIN  loss dict:  {'classification_loss': 0.9378080642223359}
2025-01-16 23:35:58,226 [INFO] Step[2500/2713]: training loss : 0.9305847442150116 TRAIN  loss dict:  {'classification_loss': 0.9305847442150116}
2025-01-16 23:36:14,290 [INFO] Step[2550/2713]: training loss : 0.9301541769504547 TRAIN  loss dict:  {'classification_loss': 0.9301541769504547}
2025-01-16 23:36:30,323 [INFO] Step[2600/2713]: training loss : 0.930285576581955 TRAIN  loss dict:  {'classification_loss': 0.930285576581955}
2025-01-16 23:36:46,481 [INFO] Step[2650/2713]: training loss : 0.9330943298339843 TRAIN  loss dict:  {'classification_loss': 0.9330943298339843}
2025-01-16 23:37:02,515 [INFO] Step[2700/2713]: training loss : 0.9618963646888733 TRAIN  loss dict:  {'classification_loss': 0.9618963646888733}
2025-01-16 23:38:20,003 [INFO] Label accuracies statistics:
2025-01-16 23:38:20,003 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 1.0, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 1.0, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 0.75, 141: 0.75, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.5, 223: 0.75, 224: 0.5, 225: 0.5, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.0, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.5, 262: 0.75, 263: 1.0, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.25, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.5, 329: 0.75, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.5, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.0, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.5, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 23:38:20,005 [INFO] [50] TRAIN  loss: 0.9401585644037576 acc: 0.9972969652291437
2025-01-16 23:38:20,005 [INFO] [50] TRAIN  loss dict: {'classification_loss': 0.9401585644037576}
2025-01-16 23:38:20,005 [INFO] [50] VALIDATION loss: 1.9024637983481687 VALIDATION acc: 0.7893416927899687
2025-01-16 23:38:20,005 [INFO] [50] VALIDATION loss dict: {'classification_loss': 1.9024637983481687}
2025-01-16 23:38:20,005 [INFO] 
2025-01-16 23:38:40,778 [INFO] Step[50/2713]: training loss : 0.9494853603839875 TRAIN  loss dict:  {'classification_loss': 0.9494853603839875}
2025-01-16 23:38:56,849 [INFO] Step[100/2713]: training loss : 0.9362324464321137 TRAIN  loss dict:  {'classification_loss': 0.9362324464321137}
2025-01-16 23:39:12,995 [INFO] Step[150/2713]: training loss : 0.9403498840332031 TRAIN  loss dict:  {'classification_loss': 0.9403498840332031}
2025-01-16 23:39:29,091 [INFO] Step[200/2713]: training loss : 0.9505598080158234 TRAIN  loss dict:  {'classification_loss': 0.9505598080158234}
2025-01-16 23:39:45,207 [INFO] Step[250/2713]: training loss : 0.9728934669494629 TRAIN  loss dict:  {'classification_loss': 0.9728934669494629}
2025-01-16 23:40:01,300 [INFO] Step[300/2713]: training loss : 0.9354079031944275 TRAIN  loss dict:  {'classification_loss': 0.9354079031944275}
2025-01-16 23:40:17,414 [INFO] Step[350/2713]: training loss : 0.9359066832065582 TRAIN  loss dict:  {'classification_loss': 0.9359066832065582}
2025-01-16 23:40:33,475 [INFO] Step[400/2713]: training loss : 0.9607465672492981 TRAIN  loss dict:  {'classification_loss': 0.9607465672492981}
2025-01-16 23:40:49,607 [INFO] Step[450/2713]: training loss : 0.9445455777645111 TRAIN  loss dict:  {'classification_loss': 0.9445455777645111}
2025-01-16 23:41:05,591 [INFO] Step[500/2713]: training loss : 0.9789635455608368 TRAIN  loss dict:  {'classification_loss': 0.9789635455608368}
2025-01-16 23:41:21,668 [INFO] Step[550/2713]: training loss : 0.9463024437427521 TRAIN  loss dict:  {'classification_loss': 0.9463024437427521}
2025-01-16 23:41:37,715 [INFO] Step[600/2713]: training loss : 0.9605819988250732 TRAIN  loss dict:  {'classification_loss': 0.9605819988250732}
2025-01-16 23:41:53,744 [INFO] Step[650/2713]: training loss : 0.9298573565483094 TRAIN  loss dict:  {'classification_loss': 0.9298573565483094}
2025-01-16 23:42:09,731 [INFO] Step[700/2713]: training loss : 0.9347146677970887 TRAIN  loss dict:  {'classification_loss': 0.9347146677970887}
2025-01-16 23:42:25,820 [INFO] Step[750/2713]: training loss : 0.9317588973045349 TRAIN  loss dict:  {'classification_loss': 0.9317588973045349}
2025-01-16 23:42:41,857 [INFO] Step[800/2713]: training loss : 0.9546673929691315 TRAIN  loss dict:  {'classification_loss': 0.9546673929691315}
2025-01-16 23:42:57,927 [INFO] Step[850/2713]: training loss : 0.9611488842964172 TRAIN  loss dict:  {'classification_loss': 0.9611488842964172}
2025-01-16 23:43:14,012 [INFO] Step[900/2713]: training loss : 0.9357588505744934 TRAIN  loss dict:  {'classification_loss': 0.9357588505744934}
2025-01-16 23:43:30,077 [INFO] Step[950/2713]: training loss : 0.9331137466430665 TRAIN  loss dict:  {'classification_loss': 0.9331137466430665}
2025-01-16 23:43:46,189 [INFO] Step[1000/2713]: training loss : 0.932571382522583 TRAIN  loss dict:  {'classification_loss': 0.932571382522583}
2025-01-16 23:44:02,219 [INFO] Step[1050/2713]: training loss : 0.9336568903923035 TRAIN  loss dict:  {'classification_loss': 0.9336568903923035}
2025-01-16 23:44:18,306 [INFO] Step[1100/2713]: training loss : 0.9473395502567291 TRAIN  loss dict:  {'classification_loss': 0.9473395502567291}
2025-01-16 23:44:34,400 [INFO] Step[1150/2713]: training loss : 0.9326914787292481 TRAIN  loss dict:  {'classification_loss': 0.9326914787292481}
2025-01-16 23:44:50,501 [INFO] Step[1200/2713]: training loss : 0.9444828009605408 TRAIN  loss dict:  {'classification_loss': 0.9444828009605408}
2025-01-16 23:45:06,608 [INFO] Step[1250/2713]: training loss : 0.938523440361023 TRAIN  loss dict:  {'classification_loss': 0.938523440361023}
2025-01-16 23:45:22,715 [INFO] Step[1300/2713]: training loss : 0.9383004081249237 TRAIN  loss dict:  {'classification_loss': 0.9383004081249237}
2025-01-16 23:45:38,863 [INFO] Step[1350/2713]: training loss : 0.9306085693836212 TRAIN  loss dict:  {'classification_loss': 0.9306085693836212}
2025-01-16 23:45:54,992 [INFO] Step[1400/2713]: training loss : 0.9617603147029876 TRAIN  loss dict:  {'classification_loss': 0.9617603147029876}
2025-01-16 23:46:11,147 [INFO] Step[1450/2713]: training loss : 0.9319948840141297 TRAIN  loss dict:  {'classification_loss': 0.9319948840141297}
2025-01-16 23:46:27,163 [INFO] Step[1500/2713]: training loss : 0.938065105676651 TRAIN  loss dict:  {'classification_loss': 0.938065105676651}
2025-01-16 23:46:43,292 [INFO] Step[1550/2713]: training loss : 0.935126222372055 TRAIN  loss dict:  {'classification_loss': 0.935126222372055}
2025-01-16 23:46:59,318 [INFO] Step[1600/2713]: training loss : 0.9397414469718933 TRAIN  loss dict:  {'classification_loss': 0.9397414469718933}
2025-01-16 23:47:15,453 [INFO] Step[1650/2713]: training loss : 0.9472398221492767 TRAIN  loss dict:  {'classification_loss': 0.9472398221492767}
2025-01-16 23:47:31,530 [INFO] Step[1700/2713]: training loss : 0.9437304747104645 TRAIN  loss dict:  {'classification_loss': 0.9437304747104645}
2025-01-16 23:47:47,636 [INFO] Step[1750/2713]: training loss : 0.9465055978298187 TRAIN  loss dict:  {'classification_loss': 0.9465055978298187}
2025-01-16 23:48:03,699 [INFO] Step[1800/2713]: training loss : 0.9307214570045471 TRAIN  loss dict:  {'classification_loss': 0.9307214570045471}
2025-01-16 23:48:19,788 [INFO] Step[1850/2713]: training loss : 0.9295728635787964 TRAIN  loss dict:  {'classification_loss': 0.9295728635787964}
2025-01-16 23:48:35,865 [INFO] Step[1900/2713]: training loss : 0.9601473987102509 TRAIN  loss dict:  {'classification_loss': 0.9601473987102509}
2025-01-16 23:48:51,972 [INFO] Step[1950/2713]: training loss : 0.9825611591339112 TRAIN  loss dict:  {'classification_loss': 0.9825611591339112}
2025-01-16 23:49:08,058 [INFO] Step[2000/2713]: training loss : 0.9341817712783813 TRAIN  loss dict:  {'classification_loss': 0.9341817712783813}
2025-01-16 23:49:24,240 [INFO] Step[2050/2713]: training loss : 0.9366414082050324 TRAIN  loss dict:  {'classification_loss': 0.9366414082050324}
2025-01-16 23:49:40,348 [INFO] Step[2100/2713]: training loss : 0.9332740318775177 TRAIN  loss dict:  {'classification_loss': 0.9332740318775177}
2025-01-16 23:49:56,435 [INFO] Step[2150/2713]: training loss : 0.9309372353553772 TRAIN  loss dict:  {'classification_loss': 0.9309372353553772}
2025-01-16 23:50:12,526 [INFO] Step[2200/2713]: training loss : 0.9323320281505585 TRAIN  loss dict:  {'classification_loss': 0.9323320281505585}
2025-01-16 23:50:28,562 [INFO] Step[2250/2713]: training loss : 0.9314144599437714 TRAIN  loss dict:  {'classification_loss': 0.9314144599437714}
2025-01-16 23:50:44,664 [INFO] Step[2300/2713]: training loss : 0.9356976461410522 TRAIN  loss dict:  {'classification_loss': 0.9356976461410522}
2025-01-16 23:51:00,786 [INFO] Step[2350/2713]: training loss : 0.9311658263206481 TRAIN  loss dict:  {'classification_loss': 0.9311658263206481}
2025-01-16 23:51:16,922 [INFO] Step[2400/2713]: training loss : 0.9458347201347351 TRAIN  loss dict:  {'classification_loss': 0.9458347201347351}
2025-01-16 23:51:32,950 [INFO] Step[2450/2713]: training loss : 0.9318234086036682 TRAIN  loss dict:  {'classification_loss': 0.9318234086036682}
2025-01-16 23:51:49,027 [INFO] Step[2500/2713]: training loss : 0.9572434043884277 TRAIN  loss dict:  {'classification_loss': 0.9572434043884277}
2025-01-16 23:52:05,152 [INFO] Step[2550/2713]: training loss : 0.9309229576587676 TRAIN  loss dict:  {'classification_loss': 0.9309229576587676}
2025-01-16 23:52:21,294 [INFO] Step[2600/2713]: training loss : 0.9686611819267273 TRAIN  loss dict:  {'classification_loss': 0.9686611819267273}
2025-01-16 23:52:37,391 [INFO] Step[2650/2713]: training loss : 0.9724927699565887 TRAIN  loss dict:  {'classification_loss': 0.9724927699565887}
2025-01-16 23:52:53,476 [INFO] Step[2700/2713]: training loss : 0.9605318570137024 TRAIN  loss dict:  {'classification_loss': 0.9605318570137024}
2025-01-16 23:54:11,248 [INFO] Label accuracies statistics:
2025-01-16 23:54:11,248 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 1.0, 20: 1.0, 21: 0.5, 22: 0.75, 23: 0.75, 24: 1.0, 25: 1.0, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.25, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 0.75, 142: 0.25, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.25, 257: 0.5, 258: 0.5, 259: 0.25, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.5, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 1.0, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.5, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.25, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-16 23:54:11,250 [INFO] [51] TRAIN  loss: 0.9438409867253288 acc: 0.9961911782774296
2025-01-16 23:54:11,250 [INFO] [51] TRAIN  loss dict: {'classification_loss': 0.9438409867253288}
2025-01-16 23:54:11,250 [INFO] [51] VALIDATION loss: 1.8958869626871626 VALIDATION acc: 0.7874608150470219
2025-01-16 23:54:11,250 [INFO] [51] VALIDATION loss dict: {'classification_loss': 1.8958869626871626}
2025-01-16 23:54:11,250 [INFO] 
2025-01-16 23:54:32,241 [INFO] Step[50/2713]: training loss : 0.9313782405853271 TRAIN  loss dict:  {'classification_loss': 0.9313782405853271}
2025-01-16 23:54:48,249 [INFO] Step[100/2713]: training loss : 0.9390344417095184 TRAIN  loss dict:  {'classification_loss': 0.9390344417095184}
2025-01-16 23:55:04,337 [INFO] Step[150/2713]: training loss : 0.9333567464351654 TRAIN  loss dict:  {'classification_loss': 0.9333567464351654}
2025-01-16 23:55:20,330 [INFO] Step[200/2713]: training loss : 0.9315735697746277 TRAIN  loss dict:  {'classification_loss': 0.9315735697746277}
2025-01-16 23:55:36,428 [INFO] Step[250/2713]: training loss : 0.9314203917980194 TRAIN  loss dict:  {'classification_loss': 0.9314203917980194}
2025-01-16 23:55:52,451 [INFO] Step[300/2713]: training loss : 0.9287364172935486 TRAIN  loss dict:  {'classification_loss': 0.9287364172935486}
2025-01-16 23:56:08,449 [INFO] Step[350/2713]: training loss : 0.9332635378837586 TRAIN  loss dict:  {'classification_loss': 0.9332635378837586}
2025-01-16 23:56:24,400 [INFO] Step[400/2713]: training loss : 0.929807060956955 TRAIN  loss dict:  {'classification_loss': 0.929807060956955}
2025-01-16 23:56:40,498 [INFO] Step[450/2713]: training loss : 0.932170877456665 TRAIN  loss dict:  {'classification_loss': 0.932170877456665}
2025-01-16 23:56:56,514 [INFO] Step[500/2713]: training loss : 0.939722684621811 TRAIN  loss dict:  {'classification_loss': 0.939722684621811}
2025-01-16 23:57:12,597 [INFO] Step[550/2713]: training loss : 0.9321597123146057 TRAIN  loss dict:  {'classification_loss': 0.9321597123146057}
2025-01-16 23:57:28,643 [INFO] Step[600/2713]: training loss : 0.9328609478473663 TRAIN  loss dict:  {'classification_loss': 0.9328609478473663}
2025-01-16 23:57:44,724 [INFO] Step[650/2713]: training loss : 0.9358057379722595 TRAIN  loss dict:  {'classification_loss': 0.9358057379722595}
2025-01-16 23:58:00,873 [INFO] Step[700/2713]: training loss : 0.9320776808261871 TRAIN  loss dict:  {'classification_loss': 0.9320776808261871}
2025-01-16 23:58:16,905 [INFO] Step[750/2713]: training loss : 0.9444656229019165 TRAIN  loss dict:  {'classification_loss': 0.9444656229019165}
2025-01-16 23:58:32,931 [INFO] Step[800/2713]: training loss : 0.9588160669803619 TRAIN  loss dict:  {'classification_loss': 0.9588160669803619}
2025-01-16 23:58:49,121 [INFO] Step[850/2713]: training loss : 0.9295900630950927 TRAIN  loss dict:  {'classification_loss': 0.9295900630950927}
2025-01-16 23:59:05,169 [INFO] Step[900/2713]: training loss : 0.9527603149414062 TRAIN  loss dict:  {'classification_loss': 0.9527603149414062}
2025-01-16 23:59:21,237 [INFO] Step[950/2713]: training loss : 0.9371979200839996 TRAIN  loss dict:  {'classification_loss': 0.9371979200839996}
2025-01-16 23:59:37,355 [INFO] Step[1000/2713]: training loss : 0.9306783223152161 TRAIN  loss dict:  {'classification_loss': 0.9306783223152161}
2025-01-16 23:59:53,408 [INFO] Step[1050/2713]: training loss : 0.9346795785427093 TRAIN  loss dict:  {'classification_loss': 0.9346795785427093}
2025-01-17 00:00:09,490 [INFO] Step[1100/2713]: training loss : 0.9289498484134674 TRAIN  loss dict:  {'classification_loss': 0.9289498484134674}
2025-01-17 00:00:25,567 [INFO] Step[1150/2713]: training loss : 0.9807614421844483 TRAIN  loss dict:  {'classification_loss': 0.9807614421844483}
2025-01-17 00:00:41,606 [INFO] Step[1200/2713]: training loss : 0.9696736812591553 TRAIN  loss dict:  {'classification_loss': 0.9696736812591553}
2025-01-17 00:00:57,699 [INFO] Step[1250/2713]: training loss : 0.9314429557323456 TRAIN  loss dict:  {'classification_loss': 0.9314429557323456}
2025-01-17 00:01:13,789 [INFO] Step[1300/2713]: training loss : 0.9296371066570281 TRAIN  loss dict:  {'classification_loss': 0.9296371066570281}
2025-01-17 00:01:29,905 [INFO] Step[1350/2713]: training loss : 0.9308451628684997 TRAIN  loss dict:  {'classification_loss': 0.9308451628684997}
2025-01-17 00:01:45,967 [INFO] Step[1400/2713]: training loss : 0.9298457181453705 TRAIN  loss dict:  {'classification_loss': 0.9298457181453705}
2025-01-17 00:02:02,083 [INFO] Step[1450/2713]: training loss : 0.9298610234260559 TRAIN  loss dict:  {'classification_loss': 0.9298610234260559}
2025-01-17 00:02:18,057 [INFO] Step[1500/2713]: training loss : 0.9302391517162323 TRAIN  loss dict:  {'classification_loss': 0.9302391517162323}
2025-01-17 00:02:34,111 [INFO] Step[1550/2713]: training loss : 0.93108318567276 TRAIN  loss dict:  {'classification_loss': 0.93108318567276}
2025-01-17 00:02:50,135 [INFO] Step[1600/2713]: training loss : 0.930405935049057 TRAIN  loss dict:  {'classification_loss': 0.930405935049057}
2025-01-17 00:03:06,175 [INFO] Step[1650/2713]: training loss : 0.9289951205253602 TRAIN  loss dict:  {'classification_loss': 0.9289951205253602}
2025-01-17 00:03:22,186 [INFO] Step[1700/2713]: training loss : 0.9297003400325775 TRAIN  loss dict:  {'classification_loss': 0.9297003400325775}
2025-01-17 00:03:38,211 [INFO] Step[1750/2713]: training loss : 0.929725044965744 TRAIN  loss dict:  {'classification_loss': 0.929725044965744}
2025-01-17 00:03:54,224 [INFO] Step[1800/2713]: training loss : 0.9296231508255005 TRAIN  loss dict:  {'classification_loss': 0.9296231508255005}
2025-01-17 00:04:10,365 [INFO] Step[1850/2713]: training loss : 0.938979377746582 TRAIN  loss dict:  {'classification_loss': 0.938979377746582}
2025-01-17 00:04:26,354 [INFO] Step[1900/2713]: training loss : 0.9305166065692901 TRAIN  loss dict:  {'classification_loss': 0.9305166065692901}
2025-01-17 00:04:42,430 [INFO] Step[1950/2713]: training loss : 0.9297999358177185 TRAIN  loss dict:  {'classification_loss': 0.9297999358177185}
2025-01-17 00:04:58,516 [INFO] Step[2000/2713]: training loss : 0.9307240581512451 TRAIN  loss dict:  {'classification_loss': 0.9307240581512451}
2025-01-17 00:05:14,558 [INFO] Step[2050/2713]: training loss : 0.9291364979743958 TRAIN  loss dict:  {'classification_loss': 0.9291364979743958}
2025-01-17 00:05:30,504 [INFO] Step[2100/2713]: training loss : 0.9316237318515778 TRAIN  loss dict:  {'classification_loss': 0.9316237318515778}
2025-01-17 00:05:46,643 [INFO] Step[2150/2713]: training loss : 0.9344671499729157 TRAIN  loss dict:  {'classification_loss': 0.9344671499729157}
2025-01-17 00:06:02,675 [INFO] Step[2200/2713]: training loss : 0.9295288944244384 TRAIN  loss dict:  {'classification_loss': 0.9295288944244384}
2025-01-17 00:06:18,826 [INFO] Step[2250/2713]: training loss : 0.9307002782821655 TRAIN  loss dict:  {'classification_loss': 0.9307002782821655}
2025-01-17 00:06:34,883 [INFO] Step[2300/2713]: training loss : 0.9307898950576782 TRAIN  loss dict:  {'classification_loss': 0.9307898950576782}
2025-01-17 00:06:51,040 [INFO] Step[2350/2713]: training loss : 0.9300939083099365 TRAIN  loss dict:  {'classification_loss': 0.9300939083099365}
2025-01-17 00:07:07,095 [INFO] Step[2400/2713]: training loss : 0.9536394381523132 TRAIN  loss dict:  {'classification_loss': 0.9536394381523132}
2025-01-17 00:07:23,165 [INFO] Step[2450/2713]: training loss : 0.9312145268917084 TRAIN  loss dict:  {'classification_loss': 0.9312145268917084}
2025-01-17 00:07:39,301 [INFO] Step[2500/2713]: training loss : 0.9389968430995941 TRAIN  loss dict:  {'classification_loss': 0.9389968430995941}
2025-01-17 00:07:55,438 [INFO] Step[2550/2713]: training loss : 0.9296107029914856 TRAIN  loss dict:  {'classification_loss': 0.9296107029914856}
2025-01-17 00:08:11,511 [INFO] Step[2600/2713]: training loss : 0.9419864356517792 TRAIN  loss dict:  {'classification_loss': 0.9419864356517792}
2025-01-17 00:08:27,606 [INFO] Step[2650/2713]: training loss : 0.933816556930542 TRAIN  loss dict:  {'classification_loss': 0.933816556930542}
2025-01-17 00:08:43,730 [INFO] Step[2700/2713]: training loss : 0.9295373249053955 TRAIN  loss dict:  {'classification_loss': 0.9295373249053955}
2025-01-17 00:10:01,527 [INFO] Label accuracies statistics:
2025-01-17 00:10:01,527 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 1.0, 21: 0.5, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 0.75, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 1.0, 220: 1.0, 221: 1.0, 222: 0.5, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.25, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 0.5, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.0, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 00:10:03,124 [INFO] [52] TRAIN  loss: 0.935124114282097 acc: 0.9985256173977147
2025-01-17 00:10:03,124 [INFO] [52] TRAIN  loss dict: {'classification_loss': 0.935124114282097}
2025-01-17 00:10:03,124 [INFO] [52] VALIDATION loss: 1.8589770546309035 VALIDATION acc: 0.7981191222570533
2025-01-17 00:10:03,124 [INFO] [52] VALIDATION loss dict: {'classification_loss': 1.8589770546309035}
2025-01-17 00:10:03,124 [INFO] 
2025-01-17 00:10:23,420 [INFO] Step[50/2713]: training loss : 0.9297336971759796 TRAIN  loss dict:  {'classification_loss': 0.9297336971759796}
2025-01-17 00:10:39,475 [INFO] Step[100/2713]: training loss : 0.9372592842578888 TRAIN  loss dict:  {'classification_loss': 0.9372592842578888}
2025-01-17 00:10:55,507 [INFO] Step[150/2713]: training loss : 0.9356550621986389 TRAIN  loss dict:  {'classification_loss': 0.9356550621986389}
2025-01-17 00:11:11,567 [INFO] Step[200/2713]: training loss : 0.936714653968811 TRAIN  loss dict:  {'classification_loss': 0.936714653968811}
2025-01-17 00:11:27,730 [INFO] Step[250/2713]: training loss : 0.9417511034011841 TRAIN  loss dict:  {'classification_loss': 0.9417511034011841}
2025-01-17 00:11:43,815 [INFO] Step[300/2713]: training loss : 0.9307090568542481 TRAIN  loss dict:  {'classification_loss': 0.9307090568542481}
2025-01-17 00:12:00,032 [INFO] Step[350/2713]: training loss : 0.9613776731491089 TRAIN  loss dict:  {'classification_loss': 0.9613776731491089}
2025-01-17 00:12:16,137 [INFO] Step[400/2713]: training loss : 0.9536871981620788 TRAIN  loss dict:  {'classification_loss': 0.9536871981620788}
2025-01-17 00:12:32,275 [INFO] Step[450/2713]: training loss : 0.9293407201766968 TRAIN  loss dict:  {'classification_loss': 0.9293407201766968}
2025-01-17 00:12:48,281 [INFO] Step[500/2713]: training loss : 0.9581508588790894 TRAIN  loss dict:  {'classification_loss': 0.9581508588790894}
2025-01-17 00:13:04,434 [INFO] Step[550/2713]: training loss : 0.9328693640232086 TRAIN  loss dict:  {'classification_loss': 0.9328693640232086}
2025-01-17 00:13:20,444 [INFO] Step[600/2713]: training loss : 0.9306646585464478 TRAIN  loss dict:  {'classification_loss': 0.9306646585464478}
2025-01-17 00:13:36,536 [INFO] Step[650/2713]: training loss : 0.9357531285285949 TRAIN  loss dict:  {'classification_loss': 0.9357531285285949}
2025-01-17 00:13:52,691 [INFO] Step[700/2713]: training loss : 0.929420291185379 TRAIN  loss dict:  {'classification_loss': 0.929420291185379}
2025-01-17 00:14:08,741 [INFO] Step[750/2713]: training loss : 0.9316883647441864 TRAIN  loss dict:  {'classification_loss': 0.9316883647441864}
2025-01-17 00:14:24,869 [INFO] Step[800/2713]: training loss : 0.9387010252475738 TRAIN  loss dict:  {'classification_loss': 0.9387010252475738}
2025-01-17 00:14:40,980 [INFO] Step[850/2713]: training loss : 0.9304133677482604 TRAIN  loss dict:  {'classification_loss': 0.9304133677482604}
2025-01-17 00:14:57,069 [INFO] Step[900/2713]: training loss : 0.9307049655914307 TRAIN  loss dict:  {'classification_loss': 0.9307049655914307}
2025-01-17 00:15:13,149 [INFO] Step[950/2713]: training loss : 0.9303124451637268 TRAIN  loss dict:  {'classification_loss': 0.9303124451637268}
2025-01-17 00:15:29,251 [INFO] Step[1000/2713]: training loss : 0.9291937506198883 TRAIN  loss dict:  {'classification_loss': 0.9291937506198883}
2025-01-17 00:15:45,354 [INFO] Step[1050/2713]: training loss : 0.9587793326377869 TRAIN  loss dict:  {'classification_loss': 0.9587793326377869}
2025-01-17 00:16:01,408 [INFO] Step[1100/2713]: training loss : 0.9296150231361389 TRAIN  loss dict:  {'classification_loss': 0.9296150231361389}
2025-01-17 00:16:17,494 [INFO] Step[1150/2713]: training loss : 0.9337982010841369 TRAIN  loss dict:  {'classification_loss': 0.9337982010841369}
2025-01-17 00:16:33,607 [INFO] Step[1200/2713]: training loss : 0.9284149491786957 TRAIN  loss dict:  {'classification_loss': 0.9284149491786957}
2025-01-17 00:16:49,694 [INFO] Step[1250/2713]: training loss : 0.9593464803695678 TRAIN  loss dict:  {'classification_loss': 0.9593464803695678}
2025-01-17 00:17:05,783 [INFO] Step[1300/2713]: training loss : 0.9372010707855225 TRAIN  loss dict:  {'classification_loss': 0.9372010707855225}
2025-01-17 00:17:21,887 [INFO] Step[1350/2713]: training loss : 0.9309510040283203 TRAIN  loss dict:  {'classification_loss': 0.9309510040283203}
2025-01-17 00:17:37,888 [INFO] Step[1400/2713]: training loss : 0.9295437455177307 TRAIN  loss dict:  {'classification_loss': 0.9295437455177307}
2025-01-17 00:17:54,002 [INFO] Step[1450/2713]: training loss : 0.9404399347305298 TRAIN  loss dict:  {'classification_loss': 0.9404399347305298}
2025-01-17 00:18:10,084 [INFO] Step[1500/2713]: training loss : 0.949983252286911 TRAIN  loss dict:  {'classification_loss': 0.949983252286911}
2025-01-17 00:18:26,212 [INFO] Step[1550/2713]: training loss : 0.9292763054370881 TRAIN  loss dict:  {'classification_loss': 0.9292763054370881}
2025-01-17 00:18:42,291 [INFO] Step[1600/2713]: training loss : 0.9329879701137542 TRAIN  loss dict:  {'classification_loss': 0.9329879701137542}
2025-01-17 00:18:58,350 [INFO] Step[1650/2713]: training loss : 0.9295852196216583 TRAIN  loss dict:  {'classification_loss': 0.9295852196216583}
2025-01-17 00:19:14,417 [INFO] Step[1700/2713]: training loss : 0.9966446471214294 TRAIN  loss dict:  {'classification_loss': 0.9966446471214294}
2025-01-17 00:19:30,509 [INFO] Step[1750/2713]: training loss : 0.9412701261043549 TRAIN  loss dict:  {'classification_loss': 0.9412701261043549}
2025-01-17 00:19:46,633 [INFO] Step[1800/2713]: training loss : 0.9503996884822845 TRAIN  loss dict:  {'classification_loss': 0.9503996884822845}
2025-01-17 00:20:02,732 [INFO] Step[1850/2713]: training loss : 0.9313392436504364 TRAIN  loss dict:  {'classification_loss': 0.9313392436504364}
2025-01-17 00:20:18,787 [INFO] Step[1900/2713]: training loss : 0.9306875467300415 TRAIN  loss dict:  {'classification_loss': 0.9306875467300415}
2025-01-17 00:20:34,962 [INFO] Step[1950/2713]: training loss : 0.9849117159843445 TRAIN  loss dict:  {'classification_loss': 0.9849117159843445}
2025-01-17 00:20:51,086 [INFO] Step[2000/2713]: training loss : 0.9300802481174469 TRAIN  loss dict:  {'classification_loss': 0.9300802481174469}
2025-01-17 00:21:07,342 [INFO] Step[2050/2713]: training loss : 0.9318462669849396 TRAIN  loss dict:  {'classification_loss': 0.9318462669849396}
2025-01-17 00:21:23,493 [INFO] Step[2100/2713]: training loss : 0.9323800241947174 TRAIN  loss dict:  {'classification_loss': 0.9323800241947174}
2025-01-17 00:21:39,752 [INFO] Step[2150/2713]: training loss : 0.930820837020874 TRAIN  loss dict:  {'classification_loss': 0.930820837020874}
2025-01-17 00:21:55,867 [INFO] Step[2200/2713]: training loss : 0.9467341458797455 TRAIN  loss dict:  {'classification_loss': 0.9467341458797455}
2025-01-17 00:22:11,986 [INFO] Step[2250/2713]: training loss : 0.9295999610424042 TRAIN  loss dict:  {'classification_loss': 0.9295999610424042}
2025-01-17 00:22:28,095 [INFO] Step[2300/2713]: training loss : 0.9322236382961273 TRAIN  loss dict:  {'classification_loss': 0.9322236382961273}
2025-01-17 00:22:44,180 [INFO] Step[2350/2713]: training loss : 0.9477171516418457 TRAIN  loss dict:  {'classification_loss': 0.9477171516418457}
2025-01-17 00:23:00,199 [INFO] Step[2400/2713]: training loss : 0.9511494779586792 TRAIN  loss dict:  {'classification_loss': 0.9511494779586792}
2025-01-17 00:23:16,336 [INFO] Step[2450/2713]: training loss : 0.9318735647201538 TRAIN  loss dict:  {'classification_loss': 0.9318735647201538}
2025-01-17 00:23:32,424 [INFO] Step[2500/2713]: training loss : 0.9389429688453674 TRAIN  loss dict:  {'classification_loss': 0.9389429688453674}
2025-01-17 00:23:48,615 [INFO] Step[2550/2713]: training loss : 0.9293636775016785 TRAIN  loss dict:  {'classification_loss': 0.9293636775016785}
2025-01-17 00:24:04,772 [INFO] Step[2600/2713]: training loss : 0.9299231731891632 TRAIN  loss dict:  {'classification_loss': 0.9299231731891632}
2025-01-17 00:24:20,911 [INFO] Step[2650/2713]: training loss : 0.9657243287563324 TRAIN  loss dict:  {'classification_loss': 0.9657243287563324}
2025-01-17 00:24:37,045 [INFO] Step[2700/2713]: training loss : 0.9510506653785705 TRAIN  loss dict:  {'classification_loss': 0.9510506653785705}
2025-01-17 00:25:55,222 [INFO] Label accuracies statistics:
2025-01-17 00:25:55,222 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.25, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.5, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.5, 131: 0.75, 132: 1.0, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.5, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.5, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.25, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 0.5, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 1.0, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.75, 371: 0.5, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 0.75, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 00:25:55,224 [INFO] [53] TRAIN  loss: 0.9395541004092325 acc: 0.9976655608797149
2025-01-17 00:25:55,224 [INFO] [53] TRAIN  loss dict: {'classification_loss': 0.9395541004092325}
2025-01-17 00:25:55,224 [INFO] [53] VALIDATION loss: 1.8899872043079002 VALIDATION acc: 0.7899686520376176
2025-01-17 00:25:55,224 [INFO] [53] VALIDATION loss dict: {'classification_loss': 1.8899872043079002}
2025-01-17 00:25:55,225 [INFO] 
2025-01-17 00:26:15,740 [INFO] Step[50/2713]: training loss : 0.9299599146842956 TRAIN  loss dict:  {'classification_loss': 0.9299599146842956}
2025-01-17 00:26:31,837 [INFO] Step[100/2713]: training loss : 0.9345087206363678 TRAIN  loss dict:  {'classification_loss': 0.9345087206363678}
2025-01-17 00:26:47,966 [INFO] Step[150/2713]: training loss : 0.9650375711917877 TRAIN  loss dict:  {'classification_loss': 0.9650375711917877}
2025-01-17 00:27:04,118 [INFO] Step[200/2713]: training loss : 0.9320156931877136 TRAIN  loss dict:  {'classification_loss': 0.9320156931877136}
2025-01-17 00:27:20,310 [INFO] Step[250/2713]: training loss : 0.9310605037212372 TRAIN  loss dict:  {'classification_loss': 0.9310605037212372}
2025-01-17 00:27:36,462 [INFO] Step[300/2713]: training loss : 0.9318989646434784 TRAIN  loss dict:  {'classification_loss': 0.9318989646434784}
2025-01-17 00:27:52,660 [INFO] Step[350/2713]: training loss : 0.9490273606777191 TRAIN  loss dict:  {'classification_loss': 0.9490273606777191}
2025-01-17 00:28:08,809 [INFO] Step[400/2713]: training loss : 0.9323355150222778 TRAIN  loss dict:  {'classification_loss': 0.9323355150222778}
2025-01-17 00:28:24,988 [INFO] Step[450/2713]: training loss : 0.9304541873931885 TRAIN  loss dict:  {'classification_loss': 0.9304541873931885}
2025-01-17 00:28:41,242 [INFO] Step[500/2713]: training loss : 0.9327515709400177 TRAIN  loss dict:  {'classification_loss': 0.9327515709400177}
2025-01-17 00:28:57,477 [INFO] Step[550/2713]: training loss : 0.9513444745540619 TRAIN  loss dict:  {'classification_loss': 0.9513444745540619}
2025-01-17 00:29:13,584 [INFO] Step[600/2713]: training loss : 0.9294010245800018 TRAIN  loss dict:  {'classification_loss': 0.9294010245800018}
2025-01-17 00:29:29,818 [INFO] Step[650/2713]: training loss : 0.9300373911857605 TRAIN  loss dict:  {'classification_loss': 0.9300373911857605}
2025-01-17 00:29:45,951 [INFO] Step[700/2713]: training loss : 0.9513126516342163 TRAIN  loss dict:  {'classification_loss': 0.9513126516342163}
2025-01-17 00:30:02,259 [INFO] Step[750/2713]: training loss : 0.9312866473197937 TRAIN  loss dict:  {'classification_loss': 0.9312866473197937}
2025-01-17 00:30:18,526 [INFO] Step[800/2713]: training loss : 0.9305375790596009 TRAIN  loss dict:  {'classification_loss': 0.9305375790596009}
2025-01-17 00:30:34,776 [INFO] Step[850/2713]: training loss : 0.9334142982959748 TRAIN  loss dict:  {'classification_loss': 0.9334142982959748}
2025-01-17 00:30:50,959 [INFO] Step[900/2713]: training loss : 0.9301662850379944 TRAIN  loss dict:  {'classification_loss': 0.9301662850379944}
2025-01-17 00:31:07,155 [INFO] Step[950/2713]: training loss : 0.9295534551143646 TRAIN  loss dict:  {'classification_loss': 0.9295534551143646}
2025-01-17 00:31:23,326 [INFO] Step[1000/2713]: training loss : 0.9301315307617187 TRAIN  loss dict:  {'classification_loss': 0.9301315307617187}
2025-01-17 00:31:39,527 [INFO] Step[1050/2713]: training loss : 0.9301184630393982 TRAIN  loss dict:  {'classification_loss': 0.9301184630393982}
2025-01-17 00:31:55,686 [INFO] Step[1100/2713]: training loss : 0.9369826579093933 TRAIN  loss dict:  {'classification_loss': 0.9369826579093933}
2025-01-17 00:32:11,964 [INFO] Step[1150/2713]: training loss : 0.9291036343574524 TRAIN  loss dict:  {'classification_loss': 0.9291036343574524}
2025-01-17 00:32:28,181 [INFO] Step[1200/2713]: training loss : 0.9297022950649262 TRAIN  loss dict:  {'classification_loss': 0.9297022950649262}
2025-01-17 00:32:44,318 [INFO] Step[1250/2713]: training loss : 0.9305436527729034 TRAIN  loss dict:  {'classification_loss': 0.9305436527729034}
2025-01-17 00:33:00,467 [INFO] Step[1300/2713]: training loss : 0.9399272930622101 TRAIN  loss dict:  {'classification_loss': 0.9399272930622101}
2025-01-17 00:33:16,685 [INFO] Step[1350/2713]: training loss : 0.9299398910999298 TRAIN  loss dict:  {'classification_loss': 0.9299398910999298}
2025-01-17 00:33:32,849 [INFO] Step[1400/2713]: training loss : 0.9294559478759765 TRAIN  loss dict:  {'classification_loss': 0.9294559478759765}
2025-01-17 00:33:49,102 [INFO] Step[1450/2713]: training loss : 0.9323769414424896 TRAIN  loss dict:  {'classification_loss': 0.9323769414424896}
2025-01-17 00:34:05,302 [INFO] Step[1500/2713]: training loss : 0.9313039505481719 TRAIN  loss dict:  {'classification_loss': 0.9313039505481719}
2025-01-17 00:34:21,461 [INFO] Step[1550/2713]: training loss : 0.9305393385887146 TRAIN  loss dict:  {'classification_loss': 0.9305393385887146}
2025-01-17 00:34:37,579 [INFO] Step[1600/2713]: training loss : 0.9304596889019012 TRAIN  loss dict:  {'classification_loss': 0.9304596889019012}
2025-01-17 00:34:53,760 [INFO] Step[1650/2713]: training loss : 0.9723850536346436 TRAIN  loss dict:  {'classification_loss': 0.9723850536346436}
2025-01-17 00:35:09,897 [INFO] Step[1700/2713]: training loss : 0.9586624073982238 TRAIN  loss dict:  {'classification_loss': 0.9586624073982238}
2025-01-17 00:35:26,076 [INFO] Step[1750/2713]: training loss : 0.9302636671066284 TRAIN  loss dict:  {'classification_loss': 0.9302636671066284}
2025-01-17 00:35:42,223 [INFO] Step[1800/2713]: training loss : 0.9342368566989898 TRAIN  loss dict:  {'classification_loss': 0.9342368566989898}
2025-01-17 00:35:58,419 [INFO] Step[1850/2713]: training loss : 0.9429049158096313 TRAIN  loss dict:  {'classification_loss': 0.9429049158096313}
2025-01-17 00:36:14,591 [INFO] Step[1900/2713]: training loss : 0.9312726759910583 TRAIN  loss dict:  {'classification_loss': 0.9312726759910583}
2025-01-17 00:36:30,823 [INFO] Step[1950/2713]: training loss : 0.9299898195266724 TRAIN  loss dict:  {'classification_loss': 0.9299898195266724}
2025-01-17 00:36:47,015 [INFO] Step[2000/2713]: training loss : 0.9300308346748352 TRAIN  loss dict:  {'classification_loss': 0.9300308346748352}
2025-01-17 00:37:03,299 [INFO] Step[2050/2713]: training loss : 0.9316194021701812 TRAIN  loss dict:  {'classification_loss': 0.9316194021701812}
2025-01-17 00:37:19,470 [INFO] Step[2100/2713]: training loss : 0.9328536283969879 TRAIN  loss dict:  {'classification_loss': 0.9328536283969879}
2025-01-17 00:37:35,696 [INFO] Step[2150/2713]: training loss : 0.9300337743759155 TRAIN  loss dict:  {'classification_loss': 0.9300337743759155}
2025-01-17 00:37:51,899 [INFO] Step[2200/2713]: training loss : 0.9296741914749146 TRAIN  loss dict:  {'classification_loss': 0.9296741914749146}
2025-01-17 00:38:08,016 [INFO] Step[2250/2713]: training loss : 0.9305661547183991 TRAIN  loss dict:  {'classification_loss': 0.9305661547183991}
2025-01-17 00:38:24,182 [INFO] Step[2300/2713]: training loss : 0.967427235841751 TRAIN  loss dict:  {'classification_loss': 0.967427235841751}
2025-01-17 00:38:40,359 [INFO] Step[2350/2713]: training loss : 0.9334531259536744 TRAIN  loss dict:  {'classification_loss': 0.9334531259536744}
2025-01-17 00:38:56,494 [INFO] Step[2400/2713]: training loss : 0.929751787185669 TRAIN  loss dict:  {'classification_loss': 0.929751787185669}
2025-01-17 00:39:12,730 [INFO] Step[2450/2713]: training loss : 0.9301951837539673 TRAIN  loss dict:  {'classification_loss': 0.9301951837539673}
2025-01-17 00:39:28,934 [INFO] Step[2500/2713]: training loss : 0.9286923718452453 TRAIN  loss dict:  {'classification_loss': 0.9286923718452453}
2025-01-17 00:39:45,104 [INFO] Step[2550/2713]: training loss : 0.929382084608078 TRAIN  loss dict:  {'classification_loss': 0.929382084608078}
2025-01-17 00:40:01,220 [INFO] Step[2600/2713]: training loss : 0.9337672781944275 TRAIN  loss dict:  {'classification_loss': 0.9337672781944275}
2025-01-17 00:40:17,535 [INFO] Step[2650/2713]: training loss : 0.9375401830673218 TRAIN  loss dict:  {'classification_loss': 0.9375401830673218}
2025-01-17 00:40:33,736 [INFO] Step[2700/2713]: training loss : 0.9292101633548736 TRAIN  loss dict:  {'classification_loss': 0.9292101633548736}
2025-01-17 00:41:51,595 [INFO] Label accuracies statistics:
2025-01-17 00:41:51,595 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 1.0, 26: 0.5, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 0.75, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 0.75, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.5, 181: 0.75, 182: 0.0, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.5, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.25, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.5, 260: 0.75, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.5, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.5, 321: 0.5, 322: 0.75, 323: 0.75, 324: 0.75, 325: 1.0, 326: 1.0, 327: 0.5, 328: 1.0, 329: 0.75, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.25, 353: 0.25, 354: 0.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.5, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 1.0, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 00:41:51,597 [INFO] [54] TRAIN  loss: 0.9351681457029863 acc: 0.9982798869640005
2025-01-17 00:41:51,597 [INFO] [54] TRAIN  loss dict: {'classification_loss': 0.9351681457029863}
2025-01-17 00:41:51,597 [INFO] [54] VALIDATION loss: 1.9460060941545587 VALIDATION acc: 0.7768025078369906
2025-01-17 00:41:51,597 [INFO] [54] VALIDATION loss dict: {'classification_loss': 1.9460060941545587}
2025-01-17 00:41:51,597 [INFO] 
2025-01-17 00:42:12,337 [INFO] Step[50/2713]: training loss : 0.9302323544025421 TRAIN  loss dict:  {'classification_loss': 0.9302323544025421}
2025-01-17 00:42:28,313 [INFO] Step[100/2713]: training loss : 0.9297163569927216 TRAIN  loss dict:  {'classification_loss': 0.9297163569927216}
2025-01-17 00:42:44,392 [INFO] Step[150/2713]: training loss : 0.9590035450458526 TRAIN  loss dict:  {'classification_loss': 0.9590035450458526}
2025-01-17 00:43:00,476 [INFO] Step[200/2713]: training loss : 0.9285015308856964 TRAIN  loss dict:  {'classification_loss': 0.9285015308856964}
2025-01-17 00:43:16,630 [INFO] Step[250/2713]: training loss : 0.9303030693531036 TRAIN  loss dict:  {'classification_loss': 0.9303030693531036}
2025-01-17 00:43:32,750 [INFO] Step[300/2713]: training loss : 0.9333589684963226 TRAIN  loss dict:  {'classification_loss': 0.9333589684963226}
2025-01-17 00:43:48,862 [INFO] Step[350/2713]: training loss : 0.9513982832431793 TRAIN  loss dict:  {'classification_loss': 0.9513982832431793}
2025-01-17 00:44:04,928 [INFO] Step[400/2713]: training loss : 0.9359266984462739 TRAIN  loss dict:  {'classification_loss': 0.9359266984462739}
2025-01-17 00:44:21,037 [INFO] Step[450/2713]: training loss : 0.9292987418174744 TRAIN  loss dict:  {'classification_loss': 0.9292987418174744}
2025-01-17 00:44:37,183 [INFO] Step[500/2713]: training loss : 0.9293324315547943 TRAIN  loss dict:  {'classification_loss': 0.9293324315547943}
2025-01-17 00:44:53,257 [INFO] Step[550/2713]: training loss : 0.9300370264053345 TRAIN  loss dict:  {'classification_loss': 0.9300370264053345}
2025-01-17 00:45:09,364 [INFO] Step[600/2713]: training loss : 0.930196807384491 TRAIN  loss dict:  {'classification_loss': 0.930196807384491}
2025-01-17 00:45:25,574 [INFO] Step[650/2713]: training loss : 0.9302142596244812 TRAIN  loss dict:  {'classification_loss': 0.9302142596244812}
2025-01-17 00:45:41,725 [INFO] Step[700/2713]: training loss : 0.9294997024536132 TRAIN  loss dict:  {'classification_loss': 0.9294997024536132}
2025-01-17 00:45:57,848 [INFO] Step[750/2713]: training loss : 0.9421535873413086 TRAIN  loss dict:  {'classification_loss': 0.9421535873413086}
2025-01-17 00:46:13,950 [INFO] Step[800/2713]: training loss : 0.9536334991455078 TRAIN  loss dict:  {'classification_loss': 0.9536334991455078}
2025-01-17 00:46:30,070 [INFO] Step[850/2713]: training loss : 0.9379642510414123 TRAIN  loss dict:  {'classification_loss': 0.9379642510414123}
2025-01-17 00:46:46,150 [INFO] Step[900/2713]: training loss : 0.9389790391921997 TRAIN  loss dict:  {'classification_loss': 0.9389790391921997}
2025-01-17 00:47:02,226 [INFO] Step[950/2713]: training loss : 0.931627037525177 TRAIN  loss dict:  {'classification_loss': 0.931627037525177}
2025-01-17 00:47:18,356 [INFO] Step[1000/2713]: training loss : 0.929602462053299 TRAIN  loss dict:  {'classification_loss': 0.929602462053299}
2025-01-17 00:47:34,514 [INFO] Step[1050/2713]: training loss : 0.9376326370239257 TRAIN  loss dict:  {'classification_loss': 0.9376326370239257}
2025-01-17 00:47:50,635 [INFO] Step[1100/2713]: training loss : 0.9302497148513794 TRAIN  loss dict:  {'classification_loss': 0.9302497148513794}
2025-01-17 00:48:06,783 [INFO] Step[1150/2713]: training loss : 0.929073623418808 TRAIN  loss dict:  {'classification_loss': 0.929073623418808}
2025-01-17 00:48:22,893 [INFO] Step[1200/2713]: training loss : 0.9475954830646515 TRAIN  loss dict:  {'classification_loss': 0.9475954830646515}
2025-01-17 00:48:39,036 [INFO] Step[1250/2713]: training loss : 0.9296686708927154 TRAIN  loss dict:  {'classification_loss': 0.9296686708927154}
2025-01-17 00:48:55,186 [INFO] Step[1300/2713]: training loss : 0.9455594325065613 TRAIN  loss dict:  {'classification_loss': 0.9455594325065613}
2025-01-17 00:49:11,376 [INFO] Step[1350/2713]: training loss : 0.9294793844223023 TRAIN  loss dict:  {'classification_loss': 0.9294793844223023}
2025-01-17 00:49:27,472 [INFO] Step[1400/2713]: training loss : 0.9307151639461517 TRAIN  loss dict:  {'classification_loss': 0.9307151639461517}
2025-01-17 00:49:43,602 [INFO] Step[1450/2713]: training loss : 0.9301648783683777 TRAIN  loss dict:  {'classification_loss': 0.9301648783683777}
2025-01-17 00:49:59,729 [INFO] Step[1500/2713]: training loss : 0.9327409529685974 TRAIN  loss dict:  {'classification_loss': 0.9327409529685974}
2025-01-17 00:50:15,923 [INFO] Step[1550/2713]: training loss : 0.93048313498497 TRAIN  loss dict:  {'classification_loss': 0.93048313498497}
2025-01-17 00:50:32,027 [INFO] Step[1600/2713]: training loss : 0.9382483553886414 TRAIN  loss dict:  {'classification_loss': 0.9382483553886414}
2025-01-17 00:50:48,166 [INFO] Step[1650/2713]: training loss : 0.9677315258979797 TRAIN  loss dict:  {'classification_loss': 0.9677315258979797}
2025-01-17 00:51:04,343 [INFO] Step[1700/2713]: training loss : 0.9302684259414673 TRAIN  loss dict:  {'classification_loss': 0.9302684259414673}
2025-01-17 00:51:20,447 [INFO] Step[1750/2713]: training loss : 0.9331031954288482 TRAIN  loss dict:  {'classification_loss': 0.9331031954288482}
2025-01-17 00:51:36,568 [INFO] Step[1800/2713]: training loss : 0.9431149351596833 TRAIN  loss dict:  {'classification_loss': 0.9431149351596833}
2025-01-17 00:51:52,765 [INFO] Step[1850/2713]: training loss : 0.9300535500049592 TRAIN  loss dict:  {'classification_loss': 0.9300535500049592}
2025-01-17 00:52:08,983 [INFO] Step[1900/2713]: training loss : 0.9602257621288299 TRAIN  loss dict:  {'classification_loss': 0.9602257621288299}
2025-01-17 00:52:25,002 [INFO] Step[1950/2713]: training loss : 0.9424124491214753 TRAIN  loss dict:  {'classification_loss': 0.9424124491214753}
2025-01-17 00:52:41,038 [INFO] Step[2000/2713]: training loss : 0.9294289433956147 TRAIN  loss dict:  {'classification_loss': 0.9294289433956147}
2025-01-17 00:52:57,116 [INFO] Step[2050/2713]: training loss : 0.9725707292556762 TRAIN  loss dict:  {'classification_loss': 0.9725707292556762}
2025-01-17 00:53:13,229 [INFO] Step[2100/2713]: training loss : 0.9521482038497925 TRAIN  loss dict:  {'classification_loss': 0.9521482038497925}
2025-01-17 00:53:29,403 [INFO] Step[2150/2713]: training loss : 0.9302993607521057 TRAIN  loss dict:  {'classification_loss': 0.9302993607521057}
2025-01-17 00:53:45,484 [INFO] Step[2200/2713]: training loss : 0.9303073382377625 TRAIN  loss dict:  {'classification_loss': 0.9303073382377625}
2025-01-17 00:54:01,586 [INFO] Step[2250/2713]: training loss : 0.9873548591136933 TRAIN  loss dict:  {'classification_loss': 0.9873548591136933}
2025-01-17 00:54:17,796 [INFO] Step[2300/2713]: training loss : 0.9297279834747314 TRAIN  loss dict:  {'classification_loss': 0.9297279834747314}
2025-01-17 00:54:34,000 [INFO] Step[2350/2713]: training loss : 0.9332572090625763 TRAIN  loss dict:  {'classification_loss': 0.9332572090625763}
2025-01-17 00:54:50,084 [INFO] Step[2400/2713]: training loss : 0.9328661775588989 TRAIN  loss dict:  {'classification_loss': 0.9328661775588989}
2025-01-17 00:55:06,180 [INFO] Step[2450/2713]: training loss : 0.9326003468036652 TRAIN  loss dict:  {'classification_loss': 0.9326003468036652}
2025-01-17 00:55:22,266 [INFO] Step[2500/2713]: training loss : 0.9476068341732025 TRAIN  loss dict:  {'classification_loss': 0.9476068341732025}
2025-01-17 00:55:38,433 [INFO] Step[2550/2713]: training loss : 0.9304158091545105 TRAIN  loss dict:  {'classification_loss': 0.9304158091545105}
2025-01-17 00:55:54,486 [INFO] Step[2600/2713]: training loss : 0.9319812381267547 TRAIN  loss dict:  {'classification_loss': 0.9319812381267547}
2025-01-17 00:56:10,615 [INFO] Step[2650/2713]: training loss : 0.944063994884491 TRAIN  loss dict:  {'classification_loss': 0.944063994884491}
2025-01-17 00:56:26,651 [INFO] Step[2700/2713]: training loss : 0.9288973498344422 TRAIN  loss dict:  {'classification_loss': 0.9288973498344422}
2025-01-17 00:57:44,257 [INFO] Label accuracies statistics:
2025-01-17 00:57:44,257 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 0.75, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.5, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 0.75, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.25, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.5, 235: 0.5, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.25, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.5, 260: 1.0, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.25, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.5, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.5, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.0, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 0.75, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 00:57:44,259 [INFO] [55] TRAIN  loss: 0.9378303053675214 acc: 0.9969283695785723
2025-01-17 00:57:44,259 [INFO] [55] TRAIN  loss dict: {'classification_loss': 0.9378303053675214}
2025-01-17 00:57:44,259 [INFO] [55] VALIDATION loss: 1.9354737154523234 VALIDATION acc: 0.7874608150470219
2025-01-17 00:57:44,259 [INFO] [55] VALIDATION loss dict: {'classification_loss': 1.9354737154523234}
2025-01-17 00:57:44,260 [INFO] 
2025-01-17 00:58:05,556 [INFO] Step[50/2713]: training loss : 0.9328919911384582 TRAIN  loss dict:  {'classification_loss': 0.9328919911384582}
2025-01-17 00:58:21,590 [INFO] Step[100/2713]: training loss : 0.9292883467674256 TRAIN  loss dict:  {'classification_loss': 0.9292883467674256}
2025-01-17 00:58:37,611 [INFO] Step[150/2713]: training loss : 0.9293981528282166 TRAIN  loss dict:  {'classification_loss': 0.9293981528282166}
2025-01-17 00:58:53,795 [INFO] Step[200/2713]: training loss : 0.9289085197448731 TRAIN  loss dict:  {'classification_loss': 0.9289085197448731}
2025-01-17 00:59:10,066 [INFO] Step[250/2713]: training loss : 0.9445290613174439 TRAIN  loss dict:  {'classification_loss': 0.9445290613174439}
2025-01-17 00:59:26,180 [INFO] Step[300/2713]: training loss : 0.9301472556591034 TRAIN  loss dict:  {'classification_loss': 0.9301472556591034}
2025-01-17 00:59:42,298 [INFO] Step[350/2713]: training loss : 0.9303822243213653 TRAIN  loss dict:  {'classification_loss': 0.9303822243213653}
2025-01-17 00:59:58,463 [INFO] Step[400/2713]: training loss : 0.9312642526626587 TRAIN  loss dict:  {'classification_loss': 0.9312642526626587}
2025-01-17 01:00:14,669 [INFO] Step[450/2713]: training loss : 0.9331581318378448 TRAIN  loss dict:  {'classification_loss': 0.9331581318378448}
2025-01-17 01:00:30,875 [INFO] Step[500/2713]: training loss : 0.9347009885311127 TRAIN  loss dict:  {'classification_loss': 0.9347009885311127}
2025-01-17 01:00:47,014 [INFO] Step[550/2713]: training loss : 0.9357124233245849 TRAIN  loss dict:  {'classification_loss': 0.9357124233245849}
2025-01-17 01:01:03,098 [INFO] Step[600/2713]: training loss : 0.9293326020240784 TRAIN  loss dict:  {'classification_loss': 0.9293326020240784}
2025-01-17 01:01:19,271 [INFO] Step[650/2713]: training loss : 0.9289134931564331 TRAIN  loss dict:  {'classification_loss': 0.9289134931564331}
2025-01-17 01:01:35,413 [INFO] Step[700/2713]: training loss : 0.9288748228549957 TRAIN  loss dict:  {'classification_loss': 0.9288748228549957}
2025-01-17 01:01:51,599 [INFO] Step[750/2713]: training loss : 0.92938068151474 TRAIN  loss dict:  {'classification_loss': 0.92938068151474}
2025-01-17 01:02:07,791 [INFO] Step[800/2713]: training loss : 0.928967354297638 TRAIN  loss dict:  {'classification_loss': 0.928967354297638}
2025-01-17 01:02:24,059 [INFO] Step[850/2713]: training loss : 0.9352732229232789 TRAIN  loss dict:  {'classification_loss': 0.9352732229232789}
2025-01-17 01:02:40,196 [INFO] Step[900/2713]: training loss : 0.9289664924144745 TRAIN  loss dict:  {'classification_loss': 0.9289664924144745}
2025-01-17 01:02:56,343 [INFO] Step[950/2713]: training loss : 0.9501680064201355 TRAIN  loss dict:  {'classification_loss': 0.9501680064201355}
2025-01-17 01:03:12,510 [INFO] Step[1000/2713]: training loss : 0.9318171644210815 TRAIN  loss dict:  {'classification_loss': 0.9318171644210815}
2025-01-17 01:03:28,682 [INFO] Step[1050/2713]: training loss : 0.9287490487098694 TRAIN  loss dict:  {'classification_loss': 0.9287490487098694}
2025-01-17 01:03:44,823 [INFO] Step[1100/2713]: training loss : 0.9300966668128967 TRAIN  loss dict:  {'classification_loss': 0.9300966668128967}
2025-01-17 01:04:00,988 [INFO] Step[1150/2713]: training loss : 0.9367063295841217 TRAIN  loss dict:  {'classification_loss': 0.9367063295841217}
2025-01-17 01:04:17,040 [INFO] Step[1200/2713]: training loss : 0.9363471281528473 TRAIN  loss dict:  {'classification_loss': 0.9363471281528473}
2025-01-17 01:04:33,234 [INFO] Step[1250/2713]: training loss : 0.9306402969360351 TRAIN  loss dict:  {'classification_loss': 0.9306402969360351}
2025-01-17 01:04:49,367 [INFO] Step[1300/2713]: training loss : 0.9441661763191224 TRAIN  loss dict:  {'classification_loss': 0.9441661763191224}
2025-01-17 01:05:05,488 [INFO] Step[1350/2713]: training loss : 0.9319373846054078 TRAIN  loss dict:  {'classification_loss': 0.9319373846054078}
2025-01-17 01:05:21,697 [INFO] Step[1400/2713]: training loss : 0.9324560594558716 TRAIN  loss dict:  {'classification_loss': 0.9324560594558716}
2025-01-17 01:05:37,895 [INFO] Step[1450/2713]: training loss : 0.9295502412319183 TRAIN  loss dict:  {'classification_loss': 0.9295502412319183}
2025-01-17 01:05:54,051 [INFO] Step[1500/2713]: training loss : 0.9285401904582977 TRAIN  loss dict:  {'classification_loss': 0.9285401904582977}
2025-01-17 01:06:10,187 [INFO] Step[1550/2713]: training loss : 0.9290208530426025 TRAIN  loss dict:  {'classification_loss': 0.9290208530426025}
2025-01-17 01:06:26,377 [INFO] Step[1600/2713]: training loss : 0.9329769468307495 TRAIN  loss dict:  {'classification_loss': 0.9329769468307495}
2025-01-17 01:06:42,520 [INFO] Step[1650/2713]: training loss : 0.9288420403003692 TRAIN  loss dict:  {'classification_loss': 0.9288420403003692}
2025-01-17 01:06:58,622 [INFO] Step[1700/2713]: training loss : 0.9560462975502014 TRAIN  loss dict:  {'classification_loss': 0.9560462975502014}
2025-01-17 01:07:14,728 [INFO] Step[1750/2713]: training loss : 0.9918632411956787 TRAIN  loss dict:  {'classification_loss': 0.9918632411956787}
2025-01-17 01:07:30,842 [INFO] Step[1800/2713]: training loss : 0.9323812937736511 TRAIN  loss dict:  {'classification_loss': 0.9323812937736511}
2025-01-17 01:07:46,961 [INFO] Step[1850/2713]: training loss : 0.930272456407547 TRAIN  loss dict:  {'classification_loss': 0.930272456407547}
2025-01-17 01:08:03,156 [INFO] Step[1900/2713]: training loss : 0.929153356552124 TRAIN  loss dict:  {'classification_loss': 0.929153356552124}
2025-01-17 01:08:19,354 [INFO] Step[1950/2713]: training loss : 0.931585887670517 TRAIN  loss dict:  {'classification_loss': 0.931585887670517}
2025-01-17 01:08:35,510 [INFO] Step[2000/2713]: training loss : 0.9285181856155396 TRAIN  loss dict:  {'classification_loss': 0.9285181856155396}
2025-01-17 01:08:51,631 [INFO] Step[2050/2713]: training loss : 0.9465193998813629 TRAIN  loss dict:  {'classification_loss': 0.9465193998813629}
2025-01-17 01:09:07,752 [INFO] Step[2100/2713]: training loss : 0.930705509185791 TRAIN  loss dict:  {'classification_loss': 0.930705509185791}
2025-01-17 01:09:23,900 [INFO] Step[2150/2713]: training loss : 0.930753642320633 TRAIN  loss dict:  {'classification_loss': 0.930753642320633}
2025-01-17 01:09:40,041 [INFO] Step[2200/2713]: training loss : 0.9299187207221985 TRAIN  loss dict:  {'classification_loss': 0.9299187207221985}
2025-01-17 01:09:56,169 [INFO] Step[2250/2713]: training loss : 0.9780415534973145 TRAIN  loss dict:  {'classification_loss': 0.9780415534973145}
2025-01-17 01:10:12,284 [INFO] Step[2300/2713]: training loss : 0.9300660228729248 TRAIN  loss dict:  {'classification_loss': 0.9300660228729248}
2025-01-17 01:10:28,472 [INFO] Step[2350/2713]: training loss : 0.9719700837135314 TRAIN  loss dict:  {'classification_loss': 0.9719700837135314}
2025-01-17 01:10:44,628 [INFO] Step[2400/2713]: training loss : 0.9678333926200867 TRAIN  loss dict:  {'classification_loss': 0.9678333926200867}
2025-01-17 01:11:00,751 [INFO] Step[2450/2713]: training loss : 0.9298852896690368 TRAIN  loss dict:  {'classification_loss': 0.9298852896690368}
2025-01-17 01:11:16,895 [INFO] Step[2500/2713]: training loss : 0.9628812849521637 TRAIN  loss dict:  {'classification_loss': 0.9628812849521637}
2025-01-17 01:11:33,086 [INFO] Step[2550/2713]: training loss : 0.9298620533943176 TRAIN  loss dict:  {'classification_loss': 0.9298620533943176}
2025-01-17 01:11:49,170 [INFO] Step[2600/2713]: training loss : 0.9316804945468903 TRAIN  loss dict:  {'classification_loss': 0.9316804945468903}
2025-01-17 01:12:05,348 [INFO] Step[2650/2713]: training loss : 0.9324106907844544 TRAIN  loss dict:  {'classification_loss': 0.9324106907844544}
2025-01-17 01:12:21,592 [INFO] Step[2700/2713]: training loss : 0.9445786046981811 TRAIN  loss dict:  {'classification_loss': 0.9445786046981811}
2025-01-17 01:13:39,787 [INFO] Label accuracies statistics:
2025-01-17 01:13:39,787 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 0.75, 142: 0.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 0.5, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.5, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.25, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 0.75, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 0.75, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 01:13:41,215 [INFO] [56] TRAIN  loss: 0.9368482827481158 acc: 0.9976655608797149
2025-01-17 01:13:41,215 [INFO] [56] TRAIN  loss dict: {'classification_loss': 0.9368482827481158}
2025-01-17 01:13:41,216 [INFO] [56] VALIDATION loss: 1.868155569622391 VALIDATION acc: 0.8006269592476489
2025-01-17 01:13:41,216 [INFO] [56] VALIDATION loss dict: {'classification_loss': 1.868155569622391}
2025-01-17 01:13:41,216 [INFO] 
2025-01-17 01:14:01,901 [INFO] Step[50/2713]: training loss : 0.9309301543235778 TRAIN  loss dict:  {'classification_loss': 0.9309301543235778}
2025-01-17 01:14:17,913 [INFO] Step[100/2713]: training loss : 0.9284988534450531 TRAIN  loss dict:  {'classification_loss': 0.9284988534450531}
2025-01-17 01:14:34,022 [INFO] Step[150/2713]: training loss : 0.9283128881454468 TRAIN  loss dict:  {'classification_loss': 0.9283128881454468}
2025-01-17 01:14:50,129 [INFO] Step[200/2713]: training loss : 0.9281142830848694 TRAIN  loss dict:  {'classification_loss': 0.9281142830848694}
2025-01-17 01:15:06,260 [INFO] Step[250/2713]: training loss : 0.9297984290122986 TRAIN  loss dict:  {'classification_loss': 0.9297984290122986}
2025-01-17 01:15:22,316 [INFO] Step[300/2713]: training loss : 0.9416812860965729 TRAIN  loss dict:  {'classification_loss': 0.9416812860965729}
2025-01-17 01:15:38,523 [INFO] Step[350/2713]: training loss : 0.9550162661075592 TRAIN  loss dict:  {'classification_loss': 0.9550162661075592}
2025-01-17 01:15:54,641 [INFO] Step[400/2713]: training loss : 0.9464906847476959 TRAIN  loss dict:  {'classification_loss': 0.9464906847476959}
2025-01-17 01:16:10,802 [INFO] Step[450/2713]: training loss : 0.9358068382740021 TRAIN  loss dict:  {'classification_loss': 0.9358068382740021}
2025-01-17 01:16:26,901 [INFO] Step[500/2713]: training loss : 0.931769711971283 TRAIN  loss dict:  {'classification_loss': 0.931769711971283}
2025-01-17 01:16:43,086 [INFO] Step[550/2713]: training loss : 0.9311727631092072 TRAIN  loss dict:  {'classification_loss': 0.9311727631092072}
2025-01-17 01:16:59,182 [INFO] Step[600/2713]: training loss : 0.9306789362430572 TRAIN  loss dict:  {'classification_loss': 0.9306789362430572}
2025-01-17 01:17:15,341 [INFO] Step[650/2713]: training loss : 0.9553163146972656 TRAIN  loss dict:  {'classification_loss': 0.9553163146972656}
2025-01-17 01:17:31,500 [INFO] Step[700/2713]: training loss : 0.9359258580207824 TRAIN  loss dict:  {'classification_loss': 0.9359258580207824}
2025-01-17 01:17:47,639 [INFO] Step[750/2713]: training loss : 0.9299796628952026 TRAIN  loss dict:  {'classification_loss': 0.9299796628952026}
2025-01-17 01:18:03,713 [INFO] Step[800/2713]: training loss : 0.9296718239784241 TRAIN  loss dict:  {'classification_loss': 0.9296718239784241}
2025-01-17 01:18:19,849 [INFO] Step[850/2713]: training loss : 0.928963314294815 TRAIN  loss dict:  {'classification_loss': 0.928963314294815}
2025-01-17 01:18:35,935 [INFO] Step[900/2713]: training loss : 0.9415541410446167 TRAIN  loss dict:  {'classification_loss': 0.9415541410446167}
2025-01-17 01:18:52,127 [INFO] Step[950/2713]: training loss : 0.9516292715072632 TRAIN  loss dict:  {'classification_loss': 0.9516292715072632}
2025-01-17 01:19:08,228 [INFO] Step[1000/2713]: training loss : 0.947116779088974 TRAIN  loss dict:  {'classification_loss': 0.947116779088974}
2025-01-17 01:19:24,365 [INFO] Step[1050/2713]: training loss : 0.936655615568161 TRAIN  loss dict:  {'classification_loss': 0.936655615568161}
2025-01-17 01:19:40,567 [INFO] Step[1100/2713]: training loss : 0.9341252446174622 TRAIN  loss dict:  {'classification_loss': 0.9341252446174622}
2025-01-17 01:19:56,741 [INFO] Step[1150/2713]: training loss : 0.9682931911945343 TRAIN  loss dict:  {'classification_loss': 0.9682931911945343}
2025-01-17 01:20:12,879 [INFO] Step[1200/2713]: training loss : 0.9299866569042206 TRAIN  loss dict:  {'classification_loss': 0.9299866569042206}
2025-01-17 01:20:29,012 [INFO] Step[1250/2713]: training loss : 0.9295980787277222 TRAIN  loss dict:  {'classification_loss': 0.9295980787277222}
2025-01-17 01:20:45,117 [INFO] Step[1300/2713]: training loss : 0.9310343706607819 TRAIN  loss dict:  {'classification_loss': 0.9310343706607819}
2025-01-17 01:21:01,267 [INFO] Step[1350/2713]: training loss : 0.9317701065540314 TRAIN  loss dict:  {'classification_loss': 0.9317701065540314}
2025-01-17 01:21:17,364 [INFO] Step[1400/2713]: training loss : 0.930768164396286 TRAIN  loss dict:  {'classification_loss': 0.930768164396286}
2025-01-17 01:21:33,461 [INFO] Step[1450/2713]: training loss : 0.9310282802581787 TRAIN  loss dict:  {'classification_loss': 0.9310282802581787}
2025-01-17 01:21:49,549 [INFO] Step[1500/2713]: training loss : 0.9309320282936097 TRAIN  loss dict:  {'classification_loss': 0.9309320282936097}
2025-01-17 01:22:05,719 [INFO] Step[1550/2713]: training loss : 0.9353555631637573 TRAIN  loss dict:  {'classification_loss': 0.9353555631637573}
2025-01-17 01:22:21,802 [INFO] Step[1600/2713]: training loss : 0.9294498908519745 TRAIN  loss dict:  {'classification_loss': 0.9294498908519745}
2025-01-17 01:22:37,912 [INFO] Step[1650/2713]: training loss : 0.9297146463394165 TRAIN  loss dict:  {'classification_loss': 0.9297146463394165}
2025-01-17 01:22:54,102 [INFO] Step[1700/2713]: training loss : 0.9473089015483857 TRAIN  loss dict:  {'classification_loss': 0.9473089015483857}
2025-01-17 01:23:10,253 [INFO] Step[1750/2713]: training loss : 0.9291089701652527 TRAIN  loss dict:  {'classification_loss': 0.9291089701652527}
2025-01-17 01:23:26,370 [INFO] Step[1800/2713]: training loss : 0.9457168340682983 TRAIN  loss dict:  {'classification_loss': 0.9457168340682983}
2025-01-17 01:23:42,519 [INFO] Step[1850/2713]: training loss : 0.9344186639785766 TRAIN  loss dict:  {'classification_loss': 0.9344186639785766}
2025-01-17 01:23:58,644 [INFO] Step[1900/2713]: training loss : 0.9599184501171112 TRAIN  loss dict:  {'classification_loss': 0.9599184501171112}
2025-01-17 01:24:14,805 [INFO] Step[1950/2713]: training loss : 0.931435843706131 TRAIN  loss dict:  {'classification_loss': 0.931435843706131}
2025-01-17 01:24:30,887 [INFO] Step[2000/2713]: training loss : 0.9579707074165345 TRAIN  loss dict:  {'classification_loss': 0.9579707074165345}
2025-01-17 01:24:46,949 [INFO] Step[2050/2713]: training loss : 0.9476930224895477 TRAIN  loss dict:  {'classification_loss': 0.9476930224895477}
2025-01-17 01:25:03,062 [INFO] Step[2100/2713]: training loss : 0.9294376373291016 TRAIN  loss dict:  {'classification_loss': 0.9294376373291016}
2025-01-17 01:25:19,182 [INFO] Step[2150/2713]: training loss : 0.9306139719486236 TRAIN  loss dict:  {'classification_loss': 0.9306139719486236}
2025-01-17 01:25:35,225 [INFO] Step[2200/2713]: training loss : 0.9446973335742951 TRAIN  loss dict:  {'classification_loss': 0.9446973335742951}
2025-01-17 01:25:51,353 [INFO] Step[2250/2713]: training loss : 0.9376058721542359 TRAIN  loss dict:  {'classification_loss': 0.9376058721542359}
2025-01-17 01:26:07,522 [INFO] Step[2300/2713]: training loss : 0.9651287877559662 TRAIN  loss dict:  {'classification_loss': 0.9651287877559662}
2025-01-17 01:26:23,639 [INFO] Step[2350/2713]: training loss : 0.9302950918674469 TRAIN  loss dict:  {'classification_loss': 0.9302950918674469}
2025-01-17 01:26:39,750 [INFO] Step[2400/2713]: training loss : 0.929882664680481 TRAIN  loss dict:  {'classification_loss': 0.929882664680481}
2025-01-17 01:26:55,869 [INFO] Step[2450/2713]: training loss : 0.9403440725803375 TRAIN  loss dict:  {'classification_loss': 0.9403440725803375}
2025-01-17 01:27:12,008 [INFO] Step[2500/2713]: training loss : 0.9364755582809449 TRAIN  loss dict:  {'classification_loss': 0.9364755582809449}
2025-01-17 01:27:28,063 [INFO] Step[2550/2713]: training loss : 0.9474148976802826 TRAIN  loss dict:  {'classification_loss': 0.9474148976802826}
2025-01-17 01:27:44,164 [INFO] Step[2600/2713]: training loss : 0.9290564215183258 TRAIN  loss dict:  {'classification_loss': 0.9290564215183258}
2025-01-17 01:28:00,357 [INFO] Step[2650/2713]: training loss : 0.9735114932060241 TRAIN  loss dict:  {'classification_loss': 0.9735114932060241}
2025-01-17 01:28:16,429 [INFO] Step[2700/2713]: training loss : 0.9291370379924774 TRAIN  loss dict:  {'classification_loss': 0.9291370379924774}
2025-01-17 01:29:34,462 [INFO] Label accuracies statistics:
2025-01-17 01:29:34,463 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.25, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.25, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 0.75, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 0.75, 144: 0.75, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.5, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.0, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 1.0, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 0.75, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.5, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.5, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 1.0, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-17 01:29:34,465 [INFO] [57] TRAIN  loss: 0.938184193566502 acc: 0.9970512347954295
2025-01-17 01:29:34,465 [INFO] [57] TRAIN  loss dict: {'classification_loss': 0.938184193566502}
2025-01-17 01:29:34,465 [INFO] [57] VALIDATION loss: 1.8979887732661755 VALIDATION acc: 0.7949843260188088
2025-01-17 01:29:34,465 [INFO] [57] VALIDATION loss dict: {'classification_loss': 1.8979887732661755}
2025-01-17 01:29:34,465 [INFO] 
2025-01-17 01:29:55,698 [INFO] Step[50/2713]: training loss : 0.9313424634933472 TRAIN  loss dict:  {'classification_loss': 0.9313424634933472}
2025-01-17 01:30:11,793 [INFO] Step[100/2713]: training loss : 0.9289450526237488 TRAIN  loss dict:  {'classification_loss': 0.9289450526237488}
2025-01-17 01:30:27,883 [INFO] Step[150/2713]: training loss : 0.9305357801914215 TRAIN  loss dict:  {'classification_loss': 0.9305357801914215}
2025-01-17 01:30:43,998 [INFO] Step[200/2713]: training loss : 0.9296317291259766 TRAIN  loss dict:  {'classification_loss': 0.9296317291259766}
2025-01-17 01:31:00,126 [INFO] Step[250/2713]: training loss : 0.956966986656189 TRAIN  loss dict:  {'classification_loss': 0.956966986656189}
2025-01-17 01:31:16,142 [INFO] Step[300/2713]: training loss : 0.9298727810382843 TRAIN  loss dict:  {'classification_loss': 0.9298727810382843}
2025-01-17 01:31:32,218 [INFO] Step[350/2713]: training loss : 0.9507668077945709 TRAIN  loss dict:  {'classification_loss': 0.9507668077945709}
2025-01-17 01:31:48,256 [INFO] Step[400/2713]: training loss : 0.9277794003486634 TRAIN  loss dict:  {'classification_loss': 0.9277794003486634}
2025-01-17 01:32:04,354 [INFO] Step[450/2713]: training loss : 0.9353695702552796 TRAIN  loss dict:  {'classification_loss': 0.9353695702552796}
2025-01-17 01:32:20,387 [INFO] Step[500/2713]: training loss : 0.9285866022109985 TRAIN  loss dict:  {'classification_loss': 0.9285866022109985}
2025-01-17 01:32:36,459 [INFO] Step[550/2713]: training loss : 0.9296334528923035 TRAIN  loss dict:  {'classification_loss': 0.9296334528923035}
2025-01-17 01:32:52,537 [INFO] Step[600/2713]: training loss : 0.9312450420856476 TRAIN  loss dict:  {'classification_loss': 0.9312450420856476}
2025-01-17 01:33:08,705 [INFO] Step[650/2713]: training loss : 0.9346508920192719 TRAIN  loss dict:  {'classification_loss': 0.9346508920192719}
2025-01-17 01:33:24,771 [INFO] Step[700/2713]: training loss : 0.9456021797657013 TRAIN  loss dict:  {'classification_loss': 0.9456021797657013}
2025-01-17 01:33:40,872 [INFO] Step[750/2713]: training loss : 0.9711250615119934 TRAIN  loss dict:  {'classification_loss': 0.9711250615119934}
2025-01-17 01:33:56,978 [INFO] Step[800/2713]: training loss : 0.9293321263790131 TRAIN  loss dict:  {'classification_loss': 0.9293321263790131}
2025-01-17 01:34:13,064 [INFO] Step[850/2713]: training loss : 0.976080060005188 TRAIN  loss dict:  {'classification_loss': 0.976080060005188}
2025-01-17 01:34:29,198 [INFO] Step[900/2713]: training loss : 0.9290368938446045 TRAIN  loss dict:  {'classification_loss': 0.9290368938446045}
2025-01-17 01:34:45,363 [INFO] Step[950/2713]: training loss : 0.931476845741272 TRAIN  loss dict:  {'classification_loss': 0.931476845741272}
2025-01-17 01:35:01,456 [INFO] Step[1000/2713]: training loss : 0.9360026514530182 TRAIN  loss dict:  {'classification_loss': 0.9360026514530182}
2025-01-17 01:35:17,597 [INFO] Step[1050/2713]: training loss : 0.9315285956859589 TRAIN  loss dict:  {'classification_loss': 0.9315285956859589}
2025-01-17 01:35:33,724 [INFO] Step[1100/2713]: training loss : 0.9462020373344422 TRAIN  loss dict:  {'classification_loss': 0.9462020373344422}
2025-01-17 01:35:49,904 [INFO] Step[1150/2713]: training loss : 0.9400094139575959 TRAIN  loss dict:  {'classification_loss': 0.9400094139575959}
2025-01-17 01:36:06,008 [INFO] Step[1200/2713]: training loss : 0.9302320790290832 TRAIN  loss dict:  {'classification_loss': 0.9302320790290832}
2025-01-17 01:36:22,078 [INFO] Step[1250/2713]: training loss : 0.9489977526664733 TRAIN  loss dict:  {'classification_loss': 0.9489977526664733}
2025-01-17 01:36:38,209 [INFO] Step[1300/2713]: training loss : 0.9290587735176087 TRAIN  loss dict:  {'classification_loss': 0.9290587735176087}
2025-01-17 01:36:54,409 [INFO] Step[1350/2713]: training loss : 0.9343353784084321 TRAIN  loss dict:  {'classification_loss': 0.9343353784084321}
2025-01-17 01:37:10,532 [INFO] Step[1400/2713]: training loss : 1.0000487422943116 TRAIN  loss dict:  {'classification_loss': 1.0000487422943116}
2025-01-17 01:37:26,657 [INFO] Step[1450/2713]: training loss : 0.9294079709053039 TRAIN  loss dict:  {'classification_loss': 0.9294079709053039}
2025-01-17 01:37:42,834 [INFO] Step[1500/2713]: training loss : 0.9290889275074005 TRAIN  loss dict:  {'classification_loss': 0.9290889275074005}
2025-01-17 01:37:58,922 [INFO] Step[1550/2713]: training loss : 0.9299703025817871 TRAIN  loss dict:  {'classification_loss': 0.9299703025817871}
2025-01-17 01:38:15,015 [INFO] Step[1600/2713]: training loss : 0.931276090145111 TRAIN  loss dict:  {'classification_loss': 0.931276090145111}
2025-01-17 01:38:31,185 [INFO] Step[1650/2713]: training loss : 0.9305422055721283 TRAIN  loss dict:  {'classification_loss': 0.9305422055721283}
2025-01-17 01:38:47,303 [INFO] Step[1700/2713]: training loss : 0.930488395690918 TRAIN  loss dict:  {'classification_loss': 0.930488395690918}
2025-01-17 01:39:03,416 [INFO] Step[1750/2713]: training loss : 0.9295716333389282 TRAIN  loss dict:  {'classification_loss': 0.9295716333389282}
2025-01-17 01:39:19,563 [INFO] Step[1800/2713]: training loss : 0.9294707584381103 TRAIN  loss dict:  {'classification_loss': 0.9294707584381103}
2025-01-17 01:39:35,671 [INFO] Step[1850/2713]: training loss : 0.9400860905647278 TRAIN  loss dict:  {'classification_loss': 0.9400860905647278}
2025-01-17 01:39:51,757 [INFO] Step[1900/2713]: training loss : 0.9291435205936431 TRAIN  loss dict:  {'classification_loss': 0.9291435205936431}
2025-01-17 01:40:07,870 [INFO] Step[1950/2713]: training loss : 0.9313945341110229 TRAIN  loss dict:  {'classification_loss': 0.9313945341110229}
2025-01-17 01:40:23,967 [INFO] Step[2000/2713]: training loss : 0.9548325252532959 TRAIN  loss dict:  {'classification_loss': 0.9548325252532959}
2025-01-17 01:40:40,198 [INFO] Step[2050/2713]: training loss : 0.9336845183372497 TRAIN  loss dict:  {'classification_loss': 0.9336845183372497}
2025-01-17 01:40:56,289 [INFO] Step[2100/2713]: training loss : 0.9423421943187713 TRAIN  loss dict:  {'classification_loss': 0.9423421943187713}
2025-01-17 01:41:12,385 [INFO] Step[2150/2713]: training loss : 0.9555422484874725 TRAIN  loss dict:  {'classification_loss': 0.9555422484874725}
2025-01-17 01:41:28,541 [INFO] Step[2200/2713]: training loss : 0.9296947813034058 TRAIN  loss dict:  {'classification_loss': 0.9296947813034058}
2025-01-17 01:41:44,678 [INFO] Step[2250/2713]: training loss : 0.9587205195426941 TRAIN  loss dict:  {'classification_loss': 0.9587205195426941}
2025-01-17 01:42:00,844 [INFO] Step[2300/2713]: training loss : 0.930386860370636 TRAIN  loss dict:  {'classification_loss': 0.930386860370636}
2025-01-17 01:42:16,964 [INFO] Step[2350/2713]: training loss : 0.9695917582511902 TRAIN  loss dict:  {'classification_loss': 0.9695917582511902}
2025-01-17 01:42:33,018 [INFO] Step[2400/2713]: training loss : 0.9407408285140991 TRAIN  loss dict:  {'classification_loss': 0.9407408285140991}
2025-01-17 01:42:49,202 [INFO] Step[2450/2713]: training loss : 0.9321892321109772 TRAIN  loss dict:  {'classification_loss': 0.9321892321109772}
2025-01-17 01:43:05,278 [INFO] Step[2500/2713]: training loss : 0.9300540709495544 TRAIN  loss dict:  {'classification_loss': 0.9300540709495544}
2025-01-17 01:43:21,298 [INFO] Step[2550/2713]: training loss : 0.9545889294147492 TRAIN  loss dict:  {'classification_loss': 0.9545889294147492}
2025-01-17 01:43:37,433 [INFO] Step[2600/2713]: training loss : 0.9563805115222931 TRAIN  loss dict:  {'classification_loss': 0.9563805115222931}
2025-01-17 01:43:53,528 [INFO] Step[2650/2713]: training loss : 0.948516685962677 TRAIN  loss dict:  {'classification_loss': 0.948516685962677}
2025-01-17 01:44:09,598 [INFO] Step[2700/2713]: training loss : 0.9371669638156891 TRAIN  loss dict:  {'classification_loss': 0.9371669638156891}
2025-01-17 01:45:28,228 [INFO] Label accuracies statistics:
2025-01-17 01:45:28,228 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.25, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 1.0, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.5, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.25, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.5, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.0, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 01:45:28,230 [INFO] [58] TRAIN  loss: 0.9395711722955729 acc: 0.9970512347954295
2025-01-17 01:45:28,230 [INFO] [58] TRAIN  loss dict: {'classification_loss': 0.9395711722955729}
2025-01-17 01:45:28,230 [INFO] [58] VALIDATION loss: 1.9342461098405652 VALIDATION acc: 0.7887147335423198
2025-01-17 01:45:28,230 [INFO] [58] VALIDATION loss dict: {'classification_loss': 1.9342461098405652}
2025-01-17 01:45:28,230 [INFO] 
2025-01-17 01:45:48,908 [INFO] Step[50/2713]: training loss : 0.9403350615501403 TRAIN  loss dict:  {'classification_loss': 0.9403350615501403}
2025-01-17 01:46:04,957 [INFO] Step[100/2713]: training loss : 0.93808633685112 TRAIN  loss dict:  {'classification_loss': 0.93808633685112}
2025-01-17 01:46:21,031 [INFO] Step[150/2713]: training loss : 0.9344689476490021 TRAIN  loss dict:  {'classification_loss': 0.9344689476490021}
2025-01-17 01:46:37,208 [INFO] Step[200/2713]: training loss : 0.9286187183856964 TRAIN  loss dict:  {'classification_loss': 0.9286187183856964}
2025-01-17 01:46:53,253 [INFO] Step[250/2713]: training loss : 0.9286753177642822 TRAIN  loss dict:  {'classification_loss': 0.9286753177642822}
2025-01-17 01:47:09,274 [INFO] Step[300/2713]: training loss : 0.9381788301467896 TRAIN  loss dict:  {'classification_loss': 0.9381788301467896}
2025-01-17 01:47:25,377 [INFO] Step[350/2713]: training loss : 0.9369399178028107 TRAIN  loss dict:  {'classification_loss': 0.9369399178028107}
2025-01-17 01:47:41,472 [INFO] Step[400/2713]: training loss : 0.9301063072681427 TRAIN  loss dict:  {'classification_loss': 0.9301063072681427}
2025-01-17 01:47:57,494 [INFO] Step[450/2713]: training loss : 0.9410103738307953 TRAIN  loss dict:  {'classification_loss': 0.9410103738307953}
2025-01-17 01:48:13,553 [INFO] Step[500/2713]: training loss : 0.9303254902362823 TRAIN  loss dict:  {'classification_loss': 0.9303254902362823}
2025-01-17 01:48:29,684 [INFO] Step[550/2713]: training loss : 0.9305805253982544 TRAIN  loss dict:  {'classification_loss': 0.9305805253982544}
2025-01-17 01:48:45,672 [INFO] Step[600/2713]: training loss : 0.9290582394599914 TRAIN  loss dict:  {'classification_loss': 0.9290582394599914}
2025-01-17 01:49:01,763 [INFO] Step[650/2713]: training loss : 0.9306649267673492 TRAIN  loss dict:  {'classification_loss': 0.9306649267673492}
2025-01-17 01:49:17,865 [INFO] Step[700/2713]: training loss : 0.9327682828903199 TRAIN  loss dict:  {'classification_loss': 0.9327682828903199}
2025-01-17 01:49:34,000 [INFO] Step[750/2713]: training loss : 0.9336981010437012 TRAIN  loss dict:  {'classification_loss': 0.9336981010437012}
2025-01-17 01:49:50,070 [INFO] Step[800/2713]: training loss : 0.9371088325977326 TRAIN  loss dict:  {'classification_loss': 0.9371088325977326}
2025-01-17 01:50:06,102 [INFO] Step[850/2713]: training loss : 0.9308261692523956 TRAIN  loss dict:  {'classification_loss': 0.9308261692523956}
2025-01-17 01:50:22,182 [INFO] Step[900/2713]: training loss : 0.9318432354927063 TRAIN  loss dict:  {'classification_loss': 0.9318432354927063}
2025-01-17 01:50:38,299 [INFO] Step[950/2713]: training loss : 0.9297511732578277 TRAIN  loss dict:  {'classification_loss': 0.9297511732578277}
2025-01-17 01:50:54,368 [INFO] Step[1000/2713]: training loss : 0.9318991768360138 TRAIN  loss dict:  {'classification_loss': 0.9318991768360138}
2025-01-17 01:51:10,476 [INFO] Step[1050/2713]: training loss : 0.9292549955844879 TRAIN  loss dict:  {'classification_loss': 0.9292549955844879}
2025-01-17 01:51:26,569 [INFO] Step[1100/2713]: training loss : 0.9309860110282898 TRAIN  loss dict:  {'classification_loss': 0.9309860110282898}
2025-01-17 01:51:42,648 [INFO] Step[1150/2713]: training loss : 0.9912794184684753 TRAIN  loss dict:  {'classification_loss': 0.9912794184684753}
2025-01-17 01:51:58,704 [INFO] Step[1200/2713]: training loss : 0.928915992975235 TRAIN  loss dict:  {'classification_loss': 0.928915992975235}
2025-01-17 01:52:14,802 [INFO] Step[1250/2713]: training loss : 0.9293263792991638 TRAIN  loss dict:  {'classification_loss': 0.9293263792991638}
2025-01-17 01:52:30,899 [INFO] Step[1300/2713]: training loss : 0.9301178336143494 TRAIN  loss dict:  {'classification_loss': 0.9301178336143494}
2025-01-17 01:52:46,984 [INFO] Step[1350/2713]: training loss : 0.9291402232646943 TRAIN  loss dict:  {'classification_loss': 0.9291402232646943}
2025-01-17 01:53:03,055 [INFO] Step[1400/2713]: training loss : 0.929919023513794 TRAIN  loss dict:  {'classification_loss': 0.929919023513794}
2025-01-17 01:53:19,144 [INFO] Step[1450/2713]: training loss : 0.9295730328559876 TRAIN  loss dict:  {'classification_loss': 0.9295730328559876}
2025-01-17 01:53:35,293 [INFO] Step[1500/2713]: training loss : 0.9313335764408112 TRAIN  loss dict:  {'classification_loss': 0.9313335764408112}
2025-01-17 01:53:51,419 [INFO] Step[1550/2713]: training loss : 0.9290278172492981 TRAIN  loss dict:  {'classification_loss': 0.9290278172492981}
2025-01-17 01:54:07,394 [INFO] Step[1600/2713]: training loss : 0.9291238534450531 TRAIN  loss dict:  {'classification_loss': 0.9291238534450531}
2025-01-17 01:54:23,435 [INFO] Step[1650/2713]: training loss : 0.9291895258426667 TRAIN  loss dict:  {'classification_loss': 0.9291895258426667}
2025-01-17 01:54:39,460 [INFO] Step[1700/2713]: training loss : 0.9422868978977204 TRAIN  loss dict:  {'classification_loss': 0.9422868978977204}
2025-01-17 01:54:55,562 [INFO] Step[1750/2713]: training loss : 0.943720647096634 TRAIN  loss dict:  {'classification_loss': 0.943720647096634}
2025-01-17 01:55:11,582 [INFO] Step[1800/2713]: training loss : 0.9309945106506348 TRAIN  loss dict:  {'classification_loss': 0.9309945106506348}
2025-01-17 01:55:27,642 [INFO] Step[1850/2713]: training loss : 0.9294108724594117 TRAIN  loss dict:  {'classification_loss': 0.9294108724594117}
2025-01-17 01:55:43,726 [INFO] Step[1900/2713]: training loss : 0.9290040326118469 TRAIN  loss dict:  {'classification_loss': 0.9290040326118469}
2025-01-17 01:55:59,825 [INFO] Step[1950/2713]: training loss : 0.9439313542842865 TRAIN  loss dict:  {'classification_loss': 0.9439313542842865}
2025-01-17 01:56:15,828 [INFO] Step[2000/2713]: training loss : 0.9292525613307953 TRAIN  loss dict:  {'classification_loss': 0.9292525613307953}
2025-01-17 01:56:31,975 [INFO] Step[2050/2713]: training loss : 0.9457348573207855 TRAIN  loss dict:  {'classification_loss': 0.9457348573207855}
2025-01-17 01:56:48,052 [INFO] Step[2100/2713]: training loss : 0.9291608822345734 TRAIN  loss dict:  {'classification_loss': 0.9291608822345734}
2025-01-17 01:57:04,200 [INFO] Step[2150/2713]: training loss : 0.9288674414157867 TRAIN  loss dict:  {'classification_loss': 0.9288674414157867}
2025-01-17 01:57:20,288 [INFO] Step[2200/2713]: training loss : 0.9310374248027802 TRAIN  loss dict:  {'classification_loss': 0.9310374248027802}
2025-01-17 01:57:36,418 [INFO] Step[2250/2713]: training loss : 0.9330802166461944 TRAIN  loss dict:  {'classification_loss': 0.9330802166461944}
2025-01-17 01:57:52,548 [INFO] Step[2300/2713]: training loss : 0.9567733573913574 TRAIN  loss dict:  {'classification_loss': 0.9567733573913574}
2025-01-17 01:58:08,643 [INFO] Step[2350/2713]: training loss : 0.9688768875598908 TRAIN  loss dict:  {'classification_loss': 0.9688768875598908}
2025-01-17 01:58:24,697 [INFO] Step[2400/2713]: training loss : 0.9364929008483887 TRAIN  loss dict:  {'classification_loss': 0.9364929008483887}
2025-01-17 01:58:40,777 [INFO] Step[2450/2713]: training loss : 0.9314491081237793 TRAIN  loss dict:  {'classification_loss': 0.9314491081237793}
2025-01-17 01:58:56,895 [INFO] Step[2500/2713]: training loss : 0.9408771932125092 TRAIN  loss dict:  {'classification_loss': 0.9408771932125092}
2025-01-17 01:59:12,993 [INFO] Step[2550/2713]: training loss : 0.9394339919090271 TRAIN  loss dict:  {'classification_loss': 0.9394339919090271}
2025-01-17 01:59:29,048 [INFO] Step[2600/2713]: training loss : 0.9280963230133057 TRAIN  loss dict:  {'classification_loss': 0.9280963230133057}
2025-01-17 01:59:45,092 [INFO] Step[2650/2713]: training loss : 0.9284267771244049 TRAIN  loss dict:  {'classification_loss': 0.9284267771244049}
2025-01-17 02:00:01,172 [INFO] Step[2700/2713]: training loss : 0.951806823015213 TRAIN  loss dict:  {'classification_loss': 0.951806823015213}
2025-01-17 02:01:19,572 [INFO] Label accuracies statistics:
2025-01-17 02:01:19,573 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 0.5, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 0.75, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.5, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 0.75, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.25, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.5, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.5, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 02:01:19,574 [INFO] [59] TRAIN  loss: 0.935361959619624 acc: 0.997788426096572
2025-01-17 02:01:19,574 [INFO] [59] TRAIN  loss dict: {'classification_loss': 0.935361959619624}
2025-01-17 02:01:19,575 [INFO] [59] VALIDATION loss: 1.9278012464817305 VALIDATION acc: 0.7874608150470219
2025-01-17 02:01:19,575 [INFO] [59] VALIDATION loss dict: {'classification_loss': 1.9278012464817305}
2025-01-17 02:01:19,575 [INFO] 
2025-01-17 02:01:40,530 [INFO] Step[50/2713]: training loss : 0.9322611498832702 TRAIN  loss dict:  {'classification_loss': 0.9322611498832702}
2025-01-17 02:01:56,649 [INFO] Step[100/2713]: training loss : 0.9279234957695007 TRAIN  loss dict:  {'classification_loss': 0.9279234957695007}
2025-01-17 02:02:12,802 [INFO] Step[150/2713]: training loss : 0.928608775138855 TRAIN  loss dict:  {'classification_loss': 0.928608775138855}
2025-01-17 02:02:28,849 [INFO] Step[200/2713]: training loss : 0.9283166563510895 TRAIN  loss dict:  {'classification_loss': 0.9283166563510895}
2025-01-17 02:02:44,946 [INFO] Step[250/2713]: training loss : 0.9390270829200744 TRAIN  loss dict:  {'classification_loss': 0.9390270829200744}
2025-01-17 02:03:01,120 [INFO] Step[300/2713]: training loss : 0.9286223542690277 TRAIN  loss dict:  {'classification_loss': 0.9286223542690277}
2025-01-17 02:03:17,230 [INFO] Step[350/2713]: training loss : 0.9335709333419799 TRAIN  loss dict:  {'classification_loss': 0.9335709333419799}
2025-01-17 02:03:33,338 [INFO] Step[400/2713]: training loss : 0.9286265873908996 TRAIN  loss dict:  {'classification_loss': 0.9286265873908996}
2025-01-17 02:03:49,408 [INFO] Step[450/2713]: training loss : 0.9287712681293487 TRAIN  loss dict:  {'classification_loss': 0.9287712681293487}
2025-01-17 02:04:05,491 [INFO] Step[500/2713]: training loss : 0.929105840921402 TRAIN  loss dict:  {'classification_loss': 0.929105840921402}
2025-01-17 02:04:21,625 [INFO] Step[550/2713]: training loss : 0.9285666990280151 TRAIN  loss dict:  {'classification_loss': 0.9285666990280151}
2025-01-17 02:04:37,757 [INFO] Step[600/2713]: training loss : 0.929396104812622 TRAIN  loss dict:  {'classification_loss': 0.929396104812622}
2025-01-17 02:04:53,876 [INFO] Step[650/2713]: training loss : 0.92799729347229 TRAIN  loss dict:  {'classification_loss': 0.92799729347229}
2025-01-17 02:05:10,028 [INFO] Step[700/2713]: training loss : 0.9299472796916962 TRAIN  loss dict:  {'classification_loss': 0.9299472796916962}
2025-01-17 02:05:26,185 [INFO] Step[750/2713]: training loss : 0.9869065642356872 TRAIN  loss dict:  {'classification_loss': 0.9869065642356872}
2025-01-17 02:05:42,328 [INFO] Step[800/2713]: training loss : 0.9473809683322907 TRAIN  loss dict:  {'classification_loss': 0.9473809683322907}
2025-01-17 02:05:58,494 [INFO] Step[850/2713]: training loss : 0.9306369364261627 TRAIN  loss dict:  {'classification_loss': 0.9306369364261627}
2025-01-17 02:06:14,611 [INFO] Step[900/2713]: training loss : 0.9292652809619903 TRAIN  loss dict:  {'classification_loss': 0.9292652809619903}
2025-01-17 02:06:30,861 [INFO] Step[950/2713]: training loss : 0.929310474395752 TRAIN  loss dict:  {'classification_loss': 0.929310474395752}
2025-01-17 02:06:47,030 [INFO] Step[1000/2713]: training loss : 0.9352822053432465 TRAIN  loss dict:  {'classification_loss': 0.9352822053432465}
2025-01-17 02:07:03,284 [INFO] Step[1050/2713]: training loss : 0.9288303422927856 TRAIN  loss dict:  {'classification_loss': 0.9288303422927856}
2025-01-17 02:07:19,427 [INFO] Step[1100/2713]: training loss : 0.9283783137798309 TRAIN  loss dict:  {'classification_loss': 0.9283783137798309}
2025-01-17 02:07:35,797 [INFO] Step[1150/2713]: training loss : 0.9298471140861512 TRAIN  loss dict:  {'classification_loss': 0.9298471140861512}
2025-01-17 02:07:52,009 [INFO] Step[1200/2713]: training loss : 0.9281949627399445 TRAIN  loss dict:  {'classification_loss': 0.9281949627399445}
2025-01-17 02:08:08,225 [INFO] Step[1250/2713]: training loss : 0.9310959541797638 TRAIN  loss dict:  {'classification_loss': 0.9310959541797638}
2025-01-17 02:08:24,370 [INFO] Step[1300/2713]: training loss : 0.928335325717926 TRAIN  loss dict:  {'classification_loss': 0.928335325717926}
2025-01-17 02:08:40,597 [INFO] Step[1350/2713]: training loss : 0.9298336744308472 TRAIN  loss dict:  {'classification_loss': 0.9298336744308472}
2025-01-17 02:08:56,856 [INFO] Step[1400/2713]: training loss : 0.9284718334674835 TRAIN  loss dict:  {'classification_loss': 0.9284718334674835}
2025-01-17 02:09:13,086 [INFO] Step[1450/2713]: training loss : 0.9289410150051117 TRAIN  loss dict:  {'classification_loss': 0.9289410150051117}
2025-01-17 02:09:29,382 [INFO] Step[1500/2713]: training loss : 0.928731369972229 TRAIN  loss dict:  {'classification_loss': 0.928731369972229}
2025-01-17 02:09:45,624 [INFO] Step[1550/2713]: training loss : 0.9284881150722504 TRAIN  loss dict:  {'classification_loss': 0.9284881150722504}
2025-01-17 02:10:01,818 [INFO] Step[1600/2713]: training loss : 0.9304248881340027 TRAIN  loss dict:  {'classification_loss': 0.9304248881340027}
2025-01-17 02:10:18,063 [INFO] Step[1650/2713]: training loss : 0.9288173723220825 TRAIN  loss dict:  {'classification_loss': 0.9288173723220825}
2025-01-17 02:10:34,240 [INFO] Step[1700/2713]: training loss : 0.9297679889202118 TRAIN  loss dict:  {'classification_loss': 0.9297679889202118}
2025-01-17 02:10:50,446 [INFO] Step[1750/2713]: training loss : 0.9425999188423156 TRAIN  loss dict:  {'classification_loss': 0.9425999188423156}
2025-01-17 02:11:06,607 [INFO] Step[1800/2713]: training loss : 0.9290342271327973 TRAIN  loss dict:  {'classification_loss': 0.9290342271327973}
2025-01-17 02:11:22,809 [INFO] Step[1850/2713]: training loss : 0.9282445001602173 TRAIN  loss dict:  {'classification_loss': 0.9282445001602173}
2025-01-17 02:11:38,930 [INFO] Step[1900/2713]: training loss : 0.9286903750896454 TRAIN  loss dict:  {'classification_loss': 0.9286903750896454}
2025-01-17 02:11:55,192 [INFO] Step[1950/2713]: training loss : 0.9285864043235779 TRAIN  loss dict:  {'classification_loss': 0.9285864043235779}
2025-01-17 02:12:11,432 [INFO] Step[2000/2713]: training loss : 0.9282140576839447 TRAIN  loss dict:  {'classification_loss': 0.9282140576839447}
2025-01-17 02:12:27,543 [INFO] Step[2050/2713]: training loss : 0.9292988646030426 TRAIN  loss dict:  {'classification_loss': 0.9292988646030426}
2025-01-17 02:12:43,657 [INFO] Step[2100/2713]: training loss : 0.9310392558574676 TRAIN  loss dict:  {'classification_loss': 0.9310392558574676}
2025-01-17 02:12:59,826 [INFO] Step[2150/2713]: training loss : 0.9285450947284698 TRAIN  loss dict:  {'classification_loss': 0.9285450947284698}
2025-01-17 02:13:15,950 [INFO] Step[2200/2713]: training loss : 0.9306947219371796 TRAIN  loss dict:  {'classification_loss': 0.9306947219371796}
2025-01-17 02:13:32,143 [INFO] Step[2250/2713]: training loss : 0.9322941029071807 TRAIN  loss dict:  {'classification_loss': 0.9322941029071807}
2025-01-17 02:13:48,341 [INFO] Step[2300/2713]: training loss : 0.9555820310115815 TRAIN  loss dict:  {'classification_loss': 0.9555820310115815}
2025-01-17 02:14:04,586 [INFO] Step[2350/2713]: training loss : 0.9282854354381561 TRAIN  loss dict:  {'classification_loss': 0.9282854354381561}
2025-01-17 02:14:20,688 [INFO] Step[2400/2713]: training loss : 0.9291503322124481 TRAIN  loss dict:  {'classification_loss': 0.9291503322124481}
2025-01-17 02:14:36,912 [INFO] Step[2450/2713]: training loss : 0.929092526435852 TRAIN  loss dict:  {'classification_loss': 0.929092526435852}
2025-01-17 02:14:53,104 [INFO] Step[2500/2713]: training loss : 0.930730037689209 TRAIN  loss dict:  {'classification_loss': 0.930730037689209}
2025-01-17 02:15:09,289 [INFO] Step[2550/2713]: training loss : 0.9291957223415375 TRAIN  loss dict:  {'classification_loss': 0.9291957223415375}
2025-01-17 02:15:25,434 [INFO] Step[2600/2713]: training loss : 0.9319916617870331 TRAIN  loss dict:  {'classification_loss': 0.9319916617870331}
2025-01-17 02:15:41,559 [INFO] Step[2650/2713]: training loss : 0.9308530485630035 TRAIN  loss dict:  {'classification_loss': 0.9308530485630035}
2025-01-17 02:15:57,753 [INFO] Step[2700/2713]: training loss : 0.9294416570663452 TRAIN  loss dict:  {'classification_loss': 0.9294416570663452}
2025-01-17 02:17:15,081 [INFO] Label accuracies statistics:
2025-01-17 02:17:15,081 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.5, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.25, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.0, 143: 0.5, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.5, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.25, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.75, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 1.0, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.25, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 1.0, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 02:17:15,083 [INFO] [60] TRAIN  loss: 0.9318214923176342 acc: 0.9992628086988573
2025-01-17 02:17:15,083 [INFO] [60] TRAIN  loss dict: {'classification_loss': 0.9318214923176342}
2025-01-17 02:17:15,083 [INFO] [60] VALIDATION loss: 1.8921381221678024 VALIDATION acc: 0.7949843260188088
2025-01-17 02:17:15,083 [INFO] [60] VALIDATION loss dict: {'classification_loss': 1.8921381221678024}
2025-01-17 02:17:15,083 [INFO] 
2025-01-17 02:17:35,893 [INFO] Step[50/2713]: training loss : 0.9324842059612274 TRAIN  loss dict:  {'classification_loss': 0.9324842059612274}
2025-01-17 02:17:51,987 [INFO] Step[100/2713]: training loss : 0.9284808099269867 TRAIN  loss dict:  {'classification_loss': 0.9284808099269867}
2025-01-17 02:18:08,182 [INFO] Step[150/2713]: training loss : 1.0180575025081635 TRAIN  loss dict:  {'classification_loss': 1.0180575025081635}
2025-01-17 02:18:24,445 [INFO] Step[200/2713]: training loss : 0.9290748167037964 TRAIN  loss dict:  {'classification_loss': 0.9290748167037964}
2025-01-17 02:18:40,667 [INFO] Step[250/2713]: training loss : 0.9414735412597657 TRAIN  loss dict:  {'classification_loss': 0.9414735412597657}
2025-01-17 02:18:56,866 [INFO] Step[300/2713]: training loss : 0.933388637304306 TRAIN  loss dict:  {'classification_loss': 0.933388637304306}
2025-01-17 02:19:13,087 [INFO] Step[350/2713]: training loss : 0.9460366952419281 TRAIN  loss dict:  {'classification_loss': 0.9460366952419281}
2025-01-17 02:19:29,274 [INFO] Step[400/2713]: training loss : 0.9306912314891815 TRAIN  loss dict:  {'classification_loss': 0.9306912314891815}
2025-01-17 02:19:45,516 [INFO] Step[450/2713]: training loss : 0.92845432639122 TRAIN  loss dict:  {'classification_loss': 0.92845432639122}
2025-01-17 02:20:01,688 [INFO] Step[500/2713]: training loss : 0.9300197958946228 TRAIN  loss dict:  {'classification_loss': 0.9300197958946228}
2025-01-17 02:20:17,959 [INFO] Step[550/2713]: training loss : 0.9279970514774323 TRAIN  loss dict:  {'classification_loss': 0.9279970514774323}
2025-01-17 02:20:34,098 [INFO] Step[600/2713]: training loss : 0.9461263751983643 TRAIN  loss dict:  {'classification_loss': 0.9461263751983643}
2025-01-17 02:20:50,158 [INFO] Step[650/2713]: training loss : 0.9299518740177155 TRAIN  loss dict:  {'classification_loss': 0.9299518740177155}
2025-01-17 02:21:06,141 [INFO] Step[700/2713]: training loss : 0.9330067276954651 TRAIN  loss dict:  {'classification_loss': 0.9330067276954651}
2025-01-17 02:21:22,213 [INFO] Step[750/2713]: training loss : 0.9278666520118714 TRAIN  loss dict:  {'classification_loss': 0.9278666520118714}
2025-01-17 02:21:38,194 [INFO] Step[800/2713]: training loss : 0.9286196684837341 TRAIN  loss dict:  {'classification_loss': 0.9286196684837341}
2025-01-17 02:21:54,343 [INFO] Step[850/2713]: training loss : 0.9283693897724151 TRAIN  loss dict:  {'classification_loss': 0.9283693897724151}
2025-01-17 02:22:10,374 [INFO] Step[900/2713]: training loss : 0.9289803433418274 TRAIN  loss dict:  {'classification_loss': 0.9289803433418274}
2025-01-17 02:22:26,414 [INFO] Step[950/2713]: training loss : 0.9279682278633118 TRAIN  loss dict:  {'classification_loss': 0.9279682278633118}
2025-01-17 02:22:42,457 [INFO] Step[1000/2713]: training loss : 0.928980028629303 TRAIN  loss dict:  {'classification_loss': 0.928980028629303}
2025-01-17 02:22:58,603 [INFO] Step[1050/2713]: training loss : 0.9290173602104187 TRAIN  loss dict:  {'classification_loss': 0.9290173602104187}
2025-01-17 02:23:14,706 [INFO] Step[1100/2713]: training loss : 0.9315984463691711 TRAIN  loss dict:  {'classification_loss': 0.9315984463691711}
2025-01-17 02:23:30,754 [INFO] Step[1150/2713]: training loss : 0.9276773023605347 TRAIN  loss dict:  {'classification_loss': 0.9276773023605347}
2025-01-17 02:23:46,802 [INFO] Step[1200/2713]: training loss : 0.9296986651420593 TRAIN  loss dict:  {'classification_loss': 0.9296986651420593}
2025-01-17 02:24:02,867 [INFO] Step[1250/2713]: training loss : 0.9273107135295868 TRAIN  loss dict:  {'classification_loss': 0.9273107135295868}
2025-01-17 02:24:18,924 [INFO] Step[1300/2713]: training loss : 0.9277484261989594 TRAIN  loss dict:  {'classification_loss': 0.9277484261989594}
2025-01-17 02:24:35,081 [INFO] Step[1350/2713]: training loss : 0.9339611232280731 TRAIN  loss dict:  {'classification_loss': 0.9339611232280731}
2025-01-17 02:24:51,192 [INFO] Step[1400/2713]: training loss : 0.9292611086368561 TRAIN  loss dict:  {'classification_loss': 0.9292611086368561}
2025-01-17 02:25:07,245 [INFO] Step[1450/2713]: training loss : 0.9285397779941559 TRAIN  loss dict:  {'classification_loss': 0.9285397779941559}
2025-01-17 02:25:23,292 [INFO] Step[1500/2713]: training loss : 0.9329058468341828 TRAIN  loss dict:  {'classification_loss': 0.9329058468341828}
2025-01-17 02:25:39,358 [INFO] Step[1550/2713]: training loss : 0.9410177040100097 TRAIN  loss dict:  {'classification_loss': 0.9410177040100097}
2025-01-17 02:25:55,411 [INFO] Step[1600/2713]: training loss : 0.9292099368572235 TRAIN  loss dict:  {'classification_loss': 0.9292099368572235}
2025-01-17 02:26:11,460 [INFO] Step[1650/2713]: training loss : 0.9285770869255066 TRAIN  loss dict:  {'classification_loss': 0.9285770869255066}
2025-01-17 02:26:27,525 [INFO] Step[1700/2713]: training loss : 0.9364294195175171 TRAIN  loss dict:  {'classification_loss': 0.9364294195175171}
2025-01-17 02:26:43,529 [INFO] Step[1750/2713]: training loss : 0.9279297244548798 TRAIN  loss dict:  {'classification_loss': 0.9279297244548798}
2025-01-17 02:26:59,592 [INFO] Step[1800/2713]: training loss : 0.9279873490333557 TRAIN  loss dict:  {'classification_loss': 0.9279873490333557}
2025-01-17 02:27:15,634 [INFO] Step[1850/2713]: training loss : 0.9298467755317688 TRAIN  loss dict:  {'classification_loss': 0.9298467755317688}
2025-01-17 02:27:31,661 [INFO] Step[1900/2713]: training loss : 0.9328011512756348 TRAIN  loss dict:  {'classification_loss': 0.9328011512756348}
2025-01-17 02:27:47,713 [INFO] Step[1950/2713]: training loss : 0.9286574280261993 TRAIN  loss dict:  {'classification_loss': 0.9286574280261993}
2025-01-17 02:28:03,812 [INFO] Step[2000/2713]: training loss : 0.9317638337612152 TRAIN  loss dict:  {'classification_loss': 0.9317638337612152}
2025-01-17 02:28:19,896 [INFO] Step[2050/2713]: training loss : 0.9288645505905151 TRAIN  loss dict:  {'classification_loss': 0.9288645505905151}
2025-01-17 02:28:35,978 [INFO] Step[2100/2713]: training loss : 0.958578292131424 TRAIN  loss dict:  {'classification_loss': 0.958578292131424}
2025-01-17 02:28:52,109 [INFO] Step[2150/2713]: training loss : 0.9292538928985595 TRAIN  loss dict:  {'classification_loss': 0.9292538928985595}
2025-01-17 02:29:08,137 [INFO] Step[2200/2713]: training loss : 0.9278835678100585 TRAIN  loss dict:  {'classification_loss': 0.9278835678100585}
2025-01-17 02:29:24,266 [INFO] Step[2250/2713]: training loss : 0.9295307946205139 TRAIN  loss dict:  {'classification_loss': 0.9295307946205139}
2025-01-17 02:29:40,332 [INFO] Step[2300/2713]: training loss : 0.928626400232315 TRAIN  loss dict:  {'classification_loss': 0.928626400232315}
2025-01-17 02:29:56,352 [INFO] Step[2350/2713]: training loss : 0.9299398505687714 TRAIN  loss dict:  {'classification_loss': 0.9299398505687714}
2025-01-17 02:30:12,475 [INFO] Step[2400/2713]: training loss : 0.9281224513053894 TRAIN  loss dict:  {'classification_loss': 0.9281224513053894}
2025-01-17 02:30:28,681 [INFO] Step[2450/2713]: training loss : 0.9601371610164642 TRAIN  loss dict:  {'classification_loss': 0.9601371610164642}
2025-01-17 02:30:44,944 [INFO] Step[2500/2713]: training loss : 0.9296118724346161 TRAIN  loss dict:  {'classification_loss': 0.9296118724346161}
2025-01-17 02:31:01,170 [INFO] Step[2550/2713]: training loss : 0.9539044618606567 TRAIN  loss dict:  {'classification_loss': 0.9539044618606567}
2025-01-17 02:31:17,332 [INFO] Step[2600/2713]: training loss : 0.9277843248844146 TRAIN  loss dict:  {'classification_loss': 0.9277843248844146}
2025-01-17 02:31:33,494 [INFO] Step[2650/2713]: training loss : 0.9301059889793396 TRAIN  loss dict:  {'classification_loss': 0.9301059889793396}
2025-01-17 02:31:49,677 [INFO] Step[2700/2713]: training loss : 0.9279145097732544 TRAIN  loss dict:  {'classification_loss': 0.9279145097732544}
2025-01-17 02:33:07,580 [INFO] Label accuracies statistics:
2025-01-17 02:33:07,580 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.25, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.75, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.5, 217: 1.0, 218: 1.0, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.5, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 1.0, 260: 1.0, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 02:33:08,913 [INFO] [61] TRAIN  loss: 0.9338237906882036 acc: 0.9982798869640005
2025-01-17 02:33:08,913 [INFO] [61] TRAIN  loss dict: {'classification_loss': 0.9338237906882036}
2025-01-17 02:33:08,913 [INFO] [61] VALIDATION loss: 1.8549847629733551 VALIDATION acc: 0.8031347962382445
2025-01-17 02:33:08,913 [INFO] [61] VALIDATION loss dict: {'classification_loss': 1.8549847629733551}
2025-01-17 02:33:08,913 [INFO] 
2025-01-17 02:33:29,613 [INFO] Step[50/2713]: training loss : 0.9288697671890259 TRAIN  loss dict:  {'classification_loss': 0.9288697671890259}
2025-01-17 02:33:45,765 [INFO] Step[100/2713]: training loss : 0.9275970697402954 TRAIN  loss dict:  {'classification_loss': 0.9275970697402954}
2025-01-17 02:34:01,981 [INFO] Step[150/2713]: training loss : 0.9281381499767304 TRAIN  loss dict:  {'classification_loss': 0.9281381499767304}
2025-01-17 02:34:18,139 [INFO] Step[200/2713]: training loss : 0.9536548674106597 TRAIN  loss dict:  {'classification_loss': 0.9536548674106597}
2025-01-17 02:34:34,412 [INFO] Step[250/2713]: training loss : 0.9297323620319367 TRAIN  loss dict:  {'classification_loss': 0.9297323620319367}
2025-01-17 02:34:50,668 [INFO] Step[300/2713]: training loss : 0.9304647886753082 TRAIN  loss dict:  {'classification_loss': 0.9304647886753082}
2025-01-17 02:35:06,835 [INFO] Step[350/2713]: training loss : 0.928738499879837 TRAIN  loss dict:  {'classification_loss': 0.928738499879837}
2025-01-17 02:35:22,994 [INFO] Step[400/2713]: training loss : 0.9277831447124482 TRAIN  loss dict:  {'classification_loss': 0.9277831447124482}
2025-01-17 02:35:39,239 [INFO] Step[450/2713]: training loss : 0.9278062736988067 TRAIN  loss dict:  {'classification_loss': 0.9278062736988067}
2025-01-17 02:35:55,440 [INFO] Step[500/2713]: training loss : 0.9372495234012603 TRAIN  loss dict:  {'classification_loss': 0.9372495234012603}
2025-01-17 02:36:11,590 [INFO] Step[550/2713]: training loss : 0.9280360770225525 TRAIN  loss dict:  {'classification_loss': 0.9280360770225525}
2025-01-17 02:36:27,551 [INFO] Step[600/2713]: training loss : 0.9279788386821747 TRAIN  loss dict:  {'classification_loss': 0.9279788386821747}
2025-01-17 02:36:43,671 [INFO] Step[650/2713]: training loss : 0.9275322234630585 TRAIN  loss dict:  {'classification_loss': 0.9275322234630585}
2025-01-17 02:36:59,626 [INFO] Step[700/2713]: training loss : 0.9292917180061341 TRAIN  loss dict:  {'classification_loss': 0.9292917180061341}
2025-01-17 02:37:15,734 [INFO] Step[750/2713]: training loss : 0.933204402923584 TRAIN  loss dict:  {'classification_loss': 0.933204402923584}
2025-01-17 02:37:31,732 [INFO] Step[800/2713]: training loss : 0.9428217160701752 TRAIN  loss dict:  {'classification_loss': 0.9428217160701752}
2025-01-17 02:37:47,786 [INFO] Step[850/2713]: training loss : 0.928115713596344 TRAIN  loss dict:  {'classification_loss': 0.928115713596344}
2025-01-17 02:38:03,828 [INFO] Step[900/2713]: training loss : 0.9623610639572143 TRAIN  loss dict:  {'classification_loss': 0.9623610639572143}
2025-01-17 02:38:19,867 [INFO] Step[950/2713]: training loss : 0.9269326519966126 TRAIN  loss dict:  {'classification_loss': 0.9269326519966126}
2025-01-17 02:38:35,893 [INFO] Step[1000/2713]: training loss : 0.9279009437561035 TRAIN  loss dict:  {'classification_loss': 0.9279009437561035}
2025-01-17 02:38:51,953 [INFO] Step[1050/2713]: training loss : 0.927530323266983 TRAIN  loss dict:  {'classification_loss': 0.927530323266983}
2025-01-17 02:39:07,886 [INFO] Step[1100/2713]: training loss : 0.954586215019226 TRAIN  loss dict:  {'classification_loss': 0.954586215019226}
2025-01-17 02:39:23,889 [INFO] Step[1150/2713]: training loss : 0.9273430824279785 TRAIN  loss dict:  {'classification_loss': 0.9273430824279785}
2025-01-17 02:39:39,860 [INFO] Step[1200/2713]: training loss : 0.9327751588821411 TRAIN  loss dict:  {'classification_loss': 0.9327751588821411}
2025-01-17 02:39:55,916 [INFO] Step[1250/2713]: training loss : 0.9279729640483856 TRAIN  loss dict:  {'classification_loss': 0.9279729640483856}
2025-01-17 02:40:11,933 [INFO] Step[1300/2713]: training loss : 0.9569885265827179 TRAIN  loss dict:  {'classification_loss': 0.9569885265827179}
2025-01-17 02:40:27,962 [INFO] Step[1350/2713]: training loss : 0.9280821704864501 TRAIN  loss dict:  {'classification_loss': 0.9280821704864501}
2025-01-17 02:40:43,974 [INFO] Step[1400/2713]: training loss : 0.9283768582344055 TRAIN  loss dict:  {'classification_loss': 0.9283768582344055}
2025-01-17 02:40:59,995 [INFO] Step[1450/2713]: training loss : 0.9276111686229705 TRAIN  loss dict:  {'classification_loss': 0.9276111686229705}
2025-01-17 02:41:16,013 [INFO] Step[1500/2713]: training loss : 0.9286514246463775 TRAIN  loss dict:  {'classification_loss': 0.9286514246463775}
2025-01-17 02:41:31,970 [INFO] Step[1550/2713]: training loss : 0.9274703800678253 TRAIN  loss dict:  {'classification_loss': 0.9274703800678253}
2025-01-17 02:41:48,005 [INFO] Step[1600/2713]: training loss : 0.9382741987705231 TRAIN  loss dict:  {'classification_loss': 0.9382741987705231}
2025-01-17 02:42:04,048 [INFO] Step[1650/2713]: training loss : 0.9282723927497863 TRAIN  loss dict:  {'classification_loss': 0.9282723927497863}
2025-01-17 02:42:20,095 [INFO] Step[1700/2713]: training loss : 0.9273078966140748 TRAIN  loss dict:  {'classification_loss': 0.9273078966140748}
2025-01-17 02:42:36,131 [INFO] Step[1750/2713]: training loss : 0.9278022837638855 TRAIN  loss dict:  {'classification_loss': 0.9278022837638855}
2025-01-17 02:42:52,174 [INFO] Step[1800/2713]: training loss : 0.9293733310699462 TRAIN  loss dict:  {'classification_loss': 0.9293733310699462}
2025-01-17 02:43:08,289 [INFO] Step[1850/2713]: training loss : 0.9284648883342743 TRAIN  loss dict:  {'classification_loss': 0.9284648883342743}
2025-01-17 02:43:24,319 [INFO] Step[1900/2713]: training loss : 0.928869172334671 TRAIN  loss dict:  {'classification_loss': 0.928869172334671}
2025-01-17 02:43:40,406 [INFO] Step[1950/2713]: training loss : 0.9296038210391998 TRAIN  loss dict:  {'classification_loss': 0.9296038210391998}
2025-01-17 02:43:56,398 [INFO] Step[2000/2713]: training loss : 0.9304417073726654 TRAIN  loss dict:  {'classification_loss': 0.9304417073726654}
2025-01-17 02:44:12,436 [INFO] Step[2050/2713]: training loss : 0.9331205415725708 TRAIN  loss dict:  {'classification_loss': 0.9331205415725708}
2025-01-17 02:44:28,419 [INFO] Step[2100/2713]: training loss : 0.9500861632823944 TRAIN  loss dict:  {'classification_loss': 0.9500861632823944}
2025-01-17 02:44:44,465 [INFO] Step[2150/2713]: training loss : 0.9572565245628357 TRAIN  loss dict:  {'classification_loss': 0.9572565245628357}
2025-01-17 02:45:00,476 [INFO] Step[2200/2713]: training loss : 0.9320889770984649 TRAIN  loss dict:  {'classification_loss': 0.9320889770984649}
2025-01-17 02:45:16,521 [INFO] Step[2250/2713]: training loss : 0.9297778367996216 TRAIN  loss dict:  {'classification_loss': 0.9297778367996216}
2025-01-17 02:45:32,518 [INFO] Step[2300/2713]: training loss : 0.929404730796814 TRAIN  loss dict:  {'classification_loss': 0.929404730796814}
2025-01-17 02:45:48,560 [INFO] Step[2350/2713]: training loss : 0.9293820714950561 TRAIN  loss dict:  {'classification_loss': 0.9293820714950561}
2025-01-17 02:46:04,545 [INFO] Step[2400/2713]: training loss : 0.9286662113666534 TRAIN  loss dict:  {'classification_loss': 0.9286662113666534}
2025-01-17 02:46:20,566 [INFO] Step[2450/2713]: training loss : 0.9281912064552307 TRAIN  loss dict:  {'classification_loss': 0.9281912064552307}
2025-01-17 02:46:36,627 [INFO] Step[2500/2713]: training loss : 0.927638887166977 TRAIN  loss dict:  {'classification_loss': 0.927638887166977}
2025-01-17 02:46:52,690 [INFO] Step[2550/2713]: training loss : 0.9275542557239532 TRAIN  loss dict:  {'classification_loss': 0.9275542557239532}
2025-01-17 02:47:08,707 [INFO] Step[2600/2713]: training loss : 0.929488183259964 TRAIN  loss dict:  {'classification_loss': 0.929488183259964}
2025-01-17 02:47:24,740 [INFO] Step[2650/2713]: training loss : 0.9285452818870544 TRAIN  loss dict:  {'classification_loss': 0.9285452818870544}
2025-01-17 02:47:40,754 [INFO] Step[2700/2713]: training loss : 0.928343094587326 TRAIN  loss dict:  {'classification_loss': 0.928343094587326}
2025-01-17 02:48:58,454 [INFO] Label accuracies statistics:
2025-01-17 02:48:58,455 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.5, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 1.0, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.25, 59: 0.75, 60: 0.25, 61: 0.5, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.5, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 1.0, 261: 0.75, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 1.0, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.5, 289: 1.0, 290: 0.25, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 1.0, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 1.0, 356: 1.0, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 02:48:59,844 [INFO] [62] TRAIN  loss: 0.9323845432334573 acc: 0.9982798869640005
2025-01-17 02:48:59,845 [INFO] [62] TRAIN  loss dict: {'classification_loss': 0.9323845432334573}
2025-01-17 02:48:59,845 [INFO] [62] VALIDATION loss: 1.890355672603263 VALIDATION acc: 0.806269592476489
2025-01-17 02:48:59,845 [INFO] [62] VALIDATION loss dict: {'classification_loss': 1.890355672603263}
2025-01-17 02:48:59,845 [INFO] 
2025-01-17 02:49:21,172 [INFO] Step[50/2713]: training loss : 0.927752822637558 TRAIN  loss dict:  {'classification_loss': 0.927752822637558}
2025-01-17 02:49:37,081 [INFO] Step[100/2713]: training loss : 0.9271181356906891 TRAIN  loss dict:  {'classification_loss': 0.9271181356906891}
2025-01-17 02:49:53,035 [INFO] Step[150/2713]: training loss : 0.929217575788498 TRAIN  loss dict:  {'classification_loss': 0.929217575788498}
2025-01-17 02:50:09,044 [INFO] Step[200/2713]: training loss : 0.9281900429725647 TRAIN  loss dict:  {'classification_loss': 0.9281900429725647}
2025-01-17 02:50:25,138 [INFO] Step[250/2713]: training loss : 0.9281831812858582 TRAIN  loss dict:  {'classification_loss': 0.9281831812858582}
2025-01-17 02:50:41,142 [INFO] Step[300/2713]: training loss : 0.930478812456131 TRAIN  loss dict:  {'classification_loss': 0.930478812456131}
2025-01-17 02:50:57,241 [INFO] Step[350/2713]: training loss : 0.9277968609333038 TRAIN  loss dict:  {'classification_loss': 0.9277968609333038}
2025-01-17 02:51:13,323 [INFO] Step[400/2713]: training loss : 0.9274529707431793 TRAIN  loss dict:  {'classification_loss': 0.9274529707431793}
2025-01-17 02:51:29,377 [INFO] Step[450/2713]: training loss : 0.9277169167995453 TRAIN  loss dict:  {'classification_loss': 0.9277169167995453}
2025-01-17 02:51:45,444 [INFO] Step[500/2713]: training loss : 0.9285816013813019 TRAIN  loss dict:  {'classification_loss': 0.9285816013813019}
2025-01-17 02:52:01,482 [INFO] Step[550/2713]: training loss : 0.9276921463012695 TRAIN  loss dict:  {'classification_loss': 0.9276921463012695}
2025-01-17 02:52:17,570 [INFO] Step[600/2713]: training loss : 0.9286728489398957 TRAIN  loss dict:  {'classification_loss': 0.9286728489398957}
2025-01-17 02:52:33,928 [INFO] Step[650/2713]: training loss : 0.9485470032691956 TRAIN  loss dict:  {'classification_loss': 0.9485470032691956}
2025-01-17 02:52:50,146 [INFO] Step[700/2713]: training loss : 0.9276592254638671 TRAIN  loss dict:  {'classification_loss': 0.9276592254638671}
2025-01-17 02:53:06,543 [INFO] Step[750/2713]: training loss : 0.9278381836414337 TRAIN  loss dict:  {'classification_loss': 0.9278381836414337}
2025-01-17 02:53:22,919 [INFO] Step[800/2713]: training loss : 0.9292662799358368 TRAIN  loss dict:  {'classification_loss': 0.9292662799358368}
2025-01-17 02:53:39,261 [INFO] Step[850/2713]: training loss : 0.9459094941616059 TRAIN  loss dict:  {'classification_loss': 0.9459094941616059}
2025-01-17 02:53:55,628 [INFO] Step[900/2713]: training loss : 0.9290178573131561 TRAIN  loss dict:  {'classification_loss': 0.9290178573131561}
2025-01-17 02:54:12,029 [INFO] Step[950/2713]: training loss : 0.9321622288227082 TRAIN  loss dict:  {'classification_loss': 0.9321622288227082}
2025-01-17 02:54:28,236 [INFO] Step[1000/2713]: training loss : 0.9380000913143158 TRAIN  loss dict:  {'classification_loss': 0.9380000913143158}
2025-01-17 02:54:44,552 [INFO] Step[1050/2713]: training loss : 0.92928466796875 TRAIN  loss dict:  {'classification_loss': 0.92928466796875}
2025-01-17 02:55:00,742 [INFO] Step[1100/2713]: training loss : 0.9283593142032623 TRAIN  loss dict:  {'classification_loss': 0.9283593142032623}
2025-01-17 02:55:17,051 [INFO] Step[1150/2713]: training loss : 0.9286853444576263 TRAIN  loss dict:  {'classification_loss': 0.9286853444576263}
2025-01-17 02:55:33,407 [INFO] Step[1200/2713]: training loss : 0.928597012758255 TRAIN  loss dict:  {'classification_loss': 0.928597012758255}
2025-01-17 02:55:49,703 [INFO] Step[1250/2713]: training loss : 0.9308153176307679 TRAIN  loss dict:  {'classification_loss': 0.9308153176307679}
2025-01-17 02:56:05,969 [INFO] Step[1300/2713]: training loss : 0.9287247622013092 TRAIN  loss dict:  {'classification_loss': 0.9287247622013092}
2025-01-17 02:56:22,301 [INFO] Step[1350/2713]: training loss : 0.928779525756836 TRAIN  loss dict:  {'classification_loss': 0.928779525756836}
2025-01-17 02:56:38,618 [INFO] Step[1400/2713]: training loss : 0.9295088255405426 TRAIN  loss dict:  {'classification_loss': 0.9295088255405426}
2025-01-17 02:56:54,923 [INFO] Step[1450/2713]: training loss : 0.928017704486847 TRAIN  loss dict:  {'classification_loss': 0.928017704486847}
2025-01-17 02:57:11,235 [INFO] Step[1500/2713]: training loss : 0.9276783609390259 TRAIN  loss dict:  {'classification_loss': 0.9276783609390259}
2025-01-17 02:57:27,569 [INFO] Step[1550/2713]: training loss : 0.9275910651683807 TRAIN  loss dict:  {'classification_loss': 0.9275910651683807}
2025-01-17 02:57:43,820 [INFO] Step[1600/2713]: training loss : 0.9274088275432587 TRAIN  loss dict:  {'classification_loss': 0.9274088275432587}
2025-01-17 02:58:00,204 [INFO] Step[1650/2713]: training loss : 0.9297004556655883 TRAIN  loss dict:  {'classification_loss': 0.9297004556655883}
2025-01-17 02:58:16,455 [INFO] Step[1700/2713]: training loss : 0.9279334020614624 TRAIN  loss dict:  {'classification_loss': 0.9279334020614624}
2025-01-17 02:58:32,826 [INFO] Step[1750/2713]: training loss : 0.9273103237152099 TRAIN  loss dict:  {'classification_loss': 0.9273103237152099}
2025-01-17 02:58:49,182 [INFO] Step[1800/2713]: training loss : 0.9272688925266266 TRAIN  loss dict:  {'classification_loss': 0.9272688925266266}
2025-01-17 02:59:05,396 [INFO] Step[1850/2713]: training loss : 0.9277130794525147 TRAIN  loss dict:  {'classification_loss': 0.9277130794525147}
2025-01-17 02:59:21,684 [INFO] Step[1900/2713]: training loss : 0.9283950579166412 TRAIN  loss dict:  {'classification_loss': 0.9283950579166412}
2025-01-17 02:59:37,988 [INFO] Step[1950/2713]: training loss : 0.9284625029563904 TRAIN  loss dict:  {'classification_loss': 0.9284625029563904}
2025-01-17 02:59:54,226 [INFO] Step[2000/2713]: training loss : 0.9275294291973114 TRAIN  loss dict:  {'classification_loss': 0.9275294291973114}
2025-01-17 03:00:10,535 [INFO] Step[2050/2713]: training loss : 0.9433324098587036 TRAIN  loss dict:  {'classification_loss': 0.9433324098587036}
2025-01-17 03:00:26,816 [INFO] Step[2100/2713]: training loss : 0.9278041088581085 TRAIN  loss dict:  {'classification_loss': 0.9278041088581085}
2025-01-17 03:00:43,098 [INFO] Step[2150/2713]: training loss : 0.9339538955688477 TRAIN  loss dict:  {'classification_loss': 0.9339538955688477}
2025-01-17 03:00:59,337 [INFO] Step[2200/2713]: training loss : 0.9286782026290894 TRAIN  loss dict:  {'classification_loss': 0.9286782026290894}
2025-01-17 03:01:15,681 [INFO] Step[2250/2713]: training loss : 0.9333249735832214 TRAIN  loss dict:  {'classification_loss': 0.9333249735832214}
2025-01-17 03:01:31,923 [INFO] Step[2300/2713]: training loss : 0.9282508254051208 TRAIN  loss dict:  {'classification_loss': 0.9282508254051208}
2025-01-17 03:01:48,290 [INFO] Step[2350/2713]: training loss : 0.9279667460918426 TRAIN  loss dict:  {'classification_loss': 0.9279667460918426}
2025-01-17 03:02:04,630 [INFO] Step[2400/2713]: training loss : 0.9284998786449432 TRAIN  loss dict:  {'classification_loss': 0.9284998786449432}
2025-01-17 03:02:20,979 [INFO] Step[2450/2713]: training loss : 0.9288449835777283 TRAIN  loss dict:  {'classification_loss': 0.9288449835777283}
2025-01-17 03:02:37,277 [INFO] Step[2500/2713]: training loss : 0.9286890757083893 TRAIN  loss dict:  {'classification_loss': 0.9286890757083893}
2025-01-17 03:02:53,609 [INFO] Step[2550/2713]: training loss : 0.9287685036659241 TRAIN  loss dict:  {'classification_loss': 0.9287685036659241}
2025-01-17 03:03:09,888 [INFO] Step[2600/2713]: training loss : 0.956641401052475 TRAIN  loss dict:  {'classification_loss': 0.956641401052475}
2025-01-17 03:03:26,294 [INFO] Step[2650/2713]: training loss : 0.9418577349185944 TRAIN  loss dict:  {'classification_loss': 0.9418577349185944}
2025-01-17 03:03:42,549 [INFO] Step[2700/2713]: training loss : 0.9368472981452942 TRAIN  loss dict:  {'classification_loss': 0.9368472981452942}
2025-01-17 03:05:00,513 [INFO] Label accuracies statistics:
2025-01-17 03:05:00,513 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.25, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.5, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 1.0, 226: 1.0, 227: 0.5, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.25, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.5, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.25, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 1.0, 352: 0.5, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.5, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 0.75, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 03:05:00,514 [INFO] [63] TRAIN  loss: 0.9306964675438962 acc: 0.9991399434820002
2025-01-17 03:05:00,515 [INFO] [63] TRAIN  loss dict: {'classification_loss': 0.9306964675438962}
2025-01-17 03:05:00,515 [INFO] [63] VALIDATION loss: 1.8926960674667717 VALIDATION acc: 0.7943573667711599
2025-01-17 03:05:00,515 [INFO] [63] VALIDATION loss dict: {'classification_loss': 1.8926960674667717}
2025-01-17 03:05:00,515 [INFO] 
2025-01-17 03:05:26,198 [INFO] Step[50/2713]: training loss : 0.931214201450348 TRAIN  loss dict:  {'classification_loss': 0.931214201450348}
2025-01-17 03:05:42,310 [INFO] Step[100/2713]: training loss : 0.9282348024845123 TRAIN  loss dict:  {'classification_loss': 0.9282348024845123}
2025-01-17 03:05:58,498 [INFO] Step[150/2713]: training loss : 0.9313028454780579 TRAIN  loss dict:  {'classification_loss': 0.9313028454780579}
2025-01-17 03:06:14,783 [INFO] Step[200/2713]: training loss : 0.9286275434494019 TRAIN  loss dict:  {'classification_loss': 0.9286275434494019}
2025-01-17 03:06:31,122 [INFO] Step[250/2713]: training loss : 0.9301330173015594 TRAIN  loss dict:  {'classification_loss': 0.9301330173015594}
2025-01-17 03:06:47,378 [INFO] Step[300/2713]: training loss : 0.9277056419849395 TRAIN  loss dict:  {'classification_loss': 0.9277056419849395}
2025-01-17 03:07:03,674 [INFO] Step[350/2713]: training loss : 0.9293510615825653 TRAIN  loss dict:  {'classification_loss': 0.9293510615825653}
2025-01-17 03:07:20,032 [INFO] Step[400/2713]: training loss : 0.9386961209774017 TRAIN  loss dict:  {'classification_loss': 0.9386961209774017}
2025-01-17 03:07:36,304 [INFO] Step[450/2713]: training loss : 0.9301037335395813 TRAIN  loss dict:  {'classification_loss': 0.9301037335395813}
2025-01-17 03:07:52,598 [INFO] Step[500/2713]: training loss : 0.9772099769115448 TRAIN  loss dict:  {'classification_loss': 0.9772099769115448}
2025-01-17 03:08:08,856 [INFO] Step[550/2713]: training loss : 0.9279192841053009 TRAIN  loss dict:  {'classification_loss': 0.9279192841053009}
2025-01-17 03:08:25,017 [INFO] Step[600/2713]: training loss : 0.9284872710704803 TRAIN  loss dict:  {'classification_loss': 0.9284872710704803}
2025-01-17 03:08:41,268 [INFO] Step[650/2713]: training loss : 0.9271636605262756 TRAIN  loss dict:  {'classification_loss': 0.9271636605262756}
2025-01-17 03:08:57,440 [INFO] Step[700/2713]: training loss : 0.929477288722992 TRAIN  loss dict:  {'classification_loss': 0.929477288722992}
2025-01-17 03:09:13,584 [INFO] Step[750/2713]: training loss : 0.9709928393363952 TRAIN  loss dict:  {'classification_loss': 0.9709928393363952}
2025-01-17 03:09:29,773 [INFO] Step[800/2713]: training loss : 0.9447525107860565 TRAIN  loss dict:  {'classification_loss': 0.9447525107860565}
2025-01-17 03:09:45,986 [INFO] Step[850/2713]: training loss : 0.9280495584011078 TRAIN  loss dict:  {'classification_loss': 0.9280495584011078}
2025-01-17 03:10:02,162 [INFO] Step[900/2713]: training loss : 0.9277649152278901 TRAIN  loss dict:  {'classification_loss': 0.9277649152278901}
2025-01-17 03:10:18,267 [INFO] Step[950/2713]: training loss : 0.928019505739212 TRAIN  loss dict:  {'classification_loss': 0.928019505739212}
2025-01-17 03:10:34,461 [INFO] Step[1000/2713]: training loss : 0.9332169103622436 TRAIN  loss dict:  {'classification_loss': 0.9332169103622436}
2025-01-17 03:10:50,602 [INFO] Step[1050/2713]: training loss : 0.9327436852455139 TRAIN  loss dict:  {'classification_loss': 0.9327436852455139}
2025-01-17 03:11:06,852 [INFO] Step[1100/2713]: training loss : 0.928570305109024 TRAIN  loss dict:  {'classification_loss': 0.928570305109024}
2025-01-17 03:11:23,115 [INFO] Step[1150/2713]: training loss : 0.9299026203155517 TRAIN  loss dict:  {'classification_loss': 0.9299026203155517}
2025-01-17 03:11:39,434 [INFO] Step[1200/2713]: training loss : 0.9282603299617768 TRAIN  loss dict:  {'classification_loss': 0.9282603299617768}
2025-01-17 03:11:55,677 [INFO] Step[1250/2713]: training loss : 0.928729031085968 TRAIN  loss dict:  {'classification_loss': 0.928729031085968}
2025-01-17 03:12:11,720 [INFO] Step[1300/2713]: training loss : 0.9278109514713287 TRAIN  loss dict:  {'classification_loss': 0.9278109514713287}
2025-01-17 03:12:27,882 [INFO] Step[1350/2713]: training loss : 0.9290635681152344 TRAIN  loss dict:  {'classification_loss': 0.9290635681152344}
2025-01-17 03:12:43,935 [INFO] Step[1400/2713]: training loss : 0.9281282639503479 TRAIN  loss dict:  {'classification_loss': 0.9281282639503479}
2025-01-17 03:13:00,082 [INFO] Step[1450/2713]: training loss : 0.9287166166305542 TRAIN  loss dict:  {'classification_loss': 0.9287166166305542}
2025-01-17 03:13:16,086 [INFO] Step[1500/2713]: training loss : 0.9312595105171204 TRAIN  loss dict:  {'classification_loss': 0.9312595105171204}
2025-01-17 03:13:32,226 [INFO] Step[1550/2713]: training loss : 0.9365875351428986 TRAIN  loss dict:  {'classification_loss': 0.9365875351428986}
2025-01-17 03:13:48,302 [INFO] Step[1600/2713]: training loss : 0.9280871152877808 TRAIN  loss dict:  {'classification_loss': 0.9280871152877808}
2025-01-17 03:14:04,419 [INFO] Step[1650/2713]: training loss : 0.9311145234107971 TRAIN  loss dict:  {'classification_loss': 0.9311145234107971}
2025-01-17 03:14:20,534 [INFO] Step[1700/2713]: training loss : 0.9330036628246308 TRAIN  loss dict:  {'classification_loss': 0.9330036628246308}
2025-01-17 03:14:36,669 [INFO] Step[1750/2713]: training loss : 0.9379641354084015 TRAIN  loss dict:  {'classification_loss': 0.9379641354084015}
2025-01-17 03:14:52,728 [INFO] Step[1800/2713]: training loss : 0.9274143028259277 TRAIN  loss dict:  {'classification_loss': 0.9274143028259277}
2025-01-17 03:15:08,825 [INFO] Step[1850/2713]: training loss : 0.929191677570343 TRAIN  loss dict:  {'classification_loss': 0.929191677570343}
2025-01-17 03:15:24,918 [INFO] Step[1900/2713]: training loss : 0.9407080066204071 TRAIN  loss dict:  {'classification_loss': 0.9407080066204071}
2025-01-17 03:15:41,002 [INFO] Step[1950/2713]: training loss : 0.9294550454616547 TRAIN  loss dict:  {'classification_loss': 0.9294550454616547}
2025-01-17 03:15:57,080 [INFO] Step[2000/2713]: training loss : 0.9596606123447419 TRAIN  loss dict:  {'classification_loss': 0.9596606123447419}
2025-01-17 03:16:13,143 [INFO] Step[2050/2713]: training loss : 0.933205578327179 TRAIN  loss dict:  {'classification_loss': 0.933205578327179}
2025-01-17 03:16:29,252 [INFO] Step[2100/2713]: training loss : 0.9275229799747468 TRAIN  loss dict:  {'classification_loss': 0.9275229799747468}
2025-01-17 03:16:45,359 [INFO] Step[2150/2713]: training loss : 0.928490400314331 TRAIN  loss dict:  {'classification_loss': 0.928490400314331}
2025-01-17 03:17:01,455 [INFO] Step[2200/2713]: training loss : 0.9304249048233032 TRAIN  loss dict:  {'classification_loss': 0.9304249048233032}
2025-01-17 03:17:17,483 [INFO] Step[2250/2713]: training loss : 0.9277089583873749 TRAIN  loss dict:  {'classification_loss': 0.9277089583873749}
2025-01-17 03:17:33,548 [INFO] Step[2300/2713]: training loss : 0.9278680968284607 TRAIN  loss dict:  {'classification_loss': 0.9278680968284607}
2025-01-17 03:17:49,718 [INFO] Step[2350/2713]: training loss : 0.9416395950317383 TRAIN  loss dict:  {'classification_loss': 0.9416395950317383}
2025-01-17 03:18:05,857 [INFO] Step[2400/2713]: training loss : 0.9277278208732604 TRAIN  loss dict:  {'classification_loss': 0.9277278208732604}
2025-01-17 03:18:21,988 [INFO] Step[2450/2713]: training loss : 0.9284659337997436 TRAIN  loss dict:  {'classification_loss': 0.9284659337997436}
2025-01-17 03:18:38,188 [INFO] Step[2500/2713]: training loss : 0.9282422351837158 TRAIN  loss dict:  {'classification_loss': 0.9282422351837158}
2025-01-17 03:18:54,405 [INFO] Step[2550/2713]: training loss : 0.9352828490734101 TRAIN  loss dict:  {'classification_loss': 0.9352828490734101}
2025-01-17 03:19:10,609 [INFO] Step[2600/2713]: training loss : 0.9291814911365509 TRAIN  loss dict:  {'classification_loss': 0.9291814911365509}
2025-01-17 03:19:26,839 [INFO] Step[2650/2713]: training loss : 0.9441110908985137 TRAIN  loss dict:  {'classification_loss': 0.9441110908985137}
2025-01-17 03:19:43,055 [INFO] Step[2700/2713]: training loss : 0.9333179593086243 TRAIN  loss dict:  {'classification_loss': 0.9333179593086243}
2025-01-17 03:21:01,050 [INFO] Label accuracies statistics:
2025-01-17 03:21:01,051 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.5, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 0.75, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 0.75, 142: 0.5, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.25, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.25, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 1.0, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.25, 353: 0.5, 354: 0.0, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 03:21:01,052 [INFO] [64] TRAIN  loss: 0.9330818143841793 acc: 0.9984027521808576
2025-01-17 03:21:01,052 [INFO] [64] TRAIN  loss dict: {'classification_loss': 0.9330818143841793}
2025-01-17 03:21:01,053 [INFO] [64] VALIDATION loss: 1.9296741485595703 VALIDATION acc: 0.786833855799373
2025-01-17 03:21:01,053 [INFO] [64] VALIDATION loss dict: {'classification_loss': 1.9296741485595703}
2025-01-17 03:21:01,053 [INFO] 
2025-01-17 03:21:21,883 [INFO] Step[50/2713]: training loss : 0.927834347486496 TRAIN  loss dict:  {'classification_loss': 0.927834347486496}
2025-01-17 03:21:37,974 [INFO] Step[100/2713]: training loss : 0.9449066233634948 TRAIN  loss dict:  {'classification_loss': 0.9449066233634948}
2025-01-17 03:21:54,144 [INFO] Step[150/2713]: training loss : 0.9275532722473144 TRAIN  loss dict:  {'classification_loss': 0.9275532722473144}
2025-01-17 03:22:10,365 [INFO] Step[200/2713]: training loss : 0.9281769037246704 TRAIN  loss dict:  {'classification_loss': 0.9281769037246704}
2025-01-17 03:22:26,640 [INFO] Step[250/2713]: training loss : 0.9371492767333984 TRAIN  loss dict:  {'classification_loss': 0.9371492767333984}
2025-01-17 03:22:42,939 [INFO] Step[300/2713]: training loss : 0.9282255280017853 TRAIN  loss dict:  {'classification_loss': 0.9282255280017853}
2025-01-17 03:22:59,088 [INFO] Step[350/2713]: training loss : 0.9752162885665894 TRAIN  loss dict:  {'classification_loss': 0.9752162885665894}
2025-01-17 03:23:15,297 [INFO] Step[400/2713]: training loss : 0.9287095808982849 TRAIN  loss dict:  {'classification_loss': 0.9287095808982849}
2025-01-17 03:23:31,471 [INFO] Step[450/2713]: training loss : 0.935000307559967 TRAIN  loss dict:  {'classification_loss': 0.935000307559967}
2025-01-17 03:23:47,562 [INFO] Step[500/2713]: training loss : 0.9280650699138642 TRAIN  loss dict:  {'classification_loss': 0.9280650699138642}
2025-01-17 03:24:03,645 [INFO] Step[550/2713]: training loss : 0.9279978132247925 TRAIN  loss dict:  {'classification_loss': 0.9279978132247925}
2025-01-17 03:24:19,794 [INFO] Step[600/2713]: training loss : 0.9280926775932312 TRAIN  loss dict:  {'classification_loss': 0.9280926775932312}
2025-01-17 03:24:35,964 [INFO] Step[650/2713]: training loss : 0.9276507139205933 TRAIN  loss dict:  {'classification_loss': 0.9276507139205933}
2025-01-17 03:24:52,082 [INFO] Step[700/2713]: training loss : 0.9478547418117523 TRAIN  loss dict:  {'classification_loss': 0.9478547418117523}
2025-01-17 03:25:08,240 [INFO] Step[750/2713]: training loss : 0.9286377406120301 TRAIN  loss dict:  {'classification_loss': 0.9286377406120301}
2025-01-17 03:25:24,420 [INFO] Step[800/2713]: training loss : 0.9536492991447448 TRAIN  loss dict:  {'classification_loss': 0.9536492991447448}
2025-01-17 03:25:40,594 [INFO] Step[850/2713]: training loss : 0.9277818882465363 TRAIN  loss dict:  {'classification_loss': 0.9277818882465363}
2025-01-17 03:25:56,693 [INFO] Step[900/2713]: training loss : 0.937990335226059 TRAIN  loss dict:  {'classification_loss': 0.937990335226059}
2025-01-17 03:26:12,905 [INFO] Step[950/2713]: training loss : 0.928626195192337 TRAIN  loss dict:  {'classification_loss': 0.928626195192337}
2025-01-17 03:26:28,931 [INFO] Step[1000/2713]: training loss : 0.9286263358592987 TRAIN  loss dict:  {'classification_loss': 0.9286263358592987}
2025-01-17 03:26:45,067 [INFO] Step[1050/2713]: training loss : 0.9272003042697906 TRAIN  loss dict:  {'classification_loss': 0.9272003042697906}
2025-01-17 03:27:01,262 [INFO] Step[1100/2713]: training loss : 0.9295118474960327 TRAIN  loss dict:  {'classification_loss': 0.9295118474960327}
2025-01-17 03:27:17,432 [INFO] Step[1150/2713]: training loss : 0.9294096148014068 TRAIN  loss dict:  {'classification_loss': 0.9294096148014068}
2025-01-17 03:27:33,498 [INFO] Step[1200/2713]: training loss : 0.9588828945159912 TRAIN  loss dict:  {'classification_loss': 0.9588828945159912}
2025-01-17 03:27:49,709 [INFO] Step[1250/2713]: training loss : 0.9276915633678436 TRAIN  loss dict:  {'classification_loss': 0.9276915633678436}
2025-01-17 03:28:05,867 [INFO] Step[1300/2713]: training loss : 0.9370079910755158 TRAIN  loss dict:  {'classification_loss': 0.9370079910755158}
2025-01-17 03:28:21,960 [INFO] Step[1350/2713]: training loss : 0.9333049404621124 TRAIN  loss dict:  {'classification_loss': 0.9333049404621124}
2025-01-17 03:28:38,086 [INFO] Step[1400/2713]: training loss : 0.9275271189212799 TRAIN  loss dict:  {'classification_loss': 0.9275271189212799}
2025-01-17 03:28:54,278 [INFO] Step[1450/2713]: training loss : 0.9389104700088501 TRAIN  loss dict:  {'classification_loss': 0.9389104700088501}
2025-01-17 03:29:10,341 [INFO] Step[1500/2713]: training loss : 0.9274711799621582 TRAIN  loss dict:  {'classification_loss': 0.9274711799621582}
2025-01-17 03:29:26,539 [INFO] Step[1550/2713]: training loss : 0.9282294583320617 TRAIN  loss dict:  {'classification_loss': 0.9282294583320617}
2025-01-17 03:29:42,719 [INFO] Step[1600/2713]: training loss : 0.9279132866859436 TRAIN  loss dict:  {'classification_loss': 0.9279132866859436}
2025-01-17 03:29:58,888 [INFO] Step[1650/2713]: training loss : 0.9277077913284302 TRAIN  loss dict:  {'classification_loss': 0.9277077913284302}
2025-01-17 03:30:14,954 [INFO] Step[1700/2713]: training loss : 0.9421058094501495 TRAIN  loss dict:  {'classification_loss': 0.9421058094501495}
2025-01-17 03:30:31,081 [INFO] Step[1750/2713]: training loss : 0.9294011712074279 TRAIN  loss dict:  {'classification_loss': 0.9294011712074279}
2025-01-17 03:30:47,156 [INFO] Step[1800/2713]: training loss : 0.929745671749115 TRAIN  loss dict:  {'classification_loss': 0.929745671749115}
2025-01-17 03:31:03,234 [INFO] Step[1850/2713]: training loss : 0.9275661301612854 TRAIN  loss dict:  {'classification_loss': 0.9275661301612854}
2025-01-17 03:31:19,309 [INFO] Step[1900/2713]: training loss : 0.927859616279602 TRAIN  loss dict:  {'classification_loss': 0.927859616279602}
2025-01-17 03:31:35,454 [INFO] Step[1950/2713]: training loss : 0.9453129255771637 TRAIN  loss dict:  {'classification_loss': 0.9453129255771637}
2025-01-17 03:31:51,604 [INFO] Step[2000/2713]: training loss : 0.9315320205688477 TRAIN  loss dict:  {'classification_loss': 0.9315320205688477}
2025-01-17 03:32:07,735 [INFO] Step[2050/2713]: training loss : 0.9270276045799255 TRAIN  loss dict:  {'classification_loss': 0.9270276045799255}
2025-01-17 03:32:23,898 [INFO] Step[2100/2713]: training loss : 0.9285354506969452 TRAIN  loss dict:  {'classification_loss': 0.9285354506969452}
2025-01-17 03:32:40,076 [INFO] Step[2150/2713]: training loss : 0.9304371583461761 TRAIN  loss dict:  {'classification_loss': 0.9304371583461761}
2025-01-17 03:32:56,202 [INFO] Step[2200/2713]: training loss : 0.9282344663143158 TRAIN  loss dict:  {'classification_loss': 0.9282344663143158}
2025-01-17 03:33:12,406 [INFO] Step[2250/2713]: training loss : 0.9280064070224762 TRAIN  loss dict:  {'classification_loss': 0.9280064070224762}
2025-01-17 03:33:28,633 [INFO] Step[2300/2713]: training loss : 0.9296758425235748 TRAIN  loss dict:  {'classification_loss': 0.9296758425235748}
2025-01-17 03:33:44,888 [INFO] Step[2350/2713]: training loss : 0.938175151348114 TRAIN  loss dict:  {'classification_loss': 0.938175151348114}
2025-01-17 03:34:01,079 [INFO] Step[2400/2713]: training loss : 0.933144724369049 TRAIN  loss dict:  {'classification_loss': 0.933144724369049}
2025-01-17 03:34:17,294 [INFO] Step[2450/2713]: training loss : 0.9273176491260529 TRAIN  loss dict:  {'classification_loss': 0.9273176491260529}
2025-01-17 03:34:33,438 [INFO] Step[2500/2713]: training loss : 0.9271384060382843 TRAIN  loss dict:  {'classification_loss': 0.9271384060382843}
2025-01-17 03:34:49,692 [INFO] Step[2550/2713]: training loss : 0.9276134395599365 TRAIN  loss dict:  {'classification_loss': 0.9276134395599365}
2025-01-17 03:35:05,822 [INFO] Step[2600/2713]: training loss : 0.9288916528224945 TRAIN  loss dict:  {'classification_loss': 0.9288916528224945}
2025-01-17 03:35:21,996 [INFO] Step[2650/2713]: training loss : 0.9534281718730927 TRAIN  loss dict:  {'classification_loss': 0.9534281718730927}
2025-01-17 03:35:38,172 [INFO] Step[2700/2713]: training loss : 0.9279536747932434 TRAIN  loss dict:  {'classification_loss': 0.9279536747932434}
2025-01-17 03:36:56,147 [INFO] Label accuracies statistics:
2025-01-17 03:36:56,147 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 1.0, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.75, 204: 1.0, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.5, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.25, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.0, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.25, 395: 0.5, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 03:36:56,149 [INFO] [65] TRAIN  loss: 0.9330747387438211 acc: 0.9984027521808576
2025-01-17 03:36:56,149 [INFO] [65] TRAIN  loss dict: {'classification_loss': 0.9330747387438211}
2025-01-17 03:36:56,149 [INFO] [65] VALIDATION loss: 1.9511807101561611 VALIDATION acc: 0.786833855799373
2025-01-17 03:36:56,149 [INFO] [65] VALIDATION loss dict: {'classification_loss': 1.9511807101561611}
2025-01-17 03:36:56,150 [INFO] 
2025-01-17 03:37:16,697 [INFO] Step[50/2713]: training loss : 0.9277218663692475 TRAIN  loss dict:  {'classification_loss': 0.9277218663692475}
2025-01-17 03:37:32,971 [INFO] Step[100/2713]: training loss : 0.9283019542694092 TRAIN  loss dict:  {'classification_loss': 0.9283019542694092}
2025-01-17 03:37:49,205 [INFO] Step[150/2713]: training loss : 0.9266578996181488 TRAIN  loss dict:  {'classification_loss': 0.9266578996181488}
2025-01-17 03:38:05,342 [INFO] Step[200/2713]: training loss : 0.9285183238983155 TRAIN  loss dict:  {'classification_loss': 0.9285183238983155}
2025-01-17 03:38:21,635 [INFO] Step[250/2713]: training loss : 0.9297410821914673 TRAIN  loss dict:  {'classification_loss': 0.9297410821914673}
2025-01-17 03:38:37,851 [INFO] Step[300/2713]: training loss : 0.9280566847324372 TRAIN  loss dict:  {'classification_loss': 0.9280566847324372}
2025-01-17 03:38:53,986 [INFO] Step[350/2713]: training loss : 0.9287643527984619 TRAIN  loss dict:  {'classification_loss': 0.9287643527984619}
2025-01-17 03:39:10,181 [INFO] Step[400/2713]: training loss : 0.9282004952430725 TRAIN  loss dict:  {'classification_loss': 0.9282004952430725}
2025-01-17 03:39:26,387 [INFO] Step[450/2713]: training loss : 0.9278614664077759 TRAIN  loss dict:  {'classification_loss': 0.9278614664077759}
2025-01-17 03:39:42,596 [INFO] Step[500/2713]: training loss : 0.9580167710781098 TRAIN  loss dict:  {'classification_loss': 0.9580167710781098}
2025-01-17 03:39:58,816 [INFO] Step[550/2713]: training loss : 0.9278062832355499 TRAIN  loss dict:  {'classification_loss': 0.9278062832355499}
2025-01-17 03:40:15,087 [INFO] Step[600/2713]: training loss : 0.9280343532562256 TRAIN  loss dict:  {'classification_loss': 0.9280343532562256}
2025-01-17 03:40:31,279 [INFO] Step[650/2713]: training loss : 0.9777714467048645 TRAIN  loss dict:  {'classification_loss': 0.9777714467048645}
2025-01-17 03:40:47,483 [INFO] Step[700/2713]: training loss : 0.9329144668579101 TRAIN  loss dict:  {'classification_loss': 0.9329144668579101}
2025-01-17 03:41:03,680 [INFO] Step[750/2713]: training loss : 0.9281195831298829 TRAIN  loss dict:  {'classification_loss': 0.9281195831298829}
2025-01-17 03:41:19,854 [INFO] Step[800/2713]: training loss : 0.9337237274646759 TRAIN  loss dict:  {'classification_loss': 0.9337237274646759}
2025-01-17 03:41:36,088 [INFO] Step[850/2713]: training loss : 0.9280337178707123 TRAIN  loss dict:  {'classification_loss': 0.9280337178707123}
2025-01-17 03:41:52,284 [INFO] Step[900/2713]: training loss : 0.9339734220504761 TRAIN  loss dict:  {'classification_loss': 0.9339734220504761}
2025-01-17 03:42:08,492 [INFO] Step[950/2713]: training loss : 0.9281532609462738 TRAIN  loss dict:  {'classification_loss': 0.9281532609462738}
2025-01-17 03:42:24,720 [INFO] Step[1000/2713]: training loss : 0.9282612442970276 TRAIN  loss dict:  {'classification_loss': 0.9282612442970276}
2025-01-17 03:42:41,010 [INFO] Step[1050/2713]: training loss : 0.9362385332584381 TRAIN  loss dict:  {'classification_loss': 0.9362385332584381}
2025-01-17 03:42:57,227 [INFO] Step[1100/2713]: training loss : 0.9532788896560669 TRAIN  loss dict:  {'classification_loss': 0.9532788896560669}
2025-01-17 03:43:13,448 [INFO] Step[1150/2713]: training loss : 0.9280946910381317 TRAIN  loss dict:  {'classification_loss': 0.9280946910381317}
2025-01-17 03:43:29,616 [INFO] Step[1200/2713]: training loss : 0.9275753474235535 TRAIN  loss dict:  {'classification_loss': 0.9275753474235535}
2025-01-17 03:43:45,960 [INFO] Step[1250/2713]: training loss : 0.9284140598773957 TRAIN  loss dict:  {'classification_loss': 0.9284140598773957}
2025-01-17 03:44:02,143 [INFO] Step[1300/2713]: training loss : 0.9285339879989624 TRAIN  loss dict:  {'classification_loss': 0.9285339879989624}
2025-01-17 03:44:18,414 [INFO] Step[1350/2713]: training loss : 0.9297833752632141 TRAIN  loss dict:  {'classification_loss': 0.9297833752632141}
2025-01-17 03:44:34,631 [INFO] Step[1400/2713]: training loss : 0.928288152217865 TRAIN  loss dict:  {'classification_loss': 0.928288152217865}
2025-01-17 03:44:50,896 [INFO] Step[1450/2713]: training loss : 0.9373189461231232 TRAIN  loss dict:  {'classification_loss': 0.9373189461231232}
2025-01-17 03:45:07,063 [INFO] Step[1500/2713]: training loss : 0.9277665805816651 TRAIN  loss dict:  {'classification_loss': 0.9277665805816651}
2025-01-17 03:45:23,282 [INFO] Step[1550/2713]: training loss : 0.9277924370765686 TRAIN  loss dict:  {'classification_loss': 0.9277924370765686}
2025-01-17 03:45:39,533 [INFO] Step[1600/2713]: training loss : 0.9295171344280243 TRAIN  loss dict:  {'classification_loss': 0.9295171344280243}
2025-01-17 03:45:55,813 [INFO] Step[1650/2713]: training loss : 0.9280019164085388 TRAIN  loss dict:  {'classification_loss': 0.9280019164085388}
2025-01-17 03:46:12,010 [INFO] Step[1700/2713]: training loss : 0.9286606454849243 TRAIN  loss dict:  {'classification_loss': 0.9286606454849243}
2025-01-17 03:46:28,257 [INFO] Step[1750/2713]: training loss : 0.9290748536586761 TRAIN  loss dict:  {'classification_loss': 0.9290748536586761}
2025-01-17 03:46:44,512 [INFO] Step[1800/2713]: training loss : 0.9306765508651733 TRAIN  loss dict:  {'classification_loss': 0.9306765508651733}
2025-01-17 03:47:00,780 [INFO] Step[1850/2713]: training loss : 0.9290589988231659 TRAIN  loss dict:  {'classification_loss': 0.9290589988231659}
2025-01-17 03:47:17,021 [INFO] Step[1900/2713]: training loss : 0.9277055597305298 TRAIN  loss dict:  {'classification_loss': 0.9277055597305298}
2025-01-17 03:47:33,257 [INFO] Step[1950/2713]: training loss : 0.947304482460022 TRAIN  loss dict:  {'classification_loss': 0.947304482460022}
2025-01-17 03:47:49,413 [INFO] Step[2000/2713]: training loss : 0.9329885387420654 TRAIN  loss dict:  {'classification_loss': 0.9329885387420654}
2025-01-17 03:48:05,618 [INFO] Step[2050/2713]: training loss : 0.9373047816753387 TRAIN  loss dict:  {'classification_loss': 0.9373047816753387}
2025-01-17 03:48:21,783 [INFO] Step[2100/2713]: training loss : 0.9281042814254761 TRAIN  loss dict:  {'classification_loss': 0.9281042814254761}
2025-01-17 03:48:38,019 [INFO] Step[2150/2713]: training loss : 0.9273868072032928 TRAIN  loss dict:  {'classification_loss': 0.9273868072032928}
2025-01-17 03:48:54,212 [INFO] Step[2200/2713]: training loss : 0.9378439033031464 TRAIN  loss dict:  {'classification_loss': 0.9378439033031464}
2025-01-17 03:49:10,463 [INFO] Step[2250/2713]: training loss : 0.9275348770618439 TRAIN  loss dict:  {'classification_loss': 0.9275348770618439}
2025-01-17 03:49:26,777 [INFO] Step[2300/2713]: training loss : 0.9304980039596558 TRAIN  loss dict:  {'classification_loss': 0.9304980039596558}
2025-01-17 03:49:43,094 [INFO] Step[2350/2713]: training loss : 0.9414045691490174 TRAIN  loss dict:  {'classification_loss': 0.9414045691490174}
2025-01-17 03:49:59,333 [INFO] Step[2400/2713]: training loss : 0.9279208517074585 TRAIN  loss dict:  {'classification_loss': 0.9279208517074585}
2025-01-17 03:50:15,581 [INFO] Step[2450/2713]: training loss : 0.9501866316795349 TRAIN  loss dict:  {'classification_loss': 0.9501866316795349}
2025-01-17 03:50:31,783 [INFO] Step[2500/2713]: training loss : 0.9276513373851776 TRAIN  loss dict:  {'classification_loss': 0.9276513373851776}
2025-01-17 03:50:48,037 [INFO] Step[2550/2713]: training loss : 0.9287714087963104 TRAIN  loss dict:  {'classification_loss': 0.9287714087963104}
2025-01-17 03:51:04,319 [INFO] Step[2600/2713]: training loss : 0.9333003735542298 TRAIN  loss dict:  {'classification_loss': 0.9333003735542298}
2025-01-17 03:51:20,565 [INFO] Step[2650/2713]: training loss : 0.932374793291092 TRAIN  loss dict:  {'classification_loss': 0.932374793291092}
2025-01-17 03:51:36,774 [INFO] Step[2700/2713]: training loss : 0.9495769703388214 TRAIN  loss dict:  {'classification_loss': 0.9495769703388214}
2025-01-17 03:52:55,733 [INFO] Label accuracies statistics:
2025-01-17 03:52:55,733 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.0, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 0.75, 25: 1.0, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.25, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.5, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.75, 262: 1.0, 263: 0.5, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 1.0, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 03:52:55,735 [INFO] [66] TRAIN  loss: 0.9328367993306991 acc: 0.9985256173977147
2025-01-17 03:52:55,735 [INFO] [66] TRAIN  loss dict: {'classification_loss': 0.9328367993306991}
2025-01-17 03:52:55,736 [INFO] [66] VALIDATION loss: 1.9917881172850616 VALIDATION acc: 0.7818181818181819
2025-01-17 03:52:55,736 [INFO] [66] VALIDATION loss dict: {'classification_loss': 1.9917881172850616}
2025-01-17 03:52:55,736 [INFO] 
2025-01-17 03:53:16,390 [INFO] Step[50/2713]: training loss : 0.9283500254154206 TRAIN  loss dict:  {'classification_loss': 0.9283500254154206}
2025-01-17 03:53:32,518 [INFO] Step[100/2713]: training loss : 0.9399824059009552 TRAIN  loss dict:  {'classification_loss': 0.9399824059009552}
2025-01-17 03:53:48,709 [INFO] Step[150/2713]: training loss : 0.9283710622787475 TRAIN  loss dict:  {'classification_loss': 0.9283710622787475}
2025-01-17 03:54:04,888 [INFO] Step[200/2713]: training loss : 0.9274792158603669 TRAIN  loss dict:  {'classification_loss': 0.9274792158603669}
2025-01-17 03:54:21,129 [INFO] Step[250/2713]: training loss : 0.9294253659248352 TRAIN  loss dict:  {'classification_loss': 0.9294253659248352}
2025-01-17 03:54:37,369 [INFO] Step[300/2713]: training loss : 0.9277268481254578 TRAIN  loss dict:  {'classification_loss': 0.9277268481254578}
2025-01-17 03:54:53,625 [INFO] Step[350/2713]: training loss : 0.9278876721858978 TRAIN  loss dict:  {'classification_loss': 0.9278876721858978}
2025-01-17 03:55:09,802 [INFO] Step[400/2713]: training loss : 0.9523162424564362 TRAIN  loss dict:  {'classification_loss': 0.9523162424564362}
2025-01-17 03:55:26,052 [INFO] Step[450/2713]: training loss : 0.9285530364513397 TRAIN  loss dict:  {'classification_loss': 0.9285530364513397}
2025-01-17 03:55:42,323 [INFO] Step[500/2713]: training loss : 0.9278317356109619 TRAIN  loss dict:  {'classification_loss': 0.9278317356109619}
2025-01-17 03:55:58,563 [INFO] Step[550/2713]: training loss : 0.9497837018966675 TRAIN  loss dict:  {'classification_loss': 0.9497837018966675}
2025-01-17 03:56:14,740 [INFO] Step[600/2713]: training loss : 0.9309137845039368 TRAIN  loss dict:  {'classification_loss': 0.9309137845039368}
2025-01-17 03:56:30,954 [INFO] Step[650/2713]: training loss : 0.9418947958946228 TRAIN  loss dict:  {'classification_loss': 0.9418947958946228}
2025-01-17 03:56:47,207 [INFO] Step[700/2713]: training loss : 0.9276800894737244 TRAIN  loss dict:  {'classification_loss': 0.9276800894737244}
2025-01-17 03:57:03,439 [INFO] Step[750/2713]: training loss : 0.9754694986343384 TRAIN  loss dict:  {'classification_loss': 0.9754694986343384}
2025-01-17 03:57:19,709 [INFO] Step[800/2713]: training loss : 0.9628677988052368 TRAIN  loss dict:  {'classification_loss': 0.9628677988052368}
2025-01-17 03:57:35,993 [INFO] Step[850/2713]: training loss : 0.9277730250358581 TRAIN  loss dict:  {'classification_loss': 0.9277730250358581}
2025-01-17 03:57:52,102 [INFO] Step[900/2713]: training loss : 0.9277313256263733 TRAIN  loss dict:  {'classification_loss': 0.9277313256263733}
2025-01-17 03:58:08,312 [INFO] Step[950/2713]: training loss : 0.9413534033298493 TRAIN  loss dict:  {'classification_loss': 0.9413534033298493}
2025-01-17 03:58:24,523 [INFO] Step[1000/2713]: training loss : 0.9276212096214295 TRAIN  loss dict:  {'classification_loss': 0.9276212096214295}
2025-01-17 03:58:40,835 [INFO] Step[1050/2713]: training loss : 0.9272569835186004 TRAIN  loss dict:  {'classification_loss': 0.9272569835186004}
2025-01-17 03:58:57,008 [INFO] Step[1100/2713]: training loss : 0.9283820772171021 TRAIN  loss dict:  {'classification_loss': 0.9283820772171021}
2025-01-17 03:59:13,238 [INFO] Step[1150/2713]: training loss : 0.9349846971035004 TRAIN  loss dict:  {'classification_loss': 0.9349846971035004}
2025-01-17 03:59:29,508 [INFO] Step[1200/2713]: training loss : 0.9387000906467438 TRAIN  loss dict:  {'classification_loss': 0.9387000906467438}
2025-01-17 03:59:45,764 [INFO] Step[1250/2713]: training loss : 0.9566218912601471 TRAIN  loss dict:  {'classification_loss': 0.9566218912601471}
2025-01-17 04:00:01,902 [INFO] Step[1300/2713]: training loss : 0.9277344954013824 TRAIN  loss dict:  {'classification_loss': 0.9277344954013824}
2025-01-17 04:00:18,146 [INFO] Step[1350/2713]: training loss : 0.9437269818782806 TRAIN  loss dict:  {'classification_loss': 0.9437269818782806}
2025-01-17 04:00:34,375 [INFO] Step[1400/2713]: training loss : 0.929296156167984 TRAIN  loss dict:  {'classification_loss': 0.929296156167984}
2025-01-17 04:00:50,611 [INFO] Step[1450/2713]: training loss : 0.9386023473739624 TRAIN  loss dict:  {'classification_loss': 0.9386023473739624}
2025-01-17 04:01:06,846 [INFO] Step[1500/2713]: training loss : 0.9288462793827057 TRAIN  loss dict:  {'classification_loss': 0.9288462793827057}
2025-01-17 04:01:23,205 [INFO] Step[1550/2713]: training loss : 0.9303459596633911 TRAIN  loss dict:  {'classification_loss': 0.9303459596633911}
2025-01-17 04:01:39,424 [INFO] Step[1600/2713]: training loss : 0.9313187265396118 TRAIN  loss dict:  {'classification_loss': 0.9313187265396118}
2025-01-17 04:01:55,616 [INFO] Step[1650/2713]: training loss : 0.9297451436519623 TRAIN  loss dict:  {'classification_loss': 0.9297451436519623}
2025-01-17 04:02:11,822 [INFO] Step[1700/2713]: training loss : 0.9293794810771943 TRAIN  loss dict:  {'classification_loss': 0.9293794810771943}
2025-01-17 04:02:28,073 [INFO] Step[1750/2713]: training loss : 0.9292498469352722 TRAIN  loss dict:  {'classification_loss': 0.9292498469352722}
2025-01-17 04:02:44,218 [INFO] Step[1800/2713]: training loss : 0.9708530831336976 TRAIN  loss dict:  {'classification_loss': 0.9708530831336976}
2025-01-17 04:03:00,295 [INFO] Step[1850/2713]: training loss : 0.9554958176612854 TRAIN  loss dict:  {'classification_loss': 0.9554958176612854}
2025-01-17 04:03:16,323 [INFO] Step[1900/2713]: training loss : 0.9278276574611664 TRAIN  loss dict:  {'classification_loss': 0.9278276574611664}
2025-01-17 04:03:32,379 [INFO] Step[1950/2713]: training loss : 0.9291004061698913 TRAIN  loss dict:  {'classification_loss': 0.9291004061698913}
2025-01-17 04:03:48,433 [INFO] Step[2000/2713]: training loss : 0.9274876034259796 TRAIN  loss dict:  {'classification_loss': 0.9274876034259796}
2025-01-17 04:04:04,446 [INFO] Step[2050/2713]: training loss : 0.9300236928462983 TRAIN  loss dict:  {'classification_loss': 0.9300236928462983}
2025-01-17 04:04:20,504 [INFO] Step[2100/2713]: training loss : 0.9392875111103058 TRAIN  loss dict:  {'classification_loss': 0.9392875111103058}
2025-01-17 04:04:36,586 [INFO] Step[2150/2713]: training loss : 0.9277969181537629 TRAIN  loss dict:  {'classification_loss': 0.9277969181537629}
2025-01-17 04:04:52,609 [INFO] Step[2200/2713]: training loss : 0.9360786890983581 TRAIN  loss dict:  {'classification_loss': 0.9360786890983581}
2025-01-17 04:05:08,656 [INFO] Step[2250/2713]: training loss : 0.9272415292263031 TRAIN  loss dict:  {'classification_loss': 0.9272415292263031}
2025-01-17 04:05:24,646 [INFO] Step[2300/2713]: training loss : 0.9806234121322632 TRAIN  loss dict:  {'classification_loss': 0.9806234121322632}
2025-01-17 04:05:40,651 [INFO] Step[2350/2713]: training loss : 0.9274308872222901 TRAIN  loss dict:  {'classification_loss': 0.9274308872222901}
2025-01-17 04:05:56,687 [INFO] Step[2400/2713]: training loss : 0.9768948924541473 TRAIN  loss dict:  {'classification_loss': 0.9768948924541473}
2025-01-17 04:06:12,723 [INFO] Step[2450/2713]: training loss : 0.9295266246795655 TRAIN  loss dict:  {'classification_loss': 0.9295266246795655}
2025-01-17 04:06:28,815 [INFO] Step[2500/2713]: training loss : 0.9279011988639831 TRAIN  loss dict:  {'classification_loss': 0.9279011988639831}
2025-01-17 04:06:44,856 [INFO] Step[2550/2713]: training loss : 1.002085987329483 TRAIN  loss dict:  {'classification_loss': 1.002085987329483}
2025-01-17 04:07:00,900 [INFO] Step[2600/2713]: training loss : 0.92769651055336 TRAIN  loss dict:  {'classification_loss': 0.92769651055336}
2025-01-17 04:07:17,032 [INFO] Step[2650/2713]: training loss : 0.9355800473690032 TRAIN  loss dict:  {'classification_loss': 0.9355800473690032}
2025-01-17 04:07:33,072 [INFO] Step[2700/2713]: training loss : 0.9506949687004089 TRAIN  loss dict:  {'classification_loss': 0.9506949687004089}
2025-01-17 04:08:50,592 [INFO] Label accuracies statistics:
2025-01-17 04:08:50,593 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 1.0, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 1.0, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.25, 260: 0.75, 261: 0.75, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 04:08:50,594 [INFO] [67] TRAIN  loss: 0.9381916389182359 acc: 0.9971741000122866
2025-01-17 04:08:50,594 [INFO] [67] TRAIN  loss dict: {'classification_loss': 0.9381916389182359}
2025-01-17 04:08:50,595 [INFO] [67] VALIDATION loss: 1.9096212987612962 VALIDATION acc: 0.7943573667711599
2025-01-17 04:08:50,595 [INFO] [67] VALIDATION loss dict: {'classification_loss': 1.9096212987612962}
2025-01-17 04:08:50,595 [INFO] 
2025-01-17 04:09:11,243 [INFO] Step[50/2713]: training loss : 0.9273662281036377 TRAIN  loss dict:  {'classification_loss': 0.9273662281036377}
2025-01-17 04:09:27,282 [INFO] Step[100/2713]: training loss : 0.930600768327713 TRAIN  loss dict:  {'classification_loss': 0.930600768327713}
2025-01-17 04:09:43,343 [INFO] Step[150/2713]: training loss : 0.9273716402053833 TRAIN  loss dict:  {'classification_loss': 0.9273716402053833}
2025-01-17 04:09:59,380 [INFO] Step[200/2713]: training loss : 0.935453245639801 TRAIN  loss dict:  {'classification_loss': 0.935453245639801}
2025-01-17 04:10:15,433 [INFO] Step[250/2713]: training loss : 0.9271153128147125 TRAIN  loss dict:  {'classification_loss': 0.9271153128147125}
2025-01-17 04:10:31,513 [INFO] Step[300/2713]: training loss : 0.9381056559085846 TRAIN  loss dict:  {'classification_loss': 0.9381056559085846}
2025-01-17 04:10:47,573 [INFO] Step[350/2713]: training loss : 0.9283171677589417 TRAIN  loss dict:  {'classification_loss': 0.9283171677589417}
2025-01-17 04:11:03,587 [INFO] Step[400/2713]: training loss : 0.9276876080036164 TRAIN  loss dict:  {'classification_loss': 0.9276876080036164}
2025-01-17 04:11:19,748 [INFO] Step[450/2713]: training loss : 0.9274380874633789 TRAIN  loss dict:  {'classification_loss': 0.9274380874633789}
2025-01-17 04:11:35,786 [INFO] Step[500/2713]: training loss : 0.9282924628257752 TRAIN  loss dict:  {'classification_loss': 0.9282924628257752}
2025-01-17 04:11:51,830 [INFO] Step[550/2713]: training loss : 0.92716752409935 TRAIN  loss dict:  {'classification_loss': 0.92716752409935}
2025-01-17 04:12:07,888 [INFO] Step[600/2713]: training loss : 0.9314840638637543 TRAIN  loss dict:  {'classification_loss': 0.9314840638637543}
2025-01-17 04:12:23,946 [INFO] Step[650/2713]: training loss : 0.9281393194198608 TRAIN  loss dict:  {'classification_loss': 0.9281393194198608}
2025-01-17 04:12:39,954 [INFO] Step[700/2713]: training loss : 0.9486046195030212 TRAIN  loss dict:  {'classification_loss': 0.9486046195030212}
2025-01-17 04:12:56,020 [INFO] Step[750/2713]: training loss : 0.9279371094703674 TRAIN  loss dict:  {'classification_loss': 0.9279371094703674}
2025-01-17 04:13:12,145 [INFO] Step[800/2713]: training loss : 0.9279645109176635 TRAIN  loss dict:  {'classification_loss': 0.9279645109176635}
2025-01-17 04:13:28,210 [INFO] Step[850/2713]: training loss : 0.9274186635017395 TRAIN  loss dict:  {'classification_loss': 0.9274186635017395}
2025-01-17 04:13:44,256 [INFO] Step[900/2713]: training loss : 0.9277398300170898 TRAIN  loss dict:  {'classification_loss': 0.9277398300170898}
2025-01-17 04:14:00,284 [INFO] Step[950/2713]: training loss : 0.9273870122432709 TRAIN  loss dict:  {'classification_loss': 0.9273870122432709}
2025-01-17 04:14:16,306 [INFO] Step[1000/2713]: training loss : 0.9280803966522216 TRAIN  loss dict:  {'classification_loss': 0.9280803966522216}
2025-01-17 04:14:32,441 [INFO] Step[1050/2713]: training loss : 0.928814148902893 TRAIN  loss dict:  {'classification_loss': 0.928814148902893}
2025-01-17 04:14:48,499 [INFO] Step[1100/2713]: training loss : 0.9307954907417297 TRAIN  loss dict:  {'classification_loss': 0.9307954907417297}
2025-01-17 04:15:04,579 [INFO] Step[1150/2713]: training loss : 0.9275652730464935 TRAIN  loss dict:  {'classification_loss': 0.9275652730464935}
2025-01-17 04:15:20,664 [INFO] Step[1200/2713]: training loss : 0.9289391720294953 TRAIN  loss dict:  {'classification_loss': 0.9289391720294953}
2025-01-17 04:15:36,707 [INFO] Step[1250/2713]: training loss : 0.9274598777294158 TRAIN  loss dict:  {'classification_loss': 0.9274598777294158}
2025-01-17 04:15:52,770 [INFO] Step[1300/2713]: training loss : 0.9330158114433289 TRAIN  loss dict:  {'classification_loss': 0.9330158114433289}
2025-01-17 04:16:08,811 [INFO] Step[1350/2713]: training loss : 0.9347760081291199 TRAIN  loss dict:  {'classification_loss': 0.9347760081291199}
2025-01-17 04:16:24,883 [INFO] Step[1400/2713]: training loss : 0.928281272649765 TRAIN  loss dict:  {'classification_loss': 0.928281272649765}
2025-01-17 04:16:40,887 [INFO] Step[1450/2713]: training loss : 0.9294540691375732 TRAIN  loss dict:  {'classification_loss': 0.9294540691375732}
2025-01-17 04:16:57,038 [INFO] Step[1500/2713]: training loss : 0.9283567261695862 TRAIN  loss dict:  {'classification_loss': 0.9283567261695862}
2025-01-17 04:17:13,193 [INFO] Step[1550/2713]: training loss : 0.9486248886585236 TRAIN  loss dict:  {'classification_loss': 0.9486248886585236}
2025-01-17 04:17:29,263 [INFO] Step[1600/2713]: training loss : 0.9270398020744324 TRAIN  loss dict:  {'classification_loss': 0.9270398020744324}
2025-01-17 04:17:45,335 [INFO] Step[1650/2713]: training loss : 0.9276351940631866 TRAIN  loss dict:  {'classification_loss': 0.9276351940631866}
2025-01-17 04:18:01,359 [INFO] Step[1700/2713]: training loss : 0.929464111328125 TRAIN  loss dict:  {'classification_loss': 0.929464111328125}
2025-01-17 04:18:17,460 [INFO] Step[1750/2713]: training loss : 0.9440989911556243 TRAIN  loss dict:  {'classification_loss': 0.9440989911556243}
2025-01-17 04:18:33,459 [INFO] Step[1800/2713]: training loss : 0.9279831516742706 TRAIN  loss dict:  {'classification_loss': 0.9279831516742706}
2025-01-17 04:18:49,565 [INFO] Step[1850/2713]: training loss : 0.9287089967727661 TRAIN  loss dict:  {'classification_loss': 0.9287089967727661}
2025-01-17 04:19:05,547 [INFO] Step[1900/2713]: training loss : 0.9276693880558013 TRAIN  loss dict:  {'classification_loss': 0.9276693880558013}
2025-01-17 04:19:21,588 [INFO] Step[1950/2713]: training loss : 0.9277678847312927 TRAIN  loss dict:  {'classification_loss': 0.9277678847312927}
2025-01-17 04:19:37,690 [INFO] Step[2000/2713]: training loss : 0.9277472698688507 TRAIN  loss dict:  {'classification_loss': 0.9277472698688507}
2025-01-17 04:19:53,719 [INFO] Step[2050/2713]: training loss : 0.9266699171066284 TRAIN  loss dict:  {'classification_loss': 0.9266699171066284}
2025-01-17 04:20:09,848 [INFO] Step[2100/2713]: training loss : 0.9333152353763581 TRAIN  loss dict:  {'classification_loss': 0.9333152353763581}
2025-01-17 04:20:25,956 [INFO] Step[2150/2713]: training loss : 0.9284450888633728 TRAIN  loss dict:  {'classification_loss': 0.9284450888633728}
2025-01-17 04:20:42,118 [INFO] Step[2200/2713]: training loss : 0.9274653029441834 TRAIN  loss dict:  {'classification_loss': 0.9274653029441834}
2025-01-17 04:20:58,097 [INFO] Step[2250/2713]: training loss : 0.9470021033287048 TRAIN  loss dict:  {'classification_loss': 0.9470021033287048}
2025-01-17 04:21:14,118 [INFO] Step[2300/2713]: training loss : 0.9276905858516693 TRAIN  loss dict:  {'classification_loss': 0.9276905858516693}
2025-01-17 04:21:30,250 [INFO] Step[2350/2713]: training loss : 0.9273955082893371 TRAIN  loss dict:  {'classification_loss': 0.9273955082893371}
2025-01-17 04:21:46,293 [INFO] Step[2400/2713]: training loss : 0.9271710968017578 TRAIN  loss dict:  {'classification_loss': 0.9271710968017578}
2025-01-17 04:22:02,290 [INFO] Step[2450/2713]: training loss : 0.9329049158096313 TRAIN  loss dict:  {'classification_loss': 0.9329049158096313}
2025-01-17 04:22:18,328 [INFO] Step[2500/2713]: training loss : 0.9293134784698487 TRAIN  loss dict:  {'classification_loss': 0.9293134784698487}
2025-01-17 04:22:34,350 [INFO] Step[2550/2713]: training loss : 0.928039014339447 TRAIN  loss dict:  {'classification_loss': 0.928039014339447}
2025-01-17 04:22:50,368 [INFO] Step[2600/2713]: training loss : 0.931720243692398 TRAIN  loss dict:  {'classification_loss': 0.931720243692398}
2025-01-17 04:23:06,488 [INFO] Step[2650/2713]: training loss : 0.9348572742938995 TRAIN  loss dict:  {'classification_loss': 0.9348572742938995}
2025-01-17 04:23:22,612 [INFO] Step[2700/2713]: training loss : 0.9272896087169648 TRAIN  loss dict:  {'classification_loss': 0.9272896087169648}
2025-01-17 04:24:40,579 [INFO] Label accuracies statistics:
2025-01-17 04:24:40,579 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.75, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 0.75, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.25, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.75, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.0, 212: 0.5, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.5, 260: 1.0, 261: 0.5, 262: 1.0, 263: 0.5, 264: 0.75, 265: 0.75, 266: 0.75, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.75, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.5, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 04:24:40,581 [INFO] [68] TRAIN  loss: 0.9304182955667711 acc: 0.9991399434820002
2025-01-17 04:24:40,581 [INFO] [68] TRAIN  loss dict: {'classification_loss': 0.9304182955667711}
2025-01-17 04:24:40,581 [INFO] [68] VALIDATION loss: 1.9035834198383461 VALIDATION acc: 0.7956112852664576
2025-01-17 04:24:40,581 [INFO] [68] VALIDATION loss dict: {'classification_loss': 1.9035834198383461}
2025-01-17 04:24:40,581 [INFO] 
2025-01-17 04:25:01,483 [INFO] Step[50/2713]: training loss : 0.9306666111946106 TRAIN  loss dict:  {'classification_loss': 0.9306666111946106}
2025-01-17 04:25:17,496 [INFO] Step[100/2713]: training loss : 0.9278710341453552 TRAIN  loss dict:  {'classification_loss': 0.9278710341453552}
2025-01-17 04:25:33,641 [INFO] Step[150/2713]: training loss : 0.9284735357761383 TRAIN  loss dict:  {'classification_loss': 0.9284735357761383}
2025-01-17 04:25:49,704 [INFO] Step[200/2713]: training loss : 0.9276758444309234 TRAIN  loss dict:  {'classification_loss': 0.9276758444309234}
2025-01-17 04:26:05,780 [INFO] Step[250/2713]: training loss : 0.9273094701766967 TRAIN  loss dict:  {'classification_loss': 0.9273094701766967}
2025-01-17 04:26:21,850 [INFO] Step[300/2713]: training loss : 0.929459707736969 TRAIN  loss dict:  {'classification_loss': 0.929459707736969}
2025-01-17 04:26:37,907 [INFO] Step[350/2713]: training loss : 0.9319153845310211 TRAIN  loss dict:  {'classification_loss': 0.9319153845310211}
2025-01-17 04:26:53,929 [INFO] Step[400/2713]: training loss : 0.9276770412921905 TRAIN  loss dict:  {'classification_loss': 0.9276770412921905}
2025-01-17 04:27:09,969 [INFO] Step[450/2713]: training loss : 0.9289641451835632 TRAIN  loss dict:  {'classification_loss': 0.9289641451835632}
2025-01-17 04:27:26,039 [INFO] Step[500/2713]: training loss : 0.9411517202854156 TRAIN  loss dict:  {'classification_loss': 0.9411517202854156}
2025-01-17 04:27:42,135 [INFO] Step[550/2713]: training loss : 0.9277131962776184 TRAIN  loss dict:  {'classification_loss': 0.9277131962776184}
2025-01-17 04:27:58,152 [INFO] Step[600/2713]: training loss : 0.9301678788661957 TRAIN  loss dict:  {'classification_loss': 0.9301678788661957}
2025-01-17 04:28:14,258 [INFO] Step[650/2713]: training loss : 0.9280599462985992 TRAIN  loss dict:  {'classification_loss': 0.9280599462985992}
2025-01-17 04:28:30,259 [INFO] Step[700/2713]: training loss : 0.927072001695633 TRAIN  loss dict:  {'classification_loss': 0.927072001695633}
2025-01-17 04:28:46,322 [INFO] Step[750/2713]: training loss : 0.9356942367553711 TRAIN  loss dict:  {'classification_loss': 0.9356942367553711}
2025-01-17 04:29:02,386 [INFO] Step[800/2713]: training loss : 0.9277026462554931 TRAIN  loss dict:  {'classification_loss': 0.9277026462554931}
2025-01-17 04:29:18,492 [INFO] Step[850/2713]: training loss : 0.9279074108600617 TRAIN  loss dict:  {'classification_loss': 0.9279074108600617}
2025-01-17 04:29:34,563 [INFO] Step[900/2713]: training loss : 0.926900554895401 TRAIN  loss dict:  {'classification_loss': 0.926900554895401}
2025-01-17 04:29:50,696 [INFO] Step[950/2713]: training loss : 0.9352442479133606 TRAIN  loss dict:  {'classification_loss': 0.9352442479133606}
2025-01-17 04:30:06,781 [INFO] Step[1000/2713]: training loss : 0.9279755365848541 TRAIN  loss dict:  {'classification_loss': 0.9279755365848541}
2025-01-17 04:30:22,863 [INFO] Step[1050/2713]: training loss : 0.9274488079547882 TRAIN  loss dict:  {'classification_loss': 0.9274488079547882}
2025-01-17 04:30:38,890 [INFO] Step[1100/2713]: training loss : 0.9596061432361602 TRAIN  loss dict:  {'classification_loss': 0.9596061432361602}
2025-01-17 04:30:54,957 [INFO] Step[1150/2713]: training loss : 0.9322629058361054 TRAIN  loss dict:  {'classification_loss': 0.9322629058361054}
2025-01-17 04:31:11,024 [INFO] Step[1200/2713]: training loss : 0.9269205117225647 TRAIN  loss dict:  {'classification_loss': 0.9269205117225647}
2025-01-17 04:31:27,060 [INFO] Step[1250/2713]: training loss : 0.9334287798404693 TRAIN  loss dict:  {'classification_loss': 0.9334287798404693}
2025-01-17 04:31:43,065 [INFO] Step[1300/2713]: training loss : 0.9468758070468902 TRAIN  loss dict:  {'classification_loss': 0.9468758070468902}
2025-01-17 04:31:59,240 [INFO] Step[1350/2713]: training loss : 0.9342506980895996 TRAIN  loss dict:  {'classification_loss': 0.9342506980895996}
2025-01-17 04:32:15,207 [INFO] Step[1400/2713]: training loss : 0.94432608127594 TRAIN  loss dict:  {'classification_loss': 0.94432608127594}
2025-01-17 04:32:31,242 [INFO] Step[1450/2713]: training loss : 0.9284375810623169 TRAIN  loss dict:  {'classification_loss': 0.9284375810623169}
2025-01-17 04:32:47,314 [INFO] Step[1500/2713]: training loss : 0.9273708212375641 TRAIN  loss dict:  {'classification_loss': 0.9273708212375641}
2025-01-17 04:33:03,453 [INFO] Step[1550/2713]: training loss : 0.9279050588607788 TRAIN  loss dict:  {'classification_loss': 0.9279050588607788}
2025-01-17 04:33:19,512 [INFO] Step[1600/2713]: training loss : 0.940459805727005 TRAIN  loss dict:  {'classification_loss': 0.940459805727005}
2025-01-17 04:33:35,577 [INFO] Step[1650/2713]: training loss : 0.9277506256103516 TRAIN  loss dict:  {'classification_loss': 0.9277506256103516}
2025-01-17 04:33:51,553 [INFO] Step[1700/2713]: training loss : 0.9278529965877533 TRAIN  loss dict:  {'classification_loss': 0.9278529965877533}
2025-01-17 04:34:07,624 [INFO] Step[1750/2713]: training loss : 0.9268755328655243 TRAIN  loss dict:  {'classification_loss': 0.9268755328655243}
2025-01-17 04:34:23,614 [INFO] Step[1800/2713]: training loss : 0.9278432846069335 TRAIN  loss dict:  {'classification_loss': 0.9278432846069335}
2025-01-17 04:34:39,666 [INFO] Step[1850/2713]: training loss : 0.9284475255012512 TRAIN  loss dict:  {'classification_loss': 0.9284475255012512}
2025-01-17 04:34:55,779 [INFO] Step[1900/2713]: training loss : 0.9277688241004944 TRAIN  loss dict:  {'classification_loss': 0.9277688241004944}
2025-01-17 04:35:11,797 [INFO] Step[1950/2713]: training loss : 0.9405640017986298 TRAIN  loss dict:  {'classification_loss': 0.9405640017986298}
2025-01-17 04:35:27,931 [INFO] Step[2000/2713]: training loss : 0.930818110704422 TRAIN  loss dict:  {'classification_loss': 0.930818110704422}
2025-01-17 04:35:44,000 [INFO] Step[2050/2713]: training loss : 0.927862092256546 TRAIN  loss dict:  {'classification_loss': 0.927862092256546}
2025-01-17 04:36:00,136 [INFO] Step[2100/2713]: training loss : 0.9278805017471313 TRAIN  loss dict:  {'classification_loss': 0.9278805017471313}
2025-01-17 04:36:16,192 [INFO] Step[2150/2713]: training loss : 0.9673773086071015 TRAIN  loss dict:  {'classification_loss': 0.9673773086071015}
2025-01-17 04:36:32,243 [INFO] Step[2200/2713]: training loss : 0.9575503242015838 TRAIN  loss dict:  {'classification_loss': 0.9575503242015838}
2025-01-17 04:36:48,345 [INFO] Step[2250/2713]: training loss : 0.9271419763565063 TRAIN  loss dict:  {'classification_loss': 0.9271419763565063}
2025-01-17 04:37:04,416 [INFO] Step[2300/2713]: training loss : 0.9741055154800415 TRAIN  loss dict:  {'classification_loss': 0.9741055154800415}
2025-01-17 04:37:20,520 [INFO] Step[2350/2713]: training loss : 0.9272248327732087 TRAIN  loss dict:  {'classification_loss': 0.9272248327732087}
2025-01-17 04:37:36,631 [INFO] Step[2400/2713]: training loss : 0.9276968562602996 TRAIN  loss dict:  {'classification_loss': 0.9276968562602996}
2025-01-17 04:37:52,724 [INFO] Step[2450/2713]: training loss : 0.9303026878833771 TRAIN  loss dict:  {'classification_loss': 0.9303026878833771}
2025-01-17 04:38:08,818 [INFO] Step[2500/2713]: training loss : 0.9308406615257263 TRAIN  loss dict:  {'classification_loss': 0.9308406615257263}
2025-01-17 04:38:24,916 [INFO] Step[2550/2713]: training loss : 0.9277518415451049 TRAIN  loss dict:  {'classification_loss': 0.9277518415451049}
2025-01-17 04:38:41,034 [INFO] Step[2600/2713]: training loss : 0.9282560551166534 TRAIN  loss dict:  {'classification_loss': 0.9282560551166534}
2025-01-17 04:38:57,134 [INFO] Step[2650/2713]: training loss : 0.9275592565536499 TRAIN  loss dict:  {'classification_loss': 0.9275592565536499}
2025-01-17 04:39:13,265 [INFO] Step[2700/2713]: training loss : 0.9278493356704712 TRAIN  loss dict:  {'classification_loss': 0.9278493356704712}
2025-01-17 04:40:31,541 [INFO] Label accuracies statistics:
2025-01-17 04:40:31,541 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.5, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 0.8, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 0.75, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.25, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.25, 262: 0.75, 263: 0.5, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.5, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.75, 313: 0.75, 314: 0.75, 315: 1.0, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 1.0, 356: 0.5, 357: 0.75, 358: 0.5, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 0.75, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 04:40:31,542 [INFO] [69] TRAIN  loss: 0.932799458284023 acc: 0.9984027521808576
2025-01-17 04:40:31,543 [INFO] [69] TRAIN  loss dict: {'classification_loss': 0.932799458284023}
2025-01-17 04:40:31,543 [INFO] [69] VALIDATION loss: 1.9092966207212074 VALIDATION acc: 0.7956112852664576
2025-01-17 04:40:31,543 [INFO] [69] VALIDATION loss dict: {'classification_loss': 1.9092966207212074}
2025-01-17 04:40:31,543 [INFO] 
2025-01-17 04:40:52,160 [INFO] Step[50/2713]: training loss : 0.9363439226150513 TRAIN  loss dict:  {'classification_loss': 0.9363439226150513}
2025-01-17 04:41:08,332 [INFO] Step[100/2713]: training loss : 0.940080634355545 TRAIN  loss dict:  {'classification_loss': 0.940080634355545}
2025-01-17 04:41:24,610 [INFO] Step[150/2713]: training loss : 0.9281626868247986 TRAIN  loss dict:  {'classification_loss': 0.9281626868247986}
2025-01-17 04:41:40,750 [INFO] Step[200/2713]: training loss : 0.9275545370578766 TRAIN  loss dict:  {'classification_loss': 0.9275545370578766}
2025-01-17 04:41:56,905 [INFO] Step[250/2713]: training loss : 0.9310306966304779 TRAIN  loss dict:  {'classification_loss': 0.9310306966304779}
2025-01-17 04:42:13,074 [INFO] Step[300/2713]: training loss : 0.9323824298381805 TRAIN  loss dict:  {'classification_loss': 0.9323824298381805}
2025-01-17 04:42:29,258 [INFO] Step[350/2713]: training loss : 0.9277541399002075 TRAIN  loss dict:  {'classification_loss': 0.9277541399002075}
2025-01-17 04:42:45,401 [INFO] Step[400/2713]: training loss : 0.9299000561237335 TRAIN  loss dict:  {'classification_loss': 0.9299000561237335}
2025-01-17 04:43:01,509 [INFO] Step[450/2713]: training loss : 0.928409194946289 TRAIN  loss dict:  {'classification_loss': 0.928409194946289}
2025-01-17 04:43:17,602 [INFO] Step[500/2713]: training loss : 0.9274415707588196 TRAIN  loss dict:  {'classification_loss': 0.9274415707588196}
2025-01-17 04:43:33,760 [INFO] Step[550/2713]: training loss : 0.9271351969242096 TRAIN  loss dict:  {'classification_loss': 0.9271351969242096}
2025-01-17 04:43:49,882 [INFO] Step[600/2713]: training loss : 0.9506716799736022 TRAIN  loss dict:  {'classification_loss': 0.9506716799736022}
2025-01-17 04:44:06,039 [INFO] Step[650/2713]: training loss : 0.9281646847724915 TRAIN  loss dict:  {'classification_loss': 0.9281646847724915}
2025-01-17 04:44:22,244 [INFO] Step[700/2713]: training loss : 0.9287267589569091 TRAIN  loss dict:  {'classification_loss': 0.9287267589569091}
2025-01-17 04:44:38,432 [INFO] Step[750/2713]: training loss : 0.9276285219192505 TRAIN  loss dict:  {'classification_loss': 0.9276285219192505}
2025-01-17 04:44:54,699 [INFO] Step[800/2713]: training loss : 0.9278626215457916 TRAIN  loss dict:  {'classification_loss': 0.9278626215457916}
2025-01-17 04:45:10,998 [INFO] Step[850/2713]: training loss : 0.927416799068451 TRAIN  loss dict:  {'classification_loss': 0.927416799068451}
2025-01-17 04:45:27,232 [INFO] Step[900/2713]: training loss : 0.953040474653244 TRAIN  loss dict:  {'classification_loss': 0.953040474653244}
2025-01-17 04:45:43,421 [INFO] Step[950/2713]: training loss : 0.9299710965156556 TRAIN  loss dict:  {'classification_loss': 0.9299710965156556}
2025-01-17 04:45:59,585 [INFO] Step[1000/2713]: training loss : 0.927330596446991 TRAIN  loss dict:  {'classification_loss': 0.927330596446991}
2025-01-17 04:46:15,685 [INFO] Step[1050/2713]: training loss : 0.9345343136787414 TRAIN  loss dict:  {'classification_loss': 0.9345343136787414}
2025-01-17 04:46:31,783 [INFO] Step[1100/2713]: training loss : 0.9331273090839386 TRAIN  loss dict:  {'classification_loss': 0.9331273090839386}
2025-01-17 04:46:47,907 [INFO] Step[1150/2713]: training loss : 0.9276295816898346 TRAIN  loss dict:  {'classification_loss': 0.9276295816898346}
2025-01-17 04:47:03,999 [INFO] Step[1200/2713]: training loss : 0.947173798084259 TRAIN  loss dict:  {'classification_loss': 0.947173798084259}
2025-01-17 04:47:20,240 [INFO] Step[1250/2713]: training loss : 0.9270235800743103 TRAIN  loss dict:  {'classification_loss': 0.9270235800743103}
2025-01-17 04:47:36,347 [INFO] Step[1300/2713]: training loss : 0.9272063279151916 TRAIN  loss dict:  {'classification_loss': 0.9272063279151916}
2025-01-17 04:47:52,562 [INFO] Step[1350/2713]: training loss : 0.9278905260562896 TRAIN  loss dict:  {'classification_loss': 0.9278905260562896}
2025-01-17 04:48:08,781 [INFO] Step[1400/2713]: training loss : 0.9273738849163056 TRAIN  loss dict:  {'classification_loss': 0.9273738849163056}
2025-01-17 04:48:24,935 [INFO] Step[1450/2713]: training loss : 0.9601709496974945 TRAIN  loss dict:  {'classification_loss': 0.9601709496974945}
2025-01-17 04:48:41,054 [INFO] Step[1500/2713]: training loss : 0.9396954846382141 TRAIN  loss dict:  {'classification_loss': 0.9396954846382141}
2025-01-17 04:48:57,228 [INFO] Step[1550/2713]: training loss : 0.9271646916866303 TRAIN  loss dict:  {'classification_loss': 0.9271646916866303}
2025-01-17 04:49:13,406 [INFO] Step[1600/2713]: training loss : 0.9272546291351318 TRAIN  loss dict:  {'classification_loss': 0.9272546291351318}
2025-01-17 04:49:29,573 [INFO] Step[1650/2713]: training loss : 0.9335449075698853 TRAIN  loss dict:  {'classification_loss': 0.9335449075698853}
2025-01-17 04:49:45,709 [INFO] Step[1700/2713]: training loss : 0.9343083369731903 TRAIN  loss dict:  {'classification_loss': 0.9343083369731903}
2025-01-17 04:50:01,859 [INFO] Step[1750/2713]: training loss : 0.9283513617515564 TRAIN  loss dict:  {'classification_loss': 0.9283513617515564}
2025-01-17 04:50:18,076 [INFO] Step[1800/2713]: training loss : 0.9270951318740844 TRAIN  loss dict:  {'classification_loss': 0.9270951318740844}
2025-01-17 04:50:34,230 [INFO] Step[1850/2713]: training loss : 0.9459753692150116 TRAIN  loss dict:  {'classification_loss': 0.9459753692150116}
2025-01-17 04:50:50,389 [INFO] Step[1900/2713]: training loss : 0.9345990157127381 TRAIN  loss dict:  {'classification_loss': 0.9345990157127381}
2025-01-17 04:51:06,458 [INFO] Step[1950/2713]: training loss : 0.9822243201732636 TRAIN  loss dict:  {'classification_loss': 0.9822243201732636}
2025-01-17 04:51:22,519 [INFO] Step[2000/2713]: training loss : 0.9274039840698243 TRAIN  loss dict:  {'classification_loss': 0.9274039840698243}
2025-01-17 04:51:38,650 [INFO] Step[2050/2713]: training loss : 0.927264324426651 TRAIN  loss dict:  {'classification_loss': 0.927264324426651}
2025-01-17 04:51:54,729 [INFO] Step[2100/2713]: training loss : 0.9274698686599732 TRAIN  loss dict:  {'classification_loss': 0.9274698686599732}
2025-01-17 04:52:10,880 [INFO] Step[2150/2713]: training loss : 0.9281033945083618 TRAIN  loss dict:  {'classification_loss': 0.9281033945083618}
2025-01-17 04:52:26,958 [INFO] Step[2200/2713]: training loss : 0.9272163343429566 TRAIN  loss dict:  {'classification_loss': 0.9272163343429566}
2025-01-17 04:52:43,012 [INFO] Step[2250/2713]: training loss : 0.9539126241207123 TRAIN  loss dict:  {'classification_loss': 0.9539126241207123}
2025-01-17 04:52:59,048 [INFO] Step[2300/2713]: training loss : 0.9553338074684143 TRAIN  loss dict:  {'classification_loss': 0.9553338074684143}
2025-01-17 04:53:15,103 [INFO] Step[2350/2713]: training loss : 0.9265131616592407 TRAIN  loss dict:  {'classification_loss': 0.9265131616592407}
2025-01-17 04:53:31,168 [INFO] Step[2400/2713]: training loss : 0.927389508485794 TRAIN  loss dict:  {'classification_loss': 0.927389508485794}
2025-01-17 04:53:47,300 [INFO] Step[2450/2713]: training loss : 0.9305714046955109 TRAIN  loss dict:  {'classification_loss': 0.9305714046955109}
2025-01-17 04:54:03,366 [INFO] Step[2500/2713]: training loss : 0.9298019015789032 TRAIN  loss dict:  {'classification_loss': 0.9298019015789032}
2025-01-17 04:54:19,495 [INFO] Step[2550/2713]: training loss : 0.9278750944137574 TRAIN  loss dict:  {'classification_loss': 0.9278750944137574}
2025-01-17 04:54:35,550 [INFO] Step[2600/2713]: training loss : 0.9277577126026153 TRAIN  loss dict:  {'classification_loss': 0.9277577126026153}
2025-01-17 04:54:51,623 [INFO] Step[2650/2713]: training loss : 0.9355342614650727 TRAIN  loss dict:  {'classification_loss': 0.9355342614650727}
2025-01-17 04:55:07,689 [INFO] Step[2700/2713]: training loss : 0.9775997185707093 TRAIN  loss dict:  {'classification_loss': 0.9775997185707093}
2025-01-17 04:56:25,376 [INFO] Label accuracies statistics:
2025-01-17 04:56:25,377 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 0.75, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 1.0, 145: 0.5, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.5, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.25, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 0.25, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.5, 329: 0.75, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.5, 357: 0.75, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 04:56:25,378 [INFO] [70] TRAIN  loss: 0.9343903732176773 acc: 0.997788426096572
2025-01-17 04:56:25,378 [INFO] [70] TRAIN  loss dict: {'classification_loss': 0.9343903732176773}
2025-01-17 04:56:25,379 [INFO] [70] VALIDATION loss: 1.9593821281105055 VALIDATION acc: 0.7887147335423198
2025-01-17 04:56:25,379 [INFO] [70] VALIDATION loss dict: {'classification_loss': 1.9593821281105055}
2025-01-17 04:56:25,379 [INFO] 
2025-01-17 04:56:46,260 [INFO] Step[50/2713]: training loss : 0.9301290500164032 TRAIN  loss dict:  {'classification_loss': 0.9301290500164032}
2025-01-17 04:57:02,286 [INFO] Step[100/2713]: training loss : 0.9268274414539337 TRAIN  loss dict:  {'classification_loss': 0.9268274414539337}
2025-01-17 04:57:18,453 [INFO] Step[150/2713]: training loss : 0.937968248128891 TRAIN  loss dict:  {'classification_loss': 0.937968248128891}
2025-01-17 04:57:34,565 [INFO] Step[200/2713]: training loss : 0.9343201363086701 TRAIN  loss dict:  {'classification_loss': 0.9343201363086701}
2025-01-17 04:57:50,798 [INFO] Step[250/2713]: training loss : 0.9279712402820587 TRAIN  loss dict:  {'classification_loss': 0.9279712402820587}
2025-01-17 04:58:06,885 [INFO] Step[300/2713]: training loss : 0.928948233127594 TRAIN  loss dict:  {'classification_loss': 0.928948233127594}
2025-01-17 04:58:23,054 [INFO] Step[350/2713]: training loss : 0.9274016141891479 TRAIN  loss dict:  {'classification_loss': 0.9274016141891479}
2025-01-17 04:58:39,184 [INFO] Step[400/2713]: training loss : 0.9272201597690582 TRAIN  loss dict:  {'classification_loss': 0.9272201597690582}
2025-01-17 04:58:55,334 [INFO] Step[450/2713]: training loss : 0.9273779571056366 TRAIN  loss dict:  {'classification_loss': 0.9273779571056366}
2025-01-17 04:59:11,500 [INFO] Step[500/2713]: training loss : 0.9540914785861969 TRAIN  loss dict:  {'classification_loss': 0.9540914785861969}
2025-01-17 04:59:27,648 [INFO] Step[550/2713]: training loss : 0.935021300315857 TRAIN  loss dict:  {'classification_loss': 0.935021300315857}
2025-01-17 04:59:43,792 [INFO] Step[600/2713]: training loss : 0.9305040228366852 TRAIN  loss dict:  {'classification_loss': 0.9305040228366852}
2025-01-17 04:59:59,890 [INFO] Step[650/2713]: training loss : 0.9279132437705994 TRAIN  loss dict:  {'classification_loss': 0.9279132437705994}
2025-01-17 05:00:15,994 [INFO] Step[700/2713]: training loss : 0.9377587616443634 TRAIN  loss dict:  {'classification_loss': 0.9377587616443634}
2025-01-17 05:00:32,160 [INFO] Step[750/2713]: training loss : 0.9462644410133362 TRAIN  loss dict:  {'classification_loss': 0.9462644410133362}
2025-01-17 05:00:48,286 [INFO] Step[800/2713]: training loss : 0.9269862306118012 TRAIN  loss dict:  {'classification_loss': 0.9269862306118012}
2025-01-17 05:01:04,457 [INFO] Step[850/2713]: training loss : 0.9272362112998962 TRAIN  loss dict:  {'classification_loss': 0.9272362112998962}
2025-01-17 05:01:20,546 [INFO] Step[900/2713]: training loss : 0.9274370765686035 TRAIN  loss dict:  {'classification_loss': 0.9274370765686035}
2025-01-17 05:01:36,593 [INFO] Step[950/2713]: training loss : 0.9355961525440216 TRAIN  loss dict:  {'classification_loss': 0.9355961525440216}
2025-01-17 05:01:52,648 [INFO] Step[1000/2713]: training loss : 0.9278996431827545 TRAIN  loss dict:  {'classification_loss': 0.9278996431827545}
2025-01-17 05:02:08,741 [INFO] Step[1050/2713]: training loss : 0.933377480506897 TRAIN  loss dict:  {'classification_loss': 0.933377480506897}
2025-01-17 05:02:24,763 [INFO] Step[1100/2713]: training loss : 0.9377270066738128 TRAIN  loss dict:  {'classification_loss': 0.9377270066738128}
2025-01-17 05:02:40,865 [INFO] Step[1150/2713]: training loss : 0.9295199871063232 TRAIN  loss dict:  {'classification_loss': 0.9295199871063232}
2025-01-17 05:02:56,967 [INFO] Step[1200/2713]: training loss : 0.9325967264175415 TRAIN  loss dict:  {'classification_loss': 0.9325967264175415}
2025-01-17 05:03:13,042 [INFO] Step[1250/2713]: training loss : 0.9285360550880433 TRAIN  loss dict:  {'classification_loss': 0.9285360550880433}
2025-01-17 05:03:29,121 [INFO] Step[1300/2713]: training loss : 0.9374087595939636 TRAIN  loss dict:  {'classification_loss': 0.9374087595939636}
2025-01-17 05:03:45,188 [INFO] Step[1350/2713]: training loss : 0.9269385862350464 TRAIN  loss dict:  {'classification_loss': 0.9269385862350464}
2025-01-17 05:04:01,264 [INFO] Step[1400/2713]: training loss : 0.9345808410644532 TRAIN  loss dict:  {'classification_loss': 0.9345808410644532}
2025-01-17 05:04:17,358 [INFO] Step[1450/2713]: training loss : 0.9265513134002685 TRAIN  loss dict:  {'classification_loss': 0.9265513134002685}
2025-01-17 05:04:33,511 [INFO] Step[1500/2713]: training loss : 0.9267656838893891 TRAIN  loss dict:  {'classification_loss': 0.9267656838893891}
2025-01-17 05:04:49,639 [INFO] Step[1550/2713]: training loss : 0.9268046641349792 TRAIN  loss dict:  {'classification_loss': 0.9268046641349792}
2025-01-17 05:05:05,702 [INFO] Step[1600/2713]: training loss : 0.9331373155117035 TRAIN  loss dict:  {'classification_loss': 0.9331373155117035}
2025-01-17 05:05:21,767 [INFO] Step[1650/2713]: training loss : 0.926133451461792 TRAIN  loss dict:  {'classification_loss': 0.926133451461792}
2025-01-17 05:05:37,800 [INFO] Step[1700/2713]: training loss : 0.9267927122116089 TRAIN  loss dict:  {'classification_loss': 0.9267927122116089}
2025-01-17 05:05:53,934 [INFO] Step[1750/2713]: training loss : 0.9282056975364685 TRAIN  loss dict:  {'classification_loss': 0.9282056975364685}
2025-01-17 05:06:09,986 [INFO] Step[1800/2713]: training loss : 0.9271880865097046 TRAIN  loss dict:  {'classification_loss': 0.9271880865097046}
2025-01-17 05:06:26,073 [INFO] Step[1850/2713]: training loss : 0.9267532467842102 TRAIN  loss dict:  {'classification_loss': 0.9267532467842102}
2025-01-17 05:06:42,195 [INFO] Step[1900/2713]: training loss : 0.9268491566181183 TRAIN  loss dict:  {'classification_loss': 0.9268491566181183}
2025-01-17 05:06:58,264 [INFO] Step[1950/2713]: training loss : 0.9283040821552276 TRAIN  loss dict:  {'classification_loss': 0.9283040821552276}
2025-01-17 05:07:14,306 [INFO] Step[2000/2713]: training loss : 0.9276836800575257 TRAIN  loss dict:  {'classification_loss': 0.9276836800575257}
2025-01-17 05:07:30,384 [INFO] Step[2050/2713]: training loss : 0.9264485239982605 TRAIN  loss dict:  {'classification_loss': 0.9264485239982605}
2025-01-17 05:07:46,422 [INFO] Step[2100/2713]: training loss : 0.9274231207370758 TRAIN  loss dict:  {'classification_loss': 0.9274231207370758}
2025-01-17 05:08:02,499 [INFO] Step[2150/2713]: training loss : 0.9274147462844848 TRAIN  loss dict:  {'classification_loss': 0.9274147462844848}
2025-01-17 05:08:18,565 [INFO] Step[2200/2713]: training loss : 0.9278689408302307 TRAIN  loss dict:  {'classification_loss': 0.9278689408302307}
2025-01-17 05:08:34,642 [INFO] Step[2250/2713]: training loss : 0.9271847903728485 TRAIN  loss dict:  {'classification_loss': 0.9271847903728485}
2025-01-17 05:08:50,687 [INFO] Step[2300/2713]: training loss : 0.9275441861152649 TRAIN  loss dict:  {'classification_loss': 0.9275441861152649}
2025-01-17 05:09:06,786 [INFO] Step[2350/2713]: training loss : 0.9289174890518188 TRAIN  loss dict:  {'classification_loss': 0.9289174890518188}
2025-01-17 05:09:22,879 [INFO] Step[2400/2713]: training loss : 0.9270959722995759 TRAIN  loss dict:  {'classification_loss': 0.9270959722995759}
2025-01-17 05:09:39,019 [INFO] Step[2450/2713]: training loss : 0.9264381265640259 TRAIN  loss dict:  {'classification_loss': 0.9264381265640259}
2025-01-17 05:09:55,062 [INFO] Step[2500/2713]: training loss : 0.9270062279701233 TRAIN  loss dict:  {'classification_loss': 0.9270062279701233}
2025-01-17 05:10:11,163 [INFO] Step[2550/2713]: training loss : 0.927152407169342 TRAIN  loss dict:  {'classification_loss': 0.927152407169342}
2025-01-17 05:10:27,234 [INFO] Step[2600/2713]: training loss : 0.926686339378357 TRAIN  loss dict:  {'classification_loss': 0.926686339378357}
2025-01-17 05:10:43,358 [INFO] Step[2650/2713]: training loss : 0.9259900856018066 TRAIN  loss dict:  {'classification_loss': 0.9259900856018066}
2025-01-17 05:10:59,413 [INFO] Step[2700/2713]: training loss : 0.9311305272579193 TRAIN  loss dict:  {'classification_loss': 0.9311305272579193}
2025-01-17 05:12:17,718 [INFO] Label accuracies statistics:
2025-01-17 05:12:17,718 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 1.0, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.5, 262: 1.0, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.25, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 05:12:17,720 [INFO] [71] TRAIN  loss: 0.9300133993913009 acc: 0.9985256173977147
2025-01-17 05:12:17,720 [INFO] [71] TRAIN  loss dict: {'classification_loss': 0.9300133993913009}
2025-01-17 05:12:17,721 [INFO] [71] VALIDATION loss: 1.898071302283079 VALIDATION acc: 0.793730407523511
2025-01-17 05:12:17,721 [INFO] [71] VALIDATION loss dict: {'classification_loss': 1.898071302283079}
2025-01-17 05:12:17,721 [INFO] 
2025-01-17 05:13:00,135 [INFO] Step[50/2713]: training loss : 0.9268552672863006 TRAIN  loss dict:  {'classification_loss': 0.9268552672863006}
2025-01-17 05:13:16,206 [INFO] Step[100/2713]: training loss : 0.926901296377182 TRAIN  loss dict:  {'classification_loss': 0.926901296377182}
2025-01-17 05:13:32,301 [INFO] Step[150/2713]: training loss : 0.9272247409820557 TRAIN  loss dict:  {'classification_loss': 0.9272247409820557}
2025-01-17 05:13:48,338 [INFO] Step[200/2713]: training loss : 0.926932373046875 TRAIN  loss dict:  {'classification_loss': 0.926932373046875}
2025-01-17 05:14:04,438 [INFO] Step[250/2713]: training loss : 0.9277931666374206 TRAIN  loss dict:  {'classification_loss': 0.9277931666374206}
2025-01-17 05:14:20,471 [INFO] Step[300/2713]: training loss : 0.9269244134426117 TRAIN  loss dict:  {'classification_loss': 0.9269244134426117}
2025-01-17 05:14:36,515 [INFO] Step[350/2713]: training loss : 0.9281811499595642 TRAIN  loss dict:  {'classification_loss': 0.9281811499595642}
2025-01-17 05:14:52,569 [INFO] Step[400/2713]: training loss : 0.9268298077583313 TRAIN  loss dict:  {'classification_loss': 0.9268298077583313}
2025-01-17 05:15:08,671 [INFO] Step[450/2713]: training loss : 0.9271365225315094 TRAIN  loss dict:  {'classification_loss': 0.9271365225315094}
2025-01-17 05:15:24,795 [INFO] Step[500/2713]: training loss : 0.9293054151535034 TRAIN  loss dict:  {'classification_loss': 0.9293054151535034}
2025-01-17 05:15:40,892 [INFO] Step[550/2713]: training loss : 0.9275012326240539 TRAIN  loss dict:  {'classification_loss': 0.9275012326240539}
2025-01-17 05:15:56,967 [INFO] Step[600/2713]: training loss : 0.9309527349472045 TRAIN  loss dict:  {'classification_loss': 0.9309527349472045}
2025-01-17 05:16:12,988 [INFO] Step[650/2713]: training loss : 0.9601237666606903 TRAIN  loss dict:  {'classification_loss': 0.9601237666606903}
2025-01-17 05:16:29,097 [INFO] Step[700/2713]: training loss : 0.9282772552967071 TRAIN  loss dict:  {'classification_loss': 0.9282772552967071}
2025-01-17 05:16:45,159 [INFO] Step[750/2713]: training loss : 0.9275178277492523 TRAIN  loss dict:  {'classification_loss': 0.9275178277492523}
2025-01-17 05:17:01,226 [INFO] Step[800/2713]: training loss : 0.9324138522148132 TRAIN  loss dict:  {'classification_loss': 0.9324138522148132}
2025-01-17 05:17:17,308 [INFO] Step[850/2713]: training loss : 0.9297910761833191 TRAIN  loss dict:  {'classification_loss': 0.9297910761833191}
2025-01-17 05:17:33,368 [INFO] Step[900/2713]: training loss : 0.9268215918540954 TRAIN  loss dict:  {'classification_loss': 0.9268215918540954}
2025-01-17 05:17:49,435 [INFO] Step[950/2713]: training loss : 0.9266240048408508 TRAIN  loss dict:  {'classification_loss': 0.9266240048408508}
2025-01-17 05:18:05,460 [INFO] Step[1000/2713]: training loss : 0.9294029676914215 TRAIN  loss dict:  {'classification_loss': 0.9294029676914215}
2025-01-17 05:18:21,531 [INFO] Step[1050/2713]: training loss : 0.9264973521232605 TRAIN  loss dict:  {'classification_loss': 0.9264973521232605}
2025-01-17 05:18:37,583 [INFO] Step[1100/2713]: training loss : 0.9276680529117585 TRAIN  loss dict:  {'classification_loss': 0.9276680529117585}
2025-01-17 05:18:53,659 [INFO] Step[1150/2713]: training loss : 0.9268546319007873 TRAIN  loss dict:  {'classification_loss': 0.9268546319007873}
2025-01-17 05:19:09,789 [INFO] Step[1200/2713]: training loss : 0.9283151507377625 TRAIN  loss dict:  {'classification_loss': 0.9283151507377625}
2025-01-17 05:19:25,839 [INFO] Step[1250/2713]: training loss : 0.9302359974384308 TRAIN  loss dict:  {'classification_loss': 0.9302359974384308}
2025-01-17 05:19:41,954 [INFO] Step[1300/2713]: training loss : 0.9757577979564667 TRAIN  loss dict:  {'classification_loss': 0.9757577979564667}
2025-01-17 05:19:58,049 [INFO] Step[1350/2713]: training loss : 0.9277637696266174 TRAIN  loss dict:  {'classification_loss': 0.9277637696266174}
2025-01-17 05:20:14,136 [INFO] Step[1400/2713]: training loss : 0.9266569817066193 TRAIN  loss dict:  {'classification_loss': 0.9266569817066193}
2025-01-17 05:20:30,171 [INFO] Step[1450/2713]: training loss : 0.9267332065105438 TRAIN  loss dict:  {'classification_loss': 0.9267332065105438}
2025-01-17 05:20:46,311 [INFO] Step[1500/2713]: training loss : 0.9266038537025452 TRAIN  loss dict:  {'classification_loss': 0.9266038537025452}
2025-01-17 05:21:02,359 [INFO] Step[1550/2713]: training loss : 0.927503297328949 TRAIN  loss dict:  {'classification_loss': 0.927503297328949}
2025-01-17 05:21:18,429 [INFO] Step[1600/2713]: training loss : 0.9280389845371246 TRAIN  loss dict:  {'classification_loss': 0.9280389845371246}
2025-01-17 05:21:34,513 [INFO] Step[1650/2713]: training loss : 0.9310569322109222 TRAIN  loss dict:  {'classification_loss': 0.9310569322109222}
2025-01-17 05:21:50,582 [INFO] Step[1700/2713]: training loss : 0.928188294172287 TRAIN  loss dict:  {'classification_loss': 0.928188294172287}
2025-01-17 05:22:06,685 [INFO] Step[1750/2713]: training loss : 0.9370721685886383 TRAIN  loss dict:  {'classification_loss': 0.9370721685886383}
2025-01-17 05:22:22,800 [INFO] Step[1800/2713]: training loss : 0.9270187950134278 TRAIN  loss dict:  {'classification_loss': 0.9270187950134278}
2025-01-17 05:22:38,845 [INFO] Step[1850/2713]: training loss : 0.9275167834758759 TRAIN  loss dict:  {'classification_loss': 0.9275167834758759}
2025-01-17 05:22:54,901 [INFO] Step[1900/2713]: training loss : 0.9327024328708649 TRAIN  loss dict:  {'classification_loss': 0.9327024328708649}
2025-01-17 05:23:11,002 [INFO] Step[1950/2713]: training loss : 0.9399359667301178 TRAIN  loss dict:  {'classification_loss': 0.9399359667301178}
2025-01-17 05:23:27,059 [INFO] Step[2000/2713]: training loss : 0.9266246783733368 TRAIN  loss dict:  {'classification_loss': 0.9266246783733368}
2025-01-17 05:23:43,152 [INFO] Step[2050/2713]: training loss : 0.9274373376369476 TRAIN  loss dict:  {'classification_loss': 0.9274373376369476}
2025-01-17 05:23:59,262 [INFO] Step[2100/2713]: training loss : 0.9270283925533295 TRAIN  loss dict:  {'classification_loss': 0.9270283925533295}
2025-01-17 05:24:15,374 [INFO] Step[2150/2713]: training loss : 0.9342222011089325 TRAIN  loss dict:  {'classification_loss': 0.9342222011089325}
2025-01-17 05:24:31,444 [INFO] Step[2200/2713]: training loss : 0.9812219202518463 TRAIN  loss dict:  {'classification_loss': 0.9812219202518463}
2025-01-17 05:24:47,538 [INFO] Step[2250/2713]: training loss : 0.9261969137191772 TRAIN  loss dict:  {'classification_loss': 0.9261969137191772}
2025-01-17 05:25:03,588 [INFO] Step[2300/2713]: training loss : 0.9287064933776855 TRAIN  loss dict:  {'classification_loss': 0.9287064933776855}
2025-01-17 05:25:19,631 [INFO] Step[2350/2713]: training loss : 0.9271263349056243 TRAIN  loss dict:  {'classification_loss': 0.9271263349056243}
2025-01-17 05:25:35,714 [INFO] Step[2400/2713]: training loss : 0.9266565036773682 TRAIN  loss dict:  {'classification_loss': 0.9266565036773682}
2025-01-17 05:25:51,798 [INFO] Step[2450/2713]: training loss : 0.9280181753635407 TRAIN  loss dict:  {'classification_loss': 0.9280181753635407}
2025-01-17 05:26:07,821 [INFO] Step[2500/2713]: training loss : 0.9270944178104401 TRAIN  loss dict:  {'classification_loss': 0.9270944178104401}
2025-01-17 05:26:23,919 [INFO] Step[2550/2713]: training loss : 0.9266120946407318 TRAIN  loss dict:  {'classification_loss': 0.9266120946407318}
2025-01-17 05:26:39,942 [INFO] Step[2600/2713]: training loss : 0.926464467048645 TRAIN  loss dict:  {'classification_loss': 0.926464467048645}
2025-01-17 05:26:56,005 [INFO] Step[2650/2713]: training loss : 0.9286285293102264 TRAIN  loss dict:  {'classification_loss': 0.9286285293102264}
2025-01-17 05:27:12,045 [INFO] Step[2700/2713]: training loss : 0.9264381921291351 TRAIN  loss dict:  {'classification_loss': 0.9264381921291351}
2025-01-17 05:28:30,103 [INFO] Label accuracies statistics:
2025-01-17 05:28:30,103 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 0.75, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.25, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.75, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.25, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 1.0, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.0, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 1.0, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.0, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 0.75, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 05:28:30,105 [INFO] [72] TRAIN  loss: 0.930799145409422 acc: 0.9991399434820002
2025-01-17 05:28:30,105 [INFO] [72] TRAIN  loss dict: {'classification_loss': 0.930799145409422}
2025-01-17 05:28:30,105 [INFO] [72] VALIDATION loss: 1.9112430572285688 VALIDATION acc: 0.7956112852664576
2025-01-17 05:28:30,105 [INFO] [72] VALIDATION loss dict: {'classification_loss': 1.9112430572285688}
2025-01-17 05:28:30,105 [INFO] 
2025-01-17 05:28:50,617 [INFO] Step[50/2713]: training loss : 0.9267989981174469 TRAIN  loss dict:  {'classification_loss': 0.9267989981174469}
2025-01-17 05:29:06,628 [INFO] Step[100/2713]: training loss : 0.952094806432724 TRAIN  loss dict:  {'classification_loss': 0.952094806432724}
2025-01-17 05:29:22,711 [INFO] Step[150/2713]: training loss : 0.9310516703128815 TRAIN  loss dict:  {'classification_loss': 0.9310516703128815}
2025-01-17 05:29:38,702 [INFO] Step[200/2713]: training loss : 0.9262393581867218 TRAIN  loss dict:  {'classification_loss': 0.9262393581867218}
2025-01-17 05:29:54,881 [INFO] Step[250/2713]: training loss : 0.9265532422065735 TRAIN  loss dict:  {'classification_loss': 0.9265532422065735}
2025-01-17 05:30:10,994 [INFO] Step[300/2713]: training loss : 0.9270117354393005 TRAIN  loss dict:  {'classification_loss': 0.9270117354393005}
2025-01-17 05:30:27,126 [INFO] Step[350/2713]: training loss : 0.9269324684143067 TRAIN  loss dict:  {'classification_loss': 0.9269324684143067}
2025-01-17 05:30:43,154 [INFO] Step[400/2713]: training loss : 0.9275436997413635 TRAIN  loss dict:  {'classification_loss': 0.9275436997413635}
2025-01-17 05:30:59,260 [INFO] Step[450/2713]: training loss : 0.9622383952140808 TRAIN  loss dict:  {'classification_loss': 0.9622383952140808}
2025-01-17 05:31:15,321 [INFO] Step[500/2713]: training loss : 0.9275060617923736 TRAIN  loss dict:  {'classification_loss': 0.9275060617923736}
2025-01-17 05:31:31,424 [INFO] Step[550/2713]: training loss : 0.9269358563423157 TRAIN  loss dict:  {'classification_loss': 0.9269358563423157}
2025-01-17 05:31:47,448 [INFO] Step[600/2713]: training loss : 0.926764577627182 TRAIN  loss dict:  {'classification_loss': 0.926764577627182}
2025-01-17 05:32:03,580 [INFO] Step[650/2713]: training loss : 0.9264039897918701 TRAIN  loss dict:  {'classification_loss': 0.9264039897918701}
2025-01-17 05:32:19,646 [INFO] Step[700/2713]: training loss : 0.927057672739029 TRAIN  loss dict:  {'classification_loss': 0.927057672739029}
2025-01-17 05:32:35,701 [INFO] Step[750/2713]: training loss : 0.9443529570102691 TRAIN  loss dict:  {'classification_loss': 0.9443529570102691}
2025-01-17 05:32:51,732 [INFO] Step[800/2713]: training loss : 0.9278383767604828 TRAIN  loss dict:  {'classification_loss': 0.9278383767604828}
2025-01-17 05:33:07,835 [INFO] Step[850/2713]: training loss : 0.9271477055549622 TRAIN  loss dict:  {'classification_loss': 0.9271477055549622}
2025-01-17 05:33:23,893 [INFO] Step[900/2713]: training loss : 0.9264757239818573 TRAIN  loss dict:  {'classification_loss': 0.9264757239818573}
2025-01-17 05:33:39,970 [INFO] Step[950/2713]: training loss : 0.9466837930679322 TRAIN  loss dict:  {'classification_loss': 0.9466837930679322}
2025-01-17 05:33:56,141 [INFO] Step[1000/2713]: training loss : 0.9276776921749115 TRAIN  loss dict:  {'classification_loss': 0.9276776921749115}
2025-01-17 05:34:12,551 [INFO] Step[1050/2713]: training loss : 0.9268558597564698 TRAIN  loss dict:  {'classification_loss': 0.9268558597564698}
2025-01-17 05:34:28,862 [INFO] Step[1100/2713]: training loss : 0.9266600406169891 TRAIN  loss dict:  {'classification_loss': 0.9266600406169891}
2025-01-17 05:34:45,218 [INFO] Step[1150/2713]: training loss : 0.9276542484760284 TRAIN  loss dict:  {'classification_loss': 0.9276542484760284}
2025-01-17 05:35:01,455 [INFO] Step[1200/2713]: training loss : 0.9539810013771057 TRAIN  loss dict:  {'classification_loss': 0.9539810013771057}
2025-01-17 05:35:17,849 [INFO] Step[1250/2713]: training loss : 0.9266599440574645 TRAIN  loss dict:  {'classification_loss': 0.9266599440574645}
2025-01-17 05:35:34,109 [INFO] Step[1300/2713]: training loss : 0.9269786858558655 TRAIN  loss dict:  {'classification_loss': 0.9269786858558655}
2025-01-17 05:35:50,384 [INFO] Step[1350/2713]: training loss : 0.9268310105800629 TRAIN  loss dict:  {'classification_loss': 0.9268310105800629}
2025-01-17 05:36:06,666 [INFO] Step[1400/2713]: training loss : 0.926336966753006 TRAIN  loss dict:  {'classification_loss': 0.926336966753006}
2025-01-17 05:36:22,990 [INFO] Step[1450/2713]: training loss : 0.9299548876285553 TRAIN  loss dict:  {'classification_loss': 0.9299548876285553}
2025-01-17 05:36:39,247 [INFO] Step[1500/2713]: training loss : 0.926789083480835 TRAIN  loss dict:  {'classification_loss': 0.926789083480835}
2025-01-17 05:36:55,609 [INFO] Step[1550/2713]: training loss : 0.9275065159797669 TRAIN  loss dict:  {'classification_loss': 0.9275065159797669}
2025-01-17 05:37:11,930 [INFO] Step[1600/2713]: training loss : 0.927089124917984 TRAIN  loss dict:  {'classification_loss': 0.927089124917984}
2025-01-17 05:37:28,225 [INFO] Step[1650/2713]: training loss : 0.9270242357254028 TRAIN  loss dict:  {'classification_loss': 0.9270242357254028}
2025-01-17 05:37:44,482 [INFO] Step[1700/2713]: training loss : 0.955967184305191 TRAIN  loss dict:  {'classification_loss': 0.955967184305191}
2025-01-17 05:38:00,780 [INFO] Step[1750/2713]: training loss : 0.930164144039154 TRAIN  loss dict:  {'classification_loss': 0.930164144039154}
2025-01-17 05:38:17,056 [INFO] Step[1800/2713]: training loss : 0.9328970158100128 TRAIN  loss dict:  {'classification_loss': 0.9328970158100128}
2025-01-17 05:38:33,345 [INFO] Step[1850/2713]: training loss : 0.9265903151035308 TRAIN  loss dict:  {'classification_loss': 0.9265903151035308}
2025-01-17 05:38:49,643 [INFO] Step[1900/2713]: training loss : 0.9273795402050018 TRAIN  loss dict:  {'classification_loss': 0.9273795402050018}
2025-01-17 05:39:05,971 [INFO] Step[1950/2713]: training loss : 0.9267255961894989 TRAIN  loss dict:  {'classification_loss': 0.9267255961894989}
2025-01-17 05:39:22,234 [INFO] Step[2000/2713]: training loss : 0.9264071047306061 TRAIN  loss dict:  {'classification_loss': 0.9264071047306061}
2025-01-17 05:39:38,476 [INFO] Step[2050/2713]: training loss : 0.9267606580257416 TRAIN  loss dict:  {'classification_loss': 0.9267606580257416}
2025-01-17 05:39:54,712 [INFO] Step[2100/2713]: training loss : 0.927980477809906 TRAIN  loss dict:  {'classification_loss': 0.927980477809906}
2025-01-17 05:40:11,063 [INFO] Step[2150/2713]: training loss : 0.9275455033779144 TRAIN  loss dict:  {'classification_loss': 0.9275455033779144}
2025-01-17 05:40:27,354 [INFO] Step[2200/2713]: training loss : 0.9267704021930695 TRAIN  loss dict:  {'classification_loss': 0.9267704021930695}
2025-01-17 05:40:43,589 [INFO] Step[2250/2713]: training loss : 0.9270835113525391 TRAIN  loss dict:  {'classification_loss': 0.9270835113525391}
2025-01-17 05:40:59,847 [INFO] Step[2300/2713]: training loss : 0.9270988261699676 TRAIN  loss dict:  {'classification_loss': 0.9270988261699676}
2025-01-17 05:41:16,233 [INFO] Step[2350/2713]: training loss : 0.9265377819538116 TRAIN  loss dict:  {'classification_loss': 0.9265377819538116}
2025-01-17 05:41:32,486 [INFO] Step[2400/2713]: training loss : 0.9272201800346375 TRAIN  loss dict:  {'classification_loss': 0.9272201800346375}
2025-01-17 05:41:48,890 [INFO] Step[2450/2713]: training loss : 0.9281663393974304 TRAIN  loss dict:  {'classification_loss': 0.9281663393974304}
2025-01-17 05:42:05,130 [INFO] Step[2500/2713]: training loss : 0.9267207598686218 TRAIN  loss dict:  {'classification_loss': 0.9267207598686218}
2025-01-17 05:42:21,428 [INFO] Step[2550/2713]: training loss : 0.927153480052948 TRAIN  loss dict:  {'classification_loss': 0.927153480052948}
2025-01-17 05:42:37,752 [INFO] Step[2600/2713]: training loss : 0.9269224774837493 TRAIN  loss dict:  {'classification_loss': 0.9269224774837493}
2025-01-17 05:42:54,062 [INFO] Step[2650/2713]: training loss : 0.9400293719768524 TRAIN  loss dict:  {'classification_loss': 0.9400293719768524}
2025-01-17 05:43:10,442 [INFO] Step[2700/2713]: training loss : 0.9275428414344787 TRAIN  loss dict:  {'classification_loss': 0.9275428414344787}
2025-01-17 05:44:28,616 [INFO] Label accuracies statistics:
2025-01-17 05:44:28,616 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.5, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.25, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 1.0, 261: 0.25, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.5, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.5, 379: 1.0, 380: 0.75, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 05:44:28,618 [INFO] [73] TRAIN  loss: 0.9303754245580187 acc: 0.9990170782651432
2025-01-17 05:44:28,618 [INFO] [73] TRAIN  loss dict: {'classification_loss': 0.9303754245580187}
2025-01-17 05:44:28,618 [INFO] [73] VALIDATION loss: 1.9338090022031527 VALIDATION acc: 0.7956112852664576
2025-01-17 05:44:28,618 [INFO] [73] VALIDATION loss dict: {'classification_loss': 1.9338090022031527}
2025-01-17 05:44:28,618 [INFO] 
2025-01-17 05:44:59,371 [INFO] Step[50/2713]: training loss : 0.9274449491500855 TRAIN  loss dict:  {'classification_loss': 0.9274449491500855}
2025-01-17 05:45:15,638 [INFO] Step[100/2713]: training loss : 0.9271197581291198 TRAIN  loss dict:  {'classification_loss': 0.9271197581291198}
2025-01-17 05:45:31,873 [INFO] Step[150/2713]: training loss : 0.9267976880073547 TRAIN  loss dict:  {'classification_loss': 0.9267976880073547}
2025-01-17 05:45:48,090 [INFO] Step[200/2713]: training loss : 0.9267845451831818 TRAIN  loss dict:  {'classification_loss': 0.9267845451831818}
2025-01-17 05:46:04,340 [INFO] Step[250/2713]: training loss : 0.9596397149562835 TRAIN  loss dict:  {'classification_loss': 0.9596397149562835}
2025-01-17 05:46:20,542 [INFO] Step[300/2713]: training loss : 0.9262958705425263 TRAIN  loss dict:  {'classification_loss': 0.9262958705425263}
2025-01-17 05:46:36,805 [INFO] Step[350/2713]: training loss : 0.926930958032608 TRAIN  loss dict:  {'classification_loss': 0.926930958032608}
2025-01-17 05:46:53,039 [INFO] Step[400/2713]: training loss : 0.9265565741062164 TRAIN  loss dict:  {'classification_loss': 0.9265565741062164}
2025-01-17 05:47:09,254 [INFO] Step[450/2713]: training loss : 0.9271863496303558 TRAIN  loss dict:  {'classification_loss': 0.9271863496303558}
2025-01-17 05:47:25,480 [INFO] Step[500/2713]: training loss : 0.9270701956748962 TRAIN  loss dict:  {'classification_loss': 0.9270701956748962}
2025-01-17 05:47:41,644 [INFO] Step[550/2713]: training loss : 0.9267370128631591 TRAIN  loss dict:  {'classification_loss': 0.9267370128631591}
2025-01-17 05:47:57,781 [INFO] Step[600/2713]: training loss : 0.9258898425102234 TRAIN  loss dict:  {'classification_loss': 0.9258898425102234}
2025-01-17 05:48:14,022 [INFO] Step[650/2713]: training loss : 0.9267743968963623 TRAIN  loss dict:  {'classification_loss': 0.9267743968963623}
2025-01-17 05:48:30,219 [INFO] Step[700/2713]: training loss : 0.9266604089736938 TRAIN  loss dict:  {'classification_loss': 0.9266604089736938}
2025-01-17 05:48:46,459 [INFO] Step[750/2713]: training loss : 0.944089058637619 TRAIN  loss dict:  {'classification_loss': 0.944089058637619}
2025-01-17 05:49:02,717 [INFO] Step[800/2713]: training loss : 0.9303486096858978 TRAIN  loss dict:  {'classification_loss': 0.9303486096858978}
2025-01-17 05:49:19,008 [INFO] Step[850/2713]: training loss : 0.9266014969348908 TRAIN  loss dict:  {'classification_loss': 0.9266014969348908}
2025-01-17 05:49:35,241 [INFO] Step[900/2713]: training loss : 0.9307218062877655 TRAIN  loss dict:  {'classification_loss': 0.9307218062877655}
2025-01-17 05:49:51,412 [INFO] Step[950/2713]: training loss : 0.934617726802826 TRAIN  loss dict:  {'classification_loss': 0.934617726802826}
2025-01-17 05:50:07,610 [INFO] Step[1000/2713]: training loss : 0.9271328008174896 TRAIN  loss dict:  {'classification_loss': 0.9271328008174896}
2025-01-17 05:50:23,925 [INFO] Step[1050/2713]: training loss : 0.9271997249126435 TRAIN  loss dict:  {'classification_loss': 0.9271997249126435}
2025-01-17 05:50:40,122 [INFO] Step[1100/2713]: training loss : 0.9373856770992279 TRAIN  loss dict:  {'classification_loss': 0.9373856770992279}
2025-01-17 05:50:56,369 [INFO] Step[1150/2713]: training loss : 0.9338390874862671 TRAIN  loss dict:  {'classification_loss': 0.9338390874862671}
2025-01-17 05:51:12,637 [INFO] Step[1200/2713]: training loss : 0.9266418766975403 TRAIN  loss dict:  {'classification_loss': 0.9266418766975403}
2025-01-17 05:51:28,824 [INFO] Step[1250/2713]: training loss : 0.9313244354724884 TRAIN  loss dict:  {'classification_loss': 0.9313244354724884}
2025-01-17 05:51:45,058 [INFO] Step[1300/2713]: training loss : 0.9300201451778412 TRAIN  loss dict:  {'classification_loss': 0.9300201451778412}
2025-01-17 05:52:01,333 [INFO] Step[1350/2713]: training loss : 0.9282299530506134 TRAIN  loss dict:  {'classification_loss': 0.9282299530506134}
2025-01-17 05:52:17,491 [INFO] Step[1400/2713]: training loss : 0.9351609396934509 TRAIN  loss dict:  {'classification_loss': 0.9351609396934509}
2025-01-17 05:52:33,658 [INFO] Step[1450/2713]: training loss : 0.9277149367332459 TRAIN  loss dict:  {'classification_loss': 0.9277149367332459}
2025-01-17 05:52:49,864 [INFO] Step[1500/2713]: training loss : 0.9350822675228119 TRAIN  loss dict:  {'classification_loss': 0.9350822675228119}
2025-01-17 05:53:06,176 [INFO] Step[1550/2713]: training loss : 0.9443188846111298 TRAIN  loss dict:  {'classification_loss': 0.9443188846111298}
2025-01-17 05:53:22,399 [INFO] Step[1600/2713]: training loss : 0.947224292755127 TRAIN  loss dict:  {'classification_loss': 0.947224292755127}
2025-01-17 05:53:38,588 [INFO] Step[1650/2713]: training loss : 0.9268021154403686 TRAIN  loss dict:  {'classification_loss': 0.9268021154403686}
2025-01-17 05:53:54,869 [INFO] Step[1700/2713]: training loss : 0.9271836221218109 TRAIN  loss dict:  {'classification_loss': 0.9271836221218109}
2025-01-17 05:54:11,091 [INFO] Step[1750/2713]: training loss : 0.9271007764339447 TRAIN  loss dict:  {'classification_loss': 0.9271007764339447}
2025-01-17 05:54:27,278 [INFO] Step[1800/2713]: training loss : 0.926282708644867 TRAIN  loss dict:  {'classification_loss': 0.926282708644867}
2025-01-17 05:54:43,626 [INFO] Step[1850/2713]: training loss : 0.9262179064750672 TRAIN  loss dict:  {'classification_loss': 0.9262179064750672}
2025-01-17 05:54:59,857 [INFO] Step[1900/2713]: training loss : 0.9342065489292145 TRAIN  loss dict:  {'classification_loss': 0.9342065489292145}
2025-01-17 05:55:16,050 [INFO] Step[1950/2713]: training loss : 0.9276376295089722 TRAIN  loss dict:  {'classification_loss': 0.9276376295089722}
2025-01-17 05:55:32,232 [INFO] Step[2000/2713]: training loss : 0.9272770130634308 TRAIN  loss dict:  {'classification_loss': 0.9272770130634308}
2025-01-17 05:55:48,417 [INFO] Step[2050/2713]: training loss : 0.9280320167541504 TRAIN  loss dict:  {'classification_loss': 0.9280320167541504}
2025-01-17 05:56:04,586 [INFO] Step[2100/2713]: training loss : 0.9266527223587037 TRAIN  loss dict:  {'classification_loss': 0.9266527223587037}
2025-01-17 05:56:20,720 [INFO] Step[2150/2713]: training loss : 0.9296958243846893 TRAIN  loss dict:  {'classification_loss': 0.9296958243846893}
2025-01-17 05:56:36,972 [INFO] Step[2200/2713]: training loss : 0.9265948641300201 TRAIN  loss dict:  {'classification_loss': 0.9265948641300201}
2025-01-17 05:56:53,189 [INFO] Step[2250/2713]: training loss : 0.9299585354328156 TRAIN  loss dict:  {'classification_loss': 0.9299585354328156}
2025-01-17 05:57:09,396 [INFO] Step[2300/2713]: training loss : 0.9265147483348847 TRAIN  loss dict:  {'classification_loss': 0.9265147483348847}
2025-01-17 05:57:25,628 [INFO] Step[2350/2713]: training loss : 0.9276407361030579 TRAIN  loss dict:  {'classification_loss': 0.9276407361030579}
2025-01-17 05:57:41,706 [INFO] Step[2400/2713]: training loss : 0.9275962936878205 TRAIN  loss dict:  {'classification_loss': 0.9275962936878205}
2025-01-17 05:57:57,773 [INFO] Step[2450/2713]: training loss : 0.9264845597743988 TRAIN  loss dict:  {'classification_loss': 0.9264845597743988}
2025-01-17 05:58:13,904 [INFO] Step[2500/2713]: training loss : 0.9265475153923035 TRAIN  loss dict:  {'classification_loss': 0.9265475153923035}
2025-01-17 05:58:30,013 [INFO] Step[2550/2713]: training loss : 0.9403670382499695 TRAIN  loss dict:  {'classification_loss': 0.9403670382499695}
2025-01-17 05:58:46,183 [INFO] Step[2600/2713]: training loss : 0.9281472039222717 TRAIN  loss dict:  {'classification_loss': 0.9281472039222717}
2025-01-17 05:59:02,286 [INFO] Step[2650/2713]: training loss : 0.9267872536182403 TRAIN  loss dict:  {'classification_loss': 0.9267872536182403}
2025-01-17 05:59:18,479 [INFO] Step[2700/2713]: training loss : 0.926772518157959 TRAIN  loss dict:  {'classification_loss': 0.926772518157959}
2025-01-17 06:00:37,027 [INFO] Label accuracies statistics:
2025-01-17 06:00:37,028 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 0.75, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.0, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.5, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.5, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.25, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 1.0, 220: 1.0, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.25, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.25, 262: 1.0, 263: 0.5, 264: 0.75, 265: 0.75, 266: 1.0, 267: 1.0, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 1.0, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 06:00:37,029 [INFO] [74] TRAIN  loss: 0.9300928851181055 acc: 0.9988942130482861
2025-01-17 06:00:37,029 [INFO] [74] TRAIN  loss dict: {'classification_loss': 0.9300928851181055}
2025-01-17 06:00:37,030 [INFO] [74] VALIDATION loss: 1.9536502771359636 VALIDATION acc: 0.7931034482758621
2025-01-17 06:00:37,030 [INFO] [74] VALIDATION loss dict: {'classification_loss': 1.9536502771359636}
2025-01-17 06:00:37,030 [INFO] 
2025-01-17 06:00:57,693 [INFO] Step[50/2713]: training loss : 0.9791682994365692 TRAIN  loss dict:  {'classification_loss': 0.9791682994365692}
2025-01-17 06:01:13,765 [INFO] Step[100/2713]: training loss : 0.9267921626567841 TRAIN  loss dict:  {'classification_loss': 0.9267921626567841}
2025-01-17 06:01:29,867 [INFO] Step[150/2713]: training loss : 0.9281246364116669 TRAIN  loss dict:  {'classification_loss': 0.9281246364116669}
2025-01-17 06:01:45,883 [INFO] Step[200/2713]: training loss : 0.9292031610012055 TRAIN  loss dict:  {'classification_loss': 0.9292031610012055}
2025-01-17 06:02:01,902 [INFO] Step[250/2713]: training loss : 0.9268241047859191 TRAIN  loss dict:  {'classification_loss': 0.9268241047859191}
2025-01-17 06:02:17,925 [INFO] Step[300/2713]: training loss : 0.9268756544589997 TRAIN  loss dict:  {'classification_loss': 0.9268756544589997}
2025-01-17 06:02:33,981 [INFO] Step[350/2713]: training loss : 0.9265505003929139 TRAIN  loss dict:  {'classification_loss': 0.9265505003929139}
2025-01-17 06:02:50,143 [INFO] Step[400/2713]: training loss : 0.9269252228736877 TRAIN  loss dict:  {'classification_loss': 0.9269252228736877}
2025-01-17 06:03:06,321 [INFO] Step[450/2713]: training loss : 0.9279107773303985 TRAIN  loss dict:  {'classification_loss': 0.9279107773303985}
2025-01-17 06:03:22,378 [INFO] Step[500/2713]: training loss : 0.93769069314003 TRAIN  loss dict:  {'classification_loss': 0.93769069314003}
2025-01-17 06:03:38,488 [INFO] Step[550/2713]: training loss : 0.9264149582386016 TRAIN  loss dict:  {'classification_loss': 0.9264149582386016}
2025-01-17 06:03:54,550 [INFO] Step[600/2713]: training loss : 0.9269422006607055 TRAIN  loss dict:  {'classification_loss': 0.9269422006607055}
2025-01-17 06:04:10,736 [INFO] Step[650/2713]: training loss : 0.9271119570732117 TRAIN  loss dict:  {'classification_loss': 0.9271119570732117}
2025-01-17 06:04:26,894 [INFO] Step[700/2713]: training loss : 0.9290173637866974 TRAIN  loss dict:  {'classification_loss': 0.9290173637866974}
2025-01-17 06:04:42,959 [INFO] Step[750/2713]: training loss : 0.9633822286128998 TRAIN  loss dict:  {'classification_loss': 0.9633822286128998}
2025-01-17 06:04:58,896 [INFO] Step[800/2713]: training loss : 0.9281806397438049 TRAIN  loss dict:  {'classification_loss': 0.9281806397438049}
2025-01-17 06:05:14,889 [INFO] Step[850/2713]: training loss : 0.9269105052947998 TRAIN  loss dict:  {'classification_loss': 0.9269105052947998}
2025-01-17 06:05:30,952 [INFO] Step[900/2713]: training loss : 0.9273170602321624 TRAIN  loss dict:  {'classification_loss': 0.9273170602321624}
2025-01-17 06:05:47,036 [INFO] Step[950/2713]: training loss : 0.9272996056079864 TRAIN  loss dict:  {'classification_loss': 0.9272996056079864}
2025-01-17 06:06:03,039 [INFO] Step[1000/2713]: training loss : 0.9264151358604431 TRAIN  loss dict:  {'classification_loss': 0.9264151358604431}
2025-01-17 06:06:19,090 [INFO] Step[1050/2713]: training loss : 0.9327575945854187 TRAIN  loss dict:  {'classification_loss': 0.9327575945854187}
2025-01-17 06:06:35,136 [INFO] Step[1100/2713]: training loss : 0.9273654901981354 TRAIN  loss dict:  {'classification_loss': 0.9273654901981354}
2025-01-17 06:06:51,287 [INFO] Step[1150/2713]: training loss : 0.9265609443187713 TRAIN  loss dict:  {'classification_loss': 0.9265609443187713}
2025-01-17 06:07:07,384 [INFO] Step[1200/2713]: training loss : 0.9264110088348388 TRAIN  loss dict:  {'classification_loss': 0.9264110088348388}
2025-01-17 06:07:23,393 [INFO] Step[1250/2713]: training loss : 0.9263832330703735 TRAIN  loss dict:  {'classification_loss': 0.9263832330703735}
2025-01-17 06:07:39,440 [INFO] Step[1300/2713]: training loss : 0.9260781562328338 TRAIN  loss dict:  {'classification_loss': 0.9260781562328338}
2025-01-17 06:07:55,507 [INFO] Step[1350/2713]: training loss : 0.9276368856430054 TRAIN  loss dict:  {'classification_loss': 0.9276368856430054}
2025-01-17 06:08:11,555 [INFO] Step[1400/2713]: training loss : 0.9271623528003693 TRAIN  loss dict:  {'classification_loss': 0.9271623528003693}
2025-01-17 06:08:27,602 [INFO] Step[1450/2713]: training loss : 0.957165139913559 TRAIN  loss dict:  {'classification_loss': 0.957165139913559}
2025-01-17 06:08:43,647 [INFO] Step[1500/2713]: training loss : 0.9266993236541748 TRAIN  loss dict:  {'classification_loss': 0.9266993236541748}
2025-01-17 06:08:59,793 [INFO] Step[1550/2713]: training loss : 0.9264589321613311 TRAIN  loss dict:  {'classification_loss': 0.9264589321613311}
2025-01-17 06:09:15,786 [INFO] Step[1600/2713]: training loss : 0.927433580160141 TRAIN  loss dict:  {'classification_loss': 0.927433580160141}
2025-01-17 06:09:31,854 [INFO] Step[1650/2713]: training loss : 0.9741549110412597 TRAIN  loss dict:  {'classification_loss': 0.9741549110412597}
2025-01-17 06:09:47,853 [INFO] Step[1700/2713]: training loss : 0.9268786489963532 TRAIN  loss dict:  {'classification_loss': 0.9268786489963532}
2025-01-17 06:10:03,914 [INFO] Step[1750/2713]: training loss : 0.9270342409610748 TRAIN  loss dict:  {'classification_loss': 0.9270342409610748}
2025-01-17 06:10:20,067 [INFO] Step[1800/2713]: training loss : 0.926784074306488 TRAIN  loss dict:  {'classification_loss': 0.926784074306488}
2025-01-17 06:10:36,136 [INFO] Step[1850/2713]: training loss : 0.9271137070655823 TRAIN  loss dict:  {'classification_loss': 0.9271137070655823}
2025-01-17 06:10:52,178 [INFO] Step[1900/2713]: training loss : 0.9290752184391021 TRAIN  loss dict:  {'classification_loss': 0.9290752184391021}
2025-01-17 06:11:08,254 [INFO] Step[1950/2713]: training loss : 0.9264871883392334 TRAIN  loss dict:  {'classification_loss': 0.9264871883392334}
2025-01-17 06:11:24,351 [INFO] Step[2000/2713]: training loss : 0.9471591472625732 TRAIN  loss dict:  {'classification_loss': 0.9471591472625732}
2025-01-17 06:11:40,435 [INFO] Step[2050/2713]: training loss : 0.9277146875858306 TRAIN  loss dict:  {'classification_loss': 0.9277146875858306}
2025-01-17 06:11:56,468 [INFO] Step[2100/2713]: training loss : 0.926837637424469 TRAIN  loss dict:  {'classification_loss': 0.926837637424469}
2025-01-17 06:12:12,547 [INFO] Step[2150/2713]: training loss : 0.9274376106262207 TRAIN  loss dict:  {'classification_loss': 0.9274376106262207}
2025-01-17 06:12:28,627 [INFO] Step[2200/2713]: training loss : 0.9344950687885284 TRAIN  loss dict:  {'classification_loss': 0.9344950687885284}
2025-01-17 06:12:44,757 [INFO] Step[2250/2713]: training loss : 0.9266868078708649 TRAIN  loss dict:  {'classification_loss': 0.9266868078708649}
2025-01-17 06:13:00,833 [INFO] Step[2300/2713]: training loss : 0.9268297410011291 TRAIN  loss dict:  {'classification_loss': 0.9268297410011291}
2025-01-17 06:13:16,912 [INFO] Step[2350/2713]: training loss : 0.9261881959438324 TRAIN  loss dict:  {'classification_loss': 0.9261881959438324}
2025-01-17 06:13:32,981 [INFO] Step[2400/2713]: training loss : 0.9452546525001526 TRAIN  loss dict:  {'classification_loss': 0.9452546525001526}
2025-01-17 06:13:49,035 [INFO] Step[2450/2713]: training loss : 0.9272535967826844 TRAIN  loss dict:  {'classification_loss': 0.9272535967826844}
2025-01-17 06:14:05,159 [INFO] Step[2500/2713]: training loss : 0.9311647570133209 TRAIN  loss dict:  {'classification_loss': 0.9311647570133209}
2025-01-17 06:14:21,310 [INFO] Step[2550/2713]: training loss : 0.9262855160236358 TRAIN  loss dict:  {'classification_loss': 0.9262855160236358}
2025-01-17 06:14:37,404 [INFO] Step[2600/2713]: training loss : 0.9266609537601471 TRAIN  loss dict:  {'classification_loss': 0.9266609537601471}
2025-01-17 06:14:53,472 [INFO] Step[2650/2713]: training loss : 0.9363962352275849 TRAIN  loss dict:  {'classification_loss': 0.9363962352275849}
2025-01-17 06:15:09,600 [INFO] Step[2700/2713]: training loss : 0.9574076092243194 TRAIN  loss dict:  {'classification_loss': 0.9574076092243194}
2025-01-17 06:16:27,508 [INFO] Label accuracies statistics:
2025-01-17 06:16:27,508 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.25, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.5, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 0.75, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 0.75, 197: 1.0, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 1.0, 205: 1.0, 206: 0.5, 207: 0.5, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.0, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 1.0, 261: 0.5, 262: 1.0, 263: 0.5, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.5, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.5, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.25, 353: 0.25, 354: 0.25, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.5, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 06:16:27,510 [INFO] [75] TRAIN  loss: 0.9320944916090314 acc: 0.9985256173977147
2025-01-17 06:16:27,510 [INFO] [75] TRAIN  loss dict: {'classification_loss': 0.9320944916090314}
2025-01-17 06:16:27,510 [INFO] [75] VALIDATION loss: 1.9476815834081262 VALIDATION acc: 0.7905956112852665
2025-01-17 06:16:27,510 [INFO] [75] VALIDATION loss dict: {'classification_loss': 1.9476815834081262}
2025-01-17 06:16:27,510 [INFO] 
2025-01-17 06:16:48,560 [INFO] Step[50/2713]: training loss : 0.927129156589508 TRAIN  loss dict:  {'classification_loss': 0.927129156589508}
2025-01-17 06:17:04,598 [INFO] Step[100/2713]: training loss : 0.9265015316009522 TRAIN  loss dict:  {'classification_loss': 0.9265015316009522}
2025-01-17 06:17:20,722 [INFO] Step[150/2713]: training loss : 0.9269267427921295 TRAIN  loss dict:  {'classification_loss': 0.9269267427921295}
2025-01-17 06:17:36,799 [INFO] Step[200/2713]: training loss : 0.9270715951919556 TRAIN  loss dict:  {'classification_loss': 0.9270715951919556}
2025-01-17 06:17:52,889 [INFO] Step[250/2713]: training loss : 0.9271839618682861 TRAIN  loss dict:  {'classification_loss': 0.9271839618682861}
2025-01-17 06:18:08,998 [INFO] Step[300/2713]: training loss : 0.9280603325366974 TRAIN  loss dict:  {'classification_loss': 0.9280603325366974}
2025-01-17 06:18:25,133 [INFO] Step[350/2713]: training loss : 0.9267325150966644 TRAIN  loss dict:  {'classification_loss': 0.9267325150966644}
2025-01-17 06:18:41,210 [INFO] Step[400/2713]: training loss : 0.9290367352962494 TRAIN  loss dict:  {'classification_loss': 0.9290367352962494}
2025-01-17 06:18:57,244 [INFO] Step[450/2713]: training loss : 0.9269829046726227 TRAIN  loss dict:  {'classification_loss': 0.9269829046726227}
2025-01-17 06:19:13,306 [INFO] Step[500/2713]: training loss : 0.9307390069961547 TRAIN  loss dict:  {'classification_loss': 0.9307390069961547}
2025-01-17 06:19:29,446 [INFO] Step[550/2713]: training loss : 0.9267658007144928 TRAIN  loss dict:  {'classification_loss': 0.9267658007144928}
2025-01-17 06:19:45,468 [INFO] Step[600/2713]: training loss : 0.9267687153816223 TRAIN  loss dict:  {'classification_loss': 0.9267687153816223}
2025-01-17 06:20:01,589 [INFO] Step[650/2713]: training loss : 0.926503313779831 TRAIN  loss dict:  {'classification_loss': 0.926503313779831}
2025-01-17 06:20:17,610 [INFO] Step[700/2713]: training loss : 0.9259483587741851 TRAIN  loss dict:  {'classification_loss': 0.9259483587741851}
2025-01-17 06:20:33,652 [INFO] Step[750/2713]: training loss : 0.9264875233173371 TRAIN  loss dict:  {'classification_loss': 0.9264875233173371}
2025-01-17 06:20:49,690 [INFO] Step[800/2713]: training loss : 0.9259054315090179 TRAIN  loss dict:  {'classification_loss': 0.9259054315090179}
2025-01-17 06:21:05,803 [INFO] Step[850/2713]: training loss : 0.951776157617569 TRAIN  loss dict:  {'classification_loss': 0.951776157617569}
2025-01-17 06:21:21,823 [INFO] Step[900/2713]: training loss : 0.9268037545680999 TRAIN  loss dict:  {'classification_loss': 0.9268037545680999}
2025-01-17 06:21:37,912 [INFO] Step[950/2713]: training loss : 0.926362886428833 TRAIN  loss dict:  {'classification_loss': 0.926362886428833}
2025-01-17 06:21:54,063 [INFO] Step[1000/2713]: training loss : 0.9266314470767975 TRAIN  loss dict:  {'classification_loss': 0.9266314470767975}
2025-01-17 06:22:10,136 [INFO] Step[1050/2713]: training loss : 0.9703614509105682 TRAIN  loss dict:  {'classification_loss': 0.9703614509105682}
2025-01-17 06:22:26,176 [INFO] Step[1100/2713]: training loss : 0.9269249272346497 TRAIN  loss dict:  {'classification_loss': 0.9269249272346497}
2025-01-17 06:22:42,274 [INFO] Step[1150/2713]: training loss : 0.9266071796417237 TRAIN  loss dict:  {'classification_loss': 0.9266071796417237}
2025-01-17 06:22:58,326 [INFO] Step[1200/2713]: training loss : 0.9263771855831147 TRAIN  loss dict:  {'classification_loss': 0.9263771855831147}
2025-01-17 06:23:14,441 [INFO] Step[1250/2713]: training loss : 0.9642573750019073 TRAIN  loss dict:  {'classification_loss': 0.9642573750019073}
2025-01-17 06:23:30,495 [INFO] Step[1300/2713]: training loss : 0.9269930624961853 TRAIN  loss dict:  {'classification_loss': 0.9269930624961853}
2025-01-17 06:23:46,610 [INFO] Step[1350/2713]: training loss : 0.926693274974823 TRAIN  loss dict:  {'classification_loss': 0.926693274974823}
2025-01-17 06:24:02,703 [INFO] Step[1400/2713]: training loss : 0.9269218206405639 TRAIN  loss dict:  {'classification_loss': 0.9269218206405639}
2025-01-17 06:24:18,796 [INFO] Step[1450/2713]: training loss : 0.9268684828281403 TRAIN  loss dict:  {'classification_loss': 0.9268684828281403}
2025-01-17 06:24:34,847 [INFO] Step[1500/2713]: training loss : 0.9273107612133026 TRAIN  loss dict:  {'classification_loss': 0.9273107612133026}
2025-01-17 06:24:50,875 [INFO] Step[1550/2713]: training loss : 0.9275749862194062 TRAIN  loss dict:  {'classification_loss': 0.9275749862194062}
2025-01-17 06:25:06,835 [INFO] Step[1600/2713]: training loss : 0.9272226774692536 TRAIN  loss dict:  {'classification_loss': 0.9272226774692536}
2025-01-17 06:25:22,937 [INFO] Step[1650/2713]: training loss : 0.9269856142997742 TRAIN  loss dict:  {'classification_loss': 0.9269856142997742}
2025-01-17 06:25:38,914 [INFO] Step[1700/2713]: training loss : 0.9517271375656128 TRAIN  loss dict:  {'classification_loss': 0.9517271375656128}
2025-01-17 06:25:54,919 [INFO] Step[1750/2713]: training loss : 0.9339893269538879 TRAIN  loss dict:  {'classification_loss': 0.9339893269538879}
2025-01-17 06:26:10,966 [INFO] Step[1800/2713]: training loss : 0.9267831218242645 TRAIN  loss dict:  {'classification_loss': 0.9267831218242645}
2025-01-17 06:26:27,096 [INFO] Step[1850/2713]: training loss : 0.9309819185733795 TRAIN  loss dict:  {'classification_loss': 0.9309819185733795}
2025-01-17 06:26:43,149 [INFO] Step[1900/2713]: training loss : 0.9270683419704437 TRAIN  loss dict:  {'classification_loss': 0.9270683419704437}
2025-01-17 06:26:59,192 [INFO] Step[1950/2713]: training loss : 0.926640032529831 TRAIN  loss dict:  {'classification_loss': 0.926640032529831}
2025-01-17 06:27:15,187 [INFO] Step[2000/2713]: training loss : 0.9267384469509125 TRAIN  loss dict:  {'classification_loss': 0.9267384469509125}
2025-01-17 06:27:31,228 [INFO] Step[2050/2713]: training loss : 0.9264565539360047 TRAIN  loss dict:  {'classification_loss': 0.9264565539360047}
2025-01-17 06:27:47,263 [INFO] Step[2100/2713]: training loss : 0.9264993715286255 TRAIN  loss dict:  {'classification_loss': 0.9264993715286255}
2025-01-17 06:28:03,366 [INFO] Step[2150/2713]: training loss : 0.9262266314029693 TRAIN  loss dict:  {'classification_loss': 0.9262266314029693}
2025-01-17 06:28:19,440 [INFO] Step[2200/2713]: training loss : 0.9263340151309967 TRAIN  loss dict:  {'classification_loss': 0.9263340151309967}
2025-01-17 06:28:35,564 [INFO] Step[2250/2713]: training loss : 0.949936819076538 TRAIN  loss dict:  {'classification_loss': 0.949936819076538}
2025-01-17 06:28:51,612 [INFO] Step[2300/2713]: training loss : 0.9722897338867188 TRAIN  loss dict:  {'classification_loss': 0.9722897338867188}
2025-01-17 06:29:07,724 [INFO] Step[2350/2713]: training loss : 0.9280787110328674 TRAIN  loss dict:  {'classification_loss': 0.9280787110328674}
2025-01-17 06:29:23,799 [INFO] Step[2400/2713]: training loss : 0.9265408599376679 TRAIN  loss dict:  {'classification_loss': 0.9265408599376679}
2025-01-17 06:29:39,850 [INFO] Step[2450/2713]: training loss : 0.9263808226585388 TRAIN  loss dict:  {'classification_loss': 0.9263808226585388}
2025-01-17 06:29:55,814 [INFO] Step[2500/2713]: training loss : 0.929330985546112 TRAIN  loss dict:  {'classification_loss': 0.929330985546112}
2025-01-17 06:30:11,819 [INFO] Step[2550/2713]: training loss : 0.9284142875671386 TRAIN  loss dict:  {'classification_loss': 0.9284142875671386}
2025-01-17 06:30:27,682 [INFO] Step[2600/2713]: training loss : 0.9266308259963989 TRAIN  loss dict:  {'classification_loss': 0.9266308259963989}
2025-01-17 06:30:43,675 [INFO] Step[2650/2713]: training loss : 0.9268021690845489 TRAIN  loss dict:  {'classification_loss': 0.9268021690845489}
2025-01-17 06:30:59,610 [INFO] Step[2700/2713]: training loss : 0.9269704496860505 TRAIN  loss dict:  {'classification_loss': 0.9269704496860505}
2025-01-17 06:32:17,128 [INFO] Label accuracies statistics:
2025-01-17 06:32:17,128 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.5, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.25, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.25, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.5, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.25, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 06:32:17,130 [INFO] [76] TRAIN  loss: 0.9308720882247393 acc: 0.9992628086988573
2025-01-17 06:32:17,130 [INFO] [76] TRAIN  loss dict: {'classification_loss': 0.9308720882247393}
2025-01-17 06:32:17,130 [INFO] [76] VALIDATION loss: 1.9437229976170045 VALIDATION acc: 0.793730407523511
2025-01-17 06:32:17,130 [INFO] [76] VALIDATION loss dict: {'classification_loss': 1.9437229976170045}
2025-01-17 06:32:17,130 [INFO] 
2025-01-17 06:32:38,084 [INFO] Step[50/2713]: training loss : 0.9263893675804138 TRAIN  loss dict:  {'classification_loss': 0.9263893675804138}
2025-01-17 06:32:54,037 [INFO] Step[100/2713]: training loss : 0.9264595079421997 TRAIN  loss dict:  {'classification_loss': 0.9264595079421997}
2025-01-17 06:33:10,060 [INFO] Step[150/2713]: training loss : 0.9358476674556733 TRAIN  loss dict:  {'classification_loss': 0.9358476674556733}
2025-01-17 06:33:26,036 [INFO] Step[200/2713]: training loss : 0.938378494977951 TRAIN  loss dict:  {'classification_loss': 0.938378494977951}
2025-01-17 06:33:42,043 [INFO] Step[250/2713]: training loss : 0.9266059482097626 TRAIN  loss dict:  {'classification_loss': 0.9266059482097626}
2025-01-17 06:33:57,949 [INFO] Step[300/2713]: training loss : 0.9264013862609863 TRAIN  loss dict:  {'classification_loss': 0.9264013862609863}
2025-01-17 06:34:13,955 [INFO] Step[350/2713]: training loss : 0.9265654528141022 TRAIN  loss dict:  {'classification_loss': 0.9265654528141022}
2025-01-17 06:34:29,915 [INFO] Step[400/2713]: training loss : 0.926671644449234 TRAIN  loss dict:  {'classification_loss': 0.926671644449234}
2025-01-17 06:34:45,904 [INFO] Step[450/2713]: training loss : 0.9264443087577819 TRAIN  loss dict:  {'classification_loss': 0.9264443087577819}
2025-01-17 06:35:01,884 [INFO] Step[500/2713]: training loss : 0.9259634613990784 TRAIN  loss dict:  {'classification_loss': 0.9259634613990784}
2025-01-17 06:35:17,873 [INFO] Step[550/2713]: training loss : 0.9270249164104462 TRAIN  loss dict:  {'classification_loss': 0.9270249164104462}
2025-01-17 06:35:33,826 [INFO] Step[600/2713]: training loss : 0.9276668667793274 TRAIN  loss dict:  {'classification_loss': 0.9276668667793274}
2025-01-17 06:35:49,868 [INFO] Step[650/2713]: training loss : 0.9263500416278839 TRAIN  loss dict:  {'classification_loss': 0.9263500416278839}
2025-01-17 06:36:05,825 [INFO] Step[700/2713]: training loss : 0.92669513463974 TRAIN  loss dict:  {'classification_loss': 0.92669513463974}
2025-01-17 06:36:21,802 [INFO] Step[750/2713]: training loss : 0.956110634803772 TRAIN  loss dict:  {'classification_loss': 0.956110634803772}
2025-01-17 06:36:37,765 [INFO] Step[800/2713]: training loss : 0.9269022643566132 TRAIN  loss dict:  {'classification_loss': 0.9269022643566132}
2025-01-17 06:36:53,802 [INFO] Step[850/2713]: training loss : 0.9266038513183594 TRAIN  loss dict:  {'classification_loss': 0.9266038513183594}
2025-01-17 06:37:09,748 [INFO] Step[900/2713]: training loss : 0.9273917591571808 TRAIN  loss dict:  {'classification_loss': 0.9273917591571808}
2025-01-17 06:37:25,745 [INFO] Step[950/2713]: training loss : 0.9313559174537659 TRAIN  loss dict:  {'classification_loss': 0.9313559174537659}
2025-01-17 06:37:41,706 [INFO] Step[1000/2713]: training loss : 0.9268214082717896 TRAIN  loss dict:  {'classification_loss': 0.9268214082717896}
2025-01-17 06:37:57,627 [INFO] Step[1050/2713]: training loss : 0.9270639157295227 TRAIN  loss dict:  {'classification_loss': 0.9270639157295227}
2025-01-17 06:38:13,528 [INFO] Step[1100/2713]: training loss : 0.947744666337967 TRAIN  loss dict:  {'classification_loss': 0.947744666337967}
2025-01-17 06:38:29,515 [INFO] Step[1150/2713]: training loss : 0.9471957874298096 TRAIN  loss dict:  {'classification_loss': 0.9471957874298096}
2025-01-17 06:38:45,459 [INFO] Step[1200/2713]: training loss : 0.927076404094696 TRAIN  loss dict:  {'classification_loss': 0.927076404094696}
2025-01-17 06:39:01,447 [INFO] Step[1250/2713]: training loss : 0.9266866934299469 TRAIN  loss dict:  {'classification_loss': 0.9266866934299469}
2025-01-17 06:39:17,436 [INFO] Step[1300/2713]: training loss : 0.9268127727508545 TRAIN  loss dict:  {'classification_loss': 0.9268127727508545}
2025-01-17 06:39:33,455 [INFO] Step[1350/2713]: training loss : 0.9267515623569489 TRAIN  loss dict:  {'classification_loss': 0.9267515623569489}
2025-01-17 06:39:49,397 [INFO] Step[1400/2713]: training loss : 0.9266417169570923 TRAIN  loss dict:  {'classification_loss': 0.9266417169570923}
2025-01-17 06:40:05,385 [INFO] Step[1450/2713]: training loss : 0.9272913467884064 TRAIN  loss dict:  {'classification_loss': 0.9272913467884064}
2025-01-17 06:40:21,288 [INFO] Step[1500/2713]: training loss : 0.9280773448944092 TRAIN  loss dict:  {'classification_loss': 0.9280773448944092}
2025-01-17 06:40:37,229 [INFO] Step[1550/2713]: training loss : 0.9271568417549133 TRAIN  loss dict:  {'classification_loss': 0.9271568417549133}
2025-01-17 06:40:53,220 [INFO] Step[1600/2713]: training loss : 0.9265842545032501 TRAIN  loss dict:  {'classification_loss': 0.9265842545032501}
2025-01-17 06:41:09,194 [INFO] Step[1650/2713]: training loss : 0.9268710589408875 TRAIN  loss dict:  {'classification_loss': 0.9268710589408875}
2025-01-17 06:41:25,174 [INFO] Step[1700/2713]: training loss : 0.9265712547302246 TRAIN  loss dict:  {'classification_loss': 0.9265712547302246}
2025-01-17 06:41:41,217 [INFO] Step[1750/2713]: training loss : 0.9272179353237152 TRAIN  loss dict:  {'classification_loss': 0.9272179353237152}
2025-01-17 06:41:57,141 [INFO] Step[1800/2713]: training loss : 0.9273063158988952 TRAIN  loss dict:  {'classification_loss': 0.9273063158988952}
2025-01-17 06:42:13,128 [INFO] Step[1850/2713]: training loss : 0.9608768141269683 TRAIN  loss dict:  {'classification_loss': 0.9608768141269683}
2025-01-17 06:42:29,141 [INFO] Step[1900/2713]: training loss : 0.9270159471035003 TRAIN  loss dict:  {'classification_loss': 0.9270159471035003}
2025-01-17 06:42:45,123 [INFO] Step[1950/2713]: training loss : 0.9271752882003784 TRAIN  loss dict:  {'classification_loss': 0.9271752882003784}
2025-01-17 06:43:01,155 [INFO] Step[2000/2713]: training loss : 0.9618698871135711 TRAIN  loss dict:  {'classification_loss': 0.9618698871135711}
2025-01-17 06:43:17,208 [INFO] Step[2050/2713]: training loss : 0.9264508712291718 TRAIN  loss dict:  {'classification_loss': 0.9264508712291718}
2025-01-17 06:43:33,191 [INFO] Step[2100/2713]: training loss : 0.9264007329940795 TRAIN  loss dict:  {'classification_loss': 0.9264007329940795}
2025-01-17 06:43:49,184 [INFO] Step[2150/2713]: training loss : 0.9262350714206695 TRAIN  loss dict:  {'classification_loss': 0.9262350714206695}
2025-01-17 06:44:05,059 [INFO] Step[2200/2713]: training loss : 0.926406911611557 TRAIN  loss dict:  {'classification_loss': 0.926406911611557}
2025-01-17 06:44:21,102 [INFO] Step[2250/2713]: training loss : 0.9270533299446106 TRAIN  loss dict:  {'classification_loss': 0.9270533299446106}
2025-01-17 06:44:37,071 [INFO] Step[2300/2713]: training loss : 0.927020355463028 TRAIN  loss dict:  {'classification_loss': 0.927020355463028}
2025-01-17 06:44:53,053 [INFO] Step[2350/2713]: training loss : 0.926811339855194 TRAIN  loss dict:  {'classification_loss': 0.926811339855194}
2025-01-17 06:45:09,055 [INFO] Step[2400/2713]: training loss : 0.9268481183052063 TRAIN  loss dict:  {'classification_loss': 0.9268481183052063}
2025-01-17 06:45:25,016 [INFO] Step[2450/2713]: training loss : 0.9270982253551483 TRAIN  loss dict:  {'classification_loss': 0.9270982253551483}
2025-01-17 06:45:40,978 [INFO] Step[2500/2713]: training loss : 0.9266164362430572 TRAIN  loss dict:  {'classification_loss': 0.9266164362430572}
2025-01-17 06:45:56,964 [INFO] Step[2550/2713]: training loss : 0.9269192588329315 TRAIN  loss dict:  {'classification_loss': 0.9269192588329315}
2025-01-17 06:46:12,942 [INFO] Step[2600/2713]: training loss : 0.9268295609951019 TRAIN  loss dict:  {'classification_loss': 0.9268295609951019}
2025-01-17 06:46:28,953 [INFO] Step[2650/2713]: training loss : 0.9262811470031739 TRAIN  loss dict:  {'classification_loss': 0.9262811470031739}
2025-01-17 06:46:44,899 [INFO] Step[2700/2713]: training loss : 0.9265090596675872 TRAIN  loss dict:  {'classification_loss': 0.9265090596675872}
2025-01-17 06:48:02,363 [INFO] Label accuracies statistics:
2025-01-17 06:48:02,363 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.25, 69: 0.5, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 1.0, 205: 1.0, 206: 0.25, 207: 0.5, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.5, 212: 0.5, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.5, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.5, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 06:48:02,365 [INFO] [77] TRAIN  loss: 0.9298396948470957 acc: 0.998771347831429
2025-01-17 06:48:02,365 [INFO] [77] TRAIN  loss dict: {'classification_loss': 0.9298396948470957}
2025-01-17 06:48:02,365 [INFO] [77] VALIDATION loss: 1.909435224152149 VALIDATION acc: 0.7987460815047022
2025-01-17 06:48:02,365 [INFO] [77] VALIDATION loss dict: {'classification_loss': 1.909435224152149}
2025-01-17 06:48:02,365 [INFO] 
2025-01-17 06:48:23,621 [INFO] Step[50/2713]: training loss : 0.92718496799469 TRAIN  loss dict:  {'classification_loss': 0.92718496799469}
2025-01-17 06:48:39,672 [INFO] Step[100/2713]: training loss : 0.9271127903461456 TRAIN  loss dict:  {'classification_loss': 0.9271127903461456}
2025-01-17 06:48:55,717 [INFO] Step[150/2713]: training loss : 0.9270239639282226 TRAIN  loss dict:  {'classification_loss': 0.9270239639282226}
2025-01-17 06:49:11,750 [INFO] Step[200/2713]: training loss : 0.9265839421749115 TRAIN  loss dict:  {'classification_loss': 0.9265839421749115}
2025-01-17 06:49:27,839 [INFO] Step[250/2713]: training loss : 0.9266219806671142 TRAIN  loss dict:  {'classification_loss': 0.9266219806671142}
2025-01-17 06:49:43,945 [INFO] Step[300/2713]: training loss : 0.9276509821414948 TRAIN  loss dict:  {'classification_loss': 0.9276509821414948}
2025-01-17 06:50:00,133 [INFO] Step[350/2713]: training loss : 0.9261952245235443 TRAIN  loss dict:  {'classification_loss': 0.9261952245235443}
2025-01-17 06:50:16,189 [INFO] Step[400/2713]: training loss : 0.9264292073249817 TRAIN  loss dict:  {'classification_loss': 0.9264292073249817}
2025-01-17 06:50:32,313 [INFO] Step[450/2713]: training loss : 0.926417452096939 TRAIN  loss dict:  {'classification_loss': 0.926417452096939}
2025-01-17 06:50:48,382 [INFO] Step[500/2713]: training loss : 0.9268025147914887 TRAIN  loss dict:  {'classification_loss': 0.9268025147914887}
2025-01-17 06:51:04,499 [INFO] Step[550/2713]: training loss : 0.947411607503891 TRAIN  loss dict:  {'classification_loss': 0.947411607503891}
2025-01-17 06:51:20,544 [INFO] Step[600/2713]: training loss : 0.9261901402473449 TRAIN  loss dict:  {'classification_loss': 0.9261901402473449}
2025-01-17 06:51:36,688 [INFO] Step[650/2713]: training loss : 0.926501808166504 TRAIN  loss dict:  {'classification_loss': 0.926501808166504}
2025-01-17 06:51:52,785 [INFO] Step[700/2713]: training loss : 0.9274486923217773 TRAIN  loss dict:  {'classification_loss': 0.9274486923217773}
2025-01-17 06:52:08,983 [INFO] Step[750/2713]: training loss : 0.9265558159351349 TRAIN  loss dict:  {'classification_loss': 0.9265558159351349}
2025-01-17 06:52:25,031 [INFO] Step[800/2713]: training loss : 0.9524097800254822 TRAIN  loss dict:  {'classification_loss': 0.9524097800254822}
2025-01-17 06:52:41,203 [INFO] Step[850/2713]: training loss : 0.9463822984695435 TRAIN  loss dict:  {'classification_loss': 0.9463822984695435}
2025-01-17 06:52:57,317 [INFO] Step[900/2713]: training loss : 0.928544614315033 TRAIN  loss dict:  {'classification_loss': 0.928544614315033}
2025-01-17 06:53:13,390 [INFO] Step[950/2713]: training loss : 0.9324542772769928 TRAIN  loss dict:  {'classification_loss': 0.9324542772769928}
2025-01-17 06:53:29,436 [INFO] Step[1000/2713]: training loss : 0.9668317449092865 TRAIN  loss dict:  {'classification_loss': 0.9668317449092865}
2025-01-17 06:53:45,528 [INFO] Step[1050/2713]: training loss : 0.9266875207424163 TRAIN  loss dict:  {'classification_loss': 0.9266875207424163}
2025-01-17 06:54:01,647 [INFO] Step[1100/2713]: training loss : 0.9266857373714447 TRAIN  loss dict:  {'classification_loss': 0.9266857373714447}
2025-01-17 06:54:17,745 [INFO] Step[1150/2713]: training loss : 0.926982753276825 TRAIN  loss dict:  {'classification_loss': 0.926982753276825}
2025-01-17 06:54:33,783 [INFO] Step[1200/2713]: training loss : 0.9265741741657257 TRAIN  loss dict:  {'classification_loss': 0.9265741741657257}
2025-01-17 06:54:49,956 [INFO] Step[1250/2713]: training loss : 0.9262456035614014 TRAIN  loss dict:  {'classification_loss': 0.9262456035614014}
2025-01-17 06:55:06,079 [INFO] Step[1300/2713]: training loss : 0.9318173158168793 TRAIN  loss dict:  {'classification_loss': 0.9318173158168793}
2025-01-17 06:55:22,180 [INFO] Step[1350/2713]: training loss : 0.926992974281311 TRAIN  loss dict:  {'classification_loss': 0.926992974281311}
2025-01-17 06:55:38,254 [INFO] Step[1400/2713]: training loss : 0.9262380254268646 TRAIN  loss dict:  {'classification_loss': 0.9262380254268646}
2025-01-17 06:55:54,332 [INFO] Step[1450/2713]: training loss : 0.9262007498741149 TRAIN  loss dict:  {'classification_loss': 0.9262007498741149}
2025-01-17 06:56:10,383 [INFO] Step[1500/2713]: training loss : 0.9280389022827148 TRAIN  loss dict:  {'classification_loss': 0.9280389022827148}
2025-01-17 06:56:26,514 [INFO] Step[1550/2713]: training loss : 0.9270329463481903 TRAIN  loss dict:  {'classification_loss': 0.9270329463481903}
2025-01-17 06:56:42,546 [INFO] Step[1600/2713]: training loss : 0.9268704414367676 TRAIN  loss dict:  {'classification_loss': 0.9268704414367676}
2025-01-17 06:56:58,663 [INFO] Step[1650/2713]: training loss : 0.9264669406414032 TRAIN  loss dict:  {'classification_loss': 0.9264669406414032}
2025-01-17 06:57:14,707 [INFO] Step[1700/2713]: training loss : 0.9305197548866272 TRAIN  loss dict:  {'classification_loss': 0.9305197548866272}
2025-01-17 06:57:30,869 [INFO] Step[1750/2713]: training loss : 0.9270937132835388 TRAIN  loss dict:  {'classification_loss': 0.9270937132835388}
2025-01-17 06:57:46,944 [INFO] Step[1800/2713]: training loss : 0.9267518532276153 TRAIN  loss dict:  {'classification_loss': 0.9267518532276153}
2025-01-17 06:58:03,055 [INFO] Step[1850/2713]: training loss : 0.9263877069950104 TRAIN  loss dict:  {'classification_loss': 0.9263877069950104}
2025-01-17 06:58:19,135 [INFO] Step[1900/2713]: training loss : 0.9321071720123291 TRAIN  loss dict:  {'classification_loss': 0.9321071720123291}
2025-01-17 06:58:35,265 [INFO] Step[1950/2713]: training loss : 0.9273254847526551 TRAIN  loss dict:  {'classification_loss': 0.9273254847526551}
2025-01-17 06:58:51,384 [INFO] Step[2000/2713]: training loss : 0.9269629740715026 TRAIN  loss dict:  {'classification_loss': 0.9269629740715026}
2025-01-17 06:59:07,487 [INFO] Step[2050/2713]: training loss : 0.9267571008205414 TRAIN  loss dict:  {'classification_loss': 0.9267571008205414}
2025-01-17 06:59:23,535 [INFO] Step[2100/2713]: training loss : 0.9270888268947601 TRAIN  loss dict:  {'classification_loss': 0.9270888268947601}
2025-01-17 06:59:39,656 [INFO] Step[2150/2713]: training loss : 0.926758097410202 TRAIN  loss dict:  {'classification_loss': 0.926758097410202}
2025-01-17 06:59:55,752 [INFO] Step[2200/2713]: training loss : 0.9406534278392792 TRAIN  loss dict:  {'classification_loss': 0.9406534278392792}
2025-01-17 07:00:11,868 [INFO] Step[2250/2713]: training loss : 0.9262567496299744 TRAIN  loss dict:  {'classification_loss': 0.9262567496299744}
2025-01-17 07:00:27,952 [INFO] Step[2300/2713]: training loss : 0.9260648143291473 TRAIN  loss dict:  {'classification_loss': 0.9260648143291473}
2025-01-17 07:00:44,051 [INFO] Step[2350/2713]: training loss : 0.9265857517719269 TRAIN  loss dict:  {'classification_loss': 0.9265857517719269}
2025-01-17 07:01:00,102 [INFO] Step[2400/2713]: training loss : 0.9272827696800232 TRAIN  loss dict:  {'classification_loss': 0.9272827696800232}
2025-01-17 07:01:16,198 [INFO] Step[2450/2713]: training loss : 0.9259248924255371 TRAIN  loss dict:  {'classification_loss': 0.9259248924255371}
2025-01-17 07:01:32,327 [INFO] Step[2500/2713]: training loss : 0.9266977775096893 TRAIN  loss dict:  {'classification_loss': 0.9266977775096893}
2025-01-17 07:01:48,450 [INFO] Step[2550/2713]: training loss : 0.9266693997383117 TRAIN  loss dict:  {'classification_loss': 0.9266693997383117}
2025-01-17 07:02:04,546 [INFO] Step[2600/2713]: training loss : 0.9277664816379547 TRAIN  loss dict:  {'classification_loss': 0.9277664816379547}
2025-01-17 07:02:20,606 [INFO] Step[2650/2713]: training loss : 0.9264647591114045 TRAIN  loss dict:  {'classification_loss': 0.9264647591114045}
2025-01-17 07:02:36,652 [INFO] Step[2700/2713]: training loss : 0.9427443647384643 TRAIN  loss dict:  {'classification_loss': 0.9427443647384643}
2025-01-17 07:03:54,924 [INFO] Label accuracies statistics:
2025-01-17 07:03:54,924 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.25, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 1.0, 261: 0.25, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 1.0, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 07:03:56,599 [INFO] [78] TRAIN  loss: 0.9296578142065602 acc: 0.9988942130482861
2025-01-17 07:03:56,599 [INFO] [78] TRAIN  loss dict: {'classification_loss': 0.9296578142065602}
2025-01-17 07:03:56,600 [INFO] [78] VALIDATION loss: 1.860023346274419 VALIDATION acc: 0.8075235109717869
2025-01-17 07:03:56,600 [INFO] [78] VALIDATION loss dict: {'classification_loss': 1.860023346274419}
2025-01-17 07:03:56,600 [INFO] 
2025-01-17 07:04:17,459 [INFO] Step[50/2713]: training loss : 0.9263228070735932 TRAIN  loss dict:  {'classification_loss': 0.9263228070735932}
2025-01-17 07:04:33,436 [INFO] Step[100/2713]: training loss : 0.9266841769218445 TRAIN  loss dict:  {'classification_loss': 0.9266841769218445}
2025-01-17 07:04:49,386 [INFO] Step[150/2713]: training loss : 0.9262858200073242 TRAIN  loss dict:  {'classification_loss': 0.9262858200073242}
2025-01-17 07:05:05,426 [INFO] Step[200/2713]: training loss : 0.926942048072815 TRAIN  loss dict:  {'classification_loss': 0.926942048072815}
2025-01-17 07:05:21,470 [INFO] Step[250/2713]: training loss : 0.9268468964099884 TRAIN  loss dict:  {'classification_loss': 0.9268468964099884}
2025-01-17 07:05:37,425 [INFO] Step[300/2713]: training loss : 0.9263712501525879 TRAIN  loss dict:  {'classification_loss': 0.9263712501525879}
2025-01-17 07:05:53,402 [INFO] Step[350/2713]: training loss : 0.9261247396469117 TRAIN  loss dict:  {'classification_loss': 0.9261247396469117}
2025-01-17 07:06:09,390 [INFO] Step[400/2713]: training loss : 0.9271555256843567 TRAIN  loss dict:  {'classification_loss': 0.9271555256843567}
2025-01-17 07:06:25,411 [INFO] Step[450/2713]: training loss : 0.9271029317378998 TRAIN  loss dict:  {'classification_loss': 0.9271029317378998}
2025-01-17 07:06:41,374 [INFO] Step[500/2713]: training loss : 0.9263075911998748 TRAIN  loss dict:  {'classification_loss': 0.9263075911998748}
2025-01-17 07:06:57,360 [INFO] Step[550/2713]: training loss : 0.9282762932777405 TRAIN  loss dict:  {'classification_loss': 0.9282762932777405}
2025-01-17 07:07:13,317 [INFO] Step[600/2713]: training loss : 0.9266267704963684 TRAIN  loss dict:  {'classification_loss': 0.9266267704963684}
2025-01-17 07:07:29,293 [INFO] Step[650/2713]: training loss : 0.9258648860454559 TRAIN  loss dict:  {'classification_loss': 0.9258648860454559}
2025-01-17 07:07:45,227 [INFO] Step[700/2713]: training loss : 0.92631152510643 TRAIN  loss dict:  {'classification_loss': 0.92631152510643}
2025-01-17 07:08:01,255 [INFO] Step[750/2713]: training loss : 0.9284586429595947 TRAIN  loss dict:  {'classification_loss': 0.9284586429595947}
2025-01-17 07:08:17,238 [INFO] Step[800/2713]: training loss : 0.9266191351413727 TRAIN  loss dict:  {'classification_loss': 0.9266191351413727}
2025-01-17 07:08:33,251 [INFO] Step[850/2713]: training loss : 0.9634809422492981 TRAIN  loss dict:  {'classification_loss': 0.9634809422492981}
2025-01-17 07:08:49,155 [INFO] Step[900/2713]: training loss : 0.9273174023628235 TRAIN  loss dict:  {'classification_loss': 0.9273174023628235}
2025-01-17 07:09:05,156 [INFO] Step[950/2713]: training loss : 0.9265339982509613 TRAIN  loss dict:  {'classification_loss': 0.9265339982509613}
2025-01-17 07:09:21,093 [INFO] Step[1000/2713]: training loss : 0.9270243954658508 TRAIN  loss dict:  {'classification_loss': 0.9270243954658508}
2025-01-17 07:09:37,039 [INFO] Step[1050/2713]: training loss : 0.927093471288681 TRAIN  loss dict:  {'classification_loss': 0.927093471288681}
2025-01-17 07:09:53,015 [INFO] Step[1100/2713]: training loss : 0.926556978225708 TRAIN  loss dict:  {'classification_loss': 0.926556978225708}
2025-01-17 07:10:09,022 [INFO] Step[1150/2713]: training loss : 0.92623086810112 TRAIN  loss dict:  {'classification_loss': 0.92623086810112}
2025-01-17 07:10:24,977 [INFO] Step[1200/2713]: training loss : 0.9299456322193146 TRAIN  loss dict:  {'classification_loss': 0.9299456322193146}
2025-01-17 07:10:40,951 [INFO] Step[1250/2713]: training loss : 0.9262287771701813 TRAIN  loss dict:  {'classification_loss': 0.9262287771701813}
2025-01-17 07:10:56,974 [INFO] Step[1300/2713]: training loss : 0.9263508260250092 TRAIN  loss dict:  {'classification_loss': 0.9263508260250092}
2025-01-17 07:11:12,955 [INFO] Step[1350/2713]: training loss : 0.9269828486442566 TRAIN  loss dict:  {'classification_loss': 0.9269828486442566}
2025-01-17 07:11:28,955 [INFO] Step[1400/2713]: training loss : 0.9267707514762878 TRAIN  loss dict:  {'classification_loss': 0.9267707514762878}
2025-01-17 07:11:44,987 [INFO] Step[1450/2713]: training loss : 0.9259635210037231 TRAIN  loss dict:  {'classification_loss': 0.9259635210037231}
2025-01-17 07:12:00,903 [INFO] Step[1500/2713]: training loss : 0.9446278870105743 TRAIN  loss dict:  {'classification_loss': 0.9446278870105743}
2025-01-17 07:12:16,893 [INFO] Step[1550/2713]: training loss : 0.9270486950874328 TRAIN  loss dict:  {'classification_loss': 0.9270486950874328}
2025-01-17 07:12:32,894 [INFO] Step[1600/2713]: training loss : 0.9266897130012512 TRAIN  loss dict:  {'classification_loss': 0.9266897130012512}
2025-01-17 07:12:48,877 [INFO] Step[1650/2713]: training loss : 0.9264871644973754 TRAIN  loss dict:  {'classification_loss': 0.9264871644973754}
2025-01-17 07:13:04,926 [INFO] Step[1700/2713]: training loss : 0.9261841952800751 TRAIN  loss dict:  {'classification_loss': 0.9261841952800751}
2025-01-17 07:13:20,896 [INFO] Step[1750/2713]: training loss : 0.9267492723464966 TRAIN  loss dict:  {'classification_loss': 0.9267492723464966}
2025-01-17 07:13:36,862 [INFO] Step[1800/2713]: training loss : 0.9257746040821075 TRAIN  loss dict:  {'classification_loss': 0.9257746040821075}
2025-01-17 07:13:52,823 [INFO] Step[1850/2713]: training loss : 0.9269461798667907 TRAIN  loss dict:  {'classification_loss': 0.9269461798667907}
2025-01-17 07:14:08,798 [INFO] Step[1900/2713]: training loss : 0.9351518797874451 TRAIN  loss dict:  {'classification_loss': 0.9351518797874451}
2025-01-17 07:14:24,785 [INFO] Step[1950/2713]: training loss : 0.9262695682048797 TRAIN  loss dict:  {'classification_loss': 0.9262695682048797}
2025-01-17 07:14:40,751 [INFO] Step[2000/2713]: training loss : 0.9268789029121399 TRAIN  loss dict:  {'classification_loss': 0.9268789029121399}
2025-01-17 07:14:56,706 [INFO] Step[2050/2713]: training loss : 0.926373428106308 TRAIN  loss dict:  {'classification_loss': 0.926373428106308}
2025-01-17 07:15:12,634 [INFO] Step[2100/2713]: training loss : 0.9704423570632934 TRAIN  loss dict:  {'classification_loss': 0.9704423570632934}
2025-01-17 07:15:28,622 [INFO] Step[2150/2713]: training loss : 0.9261255872249603 TRAIN  loss dict:  {'classification_loss': 0.9261255872249603}
2025-01-17 07:15:44,582 [INFO] Step[2200/2713]: training loss : 0.9267877280712128 TRAIN  loss dict:  {'classification_loss': 0.9267877280712128}
2025-01-17 07:16:00,589 [INFO] Step[2250/2713]: training loss : 0.9265252614021301 TRAIN  loss dict:  {'classification_loss': 0.9265252614021301}
2025-01-17 07:16:16,617 [INFO] Step[2300/2713]: training loss : 0.9265534830093384 TRAIN  loss dict:  {'classification_loss': 0.9265534830093384}
2025-01-17 07:16:32,579 [INFO] Step[2350/2713]: training loss : 0.9262587261199952 TRAIN  loss dict:  {'classification_loss': 0.9262587261199952}
2025-01-17 07:16:48,488 [INFO] Step[2400/2713]: training loss : 0.9272924041748047 TRAIN  loss dict:  {'classification_loss': 0.9272924041748047}
2025-01-17 07:17:04,475 [INFO] Step[2450/2713]: training loss : 0.926499183177948 TRAIN  loss dict:  {'classification_loss': 0.926499183177948}
2025-01-17 07:17:20,487 [INFO] Step[2500/2713]: training loss : 0.9289162373542785 TRAIN  loss dict:  {'classification_loss': 0.9289162373542785}
2025-01-17 07:17:36,453 [INFO] Step[2550/2713]: training loss : 0.947256647348404 TRAIN  loss dict:  {'classification_loss': 0.947256647348404}
2025-01-17 07:17:52,453 [INFO] Step[2600/2713]: training loss : 0.9267298603057861 TRAIN  loss dict:  {'classification_loss': 0.9267298603057861}
2025-01-17 07:18:08,531 [INFO] Step[2650/2713]: training loss : 0.9267509508132935 TRAIN  loss dict:  {'classification_loss': 0.9267509508132935}
2025-01-17 07:18:24,475 [INFO] Step[2700/2713]: training loss : 0.9789476561546325 TRAIN  loss dict:  {'classification_loss': 0.9789476561546325}
2025-01-17 07:19:42,618 [INFO] Label accuracies statistics:
2025-01-17 07:19:42,619 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 0.75, 149: 0.75, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.5, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.75, 203: 0.75, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 0.75, 266: 1.0, 267: 1.0, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 07:19:42,620 [INFO] [79] TRAIN  loss: 0.9300893271404397 acc: 0.9991399434820002
2025-01-17 07:19:42,620 [INFO] [79] TRAIN  loss dict: {'classification_loss': 0.9300893271404397}
2025-01-17 07:19:42,620 [INFO] [79] VALIDATION loss: 1.887125394622186 VALIDATION acc: 0.8025078369905956
2025-01-17 07:19:42,621 [INFO] [79] VALIDATION loss dict: {'classification_loss': 1.887125394622186}
2025-01-17 07:19:42,621 [INFO] 
2025-01-17 07:20:03,247 [INFO] Step[50/2713]: training loss : 0.9266831696033477 TRAIN  loss dict:  {'classification_loss': 0.9266831696033477}
2025-01-17 07:20:19,215 [INFO] Step[100/2713]: training loss : 0.9511402320861816 TRAIN  loss dict:  {'classification_loss': 0.9511402320861816}
2025-01-17 07:20:35,271 [INFO] Step[150/2713]: training loss : 0.9266532993316651 TRAIN  loss dict:  {'classification_loss': 0.9266532993316651}
2025-01-17 07:20:51,257 [INFO] Step[200/2713]: training loss : 0.9269778192043304 TRAIN  loss dict:  {'classification_loss': 0.9269778192043304}
2025-01-17 07:21:07,294 [INFO] Step[250/2713]: training loss : 0.9270003402233123 TRAIN  loss dict:  {'classification_loss': 0.9270003402233123}
2025-01-17 07:21:23,314 [INFO] Step[300/2713]: training loss : 0.926549664735794 TRAIN  loss dict:  {'classification_loss': 0.926549664735794}
2025-01-17 07:21:39,319 [INFO] Step[350/2713]: training loss : 0.9260397064685821 TRAIN  loss dict:  {'classification_loss': 0.9260397064685821}
2025-01-17 07:21:55,291 [INFO] Step[400/2713]: training loss : 0.92672189950943 TRAIN  loss dict:  {'classification_loss': 0.92672189950943}
2025-01-17 07:22:11,310 [INFO] Step[450/2713]: training loss : 0.926403341293335 TRAIN  loss dict:  {'classification_loss': 0.926403341293335}
2025-01-17 07:22:27,359 [INFO] Step[500/2713]: training loss : 0.9265435767173767 TRAIN  loss dict:  {'classification_loss': 0.9265435767173767}
2025-01-17 07:22:43,382 [INFO] Step[550/2713]: training loss : 0.9260650551319123 TRAIN  loss dict:  {'classification_loss': 0.9260650551319123}
2025-01-17 07:22:59,423 [INFO] Step[600/2713]: training loss : 0.9258651494979858 TRAIN  loss dict:  {'classification_loss': 0.9258651494979858}
2025-01-17 07:23:15,460 [INFO] Step[650/2713]: training loss : 0.9263827741146088 TRAIN  loss dict:  {'classification_loss': 0.9263827741146088}
2025-01-17 07:23:31,395 [INFO] Step[700/2713]: training loss : 0.9262825858592987 TRAIN  loss dict:  {'classification_loss': 0.9262825858592987}
2025-01-17 07:23:47,451 [INFO] Step[750/2713]: training loss : 0.926051813364029 TRAIN  loss dict:  {'classification_loss': 0.926051813364029}
2025-01-17 07:24:03,442 [INFO] Step[800/2713]: training loss : 0.9267219519615173 TRAIN  loss dict:  {'classification_loss': 0.9267219519615173}
2025-01-17 07:24:19,415 [INFO] Step[850/2713]: training loss : 0.9305091774463654 TRAIN  loss dict:  {'classification_loss': 0.9305091774463654}
2025-01-17 07:24:35,399 [INFO] Step[900/2713]: training loss : 0.9266612374782562 TRAIN  loss dict:  {'classification_loss': 0.9266612374782562}
2025-01-17 07:24:51,441 [INFO] Step[950/2713]: training loss : 0.9314869225025177 TRAIN  loss dict:  {'classification_loss': 0.9314869225025177}
2025-01-17 07:25:07,374 [INFO] Step[1000/2713]: training loss : 0.9265165519714356 TRAIN  loss dict:  {'classification_loss': 0.9265165519714356}
2025-01-17 07:25:23,338 [INFO] Step[1050/2713]: training loss : 0.925733790397644 TRAIN  loss dict:  {'classification_loss': 0.925733790397644}
2025-01-17 07:25:39,341 [INFO] Step[1100/2713]: training loss : 0.9262060606479645 TRAIN  loss dict:  {'classification_loss': 0.9262060606479645}
2025-01-17 07:25:55,320 [INFO] Step[1150/2713]: training loss : 0.9263673913478851 TRAIN  loss dict:  {'classification_loss': 0.9263673913478851}
2025-01-17 07:26:11,250 [INFO] Step[1200/2713]: training loss : 0.9264085328578949 TRAIN  loss dict:  {'classification_loss': 0.9264085328578949}
2025-01-17 07:26:27,216 [INFO] Step[1250/2713]: training loss : 0.9416127586364746 TRAIN  loss dict:  {'classification_loss': 0.9416127586364746}
2025-01-17 07:26:43,178 [INFO] Step[1300/2713]: training loss : 0.9263265240192413 TRAIN  loss dict:  {'classification_loss': 0.9263265240192413}
2025-01-17 07:26:59,115 [INFO] Step[1350/2713]: training loss : 0.9267194068431854 TRAIN  loss dict:  {'classification_loss': 0.9267194068431854}
2025-01-17 07:27:15,052 [INFO] Step[1400/2713]: training loss : 0.9266431534290314 TRAIN  loss dict:  {'classification_loss': 0.9266431534290314}
2025-01-17 07:27:30,977 [INFO] Step[1450/2713]: training loss : 0.9283071994781494 TRAIN  loss dict:  {'classification_loss': 0.9283071994781494}
2025-01-17 07:27:46,838 [INFO] Step[1500/2713]: training loss : 0.9322453725337982 TRAIN  loss dict:  {'classification_loss': 0.9322453725337982}
2025-01-17 07:28:02,802 [INFO] Step[1550/2713]: training loss : 0.9264672636985779 TRAIN  loss dict:  {'classification_loss': 0.9264672636985779}
2025-01-17 07:28:18,782 [INFO] Step[1600/2713]: training loss : 0.9275615632534027 TRAIN  loss dict:  {'classification_loss': 0.9275615632534027}
2025-01-17 07:28:34,746 [INFO] Step[1650/2713]: training loss : 0.9262108528614044 TRAIN  loss dict:  {'classification_loss': 0.9262108528614044}
2025-01-17 07:28:50,587 [INFO] Step[1700/2713]: training loss : 0.9330688011646271 TRAIN  loss dict:  {'classification_loss': 0.9330688011646271}
2025-01-17 07:29:06,555 [INFO] Step[1750/2713]: training loss : 0.9271764767169952 TRAIN  loss dict:  {'classification_loss': 0.9271764767169952}
2025-01-17 07:29:22,430 [INFO] Step[1800/2713]: training loss : 0.9263502359390259 TRAIN  loss dict:  {'classification_loss': 0.9263502359390259}
2025-01-17 07:29:38,352 [INFO] Step[1850/2713]: training loss : 0.9268631768226624 TRAIN  loss dict:  {'classification_loss': 0.9268631768226624}
2025-01-17 07:29:54,319 [INFO] Step[1900/2713]: training loss : 0.9306717419624329 TRAIN  loss dict:  {'classification_loss': 0.9306717419624329}
2025-01-17 07:30:10,209 [INFO] Step[1950/2713]: training loss : 0.9267692029476166 TRAIN  loss dict:  {'classification_loss': 0.9267692029476166}
2025-01-17 07:30:26,100 [INFO] Step[2000/2713]: training loss : 0.9298218524456024 TRAIN  loss dict:  {'classification_loss': 0.9298218524456024}
2025-01-17 07:30:42,015 [INFO] Step[2050/2713]: training loss : 0.9259936058521271 TRAIN  loss dict:  {'classification_loss': 0.9259936058521271}
2025-01-17 07:30:57,969 [INFO] Step[2100/2713]: training loss : 0.9498326408863068 TRAIN  loss dict:  {'classification_loss': 0.9498326408863068}
2025-01-17 07:31:13,971 [INFO] Step[2150/2713]: training loss : 0.9266472470760345 TRAIN  loss dict:  {'classification_loss': 0.9266472470760345}
2025-01-17 07:31:29,962 [INFO] Step[2200/2713]: training loss : 0.9262115430831909 TRAIN  loss dict:  {'classification_loss': 0.9262115430831909}
2025-01-17 07:31:45,992 [INFO] Step[2250/2713]: training loss : 0.936047431230545 TRAIN  loss dict:  {'classification_loss': 0.936047431230545}
2025-01-17 07:32:01,920 [INFO] Step[2300/2713]: training loss : 0.965449675321579 TRAIN  loss dict:  {'classification_loss': 0.965449675321579}
2025-01-17 07:32:17,915 [INFO] Step[2350/2713]: training loss : 0.9262182295322419 TRAIN  loss dict:  {'classification_loss': 0.9262182295322419}
2025-01-17 07:32:33,937 [INFO] Step[2400/2713]: training loss : 0.9273510193824768 TRAIN  loss dict:  {'classification_loss': 0.9273510193824768}
2025-01-17 07:32:49,951 [INFO] Step[2450/2713]: training loss : 0.9274384677410126 TRAIN  loss dict:  {'classification_loss': 0.9274384677410126}
2025-01-17 07:33:05,886 [INFO] Step[2500/2713]: training loss : 0.9545033013820649 TRAIN  loss dict:  {'classification_loss': 0.9545033013820649}
2025-01-17 07:33:21,934 [INFO] Step[2550/2713]: training loss : 0.9278289270401001 TRAIN  loss dict:  {'classification_loss': 0.9278289270401001}
2025-01-17 07:33:37,952 [INFO] Step[2600/2713]: training loss : 0.929846807718277 TRAIN  loss dict:  {'classification_loss': 0.929846807718277}
2025-01-17 07:33:53,947 [INFO] Step[2650/2713]: training loss : 0.9530895578861237 TRAIN  loss dict:  {'classification_loss': 0.9530895578861237}
2025-01-17 07:34:09,952 [INFO] Step[2700/2713]: training loss : 0.9260507011413575 TRAIN  loss dict:  {'classification_loss': 0.9260507011413575}
2025-01-17 07:35:27,742 [INFO] Label accuracies statistics:
2025-01-17 07:35:27,742 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.5, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 1.0, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.5, 260: 1.0, 261: 0.5, 262: 1.0, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 1.0, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.25, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 07:35:27,744 [INFO] [80] TRAIN  loss: 0.9302554747032759 acc: 0.9990170782651432
2025-01-17 07:35:27,744 [INFO] [80] TRAIN  loss dict: {'classification_loss': 0.9302554747032759}
2025-01-17 07:35:27,744 [INFO] [80] VALIDATION loss: 1.8954411320444335 VALIDATION acc: 0.7987460815047022
2025-01-17 07:35:27,744 [INFO] [80] VALIDATION loss dict: {'classification_loss': 1.8954411320444335}
2025-01-17 07:35:27,744 [INFO] 
2025-01-17 07:35:48,768 [INFO] Step[50/2713]: training loss : 0.9262966489791871 TRAIN  loss dict:  {'classification_loss': 0.9262966489791871}
2025-01-17 07:36:04,728 [INFO] Step[100/2713]: training loss : 0.9265009891986847 TRAIN  loss dict:  {'classification_loss': 0.9265009891986847}
2025-01-17 07:36:20,686 [INFO] Step[150/2713]: training loss : 0.9268898391723632 TRAIN  loss dict:  {'classification_loss': 0.9268898391723632}
2025-01-17 07:36:36,645 [INFO] Step[200/2713]: training loss : 0.9263017916679382 TRAIN  loss dict:  {'classification_loss': 0.9263017916679382}
2025-01-17 07:36:52,670 [INFO] Step[250/2713]: training loss : 0.9265204226970672 TRAIN  loss dict:  {'classification_loss': 0.9265204226970672}
2025-01-17 07:37:08,698 [INFO] Step[300/2713]: training loss : 0.9539875864982605 TRAIN  loss dict:  {'classification_loss': 0.9539875864982605}
2025-01-17 07:37:24,651 [INFO] Step[350/2713]: training loss : 0.9744673001766205 TRAIN  loss dict:  {'classification_loss': 0.9744673001766205}
2025-01-17 07:37:40,588 [INFO] Step[400/2713]: training loss : 0.9264198839664459 TRAIN  loss dict:  {'classification_loss': 0.9264198839664459}
2025-01-17 07:37:56,583 [INFO] Step[450/2713]: training loss : 0.9265728056430816 TRAIN  loss dict:  {'classification_loss': 0.9265728056430816}
2025-01-17 07:38:12,533 [INFO] Step[500/2713]: training loss : 0.9268357717990875 TRAIN  loss dict:  {'classification_loss': 0.9268357717990875}
2025-01-17 07:38:28,515 [INFO] Step[550/2713]: training loss : 0.926203647851944 TRAIN  loss dict:  {'classification_loss': 0.926203647851944}
2025-01-17 07:38:44,511 [INFO] Step[600/2713]: training loss : 0.9257250142097473 TRAIN  loss dict:  {'classification_loss': 0.9257250142097473}
2025-01-17 07:39:00,524 [INFO] Step[650/2713]: training loss : 0.9263384079933167 TRAIN  loss dict:  {'classification_loss': 0.9263384079933167}
2025-01-17 07:39:16,479 [INFO] Step[700/2713]: training loss : 0.9259002101421356 TRAIN  loss dict:  {'classification_loss': 0.9259002101421356}
2025-01-17 07:39:32,444 [INFO] Step[750/2713]: training loss : 0.9262879323959351 TRAIN  loss dict:  {'classification_loss': 0.9262879323959351}
2025-01-17 07:39:48,428 [INFO] Step[800/2713]: training loss : 0.9256937992572785 TRAIN  loss dict:  {'classification_loss': 0.9256937992572785}
2025-01-17 07:40:04,462 [INFO] Step[850/2713]: training loss : 0.9262042927742005 TRAIN  loss dict:  {'classification_loss': 0.9262042927742005}
2025-01-17 07:40:20,425 [INFO] Step[900/2713]: training loss : 0.9266135787963867 TRAIN  loss dict:  {'classification_loss': 0.9266135787963867}
2025-01-17 07:40:36,376 [INFO] Step[950/2713]: training loss : 0.9687376153469086 TRAIN  loss dict:  {'classification_loss': 0.9687376153469086}
2025-01-17 07:40:52,346 [INFO] Step[1000/2713]: training loss : 0.9253588819503784 TRAIN  loss dict:  {'classification_loss': 0.9253588819503784}
2025-01-17 07:41:08,402 [INFO] Step[1050/2713]: training loss : 0.9263915491104125 TRAIN  loss dict:  {'classification_loss': 0.9263915491104125}
2025-01-17 07:41:24,319 [INFO] Step[1100/2713]: training loss : 0.9269664096832275 TRAIN  loss dict:  {'classification_loss': 0.9269664096832275}
2025-01-17 07:41:40,304 [INFO] Step[1150/2713]: training loss : 0.9263802981376648 TRAIN  loss dict:  {'classification_loss': 0.9263802981376648}
2025-01-17 07:41:56,213 [INFO] Step[1200/2713]: training loss : 0.9264631628990173 TRAIN  loss dict:  {'classification_loss': 0.9264631628990173}
2025-01-17 07:42:12,210 [INFO] Step[1250/2713]: training loss : 0.9263123488426208 TRAIN  loss dict:  {'classification_loss': 0.9263123488426208}
2025-01-17 07:42:28,136 [INFO] Step[1300/2713]: training loss : 0.9260303449630737 TRAIN  loss dict:  {'classification_loss': 0.9260303449630737}
2025-01-17 07:42:44,150 [INFO] Step[1350/2713]: training loss : 0.9264084100723267 TRAIN  loss dict:  {'classification_loss': 0.9264084100723267}
2025-01-17 07:43:00,081 [INFO] Step[1400/2713]: training loss : 0.9260651755332947 TRAIN  loss dict:  {'classification_loss': 0.9260651755332947}
2025-01-17 07:43:16,029 [INFO] Step[1450/2713]: training loss : 0.9532547497749329 TRAIN  loss dict:  {'classification_loss': 0.9532547497749329}
2025-01-17 07:43:32,036 [INFO] Step[1500/2713]: training loss : 0.9262995433807373 TRAIN  loss dict:  {'classification_loss': 0.9262995433807373}
2025-01-17 07:43:48,020 [INFO] Step[1550/2713]: training loss : 0.9259069240093232 TRAIN  loss dict:  {'classification_loss': 0.9259069240093232}
2025-01-17 07:44:03,979 [INFO] Step[1600/2713]: training loss : 0.9259759593009949 TRAIN  loss dict:  {'classification_loss': 0.9259759593009949}
2025-01-17 07:44:19,956 [INFO] Step[1650/2713]: training loss : 0.9253082060813904 TRAIN  loss dict:  {'classification_loss': 0.9253082060813904}
2025-01-17 07:44:35,926 [INFO] Step[1700/2713]: training loss : 0.9334945821762085 TRAIN  loss dict:  {'classification_loss': 0.9334945821762085}
2025-01-17 07:44:51,911 [INFO] Step[1750/2713]: training loss : 0.9261981296539307 TRAIN  loss dict:  {'classification_loss': 0.9261981296539307}
2025-01-17 07:45:07,818 [INFO] Step[1800/2713]: training loss : 0.9259169590473175 TRAIN  loss dict:  {'classification_loss': 0.9259169590473175}
2025-01-17 07:45:23,839 [INFO] Step[1850/2713]: training loss : 0.9258010625839234 TRAIN  loss dict:  {'classification_loss': 0.9258010625839234}
2025-01-17 07:45:39,756 [INFO] Step[1900/2713]: training loss : 0.9255176651477813 TRAIN  loss dict:  {'classification_loss': 0.9255176651477813}
2025-01-17 07:45:55,727 [INFO] Step[1950/2713]: training loss : 0.9263527488708496 TRAIN  loss dict:  {'classification_loss': 0.9263527488708496}
2025-01-17 07:46:11,671 [INFO] Step[2000/2713]: training loss : 0.9256745052337646 TRAIN  loss dict:  {'classification_loss': 0.9256745052337646}
2025-01-17 07:46:27,650 [INFO] Step[2050/2713]: training loss : 0.9355043637752533 TRAIN  loss dict:  {'classification_loss': 0.9355043637752533}
2025-01-17 07:46:43,580 [INFO] Step[2100/2713]: training loss : 0.9265852165222168 TRAIN  loss dict:  {'classification_loss': 0.9265852165222168}
2025-01-17 07:46:59,612 [INFO] Step[2150/2713]: training loss : 0.9262192475795746 TRAIN  loss dict:  {'classification_loss': 0.9262192475795746}
2025-01-17 07:47:15,674 [INFO] Step[2200/2713]: training loss : 0.9260870027542114 TRAIN  loss dict:  {'classification_loss': 0.9260870027542114}
2025-01-17 07:47:31,703 [INFO] Step[2250/2713]: training loss : 0.9354067635536194 TRAIN  loss dict:  {'classification_loss': 0.9354067635536194}
2025-01-17 07:47:47,828 [INFO] Step[2300/2713]: training loss : 0.926224125623703 TRAIN  loss dict:  {'classification_loss': 0.926224125623703}
2025-01-17 07:48:03,992 [INFO] Step[2350/2713]: training loss : 0.9257868540287018 TRAIN  loss dict:  {'classification_loss': 0.9257868540287018}
2025-01-17 07:48:20,044 [INFO] Step[2400/2713]: training loss : 0.9268352925777436 TRAIN  loss dict:  {'classification_loss': 0.9268352925777436}
2025-01-17 07:48:36,112 [INFO] Step[2450/2713]: training loss : 0.9262546586990357 TRAIN  loss dict:  {'classification_loss': 0.9262546586990357}
2025-01-17 07:48:52,135 [INFO] Step[2500/2713]: training loss : 0.9265442621707917 TRAIN  loss dict:  {'classification_loss': 0.9265442621707917}
2025-01-17 07:49:08,231 [INFO] Step[2550/2713]: training loss : 0.9273879992961883 TRAIN  loss dict:  {'classification_loss': 0.9273879992961883}
2025-01-17 07:49:24,257 [INFO] Step[2600/2713]: training loss : 0.9337894093990325 TRAIN  loss dict:  {'classification_loss': 0.9337894093990325}
2025-01-17 07:49:40,321 [INFO] Step[2650/2713]: training loss : 0.9284290146827697 TRAIN  loss dict:  {'classification_loss': 0.9284290146827697}
2025-01-17 07:49:56,352 [INFO] Step[2700/2713]: training loss : 0.926830130815506 TRAIN  loss dict:  {'classification_loss': 0.926830130815506}
2025-01-17 07:51:14,442 [INFO] Label accuracies statistics:
2025-01-17 07:51:14,442 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.25, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 1.0, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 0.75, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.5, 339: 0.5, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 07:51:14,444 [INFO] [81] TRAIN  loss: 0.9295871574150167 acc: 0.9990170782651432
2025-01-17 07:51:14,444 [INFO] [81] TRAIN  loss dict: {'classification_loss': 0.9295871574150167}
2025-01-17 07:51:14,444 [INFO] [81] VALIDATION loss: 1.9352834915980361 VALIDATION acc: 0.7918495297805642
2025-01-17 07:51:14,444 [INFO] [81] VALIDATION loss dict: {'classification_loss': 1.9352834915980361}
2025-01-17 07:51:14,444 [INFO] 
2025-01-17 07:51:35,049 [INFO] Step[50/2713]: training loss : 0.9266023468971253 TRAIN  loss dict:  {'classification_loss': 0.9266023468971253}
2025-01-17 07:51:51,025 [INFO] Step[100/2713]: training loss : 0.9264604473114013 TRAIN  loss dict:  {'classification_loss': 0.9264604473114013}
2025-01-17 07:52:07,075 [INFO] Step[150/2713]: training loss : 0.9276093482971192 TRAIN  loss dict:  {'classification_loss': 0.9276093482971192}
2025-01-17 07:52:23,063 [INFO] Step[200/2713]: training loss : 0.9295336472988128 TRAIN  loss dict:  {'classification_loss': 0.9295336472988128}
2025-01-17 07:52:39,122 [INFO] Step[250/2713]: training loss : 0.9286635744571686 TRAIN  loss dict:  {'classification_loss': 0.9286635744571686}
2025-01-17 07:52:55,106 [INFO] Step[300/2713]: training loss : 0.9258053314685821 TRAIN  loss dict:  {'classification_loss': 0.9258053314685821}
2025-01-17 07:53:11,114 [INFO] Step[350/2713]: training loss : 0.9257787013053894 TRAIN  loss dict:  {'classification_loss': 0.9257787013053894}
2025-01-17 07:53:27,154 [INFO] Step[400/2713]: training loss : 0.9259517514705657 TRAIN  loss dict:  {'classification_loss': 0.9259517514705657}
2025-01-17 07:53:43,226 [INFO] Step[450/2713]: training loss : 0.9305007243156433 TRAIN  loss dict:  {'classification_loss': 0.9305007243156433}
2025-01-17 07:53:59,196 [INFO] Step[500/2713]: training loss : 0.9299209678173065 TRAIN  loss dict:  {'classification_loss': 0.9299209678173065}
2025-01-17 07:54:15,181 [INFO] Step[550/2713]: training loss : 0.9261050951480866 TRAIN  loss dict:  {'classification_loss': 0.9261050951480866}
2025-01-17 07:54:31,148 [INFO] Step[600/2713]: training loss : 0.9257642805576325 TRAIN  loss dict:  {'classification_loss': 0.9257642805576325}
2025-01-17 07:54:47,143 [INFO] Step[650/2713]: training loss : 0.9260645687580109 TRAIN  loss dict:  {'classification_loss': 0.9260645687580109}
2025-01-17 07:55:03,160 [INFO] Step[700/2713]: training loss : 0.9255749142169952 TRAIN  loss dict:  {'classification_loss': 0.9255749142169952}
2025-01-17 07:55:19,261 [INFO] Step[750/2713]: training loss : 0.9290095365047455 TRAIN  loss dict:  {'classification_loss': 0.9290095365047455}
2025-01-17 07:55:35,284 [INFO] Step[800/2713]: training loss : 0.9287818968296051 TRAIN  loss dict:  {'classification_loss': 0.9287818968296051}
2025-01-17 07:55:51,330 [INFO] Step[850/2713]: training loss : 0.926871873140335 TRAIN  loss dict:  {'classification_loss': 0.926871873140335}
2025-01-17 07:56:07,385 [INFO] Step[900/2713]: training loss : 0.9264066970348358 TRAIN  loss dict:  {'classification_loss': 0.9264066970348358}
2025-01-17 07:56:23,469 [INFO] Step[950/2713]: training loss : 0.9260281908512116 TRAIN  loss dict:  {'classification_loss': 0.9260281908512116}
2025-01-17 07:56:39,514 [INFO] Step[1000/2713]: training loss : 0.9267819988727569 TRAIN  loss dict:  {'classification_loss': 0.9267819988727569}
2025-01-17 07:56:55,536 [INFO] Step[1050/2713]: training loss : 0.9266363477706909 TRAIN  loss dict:  {'classification_loss': 0.9266363477706909}
2025-01-17 07:57:11,530 [INFO] Step[1100/2713]: training loss : 0.926240234375 TRAIN  loss dict:  {'classification_loss': 0.926240234375}
2025-01-17 07:57:27,527 [INFO] Step[1150/2713]: training loss : 0.9264197587966919 TRAIN  loss dict:  {'classification_loss': 0.9264197587966919}
2025-01-17 07:57:43,548 [INFO] Step[1200/2713]: training loss : 0.9288031351566315 TRAIN  loss dict:  {'classification_loss': 0.9288031351566315}
2025-01-17 07:57:59,611 [INFO] Step[1250/2713]: training loss : 0.9263521826267243 TRAIN  loss dict:  {'classification_loss': 0.9263521826267243}
2025-01-17 07:58:15,581 [INFO] Step[1300/2713]: training loss : 0.9259839606285095 TRAIN  loss dict:  {'classification_loss': 0.9259839606285095}
2025-01-17 07:58:31,582 [INFO] Step[1350/2713]: training loss : 0.9262708532810211 TRAIN  loss dict:  {'classification_loss': 0.9262708532810211}
2025-01-17 07:58:47,607 [INFO] Step[1400/2713]: training loss : 0.9265814292430877 TRAIN  loss dict:  {'classification_loss': 0.9265814292430877}
2025-01-17 07:59:03,617 [INFO] Step[1450/2713]: training loss : 0.9289084720611572 TRAIN  loss dict:  {'classification_loss': 0.9289084720611572}
2025-01-17 07:59:19,641 [INFO] Step[1500/2713]: training loss : 0.9269618332386017 TRAIN  loss dict:  {'classification_loss': 0.9269618332386017}
2025-01-17 07:59:35,675 [INFO] Step[1550/2713]: training loss : 0.9259529542922974 TRAIN  loss dict:  {'classification_loss': 0.9259529542922974}
2025-01-17 07:59:51,622 [INFO] Step[1600/2713]: training loss : 0.9261124491691589 TRAIN  loss dict:  {'classification_loss': 0.9261124491691589}
2025-01-17 08:00:07,639 [INFO] Step[1650/2713]: training loss : 0.9265965008735657 TRAIN  loss dict:  {'classification_loss': 0.9265965008735657}
2025-01-17 08:00:23,594 [INFO] Step[1700/2713]: training loss : 0.9269626462459564 TRAIN  loss dict:  {'classification_loss': 0.9269626462459564}
2025-01-17 08:00:39,674 [INFO] Step[1750/2713]: training loss : 0.9263685965538024 TRAIN  loss dict:  {'classification_loss': 0.9263685965538024}
2025-01-17 08:00:55,668 [INFO] Step[1800/2713]: training loss : 0.9265945744514466 TRAIN  loss dict:  {'classification_loss': 0.9265945744514466}
2025-01-17 08:01:11,665 [INFO] Step[1850/2713]: training loss : 0.9358951818943023 TRAIN  loss dict:  {'classification_loss': 0.9358951818943023}
2025-01-17 08:01:27,649 [INFO] Step[1900/2713]: training loss : 0.9506254744529724 TRAIN  loss dict:  {'classification_loss': 0.9506254744529724}
2025-01-17 08:01:43,722 [INFO] Step[1950/2713]: training loss : 0.9632175314426422 TRAIN  loss dict:  {'classification_loss': 0.9632175314426422}
2025-01-17 08:01:59,708 [INFO] Step[2000/2713]: training loss : 0.9256689596176148 TRAIN  loss dict:  {'classification_loss': 0.9256689596176148}
2025-01-17 08:02:15,742 [INFO] Step[2050/2713]: training loss : 0.9259000718593597 TRAIN  loss dict:  {'classification_loss': 0.9259000718593597}
2025-01-17 08:02:31,768 [INFO] Step[2100/2713]: training loss : 0.9383235394954681 TRAIN  loss dict:  {'classification_loss': 0.9383235394954681}
2025-01-17 08:02:47,795 [INFO] Step[2150/2713]: training loss : 0.9259132051467895 TRAIN  loss dict:  {'classification_loss': 0.9259132051467895}
2025-01-17 08:03:03,839 [INFO] Step[2200/2713]: training loss : 0.9266823995113372 TRAIN  loss dict:  {'classification_loss': 0.9266823995113372}
2025-01-17 08:03:19,867 [INFO] Step[2250/2713]: training loss : 0.926301760673523 TRAIN  loss dict:  {'classification_loss': 0.926301760673523}
2025-01-17 08:03:35,919 [INFO] Step[2300/2713]: training loss : 0.926053626537323 TRAIN  loss dict:  {'classification_loss': 0.926053626537323}
2025-01-17 08:03:52,023 [INFO] Step[2350/2713]: training loss : 0.9274643182754516 TRAIN  loss dict:  {'classification_loss': 0.9274643182754516}
2025-01-17 08:04:08,021 [INFO] Step[2400/2713]: training loss : 0.9334878540039062 TRAIN  loss dict:  {'classification_loss': 0.9334878540039062}
2025-01-17 08:04:24,061 [INFO] Step[2450/2713]: training loss : 0.9266852116584778 TRAIN  loss dict:  {'classification_loss': 0.9266852116584778}
2025-01-17 08:04:40,075 [INFO] Step[2500/2713]: training loss : 0.9357400417327881 TRAIN  loss dict:  {'classification_loss': 0.9357400417327881}
2025-01-17 08:04:56,112 [INFO] Step[2550/2713]: training loss : 0.9291761779785156 TRAIN  loss dict:  {'classification_loss': 0.9291761779785156}
2025-01-17 08:05:12,211 [INFO] Step[2600/2713]: training loss : 0.9263436198234558 TRAIN  loss dict:  {'classification_loss': 0.9263436198234558}
2025-01-17 08:05:28,264 [INFO] Step[2650/2713]: training loss : 0.9264634418487548 TRAIN  loss dict:  {'classification_loss': 0.9264634418487548}
2025-01-17 08:05:44,303 [INFO] Step[2700/2713]: training loss : 0.9262394797801972 TRAIN  loss dict:  {'classification_loss': 0.9262394797801972}
2025-01-17 08:07:02,137 [INFO] Label accuracies statistics:
2025-01-17 08:07:02,138 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.5, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.75, 225: 1.0, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 1.0, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 08:07:02,139 [INFO] [82] TRAIN  loss: 0.9286617756417173 acc: 0.9992628086988573
2025-01-17 08:07:02,140 [INFO] [82] TRAIN  loss dict: {'classification_loss': 0.9286617756417173}
2025-01-17 08:07:02,140 [INFO] [82] VALIDATION loss: 1.897251305051316 VALIDATION acc: 0.7981191222570533
2025-01-17 08:07:02,140 [INFO] [82] VALIDATION loss dict: {'classification_loss': 1.897251305051316}
2025-01-17 08:07:02,140 [INFO] 
2025-01-17 08:07:23,228 [INFO] Step[50/2713]: training loss : 0.9257269299030304 TRAIN  loss dict:  {'classification_loss': 0.9257269299030304}
2025-01-17 08:07:39,262 [INFO] Step[100/2713]: training loss : 0.9261380791664123 TRAIN  loss dict:  {'classification_loss': 0.9261380791664123}
2025-01-17 08:07:55,265 [INFO] Step[150/2713]: training loss : 0.9262361478805542 TRAIN  loss dict:  {'classification_loss': 0.9262361478805542}
2025-01-17 08:08:11,277 [INFO] Step[200/2713]: training loss : 0.9258927857875824 TRAIN  loss dict:  {'classification_loss': 0.9258927857875824}
2025-01-17 08:08:27,316 [INFO] Step[250/2713]: training loss : 0.9257524538040162 TRAIN  loss dict:  {'classification_loss': 0.9257524538040162}
2025-01-17 08:08:43,388 [INFO] Step[300/2713]: training loss : 0.9267623972892761 TRAIN  loss dict:  {'classification_loss': 0.9267623972892761}
2025-01-17 08:08:59,473 [INFO] Step[350/2713]: training loss : 0.9261166036128998 TRAIN  loss dict:  {'classification_loss': 0.9261166036128998}
2025-01-17 08:09:15,487 [INFO] Step[400/2713]: training loss : 0.9261414802074432 TRAIN  loss dict:  {'classification_loss': 0.9261414802074432}
2025-01-17 08:09:31,540 [INFO] Step[450/2713]: training loss : 0.9273543250560761 TRAIN  loss dict:  {'classification_loss': 0.9273543250560761}
2025-01-17 08:09:47,593 [INFO] Step[500/2713]: training loss : 0.925481299161911 TRAIN  loss dict:  {'classification_loss': 0.925481299161911}
2025-01-17 08:10:03,603 [INFO] Step[550/2713]: training loss : 0.9267262470722198 TRAIN  loss dict:  {'classification_loss': 0.9267262470722198}
2025-01-17 08:10:19,642 [INFO] Step[600/2713]: training loss : 0.9262424421310425 TRAIN  loss dict:  {'classification_loss': 0.9262424421310425}
2025-01-17 08:10:35,648 [INFO] Step[650/2713]: training loss : 0.95728609085083 TRAIN  loss dict:  {'classification_loss': 0.95728609085083}
2025-01-17 08:10:51,671 [INFO] Step[700/2713]: training loss : 0.92569127202034 TRAIN  loss dict:  {'classification_loss': 0.92569127202034}
2025-01-17 08:11:07,712 [INFO] Step[750/2713]: training loss : 0.9260607469081878 TRAIN  loss dict:  {'classification_loss': 0.9260607469081878}
2025-01-17 08:11:23,790 [INFO] Step[800/2713]: training loss : 0.9264340198040009 TRAIN  loss dict:  {'classification_loss': 0.9264340198040009}
2025-01-17 08:11:39,893 [INFO] Step[850/2713]: training loss : 0.9262226939201355 TRAIN  loss dict:  {'classification_loss': 0.9262226939201355}
2025-01-17 08:11:55,953 [INFO] Step[900/2713]: training loss : 0.9310026645660401 TRAIN  loss dict:  {'classification_loss': 0.9310026645660401}
2025-01-17 08:12:12,026 [INFO] Step[950/2713]: training loss : 0.9300398480892181 TRAIN  loss dict:  {'classification_loss': 0.9300398480892181}
2025-01-17 08:12:28,064 [INFO] Step[1000/2713]: training loss : 0.92644646525383 TRAIN  loss dict:  {'classification_loss': 0.92644646525383}
2025-01-17 08:12:44,054 [INFO] Step[1050/2713]: training loss : 0.926210207939148 TRAIN  loss dict:  {'classification_loss': 0.926210207939148}
2025-01-17 08:13:00,115 [INFO] Step[1100/2713]: training loss : 0.9550886404514313 TRAIN  loss dict:  {'classification_loss': 0.9550886404514313}
2025-01-17 08:13:16,151 [INFO] Step[1150/2713]: training loss : 0.9674432945251464 TRAIN  loss dict:  {'classification_loss': 0.9674432945251464}
2025-01-17 08:13:32,203 [INFO] Step[1200/2713]: training loss : 0.9261466109752655 TRAIN  loss dict:  {'classification_loss': 0.9261466109752655}
2025-01-17 08:13:48,251 [INFO] Step[1250/2713]: training loss : 0.9258088088035583 TRAIN  loss dict:  {'classification_loss': 0.9258088088035583}
2025-01-17 08:14:04,255 [INFO] Step[1300/2713]: training loss : 0.9266864514350891 TRAIN  loss dict:  {'classification_loss': 0.9266864514350891}
2025-01-17 08:14:20,310 [INFO] Step[1350/2713]: training loss : 0.9257833087444305 TRAIN  loss dict:  {'classification_loss': 0.9257833087444305}
2025-01-17 08:14:36,351 [INFO] Step[1400/2713]: training loss : 0.9262252616882324 TRAIN  loss dict:  {'classification_loss': 0.9262252616882324}
2025-01-17 08:14:52,451 [INFO] Step[1450/2713]: training loss : 0.9362499499320984 TRAIN  loss dict:  {'classification_loss': 0.9362499499320984}
2025-01-17 08:15:08,492 [INFO] Step[1500/2713]: training loss : 0.9261317694187164 TRAIN  loss dict:  {'classification_loss': 0.9261317694187164}
2025-01-17 08:15:24,492 [INFO] Step[1550/2713]: training loss : 0.926332368850708 TRAIN  loss dict:  {'classification_loss': 0.926332368850708}
2025-01-17 08:15:40,544 [INFO] Step[1600/2713]: training loss : 0.926367597579956 TRAIN  loss dict:  {'classification_loss': 0.926367597579956}
2025-01-17 08:15:56,598 [INFO] Step[1650/2713]: training loss : 0.9267278027534485 TRAIN  loss dict:  {'classification_loss': 0.9267278027534485}
2025-01-17 08:16:12,599 [INFO] Step[1700/2713]: training loss : 0.9259249377250671 TRAIN  loss dict:  {'classification_loss': 0.9259249377250671}
2025-01-17 08:16:28,627 [INFO] Step[1750/2713]: training loss : 0.9262594771385193 TRAIN  loss dict:  {'classification_loss': 0.9262594771385193}
2025-01-17 08:16:44,676 [INFO] Step[1800/2713]: training loss : 0.9268320524692535 TRAIN  loss dict:  {'classification_loss': 0.9268320524692535}
2025-01-17 08:17:00,705 [INFO] Step[1850/2713]: training loss : 0.9656109976768493 TRAIN  loss dict:  {'classification_loss': 0.9656109976768493}
2025-01-17 08:17:16,696 [INFO] Step[1900/2713]: training loss : 0.9308211040496827 TRAIN  loss dict:  {'classification_loss': 0.9308211040496827}
2025-01-17 08:17:32,721 [INFO] Step[1950/2713]: training loss : 0.926285766363144 TRAIN  loss dict:  {'classification_loss': 0.926285766363144}
2025-01-17 08:17:48,709 [INFO] Step[2000/2713]: training loss : 0.9274887096881866 TRAIN  loss dict:  {'classification_loss': 0.9274887096881866}
2025-01-17 08:18:04,711 [INFO] Step[2050/2713]: training loss : 0.9260000431537628 TRAIN  loss dict:  {'classification_loss': 0.9260000431537628}
2025-01-17 08:18:20,752 [INFO] Step[2100/2713]: training loss : 0.9255878603458405 TRAIN  loss dict:  {'classification_loss': 0.9255878603458405}
2025-01-17 08:18:36,764 [INFO] Step[2150/2713]: training loss : 0.9283073008060455 TRAIN  loss dict:  {'classification_loss': 0.9283073008060455}
2025-01-17 08:18:52,851 [INFO] Step[2200/2713]: training loss : 0.9266850006580353 TRAIN  loss dict:  {'classification_loss': 0.9266850006580353}
2025-01-17 08:19:08,893 [INFO] Step[2250/2713]: training loss : 0.9263110148906708 TRAIN  loss dict:  {'classification_loss': 0.9263110148906708}
2025-01-17 08:19:24,957 [INFO] Step[2300/2713]: training loss : 0.9258317410945892 TRAIN  loss dict:  {'classification_loss': 0.9258317410945892}
2025-01-17 08:19:41,015 [INFO] Step[2350/2713]: training loss : 0.9259620630741119 TRAIN  loss dict:  {'classification_loss': 0.9259620630741119}
2025-01-17 08:19:56,991 [INFO] Step[2400/2713]: training loss : 0.9262858784198761 TRAIN  loss dict:  {'classification_loss': 0.9262858784198761}
2025-01-17 08:20:13,001 [INFO] Step[2450/2713]: training loss : 0.9271052849292755 TRAIN  loss dict:  {'classification_loss': 0.9271052849292755}
2025-01-17 08:20:29,007 [INFO] Step[2500/2713]: training loss : 0.9262107467651367 TRAIN  loss dict:  {'classification_loss': 0.9262107467651367}
2025-01-17 08:20:45,051 [INFO] Step[2550/2713]: training loss : 0.9258601462841034 TRAIN  loss dict:  {'classification_loss': 0.9258601462841034}
2025-01-17 08:21:01,036 [INFO] Step[2600/2713]: training loss : 0.9261891758441925 TRAIN  loss dict:  {'classification_loss': 0.9261891758441925}
2025-01-17 08:21:17,051 [INFO] Step[2650/2713]: training loss : 0.945602469444275 TRAIN  loss dict:  {'classification_loss': 0.945602469444275}
2025-01-17 08:21:33,000 [INFO] Step[2700/2713]: training loss : 0.929488501548767 TRAIN  loss dict:  {'classification_loss': 0.929488501548767}
2025-01-17 08:22:50,721 [INFO] Label accuracies statistics:
2025-01-17 08:22:50,721 [INFO] {0: 0.0, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 1.0, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.25, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 1.0, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 08:22:50,723 [INFO] [83] TRAIN  loss: 0.9297156542418762 acc: 0.9992628086988573
2025-01-17 08:22:50,723 [INFO] [83] TRAIN  loss dict: {'classification_loss': 0.9297156542418762}
2025-01-17 08:22:50,723 [INFO] [83] VALIDATION loss: 1.8722256907170876 VALIDATION acc: 0.8056426332288401
2025-01-17 08:22:50,723 [INFO] [83] VALIDATION loss dict: {'classification_loss': 1.8722256907170876}
2025-01-17 08:22:50,723 [INFO] 
2025-01-17 08:23:11,843 [INFO] Step[50/2713]: training loss : 0.926145384311676 TRAIN  loss dict:  {'classification_loss': 0.926145384311676}
2025-01-17 08:23:27,887 [INFO] Step[100/2713]: training loss : 0.9263439846038818 TRAIN  loss dict:  {'classification_loss': 0.9263439846038818}
2025-01-17 08:23:44,020 [INFO] Step[150/2713]: training loss : 0.9259802067279815 TRAIN  loss dict:  {'classification_loss': 0.9259802067279815}
2025-01-17 08:24:00,064 [INFO] Step[200/2713]: training loss : 0.9283149182796478 TRAIN  loss dict:  {'classification_loss': 0.9283149182796478}
2025-01-17 08:24:16,129 [INFO] Step[250/2713]: training loss : 0.9260955786705017 TRAIN  loss dict:  {'classification_loss': 0.9260955786705017}
2025-01-17 08:24:32,168 [INFO] Step[300/2713]: training loss : 0.9289799988269806 TRAIN  loss dict:  {'classification_loss': 0.9289799988269806}
2025-01-17 08:24:48,280 [INFO] Step[350/2713]: training loss : 0.9261671817302704 TRAIN  loss dict:  {'classification_loss': 0.9261671817302704}
2025-01-17 08:25:04,319 [INFO] Step[400/2713]: training loss : 0.9262866568565369 TRAIN  loss dict:  {'classification_loss': 0.9262866568565369}
2025-01-17 08:25:20,452 [INFO] Step[450/2713]: training loss : 0.9342857265472412 TRAIN  loss dict:  {'classification_loss': 0.9342857265472412}
2025-01-17 08:25:36,532 [INFO] Step[500/2713]: training loss : 0.9259151339530944 TRAIN  loss dict:  {'classification_loss': 0.9259151339530944}
2025-01-17 08:25:52,562 [INFO] Step[550/2713]: training loss : 0.9266166985034943 TRAIN  loss dict:  {'classification_loss': 0.9266166985034943}
2025-01-17 08:26:08,599 [INFO] Step[600/2713]: training loss : 0.9264054119586944 TRAIN  loss dict:  {'classification_loss': 0.9264054119586944}
2025-01-17 08:26:24,677 [INFO] Step[650/2713]: training loss : 0.9262492036819459 TRAIN  loss dict:  {'classification_loss': 0.9262492036819459}
2025-01-17 08:26:40,762 [INFO] Step[700/2713]: training loss : 0.9279605293273926 TRAIN  loss dict:  {'classification_loss': 0.9279605293273926}
2025-01-17 08:26:56,820 [INFO] Step[750/2713]: training loss : 0.9256581151485443 TRAIN  loss dict:  {'classification_loss': 0.9256581151485443}
2025-01-17 08:27:12,832 [INFO] Step[800/2713]: training loss : 0.9263461995124816 TRAIN  loss dict:  {'classification_loss': 0.9263461995124816}
2025-01-17 08:27:28,925 [INFO] Step[850/2713]: training loss : 0.9261862063407897 TRAIN  loss dict:  {'classification_loss': 0.9261862063407897}
2025-01-17 08:27:44,990 [INFO] Step[900/2713]: training loss : 0.9261200428009033 TRAIN  loss dict:  {'classification_loss': 0.9261200428009033}
2025-01-17 08:28:01,057 [INFO] Step[950/2713]: training loss : 0.9264148283004761 TRAIN  loss dict:  {'classification_loss': 0.9264148283004761}
2025-01-17 08:28:17,084 [INFO] Step[1000/2713]: training loss : 0.9259355807304382 TRAIN  loss dict:  {'classification_loss': 0.9259355807304382}
2025-01-17 08:28:33,186 [INFO] Step[1050/2713]: training loss : 0.9267536449432373 TRAIN  loss dict:  {'classification_loss': 0.9267536449432373}
2025-01-17 08:28:49,247 [INFO] Step[1100/2713]: training loss : 0.9280367004871368 TRAIN  loss dict:  {'classification_loss': 0.9280367004871368}
2025-01-17 08:29:05,343 [INFO] Step[1150/2713]: training loss : 0.9261761701107025 TRAIN  loss dict:  {'classification_loss': 0.9261761701107025}
2025-01-17 08:29:21,373 [INFO] Step[1200/2713]: training loss : 0.9257955205440521 TRAIN  loss dict:  {'classification_loss': 0.9257955205440521}
2025-01-17 08:29:37,473 [INFO] Step[1250/2713]: training loss : 0.9261114454269409 TRAIN  loss dict:  {'classification_loss': 0.9261114454269409}
2025-01-17 08:29:53,464 [INFO] Step[1300/2713]: training loss : 0.9262996280193329 TRAIN  loss dict:  {'classification_loss': 0.9262996280193329}
2025-01-17 08:30:09,550 [INFO] Step[1350/2713]: training loss : 0.9259382307529449 TRAIN  loss dict:  {'classification_loss': 0.9259382307529449}
2025-01-17 08:30:25,582 [INFO] Step[1400/2713]: training loss : 0.9265686786174774 TRAIN  loss dict:  {'classification_loss': 0.9265686786174774}
2025-01-17 08:30:41,634 [INFO] Step[1450/2713]: training loss : 0.9260058629512787 TRAIN  loss dict:  {'classification_loss': 0.9260058629512787}
2025-01-17 08:30:57,651 [INFO] Step[1500/2713]: training loss : 0.9264913547039032 TRAIN  loss dict:  {'classification_loss': 0.9264913547039032}
2025-01-17 08:31:13,716 [INFO] Step[1550/2713]: training loss : 0.9257683265209198 TRAIN  loss dict:  {'classification_loss': 0.9257683265209198}
2025-01-17 08:31:29,708 [INFO] Step[1600/2713]: training loss : 0.94457634806633 TRAIN  loss dict:  {'classification_loss': 0.94457634806633}
2025-01-17 08:31:45,787 [INFO] Step[1650/2713]: training loss : 0.9259993612766266 TRAIN  loss dict:  {'classification_loss': 0.9259993612766266}
2025-01-17 08:32:01,830 [INFO] Step[1700/2713]: training loss : 0.9262407469749451 TRAIN  loss dict:  {'classification_loss': 0.9262407469749451}
2025-01-17 08:32:17,848 [INFO] Step[1750/2713]: training loss : 0.9262761855125428 TRAIN  loss dict:  {'classification_loss': 0.9262761855125428}
2025-01-17 08:32:33,913 [INFO] Step[1800/2713]: training loss : 0.9266118085384369 TRAIN  loss dict:  {'classification_loss': 0.9266118085384369}
2025-01-17 08:32:49,984 [INFO] Step[1850/2713]: training loss : 0.926147049665451 TRAIN  loss dict:  {'classification_loss': 0.926147049665451}
2025-01-17 08:33:06,094 [INFO] Step[1900/2713]: training loss : 0.9263507449626922 TRAIN  loss dict:  {'classification_loss': 0.9263507449626922}
2025-01-17 08:33:22,357 [INFO] Step[1950/2713]: training loss : 0.9260334479808807 TRAIN  loss dict:  {'classification_loss': 0.9260334479808807}
2025-01-17 08:33:38,563 [INFO] Step[2000/2713]: training loss : 0.9261338484287261 TRAIN  loss dict:  {'classification_loss': 0.9261338484287261}
2025-01-17 08:33:54,770 [INFO] Step[2050/2713]: training loss : 0.9253846061229706 TRAIN  loss dict:  {'classification_loss': 0.9253846061229706}
2025-01-17 08:34:10,908 [INFO] Step[2100/2713]: training loss : 0.9260317099094391 TRAIN  loss dict:  {'classification_loss': 0.9260317099094391}
2025-01-17 08:34:27,111 [INFO] Step[2150/2713]: training loss : 0.9259153807163238 TRAIN  loss dict:  {'classification_loss': 0.9259153807163238}
2025-01-17 08:34:43,280 [INFO] Step[2200/2713]: training loss : 0.9259608852863311 TRAIN  loss dict:  {'classification_loss': 0.9259608852863311}
2025-01-17 08:34:59,521 [INFO] Step[2250/2713]: training loss : 0.9261636245250702 TRAIN  loss dict:  {'classification_loss': 0.9261636245250702}
2025-01-17 08:35:15,627 [INFO] Step[2300/2713]: training loss : 0.9319595074653626 TRAIN  loss dict:  {'classification_loss': 0.9319595074653626}
2025-01-17 08:35:31,902 [INFO] Step[2350/2713]: training loss : 0.9259581863880157 TRAIN  loss dict:  {'classification_loss': 0.9259581863880157}
2025-01-17 08:35:48,075 [INFO] Step[2400/2713]: training loss : 0.9694300937652588 TRAIN  loss dict:  {'classification_loss': 0.9694300937652588}
2025-01-17 08:36:04,239 [INFO] Step[2450/2713]: training loss : 0.9267344808578492 TRAIN  loss dict:  {'classification_loss': 0.9267344808578492}
2025-01-17 08:36:20,383 [INFO] Step[2500/2713]: training loss : 0.9274978542327881 TRAIN  loss dict:  {'classification_loss': 0.9274978542327881}
2025-01-17 08:36:36,506 [INFO] Step[2550/2713]: training loss : 0.9260731422901154 TRAIN  loss dict:  {'classification_loss': 0.9260731422901154}
2025-01-17 08:36:52,569 [INFO] Step[2600/2713]: training loss : 0.9261152875423432 TRAIN  loss dict:  {'classification_loss': 0.9261152875423432}
2025-01-17 08:37:08,602 [INFO] Step[2650/2713]: training loss : 0.9260848379135131 TRAIN  loss dict:  {'classification_loss': 0.9260848379135131}
2025-01-17 08:37:24,745 [INFO] Step[2700/2713]: training loss : 0.9255321860313416 TRAIN  loss dict:  {'classification_loss': 0.9255321860313416}
2025-01-17 08:38:42,824 [INFO] Label accuracies statistics:
2025-01-17 08:38:42,824 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 1.0, 237: 0.5, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.25, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.5, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 08:38:42,827 [INFO] [84] TRAIN  loss: 0.927723398671746 acc: 0.9995085391325715
2025-01-17 08:38:42,827 [INFO] [84] TRAIN  loss dict: {'classification_loss': 0.927723398671746}
2025-01-17 08:38:42,827 [INFO] [84] VALIDATION loss: 1.8580466218684848 VALIDATION acc: 0.8043887147335423
2025-01-17 08:38:42,827 [INFO] [84] VALIDATION loss dict: {'classification_loss': 1.8580466218684848}
2025-01-17 08:38:42,827 [INFO] 
2025-01-17 08:39:04,087 [INFO] Step[50/2713]: training loss : 0.9263581705093383 TRAIN  loss dict:  {'classification_loss': 0.9263581705093383}
2025-01-17 08:39:20,176 [INFO] Step[100/2713]: training loss : 0.9263098812103272 TRAIN  loss dict:  {'classification_loss': 0.9263098812103272}
2025-01-17 08:39:36,479 [INFO] Step[150/2713]: training loss : 0.9262735342979431 TRAIN  loss dict:  {'classification_loss': 0.9262735342979431}
2025-01-17 08:39:52,809 [INFO] Step[200/2713]: training loss : 0.9264450073242188 TRAIN  loss dict:  {'classification_loss': 0.9264450073242188}
2025-01-17 08:40:08,973 [INFO] Step[250/2713]: training loss : 0.9253540563583375 TRAIN  loss dict:  {'classification_loss': 0.9253540563583375}
2025-01-17 08:40:25,284 [INFO] Step[300/2713]: training loss : 0.92607368350029 TRAIN  loss dict:  {'classification_loss': 0.92607368350029}
2025-01-17 08:40:41,566 [INFO] Step[350/2713]: training loss : 0.9257704174518585 TRAIN  loss dict:  {'classification_loss': 0.9257704174518585}
2025-01-17 08:40:57,798 [INFO] Step[400/2713]: training loss : 0.9261568939685821 TRAIN  loss dict:  {'classification_loss': 0.9261568939685821}
2025-01-17 08:41:14,127 [INFO] Step[450/2713]: training loss : 0.9269460344314575 TRAIN  loss dict:  {'classification_loss': 0.9269460344314575}
2025-01-17 08:41:30,284 [INFO] Step[500/2713]: training loss : 0.9260123229026794 TRAIN  loss dict:  {'classification_loss': 0.9260123229026794}
2025-01-17 08:41:46,473 [INFO] Step[550/2713]: training loss : 0.9263996338844299 TRAIN  loss dict:  {'classification_loss': 0.9263996338844299}
2025-01-17 08:42:02,696 [INFO] Step[600/2713]: training loss : 0.9259553992748261 TRAIN  loss dict:  {'classification_loss': 0.9259553992748261}
2025-01-17 08:42:18,873 [INFO] Step[650/2713]: training loss : 0.9259799826145172 TRAIN  loss dict:  {'classification_loss': 0.9259799826145172}
2025-01-17 08:42:35,060 [INFO] Step[700/2713]: training loss : 0.9261961019039154 TRAIN  loss dict:  {'classification_loss': 0.9261961019039154}
2025-01-17 08:42:51,351 [INFO] Step[750/2713]: training loss : 0.9262940108776092 TRAIN  loss dict:  {'classification_loss': 0.9262940108776092}
2025-01-17 08:43:07,527 [INFO] Step[800/2713]: training loss : 0.9262597537040711 TRAIN  loss dict:  {'classification_loss': 0.9262597537040711}
2025-01-17 08:43:23,760 [INFO] Step[850/2713]: training loss : 0.9258411931991577 TRAIN  loss dict:  {'classification_loss': 0.9258411931991577}
2025-01-17 08:43:40,004 [INFO] Step[900/2713]: training loss : 0.9259622609615326 TRAIN  loss dict:  {'classification_loss': 0.9259622609615326}
2025-01-17 08:43:56,251 [INFO] Step[950/2713]: training loss : 0.9263413083553315 TRAIN  loss dict:  {'classification_loss': 0.9263413083553315}
2025-01-17 08:44:12,386 [INFO] Step[1000/2713]: training loss : 0.957632863521576 TRAIN  loss dict:  {'classification_loss': 0.957632863521576}
2025-01-17 08:44:28,626 [INFO] Step[1050/2713]: training loss : 0.9259020709991455 TRAIN  loss dict:  {'classification_loss': 0.9259020709991455}
2025-01-17 08:44:44,824 [INFO] Step[1100/2713]: training loss : 0.9257016754150391 TRAIN  loss dict:  {'classification_loss': 0.9257016754150391}
2025-01-17 08:45:01,066 [INFO] Step[1150/2713]: training loss : 0.9265656852722168 TRAIN  loss dict:  {'classification_loss': 0.9265656852722168}
2025-01-17 08:45:17,273 [INFO] Step[1200/2713]: training loss : 0.9259500932693482 TRAIN  loss dict:  {'classification_loss': 0.9259500932693482}
2025-01-17 08:45:33,594 [INFO] Step[1250/2713]: training loss : 0.925888032913208 TRAIN  loss dict:  {'classification_loss': 0.925888032913208}
2025-01-17 08:45:49,779 [INFO] Step[1300/2713]: training loss : 0.9257786858081818 TRAIN  loss dict:  {'classification_loss': 0.9257786858081818}
2025-01-17 08:46:06,037 [INFO] Step[1350/2713]: training loss : 0.9262122201919556 TRAIN  loss dict:  {'classification_loss': 0.9262122201919556}
2025-01-17 08:46:22,227 [INFO] Step[1400/2713]: training loss : 0.9257463467121124 TRAIN  loss dict:  {'classification_loss': 0.9257463467121124}
2025-01-17 08:46:38,433 [INFO] Step[1450/2713]: training loss : 0.9260459756851196 TRAIN  loss dict:  {'classification_loss': 0.9260459756851196}
2025-01-17 08:46:54,619 [INFO] Step[1500/2713]: training loss : 0.9260852432250977 TRAIN  loss dict:  {'classification_loss': 0.9260852432250977}
2025-01-17 08:47:10,721 [INFO] Step[1550/2713]: training loss : 0.9299197208881378 TRAIN  loss dict:  {'classification_loss': 0.9299197208881378}
2025-01-17 08:47:26,869 [INFO] Step[1600/2713]: training loss : 0.9256837058067322 TRAIN  loss dict:  {'classification_loss': 0.9256837058067322}
2025-01-17 08:47:43,057 [INFO] Step[1650/2713]: training loss : 0.9269034850597382 TRAIN  loss dict:  {'classification_loss': 0.9269034850597382}
2025-01-17 08:47:59,330 [INFO] Step[1700/2713]: training loss : 0.925800712108612 TRAIN  loss dict:  {'classification_loss': 0.925800712108612}
2025-01-17 08:48:15,506 [INFO] Step[1750/2713]: training loss : 0.925901757478714 TRAIN  loss dict:  {'classification_loss': 0.925901757478714}
2025-01-17 08:48:31,793 [INFO] Step[1800/2713]: training loss : 0.9359795761108398 TRAIN  loss dict:  {'classification_loss': 0.9359795761108398}
2025-01-17 08:48:48,010 [INFO] Step[1850/2713]: training loss : 0.9260362207889556 TRAIN  loss dict:  {'classification_loss': 0.9260362207889556}
2025-01-17 08:49:04,219 [INFO] Step[1900/2713]: training loss : 0.9259078133106232 TRAIN  loss dict:  {'classification_loss': 0.9259078133106232}
2025-01-17 08:49:20,399 [INFO] Step[1950/2713]: training loss : 0.9259702396392823 TRAIN  loss dict:  {'classification_loss': 0.9259702396392823}
2025-01-17 08:49:36,600 [INFO] Step[2000/2713]: training loss : 0.9256321883201599 TRAIN  loss dict:  {'classification_loss': 0.9256321883201599}
2025-01-17 08:49:52,890 [INFO] Step[2050/2713]: training loss : 0.9259518444538116 TRAIN  loss dict:  {'classification_loss': 0.9259518444538116}
2025-01-17 08:50:09,082 [INFO] Step[2100/2713]: training loss : 0.926712646484375 TRAIN  loss dict:  {'classification_loss': 0.926712646484375}
2025-01-17 08:50:25,404 [INFO] Step[2150/2713]: training loss : 0.9257062542438507 TRAIN  loss dict:  {'classification_loss': 0.9257062542438507}
2025-01-17 08:50:41,779 [INFO] Step[2200/2713]: training loss : 0.9259677267074585 TRAIN  loss dict:  {'classification_loss': 0.9259677267074585}
2025-01-17 08:50:58,013 [INFO] Step[2250/2713]: training loss : 0.9256930780410767 TRAIN  loss dict:  {'classification_loss': 0.9256930780410767}
2025-01-17 08:51:14,198 [INFO] Step[2300/2713]: training loss : 0.9307032704353333 TRAIN  loss dict:  {'classification_loss': 0.9307032704353333}
2025-01-17 08:51:30,514 [INFO] Step[2350/2713]: training loss : 0.9258756077289582 TRAIN  loss dict:  {'classification_loss': 0.9258756077289582}
2025-01-17 08:51:46,743 [INFO] Step[2400/2713]: training loss : 0.9256527519226074 TRAIN  loss dict:  {'classification_loss': 0.9256527519226074}
2025-01-17 08:52:02,955 [INFO] Step[2450/2713]: training loss : 0.9264580297470093 TRAIN  loss dict:  {'classification_loss': 0.9264580297470093}
2025-01-17 08:52:19,228 [INFO] Step[2500/2713]: training loss : 0.9259440636634827 TRAIN  loss dict:  {'classification_loss': 0.9259440636634827}
2025-01-17 08:52:35,388 [INFO] Step[2550/2713]: training loss : 0.92587073802948 TRAIN  loss dict:  {'classification_loss': 0.92587073802948}
2025-01-17 08:52:51,580 [INFO] Step[2600/2713]: training loss : 0.9272716593742371 TRAIN  loss dict:  {'classification_loss': 0.9272716593742371}
2025-01-17 08:53:07,847 [INFO] Step[2650/2713]: training loss : 0.9267579936981201 TRAIN  loss dict:  {'classification_loss': 0.9267579936981201}
2025-01-17 08:53:24,098 [INFO] Step[2700/2713]: training loss : 0.9260108232498169 TRAIN  loss dict:  {'classification_loss': 0.9260108232498169}
2025-01-17 08:54:42,771 [INFO] Label accuracies statistics:
2025-01-17 08:54:42,771 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.25, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.25, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.0, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 1.0, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.0, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 08:54:42,773 [INFO] [85] TRAIN  loss: 0.9270196634379428 acc: 0.9997542695662858
2025-01-17 08:54:42,773 [INFO] [85] TRAIN  loss dict: {'classification_loss': 0.9270196634379428}
2025-01-17 08:54:42,773 [INFO] [85] VALIDATION loss: 1.8933499617908234 VALIDATION acc: 0.8018808777429467
2025-01-17 08:54:42,773 [INFO] [85] VALIDATION loss dict: {'classification_loss': 1.8933499617908234}
2025-01-17 08:54:42,773 [INFO] 
2025-01-17 08:55:04,041 [INFO] Step[50/2713]: training loss : 0.9359590125083923 TRAIN  loss dict:  {'classification_loss': 0.9359590125083923}
2025-01-17 08:55:20,256 [INFO] Step[100/2713]: training loss : 0.9260321068763733 TRAIN  loss dict:  {'classification_loss': 0.9260321068763733}
2025-01-17 08:55:36,515 [INFO] Step[150/2713]: training loss : 0.925794072151184 TRAIN  loss dict:  {'classification_loss': 0.925794072151184}
2025-01-17 08:55:52,779 [INFO] Step[200/2713]: training loss : 0.926689499616623 TRAIN  loss dict:  {'classification_loss': 0.926689499616623}
2025-01-17 08:56:09,107 [INFO] Step[250/2713]: training loss : 0.9256978964805603 TRAIN  loss dict:  {'classification_loss': 0.9256978964805603}
2025-01-17 08:56:25,399 [INFO] Step[300/2713]: training loss : 0.9259830117225647 TRAIN  loss dict:  {'classification_loss': 0.9259830117225647}
2025-01-17 08:56:41,728 [INFO] Step[350/2713]: training loss : 0.9261673414707183 TRAIN  loss dict:  {'classification_loss': 0.9261673414707183}
2025-01-17 08:56:57,979 [INFO] Step[400/2713]: training loss : 0.9255460619926452 TRAIN  loss dict:  {'classification_loss': 0.9255460619926452}
2025-01-17 08:57:14,253 [INFO] Step[450/2713]: training loss : 0.9261085832118988 TRAIN  loss dict:  {'classification_loss': 0.9261085832118988}
2025-01-17 08:57:30,436 [INFO] Step[500/2713]: training loss : 0.9262045848369599 TRAIN  loss dict:  {'classification_loss': 0.9262045848369599}
2025-01-17 08:57:46,714 [INFO] Step[550/2713]: training loss : 0.9257509112358093 TRAIN  loss dict:  {'classification_loss': 0.9257509112358093}
2025-01-17 08:58:02,970 [INFO] Step[600/2713]: training loss : 0.9259336602687835 TRAIN  loss dict:  {'classification_loss': 0.9259336602687835}
2025-01-17 08:58:19,212 [INFO] Step[650/2713]: training loss : 0.9257067453861236 TRAIN  loss dict:  {'classification_loss': 0.9257067453861236}
2025-01-17 08:58:35,475 [INFO] Step[700/2713]: training loss : 0.9268168485164643 TRAIN  loss dict:  {'classification_loss': 0.9268168485164643}
2025-01-17 08:58:51,774 [INFO] Step[750/2713]: training loss : 0.9262079763412475 TRAIN  loss dict:  {'classification_loss': 0.9262079763412475}
2025-01-17 08:59:08,067 [INFO] Step[800/2713]: training loss : 0.9262896263599396 TRAIN  loss dict:  {'classification_loss': 0.9262896263599396}
2025-01-17 08:59:24,442 [INFO] Step[850/2713]: training loss : 0.9253716683387756 TRAIN  loss dict:  {'classification_loss': 0.9253716683387756}
2025-01-17 08:59:40,659 [INFO] Step[900/2713]: training loss : 0.9570934820175171 TRAIN  loss dict:  {'classification_loss': 0.9570934820175171}
2025-01-17 08:59:57,026 [INFO] Step[950/2713]: training loss : 0.9286391627788544 TRAIN  loss dict:  {'classification_loss': 0.9286391627788544}
2025-01-17 09:00:13,290 [INFO] Step[1000/2713]: training loss : 0.9258607244491577 TRAIN  loss dict:  {'classification_loss': 0.9258607244491577}
2025-01-17 09:00:29,618 [INFO] Step[1050/2713]: training loss : 0.9259410750865936 TRAIN  loss dict:  {'classification_loss': 0.9259410750865936}
2025-01-17 09:00:45,854 [INFO] Step[1100/2713]: training loss : 0.9257310628890991 TRAIN  loss dict:  {'classification_loss': 0.9257310628890991}
2025-01-17 09:01:02,150 [INFO] Step[1150/2713]: training loss : 0.9264153778553009 TRAIN  loss dict:  {'classification_loss': 0.9264153778553009}
2025-01-17 09:01:18,479 [INFO] Step[1200/2713]: training loss : 0.9262214291095734 TRAIN  loss dict:  {'classification_loss': 0.9262214291095734}
2025-01-17 09:01:34,791 [INFO] Step[1250/2713]: training loss : 0.9258860802650452 TRAIN  loss dict:  {'classification_loss': 0.9258860802650452}
2025-01-17 09:01:51,128 [INFO] Step[1300/2713]: training loss : 0.9438020479679108 TRAIN  loss dict:  {'classification_loss': 0.9438020479679108}
2025-01-17 09:02:07,492 [INFO] Step[1350/2713]: training loss : 0.9258022475242614 TRAIN  loss dict:  {'classification_loss': 0.9258022475242614}
2025-01-17 09:02:23,724 [INFO] Step[1400/2713]: training loss : 0.9255882203578949 TRAIN  loss dict:  {'classification_loss': 0.9255882203578949}
2025-01-17 09:02:39,981 [INFO] Step[1450/2713]: training loss : 0.9256669330596924 TRAIN  loss dict:  {'classification_loss': 0.9256669330596924}
2025-01-17 09:02:56,227 [INFO] Step[1500/2713]: training loss : 0.9259886455535888 TRAIN  loss dict:  {'classification_loss': 0.9259886455535888}
2025-01-17 09:03:12,504 [INFO] Step[1550/2713]: training loss : 0.9259869349002838 TRAIN  loss dict:  {'classification_loss': 0.9259869349002838}
2025-01-17 09:03:28,752 [INFO] Step[1600/2713]: training loss : 0.9258042299747467 TRAIN  loss dict:  {'classification_loss': 0.9258042299747467}
2025-01-17 09:03:45,053 [INFO] Step[1650/2713]: training loss : 0.9395784449577331 TRAIN  loss dict:  {'classification_loss': 0.9395784449577331}
2025-01-17 09:04:01,385 [INFO] Step[1700/2713]: training loss : 0.9265764904022217 TRAIN  loss dict:  {'classification_loss': 0.9265764904022217}
2025-01-17 09:04:17,717 [INFO] Step[1750/2713]: training loss : 0.9258835899829865 TRAIN  loss dict:  {'classification_loss': 0.9258835899829865}
2025-01-17 09:04:33,911 [INFO] Step[1800/2713]: training loss : 0.925756368637085 TRAIN  loss dict:  {'classification_loss': 0.925756368637085}
2025-01-17 09:04:50,188 [INFO] Step[1850/2713]: training loss : 0.9263634657859803 TRAIN  loss dict:  {'classification_loss': 0.9263634657859803}
2025-01-17 09:05:06,537 [INFO] Step[1900/2713]: training loss : 0.9260393249988556 TRAIN  loss dict:  {'classification_loss': 0.9260393249988556}
2025-01-17 09:05:22,965 [INFO] Step[1950/2713]: training loss : 0.9260502672195434 TRAIN  loss dict:  {'classification_loss': 0.9260502672195434}
2025-01-17 09:05:39,316 [INFO] Step[2000/2713]: training loss : 0.9257210659980774 TRAIN  loss dict:  {'classification_loss': 0.9257210659980774}
2025-01-17 09:05:55,977 [INFO] Step[2050/2713]: training loss : 0.9256804025173188 TRAIN  loss dict:  {'classification_loss': 0.9256804025173188}
2025-01-17 09:06:12,439 [INFO] Step[2100/2713]: training loss : 0.9259005737304687 TRAIN  loss dict:  {'classification_loss': 0.9259005737304687}
2025-01-17 09:06:28,871 [INFO] Step[2150/2713]: training loss : 0.9322722232341767 TRAIN  loss dict:  {'classification_loss': 0.9322722232341767}
2025-01-17 09:06:45,432 [INFO] Step[2200/2713]: training loss : 0.9258680963516235 TRAIN  loss dict:  {'classification_loss': 0.9258680963516235}
2025-01-17 09:07:01,876 [INFO] Step[2250/2713]: training loss : 0.9257090592384338 TRAIN  loss dict:  {'classification_loss': 0.9257090592384338}
2025-01-17 09:07:18,286 [INFO] Step[2300/2713]: training loss : 0.9259881258010865 TRAIN  loss dict:  {'classification_loss': 0.9259881258010865}
2025-01-17 09:07:34,775 [INFO] Step[2350/2713]: training loss : 0.9258550834655762 TRAIN  loss dict:  {'classification_loss': 0.9258550834655762}
2025-01-17 09:07:51,099 [INFO] Step[2400/2713]: training loss : 0.9255790770053863 TRAIN  loss dict:  {'classification_loss': 0.9255790770053863}
2025-01-17 09:08:07,561 [INFO] Step[2450/2713]: training loss : 0.9261679470539093 TRAIN  loss dict:  {'classification_loss': 0.9261679470539093}
2025-01-17 09:08:23,990 [INFO] Step[2500/2713]: training loss : 0.926072449684143 TRAIN  loss dict:  {'classification_loss': 0.926072449684143}
2025-01-17 09:08:40,381 [INFO] Step[2550/2713]: training loss : 0.9258421277999878 TRAIN  loss dict:  {'classification_loss': 0.9258421277999878}
2025-01-17 09:08:56,840 [INFO] Step[2600/2713]: training loss : 0.9398036062717438 TRAIN  loss dict:  {'classification_loss': 0.9398036062717438}
2025-01-17 09:09:13,172 [INFO] Step[2650/2713]: training loss : 0.926205108165741 TRAIN  loss dict:  {'classification_loss': 0.926205108165741}
2025-01-17 09:09:29,581 [INFO] Step[2700/2713]: training loss : 0.925576149225235 TRAIN  loss dict:  {'classification_loss': 0.925576149225235}
2025-01-17 09:10:48,915 [INFO] Label accuracies statistics:
2025-01-17 09:10:48,915 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.25, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 0.75, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.25, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 09:10:48,916 [INFO] [86] TRAIN  loss: 0.9277246137452749 acc: 0.9995085391325715
2025-01-17 09:10:48,917 [INFO] [86] TRAIN  loss dict: {'classification_loss': 0.9277246137452749}
2025-01-17 09:10:48,917 [INFO] [86] VALIDATION loss: 1.959773608728459 VALIDATION acc: 0.7887147335423198
2025-01-17 09:10:48,917 [INFO] [86] VALIDATION loss dict: {'classification_loss': 1.959773608728459}
2025-01-17 09:10:48,917 [INFO] 
2025-01-17 09:11:10,263 [INFO] Step[50/2713]: training loss : 0.9268707716464997 TRAIN  loss dict:  {'classification_loss': 0.9268707716464997}
2025-01-17 09:11:26,621 [INFO] Step[100/2713]: training loss : 0.9264816272258759 TRAIN  loss dict:  {'classification_loss': 0.9264816272258759}
2025-01-17 09:11:43,016 [INFO] Step[150/2713]: training loss : 0.9259288036823272 TRAIN  loss dict:  {'classification_loss': 0.9259288036823272}
2025-01-17 09:11:59,344 [INFO] Step[200/2713]: training loss : 0.9313299036026002 TRAIN  loss dict:  {'classification_loss': 0.9313299036026002}
2025-01-17 09:12:15,788 [INFO] Step[250/2713]: training loss : 0.9263161146640777 TRAIN  loss dict:  {'classification_loss': 0.9263161146640777}
2025-01-17 09:12:32,175 [INFO] Step[300/2713]: training loss : 0.9265575456619263 TRAIN  loss dict:  {'classification_loss': 0.9265575456619263}
2025-01-17 09:12:48,673 [INFO] Step[350/2713]: training loss : 0.9260838317871094 TRAIN  loss dict:  {'classification_loss': 0.9260838317871094}
2025-01-17 09:13:05,093 [INFO] Step[400/2713]: training loss : 0.9262064468860626 TRAIN  loss dict:  {'classification_loss': 0.9262064468860626}
2025-01-17 09:13:21,539 [INFO] Step[450/2713]: training loss : 0.9262930619716644 TRAIN  loss dict:  {'classification_loss': 0.9262930619716644}
2025-01-17 09:13:37,955 [INFO] Step[500/2713]: training loss : 0.9259618067741394 TRAIN  loss dict:  {'classification_loss': 0.9259618067741394}
2025-01-17 09:13:54,407 [INFO] Step[550/2713]: training loss : 0.9261388576030731 TRAIN  loss dict:  {'classification_loss': 0.9261388576030731}
2025-01-17 09:14:10,760 [INFO] Step[600/2713]: training loss : 0.9263025081157684 TRAIN  loss dict:  {'classification_loss': 0.9263025081157684}
2025-01-17 09:14:27,186 [INFO] Step[650/2713]: training loss : 0.9269133794307709 TRAIN  loss dict:  {'classification_loss': 0.9269133794307709}
2025-01-17 09:14:43,621 [INFO] Step[700/2713]: training loss : 0.9256213343143463 TRAIN  loss dict:  {'classification_loss': 0.9256213343143463}
2025-01-17 09:15:00,051 [INFO] Step[750/2713]: training loss : 0.9259453594684601 TRAIN  loss dict:  {'classification_loss': 0.9259453594684601}
2025-01-17 09:15:16,450 [INFO] Step[800/2713]: training loss : 0.9262809252738953 TRAIN  loss dict:  {'classification_loss': 0.9262809252738953}
2025-01-17 09:15:32,864 [INFO] Step[850/2713]: training loss : 0.9256849265098572 TRAIN  loss dict:  {'classification_loss': 0.9256849265098572}
2025-01-17 09:15:49,334 [INFO] Step[900/2713]: training loss : 1.0017050099372864 TRAIN  loss dict:  {'classification_loss': 1.0017050099372864}
2025-01-17 09:16:05,792 [INFO] Step[950/2713]: training loss : 0.9263125216960907 TRAIN  loss dict:  {'classification_loss': 0.9263125216960907}
2025-01-17 09:16:22,234 [INFO] Step[1000/2713]: training loss : 0.926166843175888 TRAIN  loss dict:  {'classification_loss': 0.926166843175888}
2025-01-17 09:16:38,682 [INFO] Step[1050/2713]: training loss : 0.9262371385097503 TRAIN  loss dict:  {'classification_loss': 0.9262371385097503}
2025-01-17 09:16:55,176 [INFO] Step[1100/2713]: training loss : 0.9259631252288818 TRAIN  loss dict:  {'classification_loss': 0.9259631252288818}
2025-01-17 09:17:11,515 [INFO] Step[1150/2713]: training loss : 0.9257644236087799 TRAIN  loss dict:  {'classification_loss': 0.9257644236087799}
2025-01-17 09:17:27,890 [INFO] Step[1200/2713]: training loss : 0.954457916021347 TRAIN  loss dict:  {'classification_loss': 0.954457916021347}
2025-01-17 09:17:44,302 [INFO] Step[1250/2713]: training loss : 0.926711984872818 TRAIN  loss dict:  {'classification_loss': 0.926711984872818}
2025-01-17 09:18:00,552 [INFO] Step[1300/2713]: training loss : 0.925689994096756 TRAIN  loss dict:  {'classification_loss': 0.925689994096756}
2025-01-17 09:18:16,911 [INFO] Step[1350/2713]: training loss : 0.9259452164173126 TRAIN  loss dict:  {'classification_loss': 0.9259452164173126}
2025-01-17 09:18:33,224 [INFO] Step[1400/2713]: training loss : 0.9262591516971588 TRAIN  loss dict:  {'classification_loss': 0.9262591516971588}
2025-01-17 09:18:49,540 [INFO] Step[1450/2713]: training loss : 0.9259191286563874 TRAIN  loss dict:  {'classification_loss': 0.9259191286563874}
2025-01-17 09:19:05,930 [INFO] Step[1500/2713]: training loss : 0.9260454905033112 TRAIN  loss dict:  {'classification_loss': 0.9260454905033112}
2025-01-17 09:19:22,456 [INFO] Step[1550/2713]: training loss : 0.9259601318836213 TRAIN  loss dict:  {'classification_loss': 0.9259601318836213}
2025-01-17 09:19:38,727 [INFO] Step[1600/2713]: training loss : 0.9262812602519989 TRAIN  loss dict:  {'classification_loss': 0.9262812602519989}
2025-01-17 09:19:55,081 [INFO] Step[1650/2713]: training loss : 0.9260652947425843 TRAIN  loss dict:  {'classification_loss': 0.9260652947425843}
2025-01-17 09:20:11,535 [INFO] Step[1700/2713]: training loss : 0.9259288334846496 TRAIN  loss dict:  {'classification_loss': 0.9259288334846496}
2025-01-17 09:20:27,888 [INFO] Step[1750/2713]: training loss : 0.9274575102329254 TRAIN  loss dict:  {'classification_loss': 0.9274575102329254}
2025-01-17 09:20:44,230 [INFO] Step[1800/2713]: training loss : 0.9260678839683533 TRAIN  loss dict:  {'classification_loss': 0.9260678839683533}
2025-01-17 09:21:00,660 [INFO] Step[1850/2713]: training loss : 0.9254211127758026 TRAIN  loss dict:  {'classification_loss': 0.9254211127758026}
2025-01-17 09:21:17,057 [INFO] Step[1900/2713]: training loss : 0.9261541175842285 TRAIN  loss dict:  {'classification_loss': 0.9261541175842285}
2025-01-17 09:21:33,431 [INFO] Step[1950/2713]: training loss : 0.9261480057239533 TRAIN  loss dict:  {'classification_loss': 0.9261480057239533}
2025-01-17 09:21:49,772 [INFO] Step[2000/2713]: training loss : 0.9257039952278138 TRAIN  loss dict:  {'classification_loss': 0.9257039952278138}
2025-01-17 09:22:06,120 [INFO] Step[2050/2713]: training loss : 0.9258686149120331 TRAIN  loss dict:  {'classification_loss': 0.9258686149120331}
2025-01-17 09:22:22,585 [INFO] Step[2100/2713]: training loss : 0.9259867107868195 TRAIN  loss dict:  {'classification_loss': 0.9259867107868195}
2025-01-17 09:22:38,971 [INFO] Step[2150/2713]: training loss : 0.925999870300293 TRAIN  loss dict:  {'classification_loss': 0.925999870300293}
2025-01-17 09:22:55,377 [INFO] Step[2200/2713]: training loss : 0.9263569271564484 TRAIN  loss dict:  {'classification_loss': 0.9263569271564484}
2025-01-17 09:23:11,798 [INFO] Step[2250/2713]: training loss : 0.9265350782871247 TRAIN  loss dict:  {'classification_loss': 0.9265350782871247}
2025-01-17 09:23:28,090 [INFO] Step[2300/2713]: training loss : 0.9258012366294861 TRAIN  loss dict:  {'classification_loss': 0.9258012366294861}
2025-01-17 09:23:44,328 [INFO] Step[2350/2713]: training loss : 0.9259036266803742 TRAIN  loss dict:  {'classification_loss': 0.9259036266803742}
2025-01-17 09:24:00,626 [INFO] Step[2400/2713]: training loss : 0.9259286844730377 TRAIN  loss dict:  {'classification_loss': 0.9259286844730377}
2025-01-17 09:24:16,972 [INFO] Step[2450/2713]: training loss : 0.9259513366222382 TRAIN  loss dict:  {'classification_loss': 0.9259513366222382}
2025-01-17 09:24:33,344 [INFO] Step[2500/2713]: training loss : 0.9258886837959289 TRAIN  loss dict:  {'classification_loss': 0.9258886837959289}
2025-01-17 09:24:49,733 [INFO] Step[2550/2713]: training loss : 0.9325427031517028 TRAIN  loss dict:  {'classification_loss': 0.9325427031517028}
2025-01-17 09:25:06,049 [INFO] Step[2600/2713]: training loss : 0.9263025772571564 TRAIN  loss dict:  {'classification_loss': 0.9263025772571564}
2025-01-17 09:25:22,392 [INFO] Step[2650/2713]: training loss : 0.9261867415904999 TRAIN  loss dict:  {'classification_loss': 0.9261867415904999}
2025-01-17 09:25:38,791 [INFO] Step[2700/2713]: training loss : 0.9258037257194519 TRAIN  loss dict:  {'classification_loss': 0.9258037257194519}
2025-01-17 09:26:57,805 [INFO] Label accuracies statistics:
2025-01-17 09:26:57,805 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.5, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.5, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 09:26:57,807 [INFO] [87] TRAIN  loss: 0.928257194867751 acc: 0.9996314043494287
2025-01-17 09:26:57,807 [INFO] [87] TRAIN  loss dict: {'classification_loss': 0.928257194867751}
2025-01-17 09:26:57,807 [INFO] [87] VALIDATION loss: 1.9324638245249153 VALIDATION acc: 0.7968652037617555
2025-01-17 09:26:57,807 [INFO] [87] VALIDATION loss dict: {'classification_loss': 1.9324638245249153}
2025-01-17 09:26:57,807 [INFO] 
2025-01-17 09:27:19,553 [INFO] Step[50/2713]: training loss : 0.9261963033676147 TRAIN  loss dict:  {'classification_loss': 0.9261963033676147}
2025-01-17 09:27:35,739 [INFO] Step[100/2713]: training loss : 0.9260979819297791 TRAIN  loss dict:  {'classification_loss': 0.9260979819297791}
2025-01-17 09:27:52,016 [INFO] Step[150/2713]: training loss : 0.9260185384750366 TRAIN  loss dict:  {'classification_loss': 0.9260185384750366}
2025-01-17 09:28:08,444 [INFO] Step[200/2713]: training loss : 0.9563566517829895 TRAIN  loss dict:  {'classification_loss': 0.9563566517829895}
2025-01-17 09:28:24,755 [INFO] Step[250/2713]: training loss : 0.9262579751014709 TRAIN  loss dict:  {'classification_loss': 0.9262579751014709}
2025-01-17 09:28:41,109 [INFO] Step[300/2713]: training loss : 0.9586634337902069 TRAIN  loss dict:  {'classification_loss': 0.9586634337902069}
2025-01-17 09:28:57,546 [INFO] Step[350/2713]: training loss : 0.9259417963027954 TRAIN  loss dict:  {'classification_loss': 0.9259417963027954}
2025-01-17 09:29:13,945 [INFO] Step[400/2713]: training loss : 0.9259590554237366 TRAIN  loss dict:  {'classification_loss': 0.9259590554237366}
2025-01-17 09:29:30,297 [INFO] Step[450/2713]: training loss : 0.9259834706783294 TRAIN  loss dict:  {'classification_loss': 0.9259834706783294}
2025-01-17 09:29:46,790 [INFO] Step[500/2713]: training loss : 0.9260709607601165 TRAIN  loss dict:  {'classification_loss': 0.9260709607601165}
2025-01-17 09:30:03,248 [INFO] Step[550/2713]: training loss : 0.9256923186779022 TRAIN  loss dict:  {'classification_loss': 0.9256923186779022}
2025-01-17 09:30:19,620 [INFO] Step[600/2713]: training loss : 0.9258711457252502 TRAIN  loss dict:  {'classification_loss': 0.9258711457252502}
2025-01-17 09:30:36,072 [INFO] Step[650/2713]: training loss : 0.9258583915233612 TRAIN  loss dict:  {'classification_loss': 0.9258583915233612}
2025-01-17 09:30:52,429 [INFO] Step[700/2713]: training loss : 0.9266429829597473 TRAIN  loss dict:  {'classification_loss': 0.9266429829597473}
2025-01-17 09:31:08,824 [INFO] Step[750/2713]: training loss : 0.9260735833644866 TRAIN  loss dict:  {'classification_loss': 0.9260735833644866}
2025-01-17 09:31:25,159 [INFO] Step[800/2713]: training loss : 0.9265783095359802 TRAIN  loss dict:  {'classification_loss': 0.9265783095359802}
2025-01-17 09:31:41,651 [INFO] Step[850/2713]: training loss : 0.9258219397068024 TRAIN  loss dict:  {'classification_loss': 0.9258219397068024}
2025-01-17 09:31:58,076 [INFO] Step[900/2713]: training loss : 0.9268055427074432 TRAIN  loss dict:  {'classification_loss': 0.9268055427074432}
2025-01-17 09:32:14,470 [INFO] Step[950/2713]: training loss : 0.9265032744407654 TRAIN  loss dict:  {'classification_loss': 0.9265032744407654}
2025-01-17 09:32:30,865 [INFO] Step[1000/2713]: training loss : 0.9258780813217163 TRAIN  loss dict:  {'classification_loss': 0.9258780813217163}
2025-01-17 09:32:47,273 [INFO] Step[1050/2713]: training loss : 0.9258853840827942 TRAIN  loss dict:  {'classification_loss': 0.9258853840827942}
2025-01-17 09:33:03,607 [INFO] Step[1100/2713]: training loss : 0.9270864498615264 TRAIN  loss dict:  {'classification_loss': 0.9270864498615264}
2025-01-17 09:33:20,053 [INFO] Step[1150/2713]: training loss : 0.9262211072444916 TRAIN  loss dict:  {'classification_loss': 0.9262211072444916}
2025-01-17 09:33:36,427 [INFO] Step[1200/2713]: training loss : 0.9255566573143006 TRAIN  loss dict:  {'classification_loss': 0.9255566573143006}
2025-01-17 09:33:52,903 [INFO] Step[1250/2713]: training loss : 0.9262703454494476 TRAIN  loss dict:  {'classification_loss': 0.9262703454494476}
2025-01-17 09:34:09,348 [INFO] Step[1300/2713]: training loss : 0.9267271304130554 TRAIN  loss dict:  {'classification_loss': 0.9267271304130554}
2025-01-17 09:34:25,786 [INFO] Step[1350/2713]: training loss : 0.9259269320964814 TRAIN  loss dict:  {'classification_loss': 0.9259269320964814}
2025-01-17 09:34:42,223 [INFO] Step[1400/2713]: training loss : 0.9260202991962433 TRAIN  loss dict:  {'classification_loss': 0.9260202991962433}
2025-01-17 09:34:58,657 [INFO] Step[1450/2713]: training loss : 0.9254106271266938 TRAIN  loss dict:  {'classification_loss': 0.9254106271266938}
2025-01-17 09:35:15,220 [INFO] Step[1500/2713]: training loss : 0.9254848349094391 TRAIN  loss dict:  {'classification_loss': 0.9254848349094391}
2025-01-17 09:35:31,683 [INFO] Step[1550/2713]: training loss : 0.9259284520149231 TRAIN  loss dict:  {'classification_loss': 0.9259284520149231}
2025-01-17 09:35:48,109 [INFO] Step[1600/2713]: training loss : 0.930307000875473 TRAIN  loss dict:  {'classification_loss': 0.930307000875473}
2025-01-17 09:36:04,481 [INFO] Step[1650/2713]: training loss : 0.9264083886146546 TRAIN  loss dict:  {'classification_loss': 0.9264083886146546}
2025-01-17 09:36:20,881 [INFO] Step[1700/2713]: training loss : 0.9261925554275513 TRAIN  loss dict:  {'classification_loss': 0.9261925554275513}
2025-01-17 09:36:37,301 [INFO] Step[1750/2713]: training loss : 0.9262662756443024 TRAIN  loss dict:  {'classification_loss': 0.9262662756443024}
2025-01-17 09:36:53,618 [INFO] Step[1800/2713]: training loss : 0.9324342942237854 TRAIN  loss dict:  {'classification_loss': 0.9324342942237854}
2025-01-17 09:37:10,037 [INFO] Step[1850/2713]: training loss : 0.9257512772083283 TRAIN  loss dict:  {'classification_loss': 0.9257512772083283}
2025-01-17 09:37:26,328 [INFO] Step[1900/2713]: training loss : 0.9262233173847199 TRAIN  loss dict:  {'classification_loss': 0.9262233173847199}
2025-01-17 09:37:42,612 [INFO] Step[1950/2713]: training loss : 0.9263784992694855 TRAIN  loss dict:  {'classification_loss': 0.9263784992694855}
2025-01-17 09:37:58,881 [INFO] Step[2000/2713]: training loss : 0.9259480500221252 TRAIN  loss dict:  {'classification_loss': 0.9259480500221252}
2025-01-17 09:38:15,214 [INFO] Step[2050/2713]: training loss : 0.9260509598255158 TRAIN  loss dict:  {'classification_loss': 0.9260509598255158}
2025-01-17 09:38:31,524 [INFO] Step[2100/2713]: training loss : 0.9262457871437073 TRAIN  loss dict:  {'classification_loss': 0.9262457871437073}
2025-01-17 09:38:47,869 [INFO] Step[2150/2713]: training loss : 0.925637503862381 TRAIN  loss dict:  {'classification_loss': 0.925637503862381}
2025-01-17 09:39:04,188 [INFO] Step[2200/2713]: training loss : 0.9260078835487365 TRAIN  loss dict:  {'classification_loss': 0.9260078835487365}
2025-01-17 09:39:20,548 [INFO] Step[2250/2713]: training loss : 0.9263750588893891 TRAIN  loss dict:  {'classification_loss': 0.9263750588893891}
2025-01-17 09:39:36,873 [INFO] Step[2300/2713]: training loss : 0.9259954702854156 TRAIN  loss dict:  {'classification_loss': 0.9259954702854156}
2025-01-17 09:39:53,195 [INFO] Step[2350/2713]: training loss : 0.9258232259750366 TRAIN  loss dict:  {'classification_loss': 0.9258232259750366}
2025-01-17 09:40:09,512 [INFO] Step[2400/2713]: training loss : 0.9297420656681061 TRAIN  loss dict:  {'classification_loss': 0.9297420656681061}
2025-01-17 09:40:25,795 [INFO] Step[2450/2713]: training loss : 0.9293846702575683 TRAIN  loss dict:  {'classification_loss': 0.9293846702575683}
2025-01-17 09:40:42,167 [INFO] Step[2500/2713]: training loss : 0.925807877779007 TRAIN  loss dict:  {'classification_loss': 0.925807877779007}
2025-01-17 09:40:58,528 [INFO] Step[2550/2713]: training loss : 0.9260344660282135 TRAIN  loss dict:  {'classification_loss': 0.9260344660282135}
2025-01-17 09:41:14,793 [INFO] Step[2600/2713]: training loss : 0.9263595235347748 TRAIN  loss dict:  {'classification_loss': 0.9263595235347748}
2025-01-17 09:41:31,015 [INFO] Step[2650/2713]: training loss : 0.9359397375583649 TRAIN  loss dict:  {'classification_loss': 0.9359397375583649}
2025-01-17 09:41:47,314 [INFO] Step[2700/2713]: training loss : 0.9254659378528595 TRAIN  loss dict:  {'classification_loss': 0.9254659378528595}
2025-01-17 09:43:05,863 [INFO] Label accuracies statistics:
2025-01-17 09:43:05,863 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.5, 224: 0.75, 225: 1.0, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.5, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 09:43:05,865 [INFO] [88] TRAIN  loss: 0.927769328411593 acc: 0.9996314043494287
2025-01-17 09:43:05,865 [INFO] [88] TRAIN  loss dict: {'classification_loss': 0.927769328411593}
2025-01-17 09:43:05,865 [INFO] [88] VALIDATION loss: 1.9074467938197286 VALIDATION acc: 0.7981191222570533
2025-01-17 09:43:05,865 [INFO] [88] VALIDATION loss dict: {'classification_loss': 1.9074467938197286}
2025-01-17 09:43:05,865 [INFO] 
2025-01-17 09:43:27,285 [INFO] Step[50/2713]: training loss : 0.9259222102165222 TRAIN  loss dict:  {'classification_loss': 0.9259222102165222}
2025-01-17 09:43:43,506 [INFO] Step[100/2713]: training loss : 0.926087634563446 TRAIN  loss dict:  {'classification_loss': 0.926087634563446}
2025-01-17 09:43:59,666 [INFO] Step[150/2713]: training loss : 0.9261993980407714 TRAIN  loss dict:  {'classification_loss': 0.9261993980407714}
2025-01-17 09:44:15,914 [INFO] Step[200/2713]: training loss : 0.9256732821464538 TRAIN  loss dict:  {'classification_loss': 0.9256732821464538}
2025-01-17 09:44:32,253 [INFO] Step[250/2713]: training loss : 0.9259070408344269 TRAIN  loss dict:  {'classification_loss': 0.9259070408344269}
2025-01-17 09:44:48,569 [INFO] Step[300/2713]: training loss : 0.9256826734542847 TRAIN  loss dict:  {'classification_loss': 0.9256826734542847}
2025-01-17 09:45:04,864 [INFO] Step[350/2713]: training loss : 0.9279343366622925 TRAIN  loss dict:  {'classification_loss': 0.9279343366622925}
2025-01-17 09:45:21,153 [INFO] Step[400/2713]: training loss : 0.9264304721355439 TRAIN  loss dict:  {'classification_loss': 0.9264304721355439}
2025-01-17 09:45:37,470 [INFO] Step[450/2713]: training loss : 0.9260679686069488 TRAIN  loss dict:  {'classification_loss': 0.9260679686069488}
2025-01-17 09:45:53,720 [INFO] Step[500/2713]: training loss : 0.9259101116657257 TRAIN  loss dict:  {'classification_loss': 0.9259101116657257}
2025-01-17 09:46:09,962 [INFO] Step[550/2713]: training loss : 0.9260680878162384 TRAIN  loss dict:  {'classification_loss': 0.9260680878162384}
2025-01-17 09:46:26,278 [INFO] Step[600/2713]: training loss : 0.9261240041255951 TRAIN  loss dict:  {'classification_loss': 0.9261240041255951}
2025-01-17 09:46:42,485 [INFO] Step[650/2713]: training loss : 0.9257157099246979 TRAIN  loss dict:  {'classification_loss': 0.9257157099246979}
2025-01-17 09:46:58,740 [INFO] Step[700/2713]: training loss : 0.926683874130249 TRAIN  loss dict:  {'classification_loss': 0.926683874130249}
2025-01-17 09:47:15,039 [INFO] Step[750/2713]: training loss : 0.9256967294216156 TRAIN  loss dict:  {'classification_loss': 0.9256967294216156}
2025-01-17 09:47:31,304 [INFO] Step[800/2713]: training loss : 0.9258988428115845 TRAIN  loss dict:  {'classification_loss': 0.9258988428115845}
2025-01-17 09:47:47,595 [INFO] Step[850/2713]: training loss : 0.926039743423462 TRAIN  loss dict:  {'classification_loss': 0.926039743423462}
2025-01-17 09:48:03,869 [INFO] Step[900/2713]: training loss : 0.9259305727481842 TRAIN  loss dict:  {'classification_loss': 0.9259305727481842}
2025-01-17 09:48:20,211 [INFO] Step[950/2713]: training loss : 0.9261081182956695 TRAIN  loss dict:  {'classification_loss': 0.9261081182956695}
2025-01-17 09:48:36,596 [INFO] Step[1000/2713]: training loss : 0.925618007183075 TRAIN  loss dict:  {'classification_loss': 0.925618007183075}
2025-01-17 09:48:52,925 [INFO] Step[1050/2713]: training loss : 0.927434196472168 TRAIN  loss dict:  {'classification_loss': 0.927434196472168}
2025-01-17 09:49:09,230 [INFO] Step[1100/2713]: training loss : 0.925859524011612 TRAIN  loss dict:  {'classification_loss': 0.925859524011612}
2025-01-17 09:49:25,589 [INFO] Step[1150/2713]: training loss : 0.9282536661624908 TRAIN  loss dict:  {'classification_loss': 0.9282536661624908}
2025-01-17 09:49:41,883 [INFO] Step[1200/2713]: training loss : 0.9256560838222504 TRAIN  loss dict:  {'classification_loss': 0.9256560838222504}
2025-01-17 09:49:58,227 [INFO] Step[1250/2713]: training loss : 0.9262261378765106 TRAIN  loss dict:  {'classification_loss': 0.9262261378765106}
2025-01-17 09:50:14,516 [INFO] Step[1300/2713]: training loss : 0.926500313282013 TRAIN  loss dict:  {'classification_loss': 0.926500313282013}
2025-01-17 09:50:30,791 [INFO] Step[1350/2713]: training loss : 0.9254629266262054 TRAIN  loss dict:  {'classification_loss': 0.9254629266262054}
2025-01-17 09:50:47,117 [INFO] Step[1400/2713]: training loss : 0.9263563489913941 TRAIN  loss dict:  {'classification_loss': 0.9263563489913941}
2025-01-17 09:51:03,433 [INFO] Step[1450/2713]: training loss : 0.92578253865242 TRAIN  loss dict:  {'classification_loss': 0.92578253865242}
2025-01-17 09:51:19,671 [INFO] Step[1500/2713]: training loss : 0.925934499502182 TRAIN  loss dict:  {'classification_loss': 0.925934499502182}
2025-01-17 09:51:35,970 [INFO] Step[1550/2713]: training loss : 0.9267182922363282 TRAIN  loss dict:  {'classification_loss': 0.9267182922363282}
2025-01-17 09:51:52,224 [INFO] Step[1600/2713]: training loss : 0.9256021344661712 TRAIN  loss dict:  {'classification_loss': 0.9256021344661712}
2025-01-17 09:52:08,538 [INFO] Step[1650/2713]: training loss : 0.9257345712184906 TRAIN  loss dict:  {'classification_loss': 0.9257345712184906}
2025-01-17 09:52:24,866 [INFO] Step[1700/2713]: training loss : 0.9272553789615631 TRAIN  loss dict:  {'classification_loss': 0.9272553789615631}
2025-01-17 09:52:41,196 [INFO] Step[1750/2713]: training loss : 0.9257798337936402 TRAIN  loss dict:  {'classification_loss': 0.9257798337936402}
2025-01-17 09:52:57,506 [INFO] Step[1800/2713]: training loss : 0.9264460861682892 TRAIN  loss dict:  {'classification_loss': 0.9264460861682892}
2025-01-17 09:53:13,831 [INFO] Step[1850/2713]: training loss : 0.959392158985138 TRAIN  loss dict:  {'classification_loss': 0.959392158985138}
2025-01-17 09:53:30,086 [INFO] Step[1900/2713]: training loss : 0.9257732725143433 TRAIN  loss dict:  {'classification_loss': 0.9257732725143433}
2025-01-17 09:53:46,356 [INFO] Step[1950/2713]: training loss : 0.9258794140815735 TRAIN  loss dict:  {'classification_loss': 0.9258794140815735}
2025-01-17 09:54:02,684 [INFO] Step[2000/2713]: training loss : 0.9258523869514466 TRAIN  loss dict:  {'classification_loss': 0.9258523869514466}
2025-01-17 09:54:18,934 [INFO] Step[2050/2713]: training loss : 0.9257153248786927 TRAIN  loss dict:  {'classification_loss': 0.9257153248786927}
2025-01-17 09:54:35,198 [INFO] Step[2100/2713]: training loss : 0.9257820212841034 TRAIN  loss dict:  {'classification_loss': 0.9257820212841034}
2025-01-17 09:54:51,503 [INFO] Step[2150/2713]: training loss : 0.9264273297786713 TRAIN  loss dict:  {'classification_loss': 0.9264273297786713}
2025-01-17 09:55:07,798 [INFO] Step[2200/2713]: training loss : 0.9265715086460113 TRAIN  loss dict:  {'classification_loss': 0.9265715086460113}
2025-01-17 09:55:24,064 [INFO] Step[2250/2713]: training loss : 0.9256887698173523 TRAIN  loss dict:  {'classification_loss': 0.9256887698173523}
2025-01-17 09:55:40,312 [INFO] Step[2300/2713]: training loss : 0.9261112403869629 TRAIN  loss dict:  {'classification_loss': 0.9261112403869629}
2025-01-17 09:55:56,574 [INFO] Step[2350/2713]: training loss : 0.925479747056961 TRAIN  loss dict:  {'classification_loss': 0.925479747056961}
2025-01-17 09:56:12,881 [INFO] Step[2400/2713]: training loss : 0.9260886144638062 TRAIN  loss dict:  {'classification_loss': 0.9260886144638062}
2025-01-17 09:56:29,137 [INFO] Step[2450/2713]: training loss : 0.9579849815368653 TRAIN  loss dict:  {'classification_loss': 0.9579849815368653}
2025-01-17 09:56:45,411 [INFO] Step[2500/2713]: training loss : 0.9385803997516632 TRAIN  loss dict:  {'classification_loss': 0.9385803997516632}
2025-01-17 09:57:01,728 [INFO] Step[2550/2713]: training loss : 0.9255841588973999 TRAIN  loss dict:  {'classification_loss': 0.9255841588973999}
2025-01-17 09:57:17,991 [INFO] Step[2600/2713]: training loss : 0.926145828962326 TRAIN  loss dict:  {'classification_loss': 0.926145828962326}
2025-01-17 09:57:34,268 [INFO] Step[2650/2713]: training loss : 0.9261375653743744 TRAIN  loss dict:  {'classification_loss': 0.9261375653743744}
2025-01-17 09:57:50,513 [INFO] Step[2700/2713]: training loss : 0.9262819516658783 TRAIN  loss dict:  {'classification_loss': 0.9262819516658783}
2025-01-17 09:59:09,697 [INFO] Label accuracies statistics:
2025-01-17 09:59:09,697 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.5, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.25, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 0.75, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-17 09:59:09,698 [INFO] [89] TRAIN  loss: 0.9275503255676792 acc: 0.9996314043494287
2025-01-17 09:59:09,698 [INFO] [89] TRAIN  loss dict: {'classification_loss': 0.9275503255676792}
2025-01-17 09:59:09,698 [INFO] [89] VALIDATION loss: 1.9096846600672357 VALIDATION acc: 0.8006269592476489
2025-01-17 09:59:09,698 [INFO] [89] VALIDATION loss dict: {'classification_loss': 1.9096846600672357}
2025-01-17 09:59:09,699 [INFO] 
2025-01-17 09:59:30,216 [INFO] Step[50/2713]: training loss : 0.9266434776782989 TRAIN  loss dict:  {'classification_loss': 0.9266434776782989}
2025-01-17 09:59:46,424 [INFO] Step[100/2713]: training loss : 0.9262524032592774 TRAIN  loss dict:  {'classification_loss': 0.9262524032592774}
2025-01-17 10:00:02,635 [INFO] Step[150/2713]: training loss : 0.9265616250038147 TRAIN  loss dict:  {'classification_loss': 0.9265616250038147}
2025-01-17 10:00:18,905 [INFO] Step[200/2713]: training loss : 0.9257593083381653 TRAIN  loss dict:  {'classification_loss': 0.9257593083381653}
2025-01-17 10:00:35,121 [INFO] Step[250/2713]: training loss : 0.9259083914756775 TRAIN  loss dict:  {'classification_loss': 0.9259083914756775}
2025-01-17 10:00:51,323 [INFO] Step[300/2713]: training loss : 0.9277861821651459 TRAIN  loss dict:  {'classification_loss': 0.9277861821651459}
2025-01-17 10:01:07,559 [INFO] Step[350/2713]: training loss : 0.9262066543102264 TRAIN  loss dict:  {'classification_loss': 0.9262066543102264}
2025-01-17 10:01:23,783 [INFO] Step[400/2713]: training loss : 0.9252428221702576 TRAIN  loss dict:  {'classification_loss': 0.9252428221702576}
2025-01-17 10:01:40,006 [INFO] Step[450/2713]: training loss : 0.9518008744716644 TRAIN  loss dict:  {'classification_loss': 0.9518008744716644}
2025-01-17 10:01:56,197 [INFO] Step[500/2713]: training loss : 0.925859785079956 TRAIN  loss dict:  {'classification_loss': 0.925859785079956}
2025-01-17 10:02:12,488 [INFO] Step[550/2713]: training loss : 0.9264674091339111 TRAIN  loss dict:  {'classification_loss': 0.9264674091339111}
2025-01-17 10:02:28,724 [INFO] Step[600/2713]: training loss : 0.9287917387485504 TRAIN  loss dict:  {'classification_loss': 0.9287917387485504}
2025-01-17 10:02:44,969 [INFO] Step[650/2713]: training loss : 0.9265972328186035 TRAIN  loss dict:  {'classification_loss': 0.9265972328186035}
2025-01-17 10:03:01,176 [INFO] Step[700/2713]: training loss : 0.9257618308067321 TRAIN  loss dict:  {'classification_loss': 0.9257618308067321}
2025-01-17 10:03:17,397 [INFO] Step[750/2713]: training loss : 0.9254966151714324 TRAIN  loss dict:  {'classification_loss': 0.9254966151714324}
2025-01-17 10:03:33,597 [INFO] Step[800/2713]: training loss : 0.9260099840164184 TRAIN  loss dict:  {'classification_loss': 0.9260099840164184}
2025-01-17 10:03:49,830 [INFO] Step[850/2713]: training loss : 0.9262108743190766 TRAIN  loss dict:  {'classification_loss': 0.9262108743190766}
2025-01-17 10:04:05,994 [INFO] Step[900/2713]: training loss : 0.9262934136390686 TRAIN  loss dict:  {'classification_loss': 0.9262934136390686}
2025-01-17 10:04:22,226 [INFO] Step[950/2713]: training loss : 0.9267536926269532 TRAIN  loss dict:  {'classification_loss': 0.9267536926269532}
2025-01-17 10:04:38,426 [INFO] Step[1000/2713]: training loss : 0.9258448612689972 TRAIN  loss dict:  {'classification_loss': 0.9258448612689972}
2025-01-17 10:04:54,638 [INFO] Step[1050/2713]: training loss : 0.925719084739685 TRAIN  loss dict:  {'classification_loss': 0.925719084739685}
2025-01-17 10:05:10,800 [INFO] Step[1100/2713]: training loss : 0.9260673284530639 TRAIN  loss dict:  {'classification_loss': 0.9260673284530639}
2025-01-17 10:05:27,061 [INFO] Step[1150/2713]: training loss : 0.9257369768619538 TRAIN  loss dict:  {'classification_loss': 0.9257369768619538}
2025-01-17 10:05:43,328 [INFO] Step[1200/2713]: training loss : 0.9254688370227814 TRAIN  loss dict:  {'classification_loss': 0.9254688370227814}
2025-01-17 10:05:59,518 [INFO] Step[1250/2713]: training loss : 0.9257063853740692 TRAIN  loss dict:  {'classification_loss': 0.9257063853740692}
2025-01-17 10:06:15,715 [INFO] Step[1300/2713]: training loss : 0.9281151247024536 TRAIN  loss dict:  {'classification_loss': 0.9281151247024536}
2025-01-17 10:06:31,977 [INFO] Step[1350/2713]: training loss : 0.9258514404296875 TRAIN  loss dict:  {'classification_loss': 0.9258514404296875}
2025-01-17 10:06:48,154 [INFO] Step[1400/2713]: training loss : 0.9267247354984284 TRAIN  loss dict:  {'classification_loss': 0.9267247354984284}
2025-01-17 10:07:04,392 [INFO] Step[1450/2713]: training loss : 0.926222265958786 TRAIN  loss dict:  {'classification_loss': 0.926222265958786}
2025-01-17 10:07:20,538 [INFO] Step[1500/2713]: training loss : 0.9264788556098938 TRAIN  loss dict:  {'classification_loss': 0.9264788556098938}
2025-01-17 10:07:36,769 [INFO] Step[1550/2713]: training loss : 0.9267348194122315 TRAIN  loss dict:  {'classification_loss': 0.9267348194122315}
2025-01-17 10:07:52,966 [INFO] Step[1600/2713]: training loss : 0.9265779721736908 TRAIN  loss dict:  {'classification_loss': 0.9265779721736908}
2025-01-17 10:08:09,217 [INFO] Step[1650/2713]: training loss : 0.9410350084304809 TRAIN  loss dict:  {'classification_loss': 0.9410350084304809}
2025-01-17 10:08:25,412 [INFO] Step[1700/2713]: training loss : 0.9281915032863617 TRAIN  loss dict:  {'classification_loss': 0.9281915032863617}
2025-01-17 10:08:41,567 [INFO] Step[1750/2713]: training loss : 0.9259806740283966 TRAIN  loss dict:  {'classification_loss': 0.9259806740283966}
2025-01-17 10:08:57,777 [INFO] Step[1800/2713]: training loss : 0.9254057109355927 TRAIN  loss dict:  {'classification_loss': 0.9254057109355927}
2025-01-17 10:09:14,076 [INFO] Step[1850/2713]: training loss : 0.9255800533294678 TRAIN  loss dict:  {'classification_loss': 0.9255800533294678}
2025-01-17 10:09:30,238 [INFO] Step[1900/2713]: training loss : 0.9253940963745118 TRAIN  loss dict:  {'classification_loss': 0.9253940963745118}
2025-01-17 10:09:46,474 [INFO] Step[1950/2713]: training loss : 0.9256382298469543 TRAIN  loss dict:  {'classification_loss': 0.9256382298469543}
2025-01-17 10:10:02,783 [INFO] Step[2000/2713]: training loss : 0.9257681620121002 TRAIN  loss dict:  {'classification_loss': 0.9257681620121002}
2025-01-17 10:10:19,106 [INFO] Step[2050/2713]: training loss : 0.926406387090683 TRAIN  loss dict:  {'classification_loss': 0.926406387090683}
2025-01-17 10:10:35,448 [INFO] Step[2100/2713]: training loss : 0.9262193191051483 TRAIN  loss dict:  {'classification_loss': 0.9262193191051483}
2025-01-17 10:10:51,807 [INFO] Step[2150/2713]: training loss : 0.925611116886139 TRAIN  loss dict:  {'classification_loss': 0.925611116886139}
2025-01-17 10:11:08,242 [INFO] Step[2200/2713]: training loss : 0.9254913866519928 TRAIN  loss dict:  {'classification_loss': 0.9254913866519928}
2025-01-17 10:11:24,666 [INFO] Step[2250/2713]: training loss : 0.9261830163002014 TRAIN  loss dict:  {'classification_loss': 0.9261830163002014}
2025-01-17 10:11:41,008 [INFO] Step[2300/2713]: training loss : 0.9264547860622406 TRAIN  loss dict:  {'classification_loss': 0.9264547860622406}
2025-01-17 10:11:57,364 [INFO] Step[2350/2713]: training loss : 0.9261426973342896 TRAIN  loss dict:  {'classification_loss': 0.9261426973342896}
2025-01-17 10:12:13,749 [INFO] Step[2400/2713]: training loss : 0.9280647933483124 TRAIN  loss dict:  {'classification_loss': 0.9280647933483124}
2025-01-17 10:12:30,137 [INFO] Step[2450/2713]: training loss : 0.9267824578285218 TRAIN  loss dict:  {'classification_loss': 0.9267824578285218}
2025-01-17 10:12:46,512 [INFO] Step[2500/2713]: training loss : 0.9257276666164398 TRAIN  loss dict:  {'classification_loss': 0.9257276666164398}
2025-01-17 10:13:02,967 [INFO] Step[2550/2713]: training loss : 0.9262775123119354 TRAIN  loss dict:  {'classification_loss': 0.9262775123119354}
2025-01-17 10:13:19,256 [INFO] Step[2600/2713]: training loss : 0.9253902113437653 TRAIN  loss dict:  {'classification_loss': 0.9253902113437653}
2025-01-17 10:13:35,649 [INFO] Step[2650/2713]: training loss : 0.9259899592399597 TRAIN  loss dict:  {'classification_loss': 0.9259899592399597}
2025-01-17 10:13:52,111 [INFO] Step[2700/2713]: training loss : 0.9307752275466918 TRAIN  loss dict:  {'classification_loss': 0.9307752275466918}
2025-01-17 10:15:11,468 [INFO] Label accuracies statistics:
2025-01-17 10:15:11,469 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.5, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.5, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.5, 217: 0.75, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.25, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.75, 354: 0.5, 355: 0.5, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 10:15:11,470 [INFO] [90] TRAIN  loss: 0.927069837388806 acc: 0.9996314043494287
2025-01-17 10:15:11,470 [INFO] [90] TRAIN  loss dict: {'classification_loss': 0.927069837388806}
2025-01-17 10:15:11,470 [INFO] [90] VALIDATION loss: 1.9030584636935615 VALIDATION acc: 0.8006269592476489
2025-01-17 10:15:11,470 [INFO] [90] VALIDATION loss dict: {'classification_loss': 1.9030584636935615}
2025-01-17 10:15:11,470 [INFO] 
2025-01-17 10:15:32,528 [INFO] Step[50/2713]: training loss : 0.9251833069324493 TRAIN  loss dict:  {'classification_loss': 0.9251833069324493}
2025-01-17 10:15:48,885 [INFO] Step[100/2713]: training loss : 0.9262819385528565 TRAIN  loss dict:  {'classification_loss': 0.9262819385528565}
2025-01-17 10:16:05,383 [INFO] Step[150/2713]: training loss : 0.925424177646637 TRAIN  loss dict:  {'classification_loss': 0.925424177646637}
2025-01-17 10:16:21,884 [INFO] Step[200/2713]: training loss : 0.925854766368866 TRAIN  loss dict:  {'classification_loss': 0.925854766368866}
2025-01-17 10:16:38,289 [INFO] Step[250/2713]: training loss : 0.9257215738296509 TRAIN  loss dict:  {'classification_loss': 0.9257215738296509}
2025-01-17 10:16:54,647 [INFO] Step[300/2713]: training loss : 0.9264175021648406 TRAIN  loss dict:  {'classification_loss': 0.9264175021648406}
2025-01-17 10:17:11,083 [INFO] Step[350/2713]: training loss : 0.9284269547462464 TRAIN  loss dict:  {'classification_loss': 0.9284269547462464}
2025-01-17 10:17:27,462 [INFO] Step[400/2713]: training loss : 0.9269665539264679 TRAIN  loss dict:  {'classification_loss': 0.9269665539264679}
2025-01-17 10:17:44,018 [INFO] Step[450/2713]: training loss : 0.9257740843296051 TRAIN  loss dict:  {'classification_loss': 0.9257740843296051}
2025-01-17 10:18:00,440 [INFO] Step[500/2713]: training loss : 0.9253176569938659 TRAIN  loss dict:  {'classification_loss': 0.9253176569938659}
2025-01-17 10:18:16,810 [INFO] Step[550/2713]: training loss : 0.9258914721012116 TRAIN  loss dict:  {'classification_loss': 0.9258914721012116}
2025-01-17 10:18:33,234 [INFO] Step[600/2713]: training loss : 0.9254446935653686 TRAIN  loss dict:  {'classification_loss': 0.9254446935653686}
2025-01-17 10:18:49,694 [INFO] Step[650/2713]: training loss : 0.9256603670120239 TRAIN  loss dict:  {'classification_loss': 0.9256603670120239}
2025-01-17 10:19:06,129 [INFO] Step[700/2713]: training loss : 0.9256146037578583 TRAIN  loss dict:  {'classification_loss': 0.9256146037578583}
2025-01-17 10:19:22,590 [INFO] Step[750/2713]: training loss : 0.9252873909473419 TRAIN  loss dict:  {'classification_loss': 0.9252873909473419}
2025-01-17 10:19:39,033 [INFO] Step[800/2713]: training loss : 0.9274690735340119 TRAIN  loss dict:  {'classification_loss': 0.9274690735340119}
2025-01-17 10:19:55,524 [INFO] Step[850/2713]: training loss : 0.9336067962646485 TRAIN  loss dict:  {'classification_loss': 0.9336067962646485}
2025-01-17 10:20:11,925 [INFO] Step[900/2713]: training loss : 0.9272865986824036 TRAIN  loss dict:  {'classification_loss': 0.9272865986824036}
2025-01-17 10:20:28,302 [INFO] Step[950/2713]: training loss : 0.9259289693832398 TRAIN  loss dict:  {'classification_loss': 0.9259289693832398}
2025-01-17 10:20:44,674 [INFO] Step[1000/2713]: training loss : 0.9257789349555969 TRAIN  loss dict:  {'classification_loss': 0.9257789349555969}
2025-01-17 10:21:01,175 [INFO] Step[1050/2713]: training loss : 0.9413736462593079 TRAIN  loss dict:  {'classification_loss': 0.9413736462593079}
2025-01-17 10:21:17,619 [INFO] Step[1100/2713]: training loss : 0.9258225047588349 TRAIN  loss dict:  {'classification_loss': 0.9258225047588349}
2025-01-17 10:21:34,083 [INFO] Step[1150/2713]: training loss : 0.9255919849872589 TRAIN  loss dict:  {'classification_loss': 0.9255919849872589}
2025-01-17 10:21:50,540 [INFO] Step[1200/2713]: training loss : 0.925476735830307 TRAIN  loss dict:  {'classification_loss': 0.925476735830307}
2025-01-17 10:22:06,986 [INFO] Step[1250/2713]: training loss : 0.9256852567195892 TRAIN  loss dict:  {'classification_loss': 0.9256852567195892}
2025-01-17 10:22:23,334 [INFO] Step[1300/2713]: training loss : 0.9488390243053436 TRAIN  loss dict:  {'classification_loss': 0.9488390243053436}
2025-01-17 10:22:39,726 [INFO] Step[1350/2713]: training loss : 0.9253416609764099 TRAIN  loss dict:  {'classification_loss': 0.9253416609764099}
2025-01-17 10:22:56,101 [INFO] Step[1400/2713]: training loss : 0.9253634130954742 TRAIN  loss dict:  {'classification_loss': 0.9253634130954742}
2025-01-17 10:23:12,525 [INFO] Step[1450/2713]: training loss : 0.9261323821544647 TRAIN  loss dict:  {'classification_loss': 0.9261323821544647}
2025-01-17 10:23:29,007 [INFO] Step[1500/2713]: training loss : 0.9257578992843628 TRAIN  loss dict:  {'classification_loss': 0.9257578992843628}
2025-01-17 10:23:45,449 [INFO] Step[1550/2713]: training loss : 0.9253714489936828 TRAIN  loss dict:  {'classification_loss': 0.9253714489936828}
2025-01-17 10:24:01,807 [INFO] Step[1600/2713]: training loss : 0.9266121816635132 TRAIN  loss dict:  {'classification_loss': 0.9266121816635132}
2025-01-17 10:24:18,175 [INFO] Step[1650/2713]: training loss : 0.9257634627819061 TRAIN  loss dict:  {'classification_loss': 0.9257634627819061}
2025-01-17 10:24:34,640 [INFO] Step[1700/2713]: training loss : 0.9258208858966828 TRAIN  loss dict:  {'classification_loss': 0.9258208858966828}
2025-01-17 10:24:51,092 [INFO] Step[1750/2713]: training loss : 0.9259208106994629 TRAIN  loss dict:  {'classification_loss': 0.9259208106994629}
2025-01-17 10:25:07,501 [INFO] Step[1800/2713]: training loss : 0.9276110637187958 TRAIN  loss dict:  {'classification_loss': 0.9276110637187958}
2025-01-17 10:25:23,913 [INFO] Step[1850/2713]: training loss : 0.9252850830554962 TRAIN  loss dict:  {'classification_loss': 0.9252850830554962}
2025-01-17 10:25:40,309 [INFO] Step[1900/2713]: training loss : 0.9254007744789123 TRAIN  loss dict:  {'classification_loss': 0.9254007744789123}
2025-01-17 10:25:56,732 [INFO] Step[1950/2713]: training loss : 0.9259393167495727 TRAIN  loss dict:  {'classification_loss': 0.9259393167495727}
2025-01-17 10:26:13,135 [INFO] Step[2000/2713]: training loss : 0.9255532324314117 TRAIN  loss dict:  {'classification_loss': 0.9255532324314117}
2025-01-17 10:26:29,600 [INFO] Step[2050/2713]: training loss : 0.9255920338630677 TRAIN  loss dict:  {'classification_loss': 0.9255920338630677}
2025-01-17 10:26:46,015 [INFO] Step[2100/2713]: training loss : 0.925863687992096 TRAIN  loss dict:  {'classification_loss': 0.925863687992096}
2025-01-17 10:27:02,462 [INFO] Step[2150/2713]: training loss : 0.9256603622436523 TRAIN  loss dict:  {'classification_loss': 0.9256603622436523}
2025-01-17 10:27:18,888 [INFO] Step[2200/2713]: training loss : 0.9265185236930847 TRAIN  loss dict:  {'classification_loss': 0.9265185236930847}
2025-01-17 10:27:35,321 [INFO] Step[2250/2713]: training loss : 0.9266560316085816 TRAIN  loss dict:  {'classification_loss': 0.9266560316085816}
2025-01-17 10:27:51,846 [INFO] Step[2300/2713]: training loss : 0.9258009743690491 TRAIN  loss dict:  {'classification_loss': 0.9258009743690491}
2025-01-17 10:28:08,311 [INFO] Step[2350/2713]: training loss : 0.9252813303470612 TRAIN  loss dict:  {'classification_loss': 0.9252813303470612}
2025-01-17 10:28:24,672 [INFO] Step[2400/2713]: training loss : 0.9256736469268799 TRAIN  loss dict:  {'classification_loss': 0.9256736469268799}
2025-01-17 10:28:41,077 [INFO] Step[2450/2713]: training loss : 0.9256282484531403 TRAIN  loss dict:  {'classification_loss': 0.9256282484531403}
2025-01-17 10:28:57,433 [INFO] Step[2500/2713]: training loss : 0.9250483191013337 TRAIN  loss dict:  {'classification_loss': 0.9250483191013337}
2025-01-17 10:29:13,793 [INFO] Step[2550/2713]: training loss : 0.9257140552997589 TRAIN  loss dict:  {'classification_loss': 0.9257140552997589}
2025-01-17 10:29:30,102 [INFO] Step[2600/2713]: training loss : 0.9254393076896668 TRAIN  loss dict:  {'classification_loss': 0.9254393076896668}
2025-01-17 10:29:46,437 [INFO] Step[2650/2713]: training loss : 0.9265404689311981 TRAIN  loss dict:  {'classification_loss': 0.9265404689311981}
2025-01-17 10:30:02,886 [INFO] Step[2700/2713]: training loss : 0.9254564809799194 TRAIN  loss dict:  {'classification_loss': 0.9254564809799194}
2025-01-17 10:31:22,069 [INFO] Label accuracies statistics:
2025-01-17 10:31:22,069 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.25, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 0.75, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.25, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.25, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.5, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 10:31:22,071 [INFO] [91] TRAIN  loss: 0.9267476443386675 acc: 0.9996314043494287
2025-01-17 10:31:22,071 [INFO] [91] TRAIN  loss dict: {'classification_loss': 0.9267476443386675}
2025-01-17 10:31:22,071 [INFO] [91] VALIDATION loss: 1.8943953778510703 VALIDATION acc: 0.7987460815047022
2025-01-17 10:31:22,071 [INFO] [91] VALIDATION loss dict: {'classification_loss': 1.8943953778510703}
2025-01-17 10:31:22,071 [INFO] 
2025-01-17 10:31:43,217 [INFO] Step[50/2713]: training loss : 0.9254987096786499 TRAIN  loss dict:  {'classification_loss': 0.9254987096786499}
2025-01-17 10:31:59,584 [INFO] Step[100/2713]: training loss : 0.9254931724071502 TRAIN  loss dict:  {'classification_loss': 0.9254931724071502}
2025-01-17 10:32:15,980 [INFO] Step[150/2713]: training loss : 0.9272083497047424 TRAIN  loss dict:  {'classification_loss': 0.9272083497047424}
2025-01-17 10:32:32,451 [INFO] Step[200/2713]: training loss : 0.9258544445037842 TRAIN  loss dict:  {'classification_loss': 0.9258544445037842}
2025-01-17 10:32:48,892 [INFO] Step[250/2713]: training loss : 0.9254780066013336 TRAIN  loss dict:  {'classification_loss': 0.9254780066013336}
2025-01-17 10:33:05,389 [INFO] Step[300/2713]: training loss : 0.9258136749267578 TRAIN  loss dict:  {'classification_loss': 0.9258136749267578}
2025-01-17 10:33:21,733 [INFO] Step[350/2713]: training loss : 0.9253577852249145 TRAIN  loss dict:  {'classification_loss': 0.9253577852249145}
2025-01-17 10:33:38,116 [INFO] Step[400/2713]: training loss : 0.9256779170036316 TRAIN  loss dict:  {'classification_loss': 0.9256779170036316}
2025-01-17 10:33:54,562 [INFO] Step[450/2713]: training loss : 0.9252162992954254 TRAIN  loss dict:  {'classification_loss': 0.9252162992954254}
2025-01-17 10:34:10,964 [INFO] Step[500/2713]: training loss : 0.9263836097717285 TRAIN  loss dict:  {'classification_loss': 0.9263836097717285}
2025-01-17 10:34:27,355 [INFO] Step[550/2713]: training loss : 0.9296642327308655 TRAIN  loss dict:  {'classification_loss': 0.9296642327308655}
2025-01-17 10:34:43,744 [INFO] Step[600/2713]: training loss : 0.9262532579898834 TRAIN  loss dict:  {'classification_loss': 0.9262532579898834}
2025-01-17 10:35:00,193 [INFO] Step[650/2713]: training loss : 0.9253780996799469 TRAIN  loss dict:  {'classification_loss': 0.9253780996799469}
2025-01-17 10:35:16,614 [INFO] Step[700/2713]: training loss : 0.9257934582233429 TRAIN  loss dict:  {'classification_loss': 0.9257934582233429}
2025-01-17 10:35:33,129 [INFO] Step[750/2713]: training loss : 0.9253612673282623 TRAIN  loss dict:  {'classification_loss': 0.9253612673282623}
2025-01-17 10:35:49,334 [INFO] Step[800/2713]: training loss : 0.9256270039081573 TRAIN  loss dict:  {'classification_loss': 0.9256270039081573}
2025-01-17 10:36:05,572 [INFO] Step[850/2713]: training loss : 0.9255018830299377 TRAIN  loss dict:  {'classification_loss': 0.9255018830299377}
2025-01-17 10:36:21,756 [INFO] Step[900/2713]: training loss : 0.9261644601821899 TRAIN  loss dict:  {'classification_loss': 0.9261644601821899}
2025-01-17 10:36:38,094 [INFO] Step[950/2713]: training loss : 0.926911916732788 TRAIN  loss dict:  {'classification_loss': 0.926911916732788}
2025-01-17 10:36:54,317 [INFO] Step[1000/2713]: training loss : 0.9251819634437561 TRAIN  loss dict:  {'classification_loss': 0.9251819634437561}
2025-01-17 10:37:10,644 [INFO] Step[1050/2713]: training loss : 0.9257637786865235 TRAIN  loss dict:  {'classification_loss': 0.9257637786865235}
2025-01-17 10:37:26,860 [INFO] Step[1100/2713]: training loss : 0.9260406565666198 TRAIN  loss dict:  {'classification_loss': 0.9260406565666198}
2025-01-17 10:37:43,033 [INFO] Step[1150/2713]: training loss : 0.9262231969833374 TRAIN  loss dict:  {'classification_loss': 0.9262231969833374}
2025-01-17 10:37:59,254 [INFO] Step[1200/2713]: training loss : 0.9257783496379852 TRAIN  loss dict:  {'classification_loss': 0.9257783496379852}
2025-01-17 10:38:15,456 [INFO] Step[1250/2713]: training loss : 0.9255631482601165 TRAIN  loss dict:  {'classification_loss': 0.9255631482601165}
2025-01-17 10:38:31,670 [INFO] Step[1300/2713]: training loss : 0.9253021097183227 TRAIN  loss dict:  {'classification_loss': 0.9253021097183227}
2025-01-17 10:38:47,884 [INFO] Step[1350/2713]: training loss : 0.925435471534729 TRAIN  loss dict:  {'classification_loss': 0.925435471534729}
2025-01-17 10:39:04,170 [INFO] Step[1400/2713]: training loss : 0.9262254762649537 TRAIN  loss dict:  {'classification_loss': 0.9262254762649537}
2025-01-17 10:39:20,456 [INFO] Step[1450/2713]: training loss : 0.9258430325984954 TRAIN  loss dict:  {'classification_loss': 0.9258430325984954}
2025-01-17 10:39:36,810 [INFO] Step[1500/2713]: training loss : 0.9257433795928955 TRAIN  loss dict:  {'classification_loss': 0.9257433795928955}
2025-01-17 10:39:53,126 [INFO] Step[1550/2713]: training loss : 0.9303104031085968 TRAIN  loss dict:  {'classification_loss': 0.9303104031085968}
2025-01-17 10:40:09,425 [INFO] Step[1600/2713]: training loss : 0.9252171862125397 TRAIN  loss dict:  {'classification_loss': 0.9252171862125397}
2025-01-17 10:40:25,619 [INFO] Step[1650/2713]: training loss : 0.9256798350811004 TRAIN  loss dict:  {'classification_loss': 0.9256798350811004}
2025-01-17 10:40:41,939 [INFO] Step[1700/2713]: training loss : 0.9257615602016449 TRAIN  loss dict:  {'classification_loss': 0.9257615602016449}
2025-01-17 10:40:58,181 [INFO] Step[1750/2713]: training loss : 0.9253601920604706 TRAIN  loss dict:  {'classification_loss': 0.9253601920604706}
2025-01-17 10:41:14,387 [INFO] Step[1800/2713]: training loss : 0.9255317926406861 TRAIN  loss dict:  {'classification_loss': 0.9255317926406861}
2025-01-17 10:41:30,668 [INFO] Step[1850/2713]: training loss : 0.9301887333393097 TRAIN  loss dict:  {'classification_loss': 0.9301887333393097}
2025-01-17 10:41:46,979 [INFO] Step[1900/2713]: training loss : 0.9253409790992737 TRAIN  loss dict:  {'classification_loss': 0.9253409790992737}
2025-01-17 10:42:03,300 [INFO] Step[1950/2713]: training loss : 0.9258909046649932 TRAIN  loss dict:  {'classification_loss': 0.9258909046649932}
2025-01-17 10:42:19,485 [INFO] Step[2000/2713]: training loss : 0.9254090857505798 TRAIN  loss dict:  {'classification_loss': 0.9254090857505798}
2025-01-17 10:42:35,746 [INFO] Step[2050/2713]: training loss : 0.925359935760498 TRAIN  loss dict:  {'classification_loss': 0.925359935760498}
2025-01-17 10:42:51,992 [INFO] Step[2100/2713]: training loss : 0.9259543335437774 TRAIN  loss dict:  {'classification_loss': 0.9259543335437774}
2025-01-17 10:43:08,190 [INFO] Step[2150/2713]: training loss : 0.9258357989788055 TRAIN  loss dict:  {'classification_loss': 0.9258357989788055}
2025-01-17 10:43:24,480 [INFO] Step[2200/2713]: training loss : 0.928925222158432 TRAIN  loss dict:  {'classification_loss': 0.928925222158432}
2025-01-17 10:43:40,749 [INFO] Step[2250/2713]: training loss : 0.9256498825550079 TRAIN  loss dict:  {'classification_loss': 0.9256498825550079}
2025-01-17 10:43:57,007 [INFO] Step[2300/2713]: training loss : 0.9256383168697357 TRAIN  loss dict:  {'classification_loss': 0.9256383168697357}
2025-01-17 10:44:13,249 [INFO] Step[2350/2713]: training loss : 0.9259113073348999 TRAIN  loss dict:  {'classification_loss': 0.9259113073348999}
2025-01-17 10:44:29,521 [INFO] Step[2400/2713]: training loss : 0.9256729781627655 TRAIN  loss dict:  {'classification_loss': 0.9256729781627655}
2025-01-17 10:44:45,818 [INFO] Step[2450/2713]: training loss : 0.9259988725185394 TRAIN  loss dict:  {'classification_loss': 0.9259988725185394}
2025-01-17 10:45:02,028 [INFO] Step[2500/2713]: training loss : 0.9252786266803742 TRAIN  loss dict:  {'classification_loss': 0.9252786266803742}
2025-01-17 10:45:18,285 [INFO] Step[2550/2713]: training loss : 0.9250490558147431 TRAIN  loss dict:  {'classification_loss': 0.9250490558147431}
2025-01-17 10:45:34,499 [INFO] Step[2600/2713]: training loss : 0.925390533208847 TRAIN  loss dict:  {'classification_loss': 0.925390533208847}
2025-01-17 10:45:50,735 [INFO] Step[2650/2713]: training loss : 0.9253024399280548 TRAIN  loss dict:  {'classification_loss': 0.9253024399280548}
2025-01-17 10:46:06,908 [INFO] Step[2700/2713]: training loss : 0.9253689789772034 TRAIN  loss dict:  {'classification_loss': 0.9253689789772034}
2025-01-17 10:47:26,281 [INFO] Label accuracies statistics:
2025-01-17 10:47:26,281 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 1.0, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 1.0, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.25, 262: 1.0, 263: 0.75, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.5, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 1.0, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.0, 382: 0.75, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 10:47:26,283 [INFO] [92] TRAIN  loss: 0.9259962806655962 acc: 1.0
2025-01-17 10:47:26,283 [INFO] [92] TRAIN  loss dict: {'classification_loss': 0.9259962806655962}
2025-01-17 10:47:26,283 [INFO] [92] VALIDATION loss: 1.9073452678390015 VALIDATION acc: 0.8068965517241379
2025-01-17 10:47:26,283 [INFO] [92] VALIDATION loss dict: {'classification_loss': 1.9073452678390015}
2025-01-17 10:47:26,283 [INFO] 
2025-01-17 10:47:47,011 [INFO] Step[50/2713]: training loss : 0.925392986536026 TRAIN  loss dict:  {'classification_loss': 0.925392986536026}
2025-01-17 10:48:03,226 [INFO] Step[100/2713]: training loss : 0.92537073969841 TRAIN  loss dict:  {'classification_loss': 0.92537073969841}
2025-01-17 10:48:19,412 [INFO] Step[150/2713]: training loss : 0.9254208302497864 TRAIN  loss dict:  {'classification_loss': 0.9254208302497864}
2025-01-17 10:48:35,643 [INFO] Step[200/2713]: training loss : 0.92562894821167 TRAIN  loss dict:  {'classification_loss': 0.92562894821167}
2025-01-17 10:48:51,858 [INFO] Step[250/2713]: training loss : 0.9251345992088318 TRAIN  loss dict:  {'classification_loss': 0.9251345992088318}
2025-01-17 10:49:08,078 [INFO] Step[300/2713]: training loss : 0.9256663703918457 TRAIN  loss dict:  {'classification_loss': 0.9256663703918457}
2025-01-17 10:49:24,319 [INFO] Step[350/2713]: training loss : 0.9252830743789673 TRAIN  loss dict:  {'classification_loss': 0.9252830743789673}
2025-01-17 10:49:40,488 [INFO] Step[400/2713]: training loss : 0.925859739780426 TRAIN  loss dict:  {'classification_loss': 0.925859739780426}
2025-01-17 10:49:56,762 [INFO] Step[450/2713]: training loss : 0.9254138004779816 TRAIN  loss dict:  {'classification_loss': 0.9254138004779816}
2025-01-17 10:50:12,984 [INFO] Step[500/2713]: training loss : 0.925502028465271 TRAIN  loss dict:  {'classification_loss': 0.925502028465271}
2025-01-17 10:50:29,183 [INFO] Step[550/2713]: training loss : 0.9254377126693726 TRAIN  loss dict:  {'classification_loss': 0.9254377126693726}
2025-01-17 10:50:45,338 [INFO] Step[600/2713]: training loss : 0.9258648359775543 TRAIN  loss dict:  {'classification_loss': 0.9258648359775543}
2025-01-17 10:51:01,550 [INFO] Step[650/2713]: training loss : 0.9250740921497345 TRAIN  loss dict:  {'classification_loss': 0.9250740921497345}
2025-01-17 10:51:17,726 [INFO] Step[700/2713]: training loss : 0.9263710832595825 TRAIN  loss dict:  {'classification_loss': 0.9263710832595825}
2025-01-17 10:51:33,889 [INFO] Step[750/2713]: training loss : 0.9256261742115021 TRAIN  loss dict:  {'classification_loss': 0.9256261742115021}
2025-01-17 10:51:50,072 [INFO] Step[800/2713]: training loss : 0.9255311810970306 TRAIN  loss dict:  {'classification_loss': 0.9255311810970306}
2025-01-17 10:52:06,277 [INFO] Step[850/2713]: training loss : 0.9257474970817566 TRAIN  loss dict:  {'classification_loss': 0.9257474970817566}
2025-01-17 10:52:22,477 [INFO] Step[900/2713]: training loss : 0.9258259212970734 TRAIN  loss dict:  {'classification_loss': 0.9258259212970734}
2025-01-17 10:52:38,700 [INFO] Step[950/2713]: training loss : 0.9252389693260192 TRAIN  loss dict:  {'classification_loss': 0.9252389693260192}
2025-01-17 10:52:54,953 [INFO] Step[1000/2713]: training loss : 0.9257013297080994 TRAIN  loss dict:  {'classification_loss': 0.9257013297080994}
2025-01-17 10:53:11,118 [INFO] Step[1050/2713]: training loss : 0.925280612707138 TRAIN  loss dict:  {'classification_loss': 0.925280612707138}
2025-01-17 10:53:27,299 [INFO] Step[1100/2713]: training loss : 0.9256819152832031 TRAIN  loss dict:  {'classification_loss': 0.9256819152832031}
2025-01-17 10:53:43,508 [INFO] Step[1150/2713]: training loss : 0.9254471683502197 TRAIN  loss dict:  {'classification_loss': 0.9254471683502197}
2025-01-17 10:53:59,751 [INFO] Step[1200/2713]: training loss : 0.9268077135086059 TRAIN  loss dict:  {'classification_loss': 0.9268077135086059}
2025-01-17 10:54:15,986 [INFO] Step[1250/2713]: training loss : 0.9257305288314819 TRAIN  loss dict:  {'classification_loss': 0.9257305288314819}
2025-01-17 10:54:32,128 [INFO] Step[1300/2713]: training loss : 0.9254564142227173 TRAIN  loss dict:  {'classification_loss': 0.9254564142227173}
2025-01-17 10:54:48,452 [INFO] Step[1350/2713]: training loss : 0.9256818199157715 TRAIN  loss dict:  {'classification_loss': 0.9256818199157715}
2025-01-17 10:55:04,633 [INFO] Step[1400/2713]: training loss : 0.9256825411319732 TRAIN  loss dict:  {'classification_loss': 0.9256825411319732}
2025-01-17 10:55:20,858 [INFO] Step[1450/2713]: training loss : 0.9256341862678528 TRAIN  loss dict:  {'classification_loss': 0.9256341862678528}
2025-01-17 10:55:37,050 [INFO] Step[1500/2713]: training loss : 0.9250095295906067 TRAIN  loss dict:  {'classification_loss': 0.9250095295906067}
2025-01-17 10:55:53,298 [INFO] Step[1550/2713]: training loss : 0.9257608973979949 TRAIN  loss dict:  {'classification_loss': 0.9257608973979949}
2025-01-17 10:56:09,408 [INFO] Step[1600/2713]: training loss : 0.9252803814411164 TRAIN  loss dict:  {'classification_loss': 0.9252803814411164}
2025-01-17 10:56:25,633 [INFO] Step[1650/2713]: training loss : 0.9252255821228027 TRAIN  loss dict:  {'classification_loss': 0.9252255821228027}
2025-01-17 10:56:41,808 [INFO] Step[1700/2713]: training loss : 0.9265148603916168 TRAIN  loss dict:  {'classification_loss': 0.9265148603916168}
2025-01-17 10:56:57,986 [INFO] Step[1750/2713]: training loss : 0.9257879281044006 TRAIN  loss dict:  {'classification_loss': 0.9257879281044006}
2025-01-17 10:57:14,178 [INFO] Step[1800/2713]: training loss : 0.9257110178470611 TRAIN  loss dict:  {'classification_loss': 0.9257110178470611}
2025-01-17 10:57:30,408 [INFO] Step[1850/2713]: training loss : 0.9261072981357574 TRAIN  loss dict:  {'classification_loss': 0.9261072981357574}
2025-01-17 10:57:46,525 [INFO] Step[1900/2713]: training loss : 0.9257038748264312 TRAIN  loss dict:  {'classification_loss': 0.9257038748264312}
2025-01-17 10:58:02,670 [INFO] Step[1950/2713]: training loss : 0.9255197632312775 TRAIN  loss dict:  {'classification_loss': 0.9255197632312775}
2025-01-17 10:58:18,837 [INFO] Step[2000/2713]: training loss : 0.9252470922470093 TRAIN  loss dict:  {'classification_loss': 0.9252470922470093}
2025-01-17 10:58:35,094 [INFO] Step[2050/2713]: training loss : 0.9256770384311676 TRAIN  loss dict:  {'classification_loss': 0.9256770384311676}
2025-01-17 10:58:51,301 [INFO] Step[2100/2713]: training loss : 0.9257778346538543 TRAIN  loss dict:  {'classification_loss': 0.9257778346538543}
2025-01-17 10:59:07,436 [INFO] Step[2150/2713]: training loss : 0.9256162190437317 TRAIN  loss dict:  {'classification_loss': 0.9256162190437317}
2025-01-17 10:59:23,692 [INFO] Step[2200/2713]: training loss : 0.9331729006767273 TRAIN  loss dict:  {'classification_loss': 0.9331729006767273}
2025-01-17 10:59:39,896 [INFO] Step[2250/2713]: training loss : 0.9261765396595001 TRAIN  loss dict:  {'classification_loss': 0.9261765396595001}
2025-01-17 10:59:56,068 [INFO] Step[2300/2713]: training loss : 0.9258998429775238 TRAIN  loss dict:  {'classification_loss': 0.9258998429775238}
2025-01-17 11:00:12,294 [INFO] Step[2350/2713]: training loss : 0.9254993748664856 TRAIN  loss dict:  {'classification_loss': 0.9254993748664856}
2025-01-17 11:00:28,492 [INFO] Step[2400/2713]: training loss : 0.9262187170982361 TRAIN  loss dict:  {'classification_loss': 0.9262187170982361}
2025-01-17 11:00:44,686 [INFO] Step[2450/2713]: training loss : 0.9254440522193909 TRAIN  loss dict:  {'classification_loss': 0.9254440522193909}
2025-01-17 11:01:00,918 [INFO] Step[2500/2713]: training loss : 0.9259048116207123 TRAIN  loss dict:  {'classification_loss': 0.9259048116207123}
2025-01-17 11:01:17,223 [INFO] Step[2550/2713]: training loss : 0.925549008846283 TRAIN  loss dict:  {'classification_loss': 0.925549008846283}
2025-01-17 11:01:33,474 [INFO] Step[2600/2713]: training loss : 0.9257093262672424 TRAIN  loss dict:  {'classification_loss': 0.9257093262672424}
2025-01-17 11:01:49,751 [INFO] Step[2650/2713]: training loss : 0.9255797362327576 TRAIN  loss dict:  {'classification_loss': 0.9255797362327576}
2025-01-17 11:02:05,969 [INFO] Step[2700/2713]: training loss : 0.9257681357860565 TRAIN  loss dict:  {'classification_loss': 0.9257681357860565}
2025-01-17 11:03:24,760 [INFO] Label accuracies statistics:
2025-01-17 11:03:24,760 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 1.0, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 1.0, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.5, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 1.0, 261: 0.5, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 0.75, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 11:03:24,762 [INFO] [93] TRAIN  loss: 0.9257821808914131 acc: 0.9998771347831429
2025-01-17 11:03:24,762 [INFO] [93] TRAIN  loss dict: {'classification_loss': 0.9257821808914131}
2025-01-17 11:03:24,762 [INFO] [93] VALIDATION loss: 1.8673413019431264 VALIDATION acc: 0.8068965517241379
2025-01-17 11:03:24,762 [INFO] [93] VALIDATION loss dict: {'classification_loss': 1.8673413019431264}
2025-01-17 11:03:24,763 [INFO] 
2025-01-17 11:03:46,625 [INFO] Step[50/2713]: training loss : 0.9252844107151031 TRAIN  loss dict:  {'classification_loss': 0.9252844107151031}
2025-01-17 11:04:02,809 [INFO] Step[100/2713]: training loss : 0.9256693685054779 TRAIN  loss dict:  {'classification_loss': 0.9256693685054779}
2025-01-17 11:04:19,008 [INFO] Step[150/2713]: training loss : 0.9251505875587464 TRAIN  loss dict:  {'classification_loss': 0.9251505875587464}
2025-01-17 11:04:35,169 [INFO] Step[200/2713]: training loss : 0.9252875471115112 TRAIN  loss dict:  {'classification_loss': 0.9252875471115112}
2025-01-17 11:04:51,400 [INFO] Step[250/2713]: training loss : 0.9459367847442627 TRAIN  loss dict:  {'classification_loss': 0.9459367847442627}
2025-01-17 11:05:07,580 [INFO] Step[300/2713]: training loss : 0.9255101597309112 TRAIN  loss dict:  {'classification_loss': 0.9255101597309112}
2025-01-17 11:05:23,860 [INFO] Step[350/2713]: training loss : 0.9250162446498871 TRAIN  loss dict:  {'classification_loss': 0.9250162446498871}
2025-01-17 11:05:40,050 [INFO] Step[400/2713]: training loss : 0.9254731583595276 TRAIN  loss dict:  {'classification_loss': 0.9254731583595276}
2025-01-17 11:05:56,310 [INFO] Step[450/2713]: training loss : 0.9253012406826019 TRAIN  loss dict:  {'classification_loss': 0.9253012406826019}
2025-01-17 11:06:12,520 [INFO] Step[500/2713]: training loss : 0.9251448738574982 TRAIN  loss dict:  {'classification_loss': 0.9251448738574982}
2025-01-17 11:06:28,794 [INFO] Step[550/2713]: training loss : 0.9261347246170044 TRAIN  loss dict:  {'classification_loss': 0.9261347246170044}
2025-01-17 11:06:44,991 [INFO] Step[600/2713]: training loss : 0.9278356623649597 TRAIN  loss dict:  {'classification_loss': 0.9278356623649597}
2025-01-17 11:07:01,246 [INFO] Step[650/2713]: training loss : 0.9253308761119843 TRAIN  loss dict:  {'classification_loss': 0.9253308761119843}
2025-01-17 11:07:17,536 [INFO] Step[700/2713]: training loss : 0.9250947260856628 TRAIN  loss dict:  {'classification_loss': 0.9250947260856628}
2025-01-17 11:07:33,725 [INFO] Step[750/2713]: training loss : 0.9253242313861847 TRAIN  loss dict:  {'classification_loss': 0.9253242313861847}
2025-01-17 11:07:49,908 [INFO] Step[800/2713]: training loss : 0.92905721783638 TRAIN  loss dict:  {'classification_loss': 0.92905721783638}
2025-01-17 11:08:06,132 [INFO] Step[850/2713]: training loss : 0.9257080268859863 TRAIN  loss dict:  {'classification_loss': 0.9257080268859863}
2025-01-17 11:08:22,313 [INFO] Step[900/2713]: training loss : 0.925577323436737 TRAIN  loss dict:  {'classification_loss': 0.925577323436737}
2025-01-17 11:08:38,631 [INFO] Step[950/2713]: training loss : 0.9261832666397095 TRAIN  loss dict:  {'classification_loss': 0.9261832666397095}
2025-01-17 11:08:54,857 [INFO] Step[1000/2713]: training loss : 0.9261096251010895 TRAIN  loss dict:  {'classification_loss': 0.9261096251010895}
2025-01-17 11:09:11,101 [INFO] Step[1050/2713]: training loss : 0.9252343821525574 TRAIN  loss dict:  {'classification_loss': 0.9252343821525574}
2025-01-17 11:09:27,283 [INFO] Step[1100/2713]: training loss : 0.9265797185897827 TRAIN  loss dict:  {'classification_loss': 0.9265797185897827}
2025-01-17 11:09:43,478 [INFO] Step[1150/2713]: training loss : 0.9250786697864533 TRAIN  loss dict:  {'classification_loss': 0.9250786697864533}
2025-01-17 11:09:59,666 [INFO] Step[1200/2713]: training loss : 0.9252579176425934 TRAIN  loss dict:  {'classification_loss': 0.9252579176425934}
2025-01-17 11:10:15,908 [INFO] Step[1250/2713]: training loss : 0.9255710911750793 TRAIN  loss dict:  {'classification_loss': 0.9255710911750793}
2025-01-17 11:10:32,064 [INFO] Step[1300/2713]: training loss : 0.9260743772983551 TRAIN  loss dict:  {'classification_loss': 0.9260743772983551}
2025-01-17 11:10:48,263 [INFO] Step[1350/2713]: training loss : 0.9578248655796051 TRAIN  loss dict:  {'classification_loss': 0.9578248655796051}
2025-01-17 11:11:04,417 [INFO] Step[1400/2713]: training loss : 0.9258849501609803 TRAIN  loss dict:  {'classification_loss': 0.9258849501609803}
2025-01-17 11:11:20,632 [INFO] Step[1450/2713]: training loss : 0.9253885066509246 TRAIN  loss dict:  {'classification_loss': 0.9253885066509246}
2025-01-17 11:11:36,827 [INFO] Step[1500/2713]: training loss : 0.9254780030250549 TRAIN  loss dict:  {'classification_loss': 0.9254780030250549}
2025-01-17 11:11:53,032 [INFO] Step[1550/2713]: training loss : 0.9254538810253143 TRAIN  loss dict:  {'classification_loss': 0.9254538810253143}
2025-01-17 11:12:09,289 [INFO] Step[1600/2713]: training loss : 0.9253491139411927 TRAIN  loss dict:  {'classification_loss': 0.9253491139411927}
2025-01-17 11:12:25,504 [INFO] Step[1650/2713]: training loss : 0.9247793972492218 TRAIN  loss dict:  {'classification_loss': 0.9247793972492218}
2025-01-17 11:12:41,655 [INFO] Step[1700/2713]: training loss : 0.9254029166698455 TRAIN  loss dict:  {'classification_loss': 0.9254029166698455}
2025-01-17 11:12:57,813 [INFO] Step[1750/2713]: training loss : 0.9288065314292908 TRAIN  loss dict:  {'classification_loss': 0.9288065314292908}
2025-01-17 11:13:13,951 [INFO] Step[1800/2713]: training loss : 0.925524080991745 TRAIN  loss dict:  {'classification_loss': 0.925524080991745}
2025-01-17 11:13:30,110 [INFO] Step[1850/2713]: training loss : 0.9256416440010071 TRAIN  loss dict:  {'classification_loss': 0.9256416440010071}
2025-01-17 11:13:46,284 [INFO] Step[1900/2713]: training loss : 0.9256118202209472 TRAIN  loss dict:  {'classification_loss': 0.9256118202209472}
2025-01-17 11:14:02,478 [INFO] Step[1950/2713]: training loss : 0.9256418144702911 TRAIN  loss dict:  {'classification_loss': 0.9256418144702911}
2025-01-17 11:14:18,671 [INFO] Step[2000/2713]: training loss : 0.9257968878746032 TRAIN  loss dict:  {'classification_loss': 0.9257968878746032}
2025-01-17 11:14:34,890 [INFO] Step[2050/2713]: training loss : 0.925860493183136 TRAIN  loss dict:  {'classification_loss': 0.925860493183136}
2025-01-17 11:14:51,010 [INFO] Step[2100/2713]: training loss : 0.9256860041618347 TRAIN  loss dict:  {'classification_loss': 0.9256860041618347}
2025-01-17 11:15:07,215 [INFO] Step[2150/2713]: training loss : 0.9259542047977447 TRAIN  loss dict:  {'classification_loss': 0.9259542047977447}
2025-01-17 11:15:23,359 [INFO] Step[2200/2713]: training loss : 0.9253100895881653 TRAIN  loss dict:  {'classification_loss': 0.9253100895881653}
2025-01-17 11:15:39,569 [INFO] Step[2250/2713]: training loss : 0.9263295292854309 TRAIN  loss dict:  {'classification_loss': 0.9263295292854309}
2025-01-17 11:15:55,693 [INFO] Step[2300/2713]: training loss : 0.9255956530570983 TRAIN  loss dict:  {'classification_loss': 0.9255956530570983}
2025-01-17 11:16:11,857 [INFO] Step[2350/2713]: training loss : 0.9668835413455963 TRAIN  loss dict:  {'classification_loss': 0.9668835413455963}
2025-01-17 11:16:28,031 [INFO] Step[2400/2713]: training loss : 0.9259122729301452 TRAIN  loss dict:  {'classification_loss': 0.9259122729301452}
2025-01-17 11:16:44,238 [INFO] Step[2450/2713]: training loss : 0.9252336311340332 TRAIN  loss dict:  {'classification_loss': 0.9252336311340332}
2025-01-17 11:17:00,420 [INFO] Step[2500/2713]: training loss : 0.9256730902194977 TRAIN  loss dict:  {'classification_loss': 0.9256730902194977}
2025-01-17 11:17:16,646 [INFO] Step[2550/2713]: training loss : 0.9254707133769989 TRAIN  loss dict:  {'classification_loss': 0.9254707133769989}
2025-01-17 11:17:32,715 [INFO] Step[2600/2713]: training loss : 0.9256037664413452 TRAIN  loss dict:  {'classification_loss': 0.9256037664413452}
2025-01-17 11:17:48,870 [INFO] Step[2650/2713]: training loss : 0.9257050621509552 TRAIN  loss dict:  {'classification_loss': 0.9257050621509552}
2025-01-17 11:18:05,075 [INFO] Step[2700/2713]: training loss : 0.92579221367836 TRAIN  loss dict:  {'classification_loss': 0.92579221367836}
2025-01-17 11:19:23,445 [INFO] Label accuracies statistics:
2025-01-17 11:19:23,445 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.25, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 1.0, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 1.0, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.5, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 11:19:24,834 [INFO] [94] TRAIN  loss: 0.9274634821714618 acc: 0.9996314043494287
2025-01-17 11:19:24,834 [INFO] [94] TRAIN  loss dict: {'classification_loss': 0.9274634821714618}
2025-01-17 11:19:24,835 [INFO] [94] VALIDATION loss: 1.8888245059135265 VALIDATION acc: 0.8119122257053292
2025-01-17 11:19:24,835 [INFO] [94] VALIDATION loss dict: {'classification_loss': 1.8888245059135265}
2025-01-17 11:19:24,835 [INFO] 
2025-01-17 11:19:45,430 [INFO] Step[50/2713]: training loss : 0.9253841769695282 TRAIN  loss dict:  {'classification_loss': 0.9253841769695282}
2025-01-17 11:20:01,583 [INFO] Step[100/2713]: training loss : 0.9265701460838318 TRAIN  loss dict:  {'classification_loss': 0.9265701460838318}
2025-01-17 11:20:17,791 [INFO] Step[150/2713]: training loss : 0.9252439439296722 TRAIN  loss dict:  {'classification_loss': 0.9252439439296722}
2025-01-17 11:20:34,022 [INFO] Step[200/2713]: training loss : 0.9255801367759705 TRAIN  loss dict:  {'classification_loss': 0.9255801367759705}
2025-01-17 11:20:50,227 [INFO] Step[250/2713]: training loss : 0.9256283009052276 TRAIN  loss dict:  {'classification_loss': 0.9256283009052276}
2025-01-17 11:21:06,478 [INFO] Step[300/2713]: training loss : 0.9260240411758422 TRAIN  loss dict:  {'classification_loss': 0.9260240411758422}
2025-01-17 11:21:22,624 [INFO] Step[350/2713]: training loss : 0.9258514368534088 TRAIN  loss dict:  {'classification_loss': 0.9258514368534088}
2025-01-17 11:21:38,803 [INFO] Step[400/2713]: training loss : 0.9682852602005005 TRAIN  loss dict:  {'classification_loss': 0.9682852602005005}
2025-01-17 11:21:55,025 [INFO] Step[450/2713]: training loss : 0.9251585817337036 TRAIN  loss dict:  {'classification_loss': 0.9251585817337036}
2025-01-17 11:22:11,214 [INFO] Step[500/2713]: training loss : 0.9256597793102265 TRAIN  loss dict:  {'classification_loss': 0.9256597793102265}
2025-01-17 11:22:27,426 [INFO] Step[550/2713]: training loss : 0.9259669399261474 TRAIN  loss dict:  {'classification_loss': 0.9259669399261474}
2025-01-17 11:22:43,689 [INFO] Step[600/2713]: training loss : 0.9258615434169769 TRAIN  loss dict:  {'classification_loss': 0.9258615434169769}
2025-01-17 11:22:59,858 [INFO] Step[650/2713]: training loss : 0.9252973902225494 TRAIN  loss dict:  {'classification_loss': 0.9252973902225494}
2025-01-17 11:23:16,113 [INFO] Step[700/2713]: training loss : 0.9257548594474793 TRAIN  loss dict:  {'classification_loss': 0.9257548594474793}
2025-01-17 11:23:32,362 [INFO] Step[750/2713]: training loss : 0.9256712889671326 TRAIN  loss dict:  {'classification_loss': 0.9256712889671326}
2025-01-17 11:23:48,607 [INFO] Step[800/2713]: training loss : 0.9255214047431946 TRAIN  loss dict:  {'classification_loss': 0.9255214047431946}
2025-01-17 11:24:04,898 [INFO] Step[850/2713]: training loss : 0.9253119480609894 TRAIN  loss dict:  {'classification_loss': 0.9253119480609894}
2025-01-17 11:24:21,098 [INFO] Step[900/2713]: training loss : 0.9251644802093506 TRAIN  loss dict:  {'classification_loss': 0.9251644802093506}
2025-01-17 11:24:37,380 [INFO] Step[950/2713]: training loss : 0.9364538788795471 TRAIN  loss dict:  {'classification_loss': 0.9364538788795471}
2025-01-17 11:24:53,741 [INFO] Step[1000/2713]: training loss : 0.9265151298046113 TRAIN  loss dict:  {'classification_loss': 0.9265151298046113}
2025-01-17 11:25:10,157 [INFO] Step[1050/2713]: training loss : 0.9259115946292877 TRAIN  loss dict:  {'classification_loss': 0.9259115946292877}
2025-01-17 11:25:26,542 [INFO] Step[1100/2713]: training loss : 0.925531735420227 TRAIN  loss dict:  {'classification_loss': 0.925531735420227}
2025-01-17 11:25:43,027 [INFO] Step[1150/2713]: training loss : 0.925351744890213 TRAIN  loss dict:  {'classification_loss': 0.925351744890213}
2025-01-17 11:25:59,426 [INFO] Step[1200/2713]: training loss : 0.9253229403495788 TRAIN  loss dict:  {'classification_loss': 0.9253229403495788}
2025-01-17 11:26:15,774 [INFO] Step[1250/2713]: training loss : 0.9606866693496704 TRAIN  loss dict:  {'classification_loss': 0.9606866693496704}
2025-01-17 11:26:32,125 [INFO] Step[1300/2713]: training loss : 0.9262447488307953 TRAIN  loss dict:  {'classification_loss': 0.9262447488307953}
2025-01-17 11:26:48,562 [INFO] Step[1350/2713]: training loss : 0.926548912525177 TRAIN  loss dict:  {'classification_loss': 0.926548912525177}
2025-01-17 11:27:04,925 [INFO] Step[1400/2713]: training loss : 0.92735759973526 TRAIN  loss dict:  {'classification_loss': 0.92735759973526}
2025-01-17 11:27:21,335 [INFO] Step[1450/2713]: training loss : 0.9280682098865509 TRAIN  loss dict:  {'classification_loss': 0.9280682098865509}
2025-01-17 11:27:37,766 [INFO] Step[1500/2713]: training loss : 0.9255167484283447 TRAIN  loss dict:  {'classification_loss': 0.9255167484283447}
2025-01-17 11:27:54,211 [INFO] Step[1550/2713]: training loss : 0.925485473871231 TRAIN  loss dict:  {'classification_loss': 0.925485473871231}
2025-01-17 11:28:10,654 [INFO] Step[1600/2713]: training loss : 0.9253562092781067 TRAIN  loss dict:  {'classification_loss': 0.9253562092781067}
2025-01-17 11:28:26,988 [INFO] Step[1650/2713]: training loss : 0.9255795025825501 TRAIN  loss dict:  {'classification_loss': 0.9255795025825501}
2025-01-17 11:28:43,350 [INFO] Step[1700/2713]: training loss : 0.9256762552261353 TRAIN  loss dict:  {'classification_loss': 0.9256762552261353}
2025-01-17 11:28:59,809 [INFO] Step[1750/2713]: training loss : 0.9256744575500488 TRAIN  loss dict:  {'classification_loss': 0.9256744575500488}
2025-01-17 11:29:16,132 [INFO] Step[1800/2713]: training loss : 0.925647029876709 TRAIN  loss dict:  {'classification_loss': 0.925647029876709}
2025-01-17 11:29:32,528 [INFO] Step[1850/2713]: training loss : 0.9256388533115387 TRAIN  loss dict:  {'classification_loss': 0.9256388533115387}
2025-01-17 11:29:48,939 [INFO] Step[1900/2713]: training loss : 0.9256817507743835 TRAIN  loss dict:  {'classification_loss': 0.9256817507743835}
2025-01-17 11:30:05,358 [INFO] Step[1950/2713]: training loss : 0.9263259315490723 TRAIN  loss dict:  {'classification_loss': 0.9263259315490723}
2025-01-17 11:30:21,729 [INFO] Step[2000/2713]: training loss : 0.925584945678711 TRAIN  loss dict:  {'classification_loss': 0.925584945678711}
2025-01-17 11:30:38,174 [INFO] Step[2050/2713]: training loss : 0.9267425572872162 TRAIN  loss dict:  {'classification_loss': 0.9267425572872162}
2025-01-17 11:30:54,579 [INFO] Step[2100/2713]: training loss : 0.9274734020233154 TRAIN  loss dict:  {'classification_loss': 0.9274734020233154}
2025-01-17 11:31:11,010 [INFO] Step[2150/2713]: training loss : 0.9259071981906891 TRAIN  loss dict:  {'classification_loss': 0.9259071981906891}
2025-01-17 11:31:27,384 [INFO] Step[2200/2713]: training loss : 0.936858434677124 TRAIN  loss dict:  {'classification_loss': 0.936858434677124}
2025-01-17 11:31:43,746 [INFO] Step[2250/2713]: training loss : 0.9256065559387207 TRAIN  loss dict:  {'classification_loss': 0.9256065559387207}
2025-01-17 11:32:00,132 [INFO] Step[2300/2713]: training loss : 0.9254549789428711 TRAIN  loss dict:  {'classification_loss': 0.9254549789428711}
2025-01-17 11:32:16,535 [INFO] Step[2350/2713]: training loss : 0.9261789464950562 TRAIN  loss dict:  {'classification_loss': 0.9261789464950562}
2025-01-17 11:32:32,918 [INFO] Step[2400/2713]: training loss : 0.9259811556339264 TRAIN  loss dict:  {'classification_loss': 0.9259811556339264}
2025-01-17 11:32:49,268 [INFO] Step[2450/2713]: training loss : 0.925485200881958 TRAIN  loss dict:  {'classification_loss': 0.925485200881958}
2025-01-17 11:33:05,612 [INFO] Step[2500/2713]: training loss : 0.925488394498825 TRAIN  loss dict:  {'classification_loss': 0.925488394498825}
2025-01-17 11:33:21,985 [INFO] Step[2550/2713]: training loss : 0.9253949368000031 TRAIN  loss dict:  {'classification_loss': 0.9253949368000031}
2025-01-17 11:33:38,420 [INFO] Step[2600/2713]: training loss : 0.9261465334892273 TRAIN  loss dict:  {'classification_loss': 0.9261465334892273}
2025-01-17 11:33:54,824 [INFO] Step[2650/2713]: training loss : 0.9255381608009339 TRAIN  loss dict:  {'classification_loss': 0.9255381608009339}
2025-01-17 11:34:11,256 [INFO] Step[2700/2713]: training loss : 0.9251071429252624 TRAIN  loss dict:  {'classification_loss': 0.9251071429252624}
2025-01-17 11:35:30,126 [INFO] Label accuracies statistics:
2025-01-17 11:35:30,126 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.5, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.75, 97: 0.0, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.75, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 0.5, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.25, 203: 0.5, 204: 1.0, 205: 1.0, 206: 0.25, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 1.0, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.5, 262: 0.75, 263: 1.0, 264: 0.5, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 11:35:30,129 [INFO] [95] TRAIN  loss: 0.9276468268277414 acc: 0.9995085391325715
2025-01-17 11:35:30,129 [INFO] [95] TRAIN  loss dict: {'classification_loss': 0.9276468268277414}
2025-01-17 11:35:30,129 [INFO] [95] VALIDATION loss: 1.909813777060437 VALIDATION acc: 0.7987460815047022
2025-01-17 11:35:30,129 [INFO] [95] VALIDATION loss dict: {'classification_loss': 1.909813777060437}
2025-01-17 11:35:30,129 [INFO] 
2025-01-17 11:35:51,411 [INFO] Step[50/2713]: training loss : 0.9254902994632721 TRAIN  loss dict:  {'classification_loss': 0.9254902994632721}
2025-01-17 11:36:07,860 [INFO] Step[100/2713]: training loss : 0.9255174720287322 TRAIN  loss dict:  {'classification_loss': 0.9255174720287322}
2025-01-17 11:36:24,337 [INFO] Step[150/2713]: training loss : 0.9256463968753814 TRAIN  loss dict:  {'classification_loss': 0.9256463968753814}
2025-01-17 11:36:40,780 [INFO] Step[200/2713]: training loss : 0.925661689043045 TRAIN  loss dict:  {'classification_loss': 0.925661689043045}
2025-01-17 11:36:57,243 [INFO] Step[250/2713]: training loss : 0.9263234615325928 TRAIN  loss dict:  {'classification_loss': 0.9263234615325928}
2025-01-17 11:37:13,751 [INFO] Step[300/2713]: training loss : 0.9254980385303497 TRAIN  loss dict:  {'classification_loss': 0.9254980385303497}
2025-01-17 11:37:30,223 [INFO] Step[350/2713]: training loss : 0.9258328330516815 TRAIN  loss dict:  {'classification_loss': 0.9258328330516815}
2025-01-17 11:37:46,677 [INFO] Step[400/2713]: training loss : 0.9255231857299805 TRAIN  loss dict:  {'classification_loss': 0.9255231857299805}
2025-01-17 11:38:03,166 [INFO] Step[450/2713]: training loss : 0.9302825832366943 TRAIN  loss dict:  {'classification_loss': 0.9302825832366943}
2025-01-17 11:38:19,614 [INFO] Step[500/2713]: training loss : 0.9336436712741851 TRAIN  loss dict:  {'classification_loss': 0.9336436712741851}
2025-01-17 11:38:36,187 [INFO] Step[550/2713]: training loss : 0.925332772731781 TRAIN  loss dict:  {'classification_loss': 0.925332772731781}
2025-01-17 11:38:52,646 [INFO] Step[600/2713]: training loss : 0.9251812016963958 TRAIN  loss dict:  {'classification_loss': 0.9251812016963958}
2025-01-17 11:39:09,130 [INFO] Step[650/2713]: training loss : 0.9257185065746307 TRAIN  loss dict:  {'classification_loss': 0.9257185065746307}
2025-01-17 11:39:25,537 [INFO] Step[700/2713]: training loss : 0.9253722977638245 TRAIN  loss dict:  {'classification_loss': 0.9253722977638245}
2025-01-17 11:39:41,926 [INFO] Step[750/2713]: training loss : 0.9255339455604553 TRAIN  loss dict:  {'classification_loss': 0.9255339455604553}
2025-01-17 11:39:58,342 [INFO] Step[800/2713]: training loss : 0.925275399684906 TRAIN  loss dict:  {'classification_loss': 0.925275399684906}
2025-01-17 11:40:14,789 [INFO] Step[850/2713]: training loss : 0.9285511028766632 TRAIN  loss dict:  {'classification_loss': 0.9285511028766632}
2025-01-17 11:40:31,282 [INFO] Step[900/2713]: training loss : 0.9251986336708069 TRAIN  loss dict:  {'classification_loss': 0.9251986336708069}
2025-01-17 11:40:47,774 [INFO] Step[950/2713]: training loss : 0.9268651008605957 TRAIN  loss dict:  {'classification_loss': 0.9268651008605957}
2025-01-17 11:41:04,222 [INFO] Step[1000/2713]: training loss : 0.9254516804218292 TRAIN  loss dict:  {'classification_loss': 0.9254516804218292}
2025-01-17 11:41:20,684 [INFO] Step[1050/2713]: training loss : 0.9256317353248597 TRAIN  loss dict:  {'classification_loss': 0.9256317353248597}
2025-01-17 11:41:37,066 [INFO] Step[1100/2713]: training loss : 0.9255680584907532 TRAIN  loss dict:  {'classification_loss': 0.9255680584907532}
2025-01-17 11:41:53,509 [INFO] Step[1150/2713]: training loss : 0.9257361543178558 TRAIN  loss dict:  {'classification_loss': 0.9257361543178558}
2025-01-17 11:42:09,981 [INFO] Step[1200/2713]: training loss : 0.9256874668598175 TRAIN  loss dict:  {'classification_loss': 0.9256874668598175}
2025-01-17 11:42:26,476 [INFO] Step[1250/2713]: training loss : 0.9257654201984405 TRAIN  loss dict:  {'classification_loss': 0.9257654201984405}
2025-01-17 11:42:42,900 [INFO] Step[1300/2713]: training loss : 0.9259511399269104 TRAIN  loss dict:  {'classification_loss': 0.9259511399269104}
2025-01-17 11:42:59,327 [INFO] Step[1350/2713]: training loss : 0.9282559597492218 TRAIN  loss dict:  {'classification_loss': 0.9282559597492218}
2025-01-17 11:43:15,799 [INFO] Step[1400/2713]: training loss : 0.9250834000110626 TRAIN  loss dict:  {'classification_loss': 0.9250834000110626}
2025-01-17 11:43:32,180 [INFO] Step[1450/2713]: training loss : 0.9252908062934876 TRAIN  loss dict:  {'classification_loss': 0.9252908062934876}
2025-01-17 11:43:48,637 [INFO] Step[1500/2713]: training loss : 0.9256092536449433 TRAIN  loss dict:  {'classification_loss': 0.9256092536449433}
2025-01-17 11:44:05,131 [INFO] Step[1550/2713]: training loss : 0.9254056775569915 TRAIN  loss dict:  {'classification_loss': 0.9254056775569915}
2025-01-17 11:44:21,555 [INFO] Step[1600/2713]: training loss : 0.9253835332393646 TRAIN  loss dict:  {'classification_loss': 0.9253835332393646}
2025-01-17 11:44:37,926 [INFO] Step[1650/2713]: training loss : 0.925615473985672 TRAIN  loss dict:  {'classification_loss': 0.925615473985672}
2025-01-17 11:44:54,367 [INFO] Step[1700/2713]: training loss : 0.9254505729675293 TRAIN  loss dict:  {'classification_loss': 0.9254505729675293}
2025-01-17 11:45:10,788 [INFO] Step[1750/2713]: training loss : 0.9251405274868012 TRAIN  loss dict:  {'classification_loss': 0.9251405274868012}
2025-01-17 11:45:27,218 [INFO] Step[1800/2713]: training loss : 0.9251777613162995 TRAIN  loss dict:  {'classification_loss': 0.9251777613162995}
2025-01-17 11:45:43,538 [INFO] Step[1850/2713]: training loss : 0.9251688945293427 TRAIN  loss dict:  {'classification_loss': 0.9251688945293427}
2025-01-17 11:45:59,775 [INFO] Step[1900/2713]: training loss : 0.9256117665767669 TRAIN  loss dict:  {'classification_loss': 0.9256117665767669}
2025-01-17 11:46:16,090 [INFO] Step[1950/2713]: training loss : 0.9259396004676819 TRAIN  loss dict:  {'classification_loss': 0.9259396004676819}
2025-01-17 11:46:32,285 [INFO] Step[2000/2713]: training loss : 0.9255047023296357 TRAIN  loss dict:  {'classification_loss': 0.9255047023296357}
2025-01-17 11:46:48,615 [INFO] Step[2050/2713]: training loss : 0.9253156101703643 TRAIN  loss dict:  {'classification_loss': 0.9253156101703643}
2025-01-17 11:47:04,880 [INFO] Step[2100/2713]: training loss : 0.9361105298995972 TRAIN  loss dict:  {'classification_loss': 0.9361105298995972}
2025-01-17 11:47:21,237 [INFO] Step[2150/2713]: training loss : 0.9257090604305267 TRAIN  loss dict:  {'classification_loss': 0.9257090604305267}
2025-01-17 11:47:37,509 [INFO] Step[2200/2713]: training loss : 0.9257630836963654 TRAIN  loss dict:  {'classification_loss': 0.9257630836963654}
2025-01-17 11:47:53,713 [INFO] Step[2250/2713]: training loss : 0.9252838337421417 TRAIN  loss dict:  {'classification_loss': 0.9252838337421417}
2025-01-17 11:48:09,931 [INFO] Step[2300/2713]: training loss : 0.926593906879425 TRAIN  loss dict:  {'classification_loss': 0.926593906879425}
2025-01-17 11:48:26,258 [INFO] Step[2350/2713]: training loss : 0.9254424023628235 TRAIN  loss dict:  {'classification_loss': 0.9254424023628235}
2025-01-17 11:48:42,478 [INFO] Step[2400/2713]: training loss : 0.9252637481689453 TRAIN  loss dict:  {'classification_loss': 0.9252637481689453}
2025-01-17 11:48:58,729 [INFO] Step[2450/2713]: training loss : 0.9272458481788636 TRAIN  loss dict:  {'classification_loss': 0.9272458481788636}
2025-01-17 11:49:14,927 [INFO] Step[2500/2713]: training loss : 0.9283813524246216 TRAIN  loss dict:  {'classification_loss': 0.9283813524246216}
2025-01-17 11:49:31,165 [INFO] Step[2550/2713]: training loss : 0.9256670939922332 TRAIN  loss dict:  {'classification_loss': 0.9256670939922332}
2025-01-17 11:49:47,386 [INFO] Step[2600/2713]: training loss : 0.9253168714046478 TRAIN  loss dict:  {'classification_loss': 0.9253168714046478}
2025-01-17 11:50:03,700 [INFO] Step[2650/2713]: training loss : 0.9255296432971954 TRAIN  loss dict:  {'classification_loss': 0.9255296432971954}
2025-01-17 11:50:19,863 [INFO] Step[2700/2713]: training loss : 0.925327742099762 TRAIN  loss dict:  {'classification_loss': 0.925327742099762}
2025-01-17 11:51:38,579 [INFO] Label accuracies statistics:
2025-01-17 11:51:38,579 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.25, 97: 0.0, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 0.25, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.5, 205: 1.0, 206: 1.0, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.5, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.5, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 1.0, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 11:51:38,581 [INFO] [96] TRAIN  loss: 0.9261788421787989 acc: 0.9998771347831429
2025-01-17 11:51:38,581 [INFO] [96] TRAIN  loss dict: {'classification_loss': 0.9261788421787989}
2025-01-17 11:51:38,581 [INFO] [96] VALIDATION loss: 1.8914106143148322 VALIDATION acc: 0.806269592476489
2025-01-17 11:51:38,581 [INFO] [96] VALIDATION loss dict: {'classification_loss': 1.8914106143148322}
2025-01-17 11:51:38,582 [INFO] 
2025-01-17 11:51:59,725 [INFO] Step[50/2713]: training loss : 0.9256652188301087 TRAIN  loss dict:  {'classification_loss': 0.9256652188301087}
2025-01-17 11:52:15,908 [INFO] Step[100/2713]: training loss : 0.9257407736778259 TRAIN  loss dict:  {'classification_loss': 0.9257407736778259}
2025-01-17 11:52:32,077 [INFO] Step[150/2713]: training loss : 0.9257357621192932 TRAIN  loss dict:  {'classification_loss': 0.9257357621192932}
2025-01-17 11:52:48,284 [INFO] Step[200/2713]: training loss : 0.9274205148220063 TRAIN  loss dict:  {'classification_loss': 0.9274205148220063}
2025-01-17 11:53:04,656 [INFO] Step[250/2713]: training loss : 0.9259096467494965 TRAIN  loss dict:  {'classification_loss': 0.9259096467494965}
2025-01-17 11:53:20,944 [INFO] Step[300/2713]: training loss : 0.9252631008625031 TRAIN  loss dict:  {'classification_loss': 0.9252631008625031}
2025-01-17 11:53:37,193 [INFO] Step[350/2713]: training loss : 0.9375932312011719 TRAIN  loss dict:  {'classification_loss': 0.9375932312011719}
2025-01-17 11:53:53,429 [INFO] Step[400/2713]: training loss : 0.9254669368267059 TRAIN  loss dict:  {'classification_loss': 0.9254669368267059}
2025-01-17 11:54:09,735 [INFO] Step[450/2713]: training loss : 0.925712571144104 TRAIN  loss dict:  {'classification_loss': 0.925712571144104}
2025-01-17 11:54:25,995 [INFO] Step[500/2713]: training loss : 0.929863338470459 TRAIN  loss dict:  {'classification_loss': 0.929863338470459}
2025-01-17 11:54:42,254 [INFO] Step[550/2713]: training loss : 0.9267975962162018 TRAIN  loss dict:  {'classification_loss': 0.9267975962162018}
2025-01-17 11:54:58,498 [INFO] Step[600/2713]: training loss : 0.9255530464649201 TRAIN  loss dict:  {'classification_loss': 0.9255530464649201}
2025-01-17 11:55:14,800 [INFO] Step[650/2713]: training loss : 0.9275314128398895 TRAIN  loss dict:  {'classification_loss': 0.9275314128398895}
2025-01-17 11:55:31,094 [INFO] Step[700/2713]: training loss : 0.9252384459972381 TRAIN  loss dict:  {'classification_loss': 0.9252384459972381}
2025-01-17 11:55:47,457 [INFO] Step[750/2713]: training loss : 0.9259281778335571 TRAIN  loss dict:  {'classification_loss': 0.9259281778335571}
2025-01-17 11:56:03,627 [INFO] Step[800/2713]: training loss : 0.9251427567005157 TRAIN  loss dict:  {'classification_loss': 0.9251427567005157}
2025-01-17 11:56:20,075 [INFO] Step[850/2713]: training loss : 0.9255383586883545 TRAIN  loss dict:  {'classification_loss': 0.9255383586883545}
2025-01-17 11:56:36,444 [INFO] Step[900/2713]: training loss : 0.9665949249267578 TRAIN  loss dict:  {'classification_loss': 0.9665949249267578}
2025-01-17 11:56:53,038 [INFO] Step[950/2713]: training loss : 0.9273930442333221 TRAIN  loss dict:  {'classification_loss': 0.9273930442333221}
2025-01-17 11:57:08,371 [INFO] Step[1000/2713]: training loss : 0.9255128717422485 TRAIN  loss dict:  {'classification_loss': 0.9255128717422485}
2025-01-17 11:57:23,324 [INFO] Step[1050/2713]: training loss : 0.9259868049621582 TRAIN  loss dict:  {'classification_loss': 0.9259868049621582}
2025-01-17 11:57:38,268 [INFO] Step[1100/2713]: training loss : 0.9254600048065186 TRAIN  loss dict:  {'classification_loss': 0.9254600048065186}
2025-01-17 11:57:53,406 [INFO] Step[1150/2713]: training loss : 0.9809942901134491 TRAIN  loss dict:  {'classification_loss': 0.9809942901134491}
2025-01-17 11:58:08,544 [INFO] Step[1200/2713]: training loss : 0.9256388223171235 TRAIN  loss dict:  {'classification_loss': 0.9256388223171235}
2025-01-17 11:58:24,206 [INFO] Step[1250/2713]: training loss : 0.9256331241130829 TRAIN  loss dict:  {'classification_loss': 0.9256331241130829}
2025-01-17 11:58:39,209 [INFO] Step[1300/2713]: training loss : 0.9264543700218201 TRAIN  loss dict:  {'classification_loss': 0.9264543700218201}
2025-01-17 11:58:54,095 [INFO] Step[1350/2713]: training loss : 0.9258903574943542 TRAIN  loss dict:  {'classification_loss': 0.9258903574943542}
2025-01-17 11:59:09,433 [INFO] Step[1400/2713]: training loss : 0.9255692946910858 TRAIN  loss dict:  {'classification_loss': 0.9255692946910858}
2025-01-17 11:59:24,710 [INFO] Step[1450/2713]: training loss : 0.9259386050701142 TRAIN  loss dict:  {'classification_loss': 0.9259386050701142}
2025-01-17 11:59:39,942 [INFO] Step[1500/2713]: training loss : 0.9256252241134644 TRAIN  loss dict:  {'classification_loss': 0.9256252241134644}
2025-01-17 11:59:55,282 [INFO] Step[1550/2713]: training loss : 0.9260323631763459 TRAIN  loss dict:  {'classification_loss': 0.9260323631763459}
2025-01-17 12:00:10,605 [INFO] Step[1600/2713]: training loss : 0.9377380204200745 TRAIN  loss dict:  {'classification_loss': 0.9377380204200745}
2025-01-17 12:00:25,836 [INFO] Step[1650/2713]: training loss : 0.9264747500419617 TRAIN  loss dict:  {'classification_loss': 0.9264747500419617}
2025-01-17 12:00:41,482 [INFO] Step[1700/2713]: training loss : 0.9261028122901916 TRAIN  loss dict:  {'classification_loss': 0.9261028122901916}
2025-01-17 12:00:56,827 [INFO] Step[1750/2713]: training loss : 0.9255439686775208 TRAIN  loss dict:  {'classification_loss': 0.9255439686775208}
2025-01-17 12:01:12,002 [INFO] Step[1800/2713]: training loss : 0.9252543294429779 TRAIN  loss dict:  {'classification_loss': 0.9252543294429779}
2025-01-17 12:01:27,629 [INFO] Step[1850/2713]: training loss : 0.9256556081771851 TRAIN  loss dict:  {'classification_loss': 0.9256556081771851}
2025-01-17 12:01:42,719 [INFO] Step[1900/2713]: training loss : 0.9250123035907746 TRAIN  loss dict:  {'classification_loss': 0.9250123035907746}
2025-01-17 12:01:57,926 [INFO] Step[1950/2713]: training loss : 0.9250696194171906 TRAIN  loss dict:  {'classification_loss': 0.9250696194171906}
2025-01-17 12:02:13,180 [INFO] Step[2000/2713]: training loss : 0.9258497631549836 TRAIN  loss dict:  {'classification_loss': 0.9258497631549836}
2025-01-17 12:02:27,849 [INFO] Step[2050/2713]: training loss : 0.9249045991897583 TRAIN  loss dict:  {'classification_loss': 0.9249045991897583}
2025-01-17 12:02:43,175 [INFO] Step[2100/2713]: training loss : 0.925492238998413 TRAIN  loss dict:  {'classification_loss': 0.925492238998413}
2025-01-17 12:02:58,269 [INFO] Step[2150/2713]: training loss : 0.92534747838974 TRAIN  loss dict:  {'classification_loss': 0.92534747838974}
2025-01-17 12:03:13,333 [INFO] Step[2200/2713]: training loss : 0.925291907787323 TRAIN  loss dict:  {'classification_loss': 0.925291907787323}
2025-01-17 12:03:28,375 [INFO] Step[2250/2713]: training loss : 0.9257492768764496 TRAIN  loss dict:  {'classification_loss': 0.9257492768764496}
2025-01-17 12:03:43,621 [INFO] Step[2300/2713]: training loss : 0.9265369665622711 TRAIN  loss dict:  {'classification_loss': 0.9265369665622711}
2025-01-17 12:03:58,874 [INFO] Step[2350/2713]: training loss : 0.9254059219360351 TRAIN  loss dict:  {'classification_loss': 0.9254059219360351}
2025-01-17 12:04:14,214 [INFO] Step[2400/2713]: training loss : 0.9386879086494446 TRAIN  loss dict:  {'classification_loss': 0.9386879086494446}
2025-01-17 12:04:29,715 [INFO] Step[2450/2713]: training loss : 0.9389515626430511 TRAIN  loss dict:  {'classification_loss': 0.9389515626430511}
2025-01-17 12:04:45,334 [INFO] Step[2500/2713]: training loss : 0.9259022879600525 TRAIN  loss dict:  {'classification_loss': 0.9259022879600525}
2025-01-17 12:05:01,002 [INFO] Step[2550/2713]: training loss : 0.925758593082428 TRAIN  loss dict:  {'classification_loss': 0.925758593082428}
2025-01-17 12:05:15,947 [INFO] Step[2600/2713]: training loss : 0.9254270851612091 TRAIN  loss dict:  {'classification_loss': 0.9254270851612091}
2025-01-17 12:05:31,048 [INFO] Step[2650/2713]: training loss : 0.9255301642417908 TRAIN  loss dict:  {'classification_loss': 0.9255301642417908}
2025-01-17 12:05:46,555 [INFO] Step[2700/2713]: training loss : 0.9260487091541291 TRAIN  loss dict:  {'classification_loss': 0.9260487091541291}
2025-01-17 12:07:43,754 [INFO] Label accuracies statistics:
2025-01-17 12:07:43,754 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.0, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 0.75, 141: 1.0, 142: 0.75, 143: 0.75, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 0.25, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 1.0, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-17 12:07:47,759 [INFO] [97] TRAIN  loss: 0.9285473970526749 acc: 0.9991399434820002
2025-01-17 12:07:47,759 [INFO] [97] TRAIN  loss dict: {'classification_loss': 0.9285473970526749}
2025-01-17 12:07:47,759 [INFO] [97] VALIDATION loss: 1.8456065872109921 VALIDATION acc: 0.8188087774294671
2025-01-17 12:07:47,759 [INFO] [97] VALIDATION loss dict: {'classification_loss': 1.8456065872109921}
2025-01-17 12:07:47,759 [INFO] 
2025-01-17 12:08:11,818 [INFO] Step[50/2713]: training loss : 0.9255269968509674 TRAIN  loss dict:  {'classification_loss': 0.9255269968509674}
2025-01-17 12:08:27,010 [INFO] Step[100/2713]: training loss : 0.9257587313652038 TRAIN  loss dict:  {'classification_loss': 0.9257587313652038}
2025-01-17 12:08:41,974 [INFO] Step[150/2713]: training loss : 0.9252634561061859 TRAIN  loss dict:  {'classification_loss': 0.9252634561061859}
2025-01-17 12:08:57,453 [INFO] Step[200/2713]: training loss : 0.9273381328582764 TRAIN  loss dict:  {'classification_loss': 0.9273381328582764}
2025-01-17 12:09:12,798 [INFO] Step[250/2713]: training loss : 0.9249111676216125 TRAIN  loss dict:  {'classification_loss': 0.9249111676216125}
2025-01-17 12:09:27,746 [INFO] Step[300/2713]: training loss : 0.9254639863967895 TRAIN  loss dict:  {'classification_loss': 0.9254639863967895}
2025-01-17 12:09:42,923 [INFO] Step[350/2713]: training loss : 0.9257531118392944 TRAIN  loss dict:  {'classification_loss': 0.9257531118392944}
2025-01-17 12:09:58,129 [INFO] Step[400/2713]: training loss : 0.9251812696456909 TRAIN  loss dict:  {'classification_loss': 0.9251812696456909}
2025-01-17 12:10:12,930 [INFO] Step[450/2713]: training loss : 0.9255670475959777 TRAIN  loss dict:  {'classification_loss': 0.9255670475959777}
2025-01-17 12:10:28,367 [INFO] Step[500/2713]: training loss : 0.925339629650116 TRAIN  loss dict:  {'classification_loss': 0.925339629650116}
2025-01-17 12:10:43,269 [INFO] Step[550/2713]: training loss : 0.9254821240901947 TRAIN  loss dict:  {'classification_loss': 0.9254821240901947}
2025-01-17 12:10:58,178 [INFO] Step[600/2713]: training loss : 0.9251969170570373 TRAIN  loss dict:  {'classification_loss': 0.9251969170570373}
2025-01-17 12:11:13,737 [INFO] Step[650/2713]: training loss : 0.9254264879226685 TRAIN  loss dict:  {'classification_loss': 0.9254264879226685}
2025-01-17 12:11:29,285 [INFO] Step[700/2713]: training loss : 0.9256618201732636 TRAIN  loss dict:  {'classification_loss': 0.9256618201732636}
2025-01-17 12:11:44,297 [INFO] Step[750/2713]: training loss : 0.9262350106239319 TRAIN  loss dict:  {'classification_loss': 0.9262350106239319}
2025-01-17 12:12:00,376 [INFO] Step[800/2713]: training loss : 0.9260000348091125 TRAIN  loss dict:  {'classification_loss': 0.9260000348091125}
2025-01-17 12:12:16,579 [INFO] Step[850/2713]: training loss : 0.9254001307487488 TRAIN  loss dict:  {'classification_loss': 0.9254001307487488}
2025-01-17 12:12:32,652 [INFO] Step[900/2713]: training loss : 0.9260329580307007 TRAIN  loss dict:  {'classification_loss': 0.9260329580307007}
2025-01-17 12:12:48,801 [INFO] Step[950/2713]: training loss : 0.9259113585948944 TRAIN  loss dict:  {'classification_loss': 0.9259113585948944}
2025-01-17 12:13:04,965 [INFO] Step[1000/2713]: training loss : 0.9257491242885589 TRAIN  loss dict:  {'classification_loss': 0.9257491242885589}
2025-01-17 12:13:21,048 [INFO] Step[1050/2713]: training loss : 0.9254534792900085 TRAIN  loss dict:  {'classification_loss': 0.9254534792900085}
2025-01-17 12:13:37,148 [INFO] Step[1100/2713]: training loss : 0.9256053137779235 TRAIN  loss dict:  {'classification_loss': 0.9256053137779235}
2025-01-17 12:13:53,282 [INFO] Step[1150/2713]: training loss : 0.9253628468513488 TRAIN  loss dict:  {'classification_loss': 0.9253628468513488}
2025-01-17 12:14:09,396 [INFO] Step[1200/2713]: training loss : 0.9272263944149017 TRAIN  loss dict:  {'classification_loss': 0.9272263944149017}
2025-01-17 12:14:25,472 [INFO] Step[1250/2713]: training loss : 0.9258319997787475 TRAIN  loss dict:  {'classification_loss': 0.9258319997787475}
2025-01-17 12:14:41,483 [INFO] Step[1300/2713]: training loss : 0.9256245052814484 TRAIN  loss dict:  {'classification_loss': 0.9256245052814484}
2025-01-17 12:14:57,546 [INFO] Step[1350/2713]: training loss : 0.9260512280464173 TRAIN  loss dict:  {'classification_loss': 0.9260512280464173}
2025-01-17 12:15:13,557 [INFO] Step[1400/2713]: training loss : 0.9258190989494324 TRAIN  loss dict:  {'classification_loss': 0.9258190989494324}
2025-01-17 12:15:29,600 [INFO] Step[1450/2713]: training loss : 0.9256079781055451 TRAIN  loss dict:  {'classification_loss': 0.9256079781055451}
2025-01-17 12:15:45,704 [INFO] Step[1500/2713]: training loss : 0.925440366268158 TRAIN  loss dict:  {'classification_loss': 0.925440366268158}
2025-01-17 12:16:01,807 [INFO] Step[1550/2713]: training loss : 0.9276188468933105 TRAIN  loss dict:  {'classification_loss': 0.9276188468933105}
2025-01-17 12:16:17,852 [INFO] Step[1600/2713]: training loss : 0.9252215111255646 TRAIN  loss dict:  {'classification_loss': 0.9252215111255646}
2025-01-17 12:16:33,976 [INFO] Step[1650/2713]: training loss : 0.9253813695907592 TRAIN  loss dict:  {'classification_loss': 0.9253813695907592}
2025-01-17 12:16:50,020 [INFO] Step[1700/2713]: training loss : 0.9252278256416321 TRAIN  loss dict:  {'classification_loss': 0.9252278256416321}
2025-01-17 12:17:06,143 [INFO] Step[1750/2713]: training loss : 0.9258268713951111 TRAIN  loss dict:  {'classification_loss': 0.9258268713951111}
2025-01-17 12:17:22,121 [INFO] Step[1800/2713]: training loss : 0.9250317800045014 TRAIN  loss dict:  {'classification_loss': 0.9250317800045014}
2025-01-17 12:17:38,214 [INFO] Step[1850/2713]: training loss : 0.9253248143196106 TRAIN  loss dict:  {'classification_loss': 0.9253248143196106}
2025-01-17 12:17:54,345 [INFO] Step[1900/2713]: training loss : 0.9252722859382629 TRAIN  loss dict:  {'classification_loss': 0.9252722859382629}
2025-01-17 12:18:10,684 [INFO] Step[1950/2713]: training loss : 0.9255708432197571 TRAIN  loss dict:  {'classification_loss': 0.9255708432197571}
2025-01-17 12:18:26,175 [INFO] Step[2000/2713]: training loss : 0.9261945724487305 TRAIN  loss dict:  {'classification_loss': 0.9261945724487305}
2025-01-17 12:18:41,369 [INFO] Step[2050/2713]: training loss : 0.9252957773208618 TRAIN  loss dict:  {'classification_loss': 0.9252957773208618}
2025-01-17 12:18:56,755 [INFO] Step[2100/2713]: training loss : 0.930313550233841 TRAIN  loss dict:  {'classification_loss': 0.930313550233841}
2025-01-17 12:19:11,828 [INFO] Step[2150/2713]: training loss : 0.9253920006752014 TRAIN  loss dict:  {'classification_loss': 0.9253920006752014}
2025-01-17 12:19:26,915 [INFO] Step[2200/2713]: training loss : 0.9254793751239777 TRAIN  loss dict:  {'classification_loss': 0.9254793751239777}
2025-01-17 12:19:42,145 [INFO] Step[2250/2713]: training loss : 0.9250299918651581 TRAIN  loss dict:  {'classification_loss': 0.9250299918651581}
2025-01-17 12:19:57,234 [INFO] Step[2300/2713]: training loss : 0.9277421557903289 TRAIN  loss dict:  {'classification_loss': 0.9277421557903289}
2025-01-17 12:20:12,358 [INFO] Step[2350/2713]: training loss : 0.9252787160873414 TRAIN  loss dict:  {'classification_loss': 0.9252787160873414}
2025-01-17 12:20:27,411 [INFO] Step[2400/2713]: training loss : 0.9261966490745545 TRAIN  loss dict:  {'classification_loss': 0.9261966490745545}
2025-01-17 12:20:42,760 [INFO] Step[2450/2713]: training loss : 0.9259310042858124 TRAIN  loss dict:  {'classification_loss': 0.9259310042858124}
2025-01-17 12:20:57,873 [INFO] Step[2500/2713]: training loss : 0.9247955691814422 TRAIN  loss dict:  {'classification_loss': 0.9247955691814422}
2025-01-17 12:21:13,047 [INFO] Step[2550/2713]: training loss : 0.9255368888378144 TRAIN  loss dict:  {'classification_loss': 0.9255368888378144}
2025-01-17 12:21:27,839 [INFO] Step[2600/2713]: training loss : 0.925332465171814 TRAIN  loss dict:  {'classification_loss': 0.925332465171814}
2025-01-17 12:21:43,001 [INFO] Step[2650/2713]: training loss : 0.9254025793075562 TRAIN  loss dict:  {'classification_loss': 0.9254025793075562}
2025-01-17 12:21:58,640 [INFO] Step[2700/2713]: training loss : 0.9250469326972961 TRAIN  loss dict:  {'classification_loss': 0.9250469326972961}
2025-01-17 12:23:56,734 [INFO] Label accuracies statistics:
2025-01-17 12:23:56,735 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.5, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.25, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.0, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 1.0, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 0.75, 258: 0.5, 259: 1.0, 260: 1.0, 261: 0.25, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 1.0, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-17 12:23:56,739 [INFO] [98] TRAIN  loss: 0.925771026012865 acc: 1.0
2025-01-17 12:23:56,739 [INFO] [98] TRAIN  loss dict: {'classification_loss': 0.925771026012865}
2025-01-17 12:23:56,739 [INFO] [98] VALIDATION loss: 1.8887726814675152 VALIDATION acc: 0.8100313479623824
2025-01-17 12:23:56,740 [INFO] [98] VALIDATION loss dict: {'classification_loss': 1.8887726814675152}
2025-01-17 12:23:56,740 [INFO] 
2025-01-17 12:24:21,787 [INFO] Step[50/2713]: training loss : 0.9252768325805664 TRAIN  loss dict:  {'classification_loss': 0.9252768325805664}
2025-01-17 12:24:37,056 [INFO] Step[100/2713]: training loss : 0.9255078864097596 TRAIN  loss dict:  {'classification_loss': 0.9255078864097596}
2025-01-17 12:24:51,864 [INFO] Step[150/2713]: training loss : 0.9253487932682037 TRAIN  loss dict:  {'classification_loss': 0.9253487932682037}
2025-01-17 12:25:06,856 [INFO] Step[200/2713]: training loss : 0.9255364573001862 TRAIN  loss dict:  {'classification_loss': 0.9255364573001862}
2025-01-17 12:25:22,238 [INFO] Step[250/2713]: training loss : 0.9254223322868347 TRAIN  loss dict:  {'classification_loss': 0.9254223322868347}
2025-01-17 12:25:37,036 [INFO] Step[300/2713]: training loss : 0.925269387960434 TRAIN  loss dict:  {'classification_loss': 0.925269387960434}
2025-01-17 12:25:52,202 [INFO] Step[350/2713]: training loss : 0.9257740616798401 TRAIN  loss dict:  {'classification_loss': 0.9257740616798401}
2025-01-17 12:26:07,511 [INFO] Step[400/2713]: training loss : 0.9258926272392273 TRAIN  loss dict:  {'classification_loss': 0.9258926272392273}
2025-01-17 12:26:22,675 [INFO] Step[450/2713]: training loss : 0.9257048141956329 TRAIN  loss dict:  {'classification_loss': 0.9257048141956329}
2025-01-17 12:26:37,724 [INFO] Step[500/2713]: training loss : 0.925468544960022 TRAIN  loss dict:  {'classification_loss': 0.925468544960022}
2025-01-17 12:26:53,085 [INFO] Step[550/2713]: training loss : 0.9254092216491699 TRAIN  loss dict:  {'classification_loss': 0.9254092216491699}
2025-01-17 12:27:08,233 [INFO] Step[600/2713]: training loss : 0.9257797932624817 TRAIN  loss dict:  {'classification_loss': 0.9257797932624817}
2025-01-17 12:27:23,665 [INFO] Step[650/2713]: training loss : 0.9251493334770202 TRAIN  loss dict:  {'classification_loss': 0.9251493334770202}
2025-01-17 12:27:38,604 [INFO] Step[700/2713]: training loss : 0.933907026052475 TRAIN  loss dict:  {'classification_loss': 0.933907026052475}
2025-01-17 12:27:53,756 [INFO] Step[750/2713]: training loss : 0.9257818508148193 TRAIN  loss dict:  {'classification_loss': 0.9257818508148193}
2025-01-17 12:28:09,356 [INFO] Step[800/2713]: training loss : 0.9251832556724549 TRAIN  loss dict:  {'classification_loss': 0.9251832556724549}
2025-01-17 12:28:24,131 [INFO] Step[850/2713]: training loss : 0.925837848186493 TRAIN  loss dict:  {'classification_loss': 0.925837848186493}
2025-01-17 12:28:39,675 [INFO] Step[900/2713]: training loss : 0.9253296673297882 TRAIN  loss dict:  {'classification_loss': 0.9253296673297882}
2025-01-17 12:28:55,024 [INFO] Step[950/2713]: training loss : 0.925046443939209 TRAIN  loss dict:  {'classification_loss': 0.925046443939209}
2025-01-17 12:29:10,157 [INFO] Step[1000/2713]: training loss : 0.9256399929523468 TRAIN  loss dict:  {'classification_loss': 0.9256399929523468}
2025-01-17 12:29:25,228 [INFO] Step[1050/2713]: training loss : 0.9256700038909912 TRAIN  loss dict:  {'classification_loss': 0.9256700038909912}
2025-01-17 12:29:40,461 [INFO] Step[1100/2713]: training loss : 0.9254307174682617 TRAIN  loss dict:  {'classification_loss': 0.9254307174682617}
2025-01-17 12:29:55,410 [INFO] Step[1150/2713]: training loss : 0.9256687176227569 TRAIN  loss dict:  {'classification_loss': 0.9256687176227569}
2025-01-17 12:30:10,542 [INFO] Step[1200/2713]: training loss : 0.9254704368114471 TRAIN  loss dict:  {'classification_loss': 0.9254704368114471}
2025-01-17 12:30:25,647 [INFO] Step[1250/2713]: training loss : 0.9258308112621307 TRAIN  loss dict:  {'classification_loss': 0.9258308112621307}
2025-01-17 12:30:40,582 [INFO] Step[1300/2713]: training loss : 0.9249050283432007 TRAIN  loss dict:  {'classification_loss': 0.9249050283432007}
2025-01-17 12:30:55,808 [INFO] Step[1350/2713]: training loss : 0.9265533661842347 TRAIN  loss dict:  {'classification_loss': 0.9265533661842347}
2025-01-17 12:31:10,945 [INFO] Step[1400/2713]: training loss : 0.9255085289478302 TRAIN  loss dict:  {'classification_loss': 0.9255085289478302}
2025-01-17 12:31:25,932 [INFO] Step[1450/2713]: training loss : 0.925442385673523 TRAIN  loss dict:  {'classification_loss': 0.925442385673523}
2025-01-17 12:31:41,468 [INFO] Step[1500/2713]: training loss : 0.925401793718338 TRAIN  loss dict:  {'classification_loss': 0.925401793718338}
2025-01-17 12:31:56,425 [INFO] Step[1550/2713]: training loss : 0.9256214630603791 TRAIN  loss dict:  {'classification_loss': 0.9256214630603791}
2025-01-17 12:32:11,273 [INFO] Step[1600/2713]: training loss : 0.9254314410686493 TRAIN  loss dict:  {'classification_loss': 0.9254314410686493}
2025-01-17 12:32:26,508 [INFO] Step[1650/2713]: training loss : 0.9253798711299897 TRAIN  loss dict:  {'classification_loss': 0.9253798711299897}
2025-01-17 12:32:41,639 [INFO] Step[1700/2713]: training loss : 0.9253649759292603 TRAIN  loss dict:  {'classification_loss': 0.9253649759292603}
2025-01-17 12:32:56,755 [INFO] Step[1750/2713]: training loss : 0.9259069859981537 TRAIN  loss dict:  {'classification_loss': 0.9259069859981537}
2025-01-17 12:33:12,388 [INFO] Step[1800/2713]: training loss : 0.9251322782039643 TRAIN  loss dict:  {'classification_loss': 0.9251322782039643}
2025-01-17 12:33:28,632 [INFO] Step[1850/2713]: training loss : 0.9252606844902038 TRAIN  loss dict:  {'classification_loss': 0.9252606844902038}
