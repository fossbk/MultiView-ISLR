2025-01-10 23:09:43,176 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-10 23:10:05,463 [INFO] Step[50/2845]: training loss : 6.284520568847657 TRAIN  loss dict:  {'classification_loss': 6.284520568847657}
2025-01-10 23:10:19,506 [INFO] Step[100/2845]: training loss : 5.981180086135864 TRAIN  loss dict:  {'classification_loss': 5.981180086135864}
2025-01-10 23:10:33,607 [INFO] Step[150/2845]: training loss : 5.777603254318238 TRAIN  loss dict:  {'classification_loss': 5.777603254318238}
2025-01-11 11:35:36,565 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 11:35:52,854 [INFO] Step[50/8535]: training loss : 6.2326123332977295 TRAIN  loss dict:  {'classification_loss': 6.2326123332977295}
2025-01-11 11:36:04,601 [INFO] Step[100/8535]: training loss : 6.095604705810547 TRAIN  loss dict:  {'classification_loss': 6.095604705810547}
2025-01-11 11:39:23,698 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 11:39:39,729 [INFO] Step[50/8535]: training loss : 6.2330482482910154 TRAIN  loss dict:  {'classification_loss': 6.2330482482910154}
2025-01-11 11:39:50,827 [INFO] Step[100/8535]: training loss : 6.09221230506897 TRAIN  loss dict:  {'classification_loss': 6.09221230506897}
2025-01-11 11:42:01,791 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 11:42:18,191 [INFO] Step[50/8535]: training loss : 6.232705879211426 TRAIN  loss dict:  {'classification_loss': 6.232705879211426}
2025-01-11 11:42:29,841 [INFO] Step[100/8535]: training loss : 6.095218734741211 TRAIN  loss dict:  {'classification_loss': 6.095218734741211}
2025-01-11 11:45:13,997 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 11:45:30,574 [INFO] Step[50/8535]: training loss : 6.232788896560669 TRAIN  loss dict:  {'classification_loss': 6.232788896560669}
2025-01-11 11:45:42,482 [INFO] Step[100/8535]: training loss : 6.096108655929566 TRAIN  loss dict:  {'classification_loss': 6.096108655929566}
2025-01-11 11:46:55,190 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 11:47:11,722 [INFO] Step[50/8535]: training loss : 6.233137311935425 TRAIN  loss dict:  {'classification_loss': 6.233137311935425}
2025-01-11 11:47:23,325 [INFO] Step[100/8535]: training loss : 6.095582942962647 TRAIN  loss dict:  {'classification_loss': 6.095582942962647}
2025-01-11 12:15:00,685 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 12:15:25,921 [INFO] Step[50/2900]: training loss : 6.156516065597534 TRAIN  loss dict:  {'classification_loss': 6.156516065597534}
2025-01-11 12:15:37,662 [INFO] Step[100/2900]: training loss : 6.111492891311645 TRAIN  loss dict:  {'classification_loss': 6.111492891311645}
2025-01-11 12:15:49,547 [INFO] Step[150/2900]: training loss : 5.914700479507446 TRAIN  loss dict:  {'classification_loss': 5.914700479507446}
2025-01-11 12:16:01,454 [INFO] Step[200/2900]: training loss : 5.7796841239929195 TRAIN  loss dict:  {'classification_loss': 5.7796841239929195}
2025-01-11 12:40:27,656 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 12:41:01,897 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 12:41:26,832 [INFO] Step[50/2900]: training loss : 6.156354990005493 TRAIN  loss dict:  {'classification_loss': 6.156354990005493}
2025-01-11 12:41:38,506 [INFO] Step[100/2900]: training loss : 6.1112682819366455 TRAIN  loss dict:  {'classification_loss': 6.1112682819366455}
2025-01-11 12:41:50,177 [INFO] Step[150/2900]: training loss : 5.910567007064819 TRAIN  loss dict:  {'classification_loss': 5.910567007064819}
2025-01-11 12:42:01,923 [INFO] Step[200/2900]: training loss : 5.7824873352050785 TRAIN  loss dict:  {'classification_loss': 5.7824873352050785}
2025-01-11 12:48:05,771 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 12:48:30,667 [INFO] Step[50/2900]: training loss : 6.156406459808349 TRAIN  loss dict:  {'classification_loss': 6.156406459808349}
2025-01-11 12:48:42,309 [INFO] Step[100/2900]: training loss : 6.110895080566406 TRAIN  loss dict:  {'classification_loss': 6.110895080566406}
2025-01-11 12:48:54,049 [INFO] Step[150/2900]: training loss : 5.910647621154785 TRAIN  loss dict:  {'classification_loss': 5.910647621154785}
2025-01-11 12:49:05,763 [INFO] Step[200/2900]: training loss : 5.786291084289551 TRAIN  loss dict:  {'classification_loss': 5.786291084289551}
2025-01-11 12:59:02,699 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 12:59:28,145 [INFO] Step[50/2900]: training loss : 6.156390295028687 TRAIN  loss dict:  {'classification_loss': 6.156390295028687}
2025-01-11 12:59:39,995 [INFO] Step[100/2900]: training loss : 6.111264867782593 TRAIN  loss dict:  {'classification_loss': 6.111264867782593}
2025-01-11 12:59:52,126 [INFO] Step[150/2900]: training loss : 5.907409391403198 TRAIN  loss dict:  {'classification_loss': 5.907409391403198}
2025-01-11 13:00:04,096 [INFO] Step[200/2900]: training loss : 5.781783857345581 TRAIN  loss dict:  {'classification_loss': 5.781783857345581}
2025-01-11 13:08:24,367 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 13:08:49,411 [INFO] Step[50/2900]: training loss : 6.1565023612976075 TRAIN  loss dict:  {'classification_loss': 6.1565023612976075}
2025-01-11 13:09:01,081 [INFO] Step[100/2900]: training loss : 6.111898384094238 TRAIN  loss dict:  {'classification_loss': 6.111898384094238}
2025-01-11 13:09:12,726 [INFO] Step[150/2900]: training loss : 5.908880319595337 TRAIN  loss dict:  {'classification_loss': 5.908880319595337}
2025-01-11 13:09:24,472 [INFO] Step[200/2900]: training loss : 5.778862190246582 TRAIN  loss dict:  {'classification_loss': 5.778862190246582}
2025-01-11 13:17:00,887 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 13:19:11,312 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-11 13:23:08,658 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-12 14:59:10,177 [INFO] Starting 2s-CrossVTN/2s-CrossVTN finetune autsl to vsl 400 for one view w CLS (20f yHC 01ls 8h 2a 01dr)...


2025-01-12 14:59:35,182 [INFO] Step[50/2713]: training loss : 6.166086835861206 TRAIN  loss dict:  {'classification_loss': 6.166086835861206}
2025-01-12 14:59:46,928 [INFO] Step[100/2713]: training loss : 6.082719316482544 TRAIN  loss dict:  {'classification_loss': 6.082719316482544}
2025-01-12 14:59:58,691 [INFO] Step[150/2713]: training loss : 5.845401067733764 TRAIN  loss dict:  {'classification_loss': 5.845401067733764}
2025-01-12 15:00:10,469 [INFO] Step[200/2713]: training loss : 5.804428739547729 TRAIN  loss dict:  {'classification_loss': 5.804428739547729}
2025-01-12 15:00:22,308 [INFO] Step[250/2713]: training loss : 5.612853174209595 TRAIN  loss dict:  {'classification_loss': 5.612853174209595}
2025-01-12 15:00:34,212 [INFO] Step[300/2713]: training loss : 5.4517716026306156 TRAIN  loss dict:  {'classification_loss': 5.4517716026306156}
2025-01-12 15:00:46,147 [INFO] Step[350/2713]: training loss : 5.1575132083892825 TRAIN  loss dict:  {'classification_loss': 5.1575132083892825}
2025-01-12 15:00:58,050 [INFO] Step[400/2713]: training loss : 4.987116761207581 TRAIN  loss dict:  {'classification_loss': 4.987116761207581}
2025-01-12 15:01:09,954 [INFO] Step[450/2713]: training loss : 4.7826600885391235 TRAIN  loss dict:  {'classification_loss': 4.7826600885391235}
2025-01-12 15:01:21,828 [INFO] Step[500/2713]: training loss : 4.761811919212342 TRAIN  loss dict:  {'classification_loss': 4.761811919212342}
2025-01-12 15:01:33,714 [INFO] Step[550/2713]: training loss : 4.728831853866577 TRAIN  loss dict:  {'classification_loss': 4.728831853866577}
2025-01-12 15:01:45,586 [INFO] Step[600/2713]: training loss : 4.513488192558288 TRAIN  loss dict:  {'classification_loss': 4.513488192558288}
2025-01-12 15:01:57,539 [INFO] Step[650/2713]: training loss : 4.201044406890869 TRAIN  loss dict:  {'classification_loss': 4.201044406890869}
2025-01-12 15:02:09,440 [INFO] Step[700/2713]: training loss : 3.9958565473556518 TRAIN  loss dict:  {'classification_loss': 3.9958565473556518}
2025-01-12 15:02:21,377 [INFO] Step[750/2713]: training loss : 3.816421699523926 TRAIN  loss dict:  {'classification_loss': 3.816421699523926}
2025-01-12 15:02:33,281 [INFO] Step[800/2713]: training loss : 3.964326934814453 TRAIN  loss dict:  {'classification_loss': 3.964326934814453}
2025-01-12 15:02:45,279 [INFO] Step[850/2713]: training loss : 3.748792176246643 TRAIN  loss dict:  {'classification_loss': 3.748792176246643}
2025-01-12 15:02:57,177 [INFO] Step[900/2713]: training loss : 3.650118677616119 TRAIN  loss dict:  {'classification_loss': 3.650118677616119}
2025-01-12 15:03:09,203 [INFO] Step[950/2713]: training loss : 3.365718946456909 TRAIN  loss dict:  {'classification_loss': 3.365718946456909}
2025-01-12 15:03:21,213 [INFO] Step[1000/2713]: training loss : 3.381368057727814 TRAIN  loss dict:  {'classification_loss': 3.381368057727814}
2025-01-12 15:03:33,166 [INFO] Step[1050/2713]: training loss : 3.26428186416626 TRAIN  loss dict:  {'classification_loss': 3.26428186416626}
2025-01-12 15:03:45,251 [INFO] Step[1100/2713]: training loss : 3.1352176666259766 TRAIN  loss dict:  {'classification_loss': 3.1352176666259766}
2025-01-12 15:03:57,382 [INFO] Step[1150/2713]: training loss : 2.8502426075935365 TRAIN  loss dict:  {'classification_loss': 2.8502426075935365}
2025-01-12 15:04:09,313 [INFO] Step[1200/2713]: training loss : 2.8528530645370482 TRAIN  loss dict:  {'classification_loss': 2.8528530645370482}
2025-01-12 15:04:21,306 [INFO] Step[1250/2713]: training loss : 2.623378219604492 TRAIN  loss dict:  {'classification_loss': 2.623378219604492}
2025-01-12 15:04:33,661 [INFO] Step[1300/2713]: training loss : 2.7866951394081116 TRAIN  loss dict:  {'classification_loss': 2.7866951394081116}
2025-01-12 15:04:46,069 [INFO] Step[1350/2713]: training loss : 2.673114218711853 TRAIN  loss dict:  {'classification_loss': 2.673114218711853}
2025-01-12 15:04:58,541 [INFO] Step[1400/2713]: training loss : 2.71210942029953 TRAIN  loss dict:  {'classification_loss': 2.71210942029953}
2025-01-12 15:05:10,837 [INFO] Step[1450/2713]: training loss : 2.39037700176239 TRAIN  loss dict:  {'classification_loss': 2.39037700176239}
2025-01-12 15:05:23,389 [INFO] Step[1500/2713]: training loss : 2.6187284135818483 TRAIN  loss dict:  {'classification_loss': 2.6187284135818483}
2025-01-12 15:05:35,813 [INFO] Step[1550/2713]: training loss : 2.4323228335380556 TRAIN  loss dict:  {'classification_loss': 2.4323228335380556}
2025-01-12 15:05:48,196 [INFO] Step[1600/2713]: training loss : 2.2671136808395387 TRAIN  loss dict:  {'classification_loss': 2.2671136808395387}
2025-01-12 15:06:01,064 [INFO] Step[1650/2713]: training loss : 2.302928040027618 TRAIN  loss dict:  {'classification_loss': 2.302928040027618}
2025-01-12 15:06:15,203 [INFO] Step[1700/2713]: training loss : 2.1937381625175476 TRAIN  loss dict:  {'classification_loss': 2.1937381625175476}
2025-01-12 15:06:29,263 [INFO] Step[1750/2713]: training loss : 2.306117172241211 TRAIN  loss dict:  {'classification_loss': 2.306117172241211}
2025-01-12 15:06:47,901 [INFO] Step[1800/2713]: training loss : 2.246731390953064 TRAIN  loss dict:  {'classification_loss': 2.246731390953064}
2025-01-12 15:07:01,159 [INFO] Step[1850/2713]: training loss : 2.184136724472046 TRAIN  loss dict:  {'classification_loss': 2.184136724472046}
2025-01-12 15:07:13,080 [INFO] Step[1900/2713]: training loss : 2.1111979341506957 TRAIN  loss dict:  {'classification_loss': 2.1111979341506957}
2025-01-12 15:07:24,983 [INFO] Step[1950/2713]: training loss : 2.1425835418701173 TRAIN  loss dict:  {'classification_loss': 2.1425835418701173}
2025-01-12 15:07:36,867 [INFO] Step[2000/2713]: training loss : 2.20423259973526 TRAIN  loss dict:  {'classification_loss': 2.20423259973526}
2025-01-12 15:07:48,787 [INFO] Step[2050/2713]: training loss : 2.1252320551872255 TRAIN  loss dict:  {'classification_loss': 2.1252320551872255}
2025-01-12 15:08:00,719 [INFO] Step[2100/2713]: training loss : 2.1397938656806947 TRAIN  loss dict:  {'classification_loss': 2.1397938656806947}
2025-01-12 15:08:12,636 [INFO] Step[2150/2713]: training loss : 1.9187085938453674 TRAIN  loss dict:  {'classification_loss': 1.9187085938453674}
2025-01-12 15:08:24,550 [INFO] Step[2200/2713]: training loss : 2.0420736289024353 TRAIN  loss dict:  {'classification_loss': 2.0420736289024353}
2025-01-12 15:08:36,483 [INFO] Step[2250/2713]: training loss : 2.1473002767562868 TRAIN  loss dict:  {'classification_loss': 2.1473002767562868}
2025-01-12 15:08:48,417 [INFO] Step[2300/2713]: training loss : 1.9655010986328125 TRAIN  loss dict:  {'classification_loss': 1.9655010986328125}
2025-01-12 15:09:00,324 [INFO] Step[2350/2713]: training loss : 2.1396526098251343 TRAIN  loss dict:  {'classification_loss': 2.1396526098251343}
2025-01-12 15:09:12,275 [INFO] Step[2400/2713]: training loss : 2.035678861141205 TRAIN  loss dict:  {'classification_loss': 2.035678861141205}
2025-01-12 15:09:24,231 [INFO] Step[2450/2713]: training loss : 1.7672297310829164 TRAIN  loss dict:  {'classification_loss': 1.7672297310829164}
2025-01-12 15:09:36,197 [INFO] Step[2500/2713]: training loss : 2.095879788398743 TRAIN  loss dict:  {'classification_loss': 2.095879788398743}
2025-01-12 15:09:48,156 [INFO] Step[2550/2713]: training loss : 2.0350956082344056 TRAIN  loss dict:  {'classification_loss': 2.0350956082344056}
2025-01-12 15:10:00,073 [INFO] Step[2600/2713]: training loss : 1.839054615497589 TRAIN  loss dict:  {'classification_loss': 1.839054615497589}
2025-01-12 15:10:12,018 [INFO] Step[2650/2713]: training loss : 1.8869717240333557 TRAIN  loss dict:  {'classification_loss': 1.8869717240333557}
2025-01-12 15:10:23,960 [INFO] Step[2700/2713]: training loss : 1.8915792274475098 TRAIN  loss dict:  {'classification_loss': 1.8915792274475098}
2025-01-12 15:11:53,016 [INFO] Label accuracies statistics:
2025-01-12 15:11:53,017 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.25, 8: 0.5, 9: 0.5, 10: 1.0, 11: 0.5, 12: 0.25, 13: 0.75, 14: 0.25, 15: 0.6666666666666666, 16: 0.25, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 0.75, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.5, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.0, 55: 0.75, 56: 1.0, 57: 0.75, 58: 1.0, 59: 0.25, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.5, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 1.0, 99: 0.8, 100: 0.75, 101: 0.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 0.75, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.5, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 0.75, 116: 1.0, 117: 1.0, 118: 0.5, 119: 1.0, 120: 1.0, 121: 1.0, 122: 0.75, 123: 0.5, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.25, 133: 0.75, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.5, 143: 0.5, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 1.0, 154: 0.75, 155: 1.0, 156: 0.25, 157: 0.75, 158: 0.3333333333333333, 159: 0.5, 160: 0.5, 161: 1.0, 162: 0.75, 163: 0.5, 164: 1.0, 165: 1.0, 166: 0.5, 167: 0.75, 168: 0.5, 169: 0.5, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.5, 185: 0.5, 186: 0.75, 187: 1.0, 188: 0.75, 189: 1.0, 190: 0.25, 191: 0.5, 192: 0.75, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.0, 204: 0.75, 205: 0.5, 206: 0.0, 207: 0.5, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.5, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.25, 218: 0.75, 219: 0.75, 220: 0.5, 221: 0.75, 222: 0.75, 223: 0.25, 224: 0.75, 225: 0.75, 226: 0.25, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.0, 234: 0.75, 235: 0.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.0, 240: 0.5, 241: 0.75, 242: 0.75, 243: 0.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.5, 248: 0.3333333333333333, 249: 1.0, 250: 1.0, 251: 1.0, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.25, 257: 1.0, 258: 0.0, 259: 0.25, 260: 0.0, 261: 0.0, 262: 1.0, 263: 0.25, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.25, 272: 0.25, 273: 0.5, 274: 0.0, 275: 0.25, 276: 0.25, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.0, 283: 1.0, 284: 0.75, 285: 0.25, 286: 0.0, 287: 0.75, 288: 1.0, 289: 0.5, 290: 0.25, 291: 0.25, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.0, 296: 0.0, 297: 0.75, 298: 0.75, 299: 0.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.25, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.0, 317: 0.75, 318: 0.5, 319: 0.5, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.25, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.5, 332: 1.0, 333: 0.0, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.25, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.5, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.5, 347: 0.75, 348: 0.75, 349: 0.0, 350: 0.0, 351: 0.75, 352: 0.25, 353: 0.25, 354: 0.0, 355: 0.5, 356: 0.25, 357: 0.75, 358: 0.5, 359: 1.0, 360: 0.75, 361: 0.5, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.5, 366: 0.75, 367: 0.5, 368: 1.0, 369: 0.75, 370: 0.0, 371: 0.75, 372: 0.5, 373: 0.5, 374: 1.0, 375: 0.25, 376: 0.75, 377: 0.75, 378: 0.25, 379: 0.75, 380: 0.75, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.5, 385: 0.75, 386: 0.5, 387: 0.5, 388: 0.5, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-12 15:11:54,214 [INFO] [1] TRAIN  loss: 3.1860086112017125 acc: 0.4933038456812876
2025-01-12 15:11:54,214 [INFO] [1] TRAIN  loss dict: {'classification_loss': 3.1860086112017125}
2025-01-12 15:11:54,214 [INFO] [1] VALIDATION loss: 2.136874594410559 VALIDATION acc: 0.6633228840125391
2025-01-12 15:11:54,214 [INFO] [1] VALIDATION loss dict: {'classification_loss': 2.136874594410559}
2025-01-12 15:11:54,214 [INFO] 
2025-01-12 15:12:12,489 [INFO] Step[50/2713]: training loss : 1.6687299370765687 TRAIN  loss dict:  {'classification_loss': 1.6687299370765687}
2025-01-12 15:12:24,440 [INFO] Step[100/2713]: training loss : 1.8522508907318116 TRAIN  loss dict:  {'classification_loss': 1.8522508907318116}
2025-01-12 15:12:36,381 [INFO] Step[150/2713]: training loss : 1.62794203042984 TRAIN  loss dict:  {'classification_loss': 1.62794203042984}
2025-01-12 15:12:48,317 [INFO] Step[200/2713]: training loss : 1.6291900420188903 TRAIN  loss dict:  {'classification_loss': 1.6291900420188903}
2025-01-12 15:13:00,263 [INFO] Step[250/2713]: training loss : 1.8756244683265686 TRAIN  loss dict:  {'classification_loss': 1.8756244683265686}
2025-01-12 15:13:12,230 [INFO] Step[300/2713]: training loss : 1.6805086088180543 TRAIN  loss dict:  {'classification_loss': 1.6805086088180543}
2025-01-12 15:13:24,201 [INFO] Step[350/2713]: training loss : 1.6618381977081298 TRAIN  loss dict:  {'classification_loss': 1.6618381977081298}
2025-01-12 15:13:36,148 [INFO] Step[400/2713]: training loss : 1.7322856187820435 TRAIN  loss dict:  {'classification_loss': 1.7322856187820435}
2025-01-12 15:13:48,101 [INFO] Step[450/2713]: training loss : 1.7218693780899048 TRAIN  loss dict:  {'classification_loss': 1.7218693780899048}
2025-01-12 15:14:00,010 [INFO] Step[500/2713]: training loss : 1.738812916278839 TRAIN  loss dict:  {'classification_loss': 1.738812916278839}
2025-01-12 15:14:11,970 [INFO] Step[550/2713]: training loss : 1.8021137070655824 TRAIN  loss dict:  {'classification_loss': 1.8021137070655824}
2025-01-12 15:14:23,868 [INFO] Step[600/2713]: training loss : 1.574320719242096 TRAIN  loss dict:  {'classification_loss': 1.574320719242096}
2025-01-12 15:14:35,832 [INFO] Step[650/2713]: training loss : 1.822263638973236 TRAIN  loss dict:  {'classification_loss': 1.822263638973236}
2025-01-12 15:14:47,736 [INFO] Step[700/2713]: training loss : 1.8178432941436768 TRAIN  loss dict:  {'classification_loss': 1.8178432941436768}
2025-01-12 15:14:59,627 [INFO] Step[750/2713]: training loss : 1.6052344560623169 TRAIN  loss dict:  {'classification_loss': 1.6052344560623169}
2025-01-12 15:15:11,543 [INFO] Step[800/2713]: training loss : 1.5310598850250243 TRAIN  loss dict:  {'classification_loss': 1.5310598850250243}
2025-01-12 15:15:23,474 [INFO] Step[850/2713]: training loss : 1.703166391849518 TRAIN  loss dict:  {'classification_loss': 1.703166391849518}
2025-01-12 15:15:35,453 [INFO] Step[900/2713]: training loss : 1.6452618646621704 TRAIN  loss dict:  {'classification_loss': 1.6452618646621704}
2025-01-12 15:15:47,427 [INFO] Step[950/2713]: training loss : 1.8310072422027588 TRAIN  loss dict:  {'classification_loss': 1.8310072422027588}
2025-01-12 15:15:59,357 [INFO] Step[1000/2713]: training loss : 1.5588606595993042 TRAIN  loss dict:  {'classification_loss': 1.5588606595993042}
2025-01-12 15:16:11,333 [INFO] Step[1050/2713]: training loss : 1.7212141728401185 TRAIN  loss dict:  {'classification_loss': 1.7212141728401185}
2025-01-12 15:16:23,248 [INFO] Step[1100/2713]: training loss : 1.5940643739700318 TRAIN  loss dict:  {'classification_loss': 1.5940643739700318}
2025-01-12 15:16:35,192 [INFO] Step[1150/2713]: training loss : 1.7057652759552002 TRAIN  loss dict:  {'classification_loss': 1.7057652759552002}
2025-01-12 15:16:47,166 [INFO] Step[1200/2713]: training loss : 1.5028123664855957 TRAIN  loss dict:  {'classification_loss': 1.5028123664855957}
2025-01-12 15:16:59,135 [INFO] Step[1250/2713]: training loss : 1.6014165091514587 TRAIN  loss dict:  {'classification_loss': 1.6014165091514587}
2025-01-12 15:17:11,054 [INFO] Step[1300/2713]: training loss : 1.7191918063163758 TRAIN  loss dict:  {'classification_loss': 1.7191918063163758}
2025-01-12 15:17:23,021 [INFO] Step[1350/2713]: training loss : 1.6696241426467895 TRAIN  loss dict:  {'classification_loss': 1.6696241426467895}
2025-01-12 15:17:35,035 [INFO] Step[1400/2713]: training loss : 1.660489683151245 TRAIN  loss dict:  {'classification_loss': 1.660489683151245}
2025-01-12 15:17:46,978 [INFO] Step[1450/2713]: training loss : 1.5679706835746765 TRAIN  loss dict:  {'classification_loss': 1.5679706835746765}
2025-01-12 15:17:58,934 [INFO] Step[1500/2713]: training loss : 1.5796826601028442 TRAIN  loss dict:  {'classification_loss': 1.5796826601028442}
2025-01-12 15:18:10,911 [INFO] Step[1550/2713]: training loss : 1.6016935896873474 TRAIN  loss dict:  {'classification_loss': 1.6016935896873474}
2025-01-12 15:18:22,809 [INFO] Step[1600/2713]: training loss : 1.5666574788093568 TRAIN  loss dict:  {'classification_loss': 1.5666574788093568}
2025-01-12 15:18:34,775 [INFO] Step[1650/2713]: training loss : 1.7258018684387206 TRAIN  loss dict:  {'classification_loss': 1.7258018684387206}
2025-01-12 15:18:46,731 [INFO] Step[1700/2713]: training loss : 1.674112982749939 TRAIN  loss dict:  {'classification_loss': 1.674112982749939}
2025-01-12 15:18:58,741 [INFO] Step[1750/2713]: training loss : 1.5586972904205323 TRAIN  loss dict:  {'classification_loss': 1.5586972904205323}
2025-01-12 15:19:10,755 [INFO] Step[1800/2713]: training loss : 1.5980943202972413 TRAIN  loss dict:  {'classification_loss': 1.5980943202972413}
2025-01-12 15:19:22,954 [INFO] Step[1850/2713]: training loss : 1.6351871061325074 TRAIN  loss dict:  {'classification_loss': 1.6351871061325074}
2025-01-12 15:19:34,944 [INFO] Step[1900/2713]: training loss : 1.7063895273208618 TRAIN  loss dict:  {'classification_loss': 1.7063895273208618}
2025-01-12 15:19:47,080 [INFO] Step[1950/2713]: training loss : 1.539657473564148 TRAIN  loss dict:  {'classification_loss': 1.539657473564148}
2025-01-12 15:19:59,093 [INFO] Step[2000/2713]: training loss : 1.6199210906028747 TRAIN  loss dict:  {'classification_loss': 1.6199210906028747}
2025-01-12 15:20:11,035 [INFO] Step[2050/2713]: training loss : 1.6178036856651306 TRAIN  loss dict:  {'classification_loss': 1.6178036856651306}
2025-01-12 15:20:22,974 [INFO] Step[2100/2713]: training loss : 1.5179944658279418 TRAIN  loss dict:  {'classification_loss': 1.5179944658279418}
2025-01-12 15:20:34,927 [INFO] Step[2150/2713]: training loss : 1.683968505859375 TRAIN  loss dict:  {'classification_loss': 1.683968505859375}
2025-01-12 15:20:46,912 [INFO] Step[2200/2713]: training loss : 1.6173548102378845 TRAIN  loss dict:  {'classification_loss': 1.6173548102378845}
2025-01-12 15:20:58,867 [INFO] Step[2250/2713]: training loss : 1.5419280624389649 TRAIN  loss dict:  {'classification_loss': 1.5419280624389649}
2025-01-12 15:21:10,800 [INFO] Step[2300/2713]: training loss : 1.6245353055000304 TRAIN  loss dict:  {'classification_loss': 1.6245353055000304}
2025-01-12 15:21:22,779 [INFO] Step[2350/2713]: training loss : 1.6256273913383483 TRAIN  loss dict:  {'classification_loss': 1.6256273913383483}
2025-01-12 15:21:34,635 [INFO] Step[2400/2713]: training loss : 1.5991128420829772 TRAIN  loss dict:  {'classification_loss': 1.5991128420829772}
2025-01-12 15:21:46,547 [INFO] Step[2450/2713]: training loss : 1.5837737607955933 TRAIN  loss dict:  {'classification_loss': 1.5837737607955933}
2025-01-12 15:21:58,452 [INFO] Step[2500/2713]: training loss : 1.4665522837638856 TRAIN  loss dict:  {'classification_loss': 1.4665522837638856}
2025-01-12 15:22:10,366 [INFO] Step[2550/2713]: training loss : 1.6328750276565551 TRAIN  loss dict:  {'classification_loss': 1.6328750276565551}
2025-01-12 15:22:22,262 [INFO] Step[2600/2713]: training loss : 1.7359315133094788 TRAIN  loss dict:  {'classification_loss': 1.7359315133094788}
2025-01-12 15:22:34,188 [INFO] Step[2650/2713]: training loss : 1.5796275973320006 TRAIN  loss dict:  {'classification_loss': 1.5796275973320006}
2025-01-12 15:22:46,158 [INFO] Step[2700/2713]: training loss : 1.6210244512557983 TRAIN  loss dict:  {'classification_loss': 1.6210244512557983}
2025-01-12 15:24:43,733 [INFO] Label accuracies statistics:
2025-01-12 15:24:43,733 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.5, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.25, 14: 0.75, 15: 0.3333333333333333, 16: 0.25, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 1.0, 57: 0.75, 58: 0.5, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 1.0, 69: 0.5, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 1.0, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 0.75, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 0.75, 105: 0.75, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.25, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.25, 140: 1.0, 141: 1.0, 142: 0.25, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 1.0, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.75, 191: 0.5, 192: 0.75, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 1.0, 220: 1.0, 221: 0.75, 222: 0.5, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.0, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.25, 238: 1.0, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.3333333333333333, 249: 1.0, 250: 0.5, 251: 1.0, 252: 0.5, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.0, 259: 1.0, 260: 0.5, 261: 0.25, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 0.75, 267: 0.25, 268: 0.5, 269: 0.75, 270: 0.5, 271: 0.75, 272: 1.0, 273: 0.0, 274: 0.25, 275: 0.5, 276: 0.5, 277: 1.0, 278: 0.25, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.0, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.0, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.5, 295: 0.75, 296: 0.75, 297: 0.0, 298: 0.75, 299: 0.5, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 0.75, 307: 0.25, 308: 0.75, 309: 0.75, 310: 0.5, 311: 0.75, 312: 0.0, 313: 1.0, 314: 0.75, 315: 0.5, 316: 0.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 1.0, 323: 0.5, 324: 0.5, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 0.75, 330: 0.5, 331: 0.5, 332: 1.0, 333: 0.0, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.5, 347: 1.0, 348: 0.5, 349: 0.5, 350: 0.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.25, 356: 0.5, 357: 0.75, 358: 0.5, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.0, 365: 0.5, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.0, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 0.5, 380: 0.5, 381: 0.25, 382: 0.75, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 0.5, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.25, 395: 0.5, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.5}

2025-01-12 15:24:46,051 [INFO] [2] TRAIN  loss: 1.6495952205907538 acc: 0.8107875660400541
2025-01-12 15:24:46,051 [INFO] [2] TRAIN  loss dict: {'classification_loss': 1.6495952205907538}
2025-01-12 15:24:46,051 [INFO] [2] VALIDATION loss: 1.9728641332988452 VALIDATION acc: 0.722257053291536
2025-01-12 15:24:46,051 [INFO] [2] VALIDATION loss dict: {'classification_loss': 1.9728641332988452}
2025-01-12 15:24:46,051 [INFO] 
2025-01-12 15:25:05,870 [INFO] Step[50/2713]: training loss : 1.4101689434051514 TRAIN  loss dict:  {'classification_loss': 1.4101689434051514}
2025-01-12 15:25:18,342 [INFO] Step[100/2713]: training loss : 1.3463789796829224 TRAIN  loss dict:  {'classification_loss': 1.3463789796829224}
2025-01-12 15:25:30,744 [INFO] Step[150/2713]: training loss : 1.5647801446914673 TRAIN  loss dict:  {'classification_loss': 1.5647801446914673}
2025-01-12 15:25:43,293 [INFO] Step[200/2713]: training loss : 1.5531578588485717 TRAIN  loss dict:  {'classification_loss': 1.5531578588485717}
2025-01-12 15:25:56,307 [INFO] Step[250/2713]: training loss : 1.4586540055274964 TRAIN  loss dict:  {'classification_loss': 1.4586540055274964}
2025-01-12 15:26:10,110 [INFO] Step[300/2713]: training loss : 1.3801184821128845 TRAIN  loss dict:  {'classification_loss': 1.3801184821128845}
2025-01-12 15:26:23,114 [INFO] Step[350/2713]: training loss : 1.3496427536010742 TRAIN  loss dict:  {'classification_loss': 1.3496427536010742}
2025-01-12 15:26:35,181 [INFO] Step[400/2713]: training loss : 1.416101655960083 TRAIN  loss dict:  {'classification_loss': 1.416101655960083}
2025-01-12 15:26:47,051 [INFO] Step[450/2713]: training loss : 1.4568480944633484 TRAIN  loss dict:  {'classification_loss': 1.4568480944633484}
2025-01-12 15:26:58,906 [INFO] Step[500/2713]: training loss : 1.4912636971473694 TRAIN  loss dict:  {'classification_loss': 1.4912636971473694}
2025-01-12 15:27:10,811 [INFO] Step[550/2713]: training loss : 1.4686921858787536 TRAIN  loss dict:  {'classification_loss': 1.4686921858787536}
2025-01-12 15:27:22,694 [INFO] Step[600/2713]: training loss : 1.4359675860404968 TRAIN  loss dict:  {'classification_loss': 1.4359675860404968}
2025-01-12 15:27:34,579 [INFO] Step[650/2713]: training loss : 1.475861871242523 TRAIN  loss dict:  {'classification_loss': 1.475861871242523}
2025-01-12 15:27:46,518 [INFO] Step[700/2713]: training loss : 1.446066906452179 TRAIN  loss dict:  {'classification_loss': 1.446066906452179}
2025-01-12 15:27:58,439 [INFO] Step[750/2713]: training loss : 1.44624338388443 TRAIN  loss dict:  {'classification_loss': 1.44624338388443}
2025-01-12 15:28:10,300 [INFO] Step[800/2713]: training loss : 1.434194233417511 TRAIN  loss dict:  {'classification_loss': 1.434194233417511}
2025-01-12 15:28:22,230 [INFO] Step[850/2713]: training loss : 1.3802682781219482 TRAIN  loss dict:  {'classification_loss': 1.3802682781219482}
2025-01-12 15:28:34,107 [INFO] Step[900/2713]: training loss : 1.3390492939949035 TRAIN  loss dict:  {'classification_loss': 1.3390492939949035}
2025-01-12 15:28:46,020 [INFO] Step[950/2713]: training loss : 1.443863208293915 TRAIN  loss dict:  {'classification_loss': 1.443863208293915}
2025-01-12 15:28:57,946 [INFO] Step[1000/2713]: training loss : 1.3708763432502746 TRAIN  loss dict:  {'classification_loss': 1.3708763432502746}
2025-01-12 15:29:09,836 [INFO] Step[1050/2713]: training loss : 1.4723159956932068 TRAIN  loss dict:  {'classification_loss': 1.4723159956932068}
2025-01-12 15:29:21,735 [INFO] Step[1100/2713]: training loss : 1.5097250771522521 TRAIN  loss dict:  {'classification_loss': 1.5097250771522521}
2025-01-12 15:29:33,651 [INFO] Step[1150/2713]: training loss : 1.428584930896759 TRAIN  loss dict:  {'classification_loss': 1.428584930896759}
2025-01-12 15:29:45,565 [INFO] Step[1200/2713]: training loss : 1.448433747291565 TRAIN  loss dict:  {'classification_loss': 1.448433747291565}
2025-01-12 15:29:57,515 [INFO] Step[1250/2713]: training loss : 1.4422969818115234 TRAIN  loss dict:  {'classification_loss': 1.4422969818115234}
2025-01-12 15:30:09,405 [INFO] Step[1300/2713]: training loss : 1.5507311940193176 TRAIN  loss dict:  {'classification_loss': 1.5507311940193176}
2025-01-12 15:30:21,346 [INFO] Step[1350/2713]: training loss : 1.4170297360420228 TRAIN  loss dict:  {'classification_loss': 1.4170297360420228}
2025-01-12 15:30:33,313 [INFO] Step[1400/2713]: training loss : 1.4654162645339965 TRAIN  loss dict:  {'classification_loss': 1.4654162645339965}
2025-01-12 15:30:45,308 [INFO] Step[1450/2713]: training loss : 1.4387085127830506 TRAIN  loss dict:  {'classification_loss': 1.4387085127830506}
2025-01-12 15:30:57,220 [INFO] Step[1500/2713]: training loss : 1.5198989844322204 TRAIN  loss dict:  {'classification_loss': 1.5198989844322204}
2025-01-12 15:31:09,180 [INFO] Step[1550/2713]: training loss : 1.395377788543701 TRAIN  loss dict:  {'classification_loss': 1.395377788543701}
2025-01-12 15:31:21,083 [INFO] Step[1600/2713]: training loss : 1.4894152235984803 TRAIN  loss dict:  {'classification_loss': 1.4894152235984803}
2025-01-12 15:31:32,996 [INFO] Step[1650/2713]: training loss : 1.457432451248169 TRAIN  loss dict:  {'classification_loss': 1.457432451248169}
2025-01-12 15:31:44,867 [INFO] Step[1700/2713]: training loss : 1.557647693157196 TRAIN  loss dict:  {'classification_loss': 1.557647693157196}
2025-01-12 15:31:56,765 [INFO] Step[1750/2713]: training loss : 1.574414529800415 TRAIN  loss dict:  {'classification_loss': 1.574414529800415}
2025-01-12 15:32:08,637 [INFO] Step[1800/2713]: training loss : 1.4866898894309997 TRAIN  loss dict:  {'classification_loss': 1.4866898894309997}
2025-01-12 15:32:20,533 [INFO] Step[1850/2713]: training loss : 1.4100711727142334 TRAIN  loss dict:  {'classification_loss': 1.4100711727142334}
2025-01-12 15:32:32,431 [INFO] Step[1900/2713]: training loss : 1.5623178005218505 TRAIN  loss dict:  {'classification_loss': 1.5623178005218505}
2025-01-12 15:32:44,344 [INFO] Step[1950/2713]: training loss : 1.4639259934425355 TRAIN  loss dict:  {'classification_loss': 1.4639259934425355}
2025-01-12 15:32:56,203 [INFO] Step[2000/2713]: training loss : 1.5158412885665893 TRAIN  loss dict:  {'classification_loss': 1.5158412885665893}
2025-01-12 15:33:08,093 [INFO] Step[2050/2713]: training loss : 1.3367863869667054 TRAIN  loss dict:  {'classification_loss': 1.3367863869667054}
2025-01-12 15:33:20,039 [INFO] Step[2100/2713]: training loss : 1.5457074451446533 TRAIN  loss dict:  {'classification_loss': 1.5457074451446533}
2025-01-12 15:33:31,981 [INFO] Step[2150/2713]: training loss : 1.5119142317771912 TRAIN  loss dict:  {'classification_loss': 1.5119142317771912}
2025-01-12 15:33:43,885 [INFO] Step[2200/2713]: training loss : 1.3833469676971435 TRAIN  loss dict:  {'classification_loss': 1.3833469676971435}
2025-01-12 15:33:55,774 [INFO] Step[2250/2713]: training loss : 1.4022093749046325 TRAIN  loss dict:  {'classification_loss': 1.4022093749046325}
2025-01-12 15:34:07,676 [INFO] Step[2300/2713]: training loss : 1.5356536078453065 TRAIN  loss dict:  {'classification_loss': 1.5356536078453065}
2025-01-12 15:34:19,568 [INFO] Step[2350/2713]: training loss : 1.4655699229240418 TRAIN  loss dict:  {'classification_loss': 1.4655699229240418}
2025-01-12 15:34:31,444 [INFO] Step[2400/2713]: training loss : 1.4949346995353698 TRAIN  loss dict:  {'classification_loss': 1.4949346995353698}
2025-01-12 15:34:43,380 [INFO] Step[2450/2713]: training loss : 1.4618321251869202 TRAIN  loss dict:  {'classification_loss': 1.4618321251869202}
2025-01-12 15:34:55,298 [INFO] Step[2500/2713]: training loss : 1.3939815735816956 TRAIN  loss dict:  {'classification_loss': 1.3939815735816956}
2025-01-12 15:35:07,236 [INFO] Step[2550/2713]: training loss : 1.5203190922737122 TRAIN  loss dict:  {'classification_loss': 1.5203190922737122}
2025-01-12 15:35:19,160 [INFO] Step[2600/2713]: training loss : 1.6417673110961915 TRAIN  loss dict:  {'classification_loss': 1.6417673110961915}
2025-01-12 15:35:31,123 [INFO] Step[2650/2713]: training loss : 1.3973212623596192 TRAIN  loss dict:  {'classification_loss': 1.3973212623596192}
2025-01-12 15:35:43,059 [INFO] Step[2700/2713]: training loss : 1.395029945373535 TRAIN  loss dict:  {'classification_loss': 1.395029945373535}
2025-01-12 15:37:10,992 [INFO] Label accuracies statistics:
2025-01-12 15:37:10,993 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.75, 10: 0.75, 11: 0.75, 12: 0.0, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.25, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.5, 48: 0.75, 49: 0.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.25, 54: 0.25, 55: 0.75, 56: 1.0, 57: 0.5, 58: 0.5, 59: 0.5, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 0.5, 68: 1.0, 69: 0.5, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.0, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.25, 87: 0.25, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 0.5, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.25, 117: 0.5, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 0.75, 132: 0.25, 133: 0.75, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.5, 143: 0.75, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.25, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.25, 181: 0.75, 182: 0.25, 183: 0.5, 184: 0.75, 185: 0.75, 186: 0.75, 187: 0.75, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.5, 192: 0.75, 193: 1.0, 194: 0.75, 195: 0.5, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.5, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.0, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.25, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.25, 231: 0.5, 232: 0.25, 233: 0.5, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 0.75, 242: 0.75, 243: 0.5, 244: 1.0, 245: 0.75, 246: 0.75, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.75, 261: 0.75, 262: 1.0, 263: 0.75, 264: 0.25, 265: 0.75, 266: 1.0, 267: 0.25, 268: 0.5, 269: 0.5, 270: 0.75, 271: 0.25, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.0, 279: 1.0, 280: 1.0, 281: 0.5, 282: 0.5, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.0, 291: 0.5, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.0, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.25, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 0.5, 307: 0.5, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.0, 329: 1.0, 330: 0.25, 331: 1.0, 332: 0.5, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.25, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.25, 357: 0.75, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.5, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.25, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 1.0, 376: 0.5, 377: 0.75, 378: 0.5, 379: 0.75, 380: 0.75, 381: 0.0, 382: 1.0, 383: 0.25, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.25, 390: 0.5, 391: 0.75, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-12 15:37:10,994 [INFO] [3] TRAIN  loss: 1.4578370895941144 acc: 0.8717287136011795
2025-01-12 15:37:10,994 [INFO] [3] TRAIN  loss dict: {'classification_loss': 1.4578370895941144}
2025-01-12 15:37:10,995 [INFO] [3] VALIDATION loss: 2.0505699923163965 VALIDATION acc: 0.7047021943573668
2025-01-12 15:37:10,995 [INFO] [3] VALIDATION loss dict: {'classification_loss': 2.0505699923163965}
2025-01-12 15:37:10,995 [INFO] 
2025-01-12 15:37:28,260 [INFO] Step[50/2713]: training loss : 1.264022831916809 TRAIN  loss dict:  {'classification_loss': 1.264022831916809}
2025-01-12 15:37:40,163 [INFO] Step[100/2713]: training loss : 1.3692688870429992 TRAIN  loss dict:  {'classification_loss': 1.3692688870429992}
2025-01-12 15:37:52,096 [INFO] Step[150/2713]: training loss : 1.3310438895225525 TRAIN  loss dict:  {'classification_loss': 1.3310438895225525}
2025-01-12 15:38:04,027 [INFO] Step[200/2713]: training loss : 1.316612401008606 TRAIN  loss dict:  {'classification_loss': 1.316612401008606}
2025-01-12 15:38:15,984 [INFO] Step[250/2713]: training loss : 1.3761072182655334 TRAIN  loss dict:  {'classification_loss': 1.3761072182655334}
2025-01-12 15:38:27,937 [INFO] Step[300/2713]: training loss : 1.291688497066498 TRAIN  loss dict:  {'classification_loss': 1.291688497066498}
2025-01-12 15:38:39,875 [INFO] Step[350/2713]: training loss : 1.34403888463974 TRAIN  loss dict:  {'classification_loss': 1.34403888463974}
2025-01-12 15:38:51,788 [INFO] Step[400/2713]: training loss : 1.364833607673645 TRAIN  loss dict:  {'classification_loss': 1.364833607673645}
2025-01-12 15:39:03,741 [INFO] Step[450/2713]: training loss : 1.446011574268341 TRAIN  loss dict:  {'classification_loss': 1.446011574268341}
2025-01-12 15:39:15,644 [INFO] Step[500/2713]: training loss : 1.3478736805915832 TRAIN  loss dict:  {'classification_loss': 1.3478736805915832}
2025-01-12 15:39:27,578 [INFO] Step[550/2713]: training loss : 1.4408323574066162 TRAIN  loss dict:  {'classification_loss': 1.4408323574066162}
2025-01-12 15:39:39,488 [INFO] Step[600/2713]: training loss : 1.4210791969299317 TRAIN  loss dict:  {'classification_loss': 1.4210791969299317}
2025-01-12 15:39:51,378 [INFO] Step[650/2713]: training loss : 1.313470151424408 TRAIN  loss dict:  {'classification_loss': 1.313470151424408}
2025-01-12 15:40:03,304 [INFO] Step[700/2713]: training loss : 1.4133492159843444 TRAIN  loss dict:  {'classification_loss': 1.4133492159843444}
2025-01-12 15:40:15,219 [INFO] Step[750/2713]: training loss : 1.326505012512207 TRAIN  loss dict:  {'classification_loss': 1.326505012512207}
2025-01-12 15:40:27,139 [INFO] Step[800/2713]: training loss : 1.4133788108825684 TRAIN  loss dict:  {'classification_loss': 1.4133788108825684}
2025-01-12 15:40:39,135 [INFO] Step[850/2713]: training loss : 1.3042307925224303 TRAIN  loss dict:  {'classification_loss': 1.3042307925224303}
2025-01-12 15:40:51,031 [INFO] Step[900/2713]: training loss : 1.2992334389686584 TRAIN  loss dict:  {'classification_loss': 1.2992334389686584}
2025-01-12 15:41:02,956 [INFO] Step[950/2713]: training loss : 1.4007063627243042 TRAIN  loss dict:  {'classification_loss': 1.4007063627243042}
2025-01-12 15:41:14,917 [INFO] Step[1000/2713]: training loss : 1.3798005700111389 TRAIN  loss dict:  {'classification_loss': 1.3798005700111389}
2025-01-12 15:41:26,830 [INFO] Step[1050/2713]: training loss : 1.4073993730545045 TRAIN  loss dict:  {'classification_loss': 1.4073993730545045}
2025-01-12 15:41:38,777 [INFO] Step[1100/2713]: training loss : 1.3711676216125488 TRAIN  loss dict:  {'classification_loss': 1.3711676216125488}
2025-01-12 15:41:50,709 [INFO] Step[1150/2713]: training loss : 1.3617040014266968 TRAIN  loss dict:  {'classification_loss': 1.3617040014266968}
2025-01-12 15:42:02,626 [INFO] Step[1200/2713]: training loss : 1.4767206597328186 TRAIN  loss dict:  {'classification_loss': 1.4767206597328186}
2025-01-12 15:42:14,600 [INFO] Step[1250/2713]: training loss : 1.3870761132240295 TRAIN  loss dict:  {'classification_loss': 1.3870761132240295}
2025-01-12 15:42:26,574 [INFO] Step[1300/2713]: training loss : 1.3706025910377502 TRAIN  loss dict:  {'classification_loss': 1.3706025910377502}
2025-01-12 15:42:38,492 [INFO] Step[1350/2713]: training loss : 1.3107245182991027 TRAIN  loss dict:  {'classification_loss': 1.3107245182991027}
2025-01-12 15:42:50,427 [INFO] Step[1400/2713]: training loss : 1.4237001705169678 TRAIN  loss dict:  {'classification_loss': 1.4237001705169678}
2025-01-12 15:43:02,376 [INFO] Step[1450/2713]: training loss : 1.4172226095199585 TRAIN  loss dict:  {'classification_loss': 1.4172226095199585}
2025-01-12 15:43:14,304 [INFO] Step[1500/2713]: training loss : 1.323457477092743 TRAIN  loss dict:  {'classification_loss': 1.323457477092743}
2025-01-12 15:43:26,222 [INFO] Step[1550/2713]: training loss : 1.3852098345756532 TRAIN  loss dict:  {'classification_loss': 1.3852098345756532}
2025-01-12 15:43:38,193 [INFO] Step[1600/2713]: training loss : 1.4029281973838805 TRAIN  loss dict:  {'classification_loss': 1.4029281973838805}
2025-01-12 15:43:50,431 [INFO] Step[1650/2713]: training loss : 1.3488206934928895 TRAIN  loss dict:  {'classification_loss': 1.3488206934928895}
2025-01-12 15:44:02,947 [INFO] Step[1700/2713]: training loss : 1.3816895246505738 TRAIN  loss dict:  {'classification_loss': 1.3816895246505738}
2025-01-12 15:44:15,222 [INFO] Step[1750/2713]: training loss : 1.353022689819336 TRAIN  loss dict:  {'classification_loss': 1.353022689819336}
2025-01-12 15:44:27,626 [INFO] Step[1800/2713]: training loss : 1.320762233734131 TRAIN  loss dict:  {'classification_loss': 1.320762233734131}
2025-01-12 15:44:40,102 [INFO] Step[1850/2713]: training loss : 1.3660286736488343 TRAIN  loss dict:  {'classification_loss': 1.3660286736488343}
2025-01-12 15:44:52,501 [INFO] Step[1900/2713]: training loss : 1.3566530656814575 TRAIN  loss dict:  {'classification_loss': 1.3566530656814575}
2025-01-12 15:45:05,398 [INFO] Step[1950/2713]: training loss : 1.4201739525794983 TRAIN  loss dict:  {'classification_loss': 1.4201739525794983}
2025-01-12 15:45:19,206 [INFO] Step[2000/2713]: training loss : 1.3221911406517028 TRAIN  loss dict:  {'classification_loss': 1.3221911406517028}
2025-01-12 15:45:32,214 [INFO] Step[2050/2713]: training loss : 1.3667156147956847 TRAIN  loss dict:  {'classification_loss': 1.3667156147956847}
2025-01-12 15:45:44,084 [INFO] Step[2100/2713]: training loss : 1.4965407252311707 TRAIN  loss dict:  {'classification_loss': 1.4965407252311707}
2025-01-12 15:46:00,211 [INFO] Step[2150/2713]: training loss : 1.3090535020828247 TRAIN  loss dict:  {'classification_loss': 1.3090535020828247}
2025-01-12 15:46:12,157 [INFO] Step[2200/2713]: training loss : 1.4486881875991822 TRAIN  loss dict:  {'classification_loss': 1.4486881875991822}
2025-01-12 15:46:24,322 [INFO] Step[2250/2713]: training loss : 1.3268155455589294 TRAIN  loss dict:  {'classification_loss': 1.3268155455589294}
2025-01-12 15:46:36,186 [INFO] Step[2300/2713]: training loss : 1.3425654220581054 TRAIN  loss dict:  {'classification_loss': 1.3425654220581054}
2025-01-12 15:46:48,088 [INFO] Step[2350/2713]: training loss : 1.4081774854660034 TRAIN  loss dict:  {'classification_loss': 1.4081774854660034}
2025-01-12 15:46:59,962 [INFO] Step[2400/2713]: training loss : 1.4043647527694703 TRAIN  loss dict:  {'classification_loss': 1.4043647527694703}
2025-01-12 15:47:11,838 [INFO] Step[2450/2713]: training loss : 1.3267708706855774 TRAIN  loss dict:  {'classification_loss': 1.3267708706855774}
2025-01-12 15:47:23,766 [INFO] Step[2500/2713]: training loss : 1.2782043266296386 TRAIN  loss dict:  {'classification_loss': 1.2782043266296386}
2025-01-12 15:47:35,645 [INFO] Step[2550/2713]: training loss : 1.3902585005760193 TRAIN  loss dict:  {'classification_loss': 1.3902585005760193}
2025-01-12 15:47:47,547 [INFO] Step[2600/2713]: training loss : 1.2764254665374757 TRAIN  loss dict:  {'classification_loss': 1.2764254665374757}
2025-01-12 15:47:59,477 [INFO] Step[2650/2713]: training loss : 1.4737029266357422 TRAIN  loss dict:  {'classification_loss': 1.4737029266357422}
2025-01-12 15:48:11,438 [INFO] Step[2700/2713]: training loss : 1.3252052974700927 TRAIN  loss dict:  {'classification_loss': 1.3252052974700927}
2025-01-12 15:49:40,382 [INFO] Label accuracies statistics:
2025-01-12 15:49:40,382 [INFO] {0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.5, 8: 1.0, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.3333333333333333, 16: 0.0, 17: 0.5, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 0.75, 40: 0.25, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 1.0, 57: 0.5, 58: 0.75, 59: 0.75, 60: 1.0, 61: 0.5, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 1.0, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 0.75, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.25, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.5, 140: 0.75, 141: 1.0, 142: 0.25, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 1.0, 157: 0.25, 158: 0.6666666666666666, 159: 0.75, 160: 0.25, 161: 0.75, 162: 0.75, 163: 0.75, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 0.25, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.25, 184: 0.75, 185: 1.0, 186: 0.75, 187: 0.75, 188: 0.25, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.0, 240: 0.75, 241: 0.75, 242: 0.0, 243: 1.0, 244: 0.75, 245: 0.75, 246: 0.75, 247: 0.75, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.25, 262: 1.0, 263: 1.0, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.25, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 1.0, 289: 1.0, 290: 0.75, 291: 0.75, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.25, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.5, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.0, 329: 0.75, 330: 0.25, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 0.75, 358: 0.5, 359: 0.75, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.5, 378: 1.0, 379: 1.0, 380: 0.75, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 1.0, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.25, 395: 0.25, 396: 0.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-12 15:49:42,333 [INFO] [4] TRAIN  loss: 1.3673070480232083 acc: 0.8999877134783143
2025-01-12 15:49:42,333 [INFO] [4] TRAIN  loss dict: {'classification_loss': 1.3673070480232083}
2025-01-12 15:49:42,333 [INFO] [4] VALIDATION loss: 1.8951282729779868 VALIDATION acc: 0.7554858934169278
2025-01-12 15:49:42,333 [INFO] [4] VALIDATION loss dict: {'classification_loss': 1.8951282729779868}
2025-01-12 15:49:42,333 [INFO] 
2025-01-12 15:50:00,227 [INFO] Step[50/2713]: training loss : 1.27580091714859 TRAIN  loss dict:  {'classification_loss': 1.27580091714859}
2025-01-12 15:50:12,134 [INFO] Step[100/2713]: training loss : 1.27212468624115 TRAIN  loss dict:  {'classification_loss': 1.27212468624115}
2025-01-12 15:50:24,081 [INFO] Step[150/2713]: training loss : 1.2422316598892211 TRAIN  loss dict:  {'classification_loss': 1.2422316598892211}
2025-01-12 15:50:35,984 [INFO] Step[200/2713]: training loss : 1.3207834768295288 TRAIN  loss dict:  {'classification_loss': 1.3207834768295288}
2025-01-12 15:50:47,892 [INFO] Step[250/2713]: training loss : 1.2457428169250488 TRAIN  loss dict:  {'classification_loss': 1.2457428169250488}
2025-01-12 15:50:59,817 [INFO] Step[300/2713]: training loss : 1.2665512228012086 TRAIN  loss dict:  {'classification_loss': 1.2665512228012086}
2025-01-12 15:51:11,742 [INFO] Step[350/2713]: training loss : 1.326260724067688 TRAIN  loss dict:  {'classification_loss': 1.326260724067688}
2025-01-12 15:51:23,690 [INFO] Step[400/2713]: training loss : 1.287389304637909 TRAIN  loss dict:  {'classification_loss': 1.287389304637909}
2025-01-12 15:51:35,611 [INFO] Step[450/2713]: training loss : 1.2184031152725219 TRAIN  loss dict:  {'classification_loss': 1.2184031152725219}
2025-01-12 15:51:47,560 [INFO] Step[500/2713]: training loss : 1.2653359174728394 TRAIN  loss dict:  {'classification_loss': 1.2653359174728394}
2025-01-12 15:51:59,447 [INFO] Step[550/2713]: training loss : 1.248072645664215 TRAIN  loss dict:  {'classification_loss': 1.248072645664215}
2025-01-12 15:52:11,368 [INFO] Step[600/2713]: training loss : 1.2242918038368225 TRAIN  loss dict:  {'classification_loss': 1.2242918038368225}
2025-01-12 15:52:23,282 [INFO] Step[650/2713]: training loss : 1.2412442946434021 TRAIN  loss dict:  {'classification_loss': 1.2412442946434021}
2025-01-12 15:52:35,226 [INFO] Step[700/2713]: training loss : 1.3026913928985595 TRAIN  loss dict:  {'classification_loss': 1.3026913928985595}
2025-01-12 15:52:47,140 [INFO] Step[750/2713]: training loss : 1.3349991369247436 TRAIN  loss dict:  {'classification_loss': 1.3349991369247436}
2025-01-12 15:52:59,064 [INFO] Step[800/2713]: training loss : 1.2639120817184448 TRAIN  loss dict:  {'classification_loss': 1.2639120817184448}
2025-01-12 15:53:10,998 [INFO] Step[850/2713]: training loss : 1.3137301015853882 TRAIN  loss dict:  {'classification_loss': 1.3137301015853882}
2025-01-12 15:53:22,908 [INFO] Step[900/2713]: training loss : 1.3653186631202698 TRAIN  loss dict:  {'classification_loss': 1.3653186631202698}
2025-01-12 15:53:34,905 [INFO] Step[950/2713]: training loss : 1.2711112809181213 TRAIN  loss dict:  {'classification_loss': 1.2711112809181213}
2025-01-12 15:53:46,839 [INFO] Step[1000/2713]: training loss : 1.2297406697273254 TRAIN  loss dict:  {'classification_loss': 1.2297406697273254}
2025-01-12 15:53:58,795 [INFO] Step[1050/2713]: training loss : 1.2488405299186707 TRAIN  loss dict:  {'classification_loss': 1.2488405299186707}
2025-01-12 15:54:10,732 [INFO] Step[1100/2713]: training loss : 1.330262610912323 TRAIN  loss dict:  {'classification_loss': 1.330262610912323}
2025-01-12 15:54:22,637 [INFO] Step[1150/2713]: training loss : 1.3295445513725281 TRAIN  loss dict:  {'classification_loss': 1.3295445513725281}
2025-01-12 15:54:34,524 [INFO] Step[1200/2713]: training loss : 1.3307929468154907 TRAIN  loss dict:  {'classification_loss': 1.3307929468154907}
2025-01-12 15:54:46,434 [INFO] Step[1250/2713]: training loss : 1.3150723218917846 TRAIN  loss dict:  {'classification_loss': 1.3150723218917846}
2025-01-12 15:54:58,382 [INFO] Step[1300/2713]: training loss : 1.21774587392807 TRAIN  loss dict:  {'classification_loss': 1.21774587392807}
2025-01-12 15:55:10,281 [INFO] Step[1350/2713]: training loss : 1.303603205680847 TRAIN  loss dict:  {'classification_loss': 1.303603205680847}
2025-01-12 15:55:22,182 [INFO] Step[1400/2713]: training loss : 1.3008273029327393 TRAIN  loss dict:  {'classification_loss': 1.3008273029327393}
2025-01-12 15:55:34,072 [INFO] Step[1450/2713]: training loss : 1.2818622922897338 TRAIN  loss dict:  {'classification_loss': 1.2818622922897338}
2025-01-12 15:55:45,937 [INFO] Step[1500/2713]: training loss : 1.2821456646919251 TRAIN  loss dict:  {'classification_loss': 1.2821456646919251}
2025-01-12 15:55:57,886 [INFO] Step[1550/2713]: training loss : 1.3935336875915527 TRAIN  loss dict:  {'classification_loss': 1.3935336875915527}
2025-01-12 15:56:09,801 [INFO] Step[1600/2713]: training loss : 1.3839454245567322 TRAIN  loss dict:  {'classification_loss': 1.3839454245567322}
2025-01-12 15:56:21,721 [INFO] Step[1650/2713]: training loss : 1.3721023774147034 TRAIN  loss dict:  {'classification_loss': 1.3721023774147034}
2025-01-12 15:56:33,653 [INFO] Step[1700/2713]: training loss : 1.342520122528076 TRAIN  loss dict:  {'classification_loss': 1.342520122528076}
2025-01-12 15:56:45,600 [INFO] Step[1750/2713]: training loss : 1.3861537170410156 TRAIN  loss dict:  {'classification_loss': 1.3861537170410156}
2025-01-12 15:56:57,531 [INFO] Step[1800/2713]: training loss : 1.3035504221916199 TRAIN  loss dict:  {'classification_loss': 1.3035504221916199}
2025-01-12 15:57:09,459 [INFO] Step[1850/2713]: training loss : 1.3894658279418945 TRAIN  loss dict:  {'classification_loss': 1.3894658279418945}
2025-01-12 15:57:21,355 [INFO] Step[1900/2713]: training loss : 1.3166491198539734 TRAIN  loss dict:  {'classification_loss': 1.3166491198539734}
2025-01-12 15:57:33,286 [INFO] Step[1950/2713]: training loss : 1.2720118355751038 TRAIN  loss dict:  {'classification_loss': 1.2720118355751038}
2025-01-12 15:57:45,228 [INFO] Step[2000/2713]: training loss : 1.339854395389557 TRAIN  loss dict:  {'classification_loss': 1.339854395389557}
2025-01-12 15:57:57,149 [INFO] Step[2050/2713]: training loss : 1.271094675064087 TRAIN  loss dict:  {'classification_loss': 1.271094675064087}
2025-01-12 15:58:09,069 [INFO] Step[2100/2713]: training loss : 1.3250836992263795 TRAIN  loss dict:  {'classification_loss': 1.3250836992263795}
2025-01-12 15:58:20,973 [INFO] Step[2150/2713]: training loss : 1.2683967494964599 TRAIN  loss dict:  {'classification_loss': 1.2683967494964599}
2025-01-12 15:58:32,922 [INFO] Step[2200/2713]: training loss : 1.3182169389724732 TRAIN  loss dict:  {'classification_loss': 1.3182169389724732}
2025-01-12 15:58:44,849 [INFO] Step[2250/2713]: training loss : 1.3808374333381652 TRAIN  loss dict:  {'classification_loss': 1.3808374333381652}
2025-01-12 15:58:56,755 [INFO] Step[2300/2713]: training loss : 1.2807926392555238 TRAIN  loss dict:  {'classification_loss': 1.2807926392555238}
2025-01-12 15:59:08,648 [INFO] Step[2350/2713]: training loss : 1.2483080697059632 TRAIN  loss dict:  {'classification_loss': 1.2483080697059632}
2025-01-12 15:59:20,558 [INFO] Step[2400/2713]: training loss : 1.2897688055038452 TRAIN  loss dict:  {'classification_loss': 1.2897688055038452}
2025-01-12 15:59:32,501 [INFO] Step[2450/2713]: training loss : 1.3174977016448974 TRAIN  loss dict:  {'classification_loss': 1.3174977016448974}
2025-01-12 15:59:44,385 [INFO] Step[2500/2713]: training loss : 1.3502975368499757 TRAIN  loss dict:  {'classification_loss': 1.3502975368499757}
2025-01-12 15:59:56,319 [INFO] Step[2550/2713]: training loss : 1.4069763326644897 TRAIN  loss dict:  {'classification_loss': 1.4069763326644897}
2025-01-12 16:00:08,218 [INFO] Step[2600/2713]: training loss : 1.439423108100891 TRAIN  loss dict:  {'classification_loss': 1.439423108100891}
2025-01-12 16:00:20,167 [INFO] Step[2650/2713]: training loss : 1.3732189202308656 TRAIN  loss dict:  {'classification_loss': 1.3732189202308656}
2025-01-12 16:00:32,048 [INFO] Step[2700/2713]: training loss : 1.2869480061531067 TRAIN  loss dict:  {'classification_loss': 1.2869480061531067}
2025-01-12 16:02:00,238 [INFO] Label accuracies statistics:
2025-01-12 16:02:00,239 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.5, 12: 0.25, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.5, 26: 0.25, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.25, 98: 0.75, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 0.5, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.5, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 1.0, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.25, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.5, 140: 1.0, 141: 0.5, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.25, 158: 0.3333333333333333, 159: 1.0, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.5, 170: 1.0, 171: 0.5, 172: 0.75, 173: 0.5, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.25, 185: 0.75, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 0.75, 192: 0.75, 193: 1.0, 194: 0.75, 195: 0.75, 196: 0.75, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.25, 201: 0.5, 202: 0.5, 203: 0.0, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.5, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.5, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.5, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.0, 230: 1.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.5, 237: 0.75, 238: 1.0, 239: 0.75, 240: 0.75, 241: 0.75, 242: 0.25, 243: 0.25, 244: 0.75, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.5, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.25, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.5, 278: 0.5, 279: 0.5, 280: 0.75, 281: 0.5, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.5, 288: 0.75, 289: 0.75, 290: 0.0, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.25, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.5, 303: 0.5, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.25, 311: 1.0, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.25, 326: 1.0, 327: 0.5, 328: 0.25, 329: 1.0, 330: 0.5, 331: 0.5, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.25, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.0, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.25, 354: 1.0, 355: 1.0, 356: 0.0, 357: 1.0, 358: 0.75, 359: 0.5, 360: 1.0, 361: 1.0, 362: 0.5, 363: 0.5, 364: 0.5, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.25, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.5, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.5, 389: 0.5, 390: 0.5, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 16:02:00,240 [INFO] [5] TRAIN  loss: 1.3060757433669488 acc: 0.92013760904288
2025-01-12 16:02:00,240 [INFO] [5] TRAIN  loss dict: {'classification_loss': 1.3060757433669488}
2025-01-12 16:02:00,241 [INFO] [5] VALIDATION loss: 1.945646380795572 VALIDATION acc: 0.7379310344827587
2025-01-12 16:02:00,241 [INFO] [5] VALIDATION loss dict: {'classification_loss': 1.945646380795572}
2025-01-12 16:02:00,241 [INFO] 
2025-01-12 16:02:17,612 [INFO] Step[50/2713]: training loss : 1.2300868582725526 TRAIN  loss dict:  {'classification_loss': 1.2300868582725526}
2025-01-12 16:02:29,537 [INFO] Step[100/2713]: training loss : 1.2944373416900634 TRAIN  loss dict:  {'classification_loss': 1.2944373416900634}
2025-01-12 16:02:41,471 [INFO] Step[150/2713]: training loss : 1.1968275225162506 TRAIN  loss dict:  {'classification_loss': 1.1968275225162506}
2025-01-12 16:02:53,413 [INFO] Step[200/2713]: training loss : 1.2291842484474182 TRAIN  loss dict:  {'classification_loss': 1.2291842484474182}
2025-01-12 16:03:05,311 [INFO] Step[250/2713]: training loss : 1.2269322896003723 TRAIN  loss dict:  {'classification_loss': 1.2269322896003723}
2025-01-12 16:03:17,212 [INFO] Step[300/2713]: training loss : 1.2277442121505737 TRAIN  loss dict:  {'classification_loss': 1.2277442121505737}
2025-01-12 16:03:29,362 [INFO] Step[350/2713]: training loss : 1.2934717774391173 TRAIN  loss dict:  {'classification_loss': 1.2934717774391173}
2025-01-12 16:03:41,704 [INFO] Step[400/2713]: training loss : 1.209583010673523 TRAIN  loss dict:  {'classification_loss': 1.209583010673523}
2025-01-12 16:03:54,073 [INFO] Step[450/2713]: training loss : 1.255178346633911 TRAIN  loss dict:  {'classification_loss': 1.255178346633911}
2025-01-12 16:04:06,404 [INFO] Step[500/2713]: training loss : 1.2357573413848877 TRAIN  loss dict:  {'classification_loss': 1.2357573413848877}
2025-01-12 16:04:19,095 [INFO] Step[550/2713]: training loss : 1.246343083381653 TRAIN  loss dict:  {'classification_loss': 1.246343083381653}
2025-01-12 16:04:31,377 [INFO] Step[600/2713]: training loss : 1.1863467216491699 TRAIN  loss dict:  {'classification_loss': 1.1863467216491699}
2025-01-12 16:04:44,143 [INFO] Step[650/2713]: training loss : 1.2767436909675598 TRAIN  loss dict:  {'classification_loss': 1.2767436909675598}
2025-01-12 16:04:57,825 [INFO] Step[700/2713]: training loss : 1.2523767638206482 TRAIN  loss dict:  {'classification_loss': 1.2523767638206482}
2025-01-12 16:05:11,391 [INFO] Step[750/2713]: training loss : 1.2280388045310975 TRAIN  loss dict:  {'classification_loss': 1.2280388045310975}
2025-01-12 16:05:23,252 [INFO] Step[800/2713]: training loss : 1.2985792255401611 TRAIN  loss dict:  {'classification_loss': 1.2985792255401611}
2025-01-12 16:05:35,334 [INFO] Step[850/2713]: training loss : 1.3845343041419982 TRAIN  loss dict:  {'classification_loss': 1.3845343041419982}
2025-01-12 16:05:47,199 [INFO] Step[900/2713]: training loss : 1.2698561930656433 TRAIN  loss dict:  {'classification_loss': 1.2698561930656433}
2025-01-12 16:05:59,081 [INFO] Step[950/2713]: training loss : 1.3571357369422912 TRAIN  loss dict:  {'classification_loss': 1.3571357369422912}
2025-01-12 16:06:10,995 [INFO] Step[1000/2713]: training loss : 1.279436559677124 TRAIN  loss dict:  {'classification_loss': 1.279436559677124}
2025-01-12 16:06:22,873 [INFO] Step[1050/2713]: training loss : 1.3065814781188965 TRAIN  loss dict:  {'classification_loss': 1.3065814781188965}
2025-01-12 16:06:34,708 [INFO] Step[1100/2713]: training loss : 1.2128241682052612 TRAIN  loss dict:  {'classification_loss': 1.2128241682052612}
2025-01-12 16:06:46,613 [INFO] Step[1150/2713]: training loss : 1.2281314063072204 TRAIN  loss dict:  {'classification_loss': 1.2281314063072204}
2025-01-12 16:06:58,512 [INFO] Step[1200/2713]: training loss : 1.2904488825798035 TRAIN  loss dict:  {'classification_loss': 1.2904488825798035}
2025-01-12 16:07:10,433 [INFO] Step[1250/2713]: training loss : 1.2847697234153748 TRAIN  loss dict:  {'classification_loss': 1.2847697234153748}
2025-01-12 16:07:22,319 [INFO] Step[1300/2713]: training loss : 1.2222994709014892 TRAIN  loss dict:  {'classification_loss': 1.2222994709014892}
2025-01-12 16:07:34,217 [INFO] Step[1350/2713]: training loss : 1.244827263355255 TRAIN  loss dict:  {'classification_loss': 1.244827263355255}
2025-01-12 16:07:46,083 [INFO] Step[1400/2713]: training loss : 1.2811380434036255 TRAIN  loss dict:  {'classification_loss': 1.2811380434036255}
2025-01-12 16:07:58,010 [INFO] Step[1450/2713]: training loss : 1.39338445186615 TRAIN  loss dict:  {'classification_loss': 1.39338445186615}
2025-01-12 16:08:09,946 [INFO] Step[1500/2713]: training loss : 1.2363639640808106 TRAIN  loss dict:  {'classification_loss': 1.2363639640808106}
2025-01-12 16:08:21,824 [INFO] Step[1550/2713]: training loss : 1.3001486206054687 TRAIN  loss dict:  {'classification_loss': 1.3001486206054687}
2025-01-12 16:08:33,732 [INFO] Step[1600/2713]: training loss : 1.2349399709701538 TRAIN  loss dict:  {'classification_loss': 1.2349399709701538}
2025-01-12 16:08:45,650 [INFO] Step[1650/2713]: training loss : 1.2411072826385499 TRAIN  loss dict:  {'classification_loss': 1.2411072826385499}
2025-01-12 16:08:57,538 [INFO] Step[1700/2713]: training loss : 1.3745926666259765 TRAIN  loss dict:  {'classification_loss': 1.3745926666259765}
2025-01-12 16:09:09,487 [INFO] Step[1750/2713]: training loss : 1.2588618731498717 TRAIN  loss dict:  {'classification_loss': 1.2588618731498717}
2025-01-12 16:09:21,333 [INFO] Step[1800/2713]: training loss : 1.3014383387565613 TRAIN  loss dict:  {'classification_loss': 1.3014383387565613}
2025-01-12 16:09:33,240 [INFO] Step[1850/2713]: training loss : 1.2685954999923705 TRAIN  loss dict:  {'classification_loss': 1.2685954999923705}
2025-01-12 16:09:45,126 [INFO] Step[1900/2713]: training loss : 1.3356034135818482 TRAIN  loss dict:  {'classification_loss': 1.3356034135818482}
2025-01-12 16:09:57,030 [INFO] Step[1950/2713]: training loss : 1.254697253704071 TRAIN  loss dict:  {'classification_loss': 1.254697253704071}
2025-01-12 16:10:09,012 [INFO] Step[2000/2713]: training loss : 1.225709319114685 TRAIN  loss dict:  {'classification_loss': 1.225709319114685}
2025-01-12 16:10:20,909 [INFO] Step[2050/2713]: training loss : 1.2824649786949158 TRAIN  loss dict:  {'classification_loss': 1.2824649786949158}
2025-01-12 16:10:32,762 [INFO] Step[2100/2713]: training loss : 1.2529094076156617 TRAIN  loss dict:  {'classification_loss': 1.2529094076156617}
2025-01-12 16:10:44,875 [INFO] Step[2150/2713]: training loss : 1.2347810745239258 TRAIN  loss dict:  {'classification_loss': 1.2347810745239258}
2025-01-12 16:10:56,777 [INFO] Step[2200/2713]: training loss : 1.2053682827949523 TRAIN  loss dict:  {'classification_loss': 1.2053682827949523}
2025-01-12 16:11:08,738 [INFO] Step[2250/2713]: training loss : 1.2464604830741883 TRAIN  loss dict:  {'classification_loss': 1.2464604830741883}
2025-01-12 16:11:20,642 [INFO] Step[2300/2713]: training loss : 1.2922965288162231 TRAIN  loss dict:  {'classification_loss': 1.2922965288162231}
2025-01-12 16:11:32,573 [INFO] Step[2350/2713]: training loss : 1.2937433266639708 TRAIN  loss dict:  {'classification_loss': 1.2937433266639708}
2025-01-12 16:11:44,660 [INFO] Step[2400/2713]: training loss : 1.2338229060173034 TRAIN  loss dict:  {'classification_loss': 1.2338229060173034}
2025-01-12 16:11:56,605 [INFO] Step[2450/2713]: training loss : 1.2371970784664155 TRAIN  loss dict:  {'classification_loss': 1.2371970784664155}
2025-01-12 16:12:08,503 [INFO] Step[2500/2713]: training loss : 1.285436737537384 TRAIN  loss dict:  {'classification_loss': 1.285436737537384}
2025-01-12 16:12:20,782 [INFO] Step[2550/2713]: training loss : 1.2542043209075928 TRAIN  loss dict:  {'classification_loss': 1.2542043209075928}
2025-01-12 16:12:32,727 [INFO] Step[2600/2713]: training loss : 1.2417512321472168 TRAIN  loss dict:  {'classification_loss': 1.2417512321472168}
2025-01-12 16:12:44,818 [INFO] Step[2650/2713]: training loss : 1.2950379419326783 TRAIN  loss dict:  {'classification_loss': 1.2950379419326783}
2025-01-12 16:12:56,875 [INFO] Step[2700/2713]: training loss : 1.303813364505768 TRAIN  loss dict:  {'classification_loss': 1.303813364505768}
2025-01-12 16:14:25,397 [INFO] Label accuracies statistics:
2025-01-12 16:14:25,397 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 1.0, 19: 0.5, 20: 0.75, 21: 1.0, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 0.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.5, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 0.75, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.5, 46: 1.0, 47: 1.0, 48: 0.5, 49: 0.75, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.5, 59: 0.5, 60: 1.0, 61: 0.5, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 0.75, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.75, 98: 0.75, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.5, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.25, 143: 1.0, 144: 0.75, 145: 0.5, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.5, 155: 1.0, 156: 1.0, 157: 0.5, 158: 0.3333333333333333, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 0.75, 164: 0.25, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.5, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.5, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.5, 196: 1.0, 197: 0.75, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.25, 203: 0.5, 204: 0.75, 205: 0.5, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.5, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.5, 217: 0.75, 218: 0.5, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.25, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.25, 229: 0.25, 230: 0.75, 231: 0.25, 232: 0.5, 233: 0.5, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.0, 240: 1.0, 241: 0.75, 242: 0.75, 243: 0.25, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 1.0, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 0.75, 266: 0.75, 267: 0.5, 268: 0.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.0, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.5, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 1.0, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.5, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 0.5, 330: 0.5, 331: 0.75, 332: 1.0, 333: 1.0, 334: 0.75, 335: 0.75, 336: 0.5, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.75, 354: 0.75, 355: 1.0, 356: 0.25, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 0.75, 387: 1.0, 388: 0.75, 389: 0.5, 390: 0.5, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 0.75, 399: 0.5}

2025-01-12 16:14:27,359 [INFO] [6] TRAIN  loss: 1.2664106221731588 acc: 0.9331613220297333
2025-01-12 16:14:27,359 [INFO] [6] TRAIN  loss dict: {'classification_loss': 1.2664106221731588}
2025-01-12 16:14:27,359 [INFO] [6] VALIDATION loss: 1.8473049550128162 VALIDATION acc: 0.7579937304075235
2025-01-12 16:14:27,359 [INFO] [6] VALIDATION loss dict: {'classification_loss': 1.8473049550128162}
2025-01-12 16:14:27,359 [INFO] 
2025-01-12 16:14:44,970 [INFO] Step[50/2713]: training loss : 1.2645325946807862 TRAIN  loss dict:  {'classification_loss': 1.2645325946807862}
2025-01-12 16:14:56,845 [INFO] Step[100/2713]: training loss : 1.2386246705055237 TRAIN  loss dict:  {'classification_loss': 1.2386246705055237}
2025-01-12 16:15:08,777 [INFO] Step[150/2713]: training loss : 1.1608565282821655 TRAIN  loss dict:  {'classification_loss': 1.1608565282821655}
2025-01-12 16:15:20,710 [INFO] Step[200/2713]: training loss : 1.2211293148994447 TRAIN  loss dict:  {'classification_loss': 1.2211293148994447}
2025-01-12 16:15:32,721 [INFO] Step[250/2713]: training loss : 1.2192059588432311 TRAIN  loss dict:  {'classification_loss': 1.2192059588432311}
2025-01-12 16:15:44,647 [INFO] Step[300/2713]: training loss : 1.1960353302955626 TRAIN  loss dict:  {'classification_loss': 1.1960353302955626}
2025-01-12 16:15:56,603 [INFO] Step[350/2713]: training loss : 1.1519282746315003 TRAIN  loss dict:  {'classification_loss': 1.1519282746315003}
2025-01-12 16:16:08,561 [INFO] Step[400/2713]: training loss : 1.2867478024959564 TRAIN  loss dict:  {'classification_loss': 1.2867478024959564}
2025-01-12 16:16:20,505 [INFO] Step[450/2713]: training loss : 1.191360626220703 TRAIN  loss dict:  {'classification_loss': 1.191360626220703}
2025-01-12 16:16:32,433 [INFO] Step[500/2713]: training loss : 1.2062506031990052 TRAIN  loss dict:  {'classification_loss': 1.2062506031990052}
2025-01-12 16:16:44,353 [INFO] Step[550/2713]: training loss : 1.2513016772270202 TRAIN  loss dict:  {'classification_loss': 1.2513016772270202}
2025-01-12 16:16:56,254 [INFO] Step[600/2713]: training loss : 1.2398101234436034 TRAIN  loss dict:  {'classification_loss': 1.2398101234436034}
2025-01-12 16:17:08,141 [INFO] Step[650/2713]: training loss : 1.1815493965148927 TRAIN  loss dict:  {'classification_loss': 1.1815493965148927}
2025-01-12 16:17:20,098 [INFO] Step[700/2713]: training loss : 1.2538729238510131 TRAIN  loss dict:  {'classification_loss': 1.2538729238510131}
2025-01-12 16:17:32,010 [INFO] Step[750/2713]: training loss : 1.148214989900589 TRAIN  loss dict:  {'classification_loss': 1.148214989900589}
2025-01-12 16:17:43,942 [INFO] Step[800/2713]: training loss : 1.1715582346916198 TRAIN  loss dict:  {'classification_loss': 1.1715582346916198}
2025-01-12 16:17:55,872 [INFO] Step[850/2713]: training loss : 1.1690113568305969 TRAIN  loss dict:  {'classification_loss': 1.1690113568305969}
2025-01-12 16:18:07,800 [INFO] Step[900/2713]: training loss : 1.1497271823883057 TRAIN  loss dict:  {'classification_loss': 1.1497271823883057}
2025-01-12 16:18:19,701 [INFO] Step[950/2713]: training loss : 1.253828911781311 TRAIN  loss dict:  {'classification_loss': 1.253828911781311}
2025-01-12 16:18:31,663 [INFO] Step[1000/2713]: training loss : 1.2235210418701172 TRAIN  loss dict:  {'classification_loss': 1.2235210418701172}
2025-01-12 16:18:43,863 [INFO] Step[1050/2713]: training loss : 1.2830364060401918 TRAIN  loss dict:  {'classification_loss': 1.2830364060401918}
2025-01-12 16:18:55,955 [INFO] Step[1100/2713]: training loss : 1.2806557369232179 TRAIN  loss dict:  {'classification_loss': 1.2806557369232179}
2025-01-12 16:19:07,943 [INFO] Step[1150/2713]: training loss : 1.2807752561569214 TRAIN  loss dict:  {'classification_loss': 1.2807752561569214}
2025-01-12 16:19:19,977 [INFO] Step[1200/2713]: training loss : 1.2175745868682861 TRAIN  loss dict:  {'classification_loss': 1.2175745868682861}
2025-01-12 16:19:32,096 [INFO] Step[1250/2713]: training loss : 1.2297995257377625 TRAIN  loss dict:  {'classification_loss': 1.2297995257377625}
2025-01-12 16:19:44,094 [INFO] Step[1300/2713]: training loss : 1.20984881401062 TRAIN  loss dict:  {'classification_loss': 1.20984881401062}
2025-01-12 16:19:56,210 [INFO] Step[1350/2713]: training loss : 1.2385037779808044 TRAIN  loss dict:  {'classification_loss': 1.2385037779808044}
2025-01-12 16:20:08,140 [INFO] Step[1400/2713]: training loss : 1.2842677187919618 TRAIN  loss dict:  {'classification_loss': 1.2842677187919618}
2025-01-12 16:20:20,162 [INFO] Step[1450/2713]: training loss : 1.2685011053085327 TRAIN  loss dict:  {'classification_loss': 1.2685011053085327}
2025-01-12 16:20:32,079 [INFO] Step[1500/2713]: training loss : 1.219178593158722 TRAIN  loss dict:  {'classification_loss': 1.219178593158722}
2025-01-12 16:20:44,035 [INFO] Step[1550/2713]: training loss : 1.1581331849098206 TRAIN  loss dict:  {'classification_loss': 1.1581331849098206}
2025-01-12 16:20:56,002 [INFO] Step[1600/2713]: training loss : 1.1967046856880188 TRAIN  loss dict:  {'classification_loss': 1.1967046856880188}
2025-01-12 16:21:07,961 [INFO] Step[1650/2713]: training loss : 1.2228483200073241 TRAIN  loss dict:  {'classification_loss': 1.2228483200073241}
2025-01-12 16:21:19,889 [INFO] Step[1700/2713]: training loss : 1.2512049174308777 TRAIN  loss dict:  {'classification_loss': 1.2512049174308777}
2025-01-12 16:21:31,842 [INFO] Step[1750/2713]: training loss : 1.2480034935474396 TRAIN  loss dict:  {'classification_loss': 1.2480034935474396}
2025-01-12 16:21:43,773 [INFO] Step[1800/2713]: training loss : 1.2312956285476684 TRAIN  loss dict:  {'classification_loss': 1.2312956285476684}
2025-01-12 16:21:55,760 [INFO] Step[1850/2713]: training loss : 1.3168062615394591 TRAIN  loss dict:  {'classification_loss': 1.3168062615394591}
2025-01-12 16:22:07,680 [INFO] Step[1900/2713]: training loss : 1.2355877888202667 TRAIN  loss dict:  {'classification_loss': 1.2355877888202667}
2025-01-12 16:22:19,578 [INFO] Step[1950/2713]: training loss : 1.2000185799598695 TRAIN  loss dict:  {'classification_loss': 1.2000185799598695}
2025-01-12 16:22:31,587 [INFO] Step[2000/2713]: training loss : 1.3239064598083496 TRAIN  loss dict:  {'classification_loss': 1.3239064598083496}
2025-01-12 16:22:43,969 [INFO] Step[2050/2713]: training loss : 1.1950145006179809 TRAIN  loss dict:  {'classification_loss': 1.1950145006179809}
2025-01-12 16:22:56,427 [INFO] Step[2100/2713]: training loss : 1.2325827956199646 TRAIN  loss dict:  {'classification_loss': 1.2325827956199646}
2025-01-12 16:23:08,843 [INFO] Step[2150/2713]: training loss : 1.297234206199646 TRAIN  loss dict:  {'classification_loss': 1.297234206199646}
2025-01-12 16:23:21,402 [INFO] Step[2200/2713]: training loss : 1.2501560294628142 TRAIN  loss dict:  {'classification_loss': 1.2501560294628142}
2025-01-12 16:23:34,028 [INFO] Step[2250/2713]: training loss : 1.2669176197052001 TRAIN  loss dict:  {'classification_loss': 1.2669176197052001}
2025-01-12 16:23:46,655 [INFO] Step[2300/2713]: training loss : 1.2292939615249634 TRAIN  loss dict:  {'classification_loss': 1.2292939615249634}
2025-01-12 16:24:00,111 [INFO] Step[2350/2713]: training loss : 1.1595835185050964 TRAIN  loss dict:  {'classification_loss': 1.1595835185050964}
2025-01-12 16:24:13,620 [INFO] Step[2400/2713]: training loss : 1.1835210967063903 TRAIN  loss dict:  {'classification_loss': 1.1835210967063903}
2025-01-12 16:24:26,315 [INFO] Step[2450/2713]: training loss : 1.2595049929618836 TRAIN  loss dict:  {'classification_loss': 1.2595049929618836}
2025-01-12 16:24:38,364 [INFO] Step[2500/2713]: training loss : 1.273769736289978 TRAIN  loss dict:  {'classification_loss': 1.273769736289978}
2025-01-12 16:24:50,409 [INFO] Step[2550/2713]: training loss : 1.2641887068748474 TRAIN  loss dict:  {'classification_loss': 1.2641887068748474}
2025-01-12 16:25:02,412 [INFO] Step[2600/2713]: training loss : 1.195128779411316 TRAIN  loss dict:  {'classification_loss': 1.195128779411316}
2025-01-12 16:25:14,388 [INFO] Step[2650/2713]: training loss : 1.2805340933799743 TRAIN  loss dict:  {'classification_loss': 1.2805340933799743}
2025-01-12 16:25:26,338 [INFO] Step[2700/2713]: training loss : 1.2147630047798157 TRAIN  loss dict:  {'classification_loss': 1.2147630047798157}
2025-01-12 16:26:54,656 [INFO] Label accuracies statistics:
2025-01-12 16:26:54,656 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.0, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.25, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.5, 63: 0.75, 64: 0.5, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 1.0, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 1.0, 83: 0.5, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.25, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 0.8, 100: 0.75, 101: 0.5, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.25, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.25, 115: 0.5, 116: 0.5, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 1.0, 122: 1.0, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.25, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.25, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 0.75, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.0, 190: 0.25, 191: 0.25, 192: 0.75, 193: 1.0, 194: 0.75, 195: 0.5, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.5, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 1.0, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.5, 240: 0.75, 241: 0.75, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 0.75, 256: 0.5, 257: 0.75, 258: 1.0, 259: 0.75, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.5, 278: 0.75, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 0.5, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.5, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.5, 327: 0.75, 328: 1.0, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 1.0, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.25, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.0, 350: 0.75, 351: 0.75, 352: 0.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.0, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.5, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.5, 374: 0.75, 375: 0.25, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 0.75, 383: 0.25, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.5, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-12 16:26:54,658 [INFO] [7] TRAIN  loss: 1.2289552618514292 acc: 0.9467993611008724
2025-01-12 16:26:54,658 [INFO] [7] TRAIN  loss dict: {'classification_loss': 1.2289552618514292}
2025-01-12 16:26:54,658 [INFO] [7] VALIDATION loss: 1.979208715540126 VALIDATION acc: 0.7310344827586207
2025-01-12 16:26:54,658 [INFO] [7] VALIDATION loss dict: {'classification_loss': 1.979208715540126}
2025-01-12 16:26:54,658 [INFO] 
2025-01-12 16:27:12,626 [INFO] Step[50/2713]: training loss : 1.1823091781139374 TRAIN  loss dict:  {'classification_loss': 1.1823091781139374}
2025-01-12 16:27:24,543 [INFO] Step[100/2713]: training loss : 1.1655217432975769 TRAIN  loss dict:  {'classification_loss': 1.1655217432975769}
2025-01-12 16:27:36,375 [INFO] Step[150/2713]: training loss : 1.1995937490463258 TRAIN  loss dict:  {'classification_loss': 1.1995937490463258}
2025-01-12 16:27:48,324 [INFO] Step[200/2713]: training loss : 1.1842863130569459 TRAIN  loss dict:  {'classification_loss': 1.1842863130569459}
2025-01-12 16:28:00,206 [INFO] Step[250/2713]: training loss : 1.1492263555526734 TRAIN  loss dict:  {'classification_loss': 1.1492263555526734}
2025-01-12 16:28:12,081 [INFO] Step[300/2713]: training loss : 1.2037328839302064 TRAIN  loss dict:  {'classification_loss': 1.2037328839302064}
2025-01-12 16:28:23,984 [INFO] Step[350/2713]: training loss : 1.222588210105896 TRAIN  loss dict:  {'classification_loss': 1.222588210105896}
2025-01-12 16:28:35,909 [INFO] Step[400/2713]: training loss : 1.1893412494659423 TRAIN  loss dict:  {'classification_loss': 1.1893412494659423}
2025-01-12 16:28:47,834 [INFO] Step[450/2713]: training loss : 1.155891445875168 TRAIN  loss dict:  {'classification_loss': 1.155891445875168}
2025-01-12 16:28:59,848 [INFO] Step[500/2713]: training loss : 1.1443028616905213 TRAIN  loss dict:  {'classification_loss': 1.1443028616905213}
2025-01-12 16:29:11,715 [INFO] Step[550/2713]: training loss : 1.1704250121116637 TRAIN  loss dict:  {'classification_loss': 1.1704250121116637}
2025-01-12 16:29:23,596 [INFO] Step[600/2713]: training loss : 1.2059285140037537 TRAIN  loss dict:  {'classification_loss': 1.2059285140037537}
2025-01-12 16:29:35,429 [INFO] Step[650/2713]: training loss : 1.208950400352478 TRAIN  loss dict:  {'classification_loss': 1.208950400352478}
2025-01-12 16:29:47,239 [INFO] Step[700/2713]: training loss : 1.1600676894187927 TRAIN  loss dict:  {'classification_loss': 1.1600676894187927}
2025-01-12 16:29:59,026 [INFO] Step[750/2713]: training loss : 1.2105446696281432 TRAIN  loss dict:  {'classification_loss': 1.2105446696281432}
2025-01-12 16:30:10,804 [INFO] Step[800/2713]: training loss : 1.2388283860683442 TRAIN  loss dict:  {'classification_loss': 1.2388283860683442}
2025-01-12 16:30:22,620 [INFO] Step[850/2713]: training loss : 1.18318887591362 TRAIN  loss dict:  {'classification_loss': 1.18318887591362}
2025-01-12 16:30:34,432 [INFO] Step[900/2713]: training loss : 1.185092614889145 TRAIN  loss dict:  {'classification_loss': 1.185092614889145}
2025-01-12 16:30:46,192 [INFO] Step[950/2713]: training loss : 1.1505002522468566 TRAIN  loss dict:  {'classification_loss': 1.1505002522468566}
2025-01-12 16:30:57,958 [INFO] Step[1000/2713]: training loss : 1.252779748439789 TRAIN  loss dict:  {'classification_loss': 1.252779748439789}
2025-01-12 16:31:09,761 [INFO] Step[1050/2713]: training loss : 1.2450304174423217 TRAIN  loss dict:  {'classification_loss': 1.2450304174423217}
2025-01-12 16:31:21,511 [INFO] Step[1100/2713]: training loss : 1.2134355258941651 TRAIN  loss dict:  {'classification_loss': 1.2134355258941651}
2025-01-12 16:31:33,328 [INFO] Step[1150/2713]: training loss : 1.2368537747859956 TRAIN  loss dict:  {'classification_loss': 1.2368537747859956}
2025-01-12 16:31:45,102 [INFO] Step[1200/2713]: training loss : 1.1506204545497893 TRAIN  loss dict:  {'classification_loss': 1.1506204545497893}
2025-01-12 16:31:56,910 [INFO] Step[1250/2713]: training loss : 1.1356749296188355 TRAIN  loss dict:  {'classification_loss': 1.1356749296188355}
2025-01-12 16:32:08,663 [INFO] Step[1300/2713]: training loss : 1.2107886505126952 TRAIN  loss dict:  {'classification_loss': 1.2107886505126952}
2025-01-12 16:32:20,461 [INFO] Step[1350/2713]: training loss : 1.2334636425971985 TRAIN  loss dict:  {'classification_loss': 1.2334636425971985}
2025-01-12 16:32:32,258 [INFO] Step[1400/2713]: training loss : 1.188819043636322 TRAIN  loss dict:  {'classification_loss': 1.188819043636322}
2025-01-12 16:32:44,054 [INFO] Step[1450/2713]: training loss : 1.232336735725403 TRAIN  loss dict:  {'classification_loss': 1.232336735725403}
2025-01-12 16:32:55,841 [INFO] Step[1500/2713]: training loss : 1.2222033429145813 TRAIN  loss dict:  {'classification_loss': 1.2222033429145813}
2025-01-12 16:33:07,646 [INFO] Step[1550/2713]: training loss : 1.254806478023529 TRAIN  loss dict:  {'classification_loss': 1.254806478023529}
2025-01-12 16:33:19,471 [INFO] Step[1600/2713]: training loss : 1.1286309957504272 TRAIN  loss dict:  {'classification_loss': 1.1286309957504272}
2025-01-12 16:33:31,232 [INFO] Step[1650/2713]: training loss : 1.3022013998031616 TRAIN  loss dict:  {'classification_loss': 1.3022013998031616}
2025-01-12 16:33:43,006 [INFO] Step[1700/2713]: training loss : 1.2783889317512511 TRAIN  loss dict:  {'classification_loss': 1.2783889317512511}
2025-01-12 16:33:54,833 [INFO] Step[1750/2713]: training loss : 1.1732097709178924 TRAIN  loss dict:  {'classification_loss': 1.1732097709178924}
2025-01-12 16:34:06,582 [INFO] Step[1800/2713]: training loss : 1.2537676906585693 TRAIN  loss dict:  {'classification_loss': 1.2537676906585693}
2025-01-12 16:34:18,410 [INFO] Step[1850/2713]: training loss : 1.2577021145820617 TRAIN  loss dict:  {'classification_loss': 1.2577021145820617}
2025-01-12 16:34:30,146 [INFO] Step[1900/2713]: training loss : 1.1684119200706482 TRAIN  loss dict:  {'classification_loss': 1.1684119200706482}
2025-01-12 16:34:42,004 [INFO] Step[1950/2713]: training loss : 1.2679114270210265 TRAIN  loss dict:  {'classification_loss': 1.2679114270210265}
2025-01-12 16:34:53,820 [INFO] Step[2000/2713]: training loss : 1.2325314092636108 TRAIN  loss dict:  {'classification_loss': 1.2325314092636108}
2025-01-12 16:35:05,623 [INFO] Step[2050/2713]: training loss : 1.3185281205177306 TRAIN  loss dict:  {'classification_loss': 1.3185281205177306}
2025-01-12 16:35:17,403 [INFO] Step[2100/2713]: training loss : 1.1597745513916016 TRAIN  loss dict:  {'classification_loss': 1.1597745513916016}
2025-01-12 16:35:29,257 [INFO] Step[2150/2713]: training loss : 1.2905863952636718 TRAIN  loss dict:  {'classification_loss': 1.2905863952636718}
2025-01-12 16:35:41,001 [INFO] Step[2200/2713]: training loss : 1.218280725479126 TRAIN  loss dict:  {'classification_loss': 1.218280725479126}
2025-01-12 16:35:52,796 [INFO] Step[2250/2713]: training loss : 1.2999859631061554 TRAIN  loss dict:  {'classification_loss': 1.2999859631061554}
2025-01-12 16:36:04,570 [INFO] Step[2300/2713]: training loss : 1.2580303525924683 TRAIN  loss dict:  {'classification_loss': 1.2580303525924683}
2025-01-12 16:36:16,342 [INFO] Step[2350/2713]: training loss : 1.2242767822742462 TRAIN  loss dict:  {'classification_loss': 1.2242767822742462}
2025-01-12 16:36:28,136 [INFO] Step[2400/2713]: training loss : 1.2954996395111085 TRAIN  loss dict:  {'classification_loss': 1.2954996395111085}
2025-01-12 16:36:39,940 [INFO] Step[2450/2713]: training loss : 1.2214424562454225 TRAIN  loss dict:  {'classification_loss': 1.2214424562454225}
2025-01-12 16:36:51,722 [INFO] Step[2500/2713]: training loss : 1.2007996726036072 TRAIN  loss dict:  {'classification_loss': 1.2007996726036072}
2025-01-12 16:37:03,521 [INFO] Step[2550/2713]: training loss : 1.301313223838806 TRAIN  loss dict:  {'classification_loss': 1.301313223838806}
2025-01-12 16:37:15,311 [INFO] Step[2600/2713]: training loss : 1.2154983401298523 TRAIN  loss dict:  {'classification_loss': 1.2154983401298523}
2025-01-12 16:37:27,127 [INFO] Step[2650/2713]: training loss : 1.224190253019333 TRAIN  loss dict:  {'classification_loss': 1.224190253019333}
2025-01-12 16:37:38,885 [INFO] Step[2700/2713]: training loss : 1.2282950401306152 TRAIN  loss dict:  {'classification_loss': 1.2282950401306152}
2025-01-12 16:39:04,872 [INFO] Label accuracies statistics:
2025-01-12 16:39:04,872 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.25, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.5, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.5, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.5, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.25, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.0, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 0.6, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 0.75, 107: 0.5, 108: 1.0, 109: 0.25, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.5, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 0.75, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.25, 154: 0.75, 155: 1.0, 156: 0.5, 157: 0.5, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.25, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.25, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.25, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.5, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.25, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.75, 267: 1.0, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.5, 277: 0.75, 278: 1.0, 279: 0.5, 280: 1.0, 281: 0.5, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.5, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 0.5, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.25, 334: 0.75, 335: 1.0, 336: 0.5, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.5, 341: 0.25, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 0.75, 348: 0.5, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.5, 357: 0.75, 358: 0.75, 359: 1.0, 360: 0.75, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 16:39:06,865 [INFO] [8] TRAIN  loss: 1.2155970547896138 acc: 0.9488880697874432
2025-01-12 16:39:06,865 [INFO] [8] TRAIN  loss dict: {'classification_loss': 1.2155970547896138}
2025-01-12 16:39:06,865 [INFO] [8] VALIDATION loss: 1.8339013972676785 VALIDATION acc: 0.768025078369906
2025-01-12 16:39:06,865 [INFO] [8] VALIDATION loss dict: {'classification_loss': 1.8339013972676785}
2025-01-12 16:39:06,866 [INFO] 
2025-01-12 16:39:24,180 [INFO] Step[50/2713]: training loss : 1.1498099315166472 TRAIN  loss dict:  {'classification_loss': 1.1498099315166472}
2025-01-12 16:39:35,910 [INFO] Step[100/2713]: training loss : 1.1821375489234924 TRAIN  loss dict:  {'classification_loss': 1.1821375489234924}
2025-01-12 16:39:47,664 [INFO] Step[150/2713]: training loss : 1.2019239830970765 TRAIN  loss dict:  {'classification_loss': 1.2019239830970765}
2025-01-12 16:39:59,412 [INFO] Step[200/2713]: training loss : 1.2285139989852905 TRAIN  loss dict:  {'classification_loss': 1.2285139989852905}
2025-01-12 16:40:11,179 [INFO] Step[250/2713]: training loss : 1.2291460204124451 TRAIN  loss dict:  {'classification_loss': 1.2291460204124451}
2025-01-12 16:40:22,911 [INFO] Step[300/2713]: training loss : 1.1640317416191102 TRAIN  loss dict:  {'classification_loss': 1.1640317416191102}
2025-01-12 16:40:34,675 [INFO] Step[350/2713]: training loss : 1.1850616574287414 TRAIN  loss dict:  {'classification_loss': 1.1850616574287414}
2025-01-12 16:40:46,415 [INFO] Step[400/2713]: training loss : 1.1634886407852172 TRAIN  loss dict:  {'classification_loss': 1.1634886407852172}
2025-01-12 16:40:58,183 [INFO] Step[450/2713]: training loss : 1.2110351252555847 TRAIN  loss dict:  {'classification_loss': 1.2110351252555847}
2025-01-12 16:41:09,960 [INFO] Step[500/2713]: training loss : 1.2003166830539704 TRAIN  loss dict:  {'classification_loss': 1.2003166830539704}
2025-01-12 16:41:21,703 [INFO] Step[550/2713]: training loss : 1.1142531394958497 TRAIN  loss dict:  {'classification_loss': 1.1142531394958497}
2025-01-12 16:41:33,541 [INFO] Step[600/2713]: training loss : 1.1497875022888184 TRAIN  loss dict:  {'classification_loss': 1.1497875022888184}
2025-01-12 16:41:45,714 [INFO] Step[650/2713]: training loss : 1.199809616804123 TRAIN  loss dict:  {'classification_loss': 1.199809616804123}
2025-01-12 16:41:57,956 [INFO] Step[700/2713]: training loss : 1.2353013229370118 TRAIN  loss dict:  {'classification_loss': 1.2353013229370118}
2025-01-12 16:42:10,118 [INFO] Step[750/2713]: training loss : 1.2246204233169555 TRAIN  loss dict:  {'classification_loss': 1.2246204233169555}
2025-01-12 16:42:22,492 [INFO] Step[800/2713]: training loss : 1.189414882659912 TRAIN  loss dict:  {'classification_loss': 1.189414882659912}
2025-01-12 16:42:34,721 [INFO] Step[850/2713]: training loss : 1.1843945074081421 TRAIN  loss dict:  {'classification_loss': 1.1843945074081421}
2025-01-12 16:42:47,043 [INFO] Step[900/2713]: training loss : 1.137583601474762 TRAIN  loss dict:  {'classification_loss': 1.137583601474762}
2025-01-12 16:42:59,892 [INFO] Step[950/2713]: training loss : 1.1799435031414032 TRAIN  loss dict:  {'classification_loss': 1.1799435031414032}
2025-01-12 16:43:13,527 [INFO] Step[1000/2713]: training loss : 1.1936721897125244 TRAIN  loss dict:  {'classification_loss': 1.1936721897125244}
2025-01-12 16:43:26,206 [INFO] Step[1050/2713]: training loss : 1.210412573814392 TRAIN  loss dict:  {'classification_loss': 1.210412573814392}
2025-01-12 16:43:38,041 [INFO] Step[1100/2713]: training loss : 1.1622977375984191 TRAIN  loss dict:  {'classification_loss': 1.1622977375984191}
2025-01-12 16:43:49,753 [INFO] Step[1150/2713]: training loss : 1.130105222463608 TRAIN  loss dict:  {'classification_loss': 1.130105222463608}
2025-01-12 16:44:01,429 [INFO] Step[1200/2713]: training loss : 1.1471706235408783 TRAIN  loss dict:  {'classification_loss': 1.1471706235408783}
2025-01-12 16:44:13,166 [INFO] Step[1250/2713]: training loss : 1.1476792299747467 TRAIN  loss dict:  {'classification_loss': 1.1476792299747467}
2025-01-12 16:44:24,883 [INFO] Step[1300/2713]: training loss : 1.1669027829170227 TRAIN  loss dict:  {'classification_loss': 1.1669027829170227}
2025-01-12 16:44:36,596 [INFO] Step[1350/2713]: training loss : 1.1632264041900635 TRAIN  loss dict:  {'classification_loss': 1.1632264041900635}
2025-01-12 16:44:48,341 [INFO] Step[1400/2713]: training loss : 1.1478117632865905 TRAIN  loss dict:  {'classification_loss': 1.1478117632865905}
2025-01-12 16:45:00,034 [INFO] Step[1450/2713]: training loss : 1.149628838300705 TRAIN  loss dict:  {'classification_loss': 1.149628838300705}
2025-01-12 16:45:11,742 [INFO] Step[1500/2713]: training loss : 1.1562054979801177 TRAIN  loss dict:  {'classification_loss': 1.1562054979801177}
2025-01-12 16:45:23,459 [INFO] Step[1550/2713]: training loss : 1.108505860567093 TRAIN  loss dict:  {'classification_loss': 1.108505860567093}
2025-01-12 16:45:35,234 [INFO] Step[1600/2713]: training loss : 1.1983950960636138 TRAIN  loss dict:  {'classification_loss': 1.1983950960636138}
2025-01-12 16:45:47,005 [INFO] Step[1650/2713]: training loss : 1.2115339386463164 TRAIN  loss dict:  {'classification_loss': 1.2115339386463164}
2025-01-12 16:45:58,746 [INFO] Step[1700/2713]: training loss : 1.2085476160049438 TRAIN  loss dict:  {'classification_loss': 1.2085476160049438}
2025-01-12 16:46:10,522 [INFO] Step[1750/2713]: training loss : 1.1942885386943818 TRAIN  loss dict:  {'classification_loss': 1.1942885386943818}
2025-01-12 16:46:22,276 [INFO] Step[1800/2713]: training loss : 1.1903671598434449 TRAIN  loss dict:  {'classification_loss': 1.1903671598434449}
2025-01-12 16:46:34,104 [INFO] Step[1850/2713]: training loss : 1.140542871952057 TRAIN  loss dict:  {'classification_loss': 1.140542871952057}
2025-01-12 16:46:45,840 [INFO] Step[1900/2713]: training loss : 1.1372180354595185 TRAIN  loss dict:  {'classification_loss': 1.1372180354595185}
2025-01-12 16:46:57,613 [INFO] Step[1950/2713]: training loss : 1.212085587978363 TRAIN  loss dict:  {'classification_loss': 1.212085587978363}
2025-01-12 16:47:09,364 [INFO] Step[2000/2713]: training loss : 1.2151604402065277 TRAIN  loss dict:  {'classification_loss': 1.2151604402065277}
2025-01-12 16:47:21,132 [INFO] Step[2050/2713]: training loss : 1.167332661151886 TRAIN  loss dict:  {'classification_loss': 1.167332661151886}
2025-01-12 16:47:32,903 [INFO] Step[2100/2713]: training loss : 1.2322504639625549 TRAIN  loss dict:  {'classification_loss': 1.2322504639625549}
2025-01-12 16:47:44,666 [INFO] Step[2150/2713]: training loss : 1.3008480596542358 TRAIN  loss dict:  {'classification_loss': 1.3008480596542358}
2025-01-12 16:47:56,442 [INFO] Step[2200/2713]: training loss : 1.2185955703258515 TRAIN  loss dict:  {'classification_loss': 1.2185955703258515}
2025-01-12 16:48:08,176 [INFO] Step[2250/2713]: training loss : 1.1476521968841553 TRAIN  loss dict:  {'classification_loss': 1.1476521968841553}
2025-01-12 16:48:19,885 [INFO] Step[2300/2713]: training loss : 1.1873743152618408 TRAIN  loss dict:  {'classification_loss': 1.1873743152618408}
2025-01-12 16:48:31,597 [INFO] Step[2350/2713]: training loss : 1.1937003016471863 TRAIN  loss dict:  {'classification_loss': 1.1937003016471863}
2025-01-12 16:48:43,342 [INFO] Step[2400/2713]: training loss : 1.1764021289348603 TRAIN  loss dict:  {'classification_loss': 1.1764021289348603}
2025-01-12 16:48:55,072 [INFO] Step[2450/2713]: training loss : 1.2351312065124511 TRAIN  loss dict:  {'classification_loss': 1.2351312065124511}
2025-01-12 16:49:06,830 [INFO] Step[2500/2713]: training loss : 1.2107981419563294 TRAIN  loss dict:  {'classification_loss': 1.2107981419563294}
2025-01-12 16:49:18,796 [INFO] Step[2550/2713]: training loss : 1.2054191195964814 TRAIN  loss dict:  {'classification_loss': 1.2054191195964814}
2025-01-12 16:49:30,836 [INFO] Step[2600/2713]: training loss : 1.196444399356842 TRAIN  loss dict:  {'classification_loss': 1.196444399356842}
2025-01-12 16:49:42,926 [INFO] Step[2650/2713]: training loss : 1.235676987171173 TRAIN  loss dict:  {'classification_loss': 1.235676987171173}
2025-01-12 16:49:54,881 [INFO] Step[2700/2713]: training loss : 1.1568369805812835 TRAIN  loss dict:  {'classification_loss': 1.1568369805812835}
2025-01-12 16:51:22,628 [INFO] Label accuracies statistics:
2025-01-12 16:51:22,629 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.25, 21: 0.5, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.25, 55: 1.0, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.5, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.5, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.75, 98: 1.0, 99: 0.8, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.75, 115: 0.75, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 1.0, 122: 1.0, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.5, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 0.75, 141: 1.0, 142: 1.0, 143: 0.5, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 0.75, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 0.75, 178: 0.25, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 0.5, 184: 0.75, 185: 1.0, 186: 0.75, 187: 0.75, 188: 0.25, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.5, 206: 0.5, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.0, 212: 0.5, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.5, 230: 0.75, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 1.0, 241: 0.5, 242: 1.0, 243: 0.0, 244: 0.5, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.25, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 0.75, 266: 0.75, 267: 0.75, 268: 0.5, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.5, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.5, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.75, 292: 0.75, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.25, 297: 0.5, 298: 0.75, 299: 0.5, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.5, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 0.75, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.0, 391: 1.0, 392: 0.75, 393: 0.0, 394: 0.25, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 16:51:23,674 [INFO] [9] TRAIN  loss: 1.1848261953938108 acc: 0.9552770610640128
2025-01-12 16:51:23,674 [INFO] [9] TRAIN  loss dict: {'classification_loss': 1.1848261953938108}
2025-01-12 16:51:23,674 [INFO] [9] VALIDATION loss: 1.8319949339655108 VALIDATION acc: 0.7655172413793103
2025-01-12 16:51:23,674 [INFO] [9] VALIDATION loss dict: {'classification_loss': 1.8319949339655108}
2025-01-12 16:51:23,674 [INFO] 
2025-01-12 16:51:41,604 [INFO] Step[50/2713]: training loss : 1.176776292324066 TRAIN  loss dict:  {'classification_loss': 1.176776292324066}
2025-01-12 16:51:53,479 [INFO] Step[100/2713]: training loss : 1.158511483669281 TRAIN  loss dict:  {'classification_loss': 1.158511483669281}
2025-01-12 16:52:05,400 [INFO] Step[150/2713]: training loss : 1.1518229269981384 TRAIN  loss dict:  {'classification_loss': 1.1518229269981384}
2025-01-12 16:52:17,337 [INFO] Step[200/2713]: training loss : 1.1712981688976287 TRAIN  loss dict:  {'classification_loss': 1.1712981688976287}
2025-01-12 16:52:29,265 [INFO] Step[250/2713]: training loss : 1.1348588597774505 TRAIN  loss dict:  {'classification_loss': 1.1348588597774505}
2025-01-12 16:52:41,188 [INFO] Step[300/2713]: training loss : 1.131294560432434 TRAIN  loss dict:  {'classification_loss': 1.131294560432434}
2025-01-12 16:52:53,077 [INFO] Step[350/2713]: training loss : 1.1308671402931214 TRAIN  loss dict:  {'classification_loss': 1.1308671402931214}
2025-01-12 16:53:05,082 [INFO] Step[400/2713]: training loss : 1.149338811635971 TRAIN  loss dict:  {'classification_loss': 1.149338811635971}
2025-01-12 16:53:16,985 [INFO] Step[450/2713]: training loss : 1.1931511521339417 TRAIN  loss dict:  {'classification_loss': 1.1931511521339417}
2025-01-12 16:53:28,891 [INFO] Step[500/2713]: training loss : 1.174011882543564 TRAIN  loss dict:  {'classification_loss': 1.174011882543564}
2025-01-12 16:53:40,755 [INFO] Step[550/2713]: training loss : 1.1894548845291137 TRAIN  loss dict:  {'classification_loss': 1.1894548845291137}
2025-01-12 16:53:52,692 [INFO] Step[600/2713]: training loss : 1.171606457233429 TRAIN  loss dict:  {'classification_loss': 1.171606457233429}
2025-01-12 16:54:04,569 [INFO] Step[650/2713]: training loss : 1.119961986541748 TRAIN  loss dict:  {'classification_loss': 1.119961986541748}
2025-01-12 16:54:16,502 [INFO] Step[700/2713]: training loss : 1.1768556010723115 TRAIN  loss dict:  {'classification_loss': 1.1768556010723115}
2025-01-12 16:54:28,465 [INFO] Step[750/2713]: training loss : 1.1950975787639617 TRAIN  loss dict:  {'classification_loss': 1.1950975787639617}
2025-01-12 16:54:40,400 [INFO] Step[800/2713]: training loss : 1.157313916683197 TRAIN  loss dict:  {'classification_loss': 1.157313916683197}
2025-01-12 16:54:52,291 [INFO] Step[850/2713]: training loss : 1.1985498368740082 TRAIN  loss dict:  {'classification_loss': 1.1985498368740082}
2025-01-12 16:55:04,222 [INFO] Step[900/2713]: training loss : 1.160566599369049 TRAIN  loss dict:  {'classification_loss': 1.160566599369049}
2025-01-12 16:55:16,103 [INFO] Step[950/2713]: training loss : 1.121491916179657 TRAIN  loss dict:  {'classification_loss': 1.121491916179657}
2025-01-12 16:55:28,026 [INFO] Step[1000/2713]: training loss : 1.13425563454628 TRAIN  loss dict:  {'classification_loss': 1.13425563454628}
2025-01-12 16:55:39,916 [INFO] Step[1050/2713]: training loss : 1.2419871115684509 TRAIN  loss dict:  {'classification_loss': 1.2419871115684509}
2025-01-12 16:55:51,856 [INFO] Step[1100/2713]: training loss : 1.1613834297657013 TRAIN  loss dict:  {'classification_loss': 1.1613834297657013}
2025-01-12 16:56:03,762 [INFO] Step[1150/2713]: training loss : 1.1924995255470277 TRAIN  loss dict:  {'classification_loss': 1.1924995255470277}
2025-01-12 16:56:15,660 [INFO] Step[1200/2713]: training loss : 1.2266974401474 TRAIN  loss dict:  {'classification_loss': 1.2266974401474}
2025-01-12 16:56:27,596 [INFO] Step[1250/2713]: training loss : 1.2078800654411317 TRAIN  loss dict:  {'classification_loss': 1.2078800654411317}
2025-01-12 16:56:39,485 [INFO] Step[1300/2713]: training loss : 1.1531703805923461 TRAIN  loss dict:  {'classification_loss': 1.1531703805923461}
2025-01-12 16:56:51,408 [INFO] Step[1350/2713]: training loss : 1.123832972049713 TRAIN  loss dict:  {'classification_loss': 1.123832972049713}
2025-01-12 16:57:03,360 [INFO] Step[1400/2713]: training loss : 1.1151287484169006 TRAIN  loss dict:  {'classification_loss': 1.1151287484169006}
2025-01-12 16:57:15,309 [INFO] Step[1450/2713]: training loss : 1.1679206538200377 TRAIN  loss dict:  {'classification_loss': 1.1679206538200377}
2025-01-12 16:57:27,187 [INFO] Step[1500/2713]: training loss : 1.1957117319107056 TRAIN  loss dict:  {'classification_loss': 1.1957117319107056}
2025-01-12 16:57:39,106 [INFO] Step[1550/2713]: training loss : 1.2056119096279145 TRAIN  loss dict:  {'classification_loss': 1.2056119096279145}
2025-01-12 16:57:51,009 [INFO] Step[1600/2713]: training loss : 1.209896981716156 TRAIN  loss dict:  {'classification_loss': 1.209896981716156}
2025-01-12 16:58:02,924 [INFO] Step[1650/2713]: training loss : 1.1670486295223237 TRAIN  loss dict:  {'classification_loss': 1.1670486295223237}
2025-01-12 16:58:14,808 [INFO] Step[1700/2713]: training loss : 1.208212058544159 TRAIN  loss dict:  {'classification_loss': 1.208212058544159}
2025-01-12 16:58:26,721 [INFO] Step[1750/2713]: training loss : 1.137075572013855 TRAIN  loss dict:  {'classification_loss': 1.137075572013855}
2025-01-12 16:58:38,637 [INFO] Step[1800/2713]: training loss : 1.1762183439731597 TRAIN  loss dict:  {'classification_loss': 1.1762183439731597}
2025-01-12 16:58:50,565 [INFO] Step[1850/2713]: training loss : 1.1228800511360169 TRAIN  loss dict:  {'classification_loss': 1.1228800511360169}
2025-01-12 16:59:02,472 [INFO] Step[1900/2713]: training loss : 1.1978075838088988 TRAIN  loss dict:  {'classification_loss': 1.1978075838088988}
2025-01-12 16:59:14,359 [INFO] Step[1950/2713]: training loss : 1.1653244769573212 TRAIN  loss dict:  {'classification_loss': 1.1653244769573212}
2025-01-12 16:59:26,272 [INFO] Step[2000/2713]: training loss : 1.2615230393409729 TRAIN  loss dict:  {'classification_loss': 1.2615230393409729}
2025-01-12 16:59:38,165 [INFO] Step[2050/2713]: training loss : 1.1232331836223601 TRAIN  loss dict:  {'classification_loss': 1.1232331836223601}
2025-01-12 16:59:50,062 [INFO] Step[2100/2713]: training loss : 1.1667967224121094 TRAIN  loss dict:  {'classification_loss': 1.1667967224121094}
2025-01-12 17:00:01,998 [INFO] Step[2150/2713]: training loss : 1.140323110818863 TRAIN  loss dict:  {'classification_loss': 1.140323110818863}
2025-01-12 17:00:13,899 [INFO] Step[2200/2713]: training loss : 1.2099547815322875 TRAIN  loss dict:  {'classification_loss': 1.2099547815322875}
2025-01-12 17:00:25,840 [INFO] Step[2250/2713]: training loss : 1.1619524002075194 TRAIN  loss dict:  {'classification_loss': 1.1619524002075194}
2025-01-12 17:00:38,236 [INFO] Step[2300/2713]: training loss : 1.1176371026039122 TRAIN  loss dict:  {'classification_loss': 1.1176371026039122}
2025-01-12 17:00:50,696 [INFO] Step[2350/2713]: training loss : 1.1618749344348906 TRAIN  loss dict:  {'classification_loss': 1.1618749344348906}
2025-01-12 17:01:03,030 [INFO] Step[2400/2713]: training loss : 1.2035634183883668 TRAIN  loss dict:  {'classification_loss': 1.2035634183883668}
2025-01-12 17:01:15,704 [INFO] Step[2450/2713]: training loss : 1.1894582056999206 TRAIN  loss dict:  {'classification_loss': 1.1894582056999206}
2025-01-12 17:01:28,108 [INFO] Step[2500/2713]: training loss : 1.1882597887516022 TRAIN  loss dict:  {'classification_loss': 1.1882597887516022}
2025-01-12 17:01:40,724 [INFO] Step[2550/2713]: training loss : 1.1591477417945861 TRAIN  loss dict:  {'classification_loss': 1.1591477417945861}
2025-01-12 17:01:53,988 [INFO] Step[2600/2713]: training loss : 1.121338564157486 TRAIN  loss dict:  {'classification_loss': 1.121338564157486}
2025-01-12 17:02:07,434 [INFO] Step[2650/2713]: training loss : 1.1551380503177642 TRAIN  loss dict:  {'classification_loss': 1.1551380503177642}
2025-01-12 17:02:20,016 [INFO] Step[2700/2713]: training loss : 1.1492772793769837 TRAIN  loss dict:  {'classification_loss': 1.1492772793769837}
2025-01-12 17:03:48,345 [INFO] Label accuracies statistics:
2025-01-12 17:03:48,345 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 0.75, 6: 0.75, 7: 0.25, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.0, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 0.5, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 1.0, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 1.0, 84: 1.0, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.5, 118: 1.0, 119: 1.0, 120: 1.0, 121: 1.0, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.5, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.5, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.5, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 0.5, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.0, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.5, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.0, 212: 0.75, 213: 0.25, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.5, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.5, 245: 0.75, 246: 0.75, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 0.5, 253: 0.75, 254: 1.0, 255: 0.75, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.5, 261: 0.5, 262: 0.75, 263: 0.75, 264: 1.0, 265: 0.75, 266: 0.75, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.75, 274: 1.0, 275: 0.5, 276: 0.5, 277: 0.75, 278: 0.5, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.25, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 0.75, 309: 0.75, 310: 0.5, 311: 0.5, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.5, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.75, 327: 1.0, 328: 1.0, 329: 0.75, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.0, 334: 0.75, 335: 0.5, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.5, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.75, 350: 0.25, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.25, 371: 0.75, 372: 0.5, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.5, 378: 0.5, 379: 1.0, 380: 0.75, 381: 0.5, 382: 1.0, 383: 0.75, 384: 1.0, 385: 0.75, 386: 0.5, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.5}

2025-01-12 17:03:48,347 [INFO] [10] TRAIN  loss: 1.1679721725325367 acc: 0.9626489740754393
2025-01-12 17:03:48,347 [INFO] [10] TRAIN  loss dict: {'classification_loss': 1.1679721725325367}
2025-01-12 17:03:48,347 [INFO] [10] VALIDATION loss: 1.9095158590409989 VALIDATION acc: 0.7561128526645768
2025-01-12 17:03:48,347 [INFO] [10] VALIDATION loss dict: {'classification_loss': 1.9095158590409989}
2025-01-12 17:03:48,347 [INFO] 
2025-01-12 17:04:05,821 [INFO] Step[50/2713]: training loss : 1.1447402405738831 TRAIN  loss dict:  {'classification_loss': 1.1447402405738831}
2025-01-12 17:04:17,713 [INFO] Step[100/2713]: training loss : 1.1405191946029662 TRAIN  loss dict:  {'classification_loss': 1.1405191946029662}
2025-01-12 17:04:29,565 [INFO] Step[150/2713]: training loss : 1.189143579006195 TRAIN  loss dict:  {'classification_loss': 1.189143579006195}
2025-01-12 17:04:41,486 [INFO] Step[200/2713]: training loss : 1.1235258734226228 TRAIN  loss dict:  {'classification_loss': 1.1235258734226228}
2025-01-12 17:04:53,395 [INFO] Step[250/2713]: training loss : 1.1118403792381286 TRAIN  loss dict:  {'classification_loss': 1.1118403792381286}
2025-01-12 17:05:05,263 [INFO] Step[300/2713]: training loss : 1.1335534453392029 TRAIN  loss dict:  {'classification_loss': 1.1335534453392029}
2025-01-12 17:05:17,201 [INFO] Step[350/2713]: training loss : 1.144430834054947 TRAIN  loss dict:  {'classification_loss': 1.144430834054947}
2025-01-12 17:05:29,106 [INFO] Step[400/2713]: training loss : 1.1024858915805817 TRAIN  loss dict:  {'classification_loss': 1.1024858915805817}
2025-01-12 17:05:41,046 [INFO] Step[450/2713]: training loss : 1.126170904636383 TRAIN  loss dict:  {'classification_loss': 1.126170904636383}
2025-01-12 17:05:52,904 [INFO] Step[500/2713]: training loss : 1.1508597421646118 TRAIN  loss dict:  {'classification_loss': 1.1508597421646118}
2025-01-12 17:06:04,781 [INFO] Step[550/2713]: training loss : 1.1436160922050476 TRAIN  loss dict:  {'classification_loss': 1.1436160922050476}
2025-01-12 17:06:16,660 [INFO] Step[600/2713]: training loss : 1.1513141179084778 TRAIN  loss dict:  {'classification_loss': 1.1513141179084778}
2025-01-12 17:06:28,655 [INFO] Step[650/2713]: training loss : 1.1210615825653076 TRAIN  loss dict:  {'classification_loss': 1.1210615825653076}
2025-01-12 17:06:40,501 [INFO] Step[700/2713]: training loss : 1.1157519471645356 TRAIN  loss dict:  {'classification_loss': 1.1157519471645356}
2025-01-12 17:06:52,428 [INFO] Step[750/2713]: training loss : 1.1075293123722076 TRAIN  loss dict:  {'classification_loss': 1.1075293123722076}
2025-01-12 17:07:04,270 [INFO] Step[800/2713]: training loss : 1.0912416553497315 TRAIN  loss dict:  {'classification_loss': 1.0912416553497315}
2025-01-12 17:07:16,165 [INFO] Step[850/2713]: training loss : 1.0974398803710939 TRAIN  loss dict:  {'classification_loss': 1.0974398803710939}
2025-01-12 17:07:28,089 [INFO] Step[900/2713]: training loss : 1.1393271708488464 TRAIN  loss dict:  {'classification_loss': 1.1393271708488464}
2025-01-12 17:07:40,015 [INFO] Step[950/2713]: training loss : 1.1173912918567657 TRAIN  loss dict:  {'classification_loss': 1.1173912918567657}
2025-01-12 17:07:51,924 [INFO] Step[1000/2713]: training loss : 1.123546712398529 TRAIN  loss dict:  {'classification_loss': 1.123546712398529}
2025-01-12 17:08:03,822 [INFO] Step[1050/2713]: training loss : 1.0948648130893708 TRAIN  loss dict:  {'classification_loss': 1.0948648130893708}
2025-01-12 17:08:15,734 [INFO] Step[1100/2713]: training loss : 1.0955484747886657 TRAIN  loss dict:  {'classification_loss': 1.0955484747886657}
2025-01-12 17:08:27,616 [INFO] Step[1150/2713]: training loss : 1.109429636001587 TRAIN  loss dict:  {'classification_loss': 1.109429636001587}
2025-01-12 17:08:39,542 [INFO] Step[1200/2713]: training loss : 1.128498648405075 TRAIN  loss dict:  {'classification_loss': 1.128498648405075}
2025-01-12 17:08:51,441 [INFO] Step[1250/2713]: training loss : 1.1368408918380737 TRAIN  loss dict:  {'classification_loss': 1.1368408918380737}
2025-01-12 17:09:03,355 [INFO] Step[1300/2713]: training loss : 1.0885201942920686 TRAIN  loss dict:  {'classification_loss': 1.0885201942920686}
2025-01-12 17:09:15,265 [INFO] Step[1350/2713]: training loss : 1.113899267911911 TRAIN  loss dict:  {'classification_loss': 1.113899267911911}
2025-01-12 17:09:27,155 [INFO] Step[1400/2713]: training loss : 1.0967056214809419 TRAIN  loss dict:  {'classification_loss': 1.0967056214809419}
2025-01-12 17:09:39,056 [INFO] Step[1450/2713]: training loss : 1.0725396597385406 TRAIN  loss dict:  {'classification_loss': 1.0725396597385406}
2025-01-12 17:09:50,932 [INFO] Step[1500/2713]: training loss : 1.0970116257667542 TRAIN  loss dict:  {'classification_loss': 1.0970116257667542}
2025-01-12 17:10:02,840 [INFO] Step[1550/2713]: training loss : 1.0937953114509582 TRAIN  loss dict:  {'classification_loss': 1.0937953114509582}
2025-01-12 17:10:14,762 [INFO] Step[1600/2713]: training loss : 1.0764342939853668 TRAIN  loss dict:  {'classification_loss': 1.0764342939853668}
2025-01-12 17:10:26,666 [INFO] Step[1650/2713]: training loss : 1.153945564031601 TRAIN  loss dict:  {'classification_loss': 1.153945564031601}
2025-01-12 17:10:38,537 [INFO] Step[1700/2713]: training loss : 1.1272158801555634 TRAIN  loss dict:  {'classification_loss': 1.1272158801555634}
2025-01-12 17:10:50,486 [INFO] Step[1750/2713]: training loss : 1.1198710536956786 TRAIN  loss dict:  {'classification_loss': 1.1198710536956786}
2025-01-12 17:11:02,374 [INFO] Step[1800/2713]: training loss : 1.152146726846695 TRAIN  loss dict:  {'classification_loss': 1.152146726846695}
2025-01-12 17:11:14,301 [INFO] Step[1850/2713]: training loss : 1.0491799926757812 TRAIN  loss dict:  {'classification_loss': 1.0491799926757812}
2025-01-12 17:11:26,195 [INFO] Step[1900/2713]: training loss : 1.1378595948219299 TRAIN  loss dict:  {'classification_loss': 1.1378595948219299}
2025-01-12 17:11:38,090 [INFO] Step[1950/2713]: training loss : 1.1453849720954894 TRAIN  loss dict:  {'classification_loss': 1.1453849720954894}
2025-01-12 17:11:49,951 [INFO] Step[2000/2713]: training loss : 1.1365639436244965 TRAIN  loss dict:  {'classification_loss': 1.1365639436244965}
2025-01-12 17:12:01,858 [INFO] Step[2050/2713]: training loss : 1.1283879911899566 TRAIN  loss dict:  {'classification_loss': 1.1283879911899566}
2025-01-12 17:12:13,752 [INFO] Step[2100/2713]: training loss : 1.1457567453384399 TRAIN  loss dict:  {'classification_loss': 1.1457567453384399}
2025-01-12 17:12:25,671 [INFO] Step[2150/2713]: training loss : 1.1237445163726807 TRAIN  loss dict:  {'classification_loss': 1.1237445163726807}
2025-01-12 17:12:37,585 [INFO] Step[2200/2713]: training loss : 1.1468869364261627 TRAIN  loss dict:  {'classification_loss': 1.1468869364261627}
2025-01-12 17:12:49,536 [INFO] Step[2250/2713]: training loss : 1.1421897041797637 TRAIN  loss dict:  {'classification_loss': 1.1421897041797637}
2025-01-12 17:13:01,398 [INFO] Step[2300/2713]: training loss : 1.1023992955684663 TRAIN  loss dict:  {'classification_loss': 1.1023992955684663}
2025-01-12 17:13:13,280 [INFO] Step[2350/2713]: training loss : 1.1046782743930816 TRAIN  loss dict:  {'classification_loss': 1.1046782743930816}
2025-01-12 17:13:25,150 [INFO] Step[2400/2713]: training loss : 1.0690895402431488 TRAIN  loss dict:  {'classification_loss': 1.0690895402431488}
2025-01-12 17:13:37,055 [INFO] Step[2450/2713]: training loss : 1.24420649766922 TRAIN  loss dict:  {'classification_loss': 1.24420649766922}
2025-01-12 17:13:48,940 [INFO] Step[2500/2713]: training loss : 1.1323798179626465 TRAIN  loss dict:  {'classification_loss': 1.1323798179626465}
2025-01-12 17:14:00,868 [INFO] Step[2550/2713]: training loss : 1.1409001970291137 TRAIN  loss dict:  {'classification_loss': 1.1409001970291137}
2025-01-12 17:14:12,794 [INFO] Step[2600/2713]: training loss : 1.1081685054302215 TRAIN  loss dict:  {'classification_loss': 1.1081685054302215}
2025-01-12 17:14:24,698 [INFO] Step[2650/2713]: training loss : 1.1413264799118041 TRAIN  loss dict:  {'classification_loss': 1.1413264799118041}
2025-01-12 17:14:36,589 [INFO] Step[2700/2713]: training loss : 1.0984422624111176 TRAIN  loss dict:  {'classification_loss': 1.0984422624111176}
2025-01-12 17:16:03,868 [INFO] Label accuracies statistics:
2025-01-12 17:16:03,868 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 0.5, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 1.0, 61: 0.5, 62: 0.75, 63: 0.25, 64: 0.75, 65: 0.75, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 1.0, 91: 1.0, 92: 0.75, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.75, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 0.5, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 0.75, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.25, 154: 1.0, 155: 1.0, 156: 0.25, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.5, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.5, 218: 0.5, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 1.0, 234: 0.75, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 0.5, 260: 0.75, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.5, 267: 0.5, 268: 0.75, 269: 0.5, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 1.0, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.25, 284: 0.75, 285: 0.5, 286: 0.75, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 0.5, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 0.75, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.25, 394: 0.75, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 17:16:21,917 [INFO] [11] TRAIN  loss: 1.1227951149328845 acc: 0.9745669001105787
2025-01-12 17:16:21,917 [INFO] [11] TRAIN  loss dict: {'classification_loss': 1.1227951149328845}
2025-01-12 17:16:21,917 [INFO] [11] VALIDATION loss: 1.7870833229525644 VALIDATION acc: 0.7931034482758621
2025-01-12 17:16:21,917 [INFO] [11] VALIDATION loss dict: {'classification_loss': 1.7870833229525644}
2025-01-12 17:16:21,918 [INFO] 
2025-01-12 17:16:39,128 [INFO] Step[50/2713]: training loss : 1.0947270679473877 TRAIN  loss dict:  {'classification_loss': 1.0947270679473877}
2025-01-12 17:16:51,029 [INFO] Step[100/2713]: training loss : 1.0833886551856995 TRAIN  loss dict:  {'classification_loss': 1.0833886551856995}
2025-01-12 17:17:02,938 [INFO] Step[150/2713]: training loss : 1.1239290010929108 TRAIN  loss dict:  {'classification_loss': 1.1239290010929108}
2025-01-12 17:17:14,840 [INFO] Step[200/2713]: training loss : 1.1105538761615754 TRAIN  loss dict:  {'classification_loss': 1.1105538761615754}
2025-01-12 17:17:26,740 [INFO] Step[250/2713]: training loss : 1.119250545501709 TRAIN  loss dict:  {'classification_loss': 1.119250545501709}
2025-01-12 17:17:38,627 [INFO] Step[300/2713]: training loss : 1.0854248189926148 TRAIN  loss dict:  {'classification_loss': 1.0854248189926148}
2025-01-12 17:17:50,527 [INFO] Step[350/2713]: training loss : 1.0987313795089722 TRAIN  loss dict:  {'classification_loss': 1.0987313795089722}
2025-01-12 17:18:02,458 [INFO] Step[400/2713]: training loss : 1.0869630324840545 TRAIN  loss dict:  {'classification_loss': 1.0869630324840545}
2025-01-12 17:18:14,430 [INFO] Step[450/2713]: training loss : 1.1244433081150056 TRAIN  loss dict:  {'classification_loss': 1.1244433081150056}
2025-01-12 17:18:26,331 [INFO] Step[500/2713]: training loss : 1.0908616602420806 TRAIN  loss dict:  {'classification_loss': 1.0908616602420806}
2025-01-12 17:18:38,217 [INFO] Step[550/2713]: training loss : 1.1031472098827362 TRAIN  loss dict:  {'classification_loss': 1.1031472098827362}
2025-01-12 17:18:50,126 [INFO] Step[600/2713]: training loss : 1.0914181983470916 TRAIN  loss dict:  {'classification_loss': 1.0914181983470916}
2025-01-12 17:19:02,022 [INFO] Step[650/2713]: training loss : 1.1295451092720032 TRAIN  loss dict:  {'classification_loss': 1.1295451092720032}
2025-01-12 17:19:13,914 [INFO] Step[700/2713]: training loss : 1.138459358215332 TRAIN  loss dict:  {'classification_loss': 1.138459358215332}
2025-01-12 17:19:25,881 [INFO] Step[750/2713]: training loss : 1.1223170244693756 TRAIN  loss dict:  {'classification_loss': 1.1223170244693756}
2025-01-12 17:19:38,238 [INFO] Step[800/2713]: training loss : 1.078127192258835 TRAIN  loss dict:  {'classification_loss': 1.078127192258835}
2025-01-12 17:19:50,674 [INFO] Step[850/2713]: training loss : 1.114531571865082 TRAIN  loss dict:  {'classification_loss': 1.114531571865082}
2025-01-12 17:20:02,922 [INFO] Step[900/2713]: training loss : 1.0799702882766724 TRAIN  loss dict:  {'classification_loss': 1.0799702882766724}
2025-01-12 17:20:15,601 [INFO] Step[950/2713]: training loss : 1.0827684640884399 TRAIN  loss dict:  {'classification_loss': 1.0827684640884399}
2025-01-12 17:20:28,091 [INFO] Step[1000/2713]: training loss : 1.0870470988750458 TRAIN  loss dict:  {'classification_loss': 1.0870470988750458}
2025-01-12 17:20:40,656 [INFO] Step[1050/2713]: training loss : 1.1189445519447327 TRAIN  loss dict:  {'classification_loss': 1.1189445519447327}
2025-01-12 17:20:53,752 [INFO] Step[1100/2713]: training loss : 1.0840591633319854 TRAIN  loss dict:  {'classification_loss': 1.0840591633319854}
2025-01-12 17:21:07,264 [INFO] Step[1150/2713]: training loss : 1.106799179315567 TRAIN  loss dict:  {'classification_loss': 1.106799179315567}
2025-01-12 17:21:19,959 [INFO] Step[1200/2713]: training loss : 1.1218610441684722 TRAIN  loss dict:  {'classification_loss': 1.1218610441684722}
2025-01-12 17:21:31,919 [INFO] Step[1250/2713]: training loss : 1.0842307221889496 TRAIN  loss dict:  {'classification_loss': 1.0842307221889496}
2025-01-12 17:21:43,794 [INFO] Step[1300/2713]: training loss : 1.1480225896835328 TRAIN  loss dict:  {'classification_loss': 1.1480225896835328}
2025-01-12 17:21:55,672 [INFO] Step[1350/2713]: training loss : 1.1083655416965486 TRAIN  loss dict:  {'classification_loss': 1.1083655416965486}
2025-01-12 17:22:07,557 [INFO] Step[1400/2713]: training loss : 1.07480051279068 TRAIN  loss dict:  {'classification_loss': 1.07480051279068}
2025-01-12 17:22:19,443 [INFO] Step[1450/2713]: training loss : 1.0938808953762054 TRAIN  loss dict:  {'classification_loss': 1.0938808953762054}
2025-01-12 17:22:31,290 [INFO] Step[1500/2713]: training loss : 1.091266038417816 TRAIN  loss dict:  {'classification_loss': 1.091266038417816}
2025-01-12 17:22:43,214 [INFO] Step[1550/2713]: training loss : 1.0985946488380431 TRAIN  loss dict:  {'classification_loss': 1.0985946488380431}
2025-01-12 17:22:55,105 [INFO] Step[1600/2713]: training loss : 1.107285269498825 TRAIN  loss dict:  {'classification_loss': 1.107285269498825}
2025-01-12 17:23:07,002 [INFO] Step[1650/2713]: training loss : 1.0828991091251374 TRAIN  loss dict:  {'classification_loss': 1.0828991091251374}
2025-01-12 17:23:18,882 [INFO] Step[1700/2713]: training loss : 1.1128999650478364 TRAIN  loss dict:  {'classification_loss': 1.1128999650478364}
2025-01-12 17:23:30,800 [INFO] Step[1750/2713]: training loss : 1.0738573205471038 TRAIN  loss dict:  {'classification_loss': 1.0738573205471038}
2025-01-12 17:23:42,692 [INFO] Step[1800/2713]: training loss : 1.129730738401413 TRAIN  loss dict:  {'classification_loss': 1.129730738401413}
2025-01-12 17:23:54,675 [INFO] Step[1850/2713]: training loss : 1.1052346014976502 TRAIN  loss dict:  {'classification_loss': 1.1052346014976502}
2025-01-12 17:24:06,558 [INFO] Step[1900/2713]: training loss : 1.057667545080185 TRAIN  loss dict:  {'classification_loss': 1.057667545080185}
2025-01-12 17:24:18,504 [INFO] Step[1950/2713]: training loss : 1.1158995985984803 TRAIN  loss dict:  {'classification_loss': 1.1158995985984803}
2025-01-12 17:24:30,381 [INFO] Step[2000/2713]: training loss : 1.120319902896881 TRAIN  loss dict:  {'classification_loss': 1.120319902896881}
2025-01-12 17:24:42,303 [INFO] Step[2050/2713]: training loss : 1.0760332500934602 TRAIN  loss dict:  {'classification_loss': 1.0760332500934602}
2025-01-12 17:24:54,217 [INFO] Step[2100/2713]: training loss : 1.106998578310013 TRAIN  loss dict:  {'classification_loss': 1.106998578310013}
2025-01-12 17:25:06,114 [INFO] Step[2150/2713]: training loss : 1.088757483959198 TRAIN  loss dict:  {'classification_loss': 1.088757483959198}
2025-01-12 17:25:18,043 [INFO] Step[2200/2713]: training loss : 1.08532759308815 TRAIN  loss dict:  {'classification_loss': 1.08532759308815}
2025-01-12 17:25:29,949 [INFO] Step[2250/2713]: training loss : 1.0790652525424957 TRAIN  loss dict:  {'classification_loss': 1.0790652525424957}
2025-01-12 17:25:41,855 [INFO] Step[2300/2713]: training loss : 1.1222655403614044 TRAIN  loss dict:  {'classification_loss': 1.1222655403614044}
2025-01-12 17:25:53,744 [INFO] Step[2350/2713]: training loss : 1.0816799402236938 TRAIN  loss dict:  {'classification_loss': 1.0816799402236938}
2025-01-12 17:26:05,584 [INFO] Step[2400/2713]: training loss : 1.0794738173484801 TRAIN  loss dict:  {'classification_loss': 1.0794738173484801}
2025-01-12 17:26:17,492 [INFO] Step[2450/2713]: training loss : 1.0909040558338166 TRAIN  loss dict:  {'classification_loss': 1.0909040558338166}
2025-01-12 17:26:29,379 [INFO] Step[2500/2713]: training loss : 1.0825201570987701 TRAIN  loss dict:  {'classification_loss': 1.0825201570987701}
2025-01-12 17:26:41,345 [INFO] Step[2550/2713]: training loss : 1.1182340013980865 TRAIN  loss dict:  {'classification_loss': 1.1182340013980865}
2025-01-12 17:26:53,256 [INFO] Step[2600/2713]: training loss : 1.1439712786674499 TRAIN  loss dict:  {'classification_loss': 1.1439712786674499}
2025-01-12 17:27:05,159 [INFO] Step[2650/2713]: training loss : 1.1005601358413697 TRAIN  loss dict:  {'classification_loss': 1.1005601358413697}
2025-01-12 17:27:17,061 [INFO] Step[2700/2713]: training loss : 1.099959579706192 TRAIN  loss dict:  {'classification_loss': 1.099959579706192}
2025-01-12 17:28:43,669 [INFO] Label accuracies statistics:
2025-01-12 17:28:43,670 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.5, 21: 1.0, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.5, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.25, 157: 0.75, 158: 0.3333333333333333, 159: 0.75, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.3333333333333333, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 1.0, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.5, 226: 0.75, 227: 0.75, 228: 0.25, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.5, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.25, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 1.0, 283: 0.5, 284: 0.5, 285: 0.5, 286: 0.5, 287: 0.5, 288: 0.75, 289: 0.5, 290: 0.75, 291: 0.75, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.0, 329: 0.75, 330: 0.25, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.25, 339: 0.75, 340: 0.75, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.25, 350: 0.25, 351: 0.75, 352: 0.5, 353: 1.0, 354: 0.75, 355: 0.5, 356: 0.25, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 17:28:43,671 [INFO] [12] TRAIN  loss: 1.1007871742271385 acc: 0.9782528566162919
2025-01-12 17:28:43,672 [INFO] [12] TRAIN  loss dict: {'classification_loss': 1.1007871742271385}
2025-01-12 17:28:43,672 [INFO] [12] VALIDATION loss: 1.8424776727543737 VALIDATION acc: 0.7711598746081505
2025-01-12 17:28:43,672 [INFO] [12] VALIDATION loss dict: {'classification_loss': 1.8424776727543737}
2025-01-12 17:28:43,672 [INFO] 
2025-01-12 17:29:01,069 [INFO] Step[50/2713]: training loss : 1.082332410812378 TRAIN  loss dict:  {'classification_loss': 1.082332410812378}
2025-01-12 17:29:12,984 [INFO] Step[100/2713]: training loss : 1.0942670619487762 TRAIN  loss dict:  {'classification_loss': 1.0942670619487762}
2025-01-12 17:29:24,904 [INFO] Step[150/2713]: training loss : 1.0910131919384003 TRAIN  loss dict:  {'classification_loss': 1.0910131919384003}
2025-01-12 17:29:36,796 [INFO] Step[200/2713]: training loss : 1.0735834443569183 TRAIN  loss dict:  {'classification_loss': 1.0735834443569183}
2025-01-12 17:29:48,739 [INFO] Step[250/2713]: training loss : 1.1188559544086456 TRAIN  loss dict:  {'classification_loss': 1.1188559544086456}
2025-01-12 17:30:00,654 [INFO] Step[300/2713]: training loss : 1.0705593621730805 TRAIN  loss dict:  {'classification_loss': 1.0705593621730805}
2025-01-12 17:30:12,534 [INFO] Step[350/2713]: training loss : 1.0476339221000672 TRAIN  loss dict:  {'classification_loss': 1.0476339221000672}
2025-01-12 17:30:24,415 [INFO] Step[400/2713]: training loss : 1.0774405252933503 TRAIN  loss dict:  {'classification_loss': 1.0774405252933503}
2025-01-12 17:30:36,342 [INFO] Step[450/2713]: training loss : 1.080773777961731 TRAIN  loss dict:  {'classification_loss': 1.080773777961731}
2025-01-12 17:30:48,287 [INFO] Step[500/2713]: training loss : 1.085068405866623 TRAIN  loss dict:  {'classification_loss': 1.085068405866623}
2025-01-12 17:31:00,178 [INFO] Step[550/2713]: training loss : 1.0691655170917511 TRAIN  loss dict:  {'classification_loss': 1.0691655170917511}
2025-01-12 17:31:12,041 [INFO] Step[600/2713]: training loss : 1.0344460082054139 TRAIN  loss dict:  {'classification_loss': 1.0344460082054139}
2025-01-12 17:31:23,926 [INFO] Step[650/2713]: training loss : 1.0631209194660187 TRAIN  loss dict:  {'classification_loss': 1.0631209194660187}
2025-01-12 17:31:35,863 [INFO] Step[700/2713]: training loss : 1.080747083425522 TRAIN  loss dict:  {'classification_loss': 1.080747083425522}
2025-01-12 17:31:47,780 [INFO] Step[750/2713]: training loss : 1.09375514626503 TRAIN  loss dict:  {'classification_loss': 1.09375514626503}
2025-01-12 17:31:59,671 [INFO] Step[800/2713]: training loss : 1.0816407334804534 TRAIN  loss dict:  {'classification_loss': 1.0816407334804534}
2025-01-12 17:32:11,594 [INFO] Step[850/2713]: training loss : 1.1226225101947784 TRAIN  loss dict:  {'classification_loss': 1.1226225101947784}
2025-01-12 17:32:23,493 [INFO] Step[900/2713]: training loss : 1.1067095637321471 TRAIN  loss dict:  {'classification_loss': 1.1067095637321471}
2025-01-12 17:32:35,433 [INFO] Step[950/2713]: training loss : 1.0903439211845398 TRAIN  loss dict:  {'classification_loss': 1.0903439211845398}
2025-01-12 17:32:47,354 [INFO] Step[1000/2713]: training loss : 1.0625071465969085 TRAIN  loss dict:  {'classification_loss': 1.0625071465969085}
2025-01-12 17:32:59,283 [INFO] Step[1050/2713]: training loss : 1.0719229888916015 TRAIN  loss dict:  {'classification_loss': 1.0719229888916015}
2025-01-12 17:33:11,182 [INFO] Step[1100/2713]: training loss : 1.153828146457672 TRAIN  loss dict:  {'classification_loss': 1.153828146457672}
2025-01-12 17:33:23,051 [INFO] Step[1150/2713]: training loss : 1.0913065254688263 TRAIN  loss dict:  {'classification_loss': 1.0913065254688263}
2025-01-12 17:33:34,945 [INFO] Step[1200/2713]: training loss : 1.0695976543426513 TRAIN  loss dict:  {'classification_loss': 1.0695976543426513}
2025-01-12 17:33:46,853 [INFO] Step[1250/2713]: training loss : 1.0527093863487245 TRAIN  loss dict:  {'classification_loss': 1.0527093863487245}
2025-01-12 17:33:58,771 [INFO] Step[1300/2713]: training loss : 1.073789404630661 TRAIN  loss dict:  {'classification_loss': 1.073789404630661}
2025-01-12 17:34:10,635 [INFO] Step[1350/2713]: training loss : 1.0786519587039947 TRAIN  loss dict:  {'classification_loss': 1.0786519587039947}
2025-01-12 17:34:22,526 [INFO] Step[1400/2713]: training loss : 1.1217813193798065 TRAIN  loss dict:  {'classification_loss': 1.1217813193798065}
2025-01-12 17:34:34,436 [INFO] Step[1450/2713]: training loss : 1.0756935358047486 TRAIN  loss dict:  {'classification_loss': 1.0756935358047486}
2025-01-12 17:34:46,348 [INFO] Step[1500/2713]: training loss : 1.090534610748291 TRAIN  loss dict:  {'classification_loss': 1.090534610748291}
2025-01-12 17:34:58,288 [INFO] Step[1550/2713]: training loss : 1.09002703666687 TRAIN  loss dict:  {'classification_loss': 1.09002703666687}
2025-01-12 17:35:10,184 [INFO] Step[1600/2713]: training loss : 1.0538876712322236 TRAIN  loss dict:  {'classification_loss': 1.0538876712322236}
2025-01-12 17:35:22,111 [INFO] Step[1650/2713]: training loss : 1.1074632227420806 TRAIN  loss dict:  {'classification_loss': 1.1074632227420806}
2025-01-12 17:35:34,034 [INFO] Step[1700/2713]: training loss : 1.0492962336540221 TRAIN  loss dict:  {'classification_loss': 1.0492962336540221}
2025-01-12 17:35:45,950 [INFO] Step[1750/2713]: training loss : 1.1148519313335419 TRAIN  loss dict:  {'classification_loss': 1.1148519313335419}
2025-01-12 17:35:57,917 [INFO] Step[1800/2713]: training loss : 1.099966971874237 TRAIN  loss dict:  {'classification_loss': 1.099966971874237}
2025-01-12 17:36:09,852 [INFO] Step[1850/2713]: training loss : 1.04943714261055 TRAIN  loss dict:  {'classification_loss': 1.04943714261055}
2025-01-12 17:36:21,735 [INFO] Step[1900/2713]: training loss : 1.0807222545146942 TRAIN  loss dict:  {'classification_loss': 1.0807222545146942}
2025-01-12 17:36:33,629 [INFO] Step[1950/2713]: training loss : 1.1259991002082825 TRAIN  loss dict:  {'classification_loss': 1.1259991002082825}
2025-01-12 17:36:45,539 [INFO] Step[2000/2713]: training loss : 1.1109459722042083 TRAIN  loss dict:  {'classification_loss': 1.1109459722042083}
2025-01-12 17:36:57,462 [INFO] Step[2050/2713]: training loss : 1.0889849293231963 TRAIN  loss dict:  {'classification_loss': 1.0889849293231963}
2025-01-12 17:37:09,325 [INFO] Step[2100/2713]: training loss : 1.0616917073726655 TRAIN  loss dict:  {'classification_loss': 1.0616917073726655}
2025-01-12 17:37:21,269 [INFO] Step[2150/2713]: training loss : 1.0727765023708344 TRAIN  loss dict:  {'classification_loss': 1.0727765023708344}
2025-01-12 17:37:33,167 [INFO] Step[2200/2713]: training loss : 1.061741898059845 TRAIN  loss dict:  {'classification_loss': 1.061741898059845}
2025-01-12 17:37:45,052 [INFO] Step[2250/2713]: training loss : 1.0842674720287322 TRAIN  loss dict:  {'classification_loss': 1.0842674720287322}
2025-01-12 17:37:56,978 [INFO] Step[2300/2713]: training loss : 1.110266009569168 TRAIN  loss dict:  {'classification_loss': 1.110266009569168}
2025-01-12 17:38:08,896 [INFO] Step[2350/2713]: training loss : 1.066862450838089 TRAIN  loss dict:  {'classification_loss': 1.066862450838089}
2025-01-12 17:38:20,879 [INFO] Step[2400/2713]: training loss : 1.0952193462848663 TRAIN  loss dict:  {'classification_loss': 1.0952193462848663}
2025-01-12 17:38:33,382 [INFO] Step[2450/2713]: training loss : 1.0905327200889587 TRAIN  loss dict:  {'classification_loss': 1.0905327200889587}
2025-01-12 17:38:45,743 [INFO] Step[2500/2713]: training loss : 1.0834868776798248 TRAIN  loss dict:  {'classification_loss': 1.0834868776798248}
2025-01-12 17:38:58,014 [INFO] Step[2550/2713]: training loss : 1.1225623667240143 TRAIN  loss dict:  {'classification_loss': 1.1225623667240143}
2025-01-12 17:39:10,744 [INFO] Step[2600/2713]: training loss : 1.1193739926815034 TRAIN  loss dict:  {'classification_loss': 1.1193739926815034}
2025-01-12 17:39:23,035 [INFO] Step[2650/2713]: training loss : 1.094545191526413 TRAIN  loss dict:  {'classification_loss': 1.094545191526413}
2025-01-12 17:39:35,704 [INFO] Step[2700/2713]: training loss : 1.0877907240390778 TRAIN  loss dict:  {'classification_loss': 1.0877907240390778}
2025-01-12 17:41:16,267 [INFO] Label accuracies statistics:
2025-01-12 17:41:16,267 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 0.75, 11: 1.0, 12: 0.5, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.25, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.25, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.5, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 1.0, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.5, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.75, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 1.0, 122: 1.0, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.5, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 1.0, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.0, 217: 1.0, 218: 0.5, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.25, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 0.75, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.25, 261: 0.75, 262: 1.0, 263: 0.5, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 0.75, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 1.0, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 0.5, 325: 0.5, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.25, 351: 0.75, 352: 0.25, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 1.0, 372: 1.0, 373: 0.75, 374: 0.75, 375: 0.5, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 0.75, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.5, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 0.5, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 17:41:16,269 [INFO] [13] TRAIN  loss: 1.085807577670829 acc: 0.9836589261580047
2025-01-12 17:41:16,269 [INFO] [13] TRAIN  loss dict: {'classification_loss': 1.085807577670829}
2025-01-12 17:41:16,269 [INFO] [13] VALIDATION loss: 1.8417345569994217 VALIDATION acc: 0.7755485893416928
2025-01-12 17:41:16,269 [INFO] [13] VALIDATION loss dict: {'classification_loss': 1.8417345569994217}
2025-01-12 17:41:16,269 [INFO] 
2025-01-12 17:41:34,015 [INFO] Step[50/2713]: training loss : 1.084545624256134 TRAIN  loss dict:  {'classification_loss': 1.084545624256134}
2025-01-12 17:41:45,891 [INFO] Step[100/2713]: training loss : 1.0817816591262817 TRAIN  loss dict:  {'classification_loss': 1.0817816591262817}
2025-01-12 17:41:57,769 [INFO] Step[150/2713]: training loss : 1.0260323512554168 TRAIN  loss dict:  {'classification_loss': 1.0260323512554168}
2025-01-12 17:42:09,711 [INFO] Step[200/2713]: training loss : 1.0693173861503602 TRAIN  loss dict:  {'classification_loss': 1.0693173861503602}
2025-01-12 17:42:21,604 [INFO] Step[250/2713]: training loss : 1.0549110078811645 TRAIN  loss dict:  {'classification_loss': 1.0549110078811645}
2025-01-12 17:42:33,471 [INFO] Step[300/2713]: training loss : 1.0847333598136901 TRAIN  loss dict:  {'classification_loss': 1.0847333598136901}
2025-01-12 17:42:45,407 [INFO] Step[350/2713]: training loss : 1.128744238615036 TRAIN  loss dict:  {'classification_loss': 1.128744238615036}
2025-01-12 17:42:57,253 [INFO] Step[400/2713]: training loss : 1.0546914625167847 TRAIN  loss dict:  {'classification_loss': 1.0546914625167847}
2025-01-12 17:43:09,131 [INFO] Step[450/2713]: training loss : 1.0383193385601044 TRAIN  loss dict:  {'classification_loss': 1.0383193385601044}
2025-01-12 17:43:21,014 [INFO] Step[500/2713]: training loss : 1.0949526047706604 TRAIN  loss dict:  {'classification_loss': 1.0949526047706604}
2025-01-12 17:43:32,925 [INFO] Step[550/2713]: training loss : 1.0511391949653626 TRAIN  loss dict:  {'classification_loss': 1.0511391949653626}
2025-01-12 17:43:44,876 [INFO] Step[600/2713]: training loss : 1.0659758067131042 TRAIN  loss dict:  {'classification_loss': 1.0659758067131042}
2025-01-12 17:43:56,800 [INFO] Step[650/2713]: training loss : 1.0868574178218842 TRAIN  loss dict:  {'classification_loss': 1.0868574178218842}
2025-01-12 17:44:08,697 [INFO] Step[700/2713]: training loss : 1.0822167479991913 TRAIN  loss dict:  {'classification_loss': 1.0822167479991913}
2025-01-12 17:44:20,600 [INFO] Step[750/2713]: training loss : 1.08768150806427 TRAIN  loss dict:  {'classification_loss': 1.08768150806427}
2025-01-12 17:44:32,468 [INFO] Step[800/2713]: training loss : 1.087022420167923 TRAIN  loss dict:  {'classification_loss': 1.087022420167923}
2025-01-12 17:44:44,346 [INFO] Step[850/2713]: training loss : 1.1050626337528229 TRAIN  loss dict:  {'classification_loss': 1.1050626337528229}
2025-01-12 17:44:56,225 [INFO] Step[900/2713]: training loss : 1.081196621656418 TRAIN  loss dict:  {'classification_loss': 1.081196621656418}
2025-01-12 17:45:08,121 [INFO] Step[950/2713]: training loss : 1.0889143109321595 TRAIN  loss dict:  {'classification_loss': 1.0889143109321595}
2025-01-12 17:45:20,053 [INFO] Step[1000/2713]: training loss : 1.1158802652359008 TRAIN  loss dict:  {'classification_loss': 1.1158802652359008}
2025-01-12 17:45:31,949 [INFO] Step[1050/2713]: training loss : 1.0590802836418152 TRAIN  loss dict:  {'classification_loss': 1.0590802836418152}
2025-01-12 17:45:43,868 [INFO] Step[1100/2713]: training loss : 1.065941458940506 TRAIN  loss dict:  {'classification_loss': 1.065941458940506}
2025-01-12 17:45:55,821 [INFO] Step[1150/2713]: training loss : 1.1309508955478669 TRAIN  loss dict:  {'classification_loss': 1.1309508955478669}
2025-01-12 17:46:07,699 [INFO] Step[1200/2713]: training loss : 1.045865398645401 TRAIN  loss dict:  {'classification_loss': 1.045865398645401}
2025-01-12 17:46:19,628 [INFO] Step[1250/2713]: training loss : 1.0773210048675537 TRAIN  loss dict:  {'classification_loss': 1.0773210048675537}
2025-01-12 17:46:31,534 [INFO] Step[1300/2713]: training loss : 1.0758629524707795 TRAIN  loss dict:  {'classification_loss': 1.0758629524707795}
2025-01-12 17:46:43,341 [INFO] Step[1350/2713]: training loss : 1.0857040917873382 TRAIN  loss dict:  {'classification_loss': 1.0857040917873382}
2025-01-12 17:46:55,232 [INFO] Step[1400/2713]: training loss : 1.0785608172416687 TRAIN  loss dict:  {'classification_loss': 1.0785608172416687}
2025-01-12 17:47:07,136 [INFO] Step[1450/2713]: training loss : 1.1009075677394866 TRAIN  loss dict:  {'classification_loss': 1.1009075677394866}
2025-01-12 17:47:19,026 [INFO] Step[1500/2713]: training loss : 1.091212203502655 TRAIN  loss dict:  {'classification_loss': 1.091212203502655}
2025-01-12 17:47:30,897 [INFO] Step[1550/2713]: training loss : 1.1381263327598572 TRAIN  loss dict:  {'classification_loss': 1.1381263327598572}
2025-01-12 17:47:42,823 [INFO] Step[1600/2713]: training loss : 1.0743642270565033 TRAIN  loss dict:  {'classification_loss': 1.0743642270565033}
2025-01-12 17:47:54,712 [INFO] Step[1650/2713]: training loss : 1.1061958169937134 TRAIN  loss dict:  {'classification_loss': 1.1061958169937134}
2025-01-12 17:48:06,616 [INFO] Step[1700/2713]: training loss : 1.0809560108184815 TRAIN  loss dict:  {'classification_loss': 1.0809560108184815}
2025-01-12 17:48:18,535 [INFO] Step[1750/2713]: training loss : 1.09110764503479 TRAIN  loss dict:  {'classification_loss': 1.09110764503479}
2025-01-12 17:48:30,478 [INFO] Step[1800/2713]: training loss : 1.0612106812000275 TRAIN  loss dict:  {'classification_loss': 1.0612106812000275}
2025-01-12 17:48:42,432 [INFO] Step[1850/2713]: training loss : 1.1003524053096772 TRAIN  loss dict:  {'classification_loss': 1.1003524053096772}
2025-01-12 17:48:54,340 [INFO] Step[1900/2713]: training loss : 1.0910522973537444 TRAIN  loss dict:  {'classification_loss': 1.0910522973537444}
2025-01-12 17:49:06,261 [INFO] Step[1950/2713]: training loss : 1.092191491127014 TRAIN  loss dict:  {'classification_loss': 1.092191491127014}
2025-01-12 17:49:18,170 [INFO] Step[2000/2713]: training loss : 1.0829798877239227 TRAIN  loss dict:  {'classification_loss': 1.0829798877239227}
2025-01-12 17:49:30,094 [INFO] Step[2050/2713]: training loss : 1.0633967185020448 TRAIN  loss dict:  {'classification_loss': 1.0633967185020448}
2025-01-12 17:49:42,005 [INFO] Step[2100/2713]: training loss : 1.1164833724498748 TRAIN  loss dict:  {'classification_loss': 1.1164833724498748}
2025-01-12 17:49:53,931 [INFO] Step[2150/2713]: training loss : 1.1009972274303437 TRAIN  loss dict:  {'classification_loss': 1.1009972274303437}
2025-01-12 17:50:05,837 [INFO] Step[2200/2713]: training loss : 1.0811570227146148 TRAIN  loss dict:  {'classification_loss': 1.0811570227146148}
2025-01-12 17:50:17,740 [INFO] Step[2250/2713]: training loss : 1.0841405832767486 TRAIN  loss dict:  {'classification_loss': 1.0841405832767486}
2025-01-12 17:50:29,663 [INFO] Step[2300/2713]: training loss : 1.1088957917690276 TRAIN  loss dict:  {'classification_loss': 1.1088957917690276}
2025-01-12 17:50:41,563 [INFO] Step[2350/2713]: training loss : 1.0792659294605256 TRAIN  loss dict:  {'classification_loss': 1.0792659294605256}
2025-01-12 17:50:53,438 [INFO] Step[2400/2713]: training loss : 1.0642168962955474 TRAIN  loss dict:  {'classification_loss': 1.0642168962955474}
2025-01-12 17:51:05,368 [INFO] Step[2450/2713]: training loss : 1.080252867937088 TRAIN  loss dict:  {'classification_loss': 1.080252867937088}
2025-01-12 17:51:17,227 [INFO] Step[2500/2713]: training loss : 1.1162084543704986 TRAIN  loss dict:  {'classification_loss': 1.1162084543704986}
2025-01-12 17:51:29,142 [INFO] Step[2550/2713]: training loss : 1.1057692527770997 TRAIN  loss dict:  {'classification_loss': 1.1057692527770997}
2025-01-12 17:51:41,039 [INFO] Step[2600/2713]: training loss : 1.168418630361557 TRAIN  loss dict:  {'classification_loss': 1.168418630361557}
2025-01-12 17:51:52,916 [INFO] Step[2650/2713]: training loss : 1.0922877371311188 TRAIN  loss dict:  {'classification_loss': 1.0922877371311188}
2025-01-12 17:52:04,814 [INFO] Step[2700/2713]: training loss : 1.0601786267757416 TRAIN  loss dict:  {'classification_loss': 1.0601786267757416}
2025-01-12 17:53:32,776 [INFO] Label accuracies statistics:
2025-01-12 17:53:32,776 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 0.75, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.5, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 0.75, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.5, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 1.0, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.5, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.5, 185: 1.0, 186: 0.75, 187: 0.75, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.25, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 0.75, 260: 0.75, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.25, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.25, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 0.75, 313: 0.25, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.25, 357: 0.75, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 0.75, 375: 0.25, 376: 0.5, 377: 0.5, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 0.75, 383: 1.0, 384: 0.5, 385: 0.75, 386: 1.0, 387: 0.25, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.5}

2025-01-12 17:53:32,778 [INFO] [14] TRAIN  loss: 1.0853651553513246 acc: 0.9812016218208626
2025-01-12 17:53:32,778 [INFO] [14] TRAIN  loss dict: {'classification_loss': 1.0853651553513246}
2025-01-12 17:53:32,778 [INFO] [14] VALIDATION loss: 1.809688705810927 VALIDATION acc: 0.7768025078369906
2025-01-12 17:53:32,778 [INFO] [14] VALIDATION loss dict: {'classification_loss': 1.809688705810927}
2025-01-12 17:53:32,778 [INFO] 
2025-01-12 17:53:50,254 [INFO] Step[50/2713]: training loss : 1.0564014410972595 TRAIN  loss dict:  {'classification_loss': 1.0564014410972595}
2025-01-12 17:54:02,180 [INFO] Step[100/2713]: training loss : 1.087333152294159 TRAIN  loss dict:  {'classification_loss': 1.087333152294159}
2025-01-12 17:54:14,117 [INFO] Step[150/2713]: training loss : 1.0900286626815796 TRAIN  loss dict:  {'classification_loss': 1.0900286626815796}
2025-01-12 17:54:26,030 [INFO] Step[200/2713]: training loss : 1.0727066707611084 TRAIN  loss dict:  {'classification_loss': 1.0727066707611084}
2025-01-12 17:54:37,939 [INFO] Step[250/2713]: training loss : 1.0641766846179963 TRAIN  loss dict:  {'classification_loss': 1.0641766846179963}
2025-01-12 17:54:49,845 [INFO] Step[300/2713]: training loss : 1.0476827120780945 TRAIN  loss dict:  {'classification_loss': 1.0476827120780945}
2025-01-12 17:55:01,734 [INFO] Step[350/2713]: training loss : 1.0789637100696563 TRAIN  loss dict:  {'classification_loss': 1.0789637100696563}
2025-01-12 17:55:13,614 [INFO] Step[400/2713]: training loss : 1.0819925153255463 TRAIN  loss dict:  {'classification_loss': 1.0819925153255463}
2025-01-12 17:55:25,491 [INFO] Step[450/2713]: training loss : 1.0939941906929016 TRAIN  loss dict:  {'classification_loss': 1.0939941906929016}
2025-01-12 17:55:37,393 [INFO] Step[500/2713]: training loss : 1.0583737540245055 TRAIN  loss dict:  {'classification_loss': 1.0583737540245055}
2025-01-12 17:55:49,278 [INFO] Step[550/2713]: training loss : 1.0757801711559296 TRAIN  loss dict:  {'classification_loss': 1.0757801711559296}
2025-01-12 17:56:01,151 [INFO] Step[600/2713]: training loss : 1.0494653224945067 TRAIN  loss dict:  {'classification_loss': 1.0494653224945067}
2025-01-12 17:56:13,057 [INFO] Step[650/2713]: training loss : 1.0552999663352967 TRAIN  loss dict:  {'classification_loss': 1.0552999663352967}
2025-01-12 17:56:24,903 [INFO] Step[700/2713]: training loss : 1.1068985879421234 TRAIN  loss dict:  {'classification_loss': 1.1068985879421234}
2025-01-12 17:56:36,800 [INFO] Step[750/2713]: training loss : 1.1289037251472473 TRAIN  loss dict:  {'classification_loss': 1.1289037251472473}
2025-01-12 17:56:48,723 [INFO] Step[800/2713]: training loss : 1.0411045336723328 TRAIN  loss dict:  {'classification_loss': 1.0411045336723328}
2025-01-12 17:57:00,634 [INFO] Step[850/2713]: training loss : 1.08260759472847 TRAIN  loss dict:  {'classification_loss': 1.08260759472847}
2025-01-12 17:57:12,562 [INFO] Step[900/2713]: training loss : 1.0773801183700562 TRAIN  loss dict:  {'classification_loss': 1.0773801183700562}
2025-01-12 17:57:24,764 [INFO] Step[950/2713]: training loss : 1.086776943206787 TRAIN  loss dict:  {'classification_loss': 1.086776943206787}
2025-01-12 17:57:37,195 [INFO] Step[1000/2713]: training loss : 1.0930553078651428 TRAIN  loss dict:  {'classification_loss': 1.0930553078651428}
2025-01-12 17:57:49,555 [INFO] Step[1050/2713]: training loss : 1.0453499007225036 TRAIN  loss dict:  {'classification_loss': 1.0453499007225036}
2025-01-12 17:58:02,086 [INFO] Step[1100/2713]: training loss : 1.0935879790782927 TRAIN  loss dict:  {'classification_loss': 1.0935879790782927}
2025-01-12 17:58:14,649 [INFO] Step[1150/2713]: training loss : 1.0699253392219543 TRAIN  loss dict:  {'classification_loss': 1.0699253392219543}
2025-01-12 17:58:27,143 [INFO] Step[1200/2713]: training loss : 1.099028573036194 TRAIN  loss dict:  {'classification_loss': 1.099028573036194}
2025-01-12 17:58:40,132 [INFO] Step[1250/2713]: training loss : 1.080433772802353 TRAIN  loss dict:  {'classification_loss': 1.080433772802353}
2025-01-12 17:58:53,955 [INFO] Step[1300/2713]: training loss : 1.0539558148384094 TRAIN  loss dict:  {'classification_loss': 1.0539558148384094}
2025-01-12 17:59:06,893 [INFO] Step[1350/2713]: training loss : 1.0802502465248107 TRAIN  loss dict:  {'classification_loss': 1.0802502465248107}
2025-01-12 17:59:18,926 [INFO] Step[1400/2713]: training loss : 1.053496049642563 TRAIN  loss dict:  {'classification_loss': 1.053496049642563}
2025-01-12 17:59:30,802 [INFO] Step[1450/2713]: training loss : 1.0614918923377992 TRAIN  loss dict:  {'classification_loss': 1.0614918923377992}
2025-01-12 17:59:42,632 [INFO] Step[1500/2713]: training loss : 1.046382678747177 TRAIN  loss dict:  {'classification_loss': 1.046382678747177}
2025-01-12 17:59:54,526 [INFO] Step[1550/2713]: training loss : 1.0759470057487488 TRAIN  loss dict:  {'classification_loss': 1.0759470057487488}
2025-01-12 18:00:06,360 [INFO] Step[1600/2713]: training loss : 1.046910001039505 TRAIN  loss dict:  {'classification_loss': 1.046910001039505}
2025-01-12 18:00:18,278 [INFO] Step[1650/2713]: training loss : 1.0663531601428986 TRAIN  loss dict:  {'classification_loss': 1.0663531601428986}
2025-01-12 18:00:30,180 [INFO] Step[1700/2713]: training loss : 1.0643217432498933 TRAIN  loss dict:  {'classification_loss': 1.0643217432498933}
2025-01-12 18:00:42,053 [INFO] Step[1750/2713]: training loss : 1.065132339000702 TRAIN  loss dict:  {'classification_loss': 1.065132339000702}
2025-01-12 18:00:53,939 [INFO] Step[1800/2713]: training loss : 1.0910469043254851 TRAIN  loss dict:  {'classification_loss': 1.0910469043254851}
2025-01-12 18:01:05,833 [INFO] Step[1850/2713]: training loss : 1.057014549970627 TRAIN  loss dict:  {'classification_loss': 1.057014549970627}
2025-01-12 18:01:17,684 [INFO] Step[1900/2713]: training loss : 1.0563898086547852 TRAIN  loss dict:  {'classification_loss': 1.0563898086547852}
2025-01-12 18:01:29,580 [INFO] Step[1950/2713]: training loss : 1.1058922183513642 TRAIN  loss dict:  {'classification_loss': 1.1058922183513642}
2025-01-12 18:01:41,441 [INFO] Step[2000/2713]: training loss : 1.1082497239112854 TRAIN  loss dict:  {'classification_loss': 1.1082497239112854}
2025-01-12 18:01:53,335 [INFO] Step[2050/2713]: training loss : 1.0616384899616242 TRAIN  loss dict:  {'classification_loss': 1.0616384899616242}
2025-01-12 18:02:05,236 [INFO] Step[2100/2713]: training loss : 1.1123661971092225 TRAIN  loss dict:  {'classification_loss': 1.1123661971092225}
2025-01-12 18:02:17,103 [INFO] Step[2150/2713]: training loss : 1.0676673913002015 TRAIN  loss dict:  {'classification_loss': 1.0676673913002015}
2025-01-12 18:02:29,026 [INFO] Step[2200/2713]: training loss : 1.0797620677947999 TRAIN  loss dict:  {'classification_loss': 1.0797620677947999}
2025-01-12 18:02:40,936 [INFO] Step[2250/2713]: training loss : 1.0515532076358796 TRAIN  loss dict:  {'classification_loss': 1.0515532076358796}
2025-01-12 18:02:52,897 [INFO] Step[2300/2713]: training loss : 1.073212811946869 TRAIN  loss dict:  {'classification_loss': 1.073212811946869}
2025-01-12 18:03:04,749 [INFO] Step[2350/2713]: training loss : 1.1157788932323456 TRAIN  loss dict:  {'classification_loss': 1.1157788932323456}
2025-01-12 18:03:16,628 [INFO] Step[2400/2713]: training loss : 1.1040901196002961 TRAIN  loss dict:  {'classification_loss': 1.1040901196002961}
2025-01-12 18:03:28,532 [INFO] Step[2450/2713]: training loss : 1.0625609743595124 TRAIN  loss dict:  {'classification_loss': 1.0625609743595124}
2025-01-12 18:03:40,377 [INFO] Step[2500/2713]: training loss : 1.0350234580039979 TRAIN  loss dict:  {'classification_loss': 1.0350234580039979}
2025-01-12 18:03:52,273 [INFO] Step[2550/2713]: training loss : 1.1113726615905761 TRAIN  loss dict:  {'classification_loss': 1.1113726615905761}
2025-01-12 18:04:04,134 [INFO] Step[2600/2713]: training loss : 1.1576785850524902 TRAIN  loss dict:  {'classification_loss': 1.1576785850524902}
2025-01-12 18:04:16,086 [INFO] Step[2650/2713]: training loss : 1.0589594697952271 TRAIN  loss dict:  {'classification_loss': 1.0589594697952271}
2025-01-12 18:04:27,953 [INFO] Step[2700/2713]: training loss : 1.0890982580184936 TRAIN  loss dict:  {'classification_loss': 1.0890982580184936}
2025-01-12 18:05:55,517 [INFO] Label accuracies statistics:
2025-01-12 18:05:55,517 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 0.8, 100: 0.75, 101: 1.0, 102: 0.75, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.0, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.5, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 1.0, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.5, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.5, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.5, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.0, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 0.5, 242: 0.25, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.75, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.25, 291: 1.0, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.5, 346: 0.75, 347: 1.0, 348: 0.5, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 0.75, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.5, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-12 18:05:57,369 [INFO] [15] TRAIN  loss: 1.0765371736986278 acc: 0.9820616783388623
2025-01-12 18:05:57,369 [INFO] [15] TRAIN  loss dict: {'classification_loss': 1.0765371736986278}
2025-01-12 18:05:57,369 [INFO] [15] VALIDATION loss: 1.7616382535911144 VALIDATION acc: 0.7956112852664576
2025-01-12 18:05:57,369 [INFO] [15] VALIDATION loss dict: {'classification_loss': 1.7616382535911144}
2025-01-12 18:05:57,369 [INFO] 
2025-01-12 18:06:14,862 [INFO] Step[50/2713]: training loss : 1.0634343433380127 TRAIN  loss dict:  {'classification_loss': 1.0634343433380127}
2025-01-12 18:06:26,751 [INFO] Step[100/2713]: training loss : 1.0752466905117035 TRAIN  loss dict:  {'classification_loss': 1.0752466905117035}
2025-01-12 18:06:38,676 [INFO] Step[150/2713]: training loss : 1.0639876210689545 TRAIN  loss dict:  {'classification_loss': 1.0639876210689545}
2025-01-12 18:06:50,600 [INFO] Step[200/2713]: training loss : 1.0419502198696136 TRAIN  loss dict:  {'classification_loss': 1.0419502198696136}
2025-01-12 18:07:02,514 [INFO] Step[250/2713]: training loss : 1.0833742237091064 TRAIN  loss dict:  {'classification_loss': 1.0833742237091064}
2025-01-12 18:07:14,459 [INFO] Step[300/2713]: training loss : 1.0679500603675842 TRAIN  loss dict:  {'classification_loss': 1.0679500603675842}
2025-01-12 18:07:26,313 [INFO] Step[350/2713]: training loss : 1.0674266088008881 TRAIN  loss dict:  {'classification_loss': 1.0674266088008881}
2025-01-12 18:07:38,199 [INFO] Step[400/2713]: training loss : 1.0749467730522155 TRAIN  loss dict:  {'classification_loss': 1.0749467730522155}
2025-01-12 18:07:50,116 [INFO] Step[450/2713]: training loss : 1.1135449719429016 TRAIN  loss dict:  {'classification_loss': 1.1135449719429016}
2025-01-12 18:08:01,989 [INFO] Step[500/2713]: training loss : 1.0531884729862213 TRAIN  loss dict:  {'classification_loss': 1.0531884729862213}
2025-01-12 18:08:13,902 [INFO] Step[550/2713]: training loss : 1.11016428232193 TRAIN  loss dict:  {'classification_loss': 1.11016428232193}
2025-01-12 18:08:25,743 [INFO] Step[600/2713]: training loss : 1.0707983899116515 TRAIN  loss dict:  {'classification_loss': 1.0707983899116515}
2025-01-12 18:08:37,673 [INFO] Step[650/2713]: training loss : 1.0972675502300262 TRAIN  loss dict:  {'classification_loss': 1.0972675502300262}
2025-01-12 18:08:49,571 [INFO] Step[700/2713]: training loss : 1.081928058862686 TRAIN  loss dict:  {'classification_loss': 1.081928058862686}
2025-01-12 18:09:01,445 [INFO] Step[750/2713]: training loss : 1.060775920152664 TRAIN  loss dict:  {'classification_loss': 1.060775920152664}
2025-01-12 18:09:13,340 [INFO] Step[800/2713]: training loss : 1.084760113954544 TRAIN  loss dict:  {'classification_loss': 1.084760113954544}
2025-01-12 18:09:25,275 [INFO] Step[850/2713]: training loss : 1.0647885048389434 TRAIN  loss dict:  {'classification_loss': 1.0647885048389434}
2025-01-12 18:09:37,181 [INFO] Step[900/2713]: training loss : 1.0674596059322357 TRAIN  loss dict:  {'classification_loss': 1.0674596059322357}
2025-01-12 18:09:49,047 [INFO] Step[950/2713]: training loss : 1.060015459060669 TRAIN  loss dict:  {'classification_loss': 1.060015459060669}
2025-01-12 18:10:00,934 [INFO] Step[1000/2713]: training loss : 1.0955117011070252 TRAIN  loss dict:  {'classification_loss': 1.0955117011070252}
2025-01-12 18:10:12,855 [INFO] Step[1050/2713]: training loss : 1.047013018131256 TRAIN  loss dict:  {'classification_loss': 1.047013018131256}
2025-01-12 18:10:24,713 [INFO] Step[1100/2713]: training loss : 1.0714346861839295 TRAIN  loss dict:  {'classification_loss': 1.0714346861839295}
2025-01-12 18:10:36,616 [INFO] Step[1150/2713]: training loss : 1.0559047400951385 TRAIN  loss dict:  {'classification_loss': 1.0559047400951385}
2025-01-12 18:10:48,536 [INFO] Step[1200/2713]: training loss : 1.1092137229442596 TRAIN  loss dict:  {'classification_loss': 1.1092137229442596}
2025-01-12 18:11:00,453 [INFO] Step[1250/2713]: training loss : 1.0853039407730103 TRAIN  loss dict:  {'classification_loss': 1.0853039407730103}
2025-01-12 18:11:12,361 [INFO] Step[1300/2713]: training loss : 1.063458458185196 TRAIN  loss dict:  {'classification_loss': 1.063458458185196}
2025-01-12 18:11:24,238 [INFO] Step[1350/2713]: training loss : 1.1324192261695862 TRAIN  loss dict:  {'classification_loss': 1.1324192261695862}
2025-01-12 18:11:36,074 [INFO] Step[1400/2713]: training loss : 1.0704960131645203 TRAIN  loss dict:  {'classification_loss': 1.0704960131645203}
2025-01-12 18:11:47,963 [INFO] Step[1450/2713]: training loss : 1.077989957332611 TRAIN  loss dict:  {'classification_loss': 1.077989957332611}
2025-01-12 18:11:59,821 [INFO] Step[1500/2713]: training loss : 1.1128586316108704 TRAIN  loss dict:  {'classification_loss': 1.1128586316108704}
2025-01-12 18:12:11,722 [INFO] Step[1550/2713]: training loss : 1.075662487745285 TRAIN  loss dict:  {'classification_loss': 1.075662487745285}
2025-01-12 18:12:23,609 [INFO] Step[1600/2713]: training loss : 1.0686985850334167 TRAIN  loss dict:  {'classification_loss': 1.0686985850334167}
2025-01-12 18:12:35,567 [INFO] Step[1650/2713]: training loss : 1.036577570438385 TRAIN  loss dict:  {'classification_loss': 1.036577570438385}
2025-01-12 18:12:47,473 [INFO] Step[1700/2713]: training loss : 1.0587352430820465 TRAIN  loss dict:  {'classification_loss': 1.0587352430820465}
2025-01-12 18:12:59,335 [INFO] Step[1750/2713]: training loss : 1.0763909804821015 TRAIN  loss dict:  {'classification_loss': 1.0763909804821015}
2025-01-12 18:13:11,233 [INFO] Step[1800/2713]: training loss : 1.0948210668563843 TRAIN  loss dict:  {'classification_loss': 1.0948210668563843}
2025-01-12 18:13:23,152 [INFO] Step[1850/2713]: training loss : 1.0604563319683076 TRAIN  loss dict:  {'classification_loss': 1.0604563319683076}
2025-01-12 18:13:35,040 [INFO] Step[1900/2713]: training loss : 1.0820174276828767 TRAIN  loss dict:  {'classification_loss': 1.0820174276828767}
2025-01-12 18:13:46,997 [INFO] Step[1950/2713]: training loss : 1.0833183300495148 TRAIN  loss dict:  {'classification_loss': 1.0833183300495148}
2025-01-12 18:13:58,898 [INFO] Step[2000/2713]: training loss : 1.0392912483215333 TRAIN  loss dict:  {'classification_loss': 1.0392912483215333}
2025-01-12 18:14:10,787 [INFO] Step[2050/2713]: training loss : 1.069818947315216 TRAIN  loss dict:  {'classification_loss': 1.069818947315216}
2025-01-12 18:14:22,701 [INFO] Step[2100/2713]: training loss : 1.073724228143692 TRAIN  loss dict:  {'classification_loss': 1.073724228143692}
2025-01-12 18:14:34,594 [INFO] Step[2150/2713]: training loss : 1.090454293489456 TRAIN  loss dict:  {'classification_loss': 1.090454293489456}
2025-01-12 18:14:46,486 [INFO] Step[2200/2713]: training loss : 1.0868569612503052 TRAIN  loss dict:  {'classification_loss': 1.0868569612503052}
2025-01-12 18:14:58,370 [INFO] Step[2250/2713]: training loss : 1.0573945379257201 TRAIN  loss dict:  {'classification_loss': 1.0573945379257201}
2025-01-12 18:15:10,262 [INFO] Step[2300/2713]: training loss : 1.08543900847435 TRAIN  loss dict:  {'classification_loss': 1.08543900847435}
2025-01-12 18:15:22,139 [INFO] Step[2350/2713]: training loss : 1.079684088230133 TRAIN  loss dict:  {'classification_loss': 1.079684088230133}
2025-01-12 18:15:34,025 [INFO] Step[2400/2713]: training loss : 1.037864373922348 TRAIN  loss dict:  {'classification_loss': 1.037864373922348}
2025-01-12 18:15:45,910 [INFO] Step[2450/2713]: training loss : 1.0649853658676147 TRAIN  loss dict:  {'classification_loss': 1.0649853658676147}
2025-01-12 18:15:57,851 [INFO] Step[2500/2713]: training loss : 1.082792866230011 TRAIN  loss dict:  {'classification_loss': 1.082792866230011}
2025-01-12 18:16:09,778 [INFO] Step[2550/2713]: training loss : 1.0472932183742523 TRAIN  loss dict:  {'classification_loss': 1.0472932183742523}
2025-01-12 18:16:22,261 [INFO] Step[2600/2713]: training loss : 1.0641263723373413 TRAIN  loss dict:  {'classification_loss': 1.0641263723373413}
2025-01-12 18:16:34,673 [INFO] Step[2650/2713]: training loss : 1.0834593212604522 TRAIN  loss dict:  {'classification_loss': 1.0834593212604522}
2025-01-12 18:16:46,920 [INFO] Step[2700/2713]: training loss : 1.0990991377830506 TRAIN  loss dict:  {'classification_loss': 1.0990991377830506}
2025-01-12 18:18:32,298 [INFO] Label accuracies statistics:
2025-01-12 18:18:32,298 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.5, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.25, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 0.75, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.5, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.75, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 1.0, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.0, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.25, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 1.0, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 0.75, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.5, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 0.75, 358: 0.75, 359: 1.0, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.5, 377: 0.75, 378: 1.0, 379: 1.0, 380: 0.75, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 18:18:32,424 [INFO] [16] TRAIN  loss: 1.0742629736365317 acc: 0.985256173977147
2025-01-12 18:18:32,424 [INFO] [16] TRAIN  loss dict: {'classification_loss': 1.0742629736365317}
2025-01-12 18:18:32,425 [INFO] [16] VALIDATION loss: 1.7846153226113857 VALIDATION acc: 0.7931034482758621
2025-01-12 18:18:32,425 [INFO] [16] VALIDATION loss dict: {'classification_loss': 1.7846153226113857}
2025-01-12 18:18:32,425 [INFO] 
2025-01-12 18:18:50,515 [INFO] Step[50/2713]: training loss : 1.056213366985321 TRAIN  loss dict:  {'classification_loss': 1.056213366985321}
2025-01-12 18:19:02,438 [INFO] Step[100/2713]: training loss : 1.0631134414672851 TRAIN  loss dict:  {'classification_loss': 1.0631134414672851}
2025-01-12 18:19:14,353 [INFO] Step[150/2713]: training loss : 1.0601350581645965 TRAIN  loss dict:  {'classification_loss': 1.0601350581645965}
2025-01-12 18:19:26,207 [INFO] Step[200/2713]: training loss : 1.0382453095912934 TRAIN  loss dict:  {'classification_loss': 1.0382453095912934}
2025-01-12 18:19:38,117 [INFO] Step[250/2713]: training loss : 1.0854504358768464 TRAIN  loss dict:  {'classification_loss': 1.0854504358768464}
2025-01-12 18:19:50,023 [INFO] Step[300/2713]: training loss : 1.0295354747772216 TRAIN  loss dict:  {'classification_loss': 1.0295354747772216}
2025-01-12 18:20:01,913 [INFO] Step[350/2713]: training loss : 1.0643414211273194 TRAIN  loss dict:  {'classification_loss': 1.0643414211273194}
2025-01-12 18:20:13,791 [INFO] Step[400/2713]: training loss : 1.066948959827423 TRAIN  loss dict:  {'classification_loss': 1.066948959827423}
2025-01-12 18:20:25,665 [INFO] Step[450/2713]: training loss : 1.0727251172065735 TRAIN  loss dict:  {'classification_loss': 1.0727251172065735}
2025-01-12 18:20:37,557 [INFO] Step[500/2713]: training loss : 1.0687363421916962 TRAIN  loss dict:  {'classification_loss': 1.0687363421916962}
2025-01-12 18:20:49,407 [INFO] Step[550/2713]: training loss : 1.04324085354805 TRAIN  loss dict:  {'classification_loss': 1.04324085354805}
2025-01-12 18:21:01,348 [INFO] Step[600/2713]: training loss : 1.044061404466629 TRAIN  loss dict:  {'classification_loss': 1.044061404466629}
2025-01-12 18:21:13,227 [INFO] Step[650/2713]: training loss : 1.0833543646335602 TRAIN  loss dict:  {'classification_loss': 1.0833543646335602}
2025-01-12 18:21:25,138 [INFO] Step[700/2713]: training loss : 1.0366541242599487 TRAIN  loss dict:  {'classification_loss': 1.0366541242599487}
2025-01-12 18:21:37,094 [INFO] Step[750/2713]: training loss : 1.0434384906291962 TRAIN  loss dict:  {'classification_loss': 1.0434384906291962}
2025-01-12 18:21:49,023 [INFO] Step[800/2713]: training loss : 1.0323458695411682 TRAIN  loss dict:  {'classification_loss': 1.0323458695411682}
2025-01-12 18:22:00,927 [INFO] Step[850/2713]: training loss : 1.0951760256290435 TRAIN  loss dict:  {'classification_loss': 1.0951760256290435}
2025-01-12 18:22:12,797 [INFO] Step[900/2713]: training loss : 1.1028974533081055 TRAIN  loss dict:  {'classification_loss': 1.1028974533081055}
2025-01-12 18:22:24,740 [INFO] Step[950/2713]: training loss : 1.0508342230319976 TRAIN  loss dict:  {'classification_loss': 1.0508342230319976}
2025-01-12 18:22:36,643 [INFO] Step[1000/2713]: training loss : 1.0864800667762757 TRAIN  loss dict:  {'classification_loss': 1.0864800667762757}
2025-01-12 18:22:48,579 [INFO] Step[1050/2713]: training loss : 1.042237024307251 TRAIN  loss dict:  {'classification_loss': 1.042237024307251}
2025-01-12 18:23:00,510 [INFO] Step[1100/2713]: training loss : 1.0930969989299775 TRAIN  loss dict:  {'classification_loss': 1.0930969989299775}
2025-01-12 18:23:12,441 [INFO] Step[1150/2713]: training loss : 1.0653220105171204 TRAIN  loss dict:  {'classification_loss': 1.0653220105171204}
2025-01-12 18:23:24,365 [INFO] Step[1200/2713]: training loss : 1.0440025413036347 TRAIN  loss dict:  {'classification_loss': 1.0440025413036347}
2025-01-12 18:23:36,248 [INFO] Step[1250/2713]: training loss : 1.119486050605774 TRAIN  loss dict:  {'classification_loss': 1.119486050605774}
2025-01-12 18:23:48,162 [INFO] Step[1300/2713]: training loss : 1.039836128950119 TRAIN  loss dict:  {'classification_loss': 1.039836128950119}
2025-01-12 18:24:00,071 [INFO] Step[1350/2713]: training loss : 1.0753703689575196 TRAIN  loss dict:  {'classification_loss': 1.0753703689575196}
2025-01-12 18:24:11,976 [INFO] Step[1400/2713]: training loss : 1.086182734966278 TRAIN  loss dict:  {'classification_loss': 1.086182734966278}
2025-01-12 18:24:23,898 [INFO] Step[1450/2713]: training loss : 1.0502683889865876 TRAIN  loss dict:  {'classification_loss': 1.0502683889865876}
2025-01-12 18:24:35,815 [INFO] Step[1500/2713]: training loss : 1.0507382488250732 TRAIN  loss dict:  {'classification_loss': 1.0507382488250732}
2025-01-12 18:24:47,759 [INFO] Step[1550/2713]: training loss : 1.0526296496391296 TRAIN  loss dict:  {'classification_loss': 1.0526296496391296}
2025-01-12 18:24:59,724 [INFO] Step[1600/2713]: training loss : 1.0760227167606353 TRAIN  loss dict:  {'classification_loss': 1.0760227167606353}
2025-01-12 18:25:11,690 [INFO] Step[1650/2713]: training loss : 1.0681836783885956 TRAIN  loss dict:  {'classification_loss': 1.0681836783885956}
2025-01-12 18:25:23,621 [INFO] Step[1700/2713]: training loss : 1.0341350090503694 TRAIN  loss dict:  {'classification_loss': 1.0341350090503694}
2025-01-12 18:25:35,537 [INFO] Step[1750/2713]: training loss : 1.0865143656730651 TRAIN  loss dict:  {'classification_loss': 1.0865143656730651}
2025-01-12 18:25:47,440 [INFO] Step[1800/2713]: training loss : 1.0770452427864075 TRAIN  loss dict:  {'classification_loss': 1.0770452427864075}
2025-01-12 18:25:59,360 [INFO] Step[1850/2713]: training loss : 1.0700988817214965 TRAIN  loss dict:  {'classification_loss': 1.0700988817214965}
2025-01-12 18:26:11,275 [INFO] Step[1900/2713]: training loss : 1.040373251438141 TRAIN  loss dict:  {'classification_loss': 1.040373251438141}
2025-01-12 18:26:23,204 [INFO] Step[1950/2713]: training loss : 1.0845446705818176 TRAIN  loss dict:  {'classification_loss': 1.0845446705818176}
2025-01-12 18:26:35,094 [INFO] Step[2000/2713]: training loss : 1.0540809178352355 TRAIN  loss dict:  {'classification_loss': 1.0540809178352355}
2025-01-12 18:26:47,028 [INFO] Step[2050/2713]: training loss : 1.0488292729854585 TRAIN  loss dict:  {'classification_loss': 1.0488292729854585}
2025-01-12 18:26:58,972 [INFO] Step[2100/2713]: training loss : 1.0856082725524903 TRAIN  loss dict:  {'classification_loss': 1.0856082725524903}
2025-01-12 18:27:10,935 [INFO] Step[2150/2713]: training loss : 1.0693099558353425 TRAIN  loss dict:  {'classification_loss': 1.0693099558353425}
2025-01-12 18:27:22,779 [INFO] Step[2200/2713]: training loss : 1.0939106106758119 TRAIN  loss dict:  {'classification_loss': 1.0939106106758119}
2025-01-12 18:27:34,764 [INFO] Step[2250/2713]: training loss : 1.0880029273033143 TRAIN  loss dict:  {'classification_loss': 1.0880029273033143}
2025-01-12 18:27:46,717 [INFO] Step[2300/2713]: training loss : 1.083651577234268 TRAIN  loss dict:  {'classification_loss': 1.083651577234268}
2025-01-12 18:27:58,632 [INFO] Step[2350/2713]: training loss : 1.1030519604682922 TRAIN  loss dict:  {'classification_loss': 1.1030519604682922}
2025-01-12 18:28:10,565 [INFO] Step[2400/2713]: training loss : 1.0424781513214112 TRAIN  loss dict:  {'classification_loss': 1.0424781513214112}
2025-01-12 18:28:22,475 [INFO] Step[2450/2713]: training loss : 1.0654504787921906 TRAIN  loss dict:  {'classification_loss': 1.0654504787921906}
2025-01-12 18:28:34,405 [INFO] Step[2500/2713]: training loss : 1.0796188509464264 TRAIN  loss dict:  {'classification_loss': 1.0796188509464264}
2025-01-12 18:28:46,286 [INFO] Step[2550/2713]: training loss : 1.0735190093517304 TRAIN  loss dict:  {'classification_loss': 1.0735190093517304}
2025-01-12 18:28:58,180 [INFO] Step[2600/2713]: training loss : 1.030539880990982 TRAIN  loss dict:  {'classification_loss': 1.030539880990982}
2025-01-12 18:29:10,093 [INFO] Step[2650/2713]: training loss : 1.0511357855796815 TRAIN  loss dict:  {'classification_loss': 1.0511357855796815}
2025-01-12 18:29:21,963 [INFO] Step[2700/2713]: training loss : 1.061822797060013 TRAIN  loss dict:  {'classification_loss': 1.061822797060013}
2025-01-12 18:30:49,203 [INFO] Label accuracies statistics:
2025-01-12 18:30:49,204 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.5, 19: 0.75, 20: 0.75, 21: 1.0, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.25, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.25, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.75, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 1.0, 122: 0.75, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.25, 225: 1.0, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.5, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.5, 245: 1.0, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.75, 259: 0.5, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.25, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.5, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 1.0, 376: 1.0, 377: 1.0, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.5, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.75, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-12 18:30:51,488 [INFO] [17] TRAIN  loss: 1.065177458856451 acc: 0.9864848261457182
2025-01-12 18:30:51,488 [INFO] [17] TRAIN  loss dict: {'classification_loss': 1.065177458856451}
2025-01-12 18:30:51,488 [INFO] [17] VALIDATION loss: 1.6959741059550666 VALIDATION acc: 0.8131661442006269
2025-01-12 18:30:51,488 [INFO] [17] VALIDATION loss dict: {'classification_loss': 1.6959741059550666}
2025-01-12 18:30:51,488 [INFO] 
2025-01-12 18:31:09,159 [INFO] Step[50/2713]: training loss : 1.075796811580658 TRAIN  loss dict:  {'classification_loss': 1.075796811580658}
2025-01-12 18:31:21,034 [INFO] Step[100/2713]: training loss : 1.0695293486118316 TRAIN  loss dict:  {'classification_loss': 1.0695293486118316}
2025-01-12 18:31:32,965 [INFO] Step[150/2713]: training loss : 1.0599693763256073 TRAIN  loss dict:  {'classification_loss': 1.0599693763256073}
2025-01-12 18:31:44,834 [INFO] Step[200/2713]: training loss : 1.0449925827980042 TRAIN  loss dict:  {'classification_loss': 1.0449925827980042}
2025-01-12 18:31:56,726 [INFO] Step[250/2713]: training loss : 1.057772250175476 TRAIN  loss dict:  {'classification_loss': 1.057772250175476}
2025-01-12 18:32:08,654 [INFO] Step[300/2713]: training loss : 1.0251097166538239 TRAIN  loss dict:  {'classification_loss': 1.0251097166538239}
2025-01-12 18:32:20,554 [INFO] Step[350/2713]: training loss : 1.0470534479618072 TRAIN  loss dict:  {'classification_loss': 1.0470534479618072}
2025-01-12 18:32:32,495 [INFO] Step[400/2713]: training loss : 1.0660459280014039 TRAIN  loss dict:  {'classification_loss': 1.0660459280014039}
2025-01-12 18:32:44,417 [INFO] Step[450/2713]: training loss : 1.1204332602024079 TRAIN  loss dict:  {'classification_loss': 1.1204332602024079}
2025-01-12 18:32:56,302 [INFO] Step[500/2713]: training loss : 1.060868628025055 TRAIN  loss dict:  {'classification_loss': 1.060868628025055}
2025-01-12 18:33:08,214 [INFO] Step[550/2713]: training loss : 1.0795500445365906 TRAIN  loss dict:  {'classification_loss': 1.0795500445365906}
2025-01-12 18:33:20,129 [INFO] Step[600/2713]: training loss : 1.0382469499111175 TRAIN  loss dict:  {'classification_loss': 1.0382469499111175}
2025-01-12 18:33:32,064 [INFO] Step[650/2713]: training loss : 1.0637809765338897 TRAIN  loss dict:  {'classification_loss': 1.0637809765338897}
2025-01-12 18:33:43,961 [INFO] Step[700/2713]: training loss : 1.0925603210926056 TRAIN  loss dict:  {'classification_loss': 1.0925603210926056}
2025-01-12 18:33:55,851 [INFO] Step[750/2713]: training loss : 1.0586790072917938 TRAIN  loss dict:  {'classification_loss': 1.0586790072917938}
2025-01-12 18:34:07,776 [INFO] Step[800/2713]: training loss : 1.0529482460021973 TRAIN  loss dict:  {'classification_loss': 1.0529482460021973}
2025-01-12 18:34:19,682 [INFO] Step[850/2713]: training loss : 1.047087391614914 TRAIN  loss dict:  {'classification_loss': 1.047087391614914}
2025-01-12 18:34:31,574 [INFO] Step[900/2713]: training loss : 1.041572217941284 TRAIN  loss dict:  {'classification_loss': 1.041572217941284}
2025-01-12 18:34:43,451 [INFO] Step[950/2713]: training loss : 1.0286546182632446 TRAIN  loss dict:  {'classification_loss': 1.0286546182632446}
2025-01-12 18:34:55,371 [INFO] Step[1000/2713]: training loss : 1.0422272837162019 TRAIN  loss dict:  {'classification_loss': 1.0422272837162019}
2025-01-12 18:35:07,318 [INFO] Step[1050/2713]: training loss : 1.056126537322998 TRAIN  loss dict:  {'classification_loss': 1.056126537322998}
2025-01-12 18:35:19,218 [INFO] Step[1100/2713]: training loss : 1.0459567606449127 TRAIN  loss dict:  {'classification_loss': 1.0459567606449127}
2025-01-12 18:35:31,164 [INFO] Step[1150/2713]: training loss : 1.0261203384399413 TRAIN  loss dict:  {'classification_loss': 1.0261203384399413}
2025-01-12 18:35:43,061 [INFO] Step[1200/2713]: training loss : 1.0394722950458526 TRAIN  loss dict:  {'classification_loss': 1.0394722950458526}
2025-01-12 18:35:54,996 [INFO] Step[1250/2713]: training loss : 1.0344816112518311 TRAIN  loss dict:  {'classification_loss': 1.0344816112518311}
2025-01-12 18:36:07,232 [INFO] Step[1300/2713]: training loss : 1.0550718462467195 TRAIN  loss dict:  {'classification_loss': 1.0550718462467195}
2025-01-12 18:36:19,736 [INFO] Step[1350/2713]: training loss : 1.08948424577713 TRAIN  loss dict:  {'classification_loss': 1.08948424577713}
2025-01-12 18:36:32,190 [INFO] Step[1400/2713]: training loss : 1.0740004444122315 TRAIN  loss dict:  {'classification_loss': 1.0740004444122315}
2025-01-12 18:36:44,683 [INFO] Step[1450/2713]: training loss : 1.0377024960517884 TRAIN  loss dict:  {'classification_loss': 1.0377024960517884}
2025-01-12 18:36:57,266 [INFO] Step[1500/2713]: training loss : 1.058703452348709 TRAIN  loss dict:  {'classification_loss': 1.058703452348709}
2025-01-12 18:37:09,715 [INFO] Step[1550/2713]: training loss : 1.0574789154529571 TRAIN  loss dict:  {'classification_loss': 1.0574789154529571}
2025-01-12 18:37:22,814 [INFO] Step[1600/2713]: training loss : 1.0547177243232726 TRAIN  loss dict:  {'classification_loss': 1.0547177243232726}
2025-01-12 18:37:36,406 [INFO] Step[1650/2713]: training loss : 1.0350986576080323 TRAIN  loss dict:  {'classification_loss': 1.0350986576080323}
2025-01-12 18:37:49,265 [INFO] Step[1700/2713]: training loss : 1.0442482888698579 TRAIN  loss dict:  {'classification_loss': 1.0442482888698579}
2025-01-12 18:38:01,223 [INFO] Step[1750/2713]: training loss : 1.0744232177734374 TRAIN  loss dict:  {'classification_loss': 1.0744232177734374}
2025-01-12 18:38:13,056 [INFO] Step[1800/2713]: training loss : 1.0440774476528167 TRAIN  loss dict:  {'classification_loss': 1.0440774476528167}
2025-01-12 18:38:24,899 [INFO] Step[1850/2713]: training loss : 1.0970373582839965 TRAIN  loss dict:  {'classification_loss': 1.0970373582839965}
2025-01-12 18:38:36,758 [INFO] Step[1900/2713]: training loss : 1.051941978931427 TRAIN  loss dict:  {'classification_loss': 1.051941978931427}
2025-01-12 18:38:48,632 [INFO] Step[1950/2713]: training loss : 1.0567272782325745 TRAIN  loss dict:  {'classification_loss': 1.0567272782325745}
2025-01-12 18:39:00,490 [INFO] Step[2000/2713]: training loss : 1.0387532198429108 TRAIN  loss dict:  {'classification_loss': 1.0387532198429108}
2025-01-12 18:39:12,371 [INFO] Step[2050/2713]: training loss : 1.0584522342681886 TRAIN  loss dict:  {'classification_loss': 1.0584522342681886}
2025-01-12 18:39:24,204 [INFO] Step[2100/2713]: training loss : 1.0599807941913604 TRAIN  loss dict:  {'classification_loss': 1.0599807941913604}
2025-01-12 18:39:36,087 [INFO] Step[2150/2713]: training loss : 1.017261255979538 TRAIN  loss dict:  {'classification_loss': 1.017261255979538}
2025-01-12 18:39:48,033 [INFO] Step[2200/2713]: training loss : 1.0728384709358216 TRAIN  loss dict:  {'classification_loss': 1.0728384709358216}
2025-01-12 18:39:59,919 [INFO] Step[2250/2713]: training loss : 1.059002560377121 TRAIN  loss dict:  {'classification_loss': 1.059002560377121}
2025-01-12 18:40:11,828 [INFO] Step[2300/2713]: training loss : 1.048416450023651 TRAIN  loss dict:  {'classification_loss': 1.048416450023651}
2025-01-12 18:40:23,705 [INFO] Step[2350/2713]: training loss : 1.0497376930713653 TRAIN  loss dict:  {'classification_loss': 1.0497376930713653}
2025-01-12 18:40:35,509 [INFO] Step[2400/2713]: training loss : 1.0486668682098388 TRAIN  loss dict:  {'classification_loss': 1.0486668682098388}
2025-01-12 18:40:47,419 [INFO] Step[2450/2713]: training loss : 1.0771372425556183 TRAIN  loss dict:  {'classification_loss': 1.0771372425556183}
2025-01-12 18:40:59,304 [INFO] Step[2500/2713]: training loss : 1.0518215954303742 TRAIN  loss dict:  {'classification_loss': 1.0518215954303742}
2025-01-12 18:41:11,159 [INFO] Step[2550/2713]: training loss : 1.03894655585289 TRAIN  loss dict:  {'classification_loss': 1.03894655585289}
2025-01-12 18:41:23,037 [INFO] Step[2600/2713]: training loss : 1.0511804044246673 TRAIN  loss dict:  {'classification_loss': 1.0511804044246673}
2025-01-12 18:41:34,928 [INFO] Step[2650/2713]: training loss : 1.0597830879688264 TRAIN  loss dict:  {'classification_loss': 1.0597830879688264}
2025-01-12 18:41:46,806 [INFO] Step[2700/2713]: training loss : 1.0479096233844758 TRAIN  loss dict:  {'classification_loss': 1.0479096233844758}
2025-01-12 18:43:13,089 [INFO] Label accuracies statistics:
2025-01-12 18:43:13,089 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 0.75, 99: 0.8, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 0.75, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 1.0, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.25, 183: 0.25, 184: 0.5, 185: 0.75, 186: 0.5, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 0.75, 192: 1.0, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.25, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.5, 260: 0.25, 261: 0.5, 262: 0.75, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.25, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 1.0, 279: 0.75, 280: 0.75, 281: 0.75, 282: 0.75, 283: 1.0, 284: 1.0, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.5, 291: 0.5, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.25, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.5, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.5, 354: 1.0, 355: 0.5, 356: 0.5, 357: 0.75, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.0, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 18:43:13,091 [INFO] [18] TRAIN  loss: 1.0556925130953791 acc: 0.9888192652660032
2025-01-12 18:43:13,091 [INFO] [18] TRAIN  loss dict: {'classification_loss': 1.0556925130953791}
2025-01-12 18:43:13,091 [INFO] [18] VALIDATION loss: 1.7843656906283887 VALIDATION acc: 0.7887147335423198
2025-01-12 18:43:13,091 [INFO] [18] VALIDATION loss dict: {'classification_loss': 1.7843656906283887}
2025-01-12 18:43:13,091 [INFO] 
2025-01-12 18:43:30,454 [INFO] Step[50/2713]: training loss : 1.0424106431007385 TRAIN  loss dict:  {'classification_loss': 1.0424106431007385}
2025-01-12 18:43:42,323 [INFO] Step[100/2713]: training loss : 1.0410806620121003 TRAIN  loss dict:  {'classification_loss': 1.0410806620121003}
2025-01-12 18:43:54,242 [INFO] Step[150/2713]: training loss : 1.0850881445407867 TRAIN  loss dict:  {'classification_loss': 1.0850881445407867}
2025-01-12 18:44:06,131 [INFO] Step[200/2713]: training loss : 1.0489357209205628 TRAIN  loss dict:  {'classification_loss': 1.0489357209205628}
2025-01-12 18:44:18,046 [INFO] Step[250/2713]: training loss : 1.1195918834209442 TRAIN  loss dict:  {'classification_loss': 1.1195918834209442}
2025-01-12 18:44:29,975 [INFO] Step[300/2713]: training loss : 1.053485208749771 TRAIN  loss dict:  {'classification_loss': 1.053485208749771}
2025-01-12 18:44:41,884 [INFO] Step[350/2713]: training loss : 1.0531651401519775 TRAIN  loss dict:  {'classification_loss': 1.0531651401519775}
2025-01-12 18:44:53,784 [INFO] Step[400/2713]: training loss : 1.0848628234863282 TRAIN  loss dict:  {'classification_loss': 1.0848628234863282}
2025-01-12 18:45:05,700 [INFO] Step[450/2713]: training loss : 1.05702464222908 TRAIN  loss dict:  {'classification_loss': 1.05702464222908}
2025-01-12 18:45:17,591 [INFO] Step[500/2713]: training loss : 1.0481305861473083 TRAIN  loss dict:  {'classification_loss': 1.0481305861473083}
2025-01-12 18:45:29,523 [INFO] Step[550/2713]: training loss : 1.038949669599533 TRAIN  loss dict:  {'classification_loss': 1.038949669599533}
2025-01-12 18:45:41,422 [INFO] Step[600/2713]: training loss : 1.0896002769470214 TRAIN  loss dict:  {'classification_loss': 1.0896002769470214}
2025-01-12 18:45:53,306 [INFO] Step[650/2713]: training loss : 1.0301915085315705 TRAIN  loss dict:  {'classification_loss': 1.0301915085315705}
2025-01-12 18:46:05,131 [INFO] Step[700/2713]: training loss : 1.0562501299381255 TRAIN  loss dict:  {'classification_loss': 1.0562501299381255}
2025-01-12 18:46:17,006 [INFO] Step[750/2713]: training loss : 1.0148716914653777 TRAIN  loss dict:  {'classification_loss': 1.0148716914653777}
2025-01-12 18:46:28,899 [INFO] Step[800/2713]: training loss : 1.0453124284744262 TRAIN  loss dict:  {'classification_loss': 1.0453124284744262}
2025-01-12 18:46:40,798 [INFO] Step[850/2713]: training loss : 1.0629858911037444 TRAIN  loss dict:  {'classification_loss': 1.0629858911037444}
2025-01-12 18:46:52,701 [INFO] Step[900/2713]: training loss : 1.05085622549057 TRAIN  loss dict:  {'classification_loss': 1.05085622549057}
2025-01-12 18:47:04,614 [INFO] Step[950/2713]: training loss : 1.0335247480869294 TRAIN  loss dict:  {'classification_loss': 1.0335247480869294}
2025-01-12 18:47:16,454 [INFO] Step[1000/2713]: training loss : 1.0486265194416047 TRAIN  loss dict:  {'classification_loss': 1.0486265194416047}
2025-01-12 18:47:28,354 [INFO] Step[1050/2713]: training loss : 1.0592220532894134 TRAIN  loss dict:  {'classification_loss': 1.0592220532894134}
2025-01-12 18:47:40,210 [INFO] Step[1100/2713]: training loss : 1.0693600535392762 TRAIN  loss dict:  {'classification_loss': 1.0693600535392762}
2025-01-12 18:47:52,124 [INFO] Step[1150/2713]: training loss : 1.049091739654541 TRAIN  loss dict:  {'classification_loss': 1.049091739654541}
2025-01-12 18:48:04,032 [INFO] Step[1200/2713]: training loss : 1.027569500207901 TRAIN  loss dict:  {'classification_loss': 1.027569500207901}
2025-01-12 18:48:15,892 [INFO] Step[1250/2713]: training loss : 1.0835701298713685 TRAIN  loss dict:  {'classification_loss': 1.0835701298713685}
2025-01-12 18:48:27,750 [INFO] Step[1300/2713]: training loss : 1.049609373807907 TRAIN  loss dict:  {'classification_loss': 1.049609373807907}
2025-01-12 18:48:39,651 [INFO] Step[1350/2713]: training loss : 1.0477341163158416 TRAIN  loss dict:  {'classification_loss': 1.0477341163158416}
2025-01-12 18:48:51,585 [INFO] Step[1400/2713]: training loss : 1.0488123869895936 TRAIN  loss dict:  {'classification_loss': 1.0488123869895936}
2025-01-12 18:49:03,542 [INFO] Step[1450/2713]: training loss : 1.0718226540088653 TRAIN  loss dict:  {'classification_loss': 1.0718226540088653}
2025-01-12 18:49:15,451 [INFO] Step[1500/2713]: training loss : 1.0543217301368712 TRAIN  loss dict:  {'classification_loss': 1.0543217301368712}
2025-01-12 18:49:27,363 [INFO] Step[1550/2713]: training loss : 1.0900953757762908 TRAIN  loss dict:  {'classification_loss': 1.0900953757762908}
2025-01-12 18:49:39,246 [INFO] Step[1600/2713]: training loss : 1.0470550620555878 TRAIN  loss dict:  {'classification_loss': 1.0470550620555878}
2025-01-12 18:49:51,202 [INFO] Step[1650/2713]: training loss : 1.0436336588859558 TRAIN  loss dict:  {'classification_loss': 1.0436336588859558}
2025-01-12 18:50:03,100 [INFO] Step[1700/2713]: training loss : 1.020255000591278 TRAIN  loss dict:  {'classification_loss': 1.020255000591278}
2025-01-12 18:50:15,003 [INFO] Step[1750/2713]: training loss : 1.0578228795528413 TRAIN  loss dict:  {'classification_loss': 1.0578228795528413}
2025-01-12 18:50:26,960 [INFO] Step[1800/2713]: training loss : 1.0281916069984436 TRAIN  loss dict:  {'classification_loss': 1.0281916069984436}
2025-01-12 18:50:38,855 [INFO] Step[1850/2713]: training loss : 1.093958604335785 TRAIN  loss dict:  {'classification_loss': 1.093958604335785}
2025-01-12 18:50:50,776 [INFO] Step[1900/2713]: training loss : 1.0629552567005158 TRAIN  loss dict:  {'classification_loss': 1.0629552567005158}
2025-01-12 18:51:02,647 [INFO] Step[1950/2713]: training loss : 1.050202066898346 TRAIN  loss dict:  {'classification_loss': 1.050202066898346}
2025-01-12 18:51:14,588 [INFO] Step[2000/2713]: training loss : 1.052817931175232 TRAIN  loss dict:  {'classification_loss': 1.052817931175232}
2025-01-12 18:51:26,497 [INFO] Step[2050/2713]: training loss : 1.0899256241321564 TRAIN  loss dict:  {'classification_loss': 1.0899256241321564}
2025-01-12 18:51:38,406 [INFO] Step[2100/2713]: training loss : 1.042169337272644 TRAIN  loss dict:  {'classification_loss': 1.042169337272644}
2025-01-12 18:51:50,308 [INFO] Step[2150/2713]: training loss : 1.0617318594455718 TRAIN  loss dict:  {'classification_loss': 1.0617318594455718}
2025-01-12 18:52:02,169 [INFO] Step[2200/2713]: training loss : 1.0345696651935576 TRAIN  loss dict:  {'classification_loss': 1.0345696651935576}
2025-01-12 18:52:14,080 [INFO] Step[2250/2713]: training loss : 1.064187763929367 TRAIN  loss dict:  {'classification_loss': 1.064187763929367}
2025-01-12 18:52:26,016 [INFO] Step[2300/2713]: training loss : 1.0787846982479095 TRAIN  loss dict:  {'classification_loss': 1.0787846982479095}
2025-01-12 18:52:37,935 [INFO] Step[2350/2713]: training loss : 1.067239866256714 TRAIN  loss dict:  {'classification_loss': 1.067239866256714}
2025-01-12 18:52:49,813 [INFO] Step[2400/2713]: training loss : 1.0338432538509368 TRAIN  loss dict:  {'classification_loss': 1.0338432538509368}
2025-01-12 18:53:01,766 [INFO] Step[2450/2713]: training loss : 1.0445133233070374 TRAIN  loss dict:  {'classification_loss': 1.0445133233070374}
2025-01-12 18:53:13,664 [INFO] Step[2500/2713]: training loss : 1.0718242597579957 TRAIN  loss dict:  {'classification_loss': 1.0718242597579957}
2025-01-12 18:53:25,618 [INFO] Step[2550/2713]: training loss : 1.0462366127967835 TRAIN  loss dict:  {'classification_loss': 1.0462366127967835}
2025-01-12 18:53:37,514 [INFO] Step[2600/2713]: training loss : 1.044605700969696 TRAIN  loss dict:  {'classification_loss': 1.044605700969696}
2025-01-12 18:53:49,428 [INFO] Step[2650/2713]: training loss : 1.025144375562668 TRAIN  loss dict:  {'classification_loss': 1.025144375562668}
2025-01-12 18:54:01,325 [INFO] Step[2700/2713]: training loss : 1.051410415172577 TRAIN  loss dict:  {'classification_loss': 1.051410415172577}
2025-01-12 18:55:31,563 [INFO] Label accuracies statistics:
2025-01-12 18:55:31,564 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.5, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.25, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.25, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 0.75, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 0.75, 131: 1.0, 132: 0.75, 133: 0.75, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.5, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 1.0, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.5, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 0.75, 242: 1.0, 243: 0.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 0.75, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 0.75, 266: 1.0, 267: 1.0, 268: 1.0, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.5, 276: 0.5, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.25, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.5, 355: 0.75, 356: 0.5, 357: 0.75, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.25, 376: 1.0, 377: 0.75, 378: 1.0, 379: 0.75, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-12 18:55:31,567 [INFO] [19] TRAIN  loss: 1.0551322123637905 acc: 0.9891878609165745
2025-01-12 18:55:31,568 [INFO] [19] TRAIN  loss dict: {'classification_loss': 1.0551322123637905}
2025-01-12 18:55:31,568 [INFO] [19] VALIDATION loss: 1.8095835028286267 VALIDATION acc: 0.7918495297805642
2025-01-12 18:55:31,568 [INFO] [19] VALIDATION loss dict: {'classification_loss': 1.8095835028286267}
2025-01-12 18:55:31,568 [INFO] 
2025-01-12 18:55:51,659 [INFO] Step[50/2713]: training loss : 1.093072236776352 TRAIN  loss dict:  {'classification_loss': 1.093072236776352}
2025-01-12 18:56:04,265 [INFO] Step[100/2713]: training loss : 1.044333974123001 TRAIN  loss dict:  {'classification_loss': 1.044333974123001}
2025-01-12 18:56:16,718 [INFO] Step[150/2713]: training loss : 1.0636135292053224 TRAIN  loss dict:  {'classification_loss': 1.0636135292053224}
2025-01-12 18:56:29,573 [INFO] Step[200/2713]: training loss : 1.0198921656608582 TRAIN  loss dict:  {'classification_loss': 1.0198921656608582}
2025-01-12 18:56:43,396 [INFO] Step[250/2713]: training loss : 1.0218625509738921 TRAIN  loss dict:  {'classification_loss': 1.0218625509738921}
2025-01-12 18:56:56,481 [INFO] Step[300/2713]: training loss : 1.0383759891986848 TRAIN  loss dict:  {'classification_loss': 1.0383759891986848}
2025-01-12 18:57:08,689 [INFO] Step[350/2713]: training loss : 1.0308569514751433 TRAIN  loss dict:  {'classification_loss': 1.0308569514751433}
2025-01-12 18:57:20,552 [INFO] Step[400/2713]: training loss : 1.049754993915558 TRAIN  loss dict:  {'classification_loss': 1.049754993915558}
2025-01-12 18:57:32,408 [INFO] Step[450/2713]: training loss : 1.032584022283554 TRAIN  loss dict:  {'classification_loss': 1.032584022283554}
2025-01-12 18:57:44,360 [INFO] Step[500/2713]: training loss : 1.035079562664032 TRAIN  loss dict:  {'classification_loss': 1.035079562664032}
2025-01-12 18:57:56,235 [INFO] Step[550/2713]: training loss : 1.0473983502388 TRAIN  loss dict:  {'classification_loss': 1.0473983502388}
2025-01-12 18:58:08,114 [INFO] Step[600/2713]: training loss : 1.0478772044181823 TRAIN  loss dict:  {'classification_loss': 1.0478772044181823}
2025-01-12 18:58:20,032 [INFO] Step[650/2713]: training loss : 1.0501967775821686 TRAIN  loss dict:  {'classification_loss': 1.0501967775821686}
2025-01-12 18:58:31,885 [INFO] Step[700/2713]: training loss : 1.0453225815296172 TRAIN  loss dict:  {'classification_loss': 1.0453225815296172}
2025-01-12 18:58:43,833 [INFO] Step[750/2713]: training loss : 1.0461192095279694 TRAIN  loss dict:  {'classification_loss': 1.0461192095279694}
2025-01-12 18:58:55,699 [INFO] Step[800/2713]: training loss : 1.0486708116531371 TRAIN  loss dict:  {'classification_loss': 1.0486708116531371}
2025-01-12 18:59:07,568 [INFO] Step[850/2713]: training loss : 1.0689090502262115 TRAIN  loss dict:  {'classification_loss': 1.0689090502262115}
2025-01-12 18:59:19,541 [INFO] Step[900/2713]: training loss : 1.0431934082508088 TRAIN  loss dict:  {'classification_loss': 1.0431934082508088}
2025-01-12 18:59:31,474 [INFO] Step[950/2713]: training loss : 1.0596812796592712 TRAIN  loss dict:  {'classification_loss': 1.0596812796592712}
2025-01-12 18:59:43,405 [INFO] Step[1000/2713]: training loss : 1.0678793060779572 TRAIN  loss dict:  {'classification_loss': 1.0678793060779572}
2025-01-12 18:59:55,328 [INFO] Step[1050/2713]: training loss : 1.0554005408287048 TRAIN  loss dict:  {'classification_loss': 1.0554005408287048}
2025-01-12 19:00:07,252 [INFO] Step[1100/2713]: training loss : 1.0401932668685914 TRAIN  loss dict:  {'classification_loss': 1.0401932668685914}
2025-01-12 19:00:19,176 [INFO] Step[1150/2713]: training loss : 1.0549110746383668 TRAIN  loss dict:  {'classification_loss': 1.0549110746383668}
2025-01-12 19:00:31,071 [INFO] Step[1200/2713]: training loss : 1.0556542813777923 TRAIN  loss dict:  {'classification_loss': 1.0556542813777923}
2025-01-12 19:00:43,033 [INFO] Step[1250/2713]: training loss : 1.0558717346191406 TRAIN  loss dict:  {'classification_loss': 1.0558717346191406}
2025-01-12 19:00:54,940 [INFO] Step[1300/2713]: training loss : 1.075264928340912 TRAIN  loss dict:  {'classification_loss': 1.075264928340912}
2025-01-12 19:01:06,854 [INFO] Step[1350/2713]: training loss : 1.0462344670295716 TRAIN  loss dict:  {'classification_loss': 1.0462344670295716}
2025-01-12 19:01:18,747 [INFO] Step[1400/2713]: training loss : 1.0681626093387604 TRAIN  loss dict:  {'classification_loss': 1.0681626093387604}
2025-01-12 19:01:30,685 [INFO] Step[1450/2713]: training loss : 1.0448670399188995 TRAIN  loss dict:  {'classification_loss': 1.0448670399188995}
2025-01-12 19:01:42,610 [INFO] Step[1500/2713]: training loss : 1.0517529487609862 TRAIN  loss dict:  {'classification_loss': 1.0517529487609862}
2025-01-12 19:01:54,515 [INFO] Step[1550/2713]: training loss : 1.0311497926712037 TRAIN  loss dict:  {'classification_loss': 1.0311497926712037}
2025-01-12 19:02:06,447 [INFO] Step[1600/2713]: training loss : 1.0406426513195037 TRAIN  loss dict:  {'classification_loss': 1.0406426513195037}
2025-01-12 19:02:18,329 [INFO] Step[1650/2713]: training loss : 1.0379139018058776 TRAIN  loss dict:  {'classification_loss': 1.0379139018058776}
2025-01-12 19:02:30,279 [INFO] Step[1700/2713]: training loss : 1.0779668414592742 TRAIN  loss dict:  {'classification_loss': 1.0779668414592742}
2025-01-12 19:02:42,206 [INFO] Step[1750/2713]: training loss : 1.0327594184875488 TRAIN  loss dict:  {'classification_loss': 1.0327594184875488}
2025-01-12 19:02:54,088 [INFO] Step[1800/2713]: training loss : 1.0386039507389069 TRAIN  loss dict:  {'classification_loss': 1.0386039507389069}
2025-01-12 19:03:05,994 [INFO] Step[1850/2713]: training loss : 1.0357730650901795 TRAIN  loss dict:  {'classification_loss': 1.0357730650901795}
2025-01-12 19:03:17,934 [INFO] Step[1900/2713]: training loss : 1.0215100538730622 TRAIN  loss dict:  {'classification_loss': 1.0215100538730622}
2025-01-12 19:03:29,822 [INFO] Step[1950/2713]: training loss : 1.0559598112106323 TRAIN  loss dict:  {'classification_loss': 1.0559598112106323}
2025-01-12 19:03:41,726 [INFO] Step[2000/2713]: training loss : 1.0635908377170562 TRAIN  loss dict:  {'classification_loss': 1.0635908377170562}
2025-01-12 19:03:53,662 [INFO] Step[2050/2713]: training loss : 1.0451390528678894 TRAIN  loss dict:  {'classification_loss': 1.0451390528678894}
2025-01-12 19:04:05,561 [INFO] Step[2100/2713]: training loss : 1.0484287798404694 TRAIN  loss dict:  {'classification_loss': 1.0484287798404694}
2025-01-12 19:04:17,467 [INFO] Step[2150/2713]: training loss : 1.0366035544872283 TRAIN  loss dict:  {'classification_loss': 1.0366035544872283}
2025-01-12 19:04:29,374 [INFO] Step[2200/2713]: training loss : 1.0405047369003295 TRAIN  loss dict:  {'classification_loss': 1.0405047369003295}
2025-01-12 19:04:41,316 [INFO] Step[2250/2713]: training loss : 1.051547465324402 TRAIN  loss dict:  {'classification_loss': 1.051547465324402}
2025-01-12 19:04:53,251 [INFO] Step[2300/2713]: training loss : 1.0442555582523345 TRAIN  loss dict:  {'classification_loss': 1.0442555582523345}
2025-01-12 19:05:05,172 [INFO] Step[2350/2713]: training loss : 1.0404044044017793 TRAIN  loss dict:  {'classification_loss': 1.0404044044017793}
2025-01-12 19:05:17,082 [INFO] Step[2400/2713]: training loss : 1.0430478262901306 TRAIN  loss dict:  {'classification_loss': 1.0430478262901306}
2025-01-12 19:05:28,957 [INFO] Step[2450/2713]: training loss : 1.0241192626953124 TRAIN  loss dict:  {'classification_loss': 1.0241192626953124}
2025-01-12 19:05:40,835 [INFO] Step[2500/2713]: training loss : 1.0816211128234863 TRAIN  loss dict:  {'classification_loss': 1.0816211128234863}
2025-01-12 19:05:52,785 [INFO] Step[2550/2713]: training loss : 1.0374752879142761 TRAIN  loss dict:  {'classification_loss': 1.0374752879142761}
2025-01-12 19:06:04,618 [INFO] Step[2600/2713]: training loss : 1.0274686229228973 TRAIN  loss dict:  {'classification_loss': 1.0274686229228973}
2025-01-12 19:06:16,511 [INFO] Step[2650/2713]: training loss : 1.023345091342926 TRAIN  loss dict:  {'classification_loss': 1.023345091342926}
2025-01-12 19:06:28,355 [INFO] Step[2700/2713]: training loss : 1.0518641221523284 TRAIN  loss dict:  {'classification_loss': 1.0518641221523284}
2025-01-12 19:07:55,645 [INFO] Label accuracies statistics:
2025-01-12 19:07:55,646 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.5, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 0.75, 109: 0.5, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.75, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.75, 240: 1.0, 241: 0.75, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 0.75, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.75, 261: 1.0, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 0.5, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 0.75, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.5, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.25, 350: 0.25, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.5, 357: 0.5, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.25, 371: 1.0, 372: 0.75, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.5, 377: 1.0, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 0.75, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 19:07:55,647 [INFO] [20] TRAIN  loss: 1.0467697485891427 acc: 0.9899250522177172
2025-01-12 19:07:55,647 [INFO] [20] TRAIN  loss dict: {'classification_loss': 1.0467697485891427}
2025-01-12 19:07:55,648 [INFO] [20] VALIDATION loss: 1.7821013528601568 VALIDATION acc: 0.7974921630094044
2025-01-12 19:07:55,648 [INFO] [20] VALIDATION loss dict: {'classification_loss': 1.7821013528601568}
2025-01-12 19:07:55,648 [INFO] 
2025-01-12 19:08:13,509 [INFO] Step[50/2713]: training loss : 1.0352768683433533 TRAIN  loss dict:  {'classification_loss': 1.0352768683433533}
2025-01-12 19:08:25,430 [INFO] Step[100/2713]: training loss : 1.0521595692634582 TRAIN  loss dict:  {'classification_loss': 1.0521595692634582}
2025-01-12 19:08:37,379 [INFO] Step[150/2713]: training loss : 1.0447464883327484 TRAIN  loss dict:  {'classification_loss': 1.0447464883327484}
2025-01-12 19:08:49,299 [INFO] Step[200/2713]: training loss : 1.062331178188324 TRAIN  loss dict:  {'classification_loss': 1.062331178188324}
2025-01-12 19:09:01,240 [INFO] Step[250/2713]: training loss : 1.0157204103469848 TRAIN  loss dict:  {'classification_loss': 1.0157204103469848}
2025-01-12 19:09:13,123 [INFO] Step[300/2713]: training loss : 1.047979429960251 TRAIN  loss dict:  {'classification_loss': 1.047979429960251}
2025-01-12 19:09:25,097 [INFO] Step[350/2713]: training loss : 1.0249484288692474 TRAIN  loss dict:  {'classification_loss': 1.0249484288692474}
2025-01-12 19:09:37,003 [INFO] Step[400/2713]: training loss : 1.0168797981739044 TRAIN  loss dict:  {'classification_loss': 1.0168797981739044}
2025-01-12 19:09:48,900 [INFO] Step[450/2713]: training loss : 1.0386666941642761 TRAIN  loss dict:  {'classification_loss': 1.0386666941642761}
2025-01-12 19:10:00,775 [INFO] Step[500/2713]: training loss : 1.018804875612259 TRAIN  loss dict:  {'classification_loss': 1.018804875612259}
2025-01-12 19:10:12,691 [INFO] Step[550/2713]: training loss : 1.0276744437217713 TRAIN  loss dict:  {'classification_loss': 1.0276744437217713}
2025-01-12 19:10:24,592 [INFO] Step[600/2713]: training loss : 1.0283932733535766 TRAIN  loss dict:  {'classification_loss': 1.0283932733535766}
2025-01-12 19:10:36,524 [INFO] Step[650/2713]: training loss : 1.0431331312656402 TRAIN  loss dict:  {'classification_loss': 1.0431331312656402}
2025-01-12 19:10:48,438 [INFO] Step[700/2713]: training loss : 1.0438623309135437 TRAIN  loss dict:  {'classification_loss': 1.0438623309135437}
2025-01-12 19:11:00,332 [INFO] Step[750/2713]: training loss : 1.030904825925827 TRAIN  loss dict:  {'classification_loss': 1.030904825925827}
2025-01-12 19:11:12,210 [INFO] Step[800/2713]: training loss : 1.0255778753757476 TRAIN  loss dict:  {'classification_loss': 1.0255778753757476}
2025-01-12 19:11:24,142 [INFO] Step[850/2713]: training loss : 1.0223369085788727 TRAIN  loss dict:  {'classification_loss': 1.0223369085788727}
2025-01-12 19:11:36,063 [INFO] Step[900/2713]: training loss : 1.0320836186408997 TRAIN  loss dict:  {'classification_loss': 1.0320836186408997}
2025-01-12 19:11:48,000 [INFO] Step[950/2713]: training loss : 1.0109796977043153 TRAIN  loss dict:  {'classification_loss': 1.0109796977043153}
2025-01-12 19:11:59,928 [INFO] Step[1000/2713]: training loss : 1.0292529022693635 TRAIN  loss dict:  {'classification_loss': 1.0292529022693635}
2025-01-12 19:12:11,816 [INFO] Step[1050/2713]: training loss : 1.0274410688877105 TRAIN  loss dict:  {'classification_loss': 1.0274410688877105}
2025-01-12 19:12:23,731 [INFO] Step[1100/2713]: training loss : 1.0248574614524841 TRAIN  loss dict:  {'classification_loss': 1.0248574614524841}
2025-01-12 19:12:35,597 [INFO] Step[1150/2713]: training loss : 1.0314601743221283 TRAIN  loss dict:  {'classification_loss': 1.0314601743221283}
2025-01-12 19:12:47,512 [INFO] Step[1200/2713]: training loss : 1.0155116057395934 TRAIN  loss dict:  {'classification_loss': 1.0155116057395934}
2025-01-12 19:12:59,405 [INFO] Step[1250/2713]: training loss : 1.0565848600864411 TRAIN  loss dict:  {'classification_loss': 1.0565848600864411}
2025-01-12 19:13:11,301 [INFO] Step[1300/2713]: training loss : 1.029690033197403 TRAIN  loss dict:  {'classification_loss': 1.029690033197403}
2025-01-12 19:13:23,217 [INFO] Step[1350/2713]: training loss : 1.0678437411785127 TRAIN  loss dict:  {'classification_loss': 1.0678437411785127}
2025-01-12 19:13:35,123 [INFO] Step[1400/2713]: training loss : 1.0086423242092133 TRAIN  loss dict:  {'classification_loss': 1.0086423242092133}
2025-01-12 19:13:47,031 [INFO] Step[1450/2713]: training loss : 1.0362385749816894 TRAIN  loss dict:  {'classification_loss': 1.0362385749816894}
2025-01-12 19:13:58,978 [INFO] Step[1500/2713]: training loss : 1.0194762885570525 TRAIN  loss dict:  {'classification_loss': 1.0194762885570525}
2025-01-12 19:14:11,424 [INFO] Step[1550/2713]: training loss : 1.023455229997635 TRAIN  loss dict:  {'classification_loss': 1.023455229997635}
2025-01-12 19:14:23,944 [INFO] Step[1600/2713]: training loss : 1.0612370574474335 TRAIN  loss dict:  {'classification_loss': 1.0612370574474335}
2025-01-12 19:14:36,231 [INFO] Step[1650/2713]: training loss : 1.0476886343955993 TRAIN  loss dict:  {'classification_loss': 1.0476886343955993}
2025-01-12 19:14:48,919 [INFO] Step[1700/2713]: training loss : 1.008755030632019 TRAIN  loss dict:  {'classification_loss': 1.008755030632019}
2025-01-12 19:15:01,377 [INFO] Step[1750/2713]: training loss : 1.0406233847141266 TRAIN  loss dict:  {'classification_loss': 1.0406233847141266}
2025-01-12 19:15:13,960 [INFO] Step[1800/2713]: training loss : 1.0395954728126526 TRAIN  loss dict:  {'classification_loss': 1.0395954728126526}
2025-01-12 19:15:27,007 [INFO] Step[1850/2713]: training loss : 1.0151628017425538 TRAIN  loss dict:  {'classification_loss': 1.0151628017425538}
2025-01-12 19:15:40,433 [INFO] Step[1900/2713]: training loss : 1.0332263839244842 TRAIN  loss dict:  {'classification_loss': 1.0332263839244842}
2025-01-12 19:15:53,149 [INFO] Step[1950/2713]: training loss : 1.0456303322315217 TRAIN  loss dict:  {'classification_loss': 1.0456303322315217}
2025-01-12 19:16:05,139 [INFO] Step[2000/2713]: training loss : 1.0241186285018922 TRAIN  loss dict:  {'classification_loss': 1.0241186285018922}
2025-01-12 19:16:17,049 [INFO] Step[2050/2713]: training loss : 1.032854939699173 TRAIN  loss dict:  {'classification_loss': 1.032854939699173}
2025-01-12 19:16:28,906 [INFO] Step[2100/2713]: training loss : 1.0499277079105378 TRAIN  loss dict:  {'classification_loss': 1.0499277079105378}
2025-01-12 19:16:40,824 [INFO] Step[2150/2713]: training loss : 1.0555165779590607 TRAIN  loss dict:  {'classification_loss': 1.0555165779590607}
2025-01-12 19:16:52,674 [INFO] Step[2200/2713]: training loss : 1.024974434375763 TRAIN  loss dict:  {'classification_loss': 1.024974434375763}
2025-01-12 19:17:04,559 [INFO] Step[2250/2713]: training loss : 1.0165347266197204 TRAIN  loss dict:  {'classification_loss': 1.0165347266197204}
2025-01-12 19:17:16,481 [INFO] Step[2300/2713]: training loss : 1.025034600496292 TRAIN  loss dict:  {'classification_loss': 1.025034600496292}
2025-01-12 19:17:28,327 [INFO] Step[2350/2713]: training loss : 1.0267641484737395 TRAIN  loss dict:  {'classification_loss': 1.0267641484737395}
2025-01-12 19:17:40,247 [INFO] Step[2400/2713]: training loss : 1.0630698001384735 TRAIN  loss dict:  {'classification_loss': 1.0630698001384735}
2025-01-12 19:17:52,139 [INFO] Step[2450/2713]: training loss : 1.0414374768733978 TRAIN  loss dict:  {'classification_loss': 1.0414374768733978}
2025-01-12 19:18:04,073 [INFO] Step[2500/2713]: training loss : 1.0011549377441407 TRAIN  loss dict:  {'classification_loss': 1.0011549377441407}
2025-01-12 19:18:15,971 [INFO] Step[2550/2713]: training loss : 1.016385440826416 TRAIN  loss dict:  {'classification_loss': 1.016385440826416}
2025-01-12 19:18:27,870 [INFO] Step[2600/2713]: training loss : 1.0081400334835053 TRAIN  loss dict:  {'classification_loss': 1.0081400334835053}
2025-01-12 19:18:39,793 [INFO] Step[2650/2713]: training loss : 1.016255601644516 TRAIN  loss dict:  {'classification_loss': 1.016255601644516}
2025-01-12 19:18:51,625 [INFO] Step[2700/2713]: training loss : 1.0412402188777923 TRAIN  loss dict:  {'classification_loss': 1.0412402188777923}
2025-01-12 19:20:19,072 [INFO] Label accuracies statistics:
2025-01-12 19:20:19,072 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 0.5, 24: 1.0, 25: 0.5, 26: 0.75, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 0.75, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.5, 143: 0.75, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.5, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.25, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 1.0, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 0.75, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.5, 312: 0.75, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.5, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 0.75, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.25, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-12 19:20:19,074 [INFO] [21] TRAIN  loss: 1.0317289668213956 acc: 0.9922594913380022
2025-01-12 19:20:19,074 [INFO] [21] TRAIN  loss dict: {'classification_loss': 1.0317289668213956}
2025-01-12 19:20:19,074 [INFO] [21] VALIDATION loss: 1.729568123705405 VALIDATION acc: 0.8087774294670846
2025-01-12 19:20:19,074 [INFO] [21] VALIDATION loss dict: {'classification_loss': 1.729568123705405}
2025-01-12 19:20:19,074 [INFO] 
2025-01-12 19:20:37,583 [INFO] Step[50/2713]: training loss : 1.0095488917827606 TRAIN  loss dict:  {'classification_loss': 1.0095488917827606}
2025-01-12 19:20:49,458 [INFO] Step[100/2713]: training loss : 1.0374557745456696 TRAIN  loss dict:  {'classification_loss': 1.0374557745456696}
2025-01-12 19:21:01,378 [INFO] Step[150/2713]: training loss : 1.041606651544571 TRAIN  loss dict:  {'classification_loss': 1.041606651544571}
2025-01-12 19:21:13,300 [INFO] Step[200/2713]: training loss : 1.0176768171787263 TRAIN  loss dict:  {'classification_loss': 1.0176768171787263}
2025-01-12 19:21:25,247 [INFO] Step[250/2713]: training loss : 1.025255262851715 TRAIN  loss dict:  {'classification_loss': 1.025255262851715}
2025-01-12 19:21:37,148 [INFO] Step[300/2713]: training loss : 0.9921109640598297 TRAIN  loss dict:  {'classification_loss': 0.9921109640598297}
2025-01-12 19:21:49,068 [INFO] Step[350/2713]: training loss : 1.0024228525161742 TRAIN  loss dict:  {'classification_loss': 1.0024228525161742}
2025-01-12 19:22:01,006 [INFO] Step[400/2713]: training loss : 1.010061388015747 TRAIN  loss dict:  {'classification_loss': 1.010061388015747}
2025-01-12 19:22:12,910 [INFO] Step[450/2713]: training loss : 1.0277687525749206 TRAIN  loss dict:  {'classification_loss': 1.0277687525749206}
2025-01-12 19:22:24,788 [INFO] Step[500/2713]: training loss : 1.0026975274085999 TRAIN  loss dict:  {'classification_loss': 1.0026975274085999}
2025-01-12 19:22:36,734 [INFO] Step[550/2713]: training loss : 1.0205968344211578 TRAIN  loss dict:  {'classification_loss': 1.0205968344211578}
2025-01-12 19:22:48,627 [INFO] Step[600/2713]: training loss : 1.028906147480011 TRAIN  loss dict:  {'classification_loss': 1.028906147480011}
2025-01-12 19:23:00,550 [INFO] Step[650/2713]: training loss : 1.0260544192790986 TRAIN  loss dict:  {'classification_loss': 1.0260544192790986}
2025-01-12 19:23:12,461 [INFO] Step[700/2713]: training loss : 1.0169026529788971 TRAIN  loss dict:  {'classification_loss': 1.0169026529788971}
2025-01-12 19:23:24,353 [INFO] Step[750/2713]: training loss : 1.0040308964252471 TRAIN  loss dict:  {'classification_loss': 1.0040308964252471}
2025-01-12 19:23:36,250 [INFO] Step[800/2713]: training loss : 1.037125608921051 TRAIN  loss dict:  {'classification_loss': 1.037125608921051}
2025-01-12 19:23:48,127 [INFO] Step[850/2713]: training loss : 1.003376202583313 TRAIN  loss dict:  {'classification_loss': 1.003376202583313}
2025-01-12 19:24:00,046 [INFO] Step[900/2713]: training loss : 1.0206295657157898 TRAIN  loss dict:  {'classification_loss': 1.0206295657157898}
2025-01-12 19:24:11,967 [INFO] Step[950/2713]: training loss : 1.0113391363620758 TRAIN  loss dict:  {'classification_loss': 1.0113391363620758}
2025-01-12 19:24:23,896 [INFO] Step[1000/2713]: training loss : 1.007286161184311 TRAIN  loss dict:  {'classification_loss': 1.007286161184311}
2025-01-12 19:24:35,845 [INFO] Step[1050/2713]: training loss : 1.0340507054328918 TRAIN  loss dict:  {'classification_loss': 1.0340507054328918}
2025-01-12 19:24:47,740 [INFO] Step[1100/2713]: training loss : 1.0185670149326325 TRAIN  loss dict:  {'classification_loss': 1.0185670149326325}
2025-01-12 19:24:59,667 [INFO] Step[1150/2713]: training loss : 1.0250529384613036 TRAIN  loss dict:  {'classification_loss': 1.0250529384613036}
2025-01-12 19:25:11,555 [INFO] Step[1200/2713]: training loss : 1.0318021857738495 TRAIN  loss dict:  {'classification_loss': 1.0318021857738495}
2025-01-12 19:25:23,407 [INFO] Step[1250/2713]: training loss : 1.012934420108795 TRAIN  loss dict:  {'classification_loss': 1.012934420108795}
2025-01-12 19:25:35,324 [INFO] Step[1300/2713]: training loss : 1.0059928846359254 TRAIN  loss dict:  {'classification_loss': 1.0059928846359254}
2025-01-12 19:25:47,220 [INFO] Step[1350/2713]: training loss : 1.0397266256809234 TRAIN  loss dict:  {'classification_loss': 1.0397266256809234}
2025-01-12 19:25:59,094 [INFO] Step[1400/2713]: training loss : 1.0229980230331421 TRAIN  loss dict:  {'classification_loss': 1.0229980230331421}
2025-01-12 19:26:10,930 [INFO] Step[1450/2713]: training loss : 1.0116331481933594 TRAIN  loss dict:  {'classification_loss': 1.0116331481933594}
2025-01-12 19:26:22,850 [INFO] Step[1500/2713]: training loss : 1.0185469222068786 TRAIN  loss dict:  {'classification_loss': 1.0185469222068786}
2025-01-12 19:26:34,734 [INFO] Step[1550/2713]: training loss : 1.0052917969226838 TRAIN  loss dict:  {'classification_loss': 1.0052917969226838}
2025-01-12 19:26:46,621 [INFO] Step[1600/2713]: training loss : 1.019769047498703 TRAIN  loss dict:  {'classification_loss': 1.019769047498703}
2025-01-12 19:26:58,512 [INFO] Step[1650/2713]: training loss : 1.0209412729740144 TRAIN  loss dict:  {'classification_loss': 1.0209412729740144}
2025-01-12 19:27:10,391 [INFO] Step[1700/2713]: training loss : 1.040827157497406 TRAIN  loss dict:  {'classification_loss': 1.040827157497406}
2025-01-12 19:27:22,305 [INFO] Step[1750/2713]: training loss : 1.0057188558578491 TRAIN  loss dict:  {'classification_loss': 1.0057188558578491}
2025-01-12 19:27:34,185 [INFO] Step[1800/2713]: training loss : 1.0456753730773927 TRAIN  loss dict:  {'classification_loss': 1.0456753730773927}
2025-01-12 19:27:46,145 [INFO] Step[1850/2713]: training loss : 1.0015542268753053 TRAIN  loss dict:  {'classification_loss': 1.0015542268753053}
2025-01-12 19:27:58,053 [INFO] Step[1900/2713]: training loss : 1.0070862352848053 TRAIN  loss dict:  {'classification_loss': 1.0070862352848053}
2025-01-12 19:28:09,950 [INFO] Step[1950/2713]: training loss : 0.9988848149776459 TRAIN  loss dict:  {'classification_loss': 0.9988848149776459}
2025-01-12 19:28:21,857 [INFO] Step[2000/2713]: training loss : 1.027698255777359 TRAIN  loss dict:  {'classification_loss': 1.027698255777359}
2025-01-12 19:28:33,701 [INFO] Step[2050/2713]: training loss : 1.0277569162845612 TRAIN  loss dict:  {'classification_loss': 1.0277569162845612}
2025-01-12 19:28:45,576 [INFO] Step[2100/2713]: training loss : 1.0310642647743224 TRAIN  loss dict:  {'classification_loss': 1.0310642647743224}
2025-01-12 19:28:57,498 [INFO] Step[2150/2713]: training loss : 0.9922877073287963 TRAIN  loss dict:  {'classification_loss': 0.9922877073287963}
2025-01-12 19:29:09,414 [INFO] Step[2200/2713]: training loss : 1.0144948434829713 TRAIN  loss dict:  {'classification_loss': 1.0144948434829713}
2025-01-12 19:29:21,338 [INFO] Step[2250/2713]: training loss : 1.0125308692455293 TRAIN  loss dict:  {'classification_loss': 1.0125308692455293}
2025-01-12 19:29:33,251 [INFO] Step[2300/2713]: training loss : 1.0097855246067047 TRAIN  loss dict:  {'classification_loss': 1.0097855246067047}
2025-01-12 19:29:45,123 [INFO] Step[2350/2713]: training loss : 1.0363470768928529 TRAIN  loss dict:  {'classification_loss': 1.0363470768928529}
2025-01-12 19:29:57,020 [INFO] Step[2400/2713]: training loss : 1.0290907943248748 TRAIN  loss dict:  {'classification_loss': 1.0290907943248748}
2025-01-12 19:30:08,931 [INFO] Step[2450/2713]: training loss : 1.038088263273239 TRAIN  loss dict:  {'classification_loss': 1.038088263273239}
2025-01-12 19:30:20,821 [INFO] Step[2500/2713]: training loss : 1.025869039297104 TRAIN  loss dict:  {'classification_loss': 1.025869039297104}
2025-01-12 19:30:32,704 [INFO] Step[2550/2713]: training loss : 1.0409148132801056 TRAIN  loss dict:  {'classification_loss': 1.0409148132801056}
2025-01-12 19:30:44,591 [INFO] Step[2600/2713]: training loss : 1.0181136775016784 TRAIN  loss dict:  {'classification_loss': 1.0181136775016784}
2025-01-12 19:30:56,546 [INFO] Step[2650/2713]: training loss : 1.0371279990673066 TRAIN  loss dict:  {'classification_loss': 1.0371279990673066}
2025-01-12 19:31:08,439 [INFO] Step[2700/2713]: training loss : 1.0357904243469238 TRAIN  loss dict:  {'classification_loss': 1.0357904243469238}
2025-01-12 19:32:35,763 [INFO] Label accuracies statistics:
2025-01-12 19:32:35,763 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 1.0, 122: 0.5, 123: 0.5, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.25, 143: 0.75, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.5, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 0.75, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.5, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.5, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 1.0, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 1.0, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.25, 291: 0.5, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 0.75, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.5, 379: 1.0, 380: 1.0, 381: 0.25, 382: 0.75, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 19:32:35,765 [INFO] [22] TRAIN  loss: 1.0199395754528011 acc: 0.9945939304582873
2025-01-12 19:32:35,765 [INFO] [22] TRAIN  loss dict: {'classification_loss': 1.0199395754528011}
2025-01-12 19:32:35,765 [INFO] [22] VALIDATION loss: 1.742684274575764 VALIDATION acc: 0.8043887147335423
2025-01-12 19:32:35,765 [INFO] [22] VALIDATION loss dict: {'classification_loss': 1.742684274575764}
2025-01-12 19:32:35,766 [INFO] 
2025-01-12 19:32:53,825 [INFO] Step[50/2713]: training loss : 1.0213292324543 TRAIN  loss dict:  {'classification_loss': 1.0213292324543}
2025-01-12 19:33:06,054 [INFO] Step[100/2713]: training loss : 1.0220102953910828 TRAIN  loss dict:  {'classification_loss': 1.0220102953910828}
2025-01-12 19:33:18,540 [INFO] Step[150/2713]: training loss : 1.0061971497535707 TRAIN  loss dict:  {'classification_loss': 1.0061971497535707}
2025-01-12 19:33:31,027 [INFO] Step[200/2713]: training loss : 1.0060323226451873 TRAIN  loss dict:  {'classification_loss': 1.0060323226451873}
2025-01-12 19:33:43,577 [INFO] Step[250/2713]: training loss : 0.9975391864776612 TRAIN  loss dict:  {'classification_loss': 0.9975391864776612}
2025-01-12 19:33:56,122 [INFO] Step[300/2713]: training loss : 1.0194405126571655 TRAIN  loss dict:  {'classification_loss': 1.0194405126571655}
2025-01-12 19:34:08,618 [INFO] Step[350/2713]: training loss : 1.0312528383731843 TRAIN  loss dict:  {'classification_loss': 1.0312528383731843}
2025-01-12 19:34:21,626 [INFO] Step[400/2713]: training loss : 1.0024971497058868 TRAIN  loss dict:  {'classification_loss': 1.0024971497058868}
2025-01-12 19:34:34,998 [INFO] Step[450/2713]: training loss : 1.007390615940094 TRAIN  loss dict:  {'classification_loss': 1.007390615940094}
2025-01-12 19:34:48,288 [INFO] Step[500/2713]: training loss : 1.019289757013321 TRAIN  loss dict:  {'classification_loss': 1.019289757013321}
2025-01-12 19:35:00,284 [INFO] Step[550/2713]: training loss : 1.0213016879558563 TRAIN  loss dict:  {'classification_loss': 1.0213016879558563}
2025-01-12 19:35:12,150 [INFO] Step[600/2713]: training loss : 1.0131351852416992 TRAIN  loss dict:  {'classification_loss': 1.0131351852416992}
2025-01-12 19:35:24,074 [INFO] Step[650/2713]: training loss : 1.000974065065384 TRAIN  loss dict:  {'classification_loss': 1.000974065065384}
2025-01-12 19:35:35,961 [INFO] Step[700/2713]: training loss : 0.9992262530326843 TRAIN  loss dict:  {'classification_loss': 0.9992262530326843}
2025-01-12 19:35:47,893 [INFO] Step[750/2713]: training loss : 1.021949462890625 TRAIN  loss dict:  {'classification_loss': 1.021949462890625}
2025-01-12 19:35:59,808 [INFO] Step[800/2713]: training loss : 1.034286745786667 TRAIN  loss dict:  {'classification_loss': 1.034286745786667}
2025-01-12 19:36:11,749 [INFO] Step[850/2713]: training loss : 1.0071498405933381 TRAIN  loss dict:  {'classification_loss': 1.0071498405933381}
2025-01-12 19:36:23,687 [INFO] Step[900/2713]: training loss : 1.019060332775116 TRAIN  loss dict:  {'classification_loss': 1.019060332775116}
2025-01-12 19:36:35,634 [INFO] Step[950/2713]: training loss : 1.0275361621379853 TRAIN  loss dict:  {'classification_loss': 1.0275361621379853}
2025-01-12 19:36:47,563 [INFO] Step[1000/2713]: training loss : 1.001130495071411 TRAIN  loss dict:  {'classification_loss': 1.001130495071411}
2025-01-12 19:36:59,498 [INFO] Step[1050/2713]: training loss : 1.0171590864658355 TRAIN  loss dict:  {'classification_loss': 1.0171590864658355}
2025-01-12 19:37:11,429 [INFO] Step[1100/2713]: training loss : 1.0355356776714324 TRAIN  loss dict:  {'classification_loss': 1.0355356776714324}
2025-01-12 19:37:23,369 [INFO] Step[1150/2713]: training loss : 0.9986352407932282 TRAIN  loss dict:  {'classification_loss': 0.9986352407932282}
2025-01-12 19:37:35,330 [INFO] Step[1200/2713]: training loss : 1.0060974764823913 TRAIN  loss dict:  {'classification_loss': 1.0060974764823913}
2025-01-12 19:37:47,269 [INFO] Step[1250/2713]: training loss : 1.0121814870834351 TRAIN  loss dict:  {'classification_loss': 1.0121814870834351}
2025-01-12 19:37:59,209 [INFO] Step[1300/2713]: training loss : 1.0030672323703766 TRAIN  loss dict:  {'classification_loss': 1.0030672323703766}
2025-01-12 19:38:11,147 [INFO] Step[1350/2713]: training loss : 1.0005941200256347 TRAIN  loss dict:  {'classification_loss': 1.0005941200256347}
2025-01-12 19:38:23,035 [INFO] Step[1400/2713]: training loss : 1.0008568000793456 TRAIN  loss dict:  {'classification_loss': 1.0008568000793456}
2025-01-12 19:38:34,995 [INFO] Step[1450/2713]: training loss : 1.0053710508346558 TRAIN  loss dict:  {'classification_loss': 1.0053710508346558}
2025-01-12 19:38:46,928 [INFO] Step[1500/2713]: training loss : 1.0210956370830535 TRAIN  loss dict:  {'classification_loss': 1.0210956370830535}
2025-01-12 19:38:58,907 [INFO] Step[1550/2713]: training loss : 1.01529772400856 TRAIN  loss dict:  {'classification_loss': 1.01529772400856}
2025-01-12 19:39:10,883 [INFO] Step[1600/2713]: training loss : 1.0055854904651642 TRAIN  loss dict:  {'classification_loss': 1.0055854904651642}
2025-01-12 19:39:22,852 [INFO] Step[1650/2713]: training loss : 1.0167718982696534 TRAIN  loss dict:  {'classification_loss': 1.0167718982696534}
2025-01-12 19:39:34,774 [INFO] Step[1700/2713]: training loss : 1.0193138718605042 TRAIN  loss dict:  {'classification_loss': 1.0193138718605042}
2025-01-12 19:39:46,686 [INFO] Step[1750/2713]: training loss : 1.0098940217494965 TRAIN  loss dict:  {'classification_loss': 1.0098940217494965}
2025-01-12 19:39:58,617 [INFO] Step[1800/2713]: training loss : 1.0080062472820281 TRAIN  loss dict:  {'classification_loss': 1.0080062472820281}
2025-01-12 19:40:10,534 [INFO] Step[1850/2713]: training loss : 1.0238328242301942 TRAIN  loss dict:  {'classification_loss': 1.0238328242301942}
2025-01-12 19:40:22,450 [INFO] Step[1900/2713]: training loss : 1.0078495168685913 TRAIN  loss dict:  {'classification_loss': 1.0078495168685913}
2025-01-12 19:40:34,388 [INFO] Step[1950/2713]: training loss : 1.0224766945838928 TRAIN  loss dict:  {'classification_loss': 1.0224766945838928}
2025-01-12 19:40:46,312 [INFO] Step[2000/2713]: training loss : 1.0310852658748626 TRAIN  loss dict:  {'classification_loss': 1.0310852658748626}
2025-01-12 19:40:58,268 [INFO] Step[2050/2713]: training loss : 1.027066206932068 TRAIN  loss dict:  {'classification_loss': 1.027066206932068}
2025-01-12 19:41:10,211 [INFO] Step[2100/2713]: training loss : 1.015882877111435 TRAIN  loss dict:  {'classification_loss': 1.015882877111435}
2025-01-12 19:41:22,113 [INFO] Step[2150/2713]: training loss : 1.0332571697235107 TRAIN  loss dict:  {'classification_loss': 1.0332571697235107}
2025-01-12 19:41:34,019 [INFO] Step[2200/2713]: training loss : 1.011293511390686 TRAIN  loss dict:  {'classification_loss': 1.011293511390686}
2025-01-12 19:41:45,924 [INFO] Step[2250/2713]: training loss : 1.0116051506996155 TRAIN  loss dict:  {'classification_loss': 1.0116051506996155}
2025-01-12 19:41:57,849 [INFO] Step[2300/2713]: training loss : 1.0175021612644195 TRAIN  loss dict:  {'classification_loss': 1.0175021612644195}
2025-01-12 19:42:09,774 [INFO] Step[2350/2713]: training loss : 1.0346202290058135 TRAIN  loss dict:  {'classification_loss': 1.0346202290058135}
2025-01-12 19:42:21,680 [INFO] Step[2400/2713]: training loss : 0.9952619874477386 TRAIN  loss dict:  {'classification_loss': 0.9952619874477386}
2025-01-12 19:42:33,592 [INFO] Step[2450/2713]: training loss : 1.0082311832904816 TRAIN  loss dict:  {'classification_loss': 1.0082311832904816}
2025-01-12 19:42:45,488 [INFO] Step[2500/2713]: training loss : 1.0252067387104034 TRAIN  loss dict:  {'classification_loss': 1.0252067387104034}
2025-01-12 19:42:57,428 [INFO] Step[2550/2713]: training loss : 1.0122091925144197 TRAIN  loss dict:  {'classification_loss': 1.0122091925144197}
2025-01-12 19:43:09,359 [INFO] Step[2600/2713]: training loss : 1.0059622430801392 TRAIN  loss dict:  {'classification_loss': 1.0059622430801392}
2025-01-12 19:43:21,293 [INFO] Step[2650/2713]: training loss : 1.0058131456375121 TRAIN  loss dict:  {'classification_loss': 1.0058131456375121}
2025-01-12 19:43:33,248 [INFO] Step[2700/2713]: training loss : 1.0255997252464295 TRAIN  loss dict:  {'classification_loss': 1.0255997252464295}
2025-01-12 19:45:00,222 [INFO] Label accuracies statistics:
2025-01-12 19:45:00,222 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.25, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.0, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.5, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.5, 61: 0.5, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.25, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 1.0, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 0.75, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.0, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 1.0, 296: 0.5, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 0.5, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 19:45:00,224 [INFO] [23] TRAIN  loss: 1.0140612253701604 acc: 0.994348200024573
2025-01-12 19:45:00,224 [INFO] [23] TRAIN  loss dict: {'classification_loss': 1.0140612253701604}
2025-01-12 19:45:00,224 [INFO] [23] VALIDATION loss: 1.7772378838600074 VALIDATION acc: 0.799373040752351
2025-01-12 19:45:00,224 [INFO] [23] VALIDATION loss dict: {'classification_loss': 1.7772378838600074}
2025-01-12 19:45:00,224 [INFO] 
2025-01-12 19:45:17,696 [INFO] Step[50/2713]: training loss : 0.9935845363140107 TRAIN  loss dict:  {'classification_loss': 0.9935845363140107}
2025-01-12 19:45:29,566 [INFO] Step[100/2713]: training loss : 1.0103710901737213 TRAIN  loss dict:  {'classification_loss': 1.0103710901737213}
2025-01-12 19:45:41,444 [INFO] Step[150/2713]: training loss : 1.016164687871933 TRAIN  loss dict:  {'classification_loss': 1.016164687871933}
2025-01-12 19:45:53,340 [INFO] Step[200/2713]: training loss : 1.0067328429222107 TRAIN  loss dict:  {'classification_loss': 1.0067328429222107}
2025-01-12 19:46:05,268 [INFO] Step[250/2713]: training loss : 1.012281414270401 TRAIN  loss dict:  {'classification_loss': 1.012281414270401}
2025-01-12 19:46:17,243 [INFO] Step[300/2713]: training loss : 1.0042299437522888 TRAIN  loss dict:  {'classification_loss': 1.0042299437522888}
2025-01-12 19:46:29,144 [INFO] Step[350/2713]: training loss : 1.0036426830291747 TRAIN  loss dict:  {'classification_loss': 1.0036426830291747}
2025-01-12 19:46:41,086 [INFO] Step[400/2713]: training loss : 1.0177284836769105 TRAIN  loss dict:  {'classification_loss': 1.0177284836769105}
2025-01-12 19:46:53,016 [INFO] Step[450/2713]: training loss : 1.0074724566936493 TRAIN  loss dict:  {'classification_loss': 1.0074724566936493}
2025-01-12 19:47:04,897 [INFO] Step[500/2713]: training loss : 0.9991808116436005 TRAIN  loss dict:  {'classification_loss': 0.9991808116436005}
2025-01-12 19:47:16,774 [INFO] Step[550/2713]: training loss : 1.0281626403331756 TRAIN  loss dict:  {'classification_loss': 1.0281626403331756}
2025-01-12 19:47:28,657 [INFO] Step[600/2713]: training loss : 1.0056428980827332 TRAIN  loss dict:  {'classification_loss': 1.0056428980827332}
2025-01-12 19:47:40,567 [INFO] Step[650/2713]: training loss : 1.0248835635185243 TRAIN  loss dict:  {'classification_loss': 1.0248835635185243}
2025-01-12 19:47:52,498 [INFO] Step[700/2713]: training loss : 0.9940200281143189 TRAIN  loss dict:  {'classification_loss': 0.9940200281143189}
2025-01-12 19:48:04,378 [INFO] Step[750/2713]: training loss : 0.9888785016536713 TRAIN  loss dict:  {'classification_loss': 0.9888785016536713}
2025-01-12 19:48:16,311 [INFO] Step[800/2713]: training loss : 1.0023151814937592 TRAIN  loss dict:  {'classification_loss': 1.0023151814937592}
2025-01-12 19:48:28,226 [INFO] Step[850/2713]: training loss : 0.9983245766162873 TRAIN  loss dict:  {'classification_loss': 0.9983245766162873}
2025-01-12 19:48:40,095 [INFO] Step[900/2713]: training loss : 1.0068264555931092 TRAIN  loss dict:  {'classification_loss': 1.0068264555931092}
2025-01-12 19:48:52,029 [INFO] Step[950/2713]: training loss : 1.0043514037132264 TRAIN  loss dict:  {'classification_loss': 1.0043514037132264}
2025-01-12 19:49:03,936 [INFO] Step[1000/2713]: training loss : 1.0157857728004456 TRAIN  loss dict:  {'classification_loss': 1.0157857728004456}
2025-01-12 19:49:15,856 [INFO] Step[1050/2713]: training loss : 1.0123095321655273 TRAIN  loss dict:  {'classification_loss': 1.0123095321655273}
2025-01-12 19:49:27,779 [INFO] Step[1100/2713]: training loss : 1.0095573389530181 TRAIN  loss dict:  {'classification_loss': 1.0095573389530181}
2025-01-12 19:49:39,676 [INFO] Step[1150/2713]: training loss : 1.0418785393238068 TRAIN  loss dict:  {'classification_loss': 1.0418785393238068}
2025-01-12 19:49:51,561 [INFO] Step[1200/2713]: training loss : 1.0033159863948822 TRAIN  loss dict:  {'classification_loss': 1.0033159863948822}
2025-01-12 19:50:03,475 [INFO] Step[1250/2713]: training loss : 1.0107721984386444 TRAIN  loss dict:  {'classification_loss': 1.0107721984386444}
2025-01-12 19:50:15,384 [INFO] Step[1300/2713]: training loss : 1.0169890451431274 TRAIN  loss dict:  {'classification_loss': 1.0169890451431274}
2025-01-12 19:50:27,329 [INFO] Step[1350/2713]: training loss : 1.0203752970695497 TRAIN  loss dict:  {'classification_loss': 1.0203752970695497}
2025-01-12 19:50:39,237 [INFO] Step[1400/2713]: training loss : 0.9937710487842559 TRAIN  loss dict:  {'classification_loss': 0.9937710487842559}
2025-01-12 19:50:51,128 [INFO] Step[1450/2713]: training loss : 1.0149008822441101 TRAIN  loss dict:  {'classification_loss': 1.0149008822441101}
2025-01-12 19:51:03,058 [INFO] Step[1500/2713]: training loss : 1.0003090608119964 TRAIN  loss dict:  {'classification_loss': 1.0003090608119964}
2025-01-12 19:51:14,948 [INFO] Step[1550/2713]: training loss : 0.9982202243804932 TRAIN  loss dict:  {'classification_loss': 0.9982202243804932}
2025-01-12 19:51:26,819 [INFO] Step[1600/2713]: training loss : 1.0138670194149018 TRAIN  loss dict:  {'classification_loss': 1.0138670194149018}
2025-01-12 19:51:38,756 [INFO] Step[1650/2713]: training loss : 0.9846896612644196 TRAIN  loss dict:  {'classification_loss': 0.9846896612644196}
2025-01-12 19:51:50,766 [INFO] Step[1700/2713]: training loss : 1.008141508102417 TRAIN  loss dict:  {'classification_loss': 1.008141508102417}
2025-01-12 19:52:03,282 [INFO] Step[1750/2713]: training loss : 1.030478106737137 TRAIN  loss dict:  {'classification_loss': 1.030478106737137}
2025-01-12 19:52:15,650 [INFO] Step[1800/2713]: training loss : 1.0046773612499238 TRAIN  loss dict:  {'classification_loss': 1.0046773612499238}
2025-01-12 19:52:27,929 [INFO] Step[1850/2713]: training loss : 1.0256470036506653 TRAIN  loss dict:  {'classification_loss': 1.0256470036506653}
2025-01-12 19:52:40,827 [INFO] Step[1900/2713]: training loss : 1.0203598129749298 TRAIN  loss dict:  {'classification_loss': 1.0203598129749298}
2025-01-12 19:52:53,140 [INFO] Step[1950/2713]: training loss : 1.0243725335597993 TRAIN  loss dict:  {'classification_loss': 1.0243725335597993}
2025-01-12 19:53:05,834 [INFO] Step[2000/2713]: training loss : 1.0148881185054779 TRAIN  loss dict:  {'classification_loss': 1.0148881185054779}
2025-01-12 19:53:19,575 [INFO] Step[2050/2713]: training loss : 1.02532839179039 TRAIN  loss dict:  {'classification_loss': 1.02532839179039}
2025-01-12 19:53:33,431 [INFO] Step[2100/2713]: training loss : 0.9937629806995392 TRAIN  loss dict:  {'classification_loss': 0.9937629806995392}
2025-01-12 19:53:45,664 [INFO] Step[2150/2713]: training loss : 1.0228127896785737 TRAIN  loss dict:  {'classification_loss': 1.0228127896785737}
2025-01-12 19:53:57,577 [INFO] Step[2200/2713]: training loss : 1.0726645302772522 TRAIN  loss dict:  {'classification_loss': 1.0726645302772522}
2025-01-12 19:54:09,458 [INFO] Step[2250/2713]: training loss : 0.9908823978900909 TRAIN  loss dict:  {'classification_loss': 0.9908823978900909}
2025-01-12 19:54:21,333 [INFO] Step[2300/2713]: training loss : 1.0155615484714509 TRAIN  loss dict:  {'classification_loss': 1.0155615484714509}
2025-01-12 19:54:33,213 [INFO] Step[2350/2713]: training loss : 0.9942230999469757 TRAIN  loss dict:  {'classification_loss': 0.9942230999469757}
2025-01-12 19:54:45,084 [INFO] Step[2400/2713]: training loss : 1.0068148458003998 TRAIN  loss dict:  {'classification_loss': 1.0068148458003998}
2025-01-12 19:54:56,982 [INFO] Step[2450/2713]: training loss : 1.0217001807689667 TRAIN  loss dict:  {'classification_loss': 1.0217001807689667}
2025-01-12 19:55:08,814 [INFO] Step[2500/2713]: training loss : 1.0419373655319213 TRAIN  loss dict:  {'classification_loss': 1.0419373655319213}
2025-01-12 19:55:20,730 [INFO] Step[2550/2713]: training loss : 1.006329244375229 TRAIN  loss dict:  {'classification_loss': 1.006329244375229}
2025-01-12 19:55:32,598 [INFO] Step[2600/2713]: training loss : 1.0007197761535644 TRAIN  loss dict:  {'classification_loss': 1.0007197761535644}
2025-01-12 19:55:44,522 [INFO] Step[2650/2713]: training loss : 1.0032709562778472 TRAIN  loss dict:  {'classification_loss': 1.0032709562778472}
2025-01-12 19:55:56,395 [INFO] Step[2700/2713]: training loss : 1.0300140905380248 TRAIN  loss dict:  {'classification_loss': 1.0300140905380248}
2025-01-12 19:57:22,699 [INFO] Label accuracies statistics:
2025-01-12 19:57:22,699 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.5, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.5, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 1.0, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 0.75, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 0.75, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.5, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.0, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.5, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.0, 305: 1.0, 306: 0.75, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 1.0, 356: 0.5, 357: 0.75, 358: 0.75, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 1.0, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 19:57:22,701 [INFO] [24] TRAIN  loss: 1.0114239669795235 acc: 0.9954539869762871
2025-01-12 19:57:22,701 [INFO] [24] TRAIN  loss dict: {'classification_loss': 1.0114239669795235}
2025-01-12 19:57:22,701 [INFO] [24] VALIDATION loss: 1.7573117949908836 VALIDATION acc: 0.8025078369905956
2025-01-12 19:57:22,701 [INFO] [24] VALIDATION loss dict: {'classification_loss': 1.7573117949908836}
2025-01-12 19:57:22,701 [INFO] 
2025-01-12 19:57:40,308 [INFO] Step[50/2713]: training loss : 1.0064730548858642 TRAIN  loss dict:  {'classification_loss': 1.0064730548858642}
2025-01-12 19:57:52,173 [INFO] Step[100/2713]: training loss : 1.0170473611354829 TRAIN  loss dict:  {'classification_loss': 1.0170473611354829}
2025-01-12 19:58:04,056 [INFO] Step[150/2713]: training loss : 1.0031874418258666 TRAIN  loss dict:  {'classification_loss': 1.0031874418258666}
2025-01-12 19:58:15,913 [INFO] Step[200/2713]: training loss : 1.0127784538269042 TRAIN  loss dict:  {'classification_loss': 1.0127784538269042}
2025-01-12 19:58:27,813 [INFO] Step[250/2713]: training loss : 1.0088279926776886 TRAIN  loss dict:  {'classification_loss': 1.0088279926776886}
2025-01-12 19:58:39,677 [INFO] Step[300/2713]: training loss : 1.0181017994880677 TRAIN  loss dict:  {'classification_loss': 1.0181017994880677}
2025-01-12 19:58:51,602 [INFO] Step[350/2713]: training loss : 1.0101921367645263 TRAIN  loss dict:  {'classification_loss': 1.0101921367645263}
2025-01-12 19:59:03,539 [INFO] Step[400/2713]: training loss : 1.0070563673973083 TRAIN  loss dict:  {'classification_loss': 1.0070563673973083}
2025-01-12 19:59:15,469 [INFO] Step[450/2713]: training loss : 1.021209477186203 TRAIN  loss dict:  {'classification_loss': 1.021209477186203}
2025-01-12 19:59:27,391 [INFO] Step[500/2713]: training loss : 1.0073015427589416 TRAIN  loss dict:  {'classification_loss': 1.0073015427589416}
2025-01-12 19:59:39,333 [INFO] Step[550/2713]: training loss : 0.9898341298103333 TRAIN  loss dict:  {'classification_loss': 0.9898341298103333}
2025-01-12 19:59:51,220 [INFO] Step[600/2713]: training loss : 1.0047950041294098 TRAIN  loss dict:  {'classification_loss': 1.0047950041294098}
2025-01-12 20:00:03,152 [INFO] Step[650/2713]: training loss : 1.0026228821277618 TRAIN  loss dict:  {'classification_loss': 1.0026228821277618}
2025-01-12 20:00:15,064 [INFO] Step[700/2713]: training loss : 0.9953666973114014 TRAIN  loss dict:  {'classification_loss': 0.9953666973114014}
2025-01-12 20:00:26,942 [INFO] Step[750/2713]: training loss : 1.0005370354652405 TRAIN  loss dict:  {'classification_loss': 1.0005370354652405}
2025-01-12 20:00:38,881 [INFO] Step[800/2713]: training loss : 1.0161588621139526 TRAIN  loss dict:  {'classification_loss': 1.0161588621139526}
2025-01-12 20:00:50,806 [INFO] Step[850/2713]: training loss : 1.025038731098175 TRAIN  loss dict:  {'classification_loss': 1.025038731098175}
2025-01-12 20:01:02,663 [INFO] Step[900/2713]: training loss : 0.9945207285881043 TRAIN  loss dict:  {'classification_loss': 0.9945207285881043}
2025-01-12 20:01:14,587 [INFO] Step[950/2713]: training loss : 1.0047676849365235 TRAIN  loss dict:  {'classification_loss': 1.0047676849365235}
2025-01-12 20:01:26,507 [INFO] Step[1000/2713]: training loss : 0.9958004558086395 TRAIN  loss dict:  {'classification_loss': 0.9958004558086395}
2025-01-12 20:01:38,419 [INFO] Step[1050/2713]: training loss : 1.0063806748390198 TRAIN  loss dict:  {'classification_loss': 1.0063806748390198}
2025-01-12 20:01:50,369 [INFO] Step[1100/2713]: training loss : 1.0078706681728362 TRAIN  loss dict:  {'classification_loss': 1.0078706681728362}
2025-01-12 20:02:02,252 [INFO] Step[1150/2713]: training loss : 0.9994642317295075 TRAIN  loss dict:  {'classification_loss': 0.9994642317295075}
2025-01-12 20:02:14,181 [INFO] Step[1200/2713]: training loss : 1.0149338221549988 TRAIN  loss dict:  {'classification_loss': 1.0149338221549988}
2025-01-12 20:02:26,083 [INFO] Step[1250/2713]: training loss : 1.0280924880504607 TRAIN  loss dict:  {'classification_loss': 1.0280924880504607}
2025-01-12 20:02:37,954 [INFO] Step[1300/2713]: training loss : 0.9981392002105713 TRAIN  loss dict:  {'classification_loss': 0.9981392002105713}
2025-01-12 20:02:49,882 [INFO] Step[1350/2713]: training loss : 1.0034477818012237 TRAIN  loss dict:  {'classification_loss': 1.0034477818012237}
2025-01-12 20:03:01,821 [INFO] Step[1400/2713]: training loss : 1.013376054763794 TRAIN  loss dict:  {'classification_loss': 1.013376054763794}
2025-01-12 20:03:13,689 [INFO] Step[1450/2713]: training loss : 0.9837238895893097 TRAIN  loss dict:  {'classification_loss': 0.9837238895893097}
2025-01-12 20:03:25,550 [INFO] Step[1500/2713]: training loss : 1.013628319501877 TRAIN  loss dict:  {'classification_loss': 1.013628319501877}
2025-01-12 20:03:37,438 [INFO] Step[1550/2713]: training loss : 1.012316998243332 TRAIN  loss dict:  {'classification_loss': 1.012316998243332}
2025-01-12 20:03:49,332 [INFO] Step[1600/2713]: training loss : 1.026646157503128 TRAIN  loss dict:  {'classification_loss': 1.026646157503128}
2025-01-12 20:04:01,294 [INFO] Step[1650/2713]: training loss : 1.013377377986908 TRAIN  loss dict:  {'classification_loss': 1.013377377986908}
2025-01-12 20:04:13,160 [INFO] Step[1700/2713]: training loss : 1.0499864757061004 TRAIN  loss dict:  {'classification_loss': 1.0499864757061004}
2025-01-12 20:04:25,048 [INFO] Step[1750/2713]: training loss : 1.0093965566158294 TRAIN  loss dict:  {'classification_loss': 1.0093965566158294}
2025-01-12 20:04:36,952 [INFO] Step[1800/2713]: training loss : 0.9930869281291962 TRAIN  loss dict:  {'classification_loss': 0.9930869281291962}
2025-01-12 20:04:48,838 [INFO] Step[1850/2713]: training loss : 0.9911006176471711 TRAIN  loss dict:  {'classification_loss': 0.9911006176471711}
2025-01-12 20:05:00,764 [INFO] Step[1900/2713]: training loss : 1.0266524827480317 TRAIN  loss dict:  {'classification_loss': 1.0266524827480317}
2025-01-12 20:05:12,668 [INFO] Step[1950/2713]: training loss : 0.9980821752548218 TRAIN  loss dict:  {'classification_loss': 0.9980821752548218}
2025-01-12 20:05:24,544 [INFO] Step[2000/2713]: training loss : 0.9977299189567566 TRAIN  loss dict:  {'classification_loss': 0.9977299189567566}
2025-01-12 20:05:36,487 [INFO] Step[2050/2713]: training loss : 0.9978827178478241 TRAIN  loss dict:  {'classification_loss': 0.9978827178478241}
2025-01-12 20:05:48,376 [INFO] Step[2100/2713]: training loss : 1.0220618677139282 TRAIN  loss dict:  {'classification_loss': 1.0220618677139282}
2025-01-12 20:06:00,309 [INFO] Step[2150/2713]: training loss : 1.0077483260631561 TRAIN  loss dict:  {'classification_loss': 1.0077483260631561}
2025-01-12 20:06:12,201 [INFO] Step[2200/2713]: training loss : 1.027295207977295 TRAIN  loss dict:  {'classification_loss': 1.027295207977295}
2025-01-12 20:06:24,147 [INFO] Step[2250/2713]: training loss : 1.0158860731124877 TRAIN  loss dict:  {'classification_loss': 1.0158860731124877}
2025-01-12 20:06:36,014 [INFO] Step[2300/2713]: training loss : 1.0001683962345123 TRAIN  loss dict:  {'classification_loss': 1.0001683962345123}
2025-01-12 20:06:47,925 [INFO] Step[2350/2713]: training loss : 1.001806401014328 TRAIN  loss dict:  {'classification_loss': 1.001806401014328}
2025-01-12 20:06:59,808 [INFO] Step[2400/2713]: training loss : 1.0200327301025391 TRAIN  loss dict:  {'classification_loss': 1.0200327301025391}
2025-01-12 20:07:11,664 [INFO] Step[2450/2713]: training loss : 0.9985710918903351 TRAIN  loss dict:  {'classification_loss': 0.9985710918903351}
2025-01-12 20:07:23,573 [INFO] Step[2500/2713]: training loss : 0.9945756196975708 TRAIN  loss dict:  {'classification_loss': 0.9945756196975708}
2025-01-12 20:07:35,474 [INFO] Step[2550/2713]: training loss : 1.035593206882477 TRAIN  loss dict:  {'classification_loss': 1.035593206882477}
2025-01-12 20:07:47,387 [INFO] Step[2600/2713]: training loss : 1.0151466500759125 TRAIN  loss dict:  {'classification_loss': 1.0151466500759125}
2025-01-12 20:07:59,316 [INFO] Step[2650/2713]: training loss : 1.0505896139144897 TRAIN  loss dict:  {'classification_loss': 1.0505896139144897}
2025-01-12 20:08:11,209 [INFO] Step[2700/2713]: training loss : 0.9945442533493042 TRAIN  loss dict:  {'classification_loss': 0.9945442533493042}
2025-01-12 20:09:38,250 [INFO] Label accuracies statistics:
2025-01-12 20:09:38,250 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 0.5, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 0.75, 174: 1.0, 175: 0.5, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.25, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.25, 202: 0.75, 203: 0.25, 204: 1.0, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.25, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.0, 238: 0.75, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 0.75, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.75, 278: 1.0, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.25, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 0.75, 399: 1.0}

2025-01-12 20:09:39,230 [INFO] [25] TRAIN  loss: 1.009412786396412 acc: 0.9958225826268583
2025-01-12 20:09:39,230 [INFO] [25] TRAIN  loss dict: {'classification_loss': 1.009412786396412}
2025-01-12 20:09:39,230 [INFO] [25] VALIDATION loss: 1.7070030782903944 VALIDATION acc: 0.8175548589341692
2025-01-12 20:09:39,230 [INFO] [25] VALIDATION loss dict: {'classification_loss': 1.7070030782903944}
2025-01-12 20:09:39,230 [INFO] 
2025-01-12 20:09:57,132 [INFO] Step[50/2713]: training loss : 1.0114077305793763 TRAIN  loss dict:  {'classification_loss': 1.0114077305793763}
2025-01-12 20:10:09,000 [INFO] Step[100/2713]: training loss : 0.9967018818855286 TRAIN  loss dict:  {'classification_loss': 0.9967018818855286}
2025-01-12 20:10:20,928 [INFO] Step[150/2713]: training loss : 1.0188616228103637 TRAIN  loss dict:  {'classification_loss': 1.0188616228103637}
2025-01-12 20:10:32,843 [INFO] Step[200/2713]: training loss : 0.9906286263465881 TRAIN  loss dict:  {'classification_loss': 0.9906286263465881}
2025-01-12 20:10:44,744 [INFO] Step[250/2713]: training loss : 0.9962646460533142 TRAIN  loss dict:  {'classification_loss': 0.9962646460533142}
2025-01-12 20:10:56,886 [INFO] Step[300/2713]: training loss : 1.0119353425502777 TRAIN  loss dict:  {'classification_loss': 1.0119353425502777}
2025-01-12 20:11:09,351 [INFO] Step[350/2713]: training loss : 1.0192799377441406 TRAIN  loss dict:  {'classification_loss': 1.0192799377441406}
2025-01-12 20:11:21,755 [INFO] Step[400/2713]: training loss : 0.9872989165782928 TRAIN  loss dict:  {'classification_loss': 0.9872989165782928}
2025-01-12 20:11:34,288 [INFO] Step[450/2713]: training loss : 1.003220477104187 TRAIN  loss dict:  {'classification_loss': 1.003220477104187}
2025-01-12 20:11:46,795 [INFO] Step[500/2713]: training loss : 1.0147157299518585 TRAIN  loss dict:  {'classification_loss': 1.0147157299518585}
2025-01-12 20:11:59,383 [INFO] Step[550/2713]: training loss : 0.9972503578662872 TRAIN  loss dict:  {'classification_loss': 0.9972503578662872}
2025-01-12 20:12:12,476 [INFO] Step[600/2713]: training loss : 0.9980602538585663 TRAIN  loss dict:  {'classification_loss': 0.9980602538585663}
2025-01-12 20:12:26,050 [INFO] Step[650/2713]: training loss : 1.0065260052680969 TRAIN  loss dict:  {'classification_loss': 1.0065260052680969}
2025-01-12 20:12:38,744 [INFO] Step[700/2713]: training loss : 1.0042413341999055 TRAIN  loss dict:  {'classification_loss': 1.0042413341999055}
2025-01-12 20:12:50,778 [INFO] Step[750/2713]: training loss : 1.012341766357422 TRAIN  loss dict:  {'classification_loss': 1.012341766357422}
2025-01-12 20:13:02,576 [INFO] Step[800/2713]: training loss : 1.024012974500656 TRAIN  loss dict:  {'classification_loss': 1.024012974500656}
2025-01-12 20:13:14,503 [INFO] Step[850/2713]: training loss : 1.019972983598709 TRAIN  loss dict:  {'classification_loss': 1.019972983598709}
2025-01-12 20:13:26,414 [INFO] Step[900/2713]: training loss : 1.0042228627204894 TRAIN  loss dict:  {'classification_loss': 1.0042228627204894}
2025-01-12 20:13:38,262 [INFO] Step[950/2713]: training loss : 1.025424497127533 TRAIN  loss dict:  {'classification_loss': 1.025424497127533}
2025-01-12 20:13:50,133 [INFO] Step[1000/2713]: training loss : 1.0164685785770415 TRAIN  loss dict:  {'classification_loss': 1.0164685785770415}
2025-01-12 20:14:02,005 [INFO] Step[1050/2713]: training loss : 1.009718518257141 TRAIN  loss dict:  {'classification_loss': 1.009718518257141}
2025-01-12 20:14:13,890 [INFO] Step[1100/2713]: training loss : 1.0032232916355133 TRAIN  loss dict:  {'classification_loss': 1.0032232916355133}
2025-01-12 20:14:25,856 [INFO] Step[1150/2713]: training loss : 0.9959319305419921 TRAIN  loss dict:  {'classification_loss': 0.9959319305419921}
2025-01-12 20:14:37,775 [INFO] Step[1200/2713]: training loss : 1.0045641875267028 TRAIN  loss dict:  {'classification_loss': 1.0045641875267028}
2025-01-12 20:14:49,694 [INFO] Step[1250/2713]: training loss : 1.0300747418403626 TRAIN  loss dict:  {'classification_loss': 1.0300747418403626}
2025-01-12 20:15:01,622 [INFO] Step[1300/2713]: training loss : 1.0047294092178345 TRAIN  loss dict:  {'classification_loss': 1.0047294092178345}
2025-01-12 20:15:13,499 [INFO] Step[1350/2713]: training loss : 0.9901379144191742 TRAIN  loss dict:  {'classification_loss': 0.9901379144191742}
2025-01-12 20:15:25,375 [INFO] Step[1400/2713]: training loss : 0.9902582097053528 TRAIN  loss dict:  {'classification_loss': 0.9902582097053528}
2025-01-12 20:15:37,274 [INFO] Step[1450/2713]: training loss : 0.9912876939773559 TRAIN  loss dict:  {'classification_loss': 0.9912876939773559}
2025-01-12 20:15:49,189 [INFO] Step[1500/2713]: training loss : 0.9871783542633057 TRAIN  loss dict:  {'classification_loss': 0.9871783542633057}
2025-01-12 20:16:01,101 [INFO] Step[1550/2713]: training loss : 0.9967849791049957 TRAIN  loss dict:  {'classification_loss': 0.9967849791049957}
2025-01-12 20:16:13,001 [INFO] Step[1600/2713]: training loss : 1.020937385559082 TRAIN  loss dict:  {'classification_loss': 1.020937385559082}
2025-01-12 20:16:24,893 [INFO] Step[1650/2713]: training loss : 1.015413920879364 TRAIN  loss dict:  {'classification_loss': 1.015413920879364}
2025-01-12 20:16:36,810 [INFO] Step[1700/2713]: training loss : 0.9993195307254791 TRAIN  loss dict:  {'classification_loss': 0.9993195307254791}
2025-01-12 20:16:48,715 [INFO] Step[1750/2713]: training loss : 0.9951181435585021 TRAIN  loss dict:  {'classification_loss': 0.9951181435585021}
2025-01-12 20:17:00,557 [INFO] Step[1800/2713]: training loss : 0.999259489774704 TRAIN  loss dict:  {'classification_loss': 0.999259489774704}
2025-01-12 20:17:12,488 [INFO] Step[1850/2713]: training loss : 1.004799050092697 TRAIN  loss dict:  {'classification_loss': 1.004799050092697}
2025-01-12 20:17:24,364 [INFO] Step[1900/2713]: training loss : 1.0609394752979278 TRAIN  loss dict:  {'classification_loss': 1.0609394752979278}
2025-01-12 20:17:36,289 [INFO] Step[1950/2713]: training loss : 0.9947341930866241 TRAIN  loss dict:  {'classification_loss': 0.9947341930866241}
2025-01-12 20:17:48,219 [INFO] Step[2000/2713]: training loss : 1.026119863986969 TRAIN  loss dict:  {'classification_loss': 1.026119863986969}
2025-01-12 20:18:00,123 [INFO] Step[2050/2713]: training loss : 1.0029390501976012 TRAIN  loss dict:  {'classification_loss': 1.0029390501976012}
2025-01-12 20:18:12,027 [INFO] Step[2100/2713]: training loss : 1.0122558510303497 TRAIN  loss dict:  {'classification_loss': 1.0122558510303497}
2025-01-12 20:18:23,928 [INFO] Step[2150/2713]: training loss : 1.0066773462295533 TRAIN  loss dict:  {'classification_loss': 1.0066773462295533}
2025-01-12 20:18:35,794 [INFO] Step[2200/2713]: training loss : 1.020586541891098 TRAIN  loss dict:  {'classification_loss': 1.020586541891098}
2025-01-12 20:18:47,740 [INFO] Step[2250/2713]: training loss : 1.0133020412921905 TRAIN  loss dict:  {'classification_loss': 1.0133020412921905}
2025-01-12 20:18:59,605 [INFO] Step[2300/2713]: training loss : 0.9951070737838745 TRAIN  loss dict:  {'classification_loss': 0.9951070737838745}
2025-01-12 20:19:11,544 [INFO] Step[2350/2713]: training loss : 1.0245693457126617 TRAIN  loss dict:  {'classification_loss': 1.0245693457126617}
2025-01-12 20:19:23,444 [INFO] Step[2400/2713]: training loss : 0.9963260984420776 TRAIN  loss dict:  {'classification_loss': 0.9963260984420776}
2025-01-12 20:19:35,367 [INFO] Step[2450/2713]: training loss : 0.9983804285526275 TRAIN  loss dict:  {'classification_loss': 0.9983804285526275}
2025-01-12 20:19:47,265 [INFO] Step[2500/2713]: training loss : 0.9876110112667084 TRAIN  loss dict:  {'classification_loss': 0.9876110112667084}
2025-01-12 20:19:59,186 [INFO] Step[2550/2713]: training loss : 0.9993394613265991 TRAIN  loss dict:  {'classification_loss': 0.9993394613265991}
2025-01-12 20:20:11,116 [INFO] Step[2600/2713]: training loss : 1.007158727645874 TRAIN  loss dict:  {'classification_loss': 1.007158727645874}
2025-01-12 20:20:23,007 [INFO] Step[2650/2713]: training loss : 1.0045494532585144 TRAIN  loss dict:  {'classification_loss': 1.0045494532585144}
2025-01-12 20:20:34,891 [INFO] Step[2700/2713]: training loss : 1.0134474885463716 TRAIN  loss dict:  {'classification_loss': 1.0134474885463716}
2025-01-12 20:22:03,079 [INFO] Label accuracies statistics:
2025-01-12 20:22:03,079 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.75, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.75, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.5, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 0.75, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.75, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 0.6666666666666666, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.75, 291: 0.5, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.5, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.75, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 1.0, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 20:22:03,994 [INFO] [26] TRAIN  loss: 1.0066438426050452 acc: 0.9955768521931441
2025-01-12 20:22:03,994 [INFO] [26] TRAIN  loss dict: {'classification_loss': 1.0066438426050452}
2025-01-12 20:22:03,994 [INFO] [26] VALIDATION loss: 1.7298527838368165 VALIDATION acc: 0.8219435736677116
2025-01-12 20:22:03,994 [INFO] [26] VALIDATION loss dict: {'classification_loss': 1.7298527838368165}
2025-01-12 20:22:03,994 [INFO] 
2025-01-12 20:22:21,834 [INFO] Step[50/2713]: training loss : 0.9947093546390533 TRAIN  loss dict:  {'classification_loss': 0.9947093546390533}
2025-01-12 20:22:33,736 [INFO] Step[100/2713]: training loss : 0.9910988771915435 TRAIN  loss dict:  {'classification_loss': 0.9910988771915435}
2025-01-12 20:22:45,620 [INFO] Step[150/2713]: training loss : 1.0033428633213044 TRAIN  loss dict:  {'classification_loss': 1.0033428633213044}
2025-01-12 20:22:57,523 [INFO] Step[200/2713]: training loss : 1.0256752359867096 TRAIN  loss dict:  {'classification_loss': 1.0256752359867096}
2025-01-12 20:23:09,404 [INFO] Step[250/2713]: training loss : 1.0068711936473846 TRAIN  loss dict:  {'classification_loss': 1.0068711936473846}
2025-01-12 20:23:21,350 [INFO] Step[300/2713]: training loss : 0.9966737949848175 TRAIN  loss dict:  {'classification_loss': 0.9966737949848175}
2025-01-12 20:23:33,231 [INFO] Step[350/2713]: training loss : 1.0312434911727906 TRAIN  loss dict:  {'classification_loss': 1.0312434911727906}
2025-01-12 20:23:45,095 [INFO] Step[400/2713]: training loss : 0.9989886116981507 TRAIN  loss dict:  {'classification_loss': 0.9989886116981507}
2025-01-12 20:23:56,981 [INFO] Step[450/2713]: training loss : 1.0002379393577576 TRAIN  loss dict:  {'classification_loss': 1.0002379393577576}
2025-01-12 20:24:08,855 [INFO] Step[500/2713]: training loss : 1.0088273453712464 TRAIN  loss dict:  {'classification_loss': 1.0088273453712464}
2025-01-12 20:24:20,784 [INFO] Step[550/2713]: training loss : 1.0013949573040009 TRAIN  loss dict:  {'classification_loss': 1.0013949573040009}
2025-01-12 20:24:32,701 [INFO] Step[600/2713]: training loss : 0.9937083864212036 TRAIN  loss dict:  {'classification_loss': 0.9937083864212036}
2025-01-12 20:24:44,605 [INFO] Step[650/2713]: training loss : 1.0051769495010376 TRAIN  loss dict:  {'classification_loss': 1.0051769495010376}
2025-01-12 20:24:56,525 [INFO] Step[700/2713]: training loss : 0.99495525598526 TRAIN  loss dict:  {'classification_loss': 0.99495525598526}
2025-01-12 20:25:08,444 [INFO] Step[750/2713]: training loss : 1.0232397568225862 TRAIN  loss dict:  {'classification_loss': 1.0232397568225862}
2025-01-12 20:25:20,314 [INFO] Step[800/2713]: training loss : 1.0196919524669648 TRAIN  loss dict:  {'classification_loss': 1.0196919524669648}
2025-01-12 20:25:32,251 [INFO] Step[850/2713]: training loss : 0.9973187661170959 TRAIN  loss dict:  {'classification_loss': 0.9973187661170959}
2025-01-12 20:25:44,106 [INFO] Step[900/2713]: training loss : 1.0227426028251647 TRAIN  loss dict:  {'classification_loss': 1.0227426028251647}
2025-01-12 20:25:56,033 [INFO] Step[950/2713]: training loss : 1.0123870003223419 TRAIN  loss dict:  {'classification_loss': 1.0123870003223419}
2025-01-12 20:26:07,945 [INFO] Step[1000/2713]: training loss : 1.0099330699443818 TRAIN  loss dict:  {'classification_loss': 1.0099330699443818}
2025-01-12 20:26:19,840 [INFO] Step[1050/2713]: training loss : 1.0370858681201935 TRAIN  loss dict:  {'classification_loss': 1.0370858681201935}
2025-01-12 20:26:31,755 [INFO] Step[1100/2713]: training loss : 1.0216704082489014 TRAIN  loss dict:  {'classification_loss': 1.0216704082489014}
2025-01-12 20:26:43,655 [INFO] Step[1150/2713]: training loss : 1.0004044902324676 TRAIN  loss dict:  {'classification_loss': 1.0004044902324676}
2025-01-12 20:26:55,575 [INFO] Step[1200/2713]: training loss : 1.0179117381572724 TRAIN  loss dict:  {'classification_loss': 1.0179117381572724}
2025-01-12 20:27:07,512 [INFO] Step[1250/2713]: training loss : 1.0334044396877289 TRAIN  loss dict:  {'classification_loss': 1.0334044396877289}
2025-01-12 20:27:19,392 [INFO] Step[1300/2713]: training loss : 0.9933099210262298 TRAIN  loss dict:  {'classification_loss': 0.9933099210262298}
2025-01-12 20:27:31,339 [INFO] Step[1350/2713]: training loss : 0.995087914466858 TRAIN  loss dict:  {'classification_loss': 0.995087914466858}
2025-01-12 20:27:43,231 [INFO] Step[1400/2713]: training loss : 1.0362484061717987 TRAIN  loss dict:  {'classification_loss': 1.0362484061717987}
2025-01-12 20:27:55,156 [INFO] Step[1450/2713]: training loss : 1.0187402057647705 TRAIN  loss dict:  {'classification_loss': 1.0187402057647705}
2025-01-12 20:28:07,027 [INFO] Step[1500/2713]: training loss : 0.9966058325767517 TRAIN  loss dict:  {'classification_loss': 0.9966058325767517}
2025-01-12 20:28:18,966 [INFO] Step[1550/2713]: training loss : 0.9970933139324188 TRAIN  loss dict:  {'classification_loss': 0.9970933139324188}
2025-01-12 20:28:30,857 [INFO] Step[1600/2713]: training loss : 0.9916229450702667 TRAIN  loss dict:  {'classification_loss': 0.9916229450702667}
2025-01-12 20:28:42,735 [INFO] Step[1650/2713]: training loss : 1.0184329009056092 TRAIN  loss dict:  {'classification_loss': 1.0184329009056092}
2025-01-12 20:28:54,669 [INFO] Step[1700/2713]: training loss : 1.0140984964370727 TRAIN  loss dict:  {'classification_loss': 1.0140984964370727}
2025-01-12 20:29:06,614 [INFO] Step[1750/2713]: training loss : 0.9959402406215667 TRAIN  loss dict:  {'classification_loss': 0.9959402406215667}
2025-01-12 20:29:18,501 [INFO] Step[1800/2713]: training loss : 1.0151805925369262 TRAIN  loss dict:  {'classification_loss': 1.0151805925369262}
2025-01-12 20:29:30,436 [INFO] Step[1850/2713]: training loss : 1.0056667006015778 TRAIN  loss dict:  {'classification_loss': 1.0056667006015778}
2025-01-12 20:29:42,419 [INFO] Step[1900/2713]: training loss : 1.0073208868503571 TRAIN  loss dict:  {'classification_loss': 1.0073208868503571}
2025-01-12 20:29:54,936 [INFO] Step[1950/2713]: training loss : 1.0165113449096679 TRAIN  loss dict:  {'classification_loss': 1.0165113449096679}
2025-01-12 20:30:07,384 [INFO] Step[2000/2713]: training loss : 0.994371325969696 TRAIN  loss dict:  {'classification_loss': 0.994371325969696}
2025-01-12 20:30:19,655 [INFO] Step[2050/2713]: training loss : 1.0076794993877412 TRAIN  loss dict:  {'classification_loss': 1.0076794993877412}
2025-01-12 20:30:32,387 [INFO] Step[2100/2713]: training loss : 1.0028324687480927 TRAIN  loss dict:  {'classification_loss': 1.0028324687480927}
2025-01-12 20:30:44,741 [INFO] Step[2150/2713]: training loss : 0.9866624689102172 TRAIN  loss dict:  {'classification_loss': 0.9866624689102172}
2025-01-12 20:30:57,341 [INFO] Step[2200/2713]: training loss : 0.9951630187034607 TRAIN  loss dict:  {'classification_loss': 0.9951630187034607}
2025-01-12 20:31:10,642 [INFO] Step[2250/2713]: training loss : 1.0114517867565156 TRAIN  loss dict:  {'classification_loss': 1.0114517867565156}
2025-01-12 20:31:24,459 [INFO] Step[2300/2713]: training loss : 0.984552229642868 TRAIN  loss dict:  {'classification_loss': 0.984552229642868}
2025-01-12 20:31:36,930 [INFO] Step[2350/2713]: training loss : 1.005776946544647 TRAIN  loss dict:  {'classification_loss': 1.005776946544647}
2025-01-12 20:31:48,835 [INFO] Step[2400/2713]: training loss : 1.0254294538497926 TRAIN  loss dict:  {'classification_loss': 1.0254294538497926}
2025-01-12 20:32:00,672 [INFO] Step[2450/2713]: training loss : 1.00984521985054 TRAIN  loss dict:  {'classification_loss': 1.00984521985054}
2025-01-12 20:32:12,539 [INFO] Step[2500/2713]: training loss : 0.990145456790924 TRAIN  loss dict:  {'classification_loss': 0.990145456790924}
2025-01-12 20:32:24,425 [INFO] Step[2550/2713]: training loss : 1.006124621629715 TRAIN  loss dict:  {'classification_loss': 1.006124621629715}
2025-01-12 20:32:36,291 [INFO] Step[2600/2713]: training loss : 1.0235115134716033 TRAIN  loss dict:  {'classification_loss': 1.0235115134716033}
2025-01-12 20:32:48,171 [INFO] Step[2650/2713]: training loss : 0.9925431454181671 TRAIN  loss dict:  {'classification_loss': 0.9925431454181671}
2025-01-12 20:33:00,047 [INFO] Step[2700/2713]: training loss : 0.997565951347351 TRAIN  loss dict:  {'classification_loss': 0.997565951347351}
2025-01-12 20:34:57,919 [INFO] Label accuracies statistics:
2025-01-12 20:34:57,920 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 1.0, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 0.75, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.25, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.25, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.0, 397: 1.0, 398: 0.75, 399: 0.75}

2025-01-12 20:34:57,921 [INFO] [27] TRAIN  loss: 1.007084301200864 acc: 0.99533112175943
2025-01-12 20:34:57,921 [INFO] [27] TRAIN  loss dict: {'classification_loss': 1.007084301200864}
2025-01-12 20:34:57,922 [INFO] [27] VALIDATION loss: 1.7267660532230722 VALIDATION acc: 0.812539184952978
2025-01-12 20:34:57,922 [INFO] [27] VALIDATION loss dict: {'classification_loss': 1.7267660532230722}
2025-01-12 20:34:57,922 [INFO] 
2025-01-12 20:35:25,157 [INFO] Step[50/2713]: training loss : 0.9995777225494384 TRAIN  loss dict:  {'classification_loss': 0.9995777225494384}
2025-01-12 20:35:37,021 [INFO] Step[100/2713]: training loss : 0.9864503073692322 TRAIN  loss dict:  {'classification_loss': 0.9864503073692322}
2025-01-12 20:35:48,906 [INFO] Step[150/2713]: training loss : 1.0252804887294769 TRAIN  loss dict:  {'classification_loss': 1.0252804887294769}
2025-01-12 20:36:00,797 [INFO] Step[200/2713]: training loss : 0.9938021552562714 TRAIN  loss dict:  {'classification_loss': 0.9938021552562714}
2025-01-12 20:36:12,720 [INFO] Step[250/2713]: training loss : 1.0281573235988617 TRAIN  loss dict:  {'classification_loss': 1.0281573235988617}
2025-01-12 20:36:24,673 [INFO] Step[300/2713]: training loss : 1.0012120699882507 TRAIN  loss dict:  {'classification_loss': 1.0012120699882507}
2025-01-12 20:36:36,636 [INFO] Step[350/2713]: training loss : 1.0003030109405517 TRAIN  loss dict:  {'classification_loss': 1.0003030109405517}
2025-01-12 20:36:48,607 [INFO] Step[400/2713]: training loss : 1.0142614030838013 TRAIN  loss dict:  {'classification_loss': 1.0142614030838013}
2025-01-12 20:37:00,489 [INFO] Step[450/2713]: training loss : 0.9860946309566497 TRAIN  loss dict:  {'classification_loss': 0.9860946309566497}
2025-01-12 20:37:12,401 [INFO] Step[500/2713]: training loss : 0.9987886846065521 TRAIN  loss dict:  {'classification_loss': 0.9987886846065521}
2025-01-12 20:37:24,303 [INFO] Step[550/2713]: training loss : 0.9946827507019043 TRAIN  loss dict:  {'classification_loss': 0.9946827507019043}
2025-01-12 20:37:36,226 [INFO] Step[600/2713]: training loss : 1.0068472921848297 TRAIN  loss dict:  {'classification_loss': 1.0068472921848297}
2025-01-12 20:37:48,152 [INFO] Step[650/2713]: training loss : 0.9862015891075134 TRAIN  loss dict:  {'classification_loss': 0.9862015891075134}
2025-01-12 20:38:00,105 [INFO] Step[700/2713]: training loss : 1.0204697835445404 TRAIN  loss dict:  {'classification_loss': 1.0204697835445404}
2025-01-12 20:38:12,025 [INFO] Step[750/2713]: training loss : 0.9930731678009033 TRAIN  loss dict:  {'classification_loss': 0.9930731678009033}
2025-01-12 20:38:23,969 [INFO] Step[800/2713]: training loss : 0.9934085547924042 TRAIN  loss dict:  {'classification_loss': 0.9934085547924042}
2025-01-12 20:38:35,901 [INFO] Step[850/2713]: training loss : 1.0054820084571838 TRAIN  loss dict:  {'classification_loss': 1.0054820084571838}
2025-01-12 20:38:47,786 [INFO] Step[900/2713]: training loss : 0.9922984552383423 TRAIN  loss dict:  {'classification_loss': 0.9922984552383423}
2025-01-12 20:38:59,711 [INFO] Step[950/2713]: training loss : 1.0095852136611938 TRAIN  loss dict:  {'classification_loss': 1.0095852136611938}
2025-01-12 20:39:11,645 [INFO] Step[1000/2713]: training loss : 0.999530543088913 TRAIN  loss dict:  {'classification_loss': 0.999530543088913}
2025-01-12 20:39:23,572 [INFO] Step[1050/2713]: training loss : 1.0074968945980072 TRAIN  loss dict:  {'classification_loss': 1.0074968945980072}
2025-01-12 20:39:35,484 [INFO] Step[1100/2713]: training loss : 0.9963761603832245 TRAIN  loss dict:  {'classification_loss': 0.9963761603832245}
2025-01-12 20:39:47,393 [INFO] Step[1150/2713]: training loss : 1.0133136892318726 TRAIN  loss dict:  {'classification_loss': 1.0133136892318726}
2025-01-12 20:39:59,268 [INFO] Step[1200/2713]: training loss : 1.0042235279083251 TRAIN  loss dict:  {'classification_loss': 1.0042235279083251}
2025-01-12 20:40:11,189 [INFO] Step[1250/2713]: training loss : 0.9939139807224273 TRAIN  loss dict:  {'classification_loss': 0.9939139807224273}
2025-01-12 20:40:23,087 [INFO] Step[1300/2713]: training loss : 1.0198016214370726 TRAIN  loss dict:  {'classification_loss': 1.0198016214370726}
2025-01-12 20:40:34,993 [INFO] Step[1350/2713]: training loss : 0.9993060266971588 TRAIN  loss dict:  {'classification_loss': 0.9993060266971588}
2025-01-12 20:40:46,892 [INFO] Step[1400/2713]: training loss : 1.0197418057918548 TRAIN  loss dict:  {'classification_loss': 1.0197418057918548}
2025-01-12 20:40:58,815 [INFO] Step[1450/2713]: training loss : 0.9981546413898468 TRAIN  loss dict:  {'classification_loss': 0.9981546413898468}
2025-01-12 20:41:10,690 [INFO] Step[1500/2713]: training loss : 1.0037252187728882 TRAIN  loss dict:  {'classification_loss': 1.0037252187728882}
2025-01-12 20:41:22,632 [INFO] Step[1550/2713]: training loss : 1.0146123588085174 TRAIN  loss dict:  {'classification_loss': 1.0146123588085174}
2025-01-12 20:41:34,546 [INFO] Step[1600/2713]: training loss : 1.0011555314064027 TRAIN  loss dict:  {'classification_loss': 1.0011555314064027}
2025-01-12 20:41:46,444 [INFO] Step[1650/2713]: training loss : 1.0213606309890748 TRAIN  loss dict:  {'classification_loss': 1.0213606309890748}
2025-01-12 20:41:58,385 [INFO] Step[1700/2713]: training loss : 0.9956931555271149 TRAIN  loss dict:  {'classification_loss': 0.9956931555271149}
2025-01-12 20:42:10,272 [INFO] Step[1750/2713]: training loss : 1.0092525243759156 TRAIN  loss dict:  {'classification_loss': 1.0092525243759156}
2025-01-12 20:42:22,104 [INFO] Step[1800/2713]: training loss : 1.0169265258312226 TRAIN  loss dict:  {'classification_loss': 1.0169265258312226}
2025-01-12 20:42:34,052 [INFO] Step[1850/2713]: training loss : 1.0018193328380585 TRAIN  loss dict:  {'classification_loss': 1.0018193328380585}
2025-01-12 20:42:45,940 [INFO] Step[1900/2713]: training loss : 0.9887461149692536 TRAIN  loss dict:  {'classification_loss': 0.9887461149692536}
2025-01-12 20:42:57,858 [INFO] Step[1950/2713]: training loss : 0.9988747692108154 TRAIN  loss dict:  {'classification_loss': 0.9988747692108154}
2025-01-12 20:43:09,736 [INFO] Step[2000/2713]: training loss : 0.9963765048980713 TRAIN  loss dict:  {'classification_loss': 0.9963765048980713}
2025-01-12 20:43:21,684 [INFO] Step[2050/2713]: training loss : 1.0116319227218629 TRAIN  loss dict:  {'classification_loss': 1.0116319227218629}
2025-01-12 20:43:33,588 [INFO] Step[2100/2713]: training loss : 0.9969831025600433 TRAIN  loss dict:  {'classification_loss': 0.9969831025600433}
2025-01-12 20:43:45,483 [INFO] Step[2150/2713]: training loss : 1.0099201381206513 TRAIN  loss dict:  {'classification_loss': 1.0099201381206513}
2025-01-12 20:43:57,395 [INFO] Step[2200/2713]: training loss : 1.009417201280594 TRAIN  loss dict:  {'classification_loss': 1.009417201280594}
2025-01-12 20:44:09,299 [INFO] Step[2250/2713]: training loss : 1.0139991402626038 TRAIN  loss dict:  {'classification_loss': 1.0139991402626038}
2025-01-12 20:44:21,185 [INFO] Step[2300/2713]: training loss : 1.007604831457138 TRAIN  loss dict:  {'classification_loss': 1.007604831457138}
2025-01-12 20:44:33,068 [INFO] Step[2350/2713]: training loss : 1.0025349628925324 TRAIN  loss dict:  {'classification_loss': 1.0025349628925324}
2025-01-12 20:44:44,968 [INFO] Step[2400/2713]: training loss : 1.00088028550148 TRAIN  loss dict:  {'classification_loss': 1.00088028550148}
2025-01-12 20:44:56,888 [INFO] Step[2450/2713]: training loss : 1.0251761329174043 TRAIN  loss dict:  {'classification_loss': 1.0251761329174043}
2025-01-12 20:45:08,765 [INFO] Step[2500/2713]: training loss : 0.9961765003204346 TRAIN  loss dict:  {'classification_loss': 0.9961765003204346}
2025-01-12 20:45:20,645 [INFO] Step[2550/2713]: training loss : 1.003472980260849 TRAIN  loss dict:  {'classification_loss': 1.003472980260849}
2025-01-12 20:45:32,578 [INFO] Step[2600/2713]: training loss : 1.014139598608017 TRAIN  loss dict:  {'classification_loss': 1.014139598608017}
2025-01-12 20:45:44,516 [INFO] Step[2650/2713]: training loss : 1.0222674489021302 TRAIN  loss dict:  {'classification_loss': 1.0222674489021302}
2025-01-12 20:45:56,398 [INFO] Step[2700/2713]: training loss : 1.0201052522659302 TRAIN  loss dict:  {'classification_loss': 1.0201052522659302}
2025-01-12 20:47:24,632 [INFO] Label accuracies statistics:
2025-01-12 20:47:24,632 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 1.0, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.75, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.5, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 0.5, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 0.5, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.0, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 0.75, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.25, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-12 20:47:24,634 [INFO] [28] TRAIN  loss: 1.0049113192067918 acc: 0.9958225826268583
2025-01-12 20:47:24,634 [INFO] [28] TRAIN  loss dict: {'classification_loss': 1.0049113192067918}
2025-01-12 20:47:24,634 [INFO] [28] VALIDATION loss: 1.7500102411311371 VALIDATION acc: 0.8018808777429467
2025-01-12 20:47:24,634 [INFO] [28] VALIDATION loss dict: {'classification_loss': 1.7500102411311371}
2025-01-12 20:47:24,634 [INFO] 
2025-01-12 20:47:42,181 [INFO] Step[50/2713]: training loss : 0.9982013142108918 TRAIN  loss dict:  {'classification_loss': 0.9982013142108918}
2025-01-12 20:47:54,063 [INFO] Step[100/2713]: training loss : 0.9989552974700928 TRAIN  loss dict:  {'classification_loss': 0.9989552974700928}
2025-01-12 20:48:05,959 [INFO] Step[150/2713]: training loss : 0.9936542499065399 TRAIN  loss dict:  {'classification_loss': 0.9936542499065399}
2025-01-12 20:48:17,864 [INFO] Step[200/2713]: training loss : 0.9901631999015809 TRAIN  loss dict:  {'classification_loss': 0.9901631999015809}
2025-01-12 20:48:29,771 [INFO] Step[250/2713]: training loss : 1.0366931748390198 TRAIN  loss dict:  {'classification_loss': 1.0366931748390198}
2025-01-12 20:48:41,748 [INFO] Step[300/2713]: training loss : 0.9857228863239288 TRAIN  loss dict:  {'classification_loss': 0.9857228863239288}
2025-01-12 20:48:54,144 [INFO] Step[350/2713]: training loss : 0.9932895267009735 TRAIN  loss dict:  {'classification_loss': 0.9932895267009735}
2025-01-12 20:49:06,667 [INFO] Step[400/2713]: training loss : 1.0065154027938843 TRAIN  loss dict:  {'classification_loss': 1.0065154027938843}
2025-01-12 20:49:18,945 [INFO] Step[450/2713]: training loss : 1.0027830350399016 TRAIN  loss dict:  {'classification_loss': 1.0027830350399016}
2025-01-12 20:49:31,671 [INFO] Step[500/2713]: training loss : 0.9891820228099824 TRAIN  loss dict:  {'classification_loss': 0.9891820228099824}
2025-01-12 20:49:44,008 [INFO] Step[550/2713]: training loss : 1.0101527738571168 TRAIN  loss dict:  {'classification_loss': 1.0101527738571168}
2025-01-12 20:49:56,740 [INFO] Step[600/2713]: training loss : 0.9968179273605347 TRAIN  loss dict:  {'classification_loss': 0.9968179273605347}
2025-01-12 20:50:10,249 [INFO] Step[650/2713]: training loss : 1.0211343312263488 TRAIN  loss dict:  {'classification_loss': 1.0211343312263488}
2025-01-12 20:50:24,190 [INFO] Step[700/2713]: training loss : 0.992209757566452 TRAIN  loss dict:  {'classification_loss': 0.992209757566452}
2025-01-12 20:50:36,624 [INFO] Step[750/2713]: training loss : 1.0031634974479675 TRAIN  loss dict:  {'classification_loss': 1.0031634974479675}
2025-01-12 20:50:48,467 [INFO] Step[800/2713]: training loss : 0.985212013721466 TRAIN  loss dict:  {'classification_loss': 0.985212013721466}
2025-01-12 20:51:00,347 [INFO] Step[850/2713]: training loss : 1.0036926913261413 TRAIN  loss dict:  {'classification_loss': 1.0036926913261413}
2025-01-12 20:51:12,237 [INFO] Step[900/2713]: training loss : 0.9883012998104096 TRAIN  loss dict:  {'classification_loss': 0.9883012998104096}
2025-01-12 20:51:24,094 [INFO] Step[950/2713]: training loss : 0.9915582883358002 TRAIN  loss dict:  {'classification_loss': 0.9915582883358002}
2025-01-12 20:51:35,975 [INFO] Step[1000/2713]: training loss : 0.9974544072151184 TRAIN  loss dict:  {'classification_loss': 0.9974544072151184}
2025-01-12 20:51:47,892 [INFO] Step[1050/2713]: training loss : 0.9891292917728424 TRAIN  loss dict:  {'classification_loss': 0.9891292917728424}
2025-01-12 20:51:59,779 [INFO] Step[1100/2713]: training loss : 0.9971591699123382 TRAIN  loss dict:  {'classification_loss': 0.9971591699123382}
2025-01-12 20:52:11,715 [INFO] Step[1150/2713]: training loss : 1.010800929069519 TRAIN  loss dict:  {'classification_loss': 1.010800929069519}
2025-01-12 20:52:23,613 [INFO] Step[1200/2713]: training loss : 1.0003833699226379 TRAIN  loss dict:  {'classification_loss': 1.0003833699226379}
2025-01-12 20:52:35,527 [INFO] Step[1250/2713]: training loss : 0.9973765015602112 TRAIN  loss dict:  {'classification_loss': 0.9973765015602112}
2025-01-12 20:52:47,424 [INFO] Step[1300/2713]: training loss : 1.004922512769699 TRAIN  loss dict:  {'classification_loss': 1.004922512769699}
2025-01-12 20:52:59,334 [INFO] Step[1350/2713]: training loss : 0.9969767463207245 TRAIN  loss dict:  {'classification_loss': 0.9969767463207245}
2025-01-12 20:53:11,387 [INFO] Step[1400/2713]: training loss : 0.9927995359897613 TRAIN  loss dict:  {'classification_loss': 0.9927995359897613}
2025-01-12 20:53:23,446 [INFO] Step[1450/2713]: training loss : 1.0023814070224761 TRAIN  loss dict:  {'classification_loss': 1.0023814070224761}
2025-01-12 20:53:35,347 [INFO] Step[1500/2713]: training loss : 0.9966014587879181 TRAIN  loss dict:  {'classification_loss': 0.9966014587879181}
2025-01-12 20:53:47,148 [INFO] Step[1550/2713]: training loss : 0.9849583315849304 TRAIN  loss dict:  {'classification_loss': 0.9849583315849304}
2025-01-12 20:53:58,964 [INFO] Step[1600/2713]: training loss : 0.9983470940589905 TRAIN  loss dict:  {'classification_loss': 0.9983470940589905}
2025-01-12 20:54:10,735 [INFO] Step[1650/2713]: training loss : 0.9875755763053894 TRAIN  loss dict:  {'classification_loss': 0.9875755763053894}
2025-01-12 20:54:22,724 [INFO] Step[1700/2713]: training loss : 1.0041540849208832 TRAIN  loss dict:  {'classification_loss': 1.0041540849208832}
2025-01-12 20:54:34,623 [INFO] Step[1750/2713]: training loss : 1.0117240035533905 TRAIN  loss dict:  {'classification_loss': 1.0117240035533905}
2025-01-12 20:54:46,444 [INFO] Step[1800/2713]: training loss : 1.0204070341587066 TRAIN  loss dict:  {'classification_loss': 1.0204070341587066}
2025-01-12 20:54:58,186 [INFO] Step[1850/2713]: training loss : 1.0154185092449188 TRAIN  loss dict:  {'classification_loss': 1.0154185092449188}
2025-01-12 20:55:09,961 [INFO] Step[1900/2713]: training loss : 0.9786426711082459 TRAIN  loss dict:  {'classification_loss': 0.9786426711082459}
2025-01-12 20:55:21,763 [INFO] Step[1950/2713]: training loss : 1.0042618119716644 TRAIN  loss dict:  {'classification_loss': 1.0042618119716644}
2025-01-12 20:55:33,563 [INFO] Step[2000/2713]: training loss : 1.026807358264923 TRAIN  loss dict:  {'classification_loss': 1.026807358264923}
2025-01-12 20:55:45,326 [INFO] Step[2050/2713]: training loss : 0.9923034536838532 TRAIN  loss dict:  {'classification_loss': 0.9923034536838532}
2025-01-12 20:55:57,110 [INFO] Step[2100/2713]: training loss : 1.0082290244102479 TRAIN  loss dict:  {'classification_loss': 1.0082290244102479}
2025-01-12 20:56:08,913 [INFO] Step[2150/2713]: training loss : 1.0002491939067841 TRAIN  loss dict:  {'classification_loss': 1.0002491939067841}
2025-01-12 20:56:20,663 [INFO] Step[2200/2713]: training loss : 0.9880722832679748 TRAIN  loss dict:  {'classification_loss': 0.9880722832679748}
2025-01-12 20:56:32,479 [INFO] Step[2250/2713]: training loss : 0.9927712750434875 TRAIN  loss dict:  {'classification_loss': 0.9927712750434875}
2025-01-12 20:56:44,306 [INFO] Step[2300/2713]: training loss : 0.9882276356220245 TRAIN  loss dict:  {'classification_loss': 0.9882276356220245}
2025-01-12 20:56:56,084 [INFO] Step[2350/2713]: training loss : 0.9951183903217315 TRAIN  loss dict:  {'classification_loss': 0.9951183903217315}
2025-01-12 20:57:07,893 [INFO] Step[2400/2713]: training loss : 1.0069597506523131 TRAIN  loss dict:  {'classification_loss': 1.0069597506523131}
2025-01-12 20:57:19,681 [INFO] Step[2450/2713]: training loss : 0.9994810259342194 TRAIN  loss dict:  {'classification_loss': 0.9994810259342194}
2025-01-12 20:57:31,441 [INFO] Step[2500/2713]: training loss : 1.028924927711487 TRAIN  loss dict:  {'classification_loss': 1.028924927711487}
2025-01-12 20:57:43,238 [INFO] Step[2550/2713]: training loss : 0.9996181011199952 TRAIN  loss dict:  {'classification_loss': 0.9996181011199952}
2025-01-12 20:57:54,981 [INFO] Step[2600/2713]: training loss : 1.0035278654098512 TRAIN  loss dict:  {'classification_loss': 1.0035278654098512}
2025-01-12 20:58:06,759 [INFO] Step[2650/2713]: training loss : 0.9870662152767181 TRAIN  loss dict:  {'classification_loss': 0.9870662152767181}
2025-01-12 20:58:18,558 [INFO] Step[2700/2713]: training loss : 1.0004464852809907 TRAIN  loss dict:  {'classification_loss': 1.0004464852809907}
2025-01-12 20:59:44,595 [INFO] Label accuracies statistics:
2025-01-12 20:59:44,595 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.5, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 1.0, 60: 0.75, 61: 0.75, 62: 1.0, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.75, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.25, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 0.75, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 1.0, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.75, 135: 0.75, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 1.0, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.5, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.5, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.5, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.25, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 0.5, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.5, 350: 0.5, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.25, 377: 0.75, 378: 0.5, 379: 0.75, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 20:59:44,597 [INFO] [29] TRAIN  loss: 1.0001869864934843 acc: 0.9964369087111439
2025-01-12 20:59:44,597 [INFO] [29] TRAIN  loss dict: {'classification_loss': 1.0001869864934843}
2025-01-12 20:59:44,597 [INFO] [29] VALIDATION loss: 1.7783933555273186 VALIDATION acc: 0.799373040752351
2025-01-12 20:59:44,597 [INFO] [29] VALIDATION loss dict: {'classification_loss': 1.7783933555273186}
2025-01-12 20:59:44,597 [INFO] 
2025-01-12 21:00:02,175 [INFO] Step[50/2713]: training loss : 1.0009748506546021 TRAIN  loss dict:  {'classification_loss': 1.0009748506546021}
2025-01-12 21:00:13,905 [INFO] Step[100/2713]: training loss : 0.9962247109413147 TRAIN  loss dict:  {'classification_loss': 0.9962247109413147}
2025-01-12 21:00:25,687 [INFO] Step[150/2713]: training loss : 1.0035096001625061 TRAIN  loss dict:  {'classification_loss': 1.0035096001625061}
2025-01-12 21:00:37,431 [INFO] Step[200/2713]: training loss : 1.0053461277484894 TRAIN  loss dict:  {'classification_loss': 1.0053461277484894}
2025-01-12 21:00:49,170 [INFO] Step[250/2713]: training loss : 0.9881987190246582 TRAIN  loss dict:  {'classification_loss': 0.9881987190246582}
2025-01-12 21:01:00,918 [INFO] Step[300/2713]: training loss : 0.9889835381507873 TRAIN  loss dict:  {'classification_loss': 0.9889835381507873}
2025-01-12 21:01:12,644 [INFO] Step[350/2713]: training loss : 0.9889301753044129 TRAIN  loss dict:  {'classification_loss': 0.9889301753044129}
2025-01-12 21:01:24,403 [INFO] Step[400/2713]: training loss : 0.9877588009834289 TRAIN  loss dict:  {'classification_loss': 0.9877588009834289}
2025-01-12 21:01:36,136 [INFO] Step[450/2713]: training loss : 0.9867302882671356 TRAIN  loss dict:  {'classification_loss': 0.9867302882671356}
2025-01-12 21:01:47,844 [INFO] Step[500/2713]: training loss : 1.0088868236541748 TRAIN  loss dict:  {'classification_loss': 1.0088868236541748}
2025-01-12 21:01:59,588 [INFO] Step[550/2713]: training loss : 1.0048046219348907 TRAIN  loss dict:  {'classification_loss': 1.0048046219348907}
2025-01-12 21:02:11,350 [INFO] Step[600/2713]: training loss : 1.0096831834316253 TRAIN  loss dict:  {'classification_loss': 1.0096831834316253}
2025-01-12 21:02:23,102 [INFO] Step[650/2713]: training loss : 0.9873304212093353 TRAIN  loss dict:  {'classification_loss': 0.9873304212093353}
2025-01-12 21:02:34,862 [INFO] Step[700/2713]: training loss : 0.9908632373809815 TRAIN  loss dict:  {'classification_loss': 0.9908632373809815}
2025-01-12 21:02:46,588 [INFO] Step[750/2713]: training loss : 0.9833064043521881 TRAIN  loss dict:  {'classification_loss': 0.9833064043521881}
2025-01-12 21:02:58,321 [INFO] Step[800/2713]: training loss : 0.9869176745414734 TRAIN  loss dict:  {'classification_loss': 0.9869176745414734}
2025-01-12 21:03:10,014 [INFO] Step[850/2713]: training loss : 0.9914732253551484 TRAIN  loss dict:  {'classification_loss': 0.9914732253551484}
2025-01-12 21:03:21,793 [INFO] Step[900/2713]: training loss : 0.9953767251968384 TRAIN  loss dict:  {'classification_loss': 0.9953767251968384}
2025-01-12 21:03:33,514 [INFO] Step[950/2713]: training loss : 0.9970169591903687 TRAIN  loss dict:  {'classification_loss': 0.9970169591903687}
2025-01-12 21:03:45,246 [INFO] Step[1000/2713]: training loss : 0.9966767287254333 TRAIN  loss dict:  {'classification_loss': 0.9966767287254333}
2025-01-12 21:03:57,019 [INFO] Step[1050/2713]: training loss : 1.0041276895999909 TRAIN  loss dict:  {'classification_loss': 1.0041276895999909}
2025-01-12 21:04:08,766 [INFO] Step[1100/2713]: training loss : 0.9858715903759002 TRAIN  loss dict:  {'classification_loss': 0.9858715903759002}
2025-01-12 21:04:20,501 [INFO] Step[1150/2713]: training loss : 0.9862104034423829 TRAIN  loss dict:  {'classification_loss': 0.9862104034423829}
2025-01-12 21:04:32,191 [INFO] Step[1200/2713]: training loss : 1.007573766708374 TRAIN  loss dict:  {'classification_loss': 1.007573766708374}
2025-01-12 21:04:43,993 [INFO] Step[1250/2713]: training loss : 1.0051267743110657 TRAIN  loss dict:  {'classification_loss': 1.0051267743110657}
2025-01-12 21:04:55,687 [INFO] Step[1300/2713]: training loss : 0.9899293828010559 TRAIN  loss dict:  {'classification_loss': 0.9899293828010559}
2025-01-12 21:05:07,487 [INFO] Step[1350/2713]: training loss : 0.9981788420677185 TRAIN  loss dict:  {'classification_loss': 0.9981788420677185}
2025-01-12 21:05:19,205 [INFO] Step[1400/2713]: training loss : 0.995901540517807 TRAIN  loss dict:  {'classification_loss': 0.995901540517807}
2025-01-12 21:05:30,941 [INFO] Step[1450/2713]: training loss : 0.9832477128505707 TRAIN  loss dict:  {'classification_loss': 0.9832477128505707}
2025-01-12 21:05:42,642 [INFO] Step[1500/2713]: training loss : 0.991491996049881 TRAIN  loss dict:  {'classification_loss': 0.991491996049881}
2025-01-12 21:05:54,335 [INFO] Step[1550/2713]: training loss : 0.9882021200656891 TRAIN  loss dict:  {'classification_loss': 0.9882021200656891}
2025-01-12 21:06:06,089 [INFO] Step[1600/2713]: training loss : 1.0149715900421143 TRAIN  loss dict:  {'classification_loss': 1.0149715900421143}
2025-01-12 21:06:17,833 [INFO] Step[1650/2713]: training loss : 1.0006398236751557 TRAIN  loss dict:  {'classification_loss': 1.0006398236751557}
2025-01-12 21:06:29,521 [INFO] Step[1700/2713]: training loss : 0.9940805017948151 TRAIN  loss dict:  {'classification_loss': 0.9940805017948151}
2025-01-12 21:06:41,289 [INFO] Step[1750/2713]: training loss : 1.0058476150035858 TRAIN  loss dict:  {'classification_loss': 1.0058476150035858}
2025-01-12 21:06:53,008 [INFO] Step[1800/2713]: training loss : 1.00269069314003 TRAIN  loss dict:  {'classification_loss': 1.00269069314003}
2025-01-12 21:07:04,754 [INFO] Step[1850/2713]: training loss : 0.9906998896598815 TRAIN  loss dict:  {'classification_loss': 0.9906998896598815}
2025-01-12 21:07:16,456 [INFO] Step[1900/2713]: training loss : 0.978145911693573 TRAIN  loss dict:  {'classification_loss': 0.978145911693573}
2025-01-12 21:07:28,392 [INFO] Step[1950/2713]: training loss : 0.9975978302955627 TRAIN  loss dict:  {'classification_loss': 0.9975978302955627}
2025-01-12 21:07:40,486 [INFO] Step[2000/2713]: training loss : 1.0049324095249177 TRAIN  loss dict:  {'classification_loss': 1.0049324095249177}
2025-01-12 21:07:52,793 [INFO] Step[2050/2713]: training loss : 0.9951459240913391 TRAIN  loss dict:  {'classification_loss': 0.9951459240913391}
2025-01-12 21:08:04,879 [INFO] Step[2100/2713]: training loss : 1.0037183570861816 TRAIN  loss dict:  {'classification_loss': 1.0037183570861816}
2025-01-12 21:08:17,197 [INFO] Step[2150/2713]: training loss : 0.9844295001029968 TRAIN  loss dict:  {'classification_loss': 0.9844295001029968}
2025-01-12 21:08:29,484 [INFO] Step[2200/2713]: training loss : 0.9860496664047241 TRAIN  loss dict:  {'classification_loss': 0.9860496664047241}
2025-01-12 21:08:41,791 [INFO] Step[2250/2713]: training loss : 1.0098602342605592 TRAIN  loss dict:  {'classification_loss': 1.0098602342605592}
2025-01-12 21:08:54,699 [INFO] Step[2300/2713]: training loss : 0.9846377766132355 TRAIN  loss dict:  {'classification_loss': 0.9846377766132355}
2025-01-12 21:09:08,341 [INFO] Step[2350/2713]: training loss : 1.005090489387512 TRAIN  loss dict:  {'classification_loss': 1.005090489387512}
2025-01-12 21:09:21,199 [INFO] Step[2400/2713]: training loss : 1.002686346769333 TRAIN  loss dict:  {'classification_loss': 1.002686346769333}
2025-01-12 21:09:33,012 [INFO] Step[2450/2713]: training loss : 1.0266799199581147 TRAIN  loss dict:  {'classification_loss': 1.0266799199581147}
2025-01-12 21:09:44,742 [INFO] Step[2500/2713]: training loss : 0.9943302130699158 TRAIN  loss dict:  {'classification_loss': 0.9943302130699158}
2025-01-12 21:09:56,422 [INFO] Step[2550/2713]: training loss : 1.0140946066379548 TRAIN  loss dict:  {'classification_loss': 1.0140946066379548}
2025-01-12 21:10:08,142 [INFO] Step[2600/2713]: training loss : 0.9896747827529907 TRAIN  loss dict:  {'classification_loss': 0.9896747827529907}
2025-01-12 21:10:19,879 [INFO] Step[2650/2713]: training loss : 1.0265674889087677 TRAIN  loss dict:  {'classification_loss': 1.0265674889087677}
2025-01-12 21:10:31,624 [INFO] Step[2700/2713]: training loss : 0.9939064538478851 TRAIN  loss dict:  {'classification_loss': 0.9939064538478851}
2025-01-12 21:11:59,270 [INFO] Label accuracies statistics:
2025-01-12 21:11:59,270 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.5, 59: 0.75, 60: 0.75, 61: 0.5, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 0.75, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.75, 121: 1.0, 122: 0.75, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.75, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 0.75, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 21:11:59,272 [INFO] [30] TRAIN  loss: 0.9971543694175217 acc: 0.9959454478437154
2025-01-12 21:11:59,272 [INFO] [30] TRAIN  loss dict: {'classification_loss': 0.9971543694175217}
2025-01-12 21:11:59,272 [INFO] [30] VALIDATION loss: 1.748785696419558 VALIDATION acc: 0.8119122257053292
2025-01-12 21:11:59,272 [INFO] [30] VALIDATION loss dict: {'classification_loss': 1.748785696419558}
2025-01-12 21:11:59,272 [INFO] 
2025-01-12 21:12:16,936 [INFO] Step[50/2713]: training loss : 0.993661949634552 TRAIN  loss dict:  {'classification_loss': 0.993661949634552}
2025-01-12 21:12:28,883 [INFO] Step[100/2713]: training loss : 0.9904431438446045 TRAIN  loss dict:  {'classification_loss': 0.9904431438446045}
2025-01-12 21:12:40,815 [INFO] Step[150/2713]: training loss : 0.9924708819389343 TRAIN  loss dict:  {'classification_loss': 0.9924708819389343}
2025-01-12 21:12:52,670 [INFO] Step[200/2713]: training loss : 0.9969683635234833 TRAIN  loss dict:  {'classification_loss': 0.9969683635234833}
2025-01-12 21:13:04,583 [INFO] Step[250/2713]: training loss : 0.9888929772377014 TRAIN  loss dict:  {'classification_loss': 0.9888929772377014}
2025-01-12 21:13:16,491 [INFO] Step[300/2713]: training loss : 0.989388906955719 TRAIN  loss dict:  {'classification_loss': 0.989388906955719}
2025-01-12 21:13:28,463 [INFO] Step[350/2713]: training loss : 0.9959676027297973 TRAIN  loss dict:  {'classification_loss': 0.9959676027297973}
2025-01-12 21:13:40,368 [INFO] Step[400/2713]: training loss : 0.9946138536930085 TRAIN  loss dict:  {'classification_loss': 0.9946138536930085}
2025-01-12 21:13:52,272 [INFO] Step[450/2713]: training loss : 0.9861197531223297 TRAIN  loss dict:  {'classification_loss': 0.9861197531223297}
2025-01-12 21:14:04,196 [INFO] Step[500/2713]: training loss : 0.9890254771709442 TRAIN  loss dict:  {'classification_loss': 0.9890254771709442}
2025-01-12 21:14:16,119 [INFO] Step[550/2713]: training loss : 0.9906345117092132 TRAIN  loss dict:  {'classification_loss': 0.9906345117092132}
2025-01-12 21:14:28,071 [INFO] Step[600/2713]: training loss : 0.9901092314720153 TRAIN  loss dict:  {'classification_loss': 0.9901092314720153}
2025-01-12 21:14:40,006 [INFO] Step[650/2713]: training loss : 0.9947862982749939 TRAIN  loss dict:  {'classification_loss': 0.9947862982749939}
2025-01-12 21:14:51,923 [INFO] Step[700/2713]: training loss : 0.9988063609600067 TRAIN  loss dict:  {'classification_loss': 0.9988063609600067}
2025-01-12 21:15:03,832 [INFO] Step[750/2713]: training loss : 0.9747125244140625 TRAIN  loss dict:  {'classification_loss': 0.9747125244140625}
2025-01-12 21:15:15,805 [INFO] Step[800/2713]: training loss : 0.9853603994846344 TRAIN  loss dict:  {'classification_loss': 0.9853603994846344}
2025-01-12 21:15:27,711 [INFO] Step[850/2713]: training loss : 0.9839117348194122 TRAIN  loss dict:  {'classification_loss': 0.9839117348194122}
2025-01-12 21:15:39,626 [INFO] Step[900/2713]: training loss : 0.9848914265632629 TRAIN  loss dict:  {'classification_loss': 0.9848914265632629}
2025-01-12 21:15:51,596 [INFO] Step[950/2713]: training loss : 0.9732548570632935 TRAIN  loss dict:  {'classification_loss': 0.9732548570632935}
2025-01-12 21:16:03,493 [INFO] Step[1000/2713]: training loss : 0.993570110797882 TRAIN  loss dict:  {'classification_loss': 0.993570110797882}
2025-01-12 21:16:15,406 [INFO] Step[1050/2713]: training loss : 0.9766702544689179 TRAIN  loss dict:  {'classification_loss': 0.9766702544689179}
2025-01-12 21:16:27,334 [INFO] Step[1100/2713]: training loss : 0.9759612739086151 TRAIN  loss dict:  {'classification_loss': 0.9759612739086151}
2025-01-12 21:16:39,265 [INFO] Step[1150/2713]: training loss : 0.9871894037723541 TRAIN  loss dict:  {'classification_loss': 0.9871894037723541}
2025-01-12 21:16:51,187 [INFO] Step[1200/2713]: training loss : 0.9894725787639618 TRAIN  loss dict:  {'classification_loss': 0.9894725787639618}
2025-01-12 21:17:03,116 [INFO] Step[1250/2713]: training loss : 0.9835407745838165 TRAIN  loss dict:  {'classification_loss': 0.9835407745838165}
2025-01-12 21:17:15,016 [INFO] Step[1300/2713]: training loss : 0.9960039186477662 TRAIN  loss dict:  {'classification_loss': 0.9960039186477662}
2025-01-12 21:17:26,969 [INFO] Step[1350/2713]: training loss : 0.9844597458839417 TRAIN  loss dict:  {'classification_loss': 0.9844597458839417}
2025-01-12 21:17:38,843 [INFO] Step[1400/2713]: training loss : 0.9738377404212951 TRAIN  loss dict:  {'classification_loss': 0.9738377404212951}
2025-01-12 21:17:50,748 [INFO] Step[1450/2713]: training loss : 0.9904825031757355 TRAIN  loss dict:  {'classification_loss': 0.9904825031757355}
2025-01-12 21:18:02,642 [INFO] Step[1500/2713]: training loss : 0.976740151643753 TRAIN  loss dict:  {'classification_loss': 0.976740151643753}
2025-01-12 21:18:14,558 [INFO] Step[1550/2713]: training loss : 0.9801290512084961 TRAIN  loss dict:  {'classification_loss': 0.9801290512084961}
2025-01-12 21:18:26,441 [INFO] Step[1600/2713]: training loss : 0.9766078436374664 TRAIN  loss dict:  {'classification_loss': 0.9766078436374664}
2025-01-12 21:18:38,378 [INFO] Step[1650/2713]: training loss : 0.9935315346717835 TRAIN  loss dict:  {'classification_loss': 0.9935315346717835}
2025-01-12 21:18:50,303 [INFO] Step[1700/2713]: training loss : 0.9931661987304687 TRAIN  loss dict:  {'classification_loss': 0.9931661987304687}
2025-01-12 21:19:02,222 [INFO] Step[1750/2713]: training loss : 1.0217819929122924 TRAIN  loss dict:  {'classification_loss': 1.0217819929122924}
2025-01-12 21:19:14,098 [INFO] Step[1800/2713]: training loss : 0.9897904014587402 TRAIN  loss dict:  {'classification_loss': 0.9897904014587402}
2025-01-12 21:19:25,956 [INFO] Step[1850/2713]: training loss : 0.9898676311969757 TRAIN  loss dict:  {'classification_loss': 0.9898676311969757}
2025-01-12 21:19:37,891 [INFO] Step[1900/2713]: training loss : 0.9905573570728302 TRAIN  loss dict:  {'classification_loss': 0.9905573570728302}
2025-01-12 21:19:49,961 [INFO] Step[1950/2713]: training loss : 0.980858918428421 TRAIN  loss dict:  {'classification_loss': 0.980858918428421}
2025-01-12 21:20:01,913 [INFO] Step[2000/2713]: training loss : 0.974660930633545 TRAIN  loss dict:  {'classification_loss': 0.974660930633545}
2025-01-12 21:20:13,855 [INFO] Step[2050/2713]: training loss : 0.990244151353836 TRAIN  loss dict:  {'classification_loss': 0.990244151353836}
2025-01-12 21:20:25,789 [INFO] Step[2100/2713]: training loss : 0.9972730839252472 TRAIN  loss dict:  {'classification_loss': 0.9972730839252472}
2025-01-12 21:20:37,654 [INFO] Step[2150/2713]: training loss : 0.9825508296489716 TRAIN  loss dict:  {'classification_loss': 0.9825508296489716}
2025-01-12 21:20:49,587 [INFO] Step[2200/2713]: training loss : 0.9873477923870086 TRAIN  loss dict:  {'classification_loss': 0.9873477923870086}
2025-01-12 21:21:01,527 [INFO] Step[2250/2713]: training loss : 1.0099753153324127 TRAIN  loss dict:  {'classification_loss': 1.0099753153324127}
2025-01-12 21:21:13,416 [INFO] Step[2300/2713]: training loss : 0.9748752725124359 TRAIN  loss dict:  {'classification_loss': 0.9748752725124359}
2025-01-12 21:21:25,325 [INFO] Step[2350/2713]: training loss : 0.9855219185352325 TRAIN  loss dict:  {'classification_loss': 0.9855219185352325}
2025-01-12 21:21:37,259 [INFO] Step[2400/2713]: training loss : 0.982300375699997 TRAIN  loss dict:  {'classification_loss': 0.982300375699997}
2025-01-12 21:21:49,201 [INFO] Step[2450/2713]: training loss : 0.9890553343296051 TRAIN  loss dict:  {'classification_loss': 0.9890553343296051}
2025-01-12 21:22:01,119 [INFO] Step[2500/2713]: training loss : 0.983251872062683 TRAIN  loss dict:  {'classification_loss': 0.983251872062683}
2025-01-12 21:22:13,036 [INFO] Step[2550/2713]: training loss : 0.9753506767749787 TRAIN  loss dict:  {'classification_loss': 0.9753506767749787}
2025-01-12 21:22:24,958 [INFO] Step[2600/2713]: training loss : 0.9893302917480469 TRAIN  loss dict:  {'classification_loss': 0.9893302917480469}
2025-01-12 21:22:36,871 [INFO] Step[2650/2713]: training loss : 0.9888270998001099 TRAIN  loss dict:  {'classification_loss': 0.9888270998001099}
2025-01-12 21:22:48,804 [INFO] Step[2700/2713]: training loss : 0.9782573878765106 TRAIN  loss dict:  {'classification_loss': 0.9782573878765106}
2025-01-12 21:24:15,815 [INFO] Label accuracies statistics:
2025-01-12 21:24:15,815 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.25, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.5, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.5, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.25, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 0.75, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 1.0, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.5, 216: 0.5, 217: 0.75, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 1.0, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.25, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.5, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 0.75, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.5, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.5, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 21:24:15,817 [INFO] [31] TRAIN  loss: 0.9875951865437371 acc: 0.9980341565302863
2025-01-12 21:24:15,817 [INFO] [31] TRAIN  loss dict: {'classification_loss': 0.9875951865437371}
2025-01-12 21:24:15,817 [INFO] [31] VALIDATION loss: 1.7311801282983077 VALIDATION acc: 0.8081504702194358
2025-01-12 21:24:15,817 [INFO] [31] VALIDATION loss dict: {'classification_loss': 1.7311801282983077}
2025-01-12 21:24:15,817 [INFO] 
2025-01-12 21:24:33,002 [INFO] Step[50/2713]: training loss : 0.9693908667564393 TRAIN  loss dict:  {'classification_loss': 0.9693908667564393}
2025-01-12 21:24:44,911 [INFO] Step[100/2713]: training loss : 0.9798139107227325 TRAIN  loss dict:  {'classification_loss': 0.9798139107227325}
2025-01-12 21:24:56,801 [INFO] Step[150/2713]: training loss : 0.9687132489681244 TRAIN  loss dict:  {'classification_loss': 0.9687132489681244}
2025-01-12 21:25:08,694 [INFO] Step[200/2713]: training loss : 0.9848999094963073 TRAIN  loss dict:  {'classification_loss': 0.9848999094963073}
2025-01-12 21:25:20,597 [INFO] Step[250/2713]: training loss : 0.9763193202018737 TRAIN  loss dict:  {'classification_loss': 0.9763193202018737}
2025-01-12 21:25:32,515 [INFO] Step[300/2713]: training loss : 0.9898669409751892 TRAIN  loss dict:  {'classification_loss': 0.9898669409751892}
2025-01-12 21:25:44,438 [INFO] Step[350/2713]: training loss : 0.9958615863323211 TRAIN  loss dict:  {'classification_loss': 0.9958615863323211}
2025-01-12 21:25:56,342 [INFO] Step[400/2713]: training loss : 0.9871458613872528 TRAIN  loss dict:  {'classification_loss': 0.9871458613872528}
2025-01-12 21:26:08,279 [INFO] Step[450/2713]: training loss : 0.9918136715888977 TRAIN  loss dict:  {'classification_loss': 0.9918136715888977}
2025-01-12 21:26:20,222 [INFO] Step[500/2713]: training loss : 1.0053385627269744 TRAIN  loss dict:  {'classification_loss': 1.0053385627269744}
2025-01-12 21:26:32,261 [INFO] Step[550/2713]: training loss : 0.9828824400901794 TRAIN  loss dict:  {'classification_loss': 0.9828824400901794}
2025-01-12 21:26:44,601 [INFO] Step[600/2713]: training loss : 0.9671290802955628 TRAIN  loss dict:  {'classification_loss': 0.9671290802955628}
2025-01-12 21:26:56,943 [INFO] Step[650/2713]: training loss : 0.9754574692249298 TRAIN  loss dict:  {'classification_loss': 0.9754574692249298}
2025-01-12 21:27:09,151 [INFO] Step[700/2713]: training loss : 0.9881553065776825 TRAIN  loss dict:  {'classification_loss': 0.9881553065776825}
2025-01-12 21:27:22,072 [INFO] Step[750/2713]: training loss : 0.9675850057601929 TRAIN  loss dict:  {'classification_loss': 0.9675850057601929}
2025-01-12 21:27:34,360 [INFO] Step[800/2713]: training loss : 0.9795624804496765 TRAIN  loss dict:  {'classification_loss': 0.9795624804496765}
2025-01-12 21:27:47,024 [INFO] Step[850/2713]: training loss : 0.9882273268699646 TRAIN  loss dict:  {'classification_loss': 0.9882273268699646}
2025-01-12 21:28:00,589 [INFO] Step[900/2713]: training loss : 0.9748955595493317 TRAIN  loss dict:  {'classification_loss': 0.9748955595493317}
2025-01-12 21:28:14,105 [INFO] Step[950/2713]: training loss : 0.9811415636539459 TRAIN  loss dict:  {'classification_loss': 0.9811415636539459}
2025-01-12 21:28:26,355 [INFO] Step[1000/2713]: training loss : 0.9966150259971619 TRAIN  loss dict:  {'classification_loss': 0.9966150259971619}
2025-01-12 21:28:38,232 [INFO] Step[1050/2713]: training loss : 0.9811235988140106 TRAIN  loss dict:  {'classification_loss': 0.9811235988140106}
2025-01-12 21:28:50,092 [INFO] Step[1100/2713]: training loss : 0.9721555554866791 TRAIN  loss dict:  {'classification_loss': 0.9721555554866791}
2025-01-12 21:29:02,015 [INFO] Step[1150/2713]: training loss : 0.9945265936851502 TRAIN  loss dict:  {'classification_loss': 0.9945265936851502}
2025-01-12 21:29:13,891 [INFO] Step[1200/2713]: training loss : 0.9718712043762207 TRAIN  loss dict:  {'classification_loss': 0.9718712043762207}
2025-01-12 21:29:25,819 [INFO] Step[1250/2713]: training loss : 0.9706062078475952 TRAIN  loss dict:  {'classification_loss': 0.9706062078475952}
2025-01-12 21:29:37,703 [INFO] Step[1300/2713]: training loss : 0.9796573114395142 TRAIN  loss dict:  {'classification_loss': 0.9796573114395142}
2025-01-12 21:29:49,591 [INFO] Step[1350/2713]: training loss : 0.9781322944164276 TRAIN  loss dict:  {'classification_loss': 0.9781322944164276}
2025-01-12 21:30:01,496 [INFO] Step[1400/2713]: training loss : 0.9800027215480804 TRAIN  loss dict:  {'classification_loss': 0.9800027215480804}
2025-01-12 21:30:13,403 [INFO] Step[1450/2713]: training loss : 0.9879130589962005 TRAIN  loss dict:  {'classification_loss': 0.9879130589962005}
2025-01-12 21:30:25,299 [INFO] Step[1500/2713]: training loss : 0.9743971359729767 TRAIN  loss dict:  {'classification_loss': 0.9743971359729767}
2025-01-12 21:30:37,248 [INFO] Step[1550/2713]: training loss : 0.9805578958988189 TRAIN  loss dict:  {'classification_loss': 0.9805578958988189}
2025-01-12 21:30:49,172 [INFO] Step[1600/2713]: training loss : 0.9952838230133056 TRAIN  loss dict:  {'classification_loss': 0.9952838230133056}
2025-01-12 21:31:01,081 [INFO] Step[1650/2713]: training loss : 0.9902779078483581 TRAIN  loss dict:  {'classification_loss': 0.9902779078483581}
2025-01-12 21:31:12,988 [INFO] Step[1700/2713]: training loss : 0.9804162681102753 TRAIN  loss dict:  {'classification_loss': 0.9804162681102753}
2025-01-12 21:31:24,856 [INFO] Step[1750/2713]: training loss : 0.9779134333133698 TRAIN  loss dict:  {'classification_loss': 0.9779134333133698}
2025-01-12 21:31:36,738 [INFO] Step[1800/2713]: training loss : 0.9719995546340943 TRAIN  loss dict:  {'classification_loss': 0.9719995546340943}
2025-01-12 21:31:48,665 [INFO] Step[1850/2713]: training loss : 0.9826231670379638 TRAIN  loss dict:  {'classification_loss': 0.9826231670379638}
2025-01-12 21:32:00,565 [INFO] Step[1900/2713]: training loss : 0.9883689570426941 TRAIN  loss dict:  {'classification_loss': 0.9883689570426941}
2025-01-12 21:32:12,480 [INFO] Step[1950/2713]: training loss : 0.9864567613601685 TRAIN  loss dict:  {'classification_loss': 0.9864567613601685}
2025-01-12 21:32:24,403 [INFO] Step[2000/2713]: training loss : 0.9834038949012757 TRAIN  loss dict:  {'classification_loss': 0.9834038949012757}
2025-01-12 21:32:36,322 [INFO] Step[2050/2713]: training loss : 0.9811288988590241 TRAIN  loss dict:  {'classification_loss': 0.9811288988590241}
2025-01-12 21:32:48,245 [INFO] Step[2100/2713]: training loss : 0.9862262344360352 TRAIN  loss dict:  {'classification_loss': 0.9862262344360352}
2025-01-12 21:33:00,189 [INFO] Step[2150/2713]: training loss : 0.9998123240470886 TRAIN  loss dict:  {'classification_loss': 0.9998123240470886}
2025-01-12 21:33:12,069 [INFO] Step[2200/2713]: training loss : 0.9842551624774933 TRAIN  loss dict:  {'classification_loss': 0.9842551624774933}
2025-01-12 21:33:24,054 [INFO] Step[2250/2713]: training loss : 0.9787810599803924 TRAIN  loss dict:  {'classification_loss': 0.9787810599803924}
2025-01-12 21:33:35,970 [INFO] Step[2300/2713]: training loss : 0.9896370375156402 TRAIN  loss dict:  {'classification_loss': 0.9896370375156402}
2025-01-12 21:33:47,894 [INFO] Step[2350/2713]: training loss : 0.9850226271152497 TRAIN  loss dict:  {'classification_loss': 0.9850226271152497}
2025-01-12 21:33:59,811 [INFO] Step[2400/2713]: training loss : 0.9779557108879089 TRAIN  loss dict:  {'classification_loss': 0.9779557108879089}
2025-01-12 21:34:11,725 [INFO] Step[2450/2713]: training loss : 0.9787851643562316 TRAIN  loss dict:  {'classification_loss': 0.9787851643562316}
2025-01-12 21:34:23,644 [INFO] Step[2500/2713]: training loss : 0.9698320257663727 TRAIN  loss dict:  {'classification_loss': 0.9698320257663727}
2025-01-12 21:34:35,579 [INFO] Step[2550/2713]: training loss : 0.9788579213619232 TRAIN  loss dict:  {'classification_loss': 0.9788579213619232}
2025-01-12 21:34:47,450 [INFO] Step[2600/2713]: training loss : 0.9995457887649536 TRAIN  loss dict:  {'classification_loss': 0.9995457887649536}
2025-01-12 21:34:59,319 [INFO] Step[2650/2713]: training loss : 0.9767733180522918 TRAIN  loss dict:  {'classification_loss': 0.9767733180522918}
2025-01-12 21:35:11,218 [INFO] Step[2700/2713]: training loss : 0.9835947751998901 TRAIN  loss dict:  {'classification_loss': 0.9835947751998901}
2025-01-12 21:36:39,473 [INFO] Label accuracies statistics:
2025-01-12 21:36:39,473 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 0.75, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.25, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 0.75, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.5, 154: 0.75, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 0.75, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 0.75, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 1.0, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 1.0, 355: 0.5, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 21:36:39,475 [INFO] [32] TRAIN  loss: 0.9823620296957042 acc: 0.9986484826145718
2025-01-12 21:36:39,475 [INFO] [32] TRAIN  loss dict: {'classification_loss': 0.9823620296957042}
2025-01-12 21:36:39,475 [INFO] [32] VALIDATION loss: 1.7663262596255855 VALIDATION acc: 0.8106583072100313
2025-01-12 21:36:39,475 [INFO] [32] VALIDATION loss dict: {'classification_loss': 1.7663262596255855}
2025-01-12 21:36:39,475 [INFO] 
2025-01-12 21:36:57,204 [INFO] Step[50/2713]: training loss : 0.9944725680351257 TRAIN  loss dict:  {'classification_loss': 0.9944725680351257}
2025-01-12 21:37:09,070 [INFO] Step[100/2713]: training loss : 0.9991163611412048 TRAIN  loss dict:  {'classification_loss': 0.9991163611412048}
2025-01-12 21:37:20,991 [INFO] Step[150/2713]: training loss : 0.9904291582107544 TRAIN  loss dict:  {'classification_loss': 0.9904291582107544}
2025-01-12 21:37:32,882 [INFO] Step[200/2713]: training loss : 0.9778443896770477 TRAIN  loss dict:  {'classification_loss': 0.9778443896770477}
2025-01-12 21:37:44,775 [INFO] Step[250/2713]: training loss : 0.9813010048866272 TRAIN  loss dict:  {'classification_loss': 0.9813010048866272}
2025-01-12 21:37:56,689 [INFO] Step[300/2713]: training loss : 0.9745266389846802 TRAIN  loss dict:  {'classification_loss': 0.9745266389846802}
2025-01-12 21:38:08,578 [INFO] Step[350/2713]: training loss : 0.9769893372058869 TRAIN  loss dict:  {'classification_loss': 0.9769893372058869}
2025-01-12 21:38:20,506 [INFO] Step[400/2713]: training loss : 0.9951035237312317 TRAIN  loss dict:  {'classification_loss': 0.9951035237312317}
2025-01-12 21:38:32,377 [INFO] Step[450/2713]: training loss : 0.9839344954490662 TRAIN  loss dict:  {'classification_loss': 0.9839344954490662}
2025-01-12 21:38:44,237 [INFO] Step[500/2713]: training loss : 0.977629714012146 TRAIN  loss dict:  {'classification_loss': 0.977629714012146}
2025-01-12 21:38:56,126 [INFO] Step[550/2713]: training loss : 0.975770697593689 TRAIN  loss dict:  {'classification_loss': 0.975770697593689}
2025-01-12 21:39:08,045 [INFO] Step[600/2713]: training loss : 0.99339235663414 TRAIN  loss dict:  {'classification_loss': 0.99339235663414}
2025-01-12 21:39:19,974 [INFO] Step[650/2713]: training loss : 0.9911963069438934 TRAIN  loss dict:  {'classification_loss': 0.9911963069438934}
2025-01-12 21:39:31,848 [INFO] Step[700/2713]: training loss : 0.973245017528534 TRAIN  loss dict:  {'classification_loss': 0.973245017528534}
2025-01-12 21:39:43,804 [INFO] Step[750/2713]: training loss : 0.976765810251236 TRAIN  loss dict:  {'classification_loss': 0.976765810251236}
2025-01-12 21:39:55,748 [INFO] Step[800/2713]: training loss : 0.9870469164848328 TRAIN  loss dict:  {'classification_loss': 0.9870469164848328}
2025-01-12 21:40:07,606 [INFO] Step[850/2713]: training loss : 0.9864985501766205 TRAIN  loss dict:  {'classification_loss': 0.9864985501766205}
2025-01-12 21:40:19,504 [INFO] Step[900/2713]: training loss : 0.9749763369560241 TRAIN  loss dict:  {'classification_loss': 0.9749763369560241}
2025-01-12 21:40:31,432 [INFO] Step[950/2713]: training loss : 0.9761395597457886 TRAIN  loss dict:  {'classification_loss': 0.9761395597457886}
2025-01-12 21:40:43,353 [INFO] Step[1000/2713]: training loss : 0.9983895635604858 TRAIN  loss dict:  {'classification_loss': 0.9983895635604858}
2025-01-12 21:40:55,237 [INFO] Step[1050/2713]: training loss : 1.012319357395172 TRAIN  loss dict:  {'classification_loss': 1.012319357395172}
2025-01-12 21:41:07,147 [INFO] Step[1100/2713]: training loss : 0.9744627523422241 TRAIN  loss dict:  {'classification_loss': 0.9744627523422241}
2025-01-12 21:41:19,055 [INFO] Step[1150/2713]: training loss : 0.9741080808639526 TRAIN  loss dict:  {'classification_loss': 0.9741080808639526}
2025-01-12 21:41:30,943 [INFO] Step[1200/2713]: training loss : 0.9737996292114258 TRAIN  loss dict:  {'classification_loss': 0.9737996292114258}
2025-01-12 21:41:42,851 [INFO] Step[1250/2713]: training loss : 0.9820419800281525 TRAIN  loss dict:  {'classification_loss': 0.9820419800281525}
2025-01-12 21:41:54,725 [INFO] Step[1300/2713]: training loss : 0.9904074335098266 TRAIN  loss dict:  {'classification_loss': 0.9904074335098266}
2025-01-12 21:42:06,621 [INFO] Step[1350/2713]: training loss : 0.9751047933101654 TRAIN  loss dict:  {'classification_loss': 0.9751047933101654}
2025-01-12 21:42:18,522 [INFO] Step[1400/2713]: training loss : 0.9817056334018708 TRAIN  loss dict:  {'classification_loss': 0.9817056334018708}
2025-01-12 21:42:30,442 [INFO] Step[1450/2713]: training loss : 0.9742783904075623 TRAIN  loss dict:  {'classification_loss': 0.9742783904075623}
2025-01-12 21:42:42,346 [INFO] Step[1500/2713]: training loss : 0.9773053002357482 TRAIN  loss dict:  {'classification_loss': 0.9773053002357482}
2025-01-12 21:42:54,255 [INFO] Step[1550/2713]: training loss : 0.9869240713119507 TRAIN  loss dict:  {'classification_loss': 0.9869240713119507}
2025-01-12 21:43:06,171 [INFO] Step[1600/2713]: training loss : 0.9859370040893555 TRAIN  loss dict:  {'classification_loss': 0.9859370040893555}
2025-01-12 21:43:18,111 [INFO] Step[1650/2713]: training loss : 0.9803089356422424 TRAIN  loss dict:  {'classification_loss': 0.9803089356422424}
2025-01-12 21:43:30,013 [INFO] Step[1700/2713]: training loss : 0.980325984954834 TRAIN  loss dict:  {'classification_loss': 0.980325984954834}
2025-01-12 21:43:41,962 [INFO] Step[1750/2713]: training loss : 0.9733457016944885 TRAIN  loss dict:  {'classification_loss': 0.9733457016944885}
2025-01-12 21:43:53,861 [INFO] Step[1800/2713]: training loss : 0.9737887394428253 TRAIN  loss dict:  {'classification_loss': 0.9737887394428253}
2025-01-12 21:44:05,739 [INFO] Step[1850/2713]: training loss : 0.9757244980335236 TRAIN  loss dict:  {'classification_loss': 0.9757244980335236}
2025-01-12 21:44:17,641 [INFO] Step[1900/2713]: training loss : 0.9877665042877197 TRAIN  loss dict:  {'classification_loss': 0.9877665042877197}
2025-01-12 21:44:29,564 [INFO] Step[1950/2713]: training loss : 0.9710550439357758 TRAIN  loss dict:  {'classification_loss': 0.9710550439357758}
2025-01-12 21:44:41,478 [INFO] Step[2000/2713]: training loss : 0.9925546431541443 TRAIN  loss dict:  {'classification_loss': 0.9925546431541443}
2025-01-12 21:44:53,421 [INFO] Step[2050/2713]: training loss : 0.9743953096866608 TRAIN  loss dict:  {'classification_loss': 0.9743953096866608}
2025-01-12 21:45:05,308 [INFO] Step[2100/2713]: training loss : 0.9776220798492432 TRAIN  loss dict:  {'classification_loss': 0.9776220798492432}
2025-01-12 21:45:17,294 [INFO] Step[2150/2713]: training loss : 1.0117278575897217 TRAIN  loss dict:  {'classification_loss': 1.0117278575897217}
2025-01-12 21:45:29,648 [INFO] Step[2200/2713]: training loss : 0.9744578266143799 TRAIN  loss dict:  {'classification_loss': 0.9744578266143799}
2025-01-12 21:45:42,141 [INFO] Step[2250/2713]: training loss : 0.9736943054199219 TRAIN  loss dict:  {'classification_loss': 0.9736943054199219}
2025-01-12 21:45:54,436 [INFO] Step[2300/2713]: training loss : 0.9744599413871765 TRAIN  loss dict:  {'classification_loss': 0.9744599413871765}
2025-01-12 21:46:06,899 [INFO] Step[2350/2713]: training loss : 0.9747241842746734 TRAIN  loss dict:  {'classification_loss': 0.9747241842746734}
2025-01-12 21:46:19,464 [INFO] Step[2400/2713]: training loss : 0.9617589819431305 TRAIN  loss dict:  {'classification_loss': 0.9617589819431305}
2025-01-12 21:46:31,951 [INFO] Step[2450/2713]: training loss : 0.9752824413776398 TRAIN  loss dict:  {'classification_loss': 0.9752824413776398}
2025-01-12 21:46:45,080 [INFO] Step[2500/2713]: training loss : 0.9795618844032288 TRAIN  loss dict:  {'classification_loss': 0.9795618844032288}
2025-01-12 21:46:58,492 [INFO] Step[2550/2713]: training loss : 0.9809850323200225 TRAIN  loss dict:  {'classification_loss': 0.9809850323200225}
2025-01-12 21:47:11,608 [INFO] Step[2600/2713]: training loss : 0.9897335743904114 TRAIN  loss dict:  {'classification_loss': 0.9897335743904114}
2025-01-12 21:47:23,701 [INFO] Step[2650/2713]: training loss : 0.9883306920528412 TRAIN  loss dict:  {'classification_loss': 0.9883306920528412}
2025-01-12 21:47:35,615 [INFO] Step[2700/2713]: training loss : 0.9953005611896515 TRAIN  loss dict:  {'classification_loss': 0.9953005611896515}
2025-01-12 21:49:02,222 [INFO] Label accuracies statistics:
2025-01-12 21:49:02,222 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 0.75, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.25, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 1.0, 259: 1.0, 260: 1.0, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 0.75, 348: 0.75, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 21:49:02,224 [INFO] [33] TRAIN  loss: 0.982210329371627 acc: 0.9972969652291437
2025-01-12 21:49:02,224 [INFO] [33] TRAIN  loss dict: {'classification_loss': 0.982210329371627}
2025-01-12 21:49:02,224 [INFO] [33] VALIDATION loss: 1.7378095522859043 VALIDATION acc: 0.806269592476489
2025-01-12 21:49:02,224 [INFO] [33] VALIDATION loss dict: {'classification_loss': 1.7378095522859043}
2025-01-12 21:49:02,224 [INFO] 
2025-01-12 21:49:20,065 [INFO] Step[50/2713]: training loss : 0.991801575422287 TRAIN  loss dict:  {'classification_loss': 0.991801575422287}
2025-01-12 21:49:31,929 [INFO] Step[100/2713]: training loss : 0.9730236721038819 TRAIN  loss dict:  {'classification_loss': 0.9730236721038819}
2025-01-12 21:49:43,837 [INFO] Step[150/2713]: training loss : 0.9631311118602752 TRAIN  loss dict:  {'classification_loss': 0.9631311118602752}
2025-01-12 21:49:55,729 [INFO] Step[200/2713]: training loss : 0.9740268528461457 TRAIN  loss dict:  {'classification_loss': 0.9740268528461457}
2025-01-12 21:50:07,647 [INFO] Step[250/2713]: training loss : 0.977484472990036 TRAIN  loss dict:  {'classification_loss': 0.977484472990036}
2025-01-12 21:50:19,576 [INFO] Step[300/2713]: training loss : 0.9789675056934357 TRAIN  loss dict:  {'classification_loss': 0.9789675056934357}
2025-01-12 21:50:31,474 [INFO] Step[350/2713]: training loss : 0.9795885455608367 TRAIN  loss dict:  {'classification_loss': 0.9795885455608367}
2025-01-12 21:50:43,342 [INFO] Step[400/2713]: training loss : 0.9716557872295379 TRAIN  loss dict:  {'classification_loss': 0.9716557872295379}
2025-01-12 21:50:55,267 [INFO] Step[450/2713]: training loss : 0.9843803215026855 TRAIN  loss dict:  {'classification_loss': 0.9843803215026855}
2025-01-12 21:51:07,131 [INFO] Step[500/2713]: training loss : 0.9816458010673523 TRAIN  loss dict:  {'classification_loss': 0.9816458010673523}
2025-01-12 21:51:19,049 [INFO] Step[550/2713]: training loss : 0.9721386802196502 TRAIN  loss dict:  {'classification_loss': 0.9721386802196502}
2025-01-12 21:51:30,955 [INFO] Step[600/2713]: training loss : 0.9825335276126862 TRAIN  loss dict:  {'classification_loss': 0.9825335276126862}
2025-01-12 21:51:42,820 [INFO] Step[650/2713]: training loss : 0.9714848613739013 TRAIN  loss dict:  {'classification_loss': 0.9714848613739013}
2025-01-12 21:51:54,735 [INFO] Step[700/2713]: training loss : 0.9774208486080169 TRAIN  loss dict:  {'classification_loss': 0.9774208486080169}
2025-01-12 21:52:06,645 [INFO] Step[750/2713]: training loss : 0.9967698752880096 TRAIN  loss dict:  {'classification_loss': 0.9967698752880096}
2025-01-12 21:52:18,521 [INFO] Step[800/2713]: training loss : 0.9758051288127899 TRAIN  loss dict:  {'classification_loss': 0.9758051288127899}
2025-01-12 21:52:30,458 [INFO] Step[850/2713]: training loss : 0.987476464509964 TRAIN  loss dict:  {'classification_loss': 0.987476464509964}
2025-01-12 21:52:42,359 [INFO] Step[900/2713]: training loss : 0.974201774597168 TRAIN  loss dict:  {'classification_loss': 0.974201774597168}
2025-01-12 21:52:54,251 [INFO] Step[950/2713]: training loss : 0.9704503202438355 TRAIN  loss dict:  {'classification_loss': 0.9704503202438355}
2025-01-12 21:53:06,163 [INFO] Step[1000/2713]: training loss : 1.0056393015384675 TRAIN  loss dict:  {'classification_loss': 1.0056393015384675}
2025-01-12 21:53:18,053 [INFO] Step[1050/2713]: training loss : 0.9716049420833588 TRAIN  loss dict:  {'classification_loss': 0.9716049420833588}
2025-01-12 21:53:29,921 [INFO] Step[1100/2713]: training loss : 0.9743216264247895 TRAIN  loss dict:  {'classification_loss': 0.9743216264247895}
2025-01-12 21:53:41,813 [INFO] Step[1150/2713]: training loss : 0.9688191056251526 TRAIN  loss dict:  {'classification_loss': 0.9688191056251526}
2025-01-12 21:53:53,694 [INFO] Step[1200/2713]: training loss : 0.9823827707767486 TRAIN  loss dict:  {'classification_loss': 0.9823827707767486}
2025-01-12 21:54:05,577 [INFO] Step[1250/2713]: training loss : 0.9702580344676971 TRAIN  loss dict:  {'classification_loss': 0.9702580344676971}
2025-01-12 21:54:17,476 [INFO] Step[1300/2713]: training loss : 0.9870266461372376 TRAIN  loss dict:  {'classification_loss': 0.9870266461372376}
2025-01-12 21:54:29,385 [INFO] Step[1350/2713]: training loss : 0.9788302993774414 TRAIN  loss dict:  {'classification_loss': 0.9788302993774414}
2025-01-12 21:54:41,301 [INFO] Step[1400/2713]: training loss : 0.9796781015396118 TRAIN  loss dict:  {'classification_loss': 0.9796781015396118}
2025-01-12 21:54:53,154 [INFO] Step[1450/2713]: training loss : 0.9923792636394501 TRAIN  loss dict:  {'classification_loss': 0.9923792636394501}
2025-01-12 21:55:05,088 [INFO] Step[1500/2713]: training loss : 0.9870536768436432 TRAIN  loss dict:  {'classification_loss': 0.9870536768436432}
2025-01-12 21:55:17,013 [INFO] Step[1550/2713]: training loss : 0.9845448648929596 TRAIN  loss dict:  {'classification_loss': 0.9845448648929596}
2025-01-12 21:55:28,904 [INFO] Step[1600/2713]: training loss : 0.9868579697608948 TRAIN  loss dict:  {'classification_loss': 0.9868579697608948}
2025-01-12 21:55:40,829 [INFO] Step[1650/2713]: training loss : 0.9733534038066864 TRAIN  loss dict:  {'classification_loss': 0.9733534038066864}
2025-01-12 21:55:52,700 [INFO] Step[1700/2713]: training loss : 1.0107356739044189 TRAIN  loss dict:  {'classification_loss': 1.0107356739044189}
2025-01-12 21:56:04,594 [INFO] Step[1750/2713]: training loss : 0.9920840680599212 TRAIN  loss dict:  {'classification_loss': 0.9920840680599212}
2025-01-12 21:56:16,469 [INFO] Step[1800/2713]: training loss : 0.9845225286483764 TRAIN  loss dict:  {'classification_loss': 0.9845225286483764}
2025-01-12 21:56:28,383 [INFO] Step[1850/2713]: training loss : 0.9976554024219513 TRAIN  loss dict:  {'classification_loss': 0.9976554024219513}
2025-01-12 21:56:40,293 [INFO] Step[1900/2713]: training loss : 0.9933621001243591 TRAIN  loss dict:  {'classification_loss': 0.9933621001243591}
2025-01-12 21:56:52,247 [INFO] Step[1950/2713]: training loss : 0.9762957751750946 TRAIN  loss dict:  {'classification_loss': 0.9762957751750946}
2025-01-12 21:57:04,164 [INFO] Step[2000/2713]: training loss : 0.9763747930526734 TRAIN  loss dict:  {'classification_loss': 0.9763747930526734}
2025-01-12 21:57:16,077 [INFO] Step[2050/2713]: training loss : 0.9765186393260956 TRAIN  loss dict:  {'classification_loss': 0.9765186393260956}
2025-01-12 21:57:27,972 [INFO] Step[2100/2713]: training loss : 0.9802714824676514 TRAIN  loss dict:  {'classification_loss': 0.9802714824676514}
2025-01-12 21:57:39,864 [INFO] Step[2150/2713]: training loss : 0.9759473633766175 TRAIN  loss dict:  {'classification_loss': 0.9759473633766175}
2025-01-12 21:57:51,753 [INFO] Step[2200/2713]: training loss : 0.981975337266922 TRAIN  loss dict:  {'classification_loss': 0.981975337266922}
2025-01-12 21:58:03,678 [INFO] Step[2250/2713]: training loss : 0.9902540421485901 TRAIN  loss dict:  {'classification_loss': 0.9902540421485901}
2025-01-12 21:58:15,531 [INFO] Step[2300/2713]: training loss : 0.9689841723442078 TRAIN  loss dict:  {'classification_loss': 0.9689841723442078}
2025-01-12 21:58:27,480 [INFO] Step[2350/2713]: training loss : 0.9848332893848419 TRAIN  loss dict:  {'classification_loss': 0.9848332893848419}
2025-01-12 21:58:39,355 [INFO] Step[2400/2713]: training loss : 0.975399922132492 TRAIN  loss dict:  {'classification_loss': 0.975399922132492}
2025-01-12 21:58:51,265 [INFO] Step[2450/2713]: training loss : 0.9850375711917877 TRAIN  loss dict:  {'classification_loss': 0.9850375711917877}
2025-01-12 21:59:03,140 [INFO] Step[2500/2713]: training loss : 0.9809946489334106 TRAIN  loss dict:  {'classification_loss': 0.9809946489334106}
2025-01-12 21:59:15,031 [INFO] Step[2550/2713]: training loss : 0.9756944692134857 TRAIN  loss dict:  {'classification_loss': 0.9756944692134857}
2025-01-12 21:59:26,975 [INFO] Step[2600/2713]: training loss : 1.0049310505390168 TRAIN  loss dict:  {'classification_loss': 1.0049310505390168}
2025-01-12 21:59:38,894 [INFO] Step[2650/2713]: training loss : 0.9753140163421631 TRAIN  loss dict:  {'classification_loss': 0.9753140163421631}
2025-01-12 21:59:50,795 [INFO] Step[2700/2713]: training loss : 0.9764929330348968 TRAIN  loss dict:  {'classification_loss': 0.9764929330348968}
2025-01-12 22:01:17,514 [INFO] Label accuracies statistics:
2025-01-12 22:01:17,515 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 1.0, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.75, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 0.75, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 0.75, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 0.75, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 1.0, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 0.75, 281: 0.5, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 1.0, 329: 0.75, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 0.75, 336: 0.75, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.5, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 22:01:18,565 [INFO] [34] TRAIN  loss: 0.981221271866237 acc: 0.9975426956628578
2025-01-12 22:01:18,565 [INFO] [34] TRAIN  loss dict: {'classification_loss': 0.981221271866237}
2025-01-12 22:01:18,565 [INFO] [34] VALIDATION loss: 1.6967403163811319 VALIDATION acc: 0.826332288401254
2025-01-12 22:01:18,565 [INFO] [34] VALIDATION loss dict: {'classification_loss': 1.6967403163811319}
2025-01-12 22:01:18,565 [INFO] 
2025-01-12 22:01:36,257 [INFO] Step[50/2713]: training loss : 0.9794928026199341 TRAIN  loss dict:  {'classification_loss': 0.9794928026199341}
2025-01-12 22:01:48,200 [INFO] Step[100/2713]: training loss : 0.9747845315933228 TRAIN  loss dict:  {'classification_loss': 0.9747845315933228}
2025-01-12 22:02:00,108 [INFO] Step[150/2713]: training loss : 0.971021374464035 TRAIN  loss dict:  {'classification_loss': 0.971021374464035}
2025-01-12 22:02:12,038 [INFO] Step[200/2713]: training loss : 0.969484088420868 TRAIN  loss dict:  {'classification_loss': 0.969484088420868}
2025-01-12 22:02:23,994 [INFO] Step[250/2713]: training loss : 0.9831431770324707 TRAIN  loss dict:  {'classification_loss': 0.9831431770324707}
2025-01-12 22:02:35,903 [INFO] Step[300/2713]: training loss : 0.9709970438480378 TRAIN  loss dict:  {'classification_loss': 0.9709970438480378}
2025-01-12 22:02:47,831 [INFO] Step[350/2713]: training loss : 0.9677864170074463 TRAIN  loss dict:  {'classification_loss': 0.9677864170074463}
2025-01-12 22:02:59,738 [INFO] Step[400/2713]: training loss : 0.9745634722709656 TRAIN  loss dict:  {'classification_loss': 0.9745634722709656}
2025-01-12 22:03:11,625 [INFO] Step[450/2713]: training loss : 0.9721726334095001 TRAIN  loss dict:  {'classification_loss': 0.9721726334095001}
2025-01-12 22:03:23,549 [INFO] Step[500/2713]: training loss : 0.9794338655471801 TRAIN  loss dict:  {'classification_loss': 0.9794338655471801}
2025-01-12 22:03:35,468 [INFO] Step[550/2713]: training loss : 0.9922647392749786 TRAIN  loss dict:  {'classification_loss': 0.9922647392749786}
2025-01-12 22:03:47,396 [INFO] Step[600/2713]: training loss : 0.9772553312778473 TRAIN  loss dict:  {'classification_loss': 0.9772553312778473}
2025-01-12 22:03:59,308 [INFO] Step[650/2713]: training loss : 0.9722574424743652 TRAIN  loss dict:  {'classification_loss': 0.9722574424743652}
2025-01-12 22:04:11,244 [INFO] Step[700/2713]: training loss : 0.9753795576095581 TRAIN  loss dict:  {'classification_loss': 0.9753795576095581}
2025-01-12 22:04:23,435 [INFO] Step[750/2713]: training loss : 0.9821013522148132 TRAIN  loss dict:  {'classification_loss': 0.9821013522148132}
2025-01-12 22:04:35,862 [INFO] Step[800/2713]: training loss : 0.9730216300487519 TRAIN  loss dict:  {'classification_loss': 0.9730216300487519}
2025-01-12 22:04:48,254 [INFO] Step[850/2713]: training loss : 0.9649562692642212 TRAIN  loss dict:  {'classification_loss': 0.9649562692642212}
2025-01-12 22:05:00,527 [INFO] Step[900/2713]: training loss : 0.9782421684265137 TRAIN  loss dict:  {'classification_loss': 0.9782421684265137}
2025-01-12 22:05:13,242 [INFO] Step[950/2713]: training loss : 0.9805202543735504 TRAIN  loss dict:  {'classification_loss': 0.9805202543735504}
2025-01-12 22:05:25,490 [INFO] Step[1000/2713]: training loss : 0.9718884217739105 TRAIN  loss dict:  {'classification_loss': 0.9718884217739105}
2025-01-12 22:05:38,221 [INFO] Step[1050/2713]: training loss : 0.9853992938995362 TRAIN  loss dict:  {'classification_loss': 0.9853992938995362}
2025-01-12 22:05:52,009 [INFO] Step[1100/2713]: training loss : 0.9843107032775879 TRAIN  loss dict:  {'classification_loss': 0.9843107032775879}
2025-01-12 22:06:05,312 [INFO] Step[1150/2713]: training loss : 0.9722500360012054 TRAIN  loss dict:  {'classification_loss': 0.9722500360012054}
2025-01-12 22:06:17,421 [INFO] Step[1200/2713]: training loss : 0.9808176052570343 TRAIN  loss dict:  {'classification_loss': 0.9808176052570343}
2025-01-12 22:06:29,303 [INFO] Step[1250/2713]: training loss : 0.9707911944389344 TRAIN  loss dict:  {'classification_loss': 0.9707911944389344}
2025-01-12 22:06:41,175 [INFO] Step[1300/2713]: training loss : 0.9733137011528015 TRAIN  loss dict:  {'classification_loss': 0.9733137011528015}
2025-01-12 22:06:53,056 [INFO] Step[1350/2713]: training loss : 0.9699323225021362 TRAIN  loss dict:  {'classification_loss': 0.9699323225021362}
2025-01-12 22:07:04,937 [INFO] Step[1400/2713]: training loss : 0.9753573036193848 TRAIN  loss dict:  {'classification_loss': 0.9753573036193848}
2025-01-12 22:07:16,853 [INFO] Step[1450/2713]: training loss : 0.9959602308273315 TRAIN  loss dict:  {'classification_loss': 0.9959602308273315}
2025-01-12 22:07:28,732 [INFO] Step[1500/2713]: training loss : 0.9743162071704865 TRAIN  loss dict:  {'classification_loss': 0.9743162071704865}
2025-01-12 22:07:40,658 [INFO] Step[1550/2713]: training loss : 0.9893924963474273 TRAIN  loss dict:  {'classification_loss': 0.9893924963474273}
2025-01-12 22:07:52,558 [INFO] Step[1600/2713]: training loss : 0.965737863779068 TRAIN  loss dict:  {'classification_loss': 0.965737863779068}
2025-01-12 22:08:04,578 [INFO] Step[1650/2713]: training loss : 0.9913217687606811 TRAIN  loss dict:  {'classification_loss': 0.9913217687606811}
2025-01-12 22:08:16,444 [INFO] Step[1700/2713]: training loss : 0.9879362881183624 TRAIN  loss dict:  {'classification_loss': 0.9879362881183624}
2025-01-12 22:08:28,185 [INFO] Step[1750/2713]: training loss : 0.9746164488792419 TRAIN  loss dict:  {'classification_loss': 0.9746164488792419}
2025-01-12 22:08:40,160 [INFO] Step[1800/2713]: training loss : 0.9663308441638947 TRAIN  loss dict:  {'classification_loss': 0.9663308441638947}
2025-01-12 22:08:52,165 [INFO] Step[1850/2713]: training loss : 0.9750865316390991 TRAIN  loss dict:  {'classification_loss': 0.9750865316390991}
2025-01-12 22:09:04,295 [INFO] Step[1900/2713]: training loss : 0.9775975024700165 TRAIN  loss dict:  {'classification_loss': 0.9775975024700165}
2025-01-12 22:09:16,211 [INFO] Step[1950/2713]: training loss : 0.9839125788211822 TRAIN  loss dict:  {'classification_loss': 0.9839125788211822}
2025-01-12 22:09:28,122 [INFO] Step[2000/2713]: training loss : 0.9887153470516205 TRAIN  loss dict:  {'classification_loss': 0.9887153470516205}
2025-01-12 22:09:40,073 [INFO] Step[2050/2713]: training loss : 0.9798472285270691 TRAIN  loss dict:  {'classification_loss': 0.9798472285270691}
2025-01-12 22:09:52,038 [INFO] Step[2100/2713]: training loss : 0.9845662140846252 TRAIN  loss dict:  {'classification_loss': 0.9845662140846252}
2025-01-12 22:10:04,000 [INFO] Step[2150/2713]: training loss : 0.9689111351966858 TRAIN  loss dict:  {'classification_loss': 0.9689111351966858}
2025-01-12 22:10:15,897 [INFO] Step[2200/2713]: training loss : 0.9898238062858582 TRAIN  loss dict:  {'classification_loss': 0.9898238062858582}
2025-01-12 22:10:27,844 [INFO] Step[2250/2713]: training loss : 0.9766344439983368 TRAIN  loss dict:  {'classification_loss': 0.9766344439983368}
2025-01-12 22:10:39,787 [INFO] Step[2300/2713]: training loss : 0.9796108508110046 TRAIN  loss dict:  {'classification_loss': 0.9796108508110046}
2025-01-12 22:10:51,760 [INFO] Step[2350/2713]: training loss : 0.9733449864387512 TRAIN  loss dict:  {'classification_loss': 0.9733449864387512}
2025-01-12 22:11:03,723 [INFO] Step[2400/2713]: training loss : 0.9853784704208374 TRAIN  loss dict:  {'classification_loss': 0.9853784704208374}
2025-01-12 22:11:15,671 [INFO] Step[2450/2713]: training loss : 0.9935031366348267 TRAIN  loss dict:  {'classification_loss': 0.9935031366348267}
2025-01-12 22:11:27,633 [INFO] Step[2500/2713]: training loss : 0.9741865861415863 TRAIN  loss dict:  {'classification_loss': 0.9741865861415863}
2025-01-12 22:11:39,591 [INFO] Step[2550/2713]: training loss : 0.976725286245346 TRAIN  loss dict:  {'classification_loss': 0.976725286245346}
2025-01-12 22:11:51,523 [INFO] Step[2600/2713]: training loss : 0.991724054813385 TRAIN  loss dict:  {'classification_loss': 0.991724054813385}
2025-01-12 22:12:03,502 [INFO] Step[2650/2713]: training loss : 0.9750108051300049 TRAIN  loss dict:  {'classification_loss': 0.9750108051300049}
2025-01-12 22:12:15,458 [INFO] Step[2700/2713]: training loss : 0.9706694436073303 TRAIN  loss dict:  {'classification_loss': 0.9706694436073303}
2025-01-12 22:13:43,052 [INFO] Label accuracies statistics:
2025-01-12 22:13:43,052 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.25, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 0.5, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.5, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.25, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 0.75, 264: 1.0, 265: 0.75, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.5, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.5, 335: 1.0, 336: 0.75, 337: 0.5, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.25, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 0.75, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 22:13:43,054 [INFO] [35] TRAIN  loss: 0.9780055061284791 acc: 0.9981570217471434
2025-01-12 22:13:43,054 [INFO] [35] TRAIN  loss dict: {'classification_loss': 0.9780055061284791}
2025-01-12 22:13:43,054 [INFO] [35] VALIDATION loss: 1.804251714420498 VALIDATION acc: 0.8018808777429467
2025-01-12 22:13:43,054 [INFO] [35] VALIDATION loss dict: {'classification_loss': 1.804251714420498}
2025-01-12 22:13:43,054 [INFO] 
2025-01-12 22:14:00,860 [INFO] Step[50/2713]: training loss : 0.9876653122901916 TRAIN  loss dict:  {'classification_loss': 0.9876653122901916}
2025-01-12 22:14:12,762 [INFO] Step[100/2713]: training loss : 0.9754725587368012 TRAIN  loss dict:  {'classification_loss': 0.9754725587368012}
2025-01-12 22:14:24,670 [INFO] Step[150/2713]: training loss : 0.9742476236820221 TRAIN  loss dict:  {'classification_loss': 0.9742476236820221}
2025-01-12 22:14:36,572 [INFO] Step[200/2713]: training loss : 0.9770967555046082 TRAIN  loss dict:  {'classification_loss': 0.9770967555046082}
2025-01-12 22:14:48,525 [INFO] Step[250/2713]: training loss : 0.9739527916908264 TRAIN  loss dict:  {'classification_loss': 0.9739527916908264}
2025-01-12 22:15:00,456 [INFO] Step[300/2713]: training loss : 0.9748450899124146 TRAIN  loss dict:  {'classification_loss': 0.9748450899124146}
2025-01-12 22:15:12,404 [INFO] Step[350/2713]: training loss : 0.9663522136211395 TRAIN  loss dict:  {'classification_loss': 0.9663522136211395}
2025-01-12 22:15:24,357 [INFO] Step[400/2713]: training loss : 0.9794490885734558 TRAIN  loss dict:  {'classification_loss': 0.9794490885734558}
2025-01-12 22:15:36,293 [INFO] Step[450/2713]: training loss : 0.9707405269145966 TRAIN  loss dict:  {'classification_loss': 0.9707405269145966}
2025-01-12 22:15:48,194 [INFO] Step[500/2713]: training loss : 0.9763973844051361 TRAIN  loss dict:  {'classification_loss': 0.9763973844051361}
2025-01-12 22:16:00,175 [INFO] Step[550/2713]: training loss : 0.9722322404384613 TRAIN  loss dict:  {'classification_loss': 0.9722322404384613}
2025-01-12 22:16:12,073 [INFO] Step[600/2713]: training loss : 0.9693002033233643 TRAIN  loss dict:  {'classification_loss': 0.9693002033233643}
2025-01-12 22:16:24,022 [INFO] Step[650/2713]: training loss : 0.9682573330402374 TRAIN  loss dict:  {'classification_loss': 0.9682573330402374}
2025-01-12 22:16:35,955 [INFO] Step[700/2713]: training loss : 0.99869575381279 TRAIN  loss dict:  {'classification_loss': 0.99869575381279}
2025-01-12 22:16:47,890 [INFO] Step[750/2713]: training loss : 0.9793643605709076 TRAIN  loss dict:  {'classification_loss': 0.9793643605709076}
2025-01-12 22:16:59,847 [INFO] Step[800/2713]: training loss : 0.9695316588878632 TRAIN  loss dict:  {'classification_loss': 0.9695316588878632}
2025-01-12 22:17:11,739 [INFO] Step[850/2713]: training loss : 0.974280618429184 TRAIN  loss dict:  {'classification_loss': 0.974280618429184}
2025-01-12 22:17:23,652 [INFO] Step[900/2713]: training loss : 0.9796769988536834 TRAIN  loss dict:  {'classification_loss': 0.9796769988536834}
2025-01-12 22:17:35,565 [INFO] Step[950/2713]: training loss : 0.9724687945842743 TRAIN  loss dict:  {'classification_loss': 0.9724687945842743}
2025-01-12 22:17:47,483 [INFO] Step[1000/2713]: training loss : 0.9701935935020447 TRAIN  loss dict:  {'classification_loss': 0.9701935935020447}
2025-01-12 22:17:59,390 [INFO] Step[1050/2713]: training loss : 0.9866838884353638 TRAIN  loss dict:  {'classification_loss': 0.9866838884353638}
2025-01-12 22:18:11,296 [INFO] Step[1100/2713]: training loss : 0.9701102983951568 TRAIN  loss dict:  {'classification_loss': 0.9701102983951568}
2025-01-12 22:18:23,181 [INFO] Step[1150/2713]: training loss : 0.9768193781375885 TRAIN  loss dict:  {'classification_loss': 0.9768193781375885}
2025-01-12 22:18:35,062 [INFO] Step[1200/2713]: training loss : 0.9691197872161865 TRAIN  loss dict:  {'classification_loss': 0.9691197872161865}
2025-01-12 22:18:47,004 [INFO] Step[1250/2713]: training loss : 0.9873520982265472 TRAIN  loss dict:  {'classification_loss': 0.9873520982265472}
2025-01-12 22:18:58,861 [INFO] Step[1300/2713]: training loss : 0.9714463579654694 TRAIN  loss dict:  {'classification_loss': 0.9714463579654694}
2025-01-12 22:19:10,797 [INFO] Step[1350/2713]: training loss : 0.9700449585914612 TRAIN  loss dict:  {'classification_loss': 0.9700449585914612}
2025-01-12 22:19:22,668 [INFO] Step[1400/2713]: training loss : 0.9819913041591645 TRAIN  loss dict:  {'classification_loss': 0.9819913041591645}
2025-01-12 22:19:34,612 [INFO] Step[1450/2713]: training loss : 0.9795620679855347 TRAIN  loss dict:  {'classification_loss': 0.9795620679855347}
2025-01-12 22:19:46,515 [INFO] Step[1500/2713]: training loss : 0.9898967659473419 TRAIN  loss dict:  {'classification_loss': 0.9898967659473419}
2025-01-12 22:19:58,448 [INFO] Step[1550/2713]: training loss : 0.9710422086715699 TRAIN  loss dict:  {'classification_loss': 0.9710422086715699}
2025-01-12 22:20:10,342 [INFO] Step[1600/2713]: training loss : 0.9653286910057068 TRAIN  loss dict:  {'classification_loss': 0.9653286910057068}
2025-01-12 22:20:22,282 [INFO] Step[1650/2713]: training loss : 0.9800362825393677 TRAIN  loss dict:  {'classification_loss': 0.9800362825393677}
2025-01-12 22:20:34,168 [INFO] Step[1700/2713]: training loss : 0.9706552803516388 TRAIN  loss dict:  {'classification_loss': 0.9706552803516388}
2025-01-12 22:20:46,112 [INFO] Step[1750/2713]: training loss : 0.9810570847988128 TRAIN  loss dict:  {'classification_loss': 0.9810570847988128}
2025-01-12 22:20:58,053 [INFO] Step[1800/2713]: training loss : 0.977915278673172 TRAIN  loss dict:  {'classification_loss': 0.977915278673172}
2025-01-12 22:21:09,916 [INFO] Step[1850/2713]: training loss : 0.9709891629219055 TRAIN  loss dict:  {'classification_loss': 0.9709891629219055}
2025-01-12 22:21:21,776 [INFO] Step[1900/2713]: training loss : 0.9804878127574921 TRAIN  loss dict:  {'classification_loss': 0.9804878127574921}
2025-01-12 22:21:33,719 [INFO] Step[1950/2713]: training loss : 0.980364089012146 TRAIN  loss dict:  {'classification_loss': 0.980364089012146}
2025-01-12 22:21:45,630 [INFO] Step[2000/2713]: training loss : 0.9874300193786621 TRAIN  loss dict:  {'classification_loss': 0.9874300193786621}
2025-01-12 22:21:57,608 [INFO] Step[2050/2713]: training loss : 0.9785500228404999 TRAIN  loss dict:  {'classification_loss': 0.9785500228404999}
2025-01-12 22:22:09,525 [INFO] Step[2100/2713]: training loss : 0.9770216143131256 TRAIN  loss dict:  {'classification_loss': 0.9770216143131256}
2025-01-12 22:22:21,468 [INFO] Step[2150/2713]: training loss : 0.9638500452041626 TRAIN  loss dict:  {'classification_loss': 0.9638500452041626}
2025-01-12 22:22:33,372 [INFO] Step[2200/2713]: training loss : 0.9842400407791138 TRAIN  loss dict:  {'classification_loss': 0.9842400407791138}
2025-01-12 22:22:45,264 [INFO] Step[2250/2713]: training loss : 0.9688216614723205 TRAIN  loss dict:  {'classification_loss': 0.9688216614723205}
2025-01-12 22:22:57,219 [INFO] Step[2300/2713]: training loss : 0.9678419184684753 TRAIN  loss dict:  {'classification_loss': 0.9678419184684753}
2025-01-12 22:23:09,206 [INFO] Step[2350/2713]: training loss : 0.9701176130771637 TRAIN  loss dict:  {'classification_loss': 0.9701176130771637}
2025-01-12 22:23:21,595 [INFO] Step[2400/2713]: training loss : 0.974674048423767 TRAIN  loss dict:  {'classification_loss': 0.974674048423767}
2025-01-12 22:23:34,053 [INFO] Step[2450/2713]: training loss : 0.9844479429721832 TRAIN  loss dict:  {'classification_loss': 0.9844479429721832}
2025-01-12 22:23:46,311 [INFO] Step[2500/2713]: training loss : 0.993278478384018 TRAIN  loss dict:  {'classification_loss': 0.993278478384018}
2025-01-12 22:23:58,956 [INFO] Step[2550/2713]: training loss : 0.9671184408664704 TRAIN  loss dict:  {'classification_loss': 0.9671184408664704}
2025-01-12 22:24:11,393 [INFO] Step[2600/2713]: training loss : 0.9748051047325135 TRAIN  loss dict:  {'classification_loss': 0.9748051047325135}
2025-01-12 22:24:23,996 [INFO] Step[2650/2713]: training loss : 0.9673882687091827 TRAIN  loss dict:  {'classification_loss': 0.9673882687091827}
2025-01-12 22:24:37,044 [INFO] Step[2700/2713]: training loss : 0.9755648910999298 TRAIN  loss dict:  {'classification_loss': 0.9755648910999298}
2025-01-12 22:26:14,321 [INFO] Label accuracies statistics:
2025-01-12 22:26:14,321 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.25, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.25, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 1.0, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 0.75, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.25, 151: 0.75, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 0.75, 191: 0.25, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.25, 259: 1.0, 260: 0.5, 261: 0.25, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.5, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.25, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 0.75, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.5, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 22:26:14,323 [INFO] [36] TRAIN  loss: 0.9761173806636921 acc: 0.9990170782651432
2025-01-12 22:26:14,323 [INFO] [36] TRAIN  loss dict: {'classification_loss': 0.9761173806636921}
2025-01-12 22:26:14,323 [INFO] [36] VALIDATION loss: 1.734147678640552 VALIDATION acc: 0.8150470219435737
2025-01-12 22:26:14,323 [INFO] [36] VALIDATION loss dict: {'classification_loss': 1.734147678640552}
2025-01-12 22:26:14,323 [INFO] 
2025-01-12 22:26:55,190 [INFO] Step[50/2713]: training loss : 0.9712514162063599 TRAIN  loss dict:  {'classification_loss': 0.9712514162063599}
2025-01-12 22:27:07,085 [INFO] Step[100/2713]: training loss : 0.9752220094203949 TRAIN  loss dict:  {'classification_loss': 0.9752220094203949}
2025-01-12 22:27:19,034 [INFO] Step[150/2713]: training loss : 0.9799526810646058 TRAIN  loss dict:  {'classification_loss': 0.9799526810646058}
2025-01-12 22:27:30,957 [INFO] Step[200/2713]: training loss : 0.9839872229099274 TRAIN  loss dict:  {'classification_loss': 0.9839872229099274}
2025-01-12 22:27:42,914 [INFO] Step[250/2713]: training loss : 0.9669894790649414 TRAIN  loss dict:  {'classification_loss': 0.9669894790649414}
2025-01-12 22:27:54,859 [INFO] Step[300/2713]: training loss : 0.9714912617206574 TRAIN  loss dict:  {'classification_loss': 0.9714912617206574}
2025-01-12 22:28:06,858 [INFO] Step[350/2713]: training loss : 0.9735191166400909 TRAIN  loss dict:  {'classification_loss': 0.9735191166400909}
2025-01-12 22:28:18,824 [INFO] Step[400/2713]: training loss : 0.971606959104538 TRAIN  loss dict:  {'classification_loss': 0.971606959104538}
2025-01-12 22:28:30,820 [INFO] Step[450/2713]: training loss : 0.9784449160099029 TRAIN  loss dict:  {'classification_loss': 0.9784449160099029}
2025-01-12 22:28:42,805 [INFO] Step[500/2713]: training loss : 0.9640448355674743 TRAIN  loss dict:  {'classification_loss': 0.9640448355674743}
2025-01-12 22:28:54,762 [INFO] Step[550/2713]: training loss : 0.9723396265506744 TRAIN  loss dict:  {'classification_loss': 0.9723396265506744}
2025-01-12 22:29:06,675 [INFO] Step[600/2713]: training loss : 0.9707202708721161 TRAIN  loss dict:  {'classification_loss': 0.9707202708721161}
2025-01-12 22:29:18,625 [INFO] Step[650/2713]: training loss : 0.9604018461704255 TRAIN  loss dict:  {'classification_loss': 0.9604018461704255}
2025-01-12 22:29:30,594 [INFO] Step[700/2713]: training loss : 0.9732165932655334 TRAIN  loss dict:  {'classification_loss': 0.9732165932655334}
2025-01-12 22:29:42,517 [INFO] Step[750/2713]: training loss : 0.9689810097217559 TRAIN  loss dict:  {'classification_loss': 0.9689810097217559}
2025-01-12 22:29:54,471 [INFO] Step[800/2713]: training loss : 0.9746413922309876 TRAIN  loss dict:  {'classification_loss': 0.9746413922309876}
2025-01-12 22:30:06,435 [INFO] Step[850/2713]: training loss : 0.9782178997993469 TRAIN  loss dict:  {'classification_loss': 0.9782178997993469}
2025-01-12 22:30:18,397 [INFO] Step[900/2713]: training loss : 0.9734721767902375 TRAIN  loss dict:  {'classification_loss': 0.9734721767902375}
2025-01-12 22:30:30,341 [INFO] Step[950/2713]: training loss : 0.9856595647335052 TRAIN  loss dict:  {'classification_loss': 0.9856595647335052}
2025-01-12 22:30:42,298 [INFO] Step[1000/2713]: training loss : 0.9829146122932434 TRAIN  loss dict:  {'classification_loss': 0.9829146122932434}
2025-01-12 22:30:54,297 [INFO] Step[1050/2713]: training loss : 0.979413446187973 TRAIN  loss dict:  {'classification_loss': 0.979413446187973}
2025-01-12 22:31:06,232 [INFO] Step[1100/2713]: training loss : 0.9665423178672791 TRAIN  loss dict:  {'classification_loss': 0.9665423178672791}
2025-01-12 22:31:18,205 [INFO] Step[1150/2713]: training loss : 0.9679030537605285 TRAIN  loss dict:  {'classification_loss': 0.9679030537605285}
2025-01-12 22:31:30,231 [INFO] Step[1200/2713]: training loss : 0.9777371287345886 TRAIN  loss dict:  {'classification_loss': 0.9777371287345886}
2025-01-12 22:31:42,184 [INFO] Step[1250/2713]: training loss : 0.9678116416931153 TRAIN  loss dict:  {'classification_loss': 0.9678116416931153}
2025-01-12 22:31:54,148 [INFO] Step[1300/2713]: training loss : 0.982259271144867 TRAIN  loss dict:  {'classification_loss': 0.982259271144867}
2025-01-12 22:32:06,086 [INFO] Step[1350/2713]: training loss : 0.9711889910697937 TRAIN  loss dict:  {'classification_loss': 0.9711889910697937}
2025-01-12 22:32:18,033 [INFO] Step[1400/2713]: training loss : 0.9667944049835205 TRAIN  loss dict:  {'classification_loss': 0.9667944049835205}
2025-01-12 22:32:29,979 [INFO] Step[1450/2713]: training loss : 0.969321814775467 TRAIN  loss dict:  {'classification_loss': 0.969321814775467}
2025-01-12 22:32:41,930 [INFO] Step[1500/2713]: training loss : 0.9867537975311279 TRAIN  loss dict:  {'classification_loss': 0.9867537975311279}
2025-01-12 22:32:53,885 [INFO] Step[1550/2713]: training loss : 0.9678742325305939 TRAIN  loss dict:  {'classification_loss': 0.9678742325305939}
2025-01-12 22:33:05,808 [INFO] Step[1600/2713]: training loss : 0.9671152865886689 TRAIN  loss dict:  {'classification_loss': 0.9671152865886689}
2025-01-12 22:33:17,782 [INFO] Step[1650/2713]: training loss : 0.9683279860019683 TRAIN  loss dict:  {'classification_loss': 0.9683279860019683}
2025-01-12 22:33:29,752 [INFO] Step[1700/2713]: training loss : 0.9665740191936493 TRAIN  loss dict:  {'classification_loss': 0.9665740191936493}
2025-01-12 22:33:41,689 [INFO] Step[1750/2713]: training loss : 0.9682072222232818 TRAIN  loss dict:  {'classification_loss': 0.9682072222232818}
2025-01-12 22:33:53,606 [INFO] Step[1800/2713]: training loss : 0.9676926040649414 TRAIN  loss dict:  {'classification_loss': 0.9676926040649414}
2025-01-12 22:34:05,610 [INFO] Step[1850/2713]: training loss : 0.9807586622238159 TRAIN  loss dict:  {'classification_loss': 0.9807586622238159}
2025-01-12 22:34:17,561 [INFO] Step[1900/2713]: training loss : 0.9774924933910369 TRAIN  loss dict:  {'classification_loss': 0.9774924933910369}
2025-01-12 22:34:29,499 [INFO] Step[1950/2713]: training loss : 0.97825812458992 TRAIN  loss dict:  {'classification_loss': 0.97825812458992}
2025-01-12 22:34:41,468 [INFO] Step[2000/2713]: training loss : 0.9701816189289093 TRAIN  loss dict:  {'classification_loss': 0.9701816189289093}
2025-01-12 22:34:53,454 [INFO] Step[2050/2713]: training loss : 0.9648028993606568 TRAIN  loss dict:  {'classification_loss': 0.9648028993606568}
2025-01-12 22:35:05,405 [INFO] Step[2100/2713]: training loss : 0.9847501695156098 TRAIN  loss dict:  {'classification_loss': 0.9847501695156098}
2025-01-12 22:35:17,327 [INFO] Step[2150/2713]: training loss : 0.9833406841754914 TRAIN  loss dict:  {'classification_loss': 0.9833406841754914}
2025-01-12 22:35:29,259 [INFO] Step[2200/2713]: training loss : 0.9775128018856049 TRAIN  loss dict:  {'classification_loss': 0.9775128018856049}
2025-01-12 22:35:41,184 [INFO] Step[2250/2713]: training loss : 0.9795576465129853 TRAIN  loss dict:  {'classification_loss': 0.9795576465129853}
2025-01-12 22:35:53,108 [INFO] Step[2300/2713]: training loss : 0.9681524443626404 TRAIN  loss dict:  {'classification_loss': 0.9681524443626404}
2025-01-12 22:36:05,076 [INFO] Step[2350/2713]: training loss : 0.9727688360214234 TRAIN  loss dict:  {'classification_loss': 0.9727688360214234}
2025-01-12 22:36:17,039 [INFO] Step[2400/2713]: training loss : 0.9719505131244659 TRAIN  loss dict:  {'classification_loss': 0.9719505131244659}
2025-01-12 22:36:28,990 [INFO] Step[2450/2713]: training loss : 0.9677981841564178 TRAIN  loss dict:  {'classification_loss': 0.9677981841564178}
2025-01-12 22:36:40,957 [INFO] Step[2500/2713]: training loss : 1.0025960886478424 TRAIN  loss dict:  {'classification_loss': 1.0025960886478424}
2025-01-12 22:36:52,960 [INFO] Step[2550/2713]: training loss : 0.9711728870868683 TRAIN  loss dict:  {'classification_loss': 0.9711728870868683}
2025-01-12 22:37:04,896 [INFO] Step[2600/2713]: training loss : 0.9705586230754852 TRAIN  loss dict:  {'classification_loss': 0.9705586230754852}
2025-01-12 22:37:16,801 [INFO] Step[2650/2713]: training loss : 0.9860603547096253 TRAIN  loss dict:  {'classification_loss': 0.9860603547096253}
2025-01-12 22:37:28,799 [INFO] Step[2700/2713]: training loss : 0.9750003349781037 TRAIN  loss dict:  {'classification_loss': 0.9750003349781037}
2025-01-12 22:38:55,878 [INFO] Label accuracies statistics:
2025-01-12 22:38:55,878 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.5, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 1.0, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 1.0, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.5, 165: 0.75, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.5, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.0, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.0, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.5, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 0.75, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.5, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 22:38:55,880 [INFO] [37] TRAIN  loss: 0.974154393127208 acc: 0.9988942130482861
2025-01-12 22:38:55,880 [INFO] [37] TRAIN  loss dict: {'classification_loss': 0.974154393127208}
2025-01-12 22:38:55,880 [INFO] [37] VALIDATION loss: 1.7600895182082528 VALIDATION acc: 0.8119122257053292
2025-01-12 22:38:55,880 [INFO] [37] VALIDATION loss dict: {'classification_loss': 1.7600895182082528}
2025-01-12 22:38:55,881 [INFO] 
2025-01-12 22:39:13,068 [INFO] Step[50/2713]: training loss : 0.9642170572280884 TRAIN  loss dict:  {'classification_loss': 0.9642170572280884}
2025-01-12 22:39:24,985 [INFO] Step[100/2713]: training loss : 0.9614725863933563 TRAIN  loss dict:  {'classification_loss': 0.9614725863933563}
2025-01-12 22:39:36,902 [INFO] Step[150/2713]: training loss : 0.9767038309574128 TRAIN  loss dict:  {'classification_loss': 0.9767038309574128}
2025-01-12 22:39:48,859 [INFO] Step[200/2713]: training loss : 0.979942638874054 TRAIN  loss dict:  {'classification_loss': 0.979942638874054}
2025-01-12 22:40:00,856 [INFO] Step[250/2713]: training loss : 0.9612251651287079 TRAIN  loss dict:  {'classification_loss': 0.9612251651287079}
2025-01-12 22:40:12,780 [INFO] Step[300/2713]: training loss : 0.9733815610408783 TRAIN  loss dict:  {'classification_loss': 0.9733815610408783}
2025-01-12 22:40:24,742 [INFO] Step[350/2713]: training loss : 0.963945140838623 TRAIN  loss dict:  {'classification_loss': 0.963945140838623}
2025-01-12 22:40:36,701 [INFO] Step[400/2713]: training loss : 0.9830819916725159 TRAIN  loss dict:  {'classification_loss': 0.9830819916725159}
2025-01-12 22:40:48,625 [INFO] Step[450/2713]: training loss : 0.9673845219612122 TRAIN  loss dict:  {'classification_loss': 0.9673845219612122}
2025-01-12 22:41:00,569 [INFO] Step[500/2713]: training loss : 0.9773211669921875 TRAIN  loss dict:  {'classification_loss': 0.9773211669921875}
2025-01-12 22:41:12,591 [INFO] Step[550/2713]: training loss : 0.9771176850795746 TRAIN  loss dict:  {'classification_loss': 0.9771176850795746}
2025-01-12 22:41:24,570 [INFO] Step[600/2713]: training loss : 0.9683904600143433 TRAIN  loss dict:  {'classification_loss': 0.9683904600143433}
2025-01-12 22:41:36,535 [INFO] Step[650/2713]: training loss : 0.9731650257110596 TRAIN  loss dict:  {'classification_loss': 0.9731650257110596}
2025-01-12 22:41:48,559 [INFO] Step[700/2713]: training loss : 0.977592499256134 TRAIN  loss dict:  {'classification_loss': 0.977592499256134}
2025-01-12 22:42:00,517 [INFO] Step[750/2713]: training loss : 0.9726168882846832 TRAIN  loss dict:  {'classification_loss': 0.9726168882846832}
2025-01-12 22:42:12,827 [INFO] Step[800/2713]: training loss : 0.9678222846984863 TRAIN  loss dict:  {'classification_loss': 0.9678222846984863}
2025-01-12 22:42:25,301 [INFO] Step[850/2713]: training loss : 0.9935070788860321 TRAIN  loss dict:  {'classification_loss': 0.9935070788860321}
2025-01-12 22:42:37,736 [INFO] Step[900/2713]: training loss : 0.9715930926799774 TRAIN  loss dict:  {'classification_loss': 0.9715930926799774}
2025-01-12 22:42:50,103 [INFO] Step[950/2713]: training loss : 0.9806399691104889 TRAIN  loss dict:  {'classification_loss': 0.9806399691104889}
2025-01-12 22:43:02,800 [INFO] Step[1000/2713]: training loss : 0.9699153232574463 TRAIN  loss dict:  {'classification_loss': 0.9699153232574463}
2025-01-12 22:43:15,146 [INFO] Step[1050/2713]: training loss : 0.9793158090114593 TRAIN  loss dict:  {'classification_loss': 0.9793158090114593}
2025-01-12 22:43:28,041 [INFO] Step[1100/2713]: training loss : 0.9850650978088379 TRAIN  loss dict:  {'classification_loss': 0.9850650978088379}
2025-01-12 22:43:41,781 [INFO] Step[1150/2713]: training loss : 0.9864944767951965 TRAIN  loss dict:  {'classification_loss': 0.9864944767951965}
2025-01-12 22:43:55,000 [INFO] Step[1200/2713]: training loss : 0.9607114398479462 TRAIN  loss dict:  {'classification_loss': 0.9607114398479462}
2025-01-12 22:44:07,167 [INFO] Step[1250/2713]: training loss : 0.9675843036174774 TRAIN  loss dict:  {'classification_loss': 0.9675843036174774}
2025-01-12 22:44:19,168 [INFO] Step[1300/2713]: training loss : 0.9961051607131958 TRAIN  loss dict:  {'classification_loss': 0.9961051607131958}
2025-01-12 22:44:31,065 [INFO] Step[1350/2713]: training loss : 0.9878813898563386 TRAIN  loss dict:  {'classification_loss': 0.9878813898563386}
2025-01-12 22:44:42,990 [INFO] Step[1400/2713]: training loss : 0.9786081957817078 TRAIN  loss dict:  {'classification_loss': 0.9786081957817078}
2025-01-12 22:44:54,894 [INFO] Step[1450/2713]: training loss : 0.9756032729148865 TRAIN  loss dict:  {'classification_loss': 0.9756032729148865}
2025-01-12 22:45:06,793 [INFO] Step[1500/2713]: training loss : 0.9760146808624267 TRAIN  loss dict:  {'classification_loss': 0.9760146808624267}
2025-01-12 22:45:18,700 [INFO] Step[1550/2713]: training loss : 0.9864988958835602 TRAIN  loss dict:  {'classification_loss': 0.9864988958835602}
2025-01-12 22:45:30,645 [INFO] Step[1600/2713]: training loss : 0.9715173876285553 TRAIN  loss dict:  {'classification_loss': 0.9715173876285553}
2025-01-12 22:45:42,555 [INFO] Step[1650/2713]: training loss : 0.9803850102424622 TRAIN  loss dict:  {'classification_loss': 0.9803850102424622}
2025-01-12 22:45:54,478 [INFO] Step[1700/2713]: training loss : 0.9690691721439362 TRAIN  loss dict:  {'classification_loss': 0.9690691721439362}
2025-01-12 22:46:06,452 [INFO] Step[1750/2713]: training loss : 0.9664516222476959 TRAIN  loss dict:  {'classification_loss': 0.9664516222476959}
2025-01-12 22:46:18,414 [INFO] Step[1800/2713]: training loss : 0.9720541167259217 TRAIN  loss dict:  {'classification_loss': 0.9720541167259217}
2025-01-12 22:46:30,386 [INFO] Step[1850/2713]: training loss : 0.9667650437355042 TRAIN  loss dict:  {'classification_loss': 0.9667650437355042}
2025-01-12 22:46:42,355 [INFO] Step[1900/2713]: training loss : 0.981312643289566 TRAIN  loss dict:  {'classification_loss': 0.981312643289566}
2025-01-12 22:46:54,282 [INFO] Step[1950/2713]: training loss : 1.0092296504974365 TRAIN  loss dict:  {'classification_loss': 1.0092296504974365}
2025-01-12 22:47:06,211 [INFO] Step[2000/2713]: training loss : 0.9800702619552613 TRAIN  loss dict:  {'classification_loss': 0.9800702619552613}
2025-01-12 22:47:18,173 [INFO] Step[2050/2713]: training loss : 0.9760153996944427 TRAIN  loss dict:  {'classification_loss': 0.9760153996944427}
2025-01-12 22:47:30,124 [INFO] Step[2100/2713]: training loss : 0.9665994560718536 TRAIN  loss dict:  {'classification_loss': 0.9665994560718536}
2025-01-12 22:47:42,075 [INFO] Step[2150/2713]: training loss : 0.9723114144802093 TRAIN  loss dict:  {'classification_loss': 0.9723114144802093}
2025-01-12 22:47:53,997 [INFO] Step[2200/2713]: training loss : 0.9868233573436737 TRAIN  loss dict:  {'classification_loss': 0.9868233573436737}
2025-01-12 22:48:05,960 [INFO] Step[2250/2713]: training loss : 0.9795163178443909 TRAIN  loss dict:  {'classification_loss': 0.9795163178443909}
2025-01-12 22:48:17,927 [INFO] Step[2300/2713]: training loss : 0.9678915905952453 TRAIN  loss dict:  {'classification_loss': 0.9678915905952453}
2025-01-12 22:48:29,944 [INFO] Step[2350/2713]: training loss : 0.9655710113048553 TRAIN  loss dict:  {'classification_loss': 0.9655710113048553}
2025-01-12 22:48:41,876 [INFO] Step[2400/2713]: training loss : 0.9699194145202636 TRAIN  loss dict:  {'classification_loss': 0.9699194145202636}
2025-01-12 22:48:53,885 [INFO] Step[2450/2713]: training loss : 0.9771406626701356 TRAIN  loss dict:  {'classification_loss': 0.9771406626701356}
2025-01-12 22:49:05,832 [INFO] Step[2500/2713]: training loss : 0.9778718078136444 TRAIN  loss dict:  {'classification_loss': 0.9778718078136444}
2025-01-12 22:49:17,823 [INFO] Step[2550/2713]: training loss : 0.9728813743591309 TRAIN  loss dict:  {'classification_loss': 0.9728813743591309}
2025-01-12 22:49:29,781 [INFO] Step[2600/2713]: training loss : 0.9736550724506379 TRAIN  loss dict:  {'classification_loss': 0.9736550724506379}
2025-01-12 22:49:41,757 [INFO] Step[2650/2713]: training loss : 0.9693972611427307 TRAIN  loss dict:  {'classification_loss': 0.9693972611427307}
2025-01-12 22:49:53,704 [INFO] Step[2700/2713]: training loss : 0.9644831693172455 TRAIN  loss dict:  {'classification_loss': 0.9644831693172455}
2025-01-12 22:51:21,356 [INFO] Label accuracies statistics:
2025-01-12 22:51:21,356 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 0.75, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 1.0, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.25, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.5, 208: 0.5, 209: 0.75, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 1.0, 277: 0.75, 278: 1.0, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 1.0, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.25, 396: 1.0, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-12 22:51:23,223 [INFO] [38] TRAIN  loss: 0.9753118474194364 acc: 0.9986484826145718
2025-01-12 22:51:23,223 [INFO] [38] TRAIN  loss dict: {'classification_loss': 0.9753118474194364}
2025-01-12 22:51:23,223 [INFO] [38] VALIDATION loss: 1.6864031287736463 VALIDATION acc: 0.831974921630094
2025-01-12 22:51:23,223 [INFO] [38] VALIDATION loss dict: {'classification_loss': 1.6864031287736463}
2025-01-12 22:51:23,223 [INFO] 
2025-01-12 22:51:40,980 [INFO] Step[50/2713]: training loss : 0.9715862989425659 TRAIN  loss dict:  {'classification_loss': 0.9715862989425659}
2025-01-12 22:51:52,843 [INFO] Step[100/2713]: training loss : 0.9829265427589416 TRAIN  loss dict:  {'classification_loss': 0.9829265427589416}
2025-01-12 22:52:04,749 [INFO] Step[150/2713]: training loss : 0.9690859496593476 TRAIN  loss dict:  {'classification_loss': 0.9690859496593476}
2025-01-12 22:52:16,695 [INFO] Step[200/2713]: training loss : 0.9682476985454559 TRAIN  loss dict:  {'classification_loss': 0.9682476985454559}
2025-01-12 22:52:28,617 [INFO] Step[250/2713]: training loss : 0.9874094009399415 TRAIN  loss dict:  {'classification_loss': 0.9874094009399415}
2025-01-12 22:52:40,491 [INFO] Step[300/2713]: training loss : 0.9598557710647583 TRAIN  loss dict:  {'classification_loss': 0.9598557710647583}
2025-01-12 22:52:52,400 [INFO] Step[350/2713]: training loss : 0.9965746927261353 TRAIN  loss dict:  {'classification_loss': 0.9965746927261353}
2025-01-12 22:53:04,314 [INFO] Step[400/2713]: training loss : 0.9676181137561798 TRAIN  loss dict:  {'classification_loss': 0.9676181137561798}
2025-01-12 22:53:16,240 [INFO] Step[450/2713]: training loss : 0.972535959482193 TRAIN  loss dict:  {'classification_loss': 0.972535959482193}
2025-01-12 22:53:28,158 [INFO] Step[500/2713]: training loss : 0.9641298198699951 TRAIN  loss dict:  {'classification_loss': 0.9641298198699951}
2025-01-12 22:53:40,086 [INFO] Step[550/2713]: training loss : 0.9719945764541627 TRAIN  loss dict:  {'classification_loss': 0.9719945764541627}
2025-01-12 22:53:51,983 [INFO] Step[600/2713]: training loss : 0.9772819268703461 TRAIN  loss dict:  {'classification_loss': 0.9772819268703461}
2025-01-12 22:54:03,940 [INFO] Step[650/2713]: training loss : 0.9673578095436096 TRAIN  loss dict:  {'classification_loss': 0.9673578095436096}
2025-01-12 22:54:15,855 [INFO] Step[700/2713]: training loss : 0.9854270541667938 TRAIN  loss dict:  {'classification_loss': 0.9854270541667938}
2025-01-12 22:54:27,749 [INFO] Step[750/2713]: training loss : 0.9586943626403809 TRAIN  loss dict:  {'classification_loss': 0.9586943626403809}
2025-01-12 22:54:39,685 [INFO] Step[800/2713]: training loss : 0.9877935671806335 TRAIN  loss dict:  {'classification_loss': 0.9877935671806335}
2025-01-12 22:54:51,606 [INFO] Step[850/2713]: training loss : 0.9726732397079467 TRAIN  loss dict:  {'classification_loss': 0.9726732397079467}
2025-01-12 22:55:03,523 [INFO] Step[900/2713]: training loss : 0.9726577687263489 TRAIN  loss dict:  {'classification_loss': 0.9726577687263489}
2025-01-12 22:55:15,486 [INFO] Step[950/2713]: training loss : 0.9713263916969299 TRAIN  loss dict:  {'classification_loss': 0.9713263916969299}
2025-01-12 22:55:27,395 [INFO] Step[1000/2713]: training loss : 0.969747531414032 TRAIN  loss dict:  {'classification_loss': 0.969747531414032}
2025-01-12 22:55:39,312 [INFO] Step[1050/2713]: training loss : 0.9737436020374298 TRAIN  loss dict:  {'classification_loss': 0.9737436020374298}
2025-01-12 22:55:51,241 [INFO] Step[1100/2713]: training loss : 0.9810322725772858 TRAIN  loss dict:  {'classification_loss': 0.9810322725772858}
2025-01-12 22:56:03,172 [INFO] Step[1150/2713]: training loss : 0.9716995406150818 TRAIN  loss dict:  {'classification_loss': 0.9716995406150818}
2025-01-12 22:56:15,069 [INFO] Step[1200/2713]: training loss : 0.9683176112174988 TRAIN  loss dict:  {'classification_loss': 0.9683176112174988}
2025-01-12 22:56:27,027 [INFO] Step[1250/2713]: training loss : 1.0078046548366546 TRAIN  loss dict:  {'classification_loss': 1.0078046548366546}
2025-01-12 22:56:38,933 [INFO] Step[1300/2713]: training loss : 0.9605783760547638 TRAIN  loss dict:  {'classification_loss': 0.9605783760547638}
2025-01-12 22:56:50,828 [INFO] Step[1350/2713]: training loss : 0.981520745754242 TRAIN  loss dict:  {'classification_loss': 0.981520745754242}
2025-01-12 22:57:02,711 [INFO] Step[1400/2713]: training loss : 0.9712417244911193 TRAIN  loss dict:  {'classification_loss': 0.9712417244911193}
2025-01-12 22:57:14,630 [INFO] Step[1450/2713]: training loss : 0.9803482007980346 TRAIN  loss dict:  {'classification_loss': 0.9803482007980346}
2025-01-12 22:57:26,551 [INFO] Step[1500/2713]: training loss : 0.9751509916782379 TRAIN  loss dict:  {'classification_loss': 0.9751509916782379}
2025-01-12 22:57:38,506 [INFO] Step[1550/2713]: training loss : 0.9682529985904693 TRAIN  loss dict:  {'classification_loss': 0.9682529985904693}
2025-01-12 22:57:50,418 [INFO] Step[1600/2713]: training loss : 0.9704155147075653 TRAIN  loss dict:  {'classification_loss': 0.9704155147075653}
2025-01-12 22:58:02,325 [INFO] Step[1650/2713]: training loss : 0.9760763263702392 TRAIN  loss dict:  {'classification_loss': 0.9760763263702392}
2025-01-12 22:58:14,246 [INFO] Step[1700/2713]: training loss : 0.9699532270431519 TRAIN  loss dict:  {'classification_loss': 0.9699532270431519}
2025-01-12 22:58:26,184 [INFO] Step[1750/2713]: training loss : 0.9685325539112091 TRAIN  loss dict:  {'classification_loss': 0.9685325539112091}
2025-01-12 22:58:38,108 [INFO] Step[1800/2713]: training loss : 0.9708762764930725 TRAIN  loss dict:  {'classification_loss': 0.9708762764930725}
2025-01-12 22:58:50,016 [INFO] Step[1850/2713]: training loss : 0.966468082666397 TRAIN  loss dict:  {'classification_loss': 0.966468082666397}
2025-01-12 22:59:01,952 [INFO] Step[1900/2713]: training loss : 0.9941685497760773 TRAIN  loss dict:  {'classification_loss': 0.9941685497760773}
2025-01-12 22:59:13,887 [INFO] Step[1950/2713]: training loss : 0.9671195733547211 TRAIN  loss dict:  {'classification_loss': 0.9671195733547211}
2025-01-12 22:59:25,800 [INFO] Step[2000/2713]: training loss : 0.9748560309410095 TRAIN  loss dict:  {'classification_loss': 0.9748560309410095}
2025-01-12 22:59:37,706 [INFO] Step[2050/2713]: training loss : 0.9641540741920471 TRAIN  loss dict:  {'classification_loss': 0.9641540741920471}
2025-01-12 22:59:49,584 [INFO] Step[2100/2713]: training loss : 0.9811650598049164 TRAIN  loss dict:  {'classification_loss': 0.9811650598049164}
2025-01-12 23:00:01,504 [INFO] Step[2150/2713]: training loss : 0.9874128806591034 TRAIN  loss dict:  {'classification_loss': 0.9874128806591034}
2025-01-12 23:00:13,436 [INFO] Step[2200/2713]: training loss : 0.9710276675224304 TRAIN  loss dict:  {'classification_loss': 0.9710276675224304}
2025-01-12 23:00:25,326 [INFO] Step[2250/2713]: training loss : 0.9687362158298493 TRAIN  loss dict:  {'classification_loss': 0.9687362158298493}
2025-01-12 23:00:37,280 [INFO] Step[2300/2713]: training loss : 0.9721589958667756 TRAIN  loss dict:  {'classification_loss': 0.9721589958667756}
2025-01-12 23:00:49,171 [INFO] Step[2350/2713]: training loss : 0.9774600744247437 TRAIN  loss dict:  {'classification_loss': 0.9774600744247437}
2025-01-12 23:01:01,140 [INFO] Step[2400/2713]: training loss : 0.9718756592273712 TRAIN  loss dict:  {'classification_loss': 0.9718756592273712}
2025-01-12 23:01:13,535 [INFO] Step[2450/2713]: training loss : 0.9877725207805633 TRAIN  loss dict:  {'classification_loss': 0.9877725207805633}
2025-01-12 23:01:25,998 [INFO] Step[2500/2713]: training loss : 0.9749033236503601 TRAIN  loss dict:  {'classification_loss': 0.9749033236503601}
2025-01-12 23:01:38,272 [INFO] Step[2550/2713]: training loss : 0.9843354439735412 TRAIN  loss dict:  {'classification_loss': 0.9843354439735412}
2025-01-12 23:01:50,892 [INFO] Step[2600/2713]: training loss : 0.9621530914306641 TRAIN  loss dict:  {'classification_loss': 0.9621530914306641}
2025-01-12 23:02:03,475 [INFO] Step[2650/2713]: training loss : 0.9657540798187256 TRAIN  loss dict:  {'classification_loss': 0.9657540798187256}
2025-01-12 23:02:16,139 [INFO] Step[2700/2713]: training loss : 0.9715286684036255 TRAIN  loss dict:  {'classification_loss': 0.9715286684036255}
2025-01-12 23:03:57,833 [INFO] Label accuracies statistics:
2025-01-12 23:03:57,834 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.5, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.5, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.5, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.5, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.5, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.5, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 0.75, 395: 0.25, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 23:03:57,836 [INFO] [39] TRAIN  loss: 0.9742786240806186 acc: 0.9982798869640005
2025-01-12 23:03:57,836 [INFO] [39] TRAIN  loss dict: {'classification_loss': 0.9742786240806186}
2025-01-12 23:03:57,836 [INFO] [39] VALIDATION loss: 1.6903131833874194 VALIDATION acc: 0.8257053291536051
2025-01-12 23:03:57,836 [INFO] [39] VALIDATION loss dict: {'classification_loss': 1.6903131833874194}
2025-01-12 23:03:57,836 [INFO] 
2025-01-12 23:04:15,176 [INFO] Step[50/2713]: training loss : 0.9910516691207886 TRAIN  loss dict:  {'classification_loss': 0.9910516691207886}
2025-01-12 23:04:27,101 [INFO] Step[100/2713]: training loss : 0.9715239262580871 TRAIN  loss dict:  {'classification_loss': 0.9715239262580871}
2025-01-12 23:04:39,038 [INFO] Step[150/2713]: training loss : 0.9850160098075866 TRAIN  loss dict:  {'classification_loss': 0.9850160098075866}
2025-01-12 23:04:50,953 [INFO] Step[200/2713]: training loss : 0.9623907816410064 TRAIN  loss dict:  {'classification_loss': 0.9623907816410064}
2025-01-12 23:05:02,937 [INFO] Step[250/2713]: training loss : 0.9861878943443299 TRAIN  loss dict:  {'classification_loss': 0.9861878943443299}
2025-01-12 23:05:14,869 [INFO] Step[300/2713]: training loss : 0.966322796344757 TRAIN  loss dict:  {'classification_loss': 0.966322796344757}
2025-01-12 23:05:26,793 [INFO] Step[350/2713]: training loss : 0.9697969448566437 TRAIN  loss dict:  {'classification_loss': 0.9697969448566437}
2025-01-12 23:05:38,758 [INFO] Step[400/2713]: training loss : 0.9707194662094116 TRAIN  loss dict:  {'classification_loss': 0.9707194662094116}
2025-01-12 23:05:50,699 [INFO] Step[450/2713]: training loss : 0.9685643219947815 TRAIN  loss dict:  {'classification_loss': 0.9685643219947815}
2025-01-12 23:06:02,599 [INFO] Step[500/2713]: training loss : 0.966147209405899 TRAIN  loss dict:  {'classification_loss': 0.966147209405899}
2025-01-12 23:06:14,497 [INFO] Step[550/2713]: training loss : 0.9650375688076019 TRAIN  loss dict:  {'classification_loss': 0.9650375688076019}
2025-01-12 23:06:26,374 [INFO] Step[600/2713]: training loss : 0.9791826748847962 TRAIN  loss dict:  {'classification_loss': 0.9791826748847962}
2025-01-12 23:06:38,284 [INFO] Step[650/2713]: training loss : 0.9718853664398194 TRAIN  loss dict:  {'classification_loss': 0.9718853664398194}
2025-01-12 23:06:50,195 [INFO] Step[700/2713]: training loss : 0.9692928075790406 TRAIN  loss dict:  {'classification_loss': 0.9692928075790406}
2025-01-12 23:07:02,123 [INFO] Step[750/2713]: training loss : 0.9687651407718658 TRAIN  loss dict:  {'classification_loss': 0.9687651407718658}
2025-01-12 23:07:14,063 [INFO] Step[800/2713]: training loss : 0.9803876721858978 TRAIN  loss dict:  {'classification_loss': 0.9803876721858978}
2025-01-12 23:07:26,019 [INFO] Step[850/2713]: training loss : 0.9776869142055511 TRAIN  loss dict:  {'classification_loss': 0.9776869142055511}
2025-01-12 23:07:37,911 [INFO] Step[900/2713]: training loss : 0.9749977385997772 TRAIN  loss dict:  {'classification_loss': 0.9749977385997772}
2025-01-12 23:07:49,864 [INFO] Step[950/2713]: training loss : 0.9717000472545624 TRAIN  loss dict:  {'classification_loss': 0.9717000472545624}
2025-01-12 23:08:01,739 [INFO] Step[1000/2713]: training loss : 0.9656227791309356 TRAIN  loss dict:  {'classification_loss': 0.9656227791309356}
2025-01-12 23:08:13,672 [INFO] Step[1050/2713]: training loss : 0.9797000312805175 TRAIN  loss dict:  {'classification_loss': 0.9797000312805175}
2025-01-12 23:08:25,553 [INFO] Step[1100/2713]: training loss : 0.9785021007061004 TRAIN  loss dict:  {'classification_loss': 0.9785021007061004}
2025-01-12 23:08:37,516 [INFO] Step[1150/2713]: training loss : 0.9730088710784912 TRAIN  loss dict:  {'classification_loss': 0.9730088710784912}
2025-01-12 23:08:49,438 [INFO] Step[1200/2713]: training loss : 0.973020408153534 TRAIN  loss dict:  {'classification_loss': 0.973020408153534}
2025-01-12 23:09:01,349 [INFO] Step[1250/2713]: training loss : 0.9731363594532013 TRAIN  loss dict:  {'classification_loss': 0.9731363594532013}
2025-01-12 23:09:13,233 [INFO] Step[1300/2713]: training loss : 0.9740449380874634 TRAIN  loss dict:  {'classification_loss': 0.9740449380874634}
2025-01-12 23:09:25,150 [INFO] Step[1350/2713]: training loss : 0.9646211659908295 TRAIN  loss dict:  {'classification_loss': 0.9646211659908295}
2025-01-12 23:09:37,068 [INFO] Step[1400/2713]: training loss : 0.969746652841568 TRAIN  loss dict:  {'classification_loss': 0.969746652841568}
2025-01-12 23:09:48,970 [INFO] Step[1450/2713]: training loss : 0.9764110803604126 TRAIN  loss dict:  {'classification_loss': 0.9764110803604126}
2025-01-12 23:10:00,948 [INFO] Step[1500/2713]: training loss : 0.9725135922431946 TRAIN  loss dict:  {'classification_loss': 0.9725135922431946}
2025-01-12 23:10:12,881 [INFO] Step[1550/2713]: training loss : 0.965209299325943 TRAIN  loss dict:  {'classification_loss': 0.965209299325943}
2025-01-12 23:10:24,790 [INFO] Step[1600/2713]: training loss : 0.9748743212223053 TRAIN  loss dict:  {'classification_loss': 0.9748743212223053}
2025-01-12 23:10:36,702 [INFO] Step[1650/2713]: training loss : 0.9723073613643646 TRAIN  loss dict:  {'classification_loss': 0.9723073613643646}
2025-01-12 23:10:48,609 [INFO] Step[1700/2713]: training loss : 0.9740586793422699 TRAIN  loss dict:  {'classification_loss': 0.9740586793422699}
2025-01-12 23:11:00,521 [INFO] Step[1750/2713]: training loss : 0.9597718513011932 TRAIN  loss dict:  {'classification_loss': 0.9597718513011932}
2025-01-12 23:11:12,431 [INFO] Step[1800/2713]: training loss : 0.9704441273212433 TRAIN  loss dict:  {'classification_loss': 0.9704441273212433}
2025-01-12 23:11:24,305 [INFO] Step[1850/2713]: training loss : 0.9732825529575347 TRAIN  loss dict:  {'classification_loss': 0.9732825529575347}
2025-01-12 23:11:36,228 [INFO] Step[1900/2713]: training loss : 0.9868704462051392 TRAIN  loss dict:  {'classification_loss': 0.9868704462051392}
2025-01-12 23:11:48,181 [INFO] Step[1950/2713]: training loss : 0.9713120996952057 TRAIN  loss dict:  {'classification_loss': 0.9713120996952057}
2025-01-12 23:12:00,103 [INFO] Step[2000/2713]: training loss : 0.9646794581413269 TRAIN  loss dict:  {'classification_loss': 0.9646794581413269}
2025-01-12 23:12:12,057 [INFO] Step[2050/2713]: training loss : 0.971688700914383 TRAIN  loss dict:  {'classification_loss': 0.971688700914383}
2025-01-12 23:12:24,012 [INFO] Step[2100/2713]: training loss : 0.9749878346920013 TRAIN  loss dict:  {'classification_loss': 0.9749878346920013}
2025-01-12 23:12:35,916 [INFO] Step[2150/2713]: training loss : 0.9662718951702118 TRAIN  loss dict:  {'classification_loss': 0.9662718951702118}
2025-01-12 23:12:47,802 [INFO] Step[2200/2713]: training loss : 0.9782644057273865 TRAIN  loss dict:  {'classification_loss': 0.9782644057273865}
2025-01-12 23:12:59,713 [INFO] Step[2250/2713]: training loss : 0.9656528651714325 TRAIN  loss dict:  {'classification_loss': 0.9656528651714325}
2025-01-12 23:13:11,622 [INFO] Step[2300/2713]: training loss : 0.9777012372016907 TRAIN  loss dict:  {'classification_loss': 0.9777012372016907}
2025-01-12 23:13:23,524 [INFO] Step[2350/2713]: training loss : 0.963564373254776 TRAIN  loss dict:  {'classification_loss': 0.963564373254776}
2025-01-12 23:13:35,441 [INFO] Step[2400/2713]: training loss : 0.9716018307209014 TRAIN  loss dict:  {'classification_loss': 0.9716018307209014}
2025-01-12 23:13:47,350 [INFO] Step[2450/2713]: training loss : 0.9752313065528869 TRAIN  loss dict:  {'classification_loss': 0.9752313065528869}
2025-01-12 23:13:59,304 [INFO] Step[2500/2713]: training loss : 0.9751251912117005 TRAIN  loss dict:  {'classification_loss': 0.9751251912117005}
2025-01-12 23:14:11,234 [INFO] Step[2550/2713]: training loss : 0.9781828331947326 TRAIN  loss dict:  {'classification_loss': 0.9781828331947326}
2025-01-12 23:14:23,117 [INFO] Step[2600/2713]: training loss : 0.9774461627006531 TRAIN  loss dict:  {'classification_loss': 0.9774461627006531}
2025-01-12 23:14:35,023 [INFO] Step[2650/2713]: training loss : 0.9658990705013275 TRAIN  loss dict:  {'classification_loss': 0.9658990705013275}
2025-01-12 23:14:46,946 [INFO] Step[2700/2713]: training loss : 0.9659743189811707 TRAIN  loss dict:  {'classification_loss': 0.9659743189811707}
2025-01-12 23:16:13,136 [INFO] Label accuracies statistics:
2025-01-12 23:16:13,137 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 1.0, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 1.0, 52: 1.0, 53: 1.0, 54: 0.25, 55: 1.0, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.5, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.75, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 0.75, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.5, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 0.75, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 23:16:15,165 [INFO] [40] TRAIN  loss: 0.9724752873764502 acc: 0.9995085391325715
2025-01-12 23:16:15,166 [INFO] [40] TRAIN  loss dict: {'classification_loss': 0.9724752873764502}
2025-01-12 23:16:15,166 [INFO] [40] VALIDATION loss: 1.6535997083760743 VALIDATION acc: 0.8326018808777429
2025-01-12 23:16:15,166 [INFO] [40] VALIDATION loss dict: {'classification_loss': 1.6535997083760743}
2025-01-12 23:16:15,166 [INFO] 
2025-01-12 23:16:32,858 [INFO] Step[50/2713]: training loss : 0.9630996823310852 TRAIN  loss dict:  {'classification_loss': 0.9630996823310852}
2025-01-12 23:16:44,812 [INFO] Step[100/2713]: training loss : 0.9635670948028564 TRAIN  loss dict:  {'classification_loss': 0.9635670948028564}
2025-01-12 23:16:56,863 [INFO] Step[150/2713]: training loss : 0.9769846284389496 TRAIN  loss dict:  {'classification_loss': 0.9769846284389496}
2025-01-12 23:17:08,788 [INFO] Step[200/2713]: training loss : 0.96447549700737 TRAIN  loss dict:  {'classification_loss': 0.96447549700737}
2025-01-12 23:17:20,724 [INFO] Step[250/2713]: training loss : 0.9722354912757873 TRAIN  loss dict:  {'classification_loss': 0.9722354912757873}
2025-01-12 23:17:32,676 [INFO] Step[300/2713]: training loss : 0.9689140474796295 TRAIN  loss dict:  {'classification_loss': 0.9689140474796295}
2025-01-12 23:17:44,619 [INFO] Step[350/2713]: training loss : 0.9710312962532044 TRAIN  loss dict:  {'classification_loss': 0.9710312962532044}
2025-01-12 23:17:56,500 [INFO] Step[400/2713]: training loss : 0.9762153601646424 TRAIN  loss dict:  {'classification_loss': 0.9762153601646424}
2025-01-12 23:18:08,408 [INFO] Step[450/2713]: training loss : 0.9654001832008362 TRAIN  loss dict:  {'classification_loss': 0.9654001832008362}
2025-01-12 23:18:20,321 [INFO] Step[500/2713]: training loss : 0.9833169746398925 TRAIN  loss dict:  {'classification_loss': 0.9833169746398925}
2025-01-12 23:18:32,225 [INFO] Step[550/2713]: training loss : 1.0031438159942627 TRAIN  loss dict:  {'classification_loss': 1.0031438159942627}
2025-01-12 23:18:44,168 [INFO] Step[600/2713]: training loss : 0.9692439413070679 TRAIN  loss dict:  {'classification_loss': 0.9692439413070679}
2025-01-12 23:18:56,086 [INFO] Step[650/2713]: training loss : 0.9635288619995117 TRAIN  loss dict:  {'classification_loss': 0.9635288619995117}
2025-01-12 23:19:08,037 [INFO] Step[700/2713]: training loss : 0.9592167103290558 TRAIN  loss dict:  {'classification_loss': 0.9592167103290558}
2025-01-12 23:19:19,866 [INFO] Step[750/2713]: training loss : 0.9696862745285034 TRAIN  loss dict:  {'classification_loss': 0.9696862745285034}
2025-01-12 23:19:31,801 [INFO] Step[800/2713]: training loss : 0.9598441779613495 TRAIN  loss dict:  {'classification_loss': 0.9598441779613495}
2025-01-12 23:19:43,706 [INFO] Step[850/2713]: training loss : 0.9666847229003906 TRAIN  loss dict:  {'classification_loss': 0.9666847229003906}
2025-01-12 23:19:55,613 [INFO] Step[900/2713]: training loss : 0.9645026171207428 TRAIN  loss dict:  {'classification_loss': 0.9645026171207428}
2025-01-12 23:20:07,769 [INFO] Step[950/2713]: training loss : 0.9674793457984925 TRAIN  loss dict:  {'classification_loss': 0.9674793457984925}
2025-01-12 23:20:20,221 [INFO] Step[1000/2713]: training loss : 0.9782917165756225 TRAIN  loss dict:  {'classification_loss': 0.9782917165756225}
2025-01-12 23:20:32,640 [INFO] Step[1050/2713]: training loss : 0.9656531679630279 TRAIN  loss dict:  {'classification_loss': 0.9656531679630279}
2025-01-12 23:20:44,949 [INFO] Step[1100/2713]: training loss : 0.9616109299659729 TRAIN  loss dict:  {'classification_loss': 0.9616109299659729}
2025-01-12 23:20:57,620 [INFO] Step[1150/2713]: training loss : 0.9726856160163879 TRAIN  loss dict:  {'classification_loss': 0.9726856160163879}
2025-01-12 23:21:09,881 [INFO] Step[1200/2713]: training loss : 0.9670508563518524 TRAIN  loss dict:  {'classification_loss': 0.9670508563518524}
2025-01-12 23:21:22,677 [INFO] Step[1250/2713]: training loss : 0.9696229755878448 TRAIN  loss dict:  {'classification_loss': 0.9696229755878448}
2025-01-12 23:21:36,103 [INFO] Step[1300/2713]: training loss : 0.9749452245235443 TRAIN  loss dict:  {'classification_loss': 0.9749452245235443}
2025-01-12 23:21:49,485 [INFO] Step[1350/2713]: training loss : 0.9627069759368897 TRAIN  loss dict:  {'classification_loss': 0.9627069759368897}
2025-01-12 23:22:01,644 [INFO] Step[1400/2713]: training loss : 0.9669317626953124 TRAIN  loss dict:  {'classification_loss': 0.9669317626953124}
2025-01-12 23:22:13,602 [INFO] Step[1450/2713]: training loss : 0.9638751184940338 TRAIN  loss dict:  {'classification_loss': 0.9638751184940338}
2025-01-12 23:22:25,544 [INFO] Step[1500/2713]: training loss : 0.9643862736225128 TRAIN  loss dict:  {'classification_loss': 0.9643862736225128}
2025-01-12 23:22:37,261 [INFO] Step[1550/2713]: training loss : 0.9633925127983093 TRAIN  loss dict:  {'classification_loss': 0.9633925127983093}
2025-01-12 23:22:49,092 [INFO] Step[1600/2713]: training loss : 0.9694049179553985 TRAIN  loss dict:  {'classification_loss': 0.9694049179553985}
2025-01-12 23:23:01,020 [INFO] Step[1650/2713]: training loss : 0.9651953995227813 TRAIN  loss dict:  {'classification_loss': 0.9651953995227813}
2025-01-12 23:23:13,022 [INFO] Step[1700/2713]: training loss : 0.96761479139328 TRAIN  loss dict:  {'classification_loss': 0.96761479139328}
2025-01-12 23:23:24,941 [INFO] Step[1750/2713]: training loss : 0.981557000875473 TRAIN  loss dict:  {'classification_loss': 0.981557000875473}
2025-01-12 23:23:36,935 [INFO] Step[1800/2713]: training loss : 0.9592776370048522 TRAIN  loss dict:  {'classification_loss': 0.9592776370048522}
2025-01-12 23:23:48,853 [INFO] Step[1850/2713]: training loss : 0.9594369053840637 TRAIN  loss dict:  {'classification_loss': 0.9594369053840637}
2025-01-12 23:24:00,778 [INFO] Step[1900/2713]: training loss : 0.9635653293132782 TRAIN  loss dict:  {'classification_loss': 0.9635653293132782}
2025-01-12 23:24:12,700 [INFO] Step[1950/2713]: training loss : 0.9621414542198181 TRAIN  loss dict:  {'classification_loss': 0.9621414542198181}
2025-01-12 23:24:24,696 [INFO] Step[2000/2713]: training loss : 0.9581572985649109 TRAIN  loss dict:  {'classification_loss': 0.9581572985649109}
2025-01-12 23:24:36,703 [INFO] Step[2050/2713]: training loss : 0.9665405035018921 TRAIN  loss dict:  {'classification_loss': 0.9665405035018921}
2025-01-12 23:24:48,816 [INFO] Step[2100/2713]: training loss : 0.9702524101734161 TRAIN  loss dict:  {'classification_loss': 0.9702524101734161}
2025-01-12 23:25:00,796 [INFO] Step[2150/2713]: training loss : 0.9637829732894897 TRAIN  loss dict:  {'classification_loss': 0.9637829732894897}
2025-01-12 23:25:12,783 [INFO] Step[2200/2713]: training loss : 0.9756099438667297 TRAIN  loss dict:  {'classification_loss': 0.9756099438667297}
2025-01-12 23:25:24,796 [INFO] Step[2250/2713]: training loss : 0.9701934933662415 TRAIN  loss dict:  {'classification_loss': 0.9701934933662415}
2025-01-12 23:25:36,801 [INFO] Step[2300/2713]: training loss : 0.9737569332122803 TRAIN  loss dict:  {'classification_loss': 0.9737569332122803}
2025-01-12 23:25:48,738 [INFO] Step[2350/2713]: training loss : 0.9706112694740295 TRAIN  loss dict:  {'classification_loss': 0.9706112694740295}
2025-01-12 23:26:00,685 [INFO] Step[2400/2713]: training loss : 0.9726726591587067 TRAIN  loss dict:  {'classification_loss': 0.9726726591587067}
2025-01-12 23:26:12,639 [INFO] Step[2450/2713]: training loss : 0.9612282085418701 TRAIN  loss dict:  {'classification_loss': 0.9612282085418701}
2025-01-12 23:26:24,584 [INFO] Step[2500/2713]: training loss : 0.9596189069747925 TRAIN  loss dict:  {'classification_loss': 0.9596189069747925}
2025-01-12 23:26:36,541 [INFO] Step[2550/2713]: training loss : 0.9849995601177216 TRAIN  loss dict:  {'classification_loss': 0.9849995601177216}
2025-01-12 23:26:48,506 [INFO] Step[2600/2713]: training loss : 0.9689025831222534 TRAIN  loss dict:  {'classification_loss': 0.9689025831222534}
2025-01-12 23:27:00,501 [INFO] Step[2650/2713]: training loss : 0.9791474270820618 TRAIN  loss dict:  {'classification_loss': 0.9791474270820618}
2025-01-12 23:27:12,452 [INFO] Step[2700/2713]: training loss : 0.9656001567840576 TRAIN  loss dict:  {'classification_loss': 0.9656001567840576}
2025-01-12 23:28:40,802 [INFO] Label accuracies statistics:
2025-01-12 23:28:40,802 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 0.75, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.5, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 0.75, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.5, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.5, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 23:28:40,804 [INFO] [41] TRAIN  loss: 0.9686759541362491 acc: 0.9990170782651432
2025-01-12 23:28:40,804 [INFO] [41] TRAIN  loss dict: {'classification_loss': 0.9686759541362491}
2025-01-12 23:28:40,804 [INFO] [41] VALIDATION loss: 1.7559496180007332 VALIDATION acc: 0.8119122257053292
2025-01-12 23:28:40,804 [INFO] [41] VALIDATION loss dict: {'classification_loss': 1.7559496180007332}
2025-01-12 23:28:40,804 [INFO] 
2025-01-12 23:28:58,367 [INFO] Step[50/2713]: training loss : 0.9650052189826965 TRAIN  loss dict:  {'classification_loss': 0.9650052189826965}
2025-01-12 23:29:10,289 [INFO] Step[100/2713]: training loss : 0.9658308112621308 TRAIN  loss dict:  {'classification_loss': 0.9658308112621308}
2025-01-12 23:29:22,213 [INFO] Step[150/2713]: training loss : 0.9567971098423004 TRAIN  loss dict:  {'classification_loss': 0.9567971098423004}
2025-01-12 23:29:34,145 [INFO] Step[200/2713]: training loss : 0.9535708057880402 TRAIN  loss dict:  {'classification_loss': 0.9535708057880402}
2025-01-12 23:29:46,072 [INFO] Step[250/2713]: training loss : 0.9746414494514465 TRAIN  loss dict:  {'classification_loss': 0.9746414494514465}
2025-01-12 23:29:58,032 [INFO] Step[300/2713]: training loss : 0.9613870334625244 TRAIN  loss dict:  {'classification_loss': 0.9613870334625244}
2025-01-12 23:30:09,945 [INFO] Step[350/2713]: training loss : 0.9637846279144288 TRAIN  loss dict:  {'classification_loss': 0.9637846279144288}
2025-01-12 23:30:21,835 [INFO] Step[400/2713]: training loss : 0.9622795987129211 TRAIN  loss dict:  {'classification_loss': 0.9622795987129211}
2025-01-12 23:30:33,765 [INFO] Step[450/2713]: training loss : 0.960336332321167 TRAIN  loss dict:  {'classification_loss': 0.960336332321167}
2025-01-12 23:30:45,702 [INFO] Step[500/2713]: training loss : 0.9619906306266784 TRAIN  loss dict:  {'classification_loss': 0.9619906306266784}
2025-01-12 23:30:57,632 [INFO] Step[550/2713]: training loss : 0.968379522562027 TRAIN  loss dict:  {'classification_loss': 0.968379522562027}
2025-01-12 23:31:09,502 [INFO] Step[600/2713]: training loss : 0.9632689809799194 TRAIN  loss dict:  {'classification_loss': 0.9632689809799194}
2025-01-12 23:31:21,416 [INFO] Step[650/2713]: training loss : 0.9756618678569794 TRAIN  loss dict:  {'classification_loss': 0.9756618678569794}
2025-01-12 23:31:33,345 [INFO] Step[700/2713]: training loss : 0.9560710859298706 TRAIN  loss dict:  {'classification_loss': 0.9560710859298706}
2025-01-12 23:31:45,372 [INFO] Step[750/2713]: training loss : 0.9607494080066681 TRAIN  loss dict:  {'classification_loss': 0.9607494080066681}
2025-01-12 23:31:57,282 [INFO] Step[800/2713]: training loss : 0.96250035405159 TRAIN  loss dict:  {'classification_loss': 0.96250035405159}
2025-01-12 23:32:09,183 [INFO] Step[850/2713]: training loss : 0.9768570041656495 TRAIN  loss dict:  {'classification_loss': 0.9768570041656495}
2025-01-12 23:32:21,116 [INFO] Step[900/2713]: training loss : 0.9617305970191956 TRAIN  loss dict:  {'classification_loss': 0.9617305970191956}
2025-01-12 23:32:33,066 [INFO] Step[950/2713]: training loss : 0.9572247815132141 TRAIN  loss dict:  {'classification_loss': 0.9572247815132141}
2025-01-12 23:32:44,997 [INFO] Step[1000/2713]: training loss : 0.9612989974021912 TRAIN  loss dict:  {'classification_loss': 0.9612989974021912}
2025-01-12 23:32:56,937 [INFO] Step[1050/2713]: training loss : 0.9885248064994812 TRAIN  loss dict:  {'classification_loss': 0.9885248064994812}
2025-01-12 23:33:08,835 [INFO] Step[1100/2713]: training loss : 0.9647786128520965 TRAIN  loss dict:  {'classification_loss': 0.9647786128520965}
2025-01-12 23:33:20,758 [INFO] Step[1150/2713]: training loss : 0.9603555226325988 TRAIN  loss dict:  {'classification_loss': 0.9603555226325988}
2025-01-12 23:33:32,689 [INFO] Step[1200/2713]: training loss : 0.9667544043064118 TRAIN  loss dict:  {'classification_loss': 0.9667544043064118}
2025-01-12 23:33:44,581 [INFO] Step[1250/2713]: training loss : 0.9601255989074707 TRAIN  loss dict:  {'classification_loss': 0.9601255989074707}
2025-01-12 23:33:56,495 [INFO] Step[1300/2713]: training loss : 0.969272563457489 TRAIN  loss dict:  {'classification_loss': 0.969272563457489}
2025-01-12 23:34:08,434 [INFO] Step[1350/2713]: training loss : 0.9630835831165314 TRAIN  loss dict:  {'classification_loss': 0.9630835831165314}
2025-01-12 23:34:20,350 [INFO] Step[1400/2713]: training loss : 0.9560191094875335 TRAIN  loss dict:  {'classification_loss': 0.9560191094875335}
2025-01-12 23:34:32,281 [INFO] Step[1450/2713]: training loss : 0.9676309251785278 TRAIN  loss dict:  {'classification_loss': 0.9676309251785278}
2025-01-12 23:34:44,216 [INFO] Step[1500/2713]: training loss : 0.974418226480484 TRAIN  loss dict:  {'classification_loss': 0.974418226480484}
2025-01-12 23:34:56,162 [INFO] Step[1550/2713]: training loss : 0.959352890253067 TRAIN  loss dict:  {'classification_loss': 0.959352890253067}
2025-01-12 23:35:08,062 [INFO] Step[1600/2713]: training loss : 0.9771085405349731 TRAIN  loss dict:  {'classification_loss': 0.9771085405349731}
2025-01-12 23:35:20,011 [INFO] Step[1650/2713]: training loss : 0.9628949439525605 TRAIN  loss dict:  {'classification_loss': 0.9628949439525605}
2025-01-12 23:35:31,901 [INFO] Step[1700/2713]: training loss : 0.9600326955318451 TRAIN  loss dict:  {'classification_loss': 0.9600326955318451}
2025-01-12 23:35:43,813 [INFO] Step[1750/2713]: training loss : 0.9581349229812622 TRAIN  loss dict:  {'classification_loss': 0.9581349229812622}
2025-01-12 23:35:55,774 [INFO] Step[1800/2713]: training loss : 0.9666281259059906 TRAIN  loss dict:  {'classification_loss': 0.9666281259059906}
2025-01-12 23:36:07,698 [INFO] Step[1850/2713]: training loss : 0.958998407125473 TRAIN  loss dict:  {'classification_loss': 0.958998407125473}
2025-01-12 23:36:19,562 [INFO] Step[1900/2713]: training loss : 0.961107120513916 TRAIN  loss dict:  {'classification_loss': 0.961107120513916}
2025-01-12 23:36:31,441 [INFO] Step[1950/2713]: training loss : 0.9583239448070526 TRAIN  loss dict:  {'classification_loss': 0.9583239448070526}
2025-01-12 23:36:43,383 [INFO] Step[2000/2713]: training loss : 0.9655361425876617 TRAIN  loss dict:  {'classification_loss': 0.9655361425876617}
2025-01-12 23:36:55,341 [INFO] Step[2050/2713]: training loss : 0.9589522290229797 TRAIN  loss dict:  {'classification_loss': 0.9589522290229797}
2025-01-12 23:37:07,219 [INFO] Step[2100/2713]: training loss : 0.9555343067646027 TRAIN  loss dict:  {'classification_loss': 0.9555343067646027}
2025-01-12 23:37:19,131 [INFO] Step[2150/2713]: training loss : 0.958253835439682 TRAIN  loss dict:  {'classification_loss': 0.958253835439682}
2025-01-12 23:37:31,047 [INFO] Step[2200/2713]: training loss : 0.9867093527317047 TRAIN  loss dict:  {'classification_loss': 0.9867093527317047}
2025-01-12 23:37:43,007 [INFO] Step[2250/2713]: training loss : 0.9912806940078736 TRAIN  loss dict:  {'classification_loss': 0.9912806940078736}
2025-01-12 23:37:54,868 [INFO] Step[2300/2713]: training loss : 0.9618556094169617 TRAIN  loss dict:  {'classification_loss': 0.9618556094169617}
2025-01-12 23:38:06,770 [INFO] Step[2350/2713]: training loss : 0.9690683579444885 TRAIN  loss dict:  {'classification_loss': 0.9690683579444885}
2025-01-12 23:38:18,665 [INFO] Step[2400/2713]: training loss : 0.9569127571582794 TRAIN  loss dict:  {'classification_loss': 0.9569127571582794}
2025-01-12 23:38:30,534 [INFO] Step[2450/2713]: training loss : 0.9794008779525757 TRAIN  loss dict:  {'classification_loss': 0.9794008779525757}
2025-01-12 23:38:42,443 [INFO] Step[2500/2713]: training loss : 0.9732655000686645 TRAIN  loss dict:  {'classification_loss': 0.9732655000686645}
2025-01-12 23:38:54,507 [INFO] Step[2550/2713]: training loss : 0.9692651534080505 TRAIN  loss dict:  {'classification_loss': 0.9692651534080505}
2025-01-12 23:39:07,002 [INFO] Step[2600/2713]: training loss : 0.9587154710292816 TRAIN  loss dict:  {'classification_loss': 0.9587154710292816}
2025-01-12 23:39:19,487 [INFO] Step[2650/2713]: training loss : 0.962486869096756 TRAIN  loss dict:  {'classification_loss': 0.962486869096756}
2025-01-12 23:39:31,645 [INFO] Step[2700/2713]: training loss : 0.9615657162666321 TRAIN  loss dict:  {'classification_loss': 0.9615657162666321}
2025-01-12 23:41:24,768 [INFO] Label accuracies statistics:
2025-01-12 23:41:24,768 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 1.0, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.25, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.5, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.25, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 0.75, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 23:41:24,771 [INFO] [42] TRAIN  loss: 0.965069736056428 acc: 0.9986484826145718
2025-01-12 23:41:24,771 [INFO] [42] TRAIN  loss dict: {'classification_loss': 0.965069736056428}
2025-01-12 23:41:24,771 [INFO] [42] VALIDATION loss: 1.6816306898468418 VALIDATION acc: 0.8326018808777429
2025-01-12 23:41:24,771 [INFO] [42] VALIDATION loss dict: {'classification_loss': 1.6816306898468418}
2025-01-12 23:41:24,772 [INFO] 
2025-01-12 23:41:42,476 [INFO] Step[50/2713]: training loss : 0.9608668208122253 TRAIN  loss dict:  {'classification_loss': 0.9608668208122253}
2025-01-12 23:41:54,381 [INFO] Step[100/2713]: training loss : 0.9566682064533234 TRAIN  loss dict:  {'classification_loss': 0.9566682064533234}
2025-01-12 23:42:06,269 [INFO] Step[150/2713]: training loss : 0.9610929238796234 TRAIN  loss dict:  {'classification_loss': 0.9610929238796234}
2025-01-12 23:42:18,163 [INFO] Step[200/2713]: training loss : 0.9573949086666107 TRAIN  loss dict:  {'classification_loss': 0.9573949086666107}
2025-01-12 23:42:30,082 [INFO] Step[250/2713]: training loss : 0.9711256468296051 TRAIN  loss dict:  {'classification_loss': 0.9711256468296051}
2025-01-12 23:42:42,000 [INFO] Step[300/2713]: training loss : 0.9622604811191559 TRAIN  loss dict:  {'classification_loss': 0.9622604811191559}
2025-01-12 23:42:53,915 [INFO] Step[350/2713]: training loss : 0.9622750794887542 TRAIN  loss dict:  {'classification_loss': 0.9622750794887542}
2025-01-12 23:43:05,785 [INFO] Step[400/2713]: training loss : 0.9731232023239136 TRAIN  loss dict:  {'classification_loss': 0.9731232023239136}
2025-01-12 23:43:17,683 [INFO] Step[450/2713]: training loss : 0.9592381966114044 TRAIN  loss dict:  {'classification_loss': 0.9592381966114044}
2025-01-12 23:43:29,567 [INFO] Step[500/2713]: training loss : 0.9780006647109986 TRAIN  loss dict:  {'classification_loss': 0.9780006647109986}
2025-01-12 23:43:41,528 [INFO] Step[550/2713]: training loss : 0.9590469217300415 TRAIN  loss dict:  {'classification_loss': 0.9590469217300415}
2025-01-12 23:43:53,464 [INFO] Step[600/2713]: training loss : 0.964061233997345 TRAIN  loss dict:  {'classification_loss': 0.964061233997345}
2025-01-12 23:44:05,414 [INFO] Step[650/2713]: training loss : 0.9612213110923767 TRAIN  loss dict:  {'classification_loss': 0.9612213110923767}
2025-01-12 23:44:17,327 [INFO] Step[700/2713]: training loss : 0.9756525957584381 TRAIN  loss dict:  {'classification_loss': 0.9756525957584381}
2025-01-12 23:44:29,216 [INFO] Step[750/2713]: training loss : 0.9568563067913055 TRAIN  loss dict:  {'classification_loss': 0.9568563067913055}
2025-01-12 23:44:41,275 [INFO] Step[800/2713]: training loss : 0.9591949427127838 TRAIN  loss dict:  {'classification_loss': 0.9591949427127838}
2025-01-12 23:44:53,425 [INFO] Step[850/2713]: training loss : 0.956661400794983 TRAIN  loss dict:  {'classification_loss': 0.956661400794983}
2025-01-12 23:45:05,389 [INFO] Step[900/2713]: training loss : 0.9604442918300629 TRAIN  loss dict:  {'classification_loss': 0.9604442918300629}
2025-01-12 23:45:17,340 [INFO] Step[950/2713]: training loss : 0.9617282378673554 TRAIN  loss dict:  {'classification_loss': 0.9617282378673554}
2025-01-12 23:45:29,248 [INFO] Step[1000/2713]: training loss : 0.9624989569187165 TRAIN  loss dict:  {'classification_loss': 0.9624989569187165}
2025-01-12 23:45:41,242 [INFO] Step[1050/2713]: training loss : 0.9595937204360961 TRAIN  loss dict:  {'classification_loss': 0.9595937204360961}
2025-01-12 23:45:53,201 [INFO] Step[1100/2713]: training loss : 0.9577049934864044 TRAIN  loss dict:  {'classification_loss': 0.9577049934864044}
2025-01-12 23:46:05,169 [INFO] Step[1150/2713]: training loss : 0.965075398683548 TRAIN  loss dict:  {'classification_loss': 0.965075398683548}
2025-01-12 23:46:17,072 [INFO] Step[1200/2713]: training loss : 0.9650591957569122 TRAIN  loss dict:  {'classification_loss': 0.9650591957569122}
2025-01-12 23:46:29,001 [INFO] Step[1250/2713]: training loss : 0.9534941613674164 TRAIN  loss dict:  {'classification_loss': 0.9534941613674164}
2025-01-12 23:46:41,018 [INFO] Step[1300/2713]: training loss : 0.9601129281520844 TRAIN  loss dict:  {'classification_loss': 0.9601129281520844}
2025-01-12 23:46:52,964 [INFO] Step[1350/2713]: training loss : 0.9616349637508392 TRAIN  loss dict:  {'classification_loss': 0.9616349637508392}
2025-01-12 23:47:05,025 [INFO] Step[1400/2713]: training loss : 0.9711808025836944 TRAIN  loss dict:  {'classification_loss': 0.9711808025836944}
2025-01-12 23:47:17,164 [INFO] Step[1450/2713]: training loss : 0.961719068288803 TRAIN  loss dict:  {'classification_loss': 0.961719068288803}
2025-01-12 23:47:29,111 [INFO] Step[1500/2713]: training loss : 0.9533381986618042 TRAIN  loss dict:  {'classification_loss': 0.9533381986618042}
2025-01-12 23:47:40,888 [INFO] Step[1550/2713]: training loss : 0.9625704228878021 TRAIN  loss dict:  {'classification_loss': 0.9625704228878021}
2025-01-12 23:47:52,660 [INFO] Step[1600/2713]: training loss : 0.9545062386989593 TRAIN  loss dict:  {'classification_loss': 0.9545062386989593}
2025-01-12 23:48:04,456 [INFO] Step[1650/2713]: training loss : 0.9549067199230195 TRAIN  loss dict:  {'classification_loss': 0.9549067199230195}
2025-01-12 23:48:16,216 [INFO] Step[1700/2713]: training loss : 0.9672633361816406 TRAIN  loss dict:  {'classification_loss': 0.9672633361816406}
2025-01-12 23:48:27,979 [INFO] Step[1750/2713]: training loss : 0.9603904211521148 TRAIN  loss dict:  {'classification_loss': 0.9603904211521148}
2025-01-12 23:48:39,767 [INFO] Step[1800/2713]: training loss : 0.9569027853012085 TRAIN  loss dict:  {'classification_loss': 0.9569027853012085}
2025-01-12 23:48:51,540 [INFO] Step[1850/2713]: training loss : 0.9652831172943115 TRAIN  loss dict:  {'classification_loss': 0.9652831172943115}
2025-01-12 23:49:03,339 [INFO] Step[1900/2713]: training loss : 0.9659076058864593 TRAIN  loss dict:  {'classification_loss': 0.9659076058864593}
2025-01-12 23:49:15,101 [INFO] Step[1950/2713]: training loss : 0.9635421776771546 TRAIN  loss dict:  {'classification_loss': 0.9635421776771546}
2025-01-12 23:49:26,890 [INFO] Step[2000/2713]: training loss : 0.9596370851993561 TRAIN  loss dict:  {'classification_loss': 0.9596370851993561}
2025-01-12 23:49:38,624 [INFO] Step[2050/2713]: training loss : 0.9596841907501221 TRAIN  loss dict:  {'classification_loss': 0.9596841907501221}
2025-01-12 23:49:50,379 [INFO] Step[2100/2713]: training loss : 0.9563239300251007 TRAIN  loss dict:  {'classification_loss': 0.9563239300251007}
2025-01-12 23:50:02,180 [INFO] Step[2150/2713]: training loss : 0.9641686081886292 TRAIN  loss dict:  {'classification_loss': 0.9641686081886292}
2025-01-12 23:50:13,932 [INFO] Step[2200/2713]: training loss : 0.9589487087726593 TRAIN  loss dict:  {'classification_loss': 0.9589487087726593}
2025-01-12 23:50:25,707 [INFO] Step[2250/2713]: training loss : 0.9704177498817443 TRAIN  loss dict:  {'classification_loss': 0.9704177498817443}
2025-01-12 23:50:37,503 [INFO] Step[2300/2713]: training loss : 0.9686001896858215 TRAIN  loss dict:  {'classification_loss': 0.9686001896858215}
2025-01-12 23:50:49,296 [INFO] Step[2350/2713]: training loss : 0.9714233469963074 TRAIN  loss dict:  {'classification_loss': 0.9714233469963074}
2025-01-12 23:51:01,085 [INFO] Step[2400/2713]: training loss : 0.9636469542980194 TRAIN  loss dict:  {'classification_loss': 0.9636469542980194}
2025-01-12 23:51:12,832 [INFO] Step[2450/2713]: training loss : 0.963902006149292 TRAIN  loss dict:  {'classification_loss': 0.963902006149292}
2025-01-12 23:51:24,592 [INFO] Step[2500/2713]: training loss : 0.9581380689144134 TRAIN  loss dict:  {'classification_loss': 0.9581380689144134}
2025-01-12 23:51:36,356 [INFO] Step[2550/2713]: training loss : 0.9561565136909485 TRAIN  loss dict:  {'classification_loss': 0.9561565136909485}
2025-01-12 23:51:48,148 [INFO] Step[2600/2713]: training loss : 0.9585383713245392 TRAIN  loss dict:  {'classification_loss': 0.9585383713245392}
2025-01-12 23:51:59,956 [INFO] Step[2650/2713]: training loss : 0.9613532900810242 TRAIN  loss dict:  {'classification_loss': 0.9613532900810242}
2025-01-12 23:52:11,674 [INFO] Step[2700/2713]: training loss : 0.9610768973827362 TRAIN  loss dict:  {'classification_loss': 0.9610768973827362}
2025-01-12 23:53:59,150 [INFO] Label accuracies statistics:
2025-01-12 23:53:59,150 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 1.0, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.5, 240: 1.0, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 0.75, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 1.0, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-12 23:53:59,153 [INFO] [43] TRAIN  loss: 0.9620210316582795 acc: 0.9991399434820002
2025-01-12 23:53:59,153 [INFO] [43] TRAIN  loss dict: {'classification_loss': 0.9620210316582795}
2025-01-12 23:53:59,153 [INFO] [43] VALIDATION loss: 1.688013841671155 VALIDATION acc: 0.8269592476489028
2025-01-12 23:53:59,153 [INFO] [43] VALIDATION loss dict: {'classification_loss': 1.688013841671155}
2025-01-12 23:53:59,153 [INFO] 
2025-01-12 23:54:16,635 [INFO] Step[50/2713]: training loss : 0.9579678523540497 TRAIN  loss dict:  {'classification_loss': 0.9579678523540497}
2025-01-12 23:54:28,384 [INFO] Step[100/2713]: training loss : 0.958287832736969 TRAIN  loss dict:  {'classification_loss': 0.958287832736969}
2025-01-12 23:54:40,166 [INFO] Step[150/2713]: training loss : 0.9619050788879394 TRAIN  loss dict:  {'classification_loss': 0.9619050788879394}
2025-01-12 23:54:51,934 [INFO] Step[200/2713]: training loss : 0.9519053733348847 TRAIN  loss dict:  {'classification_loss': 0.9519053733348847}
2025-01-12 23:55:03,841 [INFO] Step[250/2713]: training loss : 0.9529026508331299 TRAIN  loss dict:  {'classification_loss': 0.9529026508331299}
2025-01-12 23:55:16,047 [INFO] Step[300/2713]: training loss : 0.9638103556632995 TRAIN  loss dict:  {'classification_loss': 0.9638103556632995}
2025-01-12 23:55:28,073 [INFO] Step[350/2713]: training loss : 0.9600147938728333 TRAIN  loss dict:  {'classification_loss': 0.9600147938728333}
2025-01-12 23:55:40,024 [INFO] Step[400/2713]: training loss : 0.9582768368721009 TRAIN  loss dict:  {'classification_loss': 0.9582768368721009}
2025-01-12 23:55:51,983 [INFO] Step[450/2713]: training loss : 0.9697795045375824 TRAIN  loss dict:  {'classification_loss': 0.9697795045375824}
2025-01-12 23:56:03,931 [INFO] Step[500/2713]: training loss : 0.9634293448925019 TRAIN  loss dict:  {'classification_loss': 0.9634293448925019}
2025-01-12 23:56:15,905 [INFO] Step[550/2713]: training loss : 0.9621575903892517 TRAIN  loss dict:  {'classification_loss': 0.9621575903892517}
2025-01-12 23:56:27,865 [INFO] Step[600/2713]: training loss : 0.9670980286598205 TRAIN  loss dict:  {'classification_loss': 0.9670980286598205}
2025-01-12 23:56:39,835 [INFO] Step[650/2713]: training loss : 0.9546757507324218 TRAIN  loss dict:  {'classification_loss': 0.9546757507324218}
2025-01-12 23:56:51,762 [INFO] Step[700/2713]: training loss : 0.9590401089191437 TRAIN  loss dict:  {'classification_loss': 0.9590401089191437}
2025-01-12 23:57:03,755 [INFO] Step[750/2713]: training loss : 0.9596106827259063 TRAIN  loss dict:  {'classification_loss': 0.9596106827259063}
2025-01-12 23:57:15,691 [INFO] Step[800/2713]: training loss : 0.9601615297794343 TRAIN  loss dict:  {'classification_loss': 0.9601615297794343}
2025-01-12 23:57:27,672 [INFO] Step[850/2713]: training loss : 0.9596792960166931 TRAIN  loss dict:  {'classification_loss': 0.9596792960166931}
2025-01-12 23:57:39,643 [INFO] Step[900/2713]: training loss : 0.9614939594268799 TRAIN  loss dict:  {'classification_loss': 0.9614939594268799}
2025-01-12 23:57:51,614 [INFO] Step[950/2713]: training loss : 0.9586516308784485 TRAIN  loss dict:  {'classification_loss': 0.9586516308784485}
2025-01-12 23:58:03,603 [INFO] Step[1000/2713]: training loss : 0.9694089031219483 TRAIN  loss dict:  {'classification_loss': 0.9694089031219483}
2025-01-12 23:58:15,900 [INFO] Step[1050/2713]: training loss : 0.9625032031536103 TRAIN  loss dict:  {'classification_loss': 0.9625032031536103}
2025-01-12 23:58:28,380 [INFO] Step[1100/2713]: training loss : 0.9612348365783692 TRAIN  loss dict:  {'classification_loss': 0.9612348365783692}
2025-01-12 23:58:40,772 [INFO] Step[1150/2713]: training loss : 0.9632237720489502 TRAIN  loss dict:  {'classification_loss': 0.9632237720489502}
2025-01-12 23:58:53,236 [INFO] Step[1200/2713]: training loss : 0.9626912069320679 TRAIN  loss dict:  {'classification_loss': 0.9626912069320679}
2025-01-12 23:59:05,789 [INFO] Step[1250/2713]: training loss : 0.9714978849887848 TRAIN  loss dict:  {'classification_loss': 0.9714978849887848}
2025-01-12 23:59:18,245 [INFO] Step[1300/2713]: training loss : 0.9616120886802674 TRAIN  loss dict:  {'classification_loss': 0.9616120886802674}
2025-01-12 23:59:31,337 [INFO] Step[1350/2713]: training loss : 0.9581222856044769 TRAIN  loss dict:  {'classification_loss': 0.9581222856044769}
2025-01-12 23:59:44,805 [INFO] Step[1400/2713]: training loss : 0.9618028247356415 TRAIN  loss dict:  {'classification_loss': 0.9618028247356415}
2025-01-12 23:59:57,739 [INFO] Step[1450/2713]: training loss : 0.95667271733284 TRAIN  loss dict:  {'classification_loss': 0.95667271733284}
2025-01-13 00:00:09,858 [INFO] Step[1500/2713]: training loss : 0.9707783889770508 TRAIN  loss dict:  {'classification_loss': 0.9707783889770508}
2025-01-13 00:00:21,757 [INFO] Step[1550/2713]: training loss : 0.9621195065975189 TRAIN  loss dict:  {'classification_loss': 0.9621195065975189}
2025-01-13 00:00:33,673 [INFO] Step[1600/2713]: training loss : 0.9596891975402833 TRAIN  loss dict:  {'classification_loss': 0.9596891975402833}
2025-01-13 00:00:45,599 [INFO] Step[1650/2713]: training loss : 0.971189614534378 TRAIN  loss dict:  {'classification_loss': 0.971189614534378}
2025-01-13 00:00:57,532 [INFO] Step[1700/2713]: training loss : 0.9695851194858551 TRAIN  loss dict:  {'classification_loss': 0.9695851194858551}
2025-01-13 00:01:09,457 [INFO] Step[1750/2713]: training loss : 0.9684094512462615 TRAIN  loss dict:  {'classification_loss': 0.9684094512462615}
2025-01-13 00:01:21,377 [INFO] Step[1800/2713]: training loss : 0.9596585595607757 TRAIN  loss dict:  {'classification_loss': 0.9596585595607757}
2025-01-13 00:01:33,318 [INFO] Step[1850/2713]: training loss : 0.961550989151001 TRAIN  loss dict:  {'classification_loss': 0.961550989151001}
2025-01-13 00:01:45,271 [INFO] Step[1900/2713]: training loss : 0.9556739163398743 TRAIN  loss dict:  {'classification_loss': 0.9556739163398743}
2025-01-13 00:01:57,229 [INFO] Step[1950/2713]: training loss : 0.9725450158119202 TRAIN  loss dict:  {'classification_loss': 0.9725450158119202}
2025-01-13 00:02:09,184 [INFO] Step[2000/2713]: training loss : 0.9592662417888641 TRAIN  loss dict:  {'classification_loss': 0.9592662417888641}
2025-01-13 00:02:21,158 [INFO] Step[2050/2713]: training loss : 0.9546648919582367 TRAIN  loss dict:  {'classification_loss': 0.9546648919582367}
2025-01-13 00:02:33,085 [INFO] Step[2100/2713]: training loss : 0.9584967064857483 TRAIN  loss dict:  {'classification_loss': 0.9584967064857483}
2025-01-13 00:02:45,034 [INFO] Step[2150/2713]: training loss : 0.9546423852443695 TRAIN  loss dict:  {'classification_loss': 0.9546423852443695}
2025-01-13 00:02:57,014 [INFO] Step[2200/2713]: training loss : 0.966981360912323 TRAIN  loss dict:  {'classification_loss': 0.966981360912323}
2025-01-13 00:03:08,961 [INFO] Step[2250/2713]: training loss : 0.9593658852577209 TRAIN  loss dict:  {'classification_loss': 0.9593658852577209}
2025-01-13 00:03:20,905 [INFO] Step[2300/2713]: training loss : 0.9596389031410217 TRAIN  loss dict:  {'classification_loss': 0.9596389031410217}
2025-01-13 00:03:32,887 [INFO] Step[2350/2713]: training loss : 0.9658372676372529 TRAIN  loss dict:  {'classification_loss': 0.9658372676372529}
2025-01-13 00:03:44,843 [INFO] Step[2400/2713]: training loss : 0.9643237793445587 TRAIN  loss dict:  {'classification_loss': 0.9643237793445587}
2025-01-13 00:03:56,788 [INFO] Step[2450/2713]: training loss : 0.9581218540668488 TRAIN  loss dict:  {'classification_loss': 0.9581218540668488}
2025-01-13 00:04:08,755 [INFO] Step[2500/2713]: training loss : 0.963203626871109 TRAIN  loss dict:  {'classification_loss': 0.963203626871109}
2025-01-13 00:04:20,715 [INFO] Step[2550/2713]: training loss : 0.9580891871452332 TRAIN  loss dict:  {'classification_loss': 0.9580891871452332}
2025-01-13 00:04:32,687 [INFO] Step[2600/2713]: training loss : 0.9772733330726624 TRAIN  loss dict:  {'classification_loss': 0.9772733330726624}
2025-01-13 00:04:44,682 [INFO] Step[2650/2713]: training loss : 0.9604220998287201 TRAIN  loss dict:  {'classification_loss': 0.9604220998287201}
2025-01-13 00:04:56,557 [INFO] Step[2700/2713]: training loss : 0.9620944046974182 TRAIN  loss dict:  {'classification_loss': 0.9620944046974182}
2025-01-13 00:06:25,353 [INFO] Label accuracies statistics:
2025-01-13 00:06:25,353 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.25, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.5, 165: 0.75, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 1.0, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.5, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.5, 268: 0.75, 269: 1.0, 270: 0.75, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.5, 291: 0.75, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.5, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.75, 354: 0.5, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 1.0, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-13 00:06:25,356 [INFO] [44] TRAIN  loss: 0.9618966002065319 acc: 0.9996314043494287
2025-01-13 00:06:25,356 [INFO] [44] TRAIN  loss dict: {'classification_loss': 0.9618966002065319}
2025-01-13 00:06:25,356 [INFO] [44] VALIDATION loss: 1.7202794879002679 VALIDATION acc: 0.8181818181818182
2025-01-13 00:06:25,356 [INFO] [44] VALIDATION loss dict: {'classification_loss': 1.7202794879002679}
2025-01-13 00:06:25,357 [INFO] 
2025-01-13 00:06:43,623 [INFO] Step[50/2713]: training loss : 0.9595909106731415 TRAIN  loss dict:  {'classification_loss': 0.9595909106731415}
2025-01-13 00:06:55,530 [INFO] Step[100/2713]: training loss : 0.9677370643615723 TRAIN  loss dict:  {'classification_loss': 0.9677370643615723}
2025-01-13 00:07:07,462 [INFO] Step[150/2713]: training loss : 0.9576876544952393 TRAIN  loss dict:  {'classification_loss': 0.9576876544952393}
2025-01-13 00:07:19,352 [INFO] Step[200/2713]: training loss : 0.9545691728591919 TRAIN  loss dict:  {'classification_loss': 0.9545691728591919}
2025-01-13 00:07:31,282 [INFO] Step[250/2713]: training loss : 0.964486825466156 TRAIN  loss dict:  {'classification_loss': 0.964486825466156}
2025-01-13 00:07:43,206 [INFO] Step[300/2713]: training loss : 0.9615266585350036 TRAIN  loss dict:  {'classification_loss': 0.9615266585350036}
2025-01-13 00:07:55,088 [INFO] Step[350/2713]: training loss : 0.9585872566699982 TRAIN  loss dict:  {'classification_loss': 0.9585872566699982}
2025-01-13 00:08:06,984 [INFO] Step[400/2713]: training loss : 0.9708962941169739 TRAIN  loss dict:  {'classification_loss': 0.9708962941169739}
2025-01-13 00:08:18,891 [INFO] Step[450/2713]: training loss : 0.9557492208480834 TRAIN  loss dict:  {'classification_loss': 0.9557492208480834}
2025-01-13 00:08:30,823 [INFO] Step[500/2713]: training loss : 0.9553217864036561 TRAIN  loss dict:  {'classification_loss': 0.9553217864036561}
2025-01-13 00:08:42,712 [INFO] Step[550/2713]: training loss : 0.9559027075767517 TRAIN  loss dict:  {'classification_loss': 0.9559027075767517}
2025-01-13 00:08:54,588 [INFO] Step[600/2713]: training loss : 0.9621670877933503 TRAIN  loss dict:  {'classification_loss': 0.9621670877933503}
2025-01-13 00:09:06,492 [INFO] Step[650/2713]: training loss : 0.9669509720802307 TRAIN  loss dict:  {'classification_loss': 0.9669509720802307}
2025-01-13 00:09:18,395 [INFO] Step[700/2713]: training loss : 0.9535962891578674 TRAIN  loss dict:  {'classification_loss': 0.9535962891578674}
2025-01-13 00:09:30,314 [INFO] Step[750/2713]: training loss : 0.9550734567642212 TRAIN  loss dict:  {'classification_loss': 0.9550734567642212}
2025-01-13 00:09:42,240 [INFO] Step[800/2713]: training loss : 0.9624286019802093 TRAIN  loss dict:  {'classification_loss': 0.9624286019802093}
2025-01-13 00:09:54,174 [INFO] Step[850/2713]: training loss : 0.9696610546112061 TRAIN  loss dict:  {'classification_loss': 0.9696610546112061}
2025-01-13 00:10:06,087 [INFO] Step[900/2713]: training loss : 0.9686602365970611 TRAIN  loss dict:  {'classification_loss': 0.9686602365970611}
2025-01-13 00:10:17,999 [INFO] Step[950/2713]: training loss : 0.9566374719142914 TRAIN  loss dict:  {'classification_loss': 0.9566374719142914}
2025-01-13 00:10:29,869 [INFO] Step[1000/2713]: training loss : 0.9545944845676422 TRAIN  loss dict:  {'classification_loss': 0.9545944845676422}
2025-01-13 00:10:41,810 [INFO] Step[1050/2713]: training loss : 0.9604595017433166 TRAIN  loss dict:  {'classification_loss': 0.9604595017433166}
2025-01-13 00:10:53,742 [INFO] Step[1100/2713]: training loss : 0.9674429941177368 TRAIN  loss dict:  {'classification_loss': 0.9674429941177368}
2025-01-13 00:11:05,676 [INFO] Step[1150/2713]: training loss : 0.962068487405777 TRAIN  loss dict:  {'classification_loss': 0.962068487405777}
2025-01-13 00:11:17,555 [INFO] Step[1200/2713]: training loss : 0.9684216463565827 TRAIN  loss dict:  {'classification_loss': 0.9684216463565827}
2025-01-13 00:11:29,499 [INFO] Step[1250/2713]: training loss : 0.9590300583839416 TRAIN  loss dict:  {'classification_loss': 0.9590300583839416}
2025-01-13 00:11:41,418 [INFO] Step[1300/2713]: training loss : 0.9713651275634766 TRAIN  loss dict:  {'classification_loss': 0.9713651275634766}
2025-01-13 00:11:53,295 [INFO] Step[1350/2713]: training loss : 0.9698038971424103 TRAIN  loss dict:  {'classification_loss': 0.9698038971424103}
2025-01-13 00:12:05,179 [INFO] Step[1400/2713]: training loss : 0.9615677082538605 TRAIN  loss dict:  {'classification_loss': 0.9615677082538605}
2025-01-13 00:12:17,095 [INFO] Step[1450/2713]: training loss : 0.955679612159729 TRAIN  loss dict:  {'classification_loss': 0.955679612159729}
2025-01-13 00:12:29,022 [INFO] Step[1500/2713]: training loss : 0.9648343849182129 TRAIN  loss dict:  {'classification_loss': 0.9648343849182129}
2025-01-13 00:12:40,925 [INFO] Step[1550/2713]: training loss : 0.9831140625476837 TRAIN  loss dict:  {'classification_loss': 0.9831140625476837}
2025-01-13 00:12:52,837 [INFO] Step[1600/2713]: training loss : 0.9651683354377747 TRAIN  loss dict:  {'classification_loss': 0.9651683354377747}
2025-01-13 00:13:04,770 [INFO] Step[1650/2713]: training loss : 0.9610425555706024 TRAIN  loss dict:  {'classification_loss': 0.9610425555706024}
2025-01-13 00:13:16,648 [INFO] Step[1700/2713]: training loss : 0.9593563413619995 TRAIN  loss dict:  {'classification_loss': 0.9593563413619995}
2025-01-13 00:13:28,606 [INFO] Step[1750/2713]: training loss : 0.9589337003231049 TRAIN  loss dict:  {'classification_loss': 0.9589337003231049}
2025-01-13 00:13:40,545 [INFO] Step[1800/2713]: training loss : 0.9747024703025818 TRAIN  loss dict:  {'classification_loss': 0.9747024703025818}
2025-01-13 00:13:52,446 [INFO] Step[1850/2713]: training loss : 0.9619678449630737 TRAIN  loss dict:  {'classification_loss': 0.9619678449630737}
2025-01-13 00:14:04,362 [INFO] Step[1900/2713]: training loss : 0.9616841197013855 TRAIN  loss dict:  {'classification_loss': 0.9616841197013855}
2025-01-13 00:14:16,269 [INFO] Step[1950/2713]: training loss : 0.962588312625885 TRAIN  loss dict:  {'classification_loss': 0.962588312625885}
2025-01-13 00:14:28,142 [INFO] Step[2000/2713]: training loss : 0.9611410427093506 TRAIN  loss dict:  {'classification_loss': 0.9611410427093506}
2025-01-13 00:14:40,044 [INFO] Step[2050/2713]: training loss : 0.9563513791561127 TRAIN  loss dict:  {'classification_loss': 0.9563513791561127}
2025-01-13 00:14:51,935 [INFO] Step[2100/2713]: training loss : 0.9548900580406189 TRAIN  loss dict:  {'classification_loss': 0.9548900580406189}
2025-01-13 00:15:03,871 [INFO] Step[2150/2713]: training loss : 0.9578957951068878 TRAIN  loss dict:  {'classification_loss': 0.9578957951068878}
2025-01-13 00:15:15,767 [INFO] Step[2200/2713]: training loss : 0.9544383776187897 TRAIN  loss dict:  {'classification_loss': 0.9544383776187897}
2025-01-13 00:15:27,671 [INFO] Step[2250/2713]: training loss : 0.9623009777069091 TRAIN  loss dict:  {'classification_loss': 0.9623009777069091}
2025-01-13 00:15:39,564 [INFO] Step[2300/2713]: training loss : 0.9774115300178527 TRAIN  loss dict:  {'classification_loss': 0.9774115300178527}
2025-01-13 00:15:51,487 [INFO] Step[2350/2713]: training loss : 0.9581215941905975 TRAIN  loss dict:  {'classification_loss': 0.9581215941905975}
2025-01-13 00:16:03,404 [INFO] Step[2400/2713]: training loss : 0.9886712646484375 TRAIN  loss dict:  {'classification_loss': 0.9886712646484375}
2025-01-13 00:16:15,297 [INFO] Step[2450/2713]: training loss : 0.959631141424179 TRAIN  loss dict:  {'classification_loss': 0.959631141424179}
2025-01-13 00:16:27,217 [INFO] Step[2500/2713]: training loss : 0.9646252083778382 TRAIN  loss dict:  {'classification_loss': 0.9646252083778382}
2025-01-13 00:16:39,142 [INFO] Step[2550/2713]: training loss : 0.9603622794151306 TRAIN  loss dict:  {'classification_loss': 0.9603622794151306}
2025-01-13 00:16:50,986 [INFO] Step[2600/2713]: training loss : 0.9586891853809356 TRAIN  loss dict:  {'classification_loss': 0.9586891853809356}
2025-01-13 00:17:03,178 [INFO] Step[2650/2713]: training loss : 0.9559520614147187 TRAIN  loss dict:  {'classification_loss': 0.9559520614147187}
2025-01-13 00:17:15,466 [INFO] Step[2700/2713]: training loss : 0.9651120722293853 TRAIN  loss dict:  {'classification_loss': 0.9651120722293853}
2025-01-13 00:19:13,914 [INFO] Label accuracies statistics:
2025-01-13 00:19:13,915 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.5, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.25, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 1.0, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 0.75, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 00:19:13,918 [INFO] [45] TRAIN  loss: 0.9625430909241868 acc: 0.9985256173977147
2025-01-13 00:19:13,918 [INFO] [45] TRAIN  loss dict: {'classification_loss': 0.9625430909241868}
2025-01-13 00:19:13,919 [INFO] [45] VALIDATION loss: 1.6748658899301874 VALIDATION acc: 0.8288401253918495
2025-01-13 00:19:13,919 [INFO] [45] VALIDATION loss dict: {'classification_loss': 1.6748658899301874}
2025-01-13 00:19:13,919 [INFO] 
2025-01-13 00:19:32,267 [INFO] Step[50/2713]: training loss : 0.9557977735996246 TRAIN  loss dict:  {'classification_loss': 0.9557977735996246}
2025-01-13 00:19:44,133 [INFO] Step[100/2713]: training loss : 0.952789990901947 TRAIN  loss dict:  {'classification_loss': 0.952789990901947}
2025-01-13 00:19:55,988 [INFO] Step[150/2713]: training loss : 0.9624723875522614 TRAIN  loss dict:  {'classification_loss': 0.9624723875522614}
2025-01-13 00:20:07,851 [INFO] Step[200/2713]: training loss : 0.9559282827377319 TRAIN  loss dict:  {'classification_loss': 0.9559282827377319}
2025-01-13 00:20:19,761 [INFO] Step[250/2713]: training loss : 0.961687502861023 TRAIN  loss dict:  {'classification_loss': 0.961687502861023}
2025-01-13 00:20:31,637 [INFO] Step[300/2713]: training loss : 0.9545443511009216 TRAIN  loss dict:  {'classification_loss': 0.9545443511009216}
2025-01-13 00:20:43,532 [INFO] Step[350/2713]: training loss : 0.9777326273918152 TRAIN  loss dict:  {'classification_loss': 0.9777326273918152}
2025-01-13 00:20:55,375 [INFO] Step[400/2713]: training loss : 0.952083580493927 TRAIN  loss dict:  {'classification_loss': 0.952083580493927}
2025-01-13 00:21:07,255 [INFO] Step[450/2713]: training loss : 0.9659696817398071 TRAIN  loss dict:  {'classification_loss': 0.9659696817398071}
2025-01-13 00:21:19,114 [INFO] Step[500/2713]: training loss : 0.9573998010158539 TRAIN  loss dict:  {'classification_loss': 0.9573998010158539}
2025-01-13 00:21:31,016 [INFO] Step[550/2713]: training loss : 0.9702526426315308 TRAIN  loss dict:  {'classification_loss': 0.9702526426315308}
2025-01-13 00:21:42,877 [INFO] Step[600/2713]: training loss : 0.960212299823761 TRAIN  loss dict:  {'classification_loss': 0.960212299823761}
2025-01-13 00:21:54,773 [INFO] Step[650/2713]: training loss : 0.9618499851226807 TRAIN  loss dict:  {'classification_loss': 0.9618499851226807}
2025-01-13 00:22:06,694 [INFO] Step[700/2713]: training loss : 0.9638615345954895 TRAIN  loss dict:  {'classification_loss': 0.9638615345954895}
2025-01-13 00:22:18,581 [INFO] Step[750/2713]: training loss : 0.9573177862167358 TRAIN  loss dict:  {'classification_loss': 0.9573177862167358}
2025-01-13 00:22:30,506 [INFO] Step[800/2713]: training loss : 0.9564665615558624 TRAIN  loss dict:  {'classification_loss': 0.9564665615558624}
2025-01-13 00:22:42,398 [INFO] Step[850/2713]: training loss : 0.9564356768131256 TRAIN  loss dict:  {'classification_loss': 0.9564356768131256}
2025-01-13 00:22:54,293 [INFO] Step[900/2713]: training loss : 0.955117484331131 TRAIN  loss dict:  {'classification_loss': 0.955117484331131}
2025-01-13 00:23:06,220 [INFO] Step[950/2713]: training loss : 0.9532712388038636 TRAIN  loss dict:  {'classification_loss': 0.9532712388038636}
2025-01-13 00:23:18,174 [INFO] Step[1000/2713]: training loss : 0.9599104011058808 TRAIN  loss dict:  {'classification_loss': 0.9599104011058808}
2025-01-13 00:23:30,090 [INFO] Step[1050/2713]: training loss : 0.9646430909633636 TRAIN  loss dict:  {'classification_loss': 0.9646430909633636}
2025-01-13 00:23:41,971 [INFO] Step[1100/2713]: training loss : 0.965364601612091 TRAIN  loss dict:  {'classification_loss': 0.965364601612091}
2025-01-13 00:23:53,853 [INFO] Step[1150/2713]: training loss : 0.9558330953121186 TRAIN  loss dict:  {'classification_loss': 0.9558330953121186}
2025-01-13 00:24:05,691 [INFO] Step[1200/2713]: training loss : 0.9620224153995514 TRAIN  loss dict:  {'classification_loss': 0.9620224153995514}
2025-01-13 00:24:17,558 [INFO] Step[1250/2713]: training loss : 0.9634961962699891 TRAIN  loss dict:  {'classification_loss': 0.9634961962699891}
2025-01-13 00:24:29,472 [INFO] Step[1300/2713]: training loss : 0.9583537220954895 TRAIN  loss dict:  {'classification_loss': 0.9583537220954895}
2025-01-13 00:24:41,389 [INFO] Step[1350/2713]: training loss : 0.9554005753993988 TRAIN  loss dict:  {'classification_loss': 0.9554005753993988}
2025-01-13 00:24:53,222 [INFO] Step[1400/2713]: training loss : 0.9572203052043915 TRAIN  loss dict:  {'classification_loss': 0.9572203052043915}
2025-01-13 00:25:05,127 [INFO] Step[1450/2713]: training loss : 0.9549438750743866 TRAIN  loss dict:  {'classification_loss': 0.9549438750743866}
2025-01-13 00:25:17,012 [INFO] Step[1500/2713]: training loss : 0.9678112030029297 TRAIN  loss dict:  {'classification_loss': 0.9678112030029297}
2025-01-13 00:25:28,894 [INFO] Step[1550/2713]: training loss : 0.961706691980362 TRAIN  loss dict:  {'classification_loss': 0.961706691980362}
2025-01-13 00:25:40,839 [INFO] Step[1600/2713]: training loss : 0.9604029512405395 TRAIN  loss dict:  {'classification_loss': 0.9604029512405395}
2025-01-13 00:25:52,744 [INFO] Step[1650/2713]: training loss : 0.9654957902431488 TRAIN  loss dict:  {'classification_loss': 0.9654957902431488}
2025-01-13 00:26:04,623 [INFO] Step[1700/2713]: training loss : 0.9596113646030426 TRAIN  loss dict:  {'classification_loss': 0.9596113646030426}
2025-01-13 00:26:16,475 [INFO] Step[1750/2713]: training loss : 0.9699070239067078 TRAIN  loss dict:  {'classification_loss': 0.9699070239067078}
2025-01-13 00:26:28,391 [INFO] Step[1800/2713]: training loss : 0.9540287959575653 TRAIN  loss dict:  {'classification_loss': 0.9540287959575653}
2025-01-13 00:26:40,299 [INFO] Step[1850/2713]: training loss : 0.9554597342014313 TRAIN  loss dict:  {'classification_loss': 0.9554597342014313}
2025-01-13 00:26:52,165 [INFO] Step[1900/2713]: training loss : 0.9551042079925537 TRAIN  loss dict:  {'classification_loss': 0.9551042079925537}
2025-01-13 00:27:04,068 [INFO] Step[1950/2713]: training loss : 0.9616929876804352 TRAIN  loss dict:  {'classification_loss': 0.9616929876804352}
2025-01-13 00:27:15,968 [INFO] Step[2000/2713]: training loss : 0.9560314249992371 TRAIN  loss dict:  {'classification_loss': 0.9560314249992371}
2025-01-13 00:27:27,882 [INFO] Step[2050/2713]: training loss : 0.9583127284049988 TRAIN  loss dict:  {'classification_loss': 0.9583127284049988}
2025-01-13 00:27:39,793 [INFO] Step[2100/2713]: training loss : 0.9565016555786133 TRAIN  loss dict:  {'classification_loss': 0.9565016555786133}
2025-01-13 00:27:51,694 [INFO] Step[2150/2713]: training loss : 0.9660743832588196 TRAIN  loss dict:  {'classification_loss': 0.9660743832588196}
2025-01-13 00:28:03,593 [INFO] Step[2200/2713]: training loss : 0.9554587924480438 TRAIN  loss dict:  {'classification_loss': 0.9554587924480438}
2025-01-13 00:28:15,517 [INFO] Step[2250/2713]: training loss : 0.9581120979785919 TRAIN  loss dict:  {'classification_loss': 0.9581120979785919}
2025-01-13 00:28:27,439 [INFO] Step[2300/2713]: training loss : 0.954258725643158 TRAIN  loss dict:  {'classification_loss': 0.954258725643158}
2025-01-13 00:28:39,333 [INFO] Step[2350/2713]: training loss : 0.9564592480659485 TRAIN  loss dict:  {'classification_loss': 0.9564592480659485}
2025-01-13 00:28:51,236 [INFO] Step[2400/2713]: training loss : 0.9558515310287475 TRAIN  loss dict:  {'classification_loss': 0.9558515310287475}
2025-01-13 00:29:03,131 [INFO] Step[2450/2713]: training loss : 0.9667518699169159 TRAIN  loss dict:  {'classification_loss': 0.9667518699169159}
2025-01-13 00:29:15,032 [INFO] Step[2500/2713]: training loss : 0.9550537598133088 TRAIN  loss dict:  {'classification_loss': 0.9550537598133088}
2025-01-13 00:29:26,904 [INFO] Step[2550/2713]: training loss : 0.9553808033466339 TRAIN  loss dict:  {'classification_loss': 0.9553808033466339}
2025-01-13 00:29:38,765 [INFO] Step[2600/2713]: training loss : 0.9603225839138031 TRAIN  loss dict:  {'classification_loss': 0.9603225839138031}
2025-01-13 00:29:50,635 [INFO] Step[2650/2713]: training loss : 0.9648848724365234 TRAIN  loss dict:  {'classification_loss': 0.9648848724365234}
2025-01-13 00:30:02,484 [INFO] Step[2700/2713]: training loss : 0.9582161831855774 TRAIN  loss dict:  {'classification_loss': 0.9582161831855774}
2025-01-13 00:31:30,654 [INFO] Label accuracies statistics:
2025-01-13 00:31:30,655 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.75, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 0.75, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 0.75, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.5, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 0.75, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.25, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 00:31:30,656 [INFO] [46] TRAIN  loss: 0.9595565825777654 acc: 0.9996314043494287
2025-01-13 00:31:30,656 [INFO] [46] TRAIN  loss dict: {'classification_loss': 0.9595565825777654}
2025-01-13 00:31:30,657 [INFO] [46] VALIDATION loss: 1.7045662454644541 VALIDATION acc: 0.8213166144200627
2025-01-13 00:31:30,657 [INFO] [46] VALIDATION loss dict: {'classification_loss': 1.7045662454644541}
2025-01-13 00:31:30,657 [INFO] 
2025-01-13 00:31:48,613 [INFO] Step[50/2713]: training loss : 0.9610581886768341 TRAIN  loss dict:  {'classification_loss': 0.9610581886768341}
2025-01-13 00:32:00,513 [INFO] Step[100/2713]: training loss : 0.9578639078140259 TRAIN  loss dict:  {'classification_loss': 0.9578639078140259}
2025-01-13 00:32:12,477 [INFO] Step[150/2713]: training loss : 0.9570223784446716 TRAIN  loss dict:  {'classification_loss': 0.9570223784446716}
2025-01-13 00:32:24,381 [INFO] Step[200/2713]: training loss : 0.961593154668808 TRAIN  loss dict:  {'classification_loss': 0.961593154668808}
2025-01-13 00:32:36,342 [INFO] Step[250/2713]: training loss : 0.9560294282436371 TRAIN  loss dict:  {'classification_loss': 0.9560294282436371}
2025-01-13 00:32:48,232 [INFO] Step[300/2713]: training loss : 0.9577033138275146 TRAIN  loss dict:  {'classification_loss': 0.9577033138275146}
2025-01-13 00:33:00,168 [INFO] Step[350/2713]: training loss : 0.9547588515281678 TRAIN  loss dict:  {'classification_loss': 0.9547588515281678}
2025-01-13 00:33:12,061 [INFO] Step[400/2713]: training loss : 0.9569618237018586 TRAIN  loss dict:  {'classification_loss': 0.9569618237018586}
2025-01-13 00:33:23,979 [INFO] Step[450/2713]: training loss : 0.9575710010528564 TRAIN  loss dict:  {'classification_loss': 0.9575710010528564}
2025-01-13 00:33:35,881 [INFO] Step[500/2713]: training loss : 0.9617922556400299 TRAIN  loss dict:  {'classification_loss': 0.9617922556400299}
2025-01-13 00:33:47,802 [INFO] Step[550/2713]: training loss : 0.9544693863391877 TRAIN  loss dict:  {'classification_loss': 0.9544693863391877}
2025-01-13 00:33:59,749 [INFO] Step[600/2713]: training loss : 0.9564278781414032 TRAIN  loss dict:  {'classification_loss': 0.9564278781414032}
2025-01-13 00:34:11,604 [INFO] Step[650/2713]: training loss : 0.954649829864502 TRAIN  loss dict:  {'classification_loss': 0.954649829864502}
2025-01-13 00:34:23,517 [INFO] Step[700/2713]: training loss : 0.9533835875988007 TRAIN  loss dict:  {'classification_loss': 0.9533835875988007}
2025-01-13 00:34:35,476 [INFO] Step[750/2713]: training loss : 0.9529277527332306 TRAIN  loss dict:  {'classification_loss': 0.9529277527332306}
2025-01-13 00:34:47,378 [INFO] Step[800/2713]: training loss : 0.9558798789978027 TRAIN  loss dict:  {'classification_loss': 0.9558798789978027}
2025-01-13 00:34:59,341 [INFO] Step[850/2713]: training loss : 0.9540071618556977 TRAIN  loss dict:  {'classification_loss': 0.9540071618556977}
2025-01-13 00:35:11,269 [INFO] Step[900/2713]: training loss : 0.9517052125930786 TRAIN  loss dict:  {'classification_loss': 0.9517052125930786}
2025-01-13 00:35:23,186 [INFO] Step[950/2713]: training loss : 0.9579786014556885 TRAIN  loss dict:  {'classification_loss': 0.9579786014556885}
2025-01-13 00:35:35,125 [INFO] Step[1000/2713]: training loss : 0.9568365740776063 TRAIN  loss dict:  {'classification_loss': 0.9568365740776063}
2025-01-13 00:35:47,015 [INFO] Step[1050/2713]: training loss : 0.9649527585506439 TRAIN  loss dict:  {'classification_loss': 0.9649527585506439}
2025-01-13 00:35:58,939 [INFO] Step[1100/2713]: training loss : 0.9580261886119843 TRAIN  loss dict:  {'classification_loss': 0.9580261886119843}
2025-01-13 00:36:10,840 [INFO] Step[1150/2713]: training loss : 0.9593555891513824 TRAIN  loss dict:  {'classification_loss': 0.9593555891513824}
2025-01-13 00:36:23,052 [INFO] Step[1200/2713]: training loss : 0.958771002292633 TRAIN  loss dict:  {'classification_loss': 0.958771002292633}
2025-01-13 00:36:35,441 [INFO] Step[1250/2713]: training loss : 0.9568228828907013 TRAIN  loss dict:  {'classification_loss': 0.9568228828907013}
2025-01-13 00:36:47,832 [INFO] Step[1300/2713]: training loss : 0.9522345507144928 TRAIN  loss dict:  {'classification_loss': 0.9522345507144928}
2025-01-13 00:37:00,217 [INFO] Step[1350/2713]: training loss : 0.9670027220249175 TRAIN  loss dict:  {'classification_loss': 0.9670027220249175}
2025-01-13 00:37:12,946 [INFO] Step[1400/2713]: training loss : 0.9508566689491272 TRAIN  loss dict:  {'classification_loss': 0.9508566689491272}
2025-01-13 00:37:25,363 [INFO] Step[1450/2713]: training loss : 0.9557486510276795 TRAIN  loss dict:  {'classification_loss': 0.9557486510276795}
2025-01-13 00:37:38,213 [INFO] Step[1500/2713]: training loss : 0.9537742459774017 TRAIN  loss dict:  {'classification_loss': 0.9537742459774017}
2025-01-13 00:37:52,080 [INFO] Step[1550/2713]: training loss : 0.9546160960197448 TRAIN  loss dict:  {'classification_loss': 0.9546160960197448}
2025-01-13 00:38:05,265 [INFO] Step[1600/2713]: training loss : 0.9526182663440704 TRAIN  loss dict:  {'classification_loss': 0.9526182663440704}
2025-01-13 00:38:17,461 [INFO] Step[1650/2713]: training loss : 0.9553131330013275 TRAIN  loss dict:  {'classification_loss': 0.9553131330013275}
2025-01-13 00:38:29,393 [INFO] Step[1700/2713]: training loss : 0.9628627908229828 TRAIN  loss dict:  {'classification_loss': 0.9628627908229828}
2025-01-13 00:38:41,349 [INFO] Step[1750/2713]: training loss : 0.9623408365249634 TRAIN  loss dict:  {'classification_loss': 0.9623408365249634}
2025-01-13 00:38:53,291 [INFO] Step[1800/2713]: training loss : 0.9615900957584381 TRAIN  loss dict:  {'classification_loss': 0.9615900957584381}
2025-01-13 00:39:05,196 [INFO] Step[1850/2713]: training loss : 0.9602289581298828 TRAIN  loss dict:  {'classification_loss': 0.9602289581298828}
2025-01-13 00:39:17,112 [INFO] Step[1900/2713]: training loss : 0.9567511320114136 TRAIN  loss dict:  {'classification_loss': 0.9567511320114136}
2025-01-13 00:39:29,112 [INFO] Step[1950/2713]: training loss : 0.9712723994255066 TRAIN  loss dict:  {'classification_loss': 0.9712723994255066}
2025-01-13 00:39:41,044 [INFO] Step[2000/2713]: training loss : 0.9562511801719665 TRAIN  loss dict:  {'classification_loss': 0.9562511801719665}
2025-01-13 00:39:52,983 [INFO] Step[2050/2713]: training loss : 0.9601901578903198 TRAIN  loss dict:  {'classification_loss': 0.9601901578903198}
2025-01-13 00:40:04,944 [INFO] Step[2100/2713]: training loss : 0.9571797287464142 TRAIN  loss dict:  {'classification_loss': 0.9571797287464142}
2025-01-13 00:40:16,883 [INFO] Step[2150/2713]: training loss : 0.9609244477748871 TRAIN  loss dict:  {'classification_loss': 0.9609244477748871}
2025-01-13 00:40:28,793 [INFO] Step[2200/2713]: training loss : 0.9542110621929168 TRAIN  loss dict:  {'classification_loss': 0.9542110621929168}
2025-01-13 00:40:40,732 [INFO] Step[2250/2713]: training loss : 0.962513325214386 TRAIN  loss dict:  {'classification_loss': 0.962513325214386}
2025-01-13 00:40:52,670 [INFO] Step[2300/2713]: training loss : 0.9638074469566346 TRAIN  loss dict:  {'classification_loss': 0.9638074469566346}
2025-01-13 00:41:04,611 [INFO] Step[2350/2713]: training loss : 0.9555771267414093 TRAIN  loss dict:  {'classification_loss': 0.9555771267414093}
2025-01-13 00:41:16,567 [INFO] Step[2400/2713]: training loss : 0.9561260938644409 TRAIN  loss dict:  {'classification_loss': 0.9561260938644409}
2025-01-13 00:41:28,491 [INFO] Step[2450/2713]: training loss : 0.9586325061321258 TRAIN  loss dict:  {'classification_loss': 0.9586325061321258}
2025-01-13 00:41:40,457 [INFO] Step[2500/2713]: training loss : 0.9563719642162323 TRAIN  loss dict:  {'classification_loss': 0.9563719642162323}
2025-01-13 00:41:52,413 [INFO] Step[2550/2713]: training loss : 0.9543544888496399 TRAIN  loss dict:  {'classification_loss': 0.9543544888496399}
2025-01-13 00:42:04,345 [INFO] Step[2600/2713]: training loss : 0.952244622707367 TRAIN  loss dict:  {'classification_loss': 0.952244622707367}
2025-01-13 00:42:16,313 [INFO] Step[2650/2713]: training loss : 0.9600838840007782 TRAIN  loss dict:  {'classification_loss': 0.9600838840007782}
2025-01-13 00:42:28,248 [INFO] Step[2700/2713]: training loss : 0.9717524099349976 TRAIN  loss dict:  {'classification_loss': 0.9717524099349976}
2025-01-13 00:43:56,117 [INFO] Label accuracies statistics:
2025-01-13 00:43:56,117 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.25, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 0.75, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 0.75, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 1.0, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.25, 355: 1.0, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-13 00:43:56,119 [INFO] [47] TRAIN  loss: 0.9579432183994036 acc: 0.9997542695662858
2025-01-13 00:43:56,119 [INFO] [47] TRAIN  loss dict: {'classification_loss': 0.9579432183994036}
2025-01-13 00:43:56,119 [INFO] [47] VALIDATION loss: 1.7364410150768166 VALIDATION acc: 0.8206896551724138
2025-01-13 00:43:56,119 [INFO] [47] VALIDATION loss dict: {'classification_loss': 1.7364410150768166}
2025-01-13 00:43:56,120 [INFO] 
2025-01-13 00:44:13,948 [INFO] Step[50/2713]: training loss : 0.9575739538669586 TRAIN  loss dict:  {'classification_loss': 0.9575739538669586}
2025-01-13 00:44:25,857 [INFO] Step[100/2713]: training loss : 0.9592624509334564 TRAIN  loss dict:  {'classification_loss': 0.9592624509334564}
2025-01-13 00:44:37,818 [INFO] Step[150/2713]: training loss : 0.9579788076877594 TRAIN  loss dict:  {'classification_loss': 0.9579788076877594}
2025-01-13 00:44:49,755 [INFO] Step[200/2713]: training loss : 0.9575178611278534 TRAIN  loss dict:  {'classification_loss': 0.9575178611278534}
2025-01-13 00:45:01,707 [INFO] Step[250/2713]: training loss : 0.9619020795822144 TRAIN  loss dict:  {'classification_loss': 0.9619020795822144}
2025-01-13 00:45:13,610 [INFO] Step[300/2713]: training loss : 0.9719877576828003 TRAIN  loss dict:  {'classification_loss': 0.9719877576828003}
2025-01-13 00:45:25,582 [INFO] Step[350/2713]: training loss : 0.9715031123161316 TRAIN  loss dict:  {'classification_loss': 0.9715031123161316}
2025-01-13 00:45:37,497 [INFO] Step[400/2713]: training loss : 0.9535007524490356 TRAIN  loss dict:  {'classification_loss': 0.9535007524490356}
2025-01-13 00:45:49,428 [INFO] Step[450/2713]: training loss : 0.9691779518127441 TRAIN  loss dict:  {'classification_loss': 0.9691779518127441}
2025-01-13 00:46:01,388 [INFO] Step[500/2713]: training loss : 0.9582340228557586 TRAIN  loss dict:  {'classification_loss': 0.9582340228557586}
2025-01-13 00:46:13,312 [INFO] Step[550/2713]: training loss : 0.9552952647209167 TRAIN  loss dict:  {'classification_loss': 0.9552952647209167}
2025-01-13 00:46:25,216 [INFO] Step[600/2713]: training loss : 0.9518627166748047 TRAIN  loss dict:  {'classification_loss': 0.9518627166748047}
2025-01-13 00:46:37,136 [INFO] Step[650/2713]: training loss : 0.9615884685516357 TRAIN  loss dict:  {'classification_loss': 0.9615884685516357}
2025-01-13 00:46:49,020 [INFO] Step[700/2713]: training loss : 0.9757622933387756 TRAIN  loss dict:  {'classification_loss': 0.9757622933387756}
2025-01-13 00:47:00,916 [INFO] Step[750/2713]: training loss : 0.9561415934562683 TRAIN  loss dict:  {'classification_loss': 0.9561415934562683}
2025-01-13 00:47:12,819 [INFO] Step[800/2713]: training loss : 0.9680987238883972 TRAIN  loss dict:  {'classification_loss': 0.9680987238883972}
2025-01-13 00:47:24,740 [INFO] Step[850/2713]: training loss : 0.9590244710445404 TRAIN  loss dict:  {'classification_loss': 0.9590244710445404}
2025-01-13 00:47:36,637 [INFO] Step[900/2713]: training loss : 0.9508600115776062 TRAIN  loss dict:  {'classification_loss': 0.9508600115776062}
2025-01-13 00:47:48,620 [INFO] Step[950/2713]: training loss : 0.9593741822242737 TRAIN  loss dict:  {'classification_loss': 0.9593741822242737}
2025-01-13 00:48:00,509 [INFO] Step[1000/2713]: training loss : 0.9579336595535278 TRAIN  loss dict:  {'classification_loss': 0.9579336595535278}
2025-01-13 00:48:12,418 [INFO] Step[1050/2713]: training loss : 0.9538694131374359 TRAIN  loss dict:  {'classification_loss': 0.9538694131374359}
2025-01-13 00:48:24,341 [INFO] Step[1100/2713]: training loss : 0.9644188666343689 TRAIN  loss dict:  {'classification_loss': 0.9644188666343689}
2025-01-13 00:48:36,243 [INFO] Step[1150/2713]: training loss : 0.9560897529125214 TRAIN  loss dict:  {'classification_loss': 0.9560897529125214}
2025-01-13 00:48:48,185 [INFO] Step[1200/2713]: training loss : 0.9531458258628845 TRAIN  loss dict:  {'classification_loss': 0.9531458258628845}
2025-01-13 00:49:00,100 [INFO] Step[1250/2713]: training loss : 0.9577765798568726 TRAIN  loss dict:  {'classification_loss': 0.9577765798568726}
2025-01-13 00:49:12,038 [INFO] Step[1300/2713]: training loss : 0.9622103476524353 TRAIN  loss dict:  {'classification_loss': 0.9622103476524353}
2025-01-13 00:49:23,972 [INFO] Step[1350/2713]: training loss : 0.956385862827301 TRAIN  loss dict:  {'classification_loss': 0.956385862827301}
2025-01-13 00:49:35,895 [INFO] Step[1400/2713]: training loss : 0.9560720419883728 TRAIN  loss dict:  {'classification_loss': 0.9560720419883728}
2025-01-13 00:49:47,793 [INFO] Step[1450/2713]: training loss : 0.9571561014652252 TRAIN  loss dict:  {'classification_loss': 0.9571561014652252}
2025-01-13 00:49:59,682 [INFO] Step[1500/2713]: training loss : 0.9549924778938294 TRAIN  loss dict:  {'classification_loss': 0.9549924778938294}
2025-01-13 00:50:11,587 [INFO] Step[1550/2713]: training loss : 0.9553414285182953 TRAIN  loss dict:  {'classification_loss': 0.9553414285182953}
2025-01-13 00:50:23,496 [INFO] Step[1600/2713]: training loss : 0.9652222132682801 TRAIN  loss dict:  {'classification_loss': 0.9652222132682801}
2025-01-13 00:50:35,419 [INFO] Step[1650/2713]: training loss : 0.9560933518409729 TRAIN  loss dict:  {'classification_loss': 0.9560933518409729}
2025-01-13 00:50:47,338 [INFO] Step[1700/2713]: training loss : 0.9607638168334961 TRAIN  loss dict:  {'classification_loss': 0.9607638168334961}
2025-01-13 00:50:59,276 [INFO] Step[1750/2713]: training loss : 0.9623136293888092 TRAIN  loss dict:  {'classification_loss': 0.9623136293888092}
2025-01-13 00:51:11,212 [INFO] Step[1800/2713]: training loss : 0.9630727791786193 TRAIN  loss dict:  {'classification_loss': 0.9630727791786193}
2025-01-13 00:51:23,099 [INFO] Step[1850/2713]: training loss : 0.9623262917995453 TRAIN  loss dict:  {'classification_loss': 0.9623262917995453}
2025-01-13 00:51:35,034 [INFO] Step[1900/2713]: training loss : 0.9546138799190521 TRAIN  loss dict:  {'classification_loss': 0.9546138799190521}
2025-01-13 00:51:46,984 [INFO] Step[1950/2713]: training loss : 0.9546027278900147 TRAIN  loss dict:  {'classification_loss': 0.9546027278900147}
2025-01-13 00:51:58,817 [INFO] Step[2000/2713]: training loss : 0.9565820717811584 TRAIN  loss dict:  {'classification_loss': 0.9565820717811584}
2025-01-13 00:52:10,741 [INFO] Step[2050/2713]: training loss : 0.9564956653118134 TRAIN  loss dict:  {'classification_loss': 0.9564956653118134}
2025-01-13 00:52:22,700 [INFO] Step[2100/2713]: training loss : 0.9693202435970306 TRAIN  loss dict:  {'classification_loss': 0.9693202435970306}
2025-01-13 00:52:34,621 [INFO] Step[2150/2713]: training loss : 0.9560993242263794 TRAIN  loss dict:  {'classification_loss': 0.9560993242263794}
2025-01-13 00:52:46,586 [INFO] Step[2200/2713]: training loss : 0.9570291781425476 TRAIN  loss dict:  {'classification_loss': 0.9570291781425476}
2025-01-13 00:52:58,529 [INFO] Step[2250/2713]: training loss : 0.9603800499439239 TRAIN  loss dict:  {'classification_loss': 0.9603800499439239}
2025-01-13 00:53:10,443 [INFO] Step[2300/2713]: training loss : 0.9648047709465026 TRAIN  loss dict:  {'classification_loss': 0.9648047709465026}
2025-01-13 00:53:22,427 [INFO] Step[2350/2713]: training loss : 0.9608996534347534 TRAIN  loss dict:  {'classification_loss': 0.9608996534347534}
2025-01-13 00:53:34,312 [INFO] Step[2400/2713]: training loss : 0.9618876242637634 TRAIN  loss dict:  {'classification_loss': 0.9618876242637634}
2025-01-13 00:53:46,247 [INFO] Step[2450/2713]: training loss : 0.9511324107646942 TRAIN  loss dict:  {'classification_loss': 0.9511324107646942}
2025-01-13 00:53:58,163 [INFO] Step[2500/2713]: training loss : 0.9589200258255005 TRAIN  loss dict:  {'classification_loss': 0.9589200258255005}
2025-01-13 00:54:10,094 [INFO] Step[2550/2713]: training loss : 0.9525279891490936 TRAIN  loss dict:  {'classification_loss': 0.9525279891490936}
2025-01-13 00:54:21,979 [INFO] Step[2600/2713]: training loss : 0.9588929450511933 TRAIN  loss dict:  {'classification_loss': 0.9588929450511933}
2025-01-13 00:54:33,871 [INFO] Step[2650/2713]: training loss : 0.9600486302375794 TRAIN  loss dict:  {'classification_loss': 0.9600486302375794}
2025-01-13 00:54:45,770 [INFO] Step[2700/2713]: training loss : 0.9670503997802734 TRAIN  loss dict:  {'classification_loss': 0.9670503997802734}
2025-01-13 00:56:21,621 [INFO] Label accuracies statistics:
2025-01-13 00:56:21,621 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.5, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 1.0, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.75, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 1.0, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 1.0, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-13 00:56:21,626 [INFO] [48] TRAIN  loss: 0.9594529732935463 acc: 0.9995085391325715
2025-01-13 00:56:21,626 [INFO] [48] TRAIN  loss dict: {'classification_loss': 0.9594529732935463}
2025-01-13 00:56:21,627 [INFO] [48] VALIDATION loss: 1.6726364829486473 VALIDATION acc: 0.8307210031347962
2025-01-13 00:56:21,627 [INFO] [48] VALIDATION loss dict: {'classification_loss': 1.6726364829486473}
2025-01-13 00:56:21,627 [INFO] 
2025-01-13 00:56:42,954 [INFO] Step[50/2713]: training loss : 0.9604885315895081 TRAIN  loss dict:  {'classification_loss': 0.9604885315895081}
2025-01-13 00:56:55,678 [INFO] Step[100/2713]: training loss : 0.959078129529953 TRAIN  loss dict:  {'classification_loss': 0.959078129529953}
2025-01-13 00:57:09,204 [INFO] Step[150/2713]: training loss : 0.9623914754390717 TRAIN  loss dict:  {'classification_loss': 0.9623914754390717}
2025-01-13 00:57:23,048 [INFO] Step[200/2713]: training loss : 0.9562848639488221 TRAIN  loss dict:  {'classification_loss': 0.9562848639488221}
2025-01-13 00:57:35,275 [INFO] Step[250/2713]: training loss : 0.9652605211734772 TRAIN  loss dict:  {'classification_loss': 0.9652605211734772}
2025-01-13 00:57:47,116 [INFO] Step[300/2713]: training loss : 0.9550835812091827 TRAIN  loss dict:  {'classification_loss': 0.9550835812091827}
2025-01-13 00:57:59,013 [INFO] Step[350/2713]: training loss : 0.9611953687667847 TRAIN  loss dict:  {'classification_loss': 0.9611953687667847}
2025-01-13 00:58:10,888 [INFO] Step[400/2713]: training loss : 0.9553457140922547 TRAIN  loss dict:  {'classification_loss': 0.9553457140922547}
2025-01-13 00:58:22,768 [INFO] Step[450/2713]: training loss : 0.960089030265808 TRAIN  loss dict:  {'classification_loss': 0.960089030265808}
2025-01-13 00:58:34,706 [INFO] Step[500/2713]: training loss : 0.9560319876670837 TRAIN  loss dict:  {'classification_loss': 0.9560319876670837}
2025-01-13 00:58:46,654 [INFO] Step[550/2713]: training loss : 0.9556743085384369 TRAIN  loss dict:  {'classification_loss': 0.9556743085384369}
2025-01-13 00:58:58,559 [INFO] Step[600/2713]: training loss : 0.9632764101028443 TRAIN  loss dict:  {'classification_loss': 0.9632764101028443}
2025-01-13 00:59:10,494 [INFO] Step[650/2713]: training loss : 0.9590458345413208 TRAIN  loss dict:  {'classification_loss': 0.9590458345413208}
2025-01-13 00:59:22,410 [INFO] Step[700/2713]: training loss : 0.9645587182044983 TRAIN  loss dict:  {'classification_loss': 0.9645587182044983}
2025-01-13 00:59:34,355 [INFO] Step[750/2713]: training loss : 0.9536947143077851 TRAIN  loss dict:  {'classification_loss': 0.9536947143077851}
2025-01-13 00:59:46,289 [INFO] Step[800/2713]: training loss : 0.9577691006660461 TRAIN  loss dict:  {'classification_loss': 0.9577691006660461}
2025-01-13 00:59:58,205 [INFO] Step[850/2713]: training loss : 0.9588306534290314 TRAIN  loss dict:  {'classification_loss': 0.9588306534290314}
2025-01-13 01:00:10,105 [INFO] Step[900/2713]: training loss : 0.9527092516422272 TRAIN  loss dict:  {'classification_loss': 0.9527092516422272}
2025-01-13 01:00:22,009 [INFO] Step[950/2713]: training loss : 0.9543523669242859 TRAIN  loss dict:  {'classification_loss': 0.9543523669242859}
2025-01-13 01:00:33,911 [INFO] Step[1000/2713]: training loss : 0.9681411409378051 TRAIN  loss dict:  {'classification_loss': 0.9681411409378051}
2025-01-13 01:00:45,825 [INFO] Step[1050/2713]: training loss : 0.9590354061126709 TRAIN  loss dict:  {'classification_loss': 0.9590354061126709}
2025-01-13 01:00:57,723 [INFO] Step[1100/2713]: training loss : 0.9648983526229858 TRAIN  loss dict:  {'classification_loss': 0.9648983526229858}
2025-01-13 01:01:09,600 [INFO] Step[1150/2713]: training loss : 0.9534119546413422 TRAIN  loss dict:  {'classification_loss': 0.9534119546413422}
2025-01-13 01:01:21,488 [INFO] Step[1200/2713]: training loss : 0.9618731689453125 TRAIN  loss dict:  {'classification_loss': 0.9618731689453125}
2025-01-13 01:01:33,358 [INFO] Step[1250/2713]: training loss : 0.9700178670883178 TRAIN  loss dict:  {'classification_loss': 0.9700178670883178}
2025-01-13 01:01:45,257 [INFO] Step[1300/2713]: training loss : 0.963403936624527 TRAIN  loss dict:  {'classification_loss': 0.963403936624527}
2025-01-13 01:01:57,153 [INFO] Step[1350/2713]: training loss : 0.9559883975982666 TRAIN  loss dict:  {'classification_loss': 0.9559883975982666}
2025-01-13 01:02:09,048 [INFO] Step[1400/2713]: training loss : 0.960744925737381 TRAIN  loss dict:  {'classification_loss': 0.960744925737381}
2025-01-13 01:02:20,916 [INFO] Step[1450/2713]: training loss : 0.958155733346939 TRAIN  loss dict:  {'classification_loss': 0.958155733346939}
2025-01-13 01:02:32,800 [INFO] Step[1500/2713]: training loss : 0.9610026931762695 TRAIN  loss dict:  {'classification_loss': 0.9610026931762695}
2025-01-13 01:02:44,701 [INFO] Step[1550/2713]: training loss : 0.9556429445743561 TRAIN  loss dict:  {'classification_loss': 0.9556429445743561}
2025-01-13 01:02:56,598 [INFO] Step[1600/2713]: training loss : 0.9588489270210266 TRAIN  loss dict:  {'classification_loss': 0.9588489270210266}
2025-01-13 01:03:08,534 [INFO] Step[1650/2713]: training loss : 0.9615416038036346 TRAIN  loss dict:  {'classification_loss': 0.9615416038036346}
2025-01-13 01:03:20,442 [INFO] Step[1700/2713]: training loss : 0.9557745659351349 TRAIN  loss dict:  {'classification_loss': 0.9557745659351349}
2025-01-13 01:03:32,348 [INFO] Step[1750/2713]: training loss : 0.9551674318313599 TRAIN  loss dict:  {'classification_loss': 0.9551674318313599}
2025-01-13 01:03:44,202 [INFO] Step[1800/2713]: training loss : 0.9569381380081177 TRAIN  loss dict:  {'classification_loss': 0.9569381380081177}
2025-01-13 01:03:56,127 [INFO] Step[1850/2713]: training loss : 0.9547999310493469 TRAIN  loss dict:  {'classification_loss': 0.9547999310493469}
2025-01-13 01:04:08,053 [INFO] Step[1900/2713]: training loss : 0.9508221137523651 TRAIN  loss dict:  {'classification_loss': 0.9508221137523651}
2025-01-13 01:04:20,006 [INFO] Step[1950/2713]: training loss : 0.9615750527381897 TRAIN  loss dict:  {'classification_loss': 0.9615750527381897}
2025-01-13 01:04:31,872 [INFO] Step[2000/2713]: training loss : 0.9604174315929412 TRAIN  loss dict:  {'classification_loss': 0.9604174315929412}
2025-01-13 01:04:43,768 [INFO] Step[2050/2713]: training loss : 0.9597228467464447 TRAIN  loss dict:  {'classification_loss': 0.9597228467464447}
2025-01-13 01:04:55,676 [INFO] Step[2100/2713]: training loss : 0.960823734998703 TRAIN  loss dict:  {'classification_loss': 0.960823734998703}
2025-01-13 01:05:07,519 [INFO] Step[2150/2713]: training loss : 0.9606609952449798 TRAIN  loss dict:  {'classification_loss': 0.9606609952449798}
2025-01-13 01:05:19,450 [INFO] Step[2200/2713]: training loss : 0.9518534314632415 TRAIN  loss dict:  {'classification_loss': 0.9518534314632415}
2025-01-13 01:05:31,339 [INFO] Step[2250/2713]: training loss : 0.9598125970363617 TRAIN  loss dict:  {'classification_loss': 0.9598125970363617}
2025-01-13 01:05:43,230 [INFO] Step[2300/2713]: training loss : 0.9539524340629577 TRAIN  loss dict:  {'classification_loss': 0.9539524340629577}
2025-01-13 01:05:55,150 [INFO] Step[2350/2713]: training loss : 0.9528089368343353 TRAIN  loss dict:  {'classification_loss': 0.9528089368343353}
2025-01-13 01:06:07,046 [INFO] Step[2400/2713]: training loss : 0.9766341292858124 TRAIN  loss dict:  {'classification_loss': 0.9766341292858124}
2025-01-13 01:06:18,921 [INFO] Step[2450/2713]: training loss : 0.9585221803188324 TRAIN  loss dict:  {'classification_loss': 0.9585221803188324}
2025-01-13 01:06:30,815 [INFO] Step[2500/2713]: training loss : 0.957347115278244 TRAIN  loss dict:  {'classification_loss': 0.957347115278244}
2025-01-13 01:06:42,744 [INFO] Step[2550/2713]: training loss : 0.957475700378418 TRAIN  loss dict:  {'classification_loss': 0.957475700378418}
2025-01-13 01:06:54,616 [INFO] Step[2600/2713]: training loss : 0.9582420921325684 TRAIN  loss dict:  {'classification_loss': 0.9582420921325684}
2025-01-13 01:07:06,572 [INFO] Step[2650/2713]: training loss : 0.9561300265789032 TRAIN  loss dict:  {'classification_loss': 0.9561300265789032}
2025-01-13 01:07:18,449 [INFO] Step[2700/2713]: training loss : 0.9695179092884064 TRAIN  loss dict:  {'classification_loss': 0.9695179092884064}
2025-01-13 01:08:47,841 [INFO] Label accuracies statistics:
2025-01-13 01:08:47,841 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.75, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 0.75, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-13 01:08:47,844 [INFO] [49] TRAIN  loss: 0.9590713051777681 acc: 0.9996314043494287
2025-01-13 01:08:47,844 [INFO] [49] TRAIN  loss dict: {'classification_loss': 0.9590713051777681}
2025-01-13 01:08:47,844 [INFO] [49] VALIDATION loss: 1.6942027253764016 VALIDATION acc: 0.8288401253918495
2025-01-13 01:08:47,844 [INFO] [49] VALIDATION loss dict: {'classification_loss': 1.6942027253764016}
2025-01-13 01:08:47,844 [INFO] 
2025-01-13 01:09:05,290 [INFO] Step[50/2713]: training loss : 0.958079743385315 TRAIN  loss dict:  {'classification_loss': 0.958079743385315}
2025-01-13 01:09:17,187 [INFO] Step[100/2713]: training loss : 0.9572899460792541 TRAIN  loss dict:  {'classification_loss': 0.9572899460792541}
2025-01-13 01:09:29,118 [INFO] Step[150/2713]: training loss : 0.9521867907047272 TRAIN  loss dict:  {'classification_loss': 0.9521867907047272}
2025-01-13 01:09:41,015 [INFO] Step[200/2713]: training loss : 0.9616676759719849 TRAIN  loss dict:  {'classification_loss': 0.9616676759719849}
2025-01-13 01:09:52,971 [INFO] Step[250/2713]: training loss : 0.9611015868186951 TRAIN  loss dict:  {'classification_loss': 0.9611015868186951}
2025-01-13 01:10:04,877 [INFO] Step[300/2713]: training loss : 0.9594317030906677 TRAIN  loss dict:  {'classification_loss': 0.9594317030906677}
2025-01-13 01:10:16,808 [INFO] Step[350/2713]: training loss : 0.9550417602062226 TRAIN  loss dict:  {'classification_loss': 0.9550417602062226}
2025-01-13 01:10:28,678 [INFO] Step[400/2713]: training loss : 0.9651754689216614 TRAIN  loss dict:  {'classification_loss': 0.9651754689216614}
2025-01-13 01:10:40,550 [INFO] Step[450/2713]: training loss : 0.9553692615032197 TRAIN  loss dict:  {'classification_loss': 0.9553692615032197}
2025-01-13 01:10:52,457 [INFO] Step[500/2713]: training loss : 0.958386595249176 TRAIN  loss dict:  {'classification_loss': 0.958386595249176}
2025-01-13 01:11:04,411 [INFO] Step[550/2713]: training loss : 0.9563796436786651 TRAIN  loss dict:  {'classification_loss': 0.9563796436786651}
2025-01-13 01:11:16,319 [INFO] Step[600/2713]: training loss : 0.9588401758670807 TRAIN  loss dict:  {'classification_loss': 0.9588401758670807}
2025-01-13 01:11:28,210 [INFO] Step[650/2713]: training loss : 0.9554752516746521 TRAIN  loss dict:  {'classification_loss': 0.9554752516746521}
2025-01-13 01:11:40,096 [INFO] Step[700/2713]: training loss : 0.9615432798862458 TRAIN  loss dict:  {'classification_loss': 0.9615432798862458}
2025-01-13 01:11:51,996 [INFO] Step[750/2713]: training loss : 0.9553178882598877 TRAIN  loss dict:  {'classification_loss': 0.9553178882598877}
2025-01-13 01:12:03,899 [INFO] Step[800/2713]: training loss : 0.9522701489925385 TRAIN  loss dict:  {'classification_loss': 0.9522701489925385}
2025-01-13 01:12:15,827 [INFO] Step[850/2713]: training loss : 0.9570016884803771 TRAIN  loss dict:  {'classification_loss': 0.9570016884803771}
2025-01-13 01:12:27,752 [INFO] Step[900/2713]: training loss : 0.9628211438655854 TRAIN  loss dict:  {'classification_loss': 0.9628211438655854}
2025-01-13 01:12:39,672 [INFO] Step[950/2713]: training loss : 0.952612144947052 TRAIN  loss dict:  {'classification_loss': 0.952612144947052}
2025-01-13 01:12:51,548 [INFO] Step[1000/2713]: training loss : 0.9607726430892944 TRAIN  loss dict:  {'classification_loss': 0.9607726430892944}
2025-01-13 01:13:03,435 [INFO] Step[1050/2713]: training loss : 0.9569349324703217 TRAIN  loss dict:  {'classification_loss': 0.9569349324703217}
2025-01-13 01:13:15,336 [INFO] Step[1100/2713]: training loss : 0.9506254327297211 TRAIN  loss dict:  {'classification_loss': 0.9506254327297211}
2025-01-13 01:13:27,251 [INFO] Step[1150/2713]: training loss : 0.953590316772461 TRAIN  loss dict:  {'classification_loss': 0.953590316772461}
2025-01-13 01:13:39,159 [INFO] Step[1200/2713]: training loss : 0.956148648262024 TRAIN  loss dict:  {'classification_loss': 0.956148648262024}
2025-01-13 01:13:51,065 [INFO] Step[1250/2713]: training loss : 0.9514753806591034 TRAIN  loss dict:  {'classification_loss': 0.9514753806591034}
2025-01-13 01:14:02,983 [INFO] Step[1300/2713]: training loss : 0.9547445237636566 TRAIN  loss dict:  {'classification_loss': 0.9547445237636566}
2025-01-13 01:14:14,895 [INFO] Step[1350/2713]: training loss : 0.9582763969898224 TRAIN  loss dict:  {'classification_loss': 0.9582763969898224}
2025-01-13 01:14:26,824 [INFO] Step[1400/2713]: training loss : 0.9654898524284363 TRAIN  loss dict:  {'classification_loss': 0.9654898524284363}
2025-01-13 01:14:39,104 [INFO] Step[1450/2713]: training loss : 0.954115868806839 TRAIN  loss dict:  {'classification_loss': 0.954115868806839}
2025-01-13 01:14:51,454 [INFO] Step[1500/2713]: training loss : 0.9604888975620269 TRAIN  loss dict:  {'classification_loss': 0.9604888975620269}
2025-01-13 01:15:03,862 [INFO] Step[1550/2713]: training loss : 0.9603141343593598 TRAIN  loss dict:  {'classification_loss': 0.9603141343593598}
2025-01-13 01:15:16,336 [INFO] Step[1600/2713]: training loss : 0.964873571395874 TRAIN  loss dict:  {'classification_loss': 0.964873571395874}
2025-01-13 01:15:29,112 [INFO] Step[1650/2713]: training loss : 0.9559071671962738 TRAIN  loss dict:  {'classification_loss': 0.9559071671962738}
2025-01-13 01:15:41,472 [INFO] Step[1700/2713]: training loss : 0.9516146469116211 TRAIN  loss dict:  {'classification_loss': 0.9516146469116211}
2025-01-13 01:15:54,311 [INFO] Step[1750/2713]: training loss : 0.9563218998908997 TRAIN  loss dict:  {'classification_loss': 0.9563218998908997}
2025-01-13 01:16:07,797 [INFO] Step[1800/2713]: training loss : 0.9560023999214172 TRAIN  loss dict:  {'classification_loss': 0.9560023999214172}
2025-01-13 01:16:21,202 [INFO] Step[1850/2713]: training loss : 0.9539082360267639 TRAIN  loss dict:  {'classification_loss': 0.9539082360267639}
2025-01-13 01:16:33,387 [INFO] Step[1900/2713]: training loss : 0.9642007184028626 TRAIN  loss dict:  {'classification_loss': 0.9642007184028626}
2025-01-13 01:16:45,341 [INFO] Step[1950/2713]: training loss : 0.9549191677570343 TRAIN  loss dict:  {'classification_loss': 0.9549191677570343}
2025-01-13 01:16:57,295 [INFO] Step[2000/2713]: training loss : 0.9492373502254486 TRAIN  loss dict:  {'classification_loss': 0.9492373502254486}
2025-01-13 01:17:09,189 [INFO] Step[2050/2713]: training loss : 0.9563307344913483 TRAIN  loss dict:  {'classification_loss': 0.9563307344913483}
2025-01-13 01:17:21,066 [INFO] Step[2100/2713]: training loss : 0.9911876106262207 TRAIN  loss dict:  {'classification_loss': 0.9911876106262207}
2025-01-13 01:17:32,979 [INFO] Step[2150/2713]: training loss : 0.9527907121181488 TRAIN  loss dict:  {'classification_loss': 0.9527907121181488}
2025-01-13 01:17:44,912 [INFO] Step[2200/2713]: training loss : 0.9557084286212921 TRAIN  loss dict:  {'classification_loss': 0.9557084286212921}
2025-01-13 01:17:56,844 [INFO] Step[2250/2713]: training loss : 0.9562989437580108 TRAIN  loss dict:  {'classification_loss': 0.9562989437580108}
2025-01-13 01:18:08,828 [INFO] Step[2300/2713]: training loss : 0.9569556152820587 TRAIN  loss dict:  {'classification_loss': 0.9569556152820587}
2025-01-13 01:18:20,800 [INFO] Step[2350/2713]: training loss : 0.9684830224514007 TRAIN  loss dict:  {'classification_loss': 0.9684830224514007}
2025-01-13 01:18:32,727 [INFO] Step[2400/2713]: training loss : 0.9752797257900238 TRAIN  loss dict:  {'classification_loss': 0.9752797257900238}
2025-01-13 01:18:44,645 [INFO] Step[2450/2713]: training loss : 0.984093747138977 TRAIN  loss dict:  {'classification_loss': 0.984093747138977}
2025-01-13 01:18:56,567 [INFO] Step[2500/2713]: training loss : 0.9514770519733429 TRAIN  loss dict:  {'classification_loss': 0.9514770519733429}
2025-01-13 01:19:08,575 [INFO] Step[2550/2713]: training loss : 0.9655455815792083 TRAIN  loss dict:  {'classification_loss': 0.9655455815792083}
2025-01-13 01:19:20,522 [INFO] Step[2600/2713]: training loss : 0.9645962262153626 TRAIN  loss dict:  {'classification_loss': 0.9645962262153626}
2025-01-13 01:19:32,500 [INFO] Step[2650/2713]: training loss : 0.962145459651947 TRAIN  loss dict:  {'classification_loss': 0.962145459651947}
2025-01-13 01:19:44,397 [INFO] Step[2700/2713]: training loss : 0.9578721082210541 TRAIN  loss dict:  {'classification_loss': 0.9578721082210541}
2025-01-13 01:21:10,794 [INFO] Label accuracies statistics:
2025-01-13 01:21:10,794 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 1.0, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 0.75, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 1.0, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 0.75, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.5, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-13 01:21:11,719 [INFO] [50] TRAIN  loss: 0.958929076521677 acc: 0.9992628086988573
2025-01-13 01:21:11,719 [INFO] [50] TRAIN  loss dict: {'classification_loss': 0.958929076521677}
2025-01-13 01:21:11,719 [INFO] [50] VALIDATION loss: 1.6560923114306945 VALIDATION acc: 0.8394984326018808
2025-01-13 01:21:11,719 [INFO] [50] VALIDATION loss dict: {'classification_loss': 1.6560923114306945}
2025-01-13 01:21:11,720 [INFO] 
2025-01-13 01:21:29,320 [INFO] Step[50/2713]: training loss : 0.9591646075248719 TRAIN  loss dict:  {'classification_loss': 0.9591646075248719}
2025-01-13 01:21:41,308 [INFO] Step[100/2713]: training loss : 0.9525275135040283 TRAIN  loss dict:  {'classification_loss': 0.9525275135040283}
2025-01-13 01:21:53,245 [INFO] Step[150/2713]: training loss : 0.9586646747589112 TRAIN  loss dict:  {'classification_loss': 0.9586646747589112}
2025-01-13 01:22:05,202 [INFO] Step[200/2713]: training loss : 0.960952113866806 TRAIN  loss dict:  {'classification_loss': 0.960952113866806}
2025-01-13 01:22:17,158 [INFO] Step[250/2713]: training loss : 0.9540330862998962 TRAIN  loss dict:  {'classification_loss': 0.9540330862998962}
2025-01-13 01:22:29,125 [INFO] Step[300/2713]: training loss : 0.9572897708415985 TRAIN  loss dict:  {'classification_loss': 0.9572897708415985}
2025-01-13 01:22:41,067 [INFO] Step[350/2713]: training loss : 0.9529108238220215 TRAIN  loss dict:  {'classification_loss': 0.9529108238220215}
2025-01-13 01:22:53,007 [INFO] Step[400/2713]: training loss : 0.9541457307338714 TRAIN  loss dict:  {'classification_loss': 0.9541457307338714}
2025-01-13 01:23:04,984 [INFO] Step[450/2713]: training loss : 0.9493805301189423 TRAIN  loss dict:  {'classification_loss': 0.9493805301189423}
2025-01-13 01:23:16,971 [INFO] Step[500/2713]: training loss : 0.9500068354606629 TRAIN  loss dict:  {'classification_loss': 0.9500068354606629}
2025-01-13 01:23:28,936 [INFO] Step[550/2713]: training loss : 0.9687538373470307 TRAIN  loss dict:  {'classification_loss': 0.9687538373470307}
2025-01-13 01:23:40,861 [INFO] Step[600/2713]: training loss : 0.9644047546386719 TRAIN  loss dict:  {'classification_loss': 0.9644047546386719}
2025-01-13 01:23:52,785 [INFO] Step[650/2713]: training loss : 0.9513170993328095 TRAIN  loss dict:  {'classification_loss': 0.9513170993328095}
2025-01-13 01:24:04,727 [INFO] Step[700/2713]: training loss : 0.9600907135009765 TRAIN  loss dict:  {'classification_loss': 0.9600907135009765}
2025-01-13 01:24:16,664 [INFO] Step[750/2713]: training loss : 0.9578439438343048 TRAIN  loss dict:  {'classification_loss': 0.9578439438343048}
2025-01-13 01:24:28,587 [INFO] Step[800/2713]: training loss : 0.9509429776668549 TRAIN  loss dict:  {'classification_loss': 0.9509429776668549}
2025-01-13 01:24:40,534 [INFO] Step[850/2713]: training loss : 0.9514262521266937 TRAIN  loss dict:  {'classification_loss': 0.9514262521266937}
2025-01-13 01:24:52,489 [INFO] Step[900/2713]: training loss : 0.9508191061019897 TRAIN  loss dict:  {'classification_loss': 0.9508191061019897}
2025-01-13 01:25:04,434 [INFO] Step[950/2713]: training loss : 0.9518920946121215 TRAIN  loss dict:  {'classification_loss': 0.9518920946121215}
2025-01-13 01:25:16,369 [INFO] Step[1000/2713]: training loss : 0.9618622779846191 TRAIN  loss dict:  {'classification_loss': 0.9618622779846191}
2025-01-13 01:25:28,305 [INFO] Step[1050/2713]: training loss : 0.9510874497890472 TRAIN  loss dict:  {'classification_loss': 0.9510874497890472}
2025-01-13 01:25:40,222 [INFO] Step[1100/2713]: training loss : 0.9496106457710266 TRAIN  loss dict:  {'classification_loss': 0.9496106457710266}
2025-01-13 01:25:52,158 [INFO] Step[1150/2713]: training loss : 0.9591783034801483 TRAIN  loss dict:  {'classification_loss': 0.9591783034801483}
2025-01-13 01:26:04,100 [INFO] Step[1200/2713]: training loss : 0.9522707009315491 TRAIN  loss dict:  {'classification_loss': 0.9522707009315491}
2025-01-13 01:26:16,057 [INFO] Step[1250/2713]: training loss : 0.9538316595554351 TRAIN  loss dict:  {'classification_loss': 0.9538316595554351}
2025-01-13 01:26:28,016 [INFO] Step[1300/2713]: training loss : 0.9548669743537903 TRAIN  loss dict:  {'classification_loss': 0.9548669743537903}
2025-01-13 01:26:39,974 [INFO] Step[1350/2713]: training loss : 0.9542634582519531 TRAIN  loss dict:  {'classification_loss': 0.9542634582519531}
2025-01-13 01:26:51,948 [INFO] Step[1400/2713]: training loss : 0.9478882789611817 TRAIN  loss dict:  {'classification_loss': 0.9478882789611817}
2025-01-13 01:27:03,891 [INFO] Step[1450/2713]: training loss : 0.9515288603305817 TRAIN  loss dict:  {'classification_loss': 0.9515288603305817}
2025-01-13 01:27:15,836 [INFO] Step[1500/2713]: training loss : 0.9529656612873078 TRAIN  loss dict:  {'classification_loss': 0.9529656612873078}
2025-01-13 01:27:27,775 [INFO] Step[1550/2713]: training loss : 0.9545010077953339 TRAIN  loss dict:  {'classification_loss': 0.9545010077953339}
2025-01-13 01:27:39,739 [INFO] Step[1600/2713]: training loss : 0.9594356787204742 TRAIN  loss dict:  {'classification_loss': 0.9594356787204742}
2025-01-13 01:27:51,747 [INFO] Step[1650/2713]: training loss : 0.95369096159935 TRAIN  loss dict:  {'classification_loss': 0.95369096159935}
2025-01-13 01:28:03,714 [INFO] Step[1700/2713]: training loss : 0.9525061690807343 TRAIN  loss dict:  {'classification_loss': 0.9525061690807343}
2025-01-13 01:28:15,669 [INFO] Step[1750/2713]: training loss : 0.9523597002029419 TRAIN  loss dict:  {'classification_loss': 0.9523597002029419}
2025-01-13 01:28:27,625 [INFO] Step[1800/2713]: training loss : 0.9565072464942932 TRAIN  loss dict:  {'classification_loss': 0.9565072464942932}
2025-01-13 01:28:39,609 [INFO] Step[1850/2713]: training loss : 0.9525559103488922 TRAIN  loss dict:  {'classification_loss': 0.9525559103488922}
2025-01-13 01:28:51,584 [INFO] Step[1900/2713]: training loss : 0.9527457988262177 TRAIN  loss dict:  {'classification_loss': 0.9527457988262177}
2025-01-13 01:29:03,524 [INFO] Step[1950/2713]: training loss : 0.9584811818599701 TRAIN  loss dict:  {'classification_loss': 0.9584811818599701}
2025-01-13 01:29:15,421 [INFO] Step[2000/2713]: training loss : 0.9586604058742523 TRAIN  loss dict:  {'classification_loss': 0.9586604058742523}
2025-01-13 01:29:27,361 [INFO] Step[2050/2713]: training loss : 0.9675441181659699 TRAIN  loss dict:  {'classification_loss': 0.9675441181659699}
2025-01-13 01:29:39,311 [INFO] Step[2100/2713]: training loss : 0.9507166540622711 TRAIN  loss dict:  {'classification_loss': 0.9507166540622711}
2025-01-13 01:29:51,243 [INFO] Step[2150/2713]: training loss : 0.9516681921482086 TRAIN  loss dict:  {'classification_loss': 0.9516681921482086}
2025-01-13 01:30:03,199 [INFO] Step[2200/2713]: training loss : 0.9513241529464722 TRAIN  loss dict:  {'classification_loss': 0.9513241529464722}
2025-01-13 01:30:15,134 [INFO] Step[2250/2713]: training loss : 0.9531047165393829 TRAIN  loss dict:  {'classification_loss': 0.9531047165393829}
2025-01-13 01:30:27,090 [INFO] Step[2300/2713]: training loss : 0.9585561943054199 TRAIN  loss dict:  {'classification_loss': 0.9585561943054199}
2025-01-13 01:30:39,095 [INFO] Step[2350/2713]: training loss : 0.9561144518852234 TRAIN  loss dict:  {'classification_loss': 0.9561144518852234}
2025-01-13 01:30:51,013 [INFO] Step[2400/2713]: training loss : 0.95305349111557 TRAIN  loss dict:  {'classification_loss': 0.95305349111557}
2025-01-13 01:31:02,988 [INFO] Step[2450/2713]: training loss : 0.9458104336261749 TRAIN  loss dict:  {'classification_loss': 0.9458104336261749}
2025-01-13 01:31:14,938 [INFO] Step[2500/2713]: training loss : 0.9528910231590271 TRAIN  loss dict:  {'classification_loss': 0.9528910231590271}
2025-01-13 01:31:26,889 [INFO] Step[2550/2713]: training loss : 0.9551995754241943 TRAIN  loss dict:  {'classification_loss': 0.9551995754241943}
2025-01-13 01:31:38,821 [INFO] Step[2600/2713]: training loss : 0.9512272703647614 TRAIN  loss dict:  {'classification_loss': 0.9512272703647614}
2025-01-13 01:31:50,776 [INFO] Step[2650/2713]: training loss : 0.9523853921890258 TRAIN  loss dict:  {'classification_loss': 0.9523853921890258}
2025-01-13 01:32:02,728 [INFO] Step[2700/2713]: training loss : 0.9509244096279145 TRAIN  loss dict:  {'classification_loss': 0.9509244096279145}
2025-01-13 01:33:28,542 [INFO] Label accuracies statistics:
2025-01-13 01:33:28,543 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 0.75, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.5, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 1.0, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.0, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.5, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.5, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 1.0, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 01:33:28,544 [INFO] [51] TRAIN  loss: 0.9545938982499556 acc: 0.9992628086988573
2025-01-13 01:33:28,544 [INFO] [51] TRAIN  loss dict: {'classification_loss': 0.9545938982499556}
2025-01-13 01:33:28,544 [INFO] [51] VALIDATION loss: 1.672736069089488 VALIDATION acc: 0.8351097178683385
2025-01-13 01:33:28,544 [INFO] [51] VALIDATION loss dict: {'classification_loss': 1.672736069089488}
2025-01-13 01:33:28,545 [INFO] 
2025-01-13 01:33:47,802 [INFO] Step[50/2713]: training loss : 0.9473836731910705 TRAIN  loss dict:  {'classification_loss': 0.9473836731910705}
2025-01-13 01:34:00,195 [INFO] Step[100/2713]: training loss : 0.951202734708786 TRAIN  loss dict:  {'classification_loss': 0.951202734708786}
2025-01-13 01:34:12,432 [INFO] Step[150/2713]: training loss : 0.9559230732917786 TRAIN  loss dict:  {'classification_loss': 0.9559230732917786}
2025-01-13 01:34:25,299 [INFO] Step[200/2713]: training loss : 0.9490322172641754 TRAIN  loss dict:  {'classification_loss': 0.9490322172641754}
2025-01-13 01:34:37,603 [INFO] Step[250/2713]: training loss : 0.9541094601154327 TRAIN  loss dict:  {'classification_loss': 0.9541094601154327}
2025-01-13 01:34:50,538 [INFO] Step[300/2713]: training loss : 0.952490838766098 TRAIN  loss dict:  {'classification_loss': 0.952490838766098}
2025-01-13 01:35:04,701 [INFO] Step[350/2713]: training loss : 0.9543925333023071 TRAIN  loss dict:  {'classification_loss': 0.9543925333023071}
2025-01-13 01:35:18,057 [INFO] Step[400/2713]: training loss : 0.9497418928146363 TRAIN  loss dict:  {'classification_loss': 0.9497418928146363}
2025-01-13 01:35:30,163 [INFO] Step[450/2713]: training loss : 0.9546920895576477 TRAIN  loss dict:  {'classification_loss': 0.9546920895576477}
2025-01-13 01:35:42,026 [INFO] Step[500/2713]: training loss : 0.9490264749526978 TRAIN  loss dict:  {'classification_loss': 0.9490264749526978}
2025-01-13 01:35:53,924 [INFO] Step[550/2713]: training loss : 0.9530977320671081 TRAIN  loss dict:  {'classification_loss': 0.9530977320671081}
2025-01-13 01:36:05,812 [INFO] Step[600/2713]: training loss : 0.9534173822402954 TRAIN  loss dict:  {'classification_loss': 0.9534173822402954}
2025-01-13 01:36:17,676 [INFO] Step[650/2713]: training loss : 0.9522846078872681 TRAIN  loss dict:  {'classification_loss': 0.9522846078872681}
2025-01-13 01:36:29,564 [INFO] Step[700/2713]: training loss : 0.9519356739521027 TRAIN  loss dict:  {'classification_loss': 0.9519356739521027}
2025-01-13 01:36:41,450 [INFO] Step[750/2713]: training loss : 0.9511823344230652 TRAIN  loss dict:  {'classification_loss': 0.9511823344230652}
2025-01-13 01:36:53,330 [INFO] Step[800/2713]: training loss : 0.9680532360076904 TRAIN  loss dict:  {'classification_loss': 0.9680532360076904}
2025-01-13 01:37:05,251 [INFO] Step[850/2713]: training loss : 0.9511303496360779 TRAIN  loss dict:  {'classification_loss': 0.9511303496360779}
2025-01-13 01:37:17,159 [INFO] Step[900/2713]: training loss : 0.9550200080871583 TRAIN  loss dict:  {'classification_loss': 0.9550200080871583}
2025-01-13 01:37:29,046 [INFO] Step[950/2713]: training loss : 0.954015941619873 TRAIN  loss dict:  {'classification_loss': 0.954015941619873}
2025-01-13 01:37:40,961 [INFO] Step[1000/2713]: training loss : 0.9524603927135468 TRAIN  loss dict:  {'classification_loss': 0.9524603927135468}
2025-01-13 01:37:52,834 [INFO] Step[1050/2713]: training loss : 0.9469795215129853 TRAIN  loss dict:  {'classification_loss': 0.9469795215129853}
2025-01-13 01:38:04,703 [INFO] Step[1100/2713]: training loss : 0.9705733692646027 TRAIN  loss dict:  {'classification_loss': 0.9705733692646027}
2025-01-13 01:38:16,627 [INFO] Step[1150/2713]: training loss : 0.9531057548522949 TRAIN  loss dict:  {'classification_loss': 0.9531057548522949}
2025-01-13 01:38:28,547 [INFO] Step[1200/2713]: training loss : 0.9526379024982452 TRAIN  loss dict:  {'classification_loss': 0.9526379024982452}
2025-01-13 01:38:40,442 [INFO] Step[1250/2713]: training loss : 0.9483425211906433 TRAIN  loss dict:  {'classification_loss': 0.9483425211906433}
2025-01-13 01:38:52,328 [INFO] Step[1300/2713]: training loss : 0.9518632543087006 TRAIN  loss dict:  {'classification_loss': 0.9518632543087006}
2025-01-13 01:39:04,274 [INFO] Step[1350/2713]: training loss : 0.9534814584255219 TRAIN  loss dict:  {'classification_loss': 0.9534814584255219}
2025-01-13 01:39:16,143 [INFO] Step[1400/2713]: training loss : 0.9843640673160553 TRAIN  loss dict:  {'classification_loss': 0.9843640673160553}
2025-01-13 01:39:28,091 [INFO] Step[1450/2713]: training loss : 0.9509293377399445 TRAIN  loss dict:  {'classification_loss': 0.9509293377399445}
2025-01-13 01:39:40,035 [INFO] Step[1500/2713]: training loss : 0.9488291370868683 TRAIN  loss dict:  {'classification_loss': 0.9488291370868683}
2025-01-13 01:39:51,872 [INFO] Step[1550/2713]: training loss : 0.9491513574123382 TRAIN  loss dict:  {'classification_loss': 0.9491513574123382}
2025-01-13 01:40:03,762 [INFO] Step[1600/2713]: training loss : 0.9477330946922302 TRAIN  loss dict:  {'classification_loss': 0.9477330946922302}
2025-01-13 01:40:15,676 [INFO] Step[1650/2713]: training loss : 0.9551124227046967 TRAIN  loss dict:  {'classification_loss': 0.9551124227046967}
2025-01-13 01:40:27,553 [INFO] Step[1700/2713]: training loss : 0.9446570599079132 TRAIN  loss dict:  {'classification_loss': 0.9446570599079132}
2025-01-13 01:40:39,464 [INFO] Step[1750/2713]: training loss : 0.9468992519378662 TRAIN  loss dict:  {'classification_loss': 0.9468992519378662}
2025-01-13 01:40:51,402 [INFO] Step[1800/2713]: training loss : 0.9505517327785492 TRAIN  loss dict:  {'classification_loss': 0.9505517327785492}
2025-01-13 01:41:03,300 [INFO] Step[1850/2713]: training loss : 0.9520487344264984 TRAIN  loss dict:  {'classification_loss': 0.9520487344264984}
2025-01-13 01:41:15,183 [INFO] Step[1900/2713]: training loss : 0.9546855580806732 TRAIN  loss dict:  {'classification_loss': 0.9546855580806732}
2025-01-13 01:41:27,097 [INFO] Step[1950/2713]: training loss : 0.9583483266830445 TRAIN  loss dict:  {'classification_loss': 0.9583483266830445}
2025-01-13 01:41:38,969 [INFO] Step[2000/2713]: training loss : 0.9497257351875306 TRAIN  loss dict:  {'classification_loss': 0.9497257351875306}
2025-01-13 01:41:50,918 [INFO] Step[2050/2713]: training loss : 0.9503474807739258 TRAIN  loss dict:  {'classification_loss': 0.9503474807739258}
2025-01-13 01:42:02,783 [INFO] Step[2100/2713]: training loss : 0.9495929193496704 TRAIN  loss dict:  {'classification_loss': 0.9495929193496704}
2025-01-13 01:42:14,678 [INFO] Step[2150/2713]: training loss : 0.9514424562454223 TRAIN  loss dict:  {'classification_loss': 0.9514424562454223}
2025-01-13 01:42:26,533 [INFO] Step[2200/2713]: training loss : 0.9532423865795135 TRAIN  loss dict:  {'classification_loss': 0.9532423865795135}
2025-01-13 01:42:38,494 [INFO] Step[2250/2713]: training loss : 0.9524112260341644 TRAIN  loss dict:  {'classification_loss': 0.9524112260341644}
2025-01-13 01:42:50,451 [INFO] Step[2300/2713]: training loss : 0.9578042078018189 TRAIN  loss dict:  {'classification_loss': 0.9578042078018189}
2025-01-13 01:43:02,362 [INFO] Step[2350/2713]: training loss : 0.9529743170738221 TRAIN  loss dict:  {'classification_loss': 0.9529743170738221}
2025-01-13 01:43:14,273 [INFO] Step[2400/2713]: training loss : 0.9590192985534668 TRAIN  loss dict:  {'classification_loss': 0.9590192985534668}
2025-01-13 01:43:26,207 [INFO] Step[2450/2713]: training loss : 0.9468338871002198 TRAIN  loss dict:  {'classification_loss': 0.9468338871002198}
2025-01-13 01:43:38,123 [INFO] Step[2500/2713]: training loss : 0.9495187699794769 TRAIN  loss dict:  {'classification_loss': 0.9495187699794769}
2025-01-13 01:43:50,025 [INFO] Step[2550/2713]: training loss : 0.9525784599781036 TRAIN  loss dict:  {'classification_loss': 0.9525784599781036}
2025-01-13 01:44:01,934 [INFO] Step[2600/2713]: training loss : 0.9468192410469055 TRAIN  loss dict:  {'classification_loss': 0.9468192410469055}
2025-01-13 01:44:13,821 [INFO] Step[2650/2713]: training loss : 0.9522743582725525 TRAIN  loss dict:  {'classification_loss': 0.9522743582725525}
2025-01-13 01:44:25,742 [INFO] Step[2700/2713]: training loss : 0.9492322182655335 TRAIN  loss dict:  {'classification_loss': 0.9492322182655335}
2025-01-13 01:45:54,246 [INFO] Label accuracies statistics:
2025-01-13 01:45:54,246 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.25, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 1.0, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.25, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 1.0, 204: 0.75, 205: 1.0, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.5, 291: 1.0, 292: 0.75, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 0.75, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 01:45:54,251 [INFO] [52] TRAIN  loss: 0.9528775321044387 acc: 0.9995085391325715
2025-01-13 01:45:54,251 [INFO] [52] TRAIN  loss dict: {'classification_loss': 0.9528775321044387}
2025-01-13 01:45:54,251 [INFO] [52] VALIDATION loss: 1.6682999935141183 VALIDATION acc: 0.8369905956112853
2025-01-13 01:45:54,251 [INFO] [52] VALIDATION loss dict: {'classification_loss': 1.6682999935141183}
2025-01-13 01:45:54,252 [INFO] 
2025-01-13 01:46:12,034 [INFO] Step[50/2713]: training loss : 0.9530456709861755 TRAIN  loss dict:  {'classification_loss': 0.9530456709861755}
2025-01-13 01:46:23,968 [INFO] Step[100/2713]: training loss : 0.9497550642490387 TRAIN  loss dict:  {'classification_loss': 0.9497550642490387}
2025-01-13 01:46:35,889 [INFO] Step[150/2713]: training loss : 0.9532699990272522 TRAIN  loss dict:  {'classification_loss': 0.9532699990272522}
2025-01-13 01:46:47,818 [INFO] Step[200/2713]: training loss : 0.9507529711723328 TRAIN  loss dict:  {'classification_loss': 0.9507529711723328}
2025-01-13 01:46:59,725 [INFO] Step[250/2713]: training loss : 0.9481122970581055 TRAIN  loss dict:  {'classification_loss': 0.9481122970581055}
2025-01-13 01:47:11,634 [INFO] Step[300/2713]: training loss : 0.9492179179191589 TRAIN  loss dict:  {'classification_loss': 0.9492179179191589}
2025-01-13 01:47:23,609 [INFO] Step[350/2713]: training loss : 0.9533487141132355 TRAIN  loss dict:  {'classification_loss': 0.9533487141132355}
2025-01-13 01:47:35,497 [INFO] Step[400/2713]: training loss : 0.9477036154270172 TRAIN  loss dict:  {'classification_loss': 0.9477036154270172}
2025-01-13 01:47:47,441 [INFO] Step[450/2713]: training loss : 0.950143119096756 TRAIN  loss dict:  {'classification_loss': 0.950143119096756}
2025-01-13 01:47:59,369 [INFO] Step[500/2713]: training loss : 0.9503059113025665 TRAIN  loss dict:  {'classification_loss': 0.9503059113025665}
2025-01-13 01:48:11,280 [INFO] Step[550/2713]: training loss : 0.952194402217865 TRAIN  loss dict:  {'classification_loss': 0.952194402217865}
2025-01-13 01:48:23,228 [INFO] Step[600/2713]: training loss : 0.9495356059074402 TRAIN  loss dict:  {'classification_loss': 0.9495356059074402}
2025-01-13 01:48:35,197 [INFO] Step[650/2713]: training loss : 0.9503138148784638 TRAIN  loss dict:  {'classification_loss': 0.9503138148784638}
2025-01-13 01:48:47,168 [INFO] Step[700/2713]: training loss : 0.9484958183765412 TRAIN  loss dict:  {'classification_loss': 0.9484958183765412}
2025-01-13 01:48:59,098 [INFO] Step[750/2713]: training loss : 0.9683349907398224 TRAIN  loss dict:  {'classification_loss': 0.9683349907398224}
2025-01-13 01:49:11,027 [INFO] Step[800/2713]: training loss : 0.9492884612083435 TRAIN  loss dict:  {'classification_loss': 0.9492884612083435}
2025-01-13 01:49:22,974 [INFO] Step[850/2713]: training loss : 0.9522443914413452 TRAIN  loss dict:  {'classification_loss': 0.9522443914413452}
2025-01-13 01:49:34,890 [INFO] Step[900/2713]: training loss : 0.9551025235652923 TRAIN  loss dict:  {'classification_loss': 0.9551025235652923}
2025-01-13 01:49:46,786 [INFO] Step[950/2713]: training loss : 0.9513906931877136 TRAIN  loss dict:  {'classification_loss': 0.9513906931877136}
2025-01-13 01:49:58,753 [INFO] Step[1000/2713]: training loss : 0.9534376907348633 TRAIN  loss dict:  {'classification_loss': 0.9534376907348633}
2025-01-13 01:50:10,685 [INFO] Step[1050/2713]: training loss : 0.9521072399616242 TRAIN  loss dict:  {'classification_loss': 0.9521072399616242}
2025-01-13 01:50:22,633 [INFO] Step[1100/2713]: training loss : 0.9483691048622132 TRAIN  loss dict:  {'classification_loss': 0.9483691048622132}
2025-01-13 01:50:34,559 [INFO] Step[1150/2713]: training loss : 0.9522668945789338 TRAIN  loss dict:  {'classification_loss': 0.9522668945789338}
2025-01-13 01:50:46,505 [INFO] Step[1200/2713]: training loss : 0.9515003621578216 TRAIN  loss dict:  {'classification_loss': 0.9515003621578216}
2025-01-13 01:50:58,472 [INFO] Step[1250/2713]: training loss : 0.950959757566452 TRAIN  loss dict:  {'classification_loss': 0.950959757566452}
2025-01-13 01:51:10,389 [INFO] Step[1300/2713]: training loss : 0.9517242991924286 TRAIN  loss dict:  {'classification_loss': 0.9517242991924286}
2025-01-13 01:51:22,332 [INFO] Step[1350/2713]: training loss : 0.9487881731986999 TRAIN  loss dict:  {'classification_loss': 0.9487881731986999}
2025-01-13 01:51:34,265 [INFO] Step[1400/2713]: training loss : 0.9509876453876496 TRAIN  loss dict:  {'classification_loss': 0.9509876453876496}
2025-01-13 01:51:46,217 [INFO] Step[1450/2713]: training loss : 0.9491072916984558 TRAIN  loss dict:  {'classification_loss': 0.9491072916984558}
2025-01-13 01:51:58,101 [INFO] Step[1500/2713]: training loss : 0.9512256312370301 TRAIN  loss dict:  {'classification_loss': 0.9512256312370301}
2025-01-13 01:52:10,107 [INFO] Step[1550/2713]: training loss : 0.9484156441688537 TRAIN  loss dict:  {'classification_loss': 0.9484156441688537}
2025-01-13 01:52:22,027 [INFO] Step[1600/2713]: training loss : 0.9563893747329711 TRAIN  loss dict:  {'classification_loss': 0.9563893747329711}
2025-01-13 01:52:34,303 [INFO] Step[1650/2713]: training loss : 0.9496709978580475 TRAIN  loss dict:  {'classification_loss': 0.9496709978580475}
2025-01-13 01:52:46,849 [INFO] Step[1700/2713]: training loss : 0.9492625558376312 TRAIN  loss dict:  {'classification_loss': 0.9492625558376312}
2025-01-13 01:52:59,140 [INFO] Step[1750/2713]: training loss : 0.9536583364009857 TRAIN  loss dict:  {'classification_loss': 0.9536583364009857}
2025-01-13 01:53:11,593 [INFO] Step[1800/2713]: training loss : 0.9539491069316864 TRAIN  loss dict:  {'classification_loss': 0.9539491069316864}
2025-01-13 01:53:24,059 [INFO] Step[1850/2713]: training loss : 0.9484781098365783 TRAIN  loss dict:  {'classification_loss': 0.9484781098365783}
2025-01-13 01:53:36,456 [INFO] Step[1900/2713]: training loss : 0.9525259494781494 TRAIN  loss dict:  {'classification_loss': 0.9525259494781494}
2025-01-13 01:53:49,740 [INFO] Step[1950/2713]: training loss : 0.9497197997570038 TRAIN  loss dict:  {'classification_loss': 0.9497197997570038}
2025-01-13 01:54:03,791 [INFO] Step[2000/2713]: training loss : 0.9491841852664947 TRAIN  loss dict:  {'classification_loss': 0.9491841852664947}
2025-01-13 01:54:16,426 [INFO] Step[2050/2713]: training loss : 0.9556170928478241 TRAIN  loss dict:  {'classification_loss': 0.9556170928478241}
2025-01-13 01:54:28,482 [INFO] Step[2100/2713]: training loss : 0.9515765452384949 TRAIN  loss dict:  {'classification_loss': 0.9515765452384949}
2025-01-13 01:54:40,376 [INFO] Step[2150/2713]: training loss : 0.9514267146587372 TRAIN  loss dict:  {'classification_loss': 0.9514267146587372}
2025-01-13 01:54:52,254 [INFO] Step[2200/2713]: training loss : 0.9540096497535706 TRAIN  loss dict:  {'classification_loss': 0.9540096497535706}
2025-01-13 01:55:04,098 [INFO] Step[2250/2713]: training loss : 0.9498571312427521 TRAIN  loss dict:  {'classification_loss': 0.9498571312427521}
2025-01-13 01:55:15,978 [INFO] Step[2300/2713]: training loss : 0.9614514696598053 TRAIN  loss dict:  {'classification_loss': 0.9614514696598053}
2025-01-13 01:55:27,836 [INFO] Step[2350/2713]: training loss : 0.9515824949741364 TRAIN  loss dict:  {'classification_loss': 0.9515824949741364}
2025-01-13 01:55:39,731 [INFO] Step[2400/2713]: training loss : 0.9500582659244537 TRAIN  loss dict:  {'classification_loss': 0.9500582659244537}
2025-01-13 01:55:51,645 [INFO] Step[2450/2713]: training loss : 0.9528544890880585 TRAIN  loss dict:  {'classification_loss': 0.9528544890880585}
2025-01-13 01:56:03,591 [INFO] Step[2500/2713]: training loss : 0.950952730178833 TRAIN  loss dict:  {'classification_loss': 0.950952730178833}
2025-01-13 01:56:15,584 [INFO] Step[2550/2713]: training loss : 0.9767619073390961 TRAIN  loss dict:  {'classification_loss': 0.9767619073390961}
2025-01-13 01:56:27,494 [INFO] Step[2600/2713]: training loss : 0.9531982374191285 TRAIN  loss dict:  {'classification_loss': 0.9531982374191285}
2025-01-13 01:56:39,407 [INFO] Step[2650/2713]: training loss : 0.9528484499454498 TRAIN  loss dict:  {'classification_loss': 0.9528484499454498}
2025-01-13 01:56:51,349 [INFO] Step[2700/2713]: training loss : 0.95903604388237 TRAIN  loss dict:  {'classification_loss': 0.95903604388237}
2025-01-13 01:58:19,024 [INFO] Label accuracies statistics:
2025-01-13 01:58:19,024 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.5, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 1.0, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.5, 216: 0.0, 217: 0.75, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 1.0, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.25, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 01:58:20,888 [INFO] [53] TRAIN  loss: 0.9523022977399773 acc: 0.9996314043494287
2025-01-13 01:58:20,888 [INFO] [53] TRAIN  loss dict: {'classification_loss': 0.9523022977399773}
2025-01-13 01:58:20,888 [INFO] [53] VALIDATION loss: 1.6535342951914422 VALIDATION acc: 0.8426332288401254
2025-01-13 01:58:20,888 [INFO] [53] VALIDATION loss dict: {'classification_loss': 1.6535342951914422}
2025-01-13 01:58:20,888 [INFO] 
2025-01-13 01:58:38,781 [INFO] Step[50/2713]: training loss : 0.9485937976837158 TRAIN  loss dict:  {'classification_loss': 0.9485937976837158}
2025-01-13 01:58:50,716 [INFO] Step[100/2713]: training loss : 0.9506968522071838 TRAIN  loss dict:  {'classification_loss': 0.9506968522071838}
2025-01-13 01:59:02,692 [INFO] Step[150/2713]: training loss : 0.954424569606781 TRAIN  loss dict:  {'classification_loss': 0.954424569606781}
2025-01-13 01:59:14,619 [INFO] Step[200/2713]: training loss : 0.9496995353698731 TRAIN  loss dict:  {'classification_loss': 0.9496995353698731}
2025-01-13 01:59:26,530 [INFO] Step[250/2713]: training loss : 0.9525490391254425 TRAIN  loss dict:  {'classification_loss': 0.9525490391254425}
2025-01-13 01:59:38,403 [INFO] Step[300/2713]: training loss : 0.9529516410827636 TRAIN  loss dict:  {'classification_loss': 0.9529516410827636}
2025-01-13 01:59:50,344 [INFO] Step[350/2713]: training loss : 0.9552420854568482 TRAIN  loss dict:  {'classification_loss': 0.9552420854568482}
2025-01-13 02:00:02,269 [INFO] Step[400/2713]: training loss : 0.9549738800525666 TRAIN  loss dict:  {'classification_loss': 0.9549738800525666}
2025-01-13 02:00:14,206 [INFO] Step[450/2713]: training loss : 0.9483174085617065 TRAIN  loss dict:  {'classification_loss': 0.9483174085617065}
2025-01-13 02:00:26,078 [INFO] Step[500/2713]: training loss : 0.9611888980865478 TRAIN  loss dict:  {'classification_loss': 0.9611888980865478}
2025-01-13 02:00:38,020 [INFO] Step[550/2713]: training loss : 0.9470710825920104 TRAIN  loss dict:  {'classification_loss': 0.9470710825920104}
2025-01-13 02:00:49,980 [INFO] Step[600/2713]: training loss : 0.9613071990013122 TRAIN  loss dict:  {'classification_loss': 0.9613071990013122}
2025-01-13 02:01:01,932 [INFO] Step[650/2713]: training loss : 0.9500514614582062 TRAIN  loss dict:  {'classification_loss': 0.9500514614582062}
2025-01-13 02:01:13,843 [INFO] Step[700/2713]: training loss : 0.9507659602165223 TRAIN  loss dict:  {'classification_loss': 0.9507659602165223}
2025-01-13 02:01:25,782 [INFO] Step[750/2713]: training loss : 0.949277400970459 TRAIN  loss dict:  {'classification_loss': 0.949277400970459}
2025-01-13 02:01:37,718 [INFO] Step[800/2713]: training loss : 0.9547161483764648 TRAIN  loss dict:  {'classification_loss': 0.9547161483764648}
2025-01-13 02:01:49,664 [INFO] Step[850/2713]: training loss : 0.9485481762886048 TRAIN  loss dict:  {'classification_loss': 0.9485481762886048}
2025-01-13 02:02:01,568 [INFO] Step[900/2713]: training loss : 0.9520899677276611 TRAIN  loss dict:  {'classification_loss': 0.9520899677276611}
2025-01-13 02:02:13,526 [INFO] Step[950/2713]: training loss : 0.9453670871257782 TRAIN  loss dict:  {'classification_loss': 0.9453670871257782}
2025-01-13 02:02:25,415 [INFO] Step[1000/2713]: training loss : 0.9506299901008606 TRAIN  loss dict:  {'classification_loss': 0.9506299901008606}
2025-01-13 02:02:37,390 [INFO] Step[1050/2713]: training loss : 0.9483684921264648 TRAIN  loss dict:  {'classification_loss': 0.9483684921264648}
2025-01-13 02:02:49,317 [INFO] Step[1100/2713]: training loss : 0.945776436328888 TRAIN  loss dict:  {'classification_loss': 0.945776436328888}
2025-01-13 02:03:01,247 [INFO] Step[1150/2713]: training loss : 0.9548315107822418 TRAIN  loss dict:  {'classification_loss': 0.9548315107822418}
2025-01-13 02:03:13,143 [INFO] Step[1200/2713]: training loss : 0.9493624746799469 TRAIN  loss dict:  {'classification_loss': 0.9493624746799469}
2025-01-13 02:03:25,083 [INFO] Step[1250/2713]: training loss : 0.9476993370056153 TRAIN  loss dict:  {'classification_loss': 0.9476993370056153}
2025-01-13 02:03:36,993 [INFO] Step[1300/2713]: training loss : 0.9583584380149841 TRAIN  loss dict:  {'classification_loss': 0.9583584380149841}
2025-01-13 02:03:48,888 [INFO] Step[1350/2713]: training loss : 0.949043447971344 TRAIN  loss dict:  {'classification_loss': 0.949043447971344}
2025-01-13 02:04:00,806 [INFO] Step[1400/2713]: training loss : 0.9506690049171448 TRAIN  loss dict:  {'classification_loss': 0.9506690049171448}
2025-01-13 02:04:12,725 [INFO] Step[1450/2713]: training loss : 0.9513543689250946 TRAIN  loss dict:  {'classification_loss': 0.9513543689250946}
2025-01-13 02:04:24,629 [INFO] Step[1500/2713]: training loss : 0.9471026694774628 TRAIN  loss dict:  {'classification_loss': 0.9471026694774628}
2025-01-13 02:04:36,551 [INFO] Step[1550/2713]: training loss : 0.9525190138816834 TRAIN  loss dict:  {'classification_loss': 0.9525190138816834}
2025-01-13 02:04:48,479 [INFO] Step[1600/2713]: training loss : 0.9505620336532593 TRAIN  loss dict:  {'classification_loss': 0.9505620336532593}
2025-01-13 02:05:00,386 [INFO] Step[1650/2713]: training loss : 0.958230721950531 TRAIN  loss dict:  {'classification_loss': 0.958230721950531}
2025-01-13 02:05:12,296 [INFO] Step[1700/2713]: training loss : 0.957507997751236 TRAIN  loss dict:  {'classification_loss': 0.957507997751236}
2025-01-13 02:05:24,218 [INFO] Step[1750/2713]: training loss : 0.9843491041660308 TRAIN  loss dict:  {'classification_loss': 0.9843491041660308}
2025-01-13 02:05:36,119 [INFO] Step[1800/2713]: training loss : 0.9586080515384674 TRAIN  loss dict:  {'classification_loss': 0.9586080515384674}
2025-01-13 02:05:48,080 [INFO] Step[1850/2713]: training loss : 0.9524231684207917 TRAIN  loss dict:  {'classification_loss': 0.9524231684207917}
2025-01-13 02:05:59,978 [INFO] Step[1900/2713]: training loss : 0.9460373783111572 TRAIN  loss dict:  {'classification_loss': 0.9460373783111572}
2025-01-13 02:06:11,904 [INFO] Step[1950/2713]: training loss : 0.9481659746170044 TRAIN  loss dict:  {'classification_loss': 0.9481659746170044}
2025-01-13 02:06:23,835 [INFO] Step[2000/2713]: training loss : 0.9526655328273773 TRAIN  loss dict:  {'classification_loss': 0.9526655328273773}
2025-01-13 02:06:35,742 [INFO] Step[2050/2713]: training loss : 0.954321151971817 TRAIN  loss dict:  {'classification_loss': 0.954321151971817}
2025-01-13 02:06:47,644 [INFO] Step[2100/2713]: training loss : 0.9511109912395477 TRAIN  loss dict:  {'classification_loss': 0.9511109912395477}
2025-01-13 02:06:59,577 [INFO] Step[2150/2713]: training loss : 0.9518449556827545 TRAIN  loss dict:  {'classification_loss': 0.9518449556827545}
2025-01-13 02:07:11,539 [INFO] Step[2200/2713]: training loss : 0.9544405329227448 TRAIN  loss dict:  {'classification_loss': 0.9544405329227448}
2025-01-13 02:07:23,481 [INFO] Step[2250/2713]: training loss : 0.9517661309242249 TRAIN  loss dict:  {'classification_loss': 0.9517661309242249}
2025-01-13 02:07:35,400 [INFO] Step[2300/2713]: training loss : 0.9564933454990387 TRAIN  loss dict:  {'classification_loss': 0.9564933454990387}
2025-01-13 02:07:47,288 [INFO] Step[2350/2713]: training loss : 0.9574615669250488 TRAIN  loss dict:  {'classification_loss': 0.9574615669250488}
2025-01-13 02:07:59,167 [INFO] Step[2400/2713]: training loss : 0.9503044426441193 TRAIN  loss dict:  {'classification_loss': 0.9503044426441193}
2025-01-13 02:08:11,070 [INFO] Step[2450/2713]: training loss : 0.954616471529007 TRAIN  loss dict:  {'classification_loss': 0.954616471529007}
2025-01-13 02:08:23,023 [INFO] Step[2500/2713]: training loss : 0.9486178755760193 TRAIN  loss dict:  {'classification_loss': 0.9486178755760193}
2025-01-13 02:08:34,903 [INFO] Step[2550/2713]: training loss : 0.9495313119888306 TRAIN  loss dict:  {'classification_loss': 0.9495313119888306}
2025-01-13 02:08:46,821 [INFO] Step[2600/2713]: training loss : 0.9540281105041504 TRAIN  loss dict:  {'classification_loss': 0.9540281105041504}
2025-01-13 02:08:58,759 [INFO] Step[2650/2713]: training loss : 0.9501816499233245 TRAIN  loss dict:  {'classification_loss': 0.9501816499233245}
2025-01-13 02:09:10,687 [INFO] Step[2700/2713]: training loss : 0.9514612913131714 TRAIN  loss dict:  {'classification_loss': 0.9514612913131714}
2025-01-13 02:10:38,184 [INFO] Label accuracies statistics:
2025-01-13 02:10:38,185 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.25, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 1.0, 159: 0.75, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 1.0, 277: 1.0, 278: 1.0, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.75, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 02:10:38,186 [INFO] [54] TRAIN  loss: 0.9525202224714221 acc: 0.9996314043494287
2025-01-13 02:10:38,186 [INFO] [54] TRAIN  loss dict: {'classification_loss': 0.9525202224714221}
2025-01-13 02:10:38,187 [INFO] [54] VALIDATION loss: 1.6814348753681756 VALIDATION acc: 0.8369905956112853
2025-01-13 02:10:38,187 [INFO] [54] VALIDATION loss dict: {'classification_loss': 1.6814348753681756}
2025-01-13 02:10:38,187 [INFO] 
2025-01-13 02:10:55,815 [INFO] Step[50/2713]: training loss : 0.9512628448009491 TRAIN  loss dict:  {'classification_loss': 0.9512628448009491}
2025-01-13 02:11:07,711 [INFO] Step[100/2713]: training loss : 0.9521911180019379 TRAIN  loss dict:  {'classification_loss': 0.9521911180019379}
2025-01-13 02:11:19,625 [INFO] Step[150/2713]: training loss : 0.9488828980922699 TRAIN  loss dict:  {'classification_loss': 0.9488828980922699}
2025-01-13 02:11:31,855 [INFO] Step[200/2713]: training loss : 0.9540809988975525 TRAIN  loss dict:  {'classification_loss': 0.9540809988975525}
2025-01-13 02:11:44,291 [INFO] Step[250/2713]: training loss : 0.9522320401668548 TRAIN  loss dict:  {'classification_loss': 0.9522320401668548}
2025-01-13 02:11:56,648 [INFO] Step[300/2713]: training loss : 0.9565132963657379 TRAIN  loss dict:  {'classification_loss': 0.9565132963657379}
2025-01-13 02:12:09,102 [INFO] Step[350/2713]: training loss : 0.9469638633728027 TRAIN  loss dict:  {'classification_loss': 0.9469638633728027}
2025-01-13 02:12:21,606 [INFO] Step[400/2713]: training loss : 0.9571290457248688 TRAIN  loss dict:  {'classification_loss': 0.9571290457248688}
2025-01-13 02:12:33,997 [INFO] Step[450/2713]: training loss : 0.9545646345615387 TRAIN  loss dict:  {'classification_loss': 0.9545646345615387}
2025-01-13 02:12:47,374 [INFO] Step[500/2713]: training loss : 0.9492679226398468 TRAIN  loss dict:  {'classification_loss': 0.9492679226398468}
2025-01-13 02:13:00,946 [INFO] Step[550/2713]: training loss : 0.9509204769134522 TRAIN  loss dict:  {'classification_loss': 0.9509204769134522}
2025-01-13 02:13:14,247 [INFO] Step[600/2713]: training loss : 0.9543275940418243 TRAIN  loss dict:  {'classification_loss': 0.9543275940418243}
2025-01-13 02:13:26,263 [INFO] Step[650/2713]: training loss : 0.9485519015789032 TRAIN  loss dict:  {'classification_loss': 0.9485519015789032}
2025-01-13 02:13:38,176 [INFO] Step[700/2713]: training loss : 0.9648743760585785 TRAIN  loss dict:  {'classification_loss': 0.9648743760585785}
2025-01-13 02:13:50,045 [INFO] Step[750/2713]: training loss : 0.9509233927726746 TRAIN  loss dict:  {'classification_loss': 0.9509233927726746}
2025-01-13 02:14:01,942 [INFO] Step[800/2713]: training loss : 0.9497111129760742 TRAIN  loss dict:  {'classification_loss': 0.9497111129760742}
2025-01-13 02:14:13,818 [INFO] Step[850/2713]: training loss : 0.9494236791133881 TRAIN  loss dict:  {'classification_loss': 0.9494236791133881}
2025-01-13 02:14:25,730 [INFO] Step[900/2713]: training loss : 0.9680635297298431 TRAIN  loss dict:  {'classification_loss': 0.9680635297298431}
2025-01-13 02:14:37,627 [INFO] Step[950/2713]: training loss : 0.9491758334636688 TRAIN  loss dict:  {'classification_loss': 0.9491758334636688}
2025-01-13 02:14:49,511 [INFO] Step[1000/2713]: training loss : 0.9492506313323975 TRAIN  loss dict:  {'classification_loss': 0.9492506313323975}
2025-01-13 02:15:01,439 [INFO] Step[1050/2713]: training loss : 0.9515548551082611 TRAIN  loss dict:  {'classification_loss': 0.9515548551082611}
2025-01-13 02:15:13,324 [INFO] Step[1100/2713]: training loss : 0.9462373149394989 TRAIN  loss dict:  {'classification_loss': 0.9462373149394989}
2025-01-13 02:15:25,224 [INFO] Step[1150/2713]: training loss : 0.9497028684616089 TRAIN  loss dict:  {'classification_loss': 0.9497028684616089}
2025-01-13 02:15:37,116 [INFO] Step[1200/2713]: training loss : 0.9477409064769745 TRAIN  loss dict:  {'classification_loss': 0.9477409064769745}
2025-01-13 02:15:49,028 [INFO] Step[1250/2713]: training loss : 0.9504986727237701 TRAIN  loss dict:  {'classification_loss': 0.9504986727237701}
2025-01-13 02:16:00,919 [INFO] Step[1300/2713]: training loss : 0.9482827138900757 TRAIN  loss dict:  {'classification_loss': 0.9482827138900757}
2025-01-13 02:16:12,804 [INFO] Step[1350/2713]: training loss : 0.947411550283432 TRAIN  loss dict:  {'classification_loss': 0.947411550283432}
2025-01-13 02:16:24,696 [INFO] Step[1400/2713]: training loss : 0.948274804353714 TRAIN  loss dict:  {'classification_loss': 0.948274804353714}
2025-01-13 02:16:36,609 [INFO] Step[1450/2713]: training loss : 0.9489216077327728 TRAIN  loss dict:  {'classification_loss': 0.9489216077327728}
2025-01-13 02:16:48,557 [INFO] Step[1500/2713]: training loss : 0.9513988542556763 TRAIN  loss dict:  {'classification_loss': 0.9513988542556763}
2025-01-13 02:17:00,444 [INFO] Step[1550/2713]: training loss : 0.9539914691448211 TRAIN  loss dict:  {'classification_loss': 0.9539914691448211}
2025-01-13 02:17:12,331 [INFO] Step[1600/2713]: training loss : 0.9478226220607757 TRAIN  loss dict:  {'classification_loss': 0.9478226220607757}
2025-01-13 02:17:24,254 [INFO] Step[1650/2713]: training loss : 0.949610093832016 TRAIN  loss dict:  {'classification_loss': 0.949610093832016}
2025-01-13 02:17:36,163 [INFO] Step[1700/2713]: training loss : 0.9475710093975067 TRAIN  loss dict:  {'classification_loss': 0.9475710093975067}
2025-01-13 02:17:48,093 [INFO] Step[1750/2713]: training loss : 0.9535639321804047 TRAIN  loss dict:  {'classification_loss': 0.9535639321804047}
2025-01-13 02:17:59,995 [INFO] Step[1800/2713]: training loss : 0.9475418269634247 TRAIN  loss dict:  {'classification_loss': 0.9475418269634247}
2025-01-13 02:18:11,881 [INFO] Step[1850/2713]: training loss : 0.949758917093277 TRAIN  loss dict:  {'classification_loss': 0.949758917093277}
2025-01-13 02:18:23,754 [INFO] Step[1900/2713]: training loss : 0.9504344403743744 TRAIN  loss dict:  {'classification_loss': 0.9504344403743744}
2025-01-13 02:18:35,608 [INFO] Step[1950/2713]: training loss : 0.9554749953746796 TRAIN  loss dict:  {'classification_loss': 0.9554749953746796}
2025-01-13 02:18:47,480 [INFO] Step[2000/2713]: training loss : 0.9490217506885529 TRAIN  loss dict:  {'classification_loss': 0.9490217506885529}
2025-01-13 02:18:59,378 [INFO] Step[2050/2713]: training loss : 0.945253187417984 TRAIN  loss dict:  {'classification_loss': 0.945253187417984}
2025-01-13 02:19:11,262 [INFO] Step[2100/2713]: training loss : 0.9474295246601104 TRAIN  loss dict:  {'classification_loss': 0.9474295246601104}
2025-01-13 02:19:23,183 [INFO] Step[2150/2713]: training loss : 0.9500672399997712 TRAIN  loss dict:  {'classification_loss': 0.9500672399997712}
2025-01-13 02:19:35,086 [INFO] Step[2200/2713]: training loss : 0.9535065996646881 TRAIN  loss dict:  {'classification_loss': 0.9535065996646881}
2025-01-13 02:19:47,014 [INFO] Step[2250/2713]: training loss : 0.9506906282901764 TRAIN  loss dict:  {'classification_loss': 0.9506906282901764}
2025-01-13 02:19:58,914 [INFO] Step[2300/2713]: training loss : 0.9546657228469848 TRAIN  loss dict:  {'classification_loss': 0.9546657228469848}
2025-01-13 02:20:10,825 [INFO] Step[2350/2713]: training loss : 0.9467811334133148 TRAIN  loss dict:  {'classification_loss': 0.9467811334133148}
2025-01-13 02:20:22,795 [INFO] Step[2400/2713]: training loss : 0.9501180398464203 TRAIN  loss dict:  {'classification_loss': 0.9501180398464203}
2025-01-13 02:20:34,710 [INFO] Step[2450/2713]: training loss : 0.9533526611328125 TRAIN  loss dict:  {'classification_loss': 0.9533526611328125}
2025-01-13 02:20:46,641 [INFO] Step[2500/2713]: training loss : 0.9482318758964539 TRAIN  loss dict:  {'classification_loss': 0.9482318758964539}
2025-01-13 02:20:58,532 [INFO] Step[2550/2713]: training loss : 0.9513951170444489 TRAIN  loss dict:  {'classification_loss': 0.9513951170444489}
2025-01-13 02:21:10,371 [INFO] Step[2600/2713]: training loss : 0.9510500752925872 TRAIN  loss dict:  {'classification_loss': 0.9510500752925872}
2025-01-13 02:21:22,256 [INFO] Step[2650/2713]: training loss : 0.9624364149570465 TRAIN  loss dict:  {'classification_loss': 0.9624364149570465}
2025-01-13 02:21:34,191 [INFO] Step[2700/2713]: training loss : 0.9528802740573883 TRAIN  loss dict:  {'classification_loss': 0.9528802740573883}
2025-01-13 02:23:01,074 [INFO] Label accuracies statistics:
2025-01-13 02:23:01,075 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 1.0, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 1.0, 61: 0.5, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 1.0, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.5, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 02:23:01,076 [INFO] [55] TRAIN  loss: 0.9513295032509658 acc: 0.9996314043494287
2025-01-13 02:23:01,076 [INFO] [55] TRAIN  loss dict: {'classification_loss': 0.9513295032509658}
2025-01-13 02:23:01,077 [INFO] [55] VALIDATION loss: 1.691229225437444 VALIDATION acc: 0.8307210031347962
2025-01-13 02:23:01,077 [INFO] [55] VALIDATION loss dict: {'classification_loss': 1.691229225437444}
2025-01-13 02:23:01,077 [INFO] 
2025-01-13 02:23:18,533 [INFO] Step[50/2713]: training loss : 0.9475504338741303 TRAIN  loss dict:  {'classification_loss': 0.9475504338741303}
2025-01-13 02:23:30,440 [INFO] Step[100/2713]: training loss : 0.9486997294425964 TRAIN  loss dict:  {'classification_loss': 0.9486997294425964}
2025-01-13 02:23:42,360 [INFO] Step[150/2713]: training loss : 0.951919686794281 TRAIN  loss dict:  {'classification_loss': 0.951919686794281}
2025-01-13 02:23:54,261 [INFO] Step[200/2713]: training loss : 0.9531505012512207 TRAIN  loss dict:  {'classification_loss': 0.9531505012512207}
2025-01-13 02:24:06,175 [INFO] Step[250/2713]: training loss : 0.9487332201004028 TRAIN  loss dict:  {'classification_loss': 0.9487332201004028}
2025-01-13 02:24:18,068 [INFO] Step[300/2713]: training loss : 0.9542309331893921 TRAIN  loss dict:  {'classification_loss': 0.9542309331893921}
2025-01-13 02:24:30,033 [INFO] Step[350/2713]: training loss : 0.9485821640491485 TRAIN  loss dict:  {'classification_loss': 0.9485821640491485}
2025-01-13 02:24:41,974 [INFO] Step[400/2713]: training loss : 0.9500487983226776 TRAIN  loss dict:  {'classification_loss': 0.9500487983226776}
2025-01-13 02:24:53,922 [INFO] Step[450/2713]: training loss : 0.9522380495071411 TRAIN  loss dict:  {'classification_loss': 0.9522380495071411}
2025-01-13 02:25:05,802 [INFO] Step[500/2713]: training loss : 0.9515079176425933 TRAIN  loss dict:  {'classification_loss': 0.9515079176425933}
2025-01-13 02:25:17,719 [INFO] Step[550/2713]: training loss : 0.9484143793582916 TRAIN  loss dict:  {'classification_loss': 0.9484143793582916}
2025-01-13 02:25:29,666 [INFO] Step[600/2713]: training loss : 0.9498445951938629 TRAIN  loss dict:  {'classification_loss': 0.9498445951938629}
2025-01-13 02:25:41,569 [INFO] Step[650/2713]: training loss : 0.947261745929718 TRAIN  loss dict:  {'classification_loss': 0.947261745929718}
2025-01-13 02:25:53,483 [INFO] Step[700/2713]: training loss : 0.9479098153114319 TRAIN  loss dict:  {'classification_loss': 0.9479098153114319}
2025-01-13 02:26:05,373 [INFO] Step[750/2713]: training loss : 0.9566223001480103 TRAIN  loss dict:  {'classification_loss': 0.9566223001480103}
2025-01-13 02:26:17,263 [INFO] Step[800/2713]: training loss : 0.9488354420661926 TRAIN  loss dict:  {'classification_loss': 0.9488354420661926}
2025-01-13 02:26:29,295 [INFO] Step[850/2713]: training loss : 0.9503339946269989 TRAIN  loss dict:  {'classification_loss': 0.9503339946269989}
2025-01-13 02:26:41,173 [INFO] Step[900/2713]: training loss : 0.950010484457016 TRAIN  loss dict:  {'classification_loss': 0.950010484457016}
2025-01-13 02:26:53,105 [INFO] Step[950/2713]: training loss : 0.9508329832553863 TRAIN  loss dict:  {'classification_loss': 0.9508329832553863}
2025-01-13 02:27:04,999 [INFO] Step[1000/2713]: training loss : 0.9585224175453186 TRAIN  loss dict:  {'classification_loss': 0.9585224175453186}
2025-01-13 02:27:16,941 [INFO] Step[1050/2713]: training loss : 0.950899019241333 TRAIN  loss dict:  {'classification_loss': 0.950899019241333}
2025-01-13 02:27:28,873 [INFO] Step[1100/2713]: training loss : 0.951510819196701 TRAIN  loss dict:  {'classification_loss': 0.951510819196701}
2025-01-13 02:27:40,809 [INFO] Step[1150/2713]: training loss : 0.9518726432323455 TRAIN  loss dict:  {'classification_loss': 0.9518726432323455}
2025-01-13 02:27:52,723 [INFO] Step[1200/2713]: training loss : 0.9487602269649505 TRAIN  loss dict:  {'classification_loss': 0.9487602269649505}
2025-01-13 02:28:04,643 [INFO] Step[1250/2713]: training loss : 0.9559962201118469 TRAIN  loss dict:  {'classification_loss': 0.9559962201118469}
2025-01-13 02:28:16,509 [INFO] Step[1300/2713]: training loss : 0.9576942706108094 TRAIN  loss dict:  {'classification_loss': 0.9576942706108094}
2025-01-13 02:28:28,413 [INFO] Step[1350/2713]: training loss : 0.9451527488231659 TRAIN  loss dict:  {'classification_loss': 0.9451527488231659}
2025-01-13 02:28:40,325 [INFO] Step[1400/2713]: training loss : 0.9509780669212341 TRAIN  loss dict:  {'classification_loss': 0.9509780669212341}
2025-01-13 02:28:52,263 [INFO] Step[1450/2713]: training loss : 0.9538158512115479 TRAIN  loss dict:  {'classification_loss': 0.9538158512115479}
2025-01-13 02:29:04,165 [INFO] Step[1500/2713]: training loss : 0.9497827672958374 TRAIN  loss dict:  {'classification_loss': 0.9497827672958374}
2025-01-13 02:29:16,125 [INFO] Step[1550/2713]: training loss : 0.947906459569931 TRAIN  loss dict:  {'classification_loss': 0.947906459569931}
2025-01-13 02:29:28,063 [INFO] Step[1600/2713]: training loss : 0.9475198721885681 TRAIN  loss dict:  {'classification_loss': 0.9475198721885681}
2025-01-13 02:29:39,977 [INFO] Step[1650/2713]: training loss : 0.949034550189972 TRAIN  loss dict:  {'classification_loss': 0.949034550189972}
2025-01-13 02:29:51,863 [INFO] Step[1700/2713]: training loss : 0.9493095266819 TRAIN  loss dict:  {'classification_loss': 0.9493095266819}
2025-01-13 02:30:03,767 [INFO] Step[1750/2713]: training loss : 0.9515574538707733 TRAIN  loss dict:  {'classification_loss': 0.9515574538707733}
2025-01-13 02:30:15,835 [INFO] Step[1800/2713]: training loss : 0.9518512177467346 TRAIN  loss dict:  {'classification_loss': 0.9518512177467346}
2025-01-13 02:30:28,333 [INFO] Step[1850/2713]: training loss : 0.9458158946037293 TRAIN  loss dict:  {'classification_loss': 0.9458158946037293}
2025-01-13 02:30:40,736 [INFO] Step[1900/2713]: training loss : 0.954089423418045 TRAIN  loss dict:  {'classification_loss': 0.954089423418045}
2025-01-13 02:30:53,018 [INFO] Step[1950/2713]: training loss : 0.9492208898067475 TRAIN  loss dict:  {'classification_loss': 0.9492208898067475}
2025-01-13 02:31:05,752 [INFO] Step[2000/2713]: training loss : 0.9503273057937622 TRAIN  loss dict:  {'classification_loss': 0.9503273057937622}
2025-01-13 02:31:18,069 [INFO] Step[2050/2713]: training loss : 0.9520682656764984 TRAIN  loss dict:  {'classification_loss': 0.9520682656764984}
2025-01-13 02:31:30,711 [INFO] Step[2100/2713]: training loss : 0.9518565464019776 TRAIN  loss dict:  {'classification_loss': 0.9518565464019776}
2025-01-13 02:31:44,585 [INFO] Step[2150/2713]: training loss : 0.9496882116794586 TRAIN  loss dict:  {'classification_loss': 0.9496882116794586}
2025-01-13 02:31:58,323 [INFO] Step[2200/2713]: training loss : 0.9511236703395843 TRAIN  loss dict:  {'classification_loss': 0.9511236703395843}
2025-01-13 02:32:10,646 [INFO] Step[2250/2713]: training loss : 0.9461891448497772 TRAIN  loss dict:  {'classification_loss': 0.9461891448497772}
2025-01-13 02:32:22,528 [INFO] Step[2300/2713]: training loss : 0.9514025616645813 TRAIN  loss dict:  {'classification_loss': 0.9514025616645813}
2025-01-13 02:32:34,408 [INFO] Step[2350/2713]: training loss : 0.9679837489128112 TRAIN  loss dict:  {'classification_loss': 0.9679837489128112}
2025-01-13 02:32:46,230 [INFO] Step[2400/2713]: training loss : 0.9493064355850219 TRAIN  loss dict:  {'classification_loss': 0.9493064355850219}
2025-01-13 02:32:58,081 [INFO] Step[2450/2713]: training loss : 0.949055802822113 TRAIN  loss dict:  {'classification_loss': 0.949055802822113}
2025-01-13 02:33:09,955 [INFO] Step[2500/2713]: training loss : 0.9468067049980163 TRAIN  loss dict:  {'classification_loss': 0.9468067049980163}
2025-01-13 02:33:21,855 [INFO] Step[2550/2713]: training loss : 0.9491120934486389 TRAIN  loss dict:  {'classification_loss': 0.9491120934486389}
2025-01-13 02:33:33,694 [INFO] Step[2600/2713]: training loss : 0.9512990248203278 TRAIN  loss dict:  {'classification_loss': 0.9512990248203278}
2025-01-13 02:33:45,563 [INFO] Step[2650/2713]: training loss : 0.9453206431865692 TRAIN  loss dict:  {'classification_loss': 0.9453206431865692}
2025-01-13 02:33:57,490 [INFO] Step[2700/2713]: training loss : 0.9486792457103729 TRAIN  loss dict:  {'classification_loss': 0.9486792457103729}
2025-01-13 02:35:23,918 [INFO] Label accuracies statistics:
2025-01-13 02:35:23,919 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.5, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 0.75, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-13 02:35:23,920 [INFO] [56] TRAIN  loss: 0.9506782544876349 acc: 0.9996314043494287
2025-01-13 02:35:23,920 [INFO] [56] TRAIN  loss dict: {'classification_loss': 0.9506782544876349}
2025-01-13 02:35:23,921 [INFO] [56] VALIDATION loss: 1.687152086568058 VALIDATION acc: 0.8275862068965517
2025-01-13 02:35:23,921 [INFO] [56] VALIDATION loss dict: {'classification_loss': 1.687152086568058}
2025-01-13 02:35:23,921 [INFO] 
2025-01-13 02:35:41,392 [INFO] Step[50/2713]: training loss : 0.9566809058189392 TRAIN  loss dict:  {'classification_loss': 0.9566809058189392}
2025-01-13 02:35:53,357 [INFO] Step[100/2713]: training loss : 0.9509090542793274 TRAIN  loss dict:  {'classification_loss': 0.9509090542793274}
2025-01-13 02:36:05,289 [INFO] Step[150/2713]: training loss : 0.947103863954544 TRAIN  loss dict:  {'classification_loss': 0.947103863954544}
2025-01-13 02:36:17,169 [INFO] Step[200/2713]: training loss : 0.9481658780574799 TRAIN  loss dict:  {'classification_loss': 0.9481658780574799}
2025-01-13 02:36:29,117 [INFO] Step[250/2713]: training loss : 0.9485334253311157 TRAIN  loss dict:  {'classification_loss': 0.9485334253311157}
2025-01-13 02:36:41,010 [INFO] Step[300/2713]: training loss : 0.9479206848144531 TRAIN  loss dict:  {'classification_loss': 0.9479206848144531}
2025-01-13 02:36:52,935 [INFO] Step[350/2713]: training loss : 0.9449810409545898 TRAIN  loss dict:  {'classification_loss': 0.9449810409545898}
2025-01-13 02:37:04,877 [INFO] Step[400/2713]: training loss : 0.9469160592556 TRAIN  loss dict:  {'classification_loss': 0.9469160592556}
2025-01-13 02:37:16,725 [INFO] Step[450/2713]: training loss : 0.950064891576767 TRAIN  loss dict:  {'classification_loss': 0.950064891576767}
2025-01-13 02:37:28,647 [INFO] Step[500/2713]: training loss : 0.956059559583664 TRAIN  loss dict:  {'classification_loss': 0.956059559583664}
2025-01-13 02:37:40,598 [INFO] Step[550/2713]: training loss : 0.9522596263885498 TRAIN  loss dict:  {'classification_loss': 0.9522596263885498}
2025-01-13 02:37:52,507 [INFO] Step[600/2713]: training loss : 0.9475021100044251 TRAIN  loss dict:  {'classification_loss': 0.9475021100044251}
2025-01-13 02:38:04,405 [INFO] Step[650/2713]: training loss : 0.9507998430728912 TRAIN  loss dict:  {'classification_loss': 0.9507998430728912}
2025-01-13 02:38:16,338 [INFO] Step[700/2713]: training loss : 0.9482199943065643 TRAIN  loss dict:  {'classification_loss': 0.9482199943065643}
2025-01-13 02:38:28,279 [INFO] Step[750/2713]: training loss : 0.9516496467590332 TRAIN  loss dict:  {'classification_loss': 0.9516496467590332}
2025-01-13 02:38:40,234 [INFO] Step[800/2713]: training loss : 0.9503708374500275 TRAIN  loss dict:  {'classification_loss': 0.9503708374500275}
2025-01-13 02:38:52,177 [INFO] Step[850/2713]: training loss : 0.9450458061695098 TRAIN  loss dict:  {'classification_loss': 0.9450458061695098}
2025-01-13 02:39:04,096 [INFO] Step[900/2713]: training loss : 0.9505039405822754 TRAIN  loss dict:  {'classification_loss': 0.9505039405822754}
2025-01-13 02:39:16,040 [INFO] Step[950/2713]: training loss : 0.9447579753398895 TRAIN  loss dict:  {'classification_loss': 0.9447579753398895}
2025-01-13 02:39:27,965 [INFO] Step[1000/2713]: training loss : 0.955552545785904 TRAIN  loss dict:  {'classification_loss': 0.955552545785904}
2025-01-13 02:39:39,867 [INFO] Step[1050/2713]: training loss : 0.9510248982906342 TRAIN  loss dict:  {'classification_loss': 0.9510248982906342}
2025-01-13 02:39:51,796 [INFO] Step[1100/2713]: training loss : 0.9478075802326202 TRAIN  loss dict:  {'classification_loss': 0.9478075802326202}
2025-01-13 02:40:03,759 [INFO] Step[1150/2713]: training loss : 0.9472598958015442 TRAIN  loss dict:  {'classification_loss': 0.9472598958015442}
2025-01-13 02:40:15,663 [INFO] Step[1200/2713]: training loss : 0.9438117861747741 TRAIN  loss dict:  {'classification_loss': 0.9438117861747741}
2025-01-13 02:40:27,583 [INFO] Step[1250/2713]: training loss : 0.944598605632782 TRAIN  loss dict:  {'classification_loss': 0.944598605632782}
2025-01-13 02:40:39,501 [INFO] Step[1300/2713]: training loss : 0.9482094275951386 TRAIN  loss dict:  {'classification_loss': 0.9482094275951386}
2025-01-13 02:40:51,443 [INFO] Step[1350/2713]: training loss : 0.9482915389537812 TRAIN  loss dict:  {'classification_loss': 0.9482915389537812}
2025-01-13 02:41:03,389 [INFO] Step[1400/2713]: training loss : 0.944968968629837 TRAIN  loss dict:  {'classification_loss': 0.944968968629837}
2025-01-13 02:41:15,292 [INFO] Step[1450/2713]: training loss : 0.9466290175914764 TRAIN  loss dict:  {'classification_loss': 0.9466290175914764}
2025-01-13 02:41:27,186 [INFO] Step[1500/2713]: training loss : 0.9478899919986725 TRAIN  loss dict:  {'classification_loss': 0.9478899919986725}
2025-01-13 02:41:39,113 [INFO] Step[1550/2713]: training loss : 0.9526978802680969 TRAIN  loss dict:  {'classification_loss': 0.9526978802680969}
2025-01-13 02:41:51,087 [INFO] Step[1600/2713]: training loss : 0.9486820566654205 TRAIN  loss dict:  {'classification_loss': 0.9486820566654205}
2025-01-13 02:42:03,012 [INFO] Step[1650/2713]: training loss : 0.9621654057502746 TRAIN  loss dict:  {'classification_loss': 0.9621654057502746}
2025-01-13 02:42:14,954 [INFO] Step[1700/2713]: training loss : 0.952073632478714 TRAIN  loss dict:  {'classification_loss': 0.952073632478714}
2025-01-13 02:42:26,833 [INFO] Step[1750/2713]: training loss : 0.9685140109062195 TRAIN  loss dict:  {'classification_loss': 0.9685140109062195}
2025-01-13 02:42:38,755 [INFO] Step[1800/2713]: training loss : 0.9535740947723389 TRAIN  loss dict:  {'classification_loss': 0.9535740947723389}
2025-01-13 02:42:50,646 [INFO] Step[1850/2713]: training loss : 0.9476508569717407 TRAIN  loss dict:  {'classification_loss': 0.9476508569717407}
2025-01-13 02:43:02,583 [INFO] Step[1900/2713]: training loss : 0.9557697379589081 TRAIN  loss dict:  {'classification_loss': 0.9557697379589081}
2025-01-13 02:43:14,533 [INFO] Step[1950/2713]: training loss : 0.9486091792583465 TRAIN  loss dict:  {'classification_loss': 0.9486091792583465}
2025-01-13 02:43:26,435 [INFO] Step[2000/2713]: training loss : 0.9486827683448792 TRAIN  loss dict:  {'classification_loss': 0.9486827683448792}
2025-01-13 02:43:38,311 [INFO] Step[2050/2713]: training loss : 0.9438055610656738 TRAIN  loss dict:  {'classification_loss': 0.9438055610656738}
2025-01-13 02:43:50,265 [INFO] Step[2100/2713]: training loss : 0.9498312830924988 TRAIN  loss dict:  {'classification_loss': 0.9498312830924988}
2025-01-13 02:44:02,239 [INFO] Step[2150/2713]: training loss : 0.9483370411396027 TRAIN  loss dict:  {'classification_loss': 0.9483370411396027}
2025-01-13 02:44:14,218 [INFO] Step[2200/2713]: training loss : 0.9470695781707764 TRAIN  loss dict:  {'classification_loss': 0.9470695781707764}
2025-01-13 02:44:26,146 [INFO] Step[2250/2713]: training loss : 0.9500925290584564 TRAIN  loss dict:  {'classification_loss': 0.9500925290584564}
2025-01-13 02:44:38,060 [INFO] Step[2300/2713]: training loss : 0.9471535396575927 TRAIN  loss dict:  {'classification_loss': 0.9471535396575927}
2025-01-13 02:44:49,980 [INFO] Step[2350/2713]: training loss : 0.9546079897880554 TRAIN  loss dict:  {'classification_loss': 0.9546079897880554}
2025-01-13 02:45:01,845 [INFO] Step[2400/2713]: training loss : 0.9490145254135132 TRAIN  loss dict:  {'classification_loss': 0.9490145254135132}
2025-01-13 02:45:13,841 [INFO] Step[2450/2713]: training loss : 0.9449101686477661 TRAIN  loss dict:  {'classification_loss': 0.9449101686477661}
2025-01-13 02:45:25,759 [INFO] Step[2500/2713]: training loss : 0.9446849417686463 TRAIN  loss dict:  {'classification_loss': 0.9446849417686463}
2025-01-13 02:45:37,671 [INFO] Step[2550/2713]: training loss : 0.9629168689250946 TRAIN  loss dict:  {'classification_loss': 0.9629168689250946}
2025-01-13 02:45:49,590 [INFO] Step[2600/2713]: training loss : 0.9554434144496917 TRAIN  loss dict:  {'classification_loss': 0.9554434144496917}
2025-01-13 02:46:01,530 [INFO] Step[2650/2713]: training loss : 0.9473005652427673 TRAIN  loss dict:  {'classification_loss': 0.9473005652427673}
2025-01-13 02:46:13,419 [INFO] Step[2700/2713]: training loss : 0.9494243252277375 TRAIN  loss dict:  {'classification_loss': 0.9494243252277375}
2025-01-13 02:47:40,014 [INFO] Label accuracies statistics:
2025-01-13 02:47:40,014 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 1.0, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 0.75, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 1.0, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.75, 276: 1.0, 277: 1.0, 278: 1.0, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 1.0, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 02:47:40,016 [INFO] [57] TRAIN  loss: 0.9499849429483832 acc: 0.9996314043494287
2025-01-13 02:47:40,016 [INFO] [57] TRAIN  loss dict: {'classification_loss': 0.9499849429483832}
2025-01-13 02:47:40,016 [INFO] [57] VALIDATION loss: 1.7099047196762902 VALIDATION acc: 0.8351097178683385
2025-01-13 02:47:40,016 [INFO] [57] VALIDATION loss dict: {'classification_loss': 1.7099047196762902}
2025-01-13 02:47:40,016 [INFO] 
2025-01-13 02:47:57,518 [INFO] Step[50/2713]: training loss : 0.9561300027370453 TRAIN  loss dict:  {'classification_loss': 0.9561300027370453}
2025-01-13 02:48:09,428 [INFO] Step[100/2713]: training loss : 0.9470112502574921 TRAIN  loss dict:  {'classification_loss': 0.9470112502574921}
2025-01-13 02:48:21,366 [INFO] Step[150/2713]: training loss : 0.9531517934799194 TRAIN  loss dict:  {'classification_loss': 0.9531517934799194}
2025-01-13 02:48:33,262 [INFO] Step[200/2713]: training loss : 0.9507811331748962 TRAIN  loss dict:  {'classification_loss': 0.9507811331748962}
2025-01-13 02:48:45,194 [INFO] Step[250/2713]: training loss : 0.9482570278644562 TRAIN  loss dict:  {'classification_loss': 0.9482570278644562}
2025-01-13 02:48:57,103 [INFO] Step[300/2713]: training loss : 0.9466542673110961 TRAIN  loss dict:  {'classification_loss': 0.9466542673110961}
2025-01-13 02:49:09,015 [INFO] Step[350/2713]: training loss : 0.9508486235141754 TRAIN  loss dict:  {'classification_loss': 0.9508486235141754}
2025-01-13 02:49:21,265 [INFO] Step[400/2713]: training loss : 0.9477543938159942 TRAIN  loss dict:  {'classification_loss': 0.9477543938159942}
2025-01-13 02:49:33,581 [INFO] Step[450/2713]: training loss : 0.951749632358551 TRAIN  loss dict:  {'classification_loss': 0.951749632358551}
2025-01-13 02:49:46,010 [INFO] Step[500/2713]: training loss : 0.9573717629909515 TRAIN  loss dict:  {'classification_loss': 0.9573717629909515}
2025-01-13 02:49:58,376 [INFO] Step[550/2713]: training loss : 0.9553905296325683 TRAIN  loss dict:  {'classification_loss': 0.9553905296325683}
2025-01-13 02:50:10,940 [INFO] Step[600/2713]: training loss : 0.9467026543617248 TRAIN  loss dict:  {'classification_loss': 0.9467026543617248}
2025-01-13 02:50:23,364 [INFO] Step[650/2713]: training loss : 0.9635268354415893 TRAIN  loss dict:  {'classification_loss': 0.9635268354415893}
2025-01-13 02:50:36,204 [INFO] Step[700/2713]: training loss : 0.947925579547882 TRAIN  loss dict:  {'classification_loss': 0.947925579547882}
2025-01-13 02:50:49,896 [INFO] Step[750/2713]: training loss : 0.9501660287380218 TRAIN  loss dict:  {'classification_loss': 0.9501660287380218}
2025-01-13 02:51:03,343 [INFO] Step[800/2713]: training loss : 0.9612484037876129 TRAIN  loss dict:  {'classification_loss': 0.9612484037876129}
2025-01-13 02:51:15,503 [INFO] Step[850/2713]: training loss : 0.9484417271614075 TRAIN  loss dict:  {'classification_loss': 0.9484417271614075}
2025-01-13 02:51:27,424 [INFO] Step[900/2713]: training loss : 0.9447987329959869 TRAIN  loss dict:  {'classification_loss': 0.9447987329959869}
2025-01-13 02:51:39,313 [INFO] Step[950/2713]: training loss : 0.9483950996398925 TRAIN  loss dict:  {'classification_loss': 0.9483950996398925}
2025-01-13 02:51:51,169 [INFO] Step[1000/2713]: training loss : 0.949250020980835 TRAIN  loss dict:  {'classification_loss': 0.949250020980835}
2025-01-13 02:52:03,027 [INFO] Step[1050/2713]: training loss : 0.9479514575004577 TRAIN  loss dict:  {'classification_loss': 0.9479514575004577}
2025-01-13 02:52:14,912 [INFO] Step[1100/2713]: training loss : 0.9469072926044464 TRAIN  loss dict:  {'classification_loss': 0.9469072926044464}
2025-01-13 02:52:26,752 [INFO] Step[1150/2713]: training loss : 0.9486941063404083 TRAIN  loss dict:  {'classification_loss': 0.9486941063404083}
2025-01-13 02:52:38,661 [INFO] Step[1200/2713]: training loss : 0.9426992547512054 TRAIN  loss dict:  {'classification_loss': 0.9426992547512054}
2025-01-13 02:52:50,562 [INFO] Step[1250/2713]: training loss : 0.9467756235599518 TRAIN  loss dict:  {'classification_loss': 0.9467756235599518}
2025-01-13 02:53:02,454 [INFO] Step[1300/2713]: training loss : 0.9460494983196258 TRAIN  loss dict:  {'classification_loss': 0.9460494983196258}
2025-01-13 02:53:14,313 [INFO] Step[1350/2713]: training loss : 0.9462963831424713 TRAIN  loss dict:  {'classification_loss': 0.9462963831424713}
2025-01-13 02:53:26,170 [INFO] Step[1400/2713]: training loss : 0.9666514980793 TRAIN  loss dict:  {'classification_loss': 0.9666514980793}
2025-01-13 02:53:38,067 [INFO] Step[1450/2713]: training loss : 0.9485498070716858 TRAIN  loss dict:  {'classification_loss': 0.9485498070716858}
2025-01-13 02:53:49,936 [INFO] Step[1500/2713]: training loss : 0.9471664881706238 TRAIN  loss dict:  {'classification_loss': 0.9471664881706238}
2025-01-13 02:54:01,855 [INFO] Step[1550/2713]: training loss : 0.946581621170044 TRAIN  loss dict:  {'classification_loss': 0.946581621170044}
2025-01-13 02:54:13,769 [INFO] Step[1600/2713]: training loss : 0.947040183544159 TRAIN  loss dict:  {'classification_loss': 0.947040183544159}
2025-01-13 02:54:25,663 [INFO] Step[1650/2713]: training loss : 0.9507044804096222 TRAIN  loss dict:  {'classification_loss': 0.9507044804096222}
2025-01-13 02:54:37,544 [INFO] Step[1700/2713]: training loss : 0.9522278809547424 TRAIN  loss dict:  {'classification_loss': 0.9522278809547424}
2025-01-13 02:54:49,464 [INFO] Step[1750/2713]: training loss : 0.9516778349876404 TRAIN  loss dict:  {'classification_loss': 0.9516778349876404}
2025-01-13 02:55:01,346 [INFO] Step[1800/2713]: training loss : 0.9514495122432709 TRAIN  loss dict:  {'classification_loss': 0.9514495122432709}
2025-01-13 02:55:13,256 [INFO] Step[1850/2713]: training loss : 0.9532680153846741 TRAIN  loss dict:  {'classification_loss': 0.9532680153846741}
2025-01-13 02:55:25,120 [INFO] Step[1900/2713]: training loss : 0.9488279485702514 TRAIN  loss dict:  {'classification_loss': 0.9488279485702514}
2025-01-13 02:55:37,026 [INFO] Step[1950/2713]: training loss : 0.9530677223205566 TRAIN  loss dict:  {'classification_loss': 0.9530677223205566}
2025-01-13 02:55:48,917 [INFO] Step[2000/2713]: training loss : 0.9569643306732177 TRAIN  loss dict:  {'classification_loss': 0.9569643306732177}
2025-01-13 02:56:00,824 [INFO] Step[2050/2713]: training loss : 0.9491114854812622 TRAIN  loss dict:  {'classification_loss': 0.9491114854812622}
2025-01-13 02:56:12,665 [INFO] Step[2100/2713]: training loss : 0.9508086216449737 TRAIN  loss dict:  {'classification_loss': 0.9508086216449737}
2025-01-13 02:56:24,550 [INFO] Step[2150/2713]: training loss : 0.9476245975494385 TRAIN  loss dict:  {'classification_loss': 0.9476245975494385}
2025-01-13 02:56:36,402 [INFO] Step[2200/2713]: training loss : 0.9461816608905792 TRAIN  loss dict:  {'classification_loss': 0.9461816608905792}
2025-01-13 02:56:48,300 [INFO] Step[2250/2713]: training loss : 0.9512881410121917 TRAIN  loss dict:  {'classification_loss': 0.9512881410121917}
2025-01-13 02:57:00,160 [INFO] Step[2300/2713]: training loss : 0.9526712477207184 TRAIN  loss dict:  {'classification_loss': 0.9526712477207184}
2025-01-13 02:57:12,104 [INFO] Step[2350/2713]: training loss : 0.9478461384773255 TRAIN  loss dict:  {'classification_loss': 0.9478461384773255}
2025-01-13 02:57:24,031 [INFO] Step[2400/2713]: training loss : 0.9519379925727844 TRAIN  loss dict:  {'classification_loss': 0.9519379925727844}
2025-01-13 02:57:35,916 [INFO] Step[2450/2713]: training loss : 0.9461091184616088 TRAIN  loss dict:  {'classification_loss': 0.9461091184616088}
2025-01-13 02:57:47,772 [INFO] Step[2500/2713]: training loss : 0.9485127687454223 TRAIN  loss dict:  {'classification_loss': 0.9485127687454223}
2025-01-13 02:57:59,662 [INFO] Step[2550/2713]: training loss : 0.9490416145324707 TRAIN  loss dict:  {'classification_loss': 0.9490416145324707}
2025-01-13 02:58:11,552 [INFO] Step[2600/2713]: training loss : 0.9497552120685577 TRAIN  loss dict:  {'classification_loss': 0.9497552120685577}
2025-01-13 02:58:23,415 [INFO] Step[2650/2713]: training loss : 0.9482111692428589 TRAIN  loss dict:  {'classification_loss': 0.9482111692428589}
2025-01-13 02:58:35,269 [INFO] Step[2700/2713]: training loss : 0.9482982790470124 TRAIN  loss dict:  {'classification_loss': 0.9482982790470124}
2025-01-13 03:00:03,149 [INFO] Label accuracies statistics:
2025-01-13 03:00:03,149 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.75, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.5, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.75, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 1.0, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.5, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 1.0, 278: 1.0, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 03:00:03,151 [INFO] [58] TRAIN  loss: 0.9502208160598297 acc: 0.9995085391325715
2025-01-13 03:00:03,151 [INFO] [58] TRAIN  loss dict: {'classification_loss': 0.9502208160598297}
2025-01-13 03:00:03,151 [INFO] [58] VALIDATION loss: 1.69841684524278 VALIDATION acc: 0.8282131661442006
2025-01-13 03:00:03,151 [INFO] [58] VALIDATION loss dict: {'classification_loss': 1.69841684524278}
2025-01-13 03:00:03,151 [INFO] 
2025-01-13 03:00:21,285 [INFO] Step[50/2713]: training loss : 0.9586397325992584 TRAIN  loss dict:  {'classification_loss': 0.9586397325992584}
2025-01-13 03:00:33,204 [INFO] Step[100/2713]: training loss : 0.9523781085014343 TRAIN  loss dict:  {'classification_loss': 0.9523781085014343}
2025-01-13 03:00:45,074 [INFO] Step[150/2713]: training loss : 0.9494182741641999 TRAIN  loss dict:  {'classification_loss': 0.9494182741641999}
2025-01-13 03:00:56,962 [INFO] Step[200/2713]: training loss : 0.9447803211212158 TRAIN  loss dict:  {'classification_loss': 0.9447803211212158}
2025-01-13 03:01:08,934 [INFO] Step[250/2713]: training loss : 0.9465948355197906 TRAIN  loss dict:  {'classification_loss': 0.9465948355197906}
2025-01-13 03:01:20,809 [INFO] Step[300/2713]: training loss : 0.9451666080951691 TRAIN  loss dict:  {'classification_loss': 0.9451666080951691}
2025-01-13 03:01:32,742 [INFO] Step[350/2713]: training loss : 0.948275580406189 TRAIN  loss dict:  {'classification_loss': 0.948275580406189}
2025-01-13 03:01:44,620 [INFO] Step[400/2713]: training loss : 0.9452183187007904 TRAIN  loss dict:  {'classification_loss': 0.9452183187007904}
2025-01-13 03:01:56,550 [INFO] Step[450/2713]: training loss : 0.949250590801239 TRAIN  loss dict:  {'classification_loss': 0.949250590801239}
2025-01-13 03:02:08,466 [INFO] Step[500/2713]: training loss : 0.9549513506889343 TRAIN  loss dict:  {'classification_loss': 0.9549513506889343}
2025-01-13 03:02:20,343 [INFO] Step[550/2713]: training loss : 0.9467790997028351 TRAIN  loss dict:  {'classification_loss': 0.9467790997028351}
2025-01-13 03:02:32,287 [INFO] Step[600/2713]: training loss : 0.9461810088157654 TRAIN  loss dict:  {'classification_loss': 0.9461810088157654}
2025-01-13 03:02:44,171 [INFO] Step[650/2713]: training loss : 0.955382137298584 TRAIN  loss dict:  {'classification_loss': 0.955382137298584}
2025-01-13 03:02:56,084 [INFO] Step[700/2713]: training loss : 0.9436483788490295 TRAIN  loss dict:  {'classification_loss': 0.9436483788490295}
2025-01-13 03:03:07,967 [INFO] Step[750/2713]: training loss : 0.9479803764820098 TRAIN  loss dict:  {'classification_loss': 0.9479803764820098}
2025-01-13 03:03:19,869 [INFO] Step[800/2713]: training loss : 0.9507652139663696 TRAIN  loss dict:  {'classification_loss': 0.9507652139663696}
2025-01-13 03:03:31,754 [INFO] Step[850/2713]: training loss : 0.9493741929531098 TRAIN  loss dict:  {'classification_loss': 0.9493741929531098}
2025-01-13 03:03:43,630 [INFO] Step[900/2713]: training loss : 0.9441441774368287 TRAIN  loss dict:  {'classification_loss': 0.9441441774368287}
2025-01-13 03:03:55,537 [INFO] Step[950/2713]: training loss : 0.9505335521697998 TRAIN  loss dict:  {'classification_loss': 0.9505335521697998}
2025-01-13 03:04:07,440 [INFO] Step[1000/2713]: training loss : 0.9458461654186249 TRAIN  loss dict:  {'classification_loss': 0.9458461654186249}
2025-01-13 03:04:19,347 [INFO] Step[1050/2713]: training loss : 0.9545595526695252 TRAIN  loss dict:  {'classification_loss': 0.9545595526695252}
2025-01-13 03:04:31,233 [INFO] Step[1100/2713]: training loss : 0.9528165864944458 TRAIN  loss dict:  {'classification_loss': 0.9528165864944458}
2025-01-13 03:04:43,105 [INFO] Step[1150/2713]: training loss : 0.944502272605896 TRAIN  loss dict:  {'classification_loss': 0.944502272605896}
2025-01-13 03:04:54,991 [INFO] Step[1200/2713]: training loss : 0.9523948562145234 TRAIN  loss dict:  {'classification_loss': 0.9523948562145234}
2025-01-13 03:05:06,927 [INFO] Step[1250/2713]: training loss : 0.9450123524665832 TRAIN  loss dict:  {'classification_loss': 0.9450123524665832}
2025-01-13 03:05:18,827 [INFO] Step[1300/2713]: training loss : 0.9702900850772858 TRAIN  loss dict:  {'classification_loss': 0.9702900850772858}
2025-01-13 03:05:30,718 [INFO] Step[1350/2713]: training loss : 0.9513292109966278 TRAIN  loss dict:  {'classification_loss': 0.9513292109966278}
2025-01-13 03:05:42,597 [INFO] Step[1400/2713]: training loss : 0.9460478687286377 TRAIN  loss dict:  {'classification_loss': 0.9460478687286377}
2025-01-13 03:05:54,486 [INFO] Step[1450/2713]: training loss : 0.945861245393753 TRAIN  loss dict:  {'classification_loss': 0.945861245393753}
2025-01-13 03:06:06,376 [INFO] Step[1500/2713]: training loss : 0.947210156917572 TRAIN  loss dict:  {'classification_loss': 0.947210156917572}
2025-01-13 03:06:18,251 [INFO] Step[1550/2713]: training loss : 0.9479578197002411 TRAIN  loss dict:  {'classification_loss': 0.9479578197002411}
2025-01-13 03:06:30,149 [INFO] Step[1600/2713]: training loss : 0.9493657684326172 TRAIN  loss dict:  {'classification_loss': 0.9493657684326172}
2025-01-13 03:06:42,082 [INFO] Step[1650/2713]: training loss : 0.9517794454097748 TRAIN  loss dict:  {'classification_loss': 0.9517794454097748}
2025-01-13 03:06:53,939 [INFO] Step[1700/2713]: training loss : 0.9449848139286041 TRAIN  loss dict:  {'classification_loss': 0.9449848139286041}
2025-01-13 03:07:05,864 [INFO] Step[1750/2713]: training loss : 0.9469293344020844 TRAIN  loss dict:  {'classification_loss': 0.9469293344020844}
2025-01-13 03:07:17,751 [INFO] Step[1800/2713]: training loss : 0.9504378175735474 TRAIN  loss dict:  {'classification_loss': 0.9504378175735474}
2025-01-13 03:07:29,683 [INFO] Step[1850/2713]: training loss : 0.9527059268951416 TRAIN  loss dict:  {'classification_loss': 0.9527059268951416}
2025-01-13 03:07:41,562 [INFO] Step[1900/2713]: training loss : 0.947792843580246 TRAIN  loss dict:  {'classification_loss': 0.947792843580246}
2025-01-13 03:07:53,469 [INFO] Step[1950/2713]: training loss : 0.9489163470268249 TRAIN  loss dict:  {'classification_loss': 0.9489163470268249}
2025-01-13 03:08:05,496 [INFO] Step[2000/2713]: training loss : 0.9551261377334594 TRAIN  loss dict:  {'classification_loss': 0.9551261377334594}
2025-01-13 03:08:17,886 [INFO] Step[2050/2713]: training loss : 0.9496518552303315 TRAIN  loss dict:  {'classification_loss': 0.9496518552303315}
2025-01-13 03:08:30,239 [INFO] Step[2100/2713]: training loss : 0.9492229115962982 TRAIN  loss dict:  {'classification_loss': 0.9492229115962982}
2025-01-13 03:08:42,502 [INFO] Step[2150/2713]: training loss : 0.9495977926254272 TRAIN  loss dict:  {'classification_loss': 0.9495977926254272}
2025-01-13 03:08:54,971 [INFO] Step[2200/2713]: training loss : 0.9459377682209015 TRAIN  loss dict:  {'classification_loss': 0.9459377682209015}
2025-01-13 03:09:07,418 [INFO] Step[2250/2713]: training loss : 0.9476841735839844 TRAIN  loss dict:  {'classification_loss': 0.9476841735839844}
2025-01-13 03:09:19,971 [INFO] Step[2300/2713]: training loss : 0.9466617739200592 TRAIN  loss dict:  {'classification_loss': 0.9466617739200592}
2025-01-13 03:09:33,137 [INFO] Step[2350/2713]: training loss : 0.9513548970222473 TRAIN  loss dict:  {'classification_loss': 0.9513548970222473}
2025-01-13 03:09:47,102 [INFO] Step[2400/2713]: training loss : 0.946504191160202 TRAIN  loss dict:  {'classification_loss': 0.946504191160202}
2025-01-13 03:09:59,722 [INFO] Step[2450/2713]: training loss : 0.9480063879489898 TRAIN  loss dict:  {'classification_loss': 0.9480063879489898}
2025-01-13 03:10:11,664 [INFO] Step[2500/2713]: training loss : 0.9458616256713868 TRAIN  loss dict:  {'classification_loss': 0.9458616256713868}
2025-01-13 03:10:23,557 [INFO] Step[2550/2713]: training loss : 0.9480689573287964 TRAIN  loss dict:  {'classification_loss': 0.9480689573287964}
2025-01-13 03:10:35,422 [INFO] Step[2600/2713]: training loss : 0.9460268580913543 TRAIN  loss dict:  {'classification_loss': 0.9460268580913543}
2025-01-13 03:10:47,290 [INFO] Step[2650/2713]: training loss : 0.946776053905487 TRAIN  loss dict:  {'classification_loss': 0.946776053905487}
2025-01-13 03:10:59,150 [INFO] Step[2700/2713]: training loss : 0.9506606650352478 TRAIN  loss dict:  {'classification_loss': 0.9506606650352478}
2025-01-13 03:12:27,170 [INFO] Label accuracies statistics:
2025-01-13 03:12:27,170 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.5, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.75, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 03:12:27,172 [INFO] [59] TRAIN  loss: 0.949121505164854 acc: 0.9998771347831429
2025-01-13 03:12:27,172 [INFO] [59] TRAIN  loss dict: {'classification_loss': 0.949121505164854}
2025-01-13 03:12:27,172 [INFO] [59] VALIDATION loss: 1.6730310092295022 VALIDATION acc: 0.8313479623824451
2025-01-13 03:12:27,172 [INFO] [59] VALIDATION loss dict: {'classification_loss': 1.6730310092295022}
2025-01-13 03:12:27,172 [INFO] 
2025-01-13 03:12:44,691 [INFO] Step[50/2713]: training loss : 0.9505600929260254 TRAIN  loss dict:  {'classification_loss': 0.9505600929260254}
2025-01-13 03:12:56,580 [INFO] Step[100/2713]: training loss : 0.9517522227764129 TRAIN  loss dict:  {'classification_loss': 0.9517522227764129}
2025-01-13 03:13:08,494 [INFO] Step[150/2713]: training loss : 0.9472172701358795 TRAIN  loss dict:  {'classification_loss': 0.9472172701358795}
2025-01-13 03:13:20,450 [INFO] Step[200/2713]: training loss : 0.9463533759117126 TRAIN  loss dict:  {'classification_loss': 0.9463533759117126}
2025-01-13 03:13:32,376 [INFO] Step[250/2713]: training loss : 0.9482820701599121 TRAIN  loss dict:  {'classification_loss': 0.9482820701599121}
2025-01-13 03:13:44,313 [INFO] Step[300/2713]: training loss : 0.9537780237197876 TRAIN  loss dict:  {'classification_loss': 0.9537780237197876}
2025-01-13 03:13:56,225 [INFO] Step[350/2713]: training loss : 0.9443976652622222 TRAIN  loss dict:  {'classification_loss': 0.9443976652622222}
2025-01-13 03:14:08,175 [INFO] Step[400/2713]: training loss : 0.9414484786987305 TRAIN  loss dict:  {'classification_loss': 0.9414484786987305}
2025-01-13 03:14:20,136 [INFO] Step[450/2713]: training loss : 0.9514369297027588 TRAIN  loss dict:  {'classification_loss': 0.9514369297027588}
2025-01-13 03:14:32,033 [INFO] Step[500/2713]: training loss : 0.9470019340515137 TRAIN  loss dict:  {'classification_loss': 0.9470019340515137}
2025-01-13 03:14:43,972 [INFO] Step[550/2713]: training loss : 0.9458147716522217 TRAIN  loss dict:  {'classification_loss': 0.9458147716522217}
2025-01-13 03:14:55,848 [INFO] Step[600/2713]: training loss : 0.9465884947776795 TRAIN  loss dict:  {'classification_loss': 0.9465884947776795}
2025-01-13 03:15:07,765 [INFO] Step[650/2713]: training loss : 0.9451731288433075 TRAIN  loss dict:  {'classification_loss': 0.9451731288433075}
2025-01-13 03:15:19,629 [INFO] Step[700/2713]: training loss : 0.9458104443550109 TRAIN  loss dict:  {'classification_loss': 0.9458104443550109}
2025-01-13 03:15:31,560 [INFO] Step[750/2713]: training loss : 0.9491364967823028 TRAIN  loss dict:  {'classification_loss': 0.9491364967823028}
2025-01-13 03:15:43,474 [INFO] Step[800/2713]: training loss : 0.9627483785152435 TRAIN  loss dict:  {'classification_loss': 0.9627483785152435}
2025-01-13 03:15:55,429 [INFO] Step[850/2713]: training loss : 0.9502120923995971 TRAIN  loss dict:  {'classification_loss': 0.9502120923995971}
2025-01-13 03:16:07,366 [INFO] Step[900/2713]: training loss : 0.9452002322673798 TRAIN  loss dict:  {'classification_loss': 0.9452002322673798}
2025-01-13 03:16:19,307 [INFO] Step[950/2713]: training loss : 0.9512558341026306 TRAIN  loss dict:  {'classification_loss': 0.9512558341026306}
2025-01-13 03:16:31,211 [INFO] Step[1000/2713]: training loss : 0.9443764972686768 TRAIN  loss dict:  {'classification_loss': 0.9443764972686768}
2025-01-13 03:16:43,116 [INFO] Step[1050/2713]: training loss : 0.9509735429286956 TRAIN  loss dict:  {'classification_loss': 0.9509735429286956}
2025-01-13 03:16:55,048 [INFO] Step[1100/2713]: training loss : 0.9481206226348877 TRAIN  loss dict:  {'classification_loss': 0.9481206226348877}
2025-01-13 03:17:06,978 [INFO] Step[1150/2713]: training loss : 0.9481968426704407 TRAIN  loss dict:  {'classification_loss': 0.9481968426704407}
2025-01-13 03:17:18,896 [INFO] Step[1200/2713]: training loss : 0.9459775984287262 TRAIN  loss dict:  {'classification_loss': 0.9459775984287262}
2025-01-13 03:17:30,824 [INFO] Step[1250/2713]: training loss : 0.9546077537536621 TRAIN  loss dict:  {'classification_loss': 0.9546077537536621}
2025-01-13 03:17:42,729 [INFO] Step[1300/2713]: training loss : 0.9457385683059693 TRAIN  loss dict:  {'classification_loss': 0.9457385683059693}
2025-01-13 03:17:54,628 [INFO] Step[1350/2713]: training loss : 0.9503027832508087 TRAIN  loss dict:  {'classification_loss': 0.9503027832508087}
2025-01-13 03:18:06,536 [INFO] Step[1400/2713]: training loss : 0.9435184931755066 TRAIN  loss dict:  {'classification_loss': 0.9435184931755066}
2025-01-13 03:18:18,428 [INFO] Step[1450/2713]: training loss : 0.9495289671421051 TRAIN  loss dict:  {'classification_loss': 0.9495289671421051}
2025-01-13 03:18:30,354 [INFO] Step[1500/2713]: training loss : 0.9488837933540344 TRAIN  loss dict:  {'classification_loss': 0.9488837933540344}
2025-01-13 03:18:42,260 [INFO] Step[1550/2713]: training loss : 0.950173807144165 TRAIN  loss dict:  {'classification_loss': 0.950173807144165}
2025-01-13 03:18:54,162 [INFO] Step[1600/2713]: training loss : 0.9499795973300934 TRAIN  loss dict:  {'classification_loss': 0.9499795973300934}
2025-01-13 03:19:06,161 [INFO] Step[1650/2713]: training loss : 0.9437386214733123 TRAIN  loss dict:  {'classification_loss': 0.9437386214733123}
2025-01-13 03:19:18,081 [INFO] Step[1700/2713]: training loss : 0.9504839742183685 TRAIN  loss dict:  {'classification_loss': 0.9504839742183685}
2025-01-13 03:19:30,013 [INFO] Step[1750/2713]: training loss : 0.9466667425632477 TRAIN  loss dict:  {'classification_loss': 0.9466667425632477}
2025-01-13 03:19:41,894 [INFO] Step[1800/2713]: training loss : 0.9499032759666443 TRAIN  loss dict:  {'classification_loss': 0.9499032759666443}
2025-01-13 03:19:53,796 [INFO] Step[1850/2713]: training loss : 0.9611273169517517 TRAIN  loss dict:  {'classification_loss': 0.9611273169517517}
2025-01-13 03:20:05,711 [INFO] Step[1900/2713]: training loss : 0.9468863451480866 TRAIN  loss dict:  {'classification_loss': 0.9468863451480866}
2025-01-13 03:20:17,641 [INFO] Step[1950/2713]: training loss : 0.9485505616664887 TRAIN  loss dict:  {'classification_loss': 0.9485505616664887}
2025-01-13 03:20:29,610 [INFO] Step[2000/2713]: training loss : 0.9467016434669495 TRAIN  loss dict:  {'classification_loss': 0.9467016434669495}
2025-01-13 03:20:41,494 [INFO] Step[2050/2713]: training loss : 0.945175039768219 TRAIN  loss dict:  {'classification_loss': 0.945175039768219}
2025-01-13 03:20:53,396 [INFO] Step[2100/2713]: training loss : 0.9458434975147247 TRAIN  loss dict:  {'classification_loss': 0.9458434975147247}
2025-01-13 03:21:05,270 [INFO] Step[2150/2713]: training loss : 0.9467032635211945 TRAIN  loss dict:  {'classification_loss': 0.9467032635211945}
2025-01-13 03:21:17,150 [INFO] Step[2200/2713]: training loss : 0.9444666111469269 TRAIN  loss dict:  {'classification_loss': 0.9444666111469269}
2025-01-13 03:21:29,075 [INFO] Step[2250/2713]: training loss : 0.9494207334518433 TRAIN  loss dict:  {'classification_loss': 0.9494207334518433}
2025-01-13 03:21:40,957 [INFO] Step[2300/2713]: training loss : 0.9467474329471588 TRAIN  loss dict:  {'classification_loss': 0.9467474329471588}
2025-01-13 03:21:52,824 [INFO] Step[2350/2713]: training loss : 0.9487573182582856 TRAIN  loss dict:  {'classification_loss': 0.9487573182582856}
2025-01-13 03:22:04,706 [INFO] Step[2400/2713]: training loss : 0.9446914815902709 TRAIN  loss dict:  {'classification_loss': 0.9446914815902709}
2025-01-13 03:22:16,613 [INFO] Step[2450/2713]: training loss : 0.9561113703250885 TRAIN  loss dict:  {'classification_loss': 0.9561113703250885}
2025-01-13 03:22:28,518 [INFO] Step[2500/2713]: training loss : 0.9494177341461182 TRAIN  loss dict:  {'classification_loss': 0.9494177341461182}
2025-01-13 03:22:40,443 [INFO] Step[2550/2713]: training loss : 0.9475475597381592 TRAIN  loss dict:  {'classification_loss': 0.9475475597381592}
2025-01-13 03:22:52,325 [INFO] Step[2600/2713]: training loss : 0.9500246584415436 TRAIN  loss dict:  {'classification_loss': 0.9500246584415436}
2025-01-13 03:23:04,251 [INFO] Step[2650/2713]: training loss : 0.946091182231903 TRAIN  loss dict:  {'classification_loss': 0.946091182231903}
2025-01-13 03:23:16,185 [INFO] Step[2700/2713]: training loss : 0.9500066494941711 TRAIN  loss dict:  {'classification_loss': 0.9500066494941711}
2025-01-13 03:24:42,256 [INFO] Label accuracies statistics:
2025-01-13 03:24:42,256 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 1.0, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 0.75, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 0.75, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 0.75, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 03:24:42,258 [INFO] [60] TRAIN  loss: 0.9485152936974796 acc: 0.9997542695662858
2025-01-13 03:24:42,258 [INFO] [60] TRAIN  loss dict: {'classification_loss': 0.9485152936974796}
2025-01-13 03:24:42,258 [INFO] [60] VALIDATION loss: 1.718002735896218 VALIDATION acc: 0.8238244514106583
2025-01-13 03:24:42,258 [INFO] [60] VALIDATION loss dict: {'classification_loss': 1.718002735896218}
2025-01-13 03:24:42,258 [INFO] 
2025-01-13 03:24:59,571 [INFO] Step[50/2713]: training loss : 0.9464448130130768 TRAIN  loss dict:  {'classification_loss': 0.9464448130130768}
2025-01-13 03:25:11,571 [INFO] Step[100/2713]: training loss : 0.9449713551998138 TRAIN  loss dict:  {'classification_loss': 0.9449713551998138}
2025-01-13 03:25:23,440 [INFO] Step[150/2713]: training loss : 0.9471095013618469 TRAIN  loss dict:  {'classification_loss': 0.9471095013618469}
2025-01-13 03:25:35,369 [INFO] Step[200/2713]: training loss : 0.9435967624187469 TRAIN  loss dict:  {'classification_loss': 0.9435967624187469}
2025-01-13 03:25:47,234 [INFO] Step[250/2713]: training loss : 0.9468581652641297 TRAIN  loss dict:  {'classification_loss': 0.9468581652641297}
2025-01-13 03:25:59,153 [INFO] Step[300/2713]: training loss : 0.9456439924240112 TRAIN  loss dict:  {'classification_loss': 0.9456439924240112}
2025-01-13 03:26:11,038 [INFO] Step[350/2713]: training loss : 0.9460870754718781 TRAIN  loss dict:  {'classification_loss': 0.9460870754718781}
2025-01-13 03:26:22,950 [INFO] Step[400/2713]: training loss : 0.9494743287563324 TRAIN  loss dict:  {'classification_loss': 0.9494743287563324}
2025-01-13 03:26:34,888 [INFO] Step[450/2713]: training loss : 0.9484825611114502 TRAIN  loss dict:  {'classification_loss': 0.9484825611114502}
2025-01-13 03:26:46,788 [INFO] Step[500/2713]: training loss : 0.9546543836593628 TRAIN  loss dict:  {'classification_loss': 0.9546543836593628}
2025-01-13 03:26:58,701 [INFO] Step[550/2713]: training loss : 0.9440192306041717 TRAIN  loss dict:  {'classification_loss': 0.9440192306041717}
2025-01-13 03:27:10,826 [INFO] Step[600/2713]: training loss : 0.9475533699989319 TRAIN  loss dict:  {'classification_loss': 0.9475533699989319}
2025-01-13 03:27:23,176 [INFO] Step[650/2713]: training loss : 0.94835857629776 TRAIN  loss dict:  {'classification_loss': 0.94835857629776}
2025-01-13 03:27:35,522 [INFO] Step[700/2713]: training loss : 0.9462164187431336 TRAIN  loss dict:  {'classification_loss': 0.9462164187431336}
2025-01-13 03:27:47,815 [INFO] Step[750/2713]: training loss : 0.9451595914363861 TRAIN  loss dict:  {'classification_loss': 0.9451595914363861}
2025-01-13 03:28:00,520 [INFO] Step[800/2713]: training loss : 0.9476053166389465 TRAIN  loss dict:  {'classification_loss': 0.9476053166389465}
2025-01-13 03:28:12,815 [INFO] Step[850/2713]: training loss : 0.9436151778697968 TRAIN  loss dict:  {'classification_loss': 0.9436151778697968}
2025-01-13 03:28:25,549 [INFO] Step[900/2713]: training loss : 0.9488279128074646 TRAIN  loss dict:  {'classification_loss': 0.9488279128074646}
2025-01-13 03:28:39,184 [INFO] Step[950/2713]: training loss : 0.9443155086040497 TRAIN  loss dict:  {'classification_loss': 0.9443155086040497}
2025-01-13 03:28:52,963 [INFO] Step[1000/2713]: training loss : 0.9473830854892731 TRAIN  loss dict:  {'classification_loss': 0.9473830854892731}
2025-01-13 03:29:05,057 [INFO] Step[1050/2713]: training loss : 0.9493618369102478 TRAIN  loss dict:  {'classification_loss': 0.9493618369102478}
2025-01-13 03:29:16,965 [INFO] Step[1100/2713]: training loss : 0.9434699130058288 TRAIN  loss dict:  {'classification_loss': 0.9434699130058288}
2025-01-13 03:29:28,846 [INFO] Step[1150/2713]: training loss : 0.9447644400596619 TRAIN  loss dict:  {'classification_loss': 0.9447644400596619}
2025-01-13 03:29:40,700 [INFO] Step[1200/2713]: training loss : 0.9443494844436645 TRAIN  loss dict:  {'classification_loss': 0.9443494844436645}
2025-01-13 03:29:52,597 [INFO] Step[1250/2713]: training loss : 0.94462233543396 TRAIN  loss dict:  {'classification_loss': 0.94462233543396}
2025-01-13 03:30:04,483 [INFO] Step[1300/2713]: training loss : 0.9443263030052185 TRAIN  loss dict:  {'classification_loss': 0.9443263030052185}
2025-01-13 03:30:16,405 [INFO] Step[1350/2713]: training loss : 0.9474462461471558 TRAIN  loss dict:  {'classification_loss': 0.9474462461471558}
2025-01-13 03:30:28,235 [INFO] Step[1400/2713]: training loss : 0.9430467760562897 TRAIN  loss dict:  {'classification_loss': 0.9430467760562897}
2025-01-13 03:30:40,125 [INFO] Step[1450/2713]: training loss : 0.9486230492591858 TRAIN  loss dict:  {'classification_loss': 0.9486230492591858}
2025-01-13 03:30:52,047 [INFO] Step[1500/2713]: training loss : 0.9469562590122222 TRAIN  loss dict:  {'classification_loss': 0.9469562590122222}
2025-01-13 03:31:03,967 [INFO] Step[1550/2713]: training loss : 0.9538464736938477 TRAIN  loss dict:  {'classification_loss': 0.9538464736938477}
2025-01-13 03:31:15,789 [INFO] Step[1600/2713]: training loss : 0.944640474319458 TRAIN  loss dict:  {'classification_loss': 0.944640474319458}
2025-01-13 03:31:27,676 [INFO] Step[1650/2713]: training loss : 0.9454168212413788 TRAIN  loss dict:  {'classification_loss': 0.9454168212413788}
2025-01-13 03:31:39,541 [INFO] Step[1700/2713]: training loss : 0.9604499018192292 TRAIN  loss dict:  {'classification_loss': 0.9604499018192292}
2025-01-13 03:31:51,433 [INFO] Step[1750/2713]: training loss : 0.9502267456054687 TRAIN  loss dict:  {'classification_loss': 0.9502267456054687}
2025-01-13 03:32:03,312 [INFO] Step[1800/2713]: training loss : 0.942809761762619 TRAIN  loss dict:  {'classification_loss': 0.942809761762619}
2025-01-13 03:32:15,220 [INFO] Step[1850/2713]: training loss : 0.9480792963504792 TRAIN  loss dict:  {'classification_loss': 0.9480792963504792}
2025-01-13 03:32:27,086 [INFO] Step[1900/2713]: training loss : 0.9528762781620026 TRAIN  loss dict:  {'classification_loss': 0.9528762781620026}
2025-01-13 03:32:39,005 [INFO] Step[1950/2713]: training loss : 0.9421955597400665 TRAIN  loss dict:  {'classification_loss': 0.9421955597400665}
2025-01-13 03:32:50,923 [INFO] Step[2000/2713]: training loss : 0.9462581729888916 TRAIN  loss dict:  {'classification_loss': 0.9462581729888916}
2025-01-13 03:33:02,831 [INFO] Step[2050/2713]: training loss : 0.9466223895549775 TRAIN  loss dict:  {'classification_loss': 0.9466223895549775}
2025-01-13 03:33:14,725 [INFO] Step[2100/2713]: training loss : 0.9479240989685058 TRAIN  loss dict:  {'classification_loss': 0.9479240989685058}
2025-01-13 03:33:26,619 [INFO] Step[2150/2713]: training loss : 0.9600226497650146 TRAIN  loss dict:  {'classification_loss': 0.9600226497650146}
2025-01-13 03:33:38,534 [INFO] Step[2200/2713]: training loss : 0.9442242085933685 TRAIN  loss dict:  {'classification_loss': 0.9442242085933685}
2025-01-13 03:33:50,404 [INFO] Step[2250/2713]: training loss : 0.9448655283451081 TRAIN  loss dict:  {'classification_loss': 0.9448655283451081}
2025-01-13 03:34:02,256 [INFO] Step[2300/2713]: training loss : 0.9434426438808441 TRAIN  loss dict:  {'classification_loss': 0.9434426438808441}
2025-01-13 03:34:14,154 [INFO] Step[2350/2713]: training loss : 0.9489005815982818 TRAIN  loss dict:  {'classification_loss': 0.9489005815982818}
2025-01-13 03:34:26,068 [INFO] Step[2400/2713]: training loss : 0.9454057943820954 TRAIN  loss dict:  {'classification_loss': 0.9454057943820954}
2025-01-13 03:34:37,967 [INFO] Step[2450/2713]: training loss : 0.9481984627246857 TRAIN  loss dict:  {'classification_loss': 0.9481984627246857}
2025-01-13 03:34:49,886 [INFO] Step[2500/2713]: training loss : 0.9450692796707153 TRAIN  loss dict:  {'classification_loss': 0.9450692796707153}
2025-01-13 03:35:01,781 [INFO] Step[2550/2713]: training loss : 0.9462817728519439 TRAIN  loss dict:  {'classification_loss': 0.9462817728519439}
2025-01-13 03:35:13,671 [INFO] Step[2600/2713]: training loss : 0.9469216763973236 TRAIN  loss dict:  {'classification_loss': 0.9469216763973236}
2025-01-13 03:35:25,577 [INFO] Step[2650/2713]: training loss : 0.9492759609222412 TRAIN  loss dict:  {'classification_loss': 0.9492759609222412}
2025-01-13 03:35:37,510 [INFO] Step[2700/2713]: training loss : 0.9431316864490509 TRAIN  loss dict:  {'classification_loss': 0.9431316864490509}
2025-01-13 03:37:03,773 [INFO] Label accuracies statistics:
2025-01-13 03:37:03,773 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.25, 238: 1.0, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 1.0, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 03:37:03,775 [INFO] [61] TRAIN  loss: 0.9470245417659238 acc: 0.9996314043494287
2025-01-13 03:37:03,775 [INFO] [61] TRAIN  loss dict: {'classification_loss': 0.9470245417659238}
2025-01-13 03:37:03,775 [INFO] [61] VALIDATION loss: 1.6795306294260168 VALIDATION acc: 0.8363636363636363
2025-01-13 03:37:03,776 [INFO] [61] VALIDATION loss dict: {'classification_loss': 1.6795306294260168}
2025-01-13 03:37:03,776 [INFO] 
2025-01-13 03:37:21,761 [INFO] Step[50/2713]: training loss : 0.9473298251628876 TRAIN  loss dict:  {'classification_loss': 0.9473298251628876}
2025-01-13 03:37:33,666 [INFO] Step[100/2713]: training loss : 0.9495760953426361 TRAIN  loss dict:  {'classification_loss': 0.9495760953426361}
2025-01-13 03:37:45,580 [INFO] Step[150/2713]: training loss : 0.9442406082153321 TRAIN  loss dict:  {'classification_loss': 0.9442406082153321}
2025-01-13 03:37:57,495 [INFO] Step[200/2713]: training loss : 0.9457439696788787 TRAIN  loss dict:  {'classification_loss': 0.9457439696788787}
2025-01-13 03:38:09,426 [INFO] Step[250/2713]: training loss : 0.9474975275993347 TRAIN  loss dict:  {'classification_loss': 0.9474975275993347}
2025-01-13 03:38:21,351 [INFO] Step[300/2713]: training loss : 0.9450243449211121 TRAIN  loss dict:  {'classification_loss': 0.9450243449211121}
2025-01-13 03:38:33,281 [INFO] Step[350/2713]: training loss : 0.9456850516796113 TRAIN  loss dict:  {'classification_loss': 0.9456850516796113}
2025-01-13 03:38:45,199 [INFO] Step[400/2713]: training loss : 0.9420918190479278 TRAIN  loss dict:  {'classification_loss': 0.9420918190479278}
2025-01-13 03:38:57,123 [INFO] Step[450/2713]: training loss : 0.9453512597084045 TRAIN  loss dict:  {'classification_loss': 0.9453512597084045}
2025-01-13 03:39:09,028 [INFO] Step[500/2713]: training loss : 0.94370330452919 TRAIN  loss dict:  {'classification_loss': 0.94370330452919}
2025-01-13 03:39:20,911 [INFO] Step[550/2713]: training loss : 0.9461473321914673 TRAIN  loss dict:  {'classification_loss': 0.9461473321914673}
2025-01-13 03:39:32,834 [INFO] Step[600/2713]: training loss : 0.9455288755893707 TRAIN  loss dict:  {'classification_loss': 0.9455288755893707}
2025-01-13 03:39:44,772 [INFO] Step[650/2713]: training loss : 0.9430298066139221 TRAIN  loss dict:  {'classification_loss': 0.9430298066139221}
2025-01-13 03:39:56,684 [INFO] Step[700/2713]: training loss : 0.9439133262634277 TRAIN  loss dict:  {'classification_loss': 0.9439133262634277}
2025-01-13 03:40:08,630 [INFO] Step[750/2713]: training loss : 0.9463681173324585 TRAIN  loss dict:  {'classification_loss': 0.9463681173324585}
2025-01-13 03:40:20,542 [INFO] Step[800/2713]: training loss : 0.946803810596466 TRAIN  loss dict:  {'classification_loss': 0.946803810596466}
2025-01-13 03:40:32,472 [INFO] Step[850/2713]: training loss : 0.9465868246555328 TRAIN  loss dict:  {'classification_loss': 0.9465868246555328}
2025-01-13 03:40:44,389 [INFO] Step[900/2713]: training loss : 0.9438438248634339 TRAIN  loss dict:  {'classification_loss': 0.9438438248634339}
2025-01-13 03:40:56,306 [INFO] Step[950/2713]: training loss : 0.9460363614559174 TRAIN  loss dict:  {'classification_loss': 0.9460363614559174}
2025-01-13 03:41:08,159 [INFO] Step[1000/2713]: training loss : 0.9512211728096008 TRAIN  loss dict:  {'classification_loss': 0.9512211728096008}
2025-01-13 03:41:20,113 [INFO] Step[1050/2713]: training loss : 0.9445112299919128 TRAIN  loss dict:  {'classification_loss': 0.9445112299919128}
2025-01-13 03:41:31,988 [INFO] Step[1100/2713]: training loss : 0.947759530544281 TRAIN  loss dict:  {'classification_loss': 0.947759530544281}
2025-01-13 03:41:43,899 [INFO] Step[1150/2713]: training loss : 0.945591539144516 TRAIN  loss dict:  {'classification_loss': 0.945591539144516}
2025-01-13 03:41:55,808 [INFO] Step[1200/2713]: training loss : 0.9466118538379669 TRAIN  loss dict:  {'classification_loss': 0.9466118538379669}
2025-01-13 03:42:07,734 [INFO] Step[1250/2713]: training loss : 0.9444012618064881 TRAIN  loss dict:  {'classification_loss': 0.9444012618064881}
2025-01-13 03:42:19,623 [INFO] Step[1300/2713]: training loss : 0.9420259404182434 TRAIN  loss dict:  {'classification_loss': 0.9420259404182434}
2025-01-13 03:42:31,558 [INFO] Step[1350/2713]: training loss : 0.942503206729889 TRAIN  loss dict:  {'classification_loss': 0.942503206729889}
2025-01-13 03:42:43,465 [INFO] Step[1400/2713]: training loss : 0.9423165380954742 TRAIN  loss dict:  {'classification_loss': 0.9423165380954742}
2025-01-13 03:42:55,378 [INFO] Step[1450/2713]: training loss : 0.9452492892742157 TRAIN  loss dict:  {'classification_loss': 0.9452492892742157}
2025-01-13 03:43:07,290 [INFO] Step[1500/2713]: training loss : 0.9470577204227447 TRAIN  loss dict:  {'classification_loss': 0.9470577204227447}
2025-01-13 03:43:19,193 [INFO] Step[1550/2713]: training loss : 0.9457692110538483 TRAIN  loss dict:  {'classification_loss': 0.9457692110538483}
2025-01-13 03:43:31,125 [INFO] Step[1600/2713]: training loss : 0.9467062377929687 TRAIN  loss dict:  {'classification_loss': 0.9467062377929687}
2025-01-13 03:43:43,042 [INFO] Step[1650/2713]: training loss : 0.9448746764659881 TRAIN  loss dict:  {'classification_loss': 0.9448746764659881}
2025-01-13 03:43:54,946 [INFO] Step[1700/2713]: training loss : 0.9420576608180999 TRAIN  loss dict:  {'classification_loss': 0.9420576608180999}
2025-01-13 03:44:06,849 [INFO] Step[1750/2713]: training loss : 0.9442816209793091 TRAIN  loss dict:  {'classification_loss': 0.9442816209793091}
2025-01-13 03:44:18,740 [INFO] Step[1800/2713]: training loss : 0.9449202632904052 TRAIN  loss dict:  {'classification_loss': 0.9449202632904052}
2025-01-13 03:44:30,659 [INFO] Step[1850/2713]: training loss : 0.9500267446041107 TRAIN  loss dict:  {'classification_loss': 0.9500267446041107}
2025-01-13 03:44:42,550 [INFO] Step[1900/2713]: training loss : 0.9458717334270478 TRAIN  loss dict:  {'classification_loss': 0.9458717334270478}
2025-01-13 03:44:54,468 [INFO] Step[1950/2713]: training loss : 0.9459805643558502 TRAIN  loss dict:  {'classification_loss': 0.9459805643558502}
2025-01-13 03:45:06,358 [INFO] Step[2000/2713]: training loss : 0.945197023153305 TRAIN  loss dict:  {'classification_loss': 0.945197023153305}
2025-01-13 03:45:18,246 [INFO] Step[2050/2713]: training loss : 0.9437552452087402 TRAIN  loss dict:  {'classification_loss': 0.9437552452087402}
2025-01-13 03:45:30,120 [INFO] Step[2100/2713]: training loss : 0.9445826232433319 TRAIN  loss dict:  {'classification_loss': 0.9445826232433319}
2025-01-13 03:45:42,002 [INFO] Step[2150/2713]: training loss : 0.9457644081115723 TRAIN  loss dict:  {'classification_loss': 0.9457644081115723}
2025-01-13 03:45:54,034 [INFO] Step[2200/2713]: training loss : 0.9442369997501373 TRAIN  loss dict:  {'classification_loss': 0.9442369997501373}
2025-01-13 03:46:06,301 [INFO] Step[2250/2713]: training loss : 0.9472339904308319 TRAIN  loss dict:  {'classification_loss': 0.9472339904308319}
2025-01-13 03:46:18,685 [INFO] Step[2300/2713]: training loss : 0.9632892429828643 TRAIN  loss dict:  {'classification_loss': 0.9632892429828643}
2025-01-13 03:46:30,963 [INFO] Step[2350/2713]: training loss : 0.9520063841342926 TRAIN  loss dict:  {'classification_loss': 0.9520063841342926}
2025-01-13 03:46:43,454 [INFO] Step[2400/2713]: training loss : 0.9407992637157441 TRAIN  loss dict:  {'classification_loss': 0.9407992637157441}
2025-01-13 03:46:55,891 [INFO] Step[2450/2713]: training loss : 0.953331651687622 TRAIN  loss dict:  {'classification_loss': 0.953331651687622}
2025-01-13 03:47:08,397 [INFO] Step[2500/2713]: training loss : 0.9511419534683228 TRAIN  loss dict:  {'classification_loss': 0.9511419534683228}
2025-01-13 03:47:21,510 [INFO] Step[2550/2713]: training loss : 0.9476275861263275 TRAIN  loss dict:  {'classification_loss': 0.9476275861263275}
2025-01-13 03:47:34,918 [INFO] Step[2600/2713]: training loss : 0.9434224188327789 TRAIN  loss dict:  {'classification_loss': 0.9434224188327789}
2025-01-13 03:47:47,765 [INFO] Step[2650/2713]: training loss : 0.9471860349178314 TRAIN  loss dict:  {'classification_loss': 0.9471860349178314}
2025-01-13 03:47:59,790 [INFO] Step[2700/2713]: training loss : 0.9441298830509186 TRAIN  loss dict:  {'classification_loss': 0.9441298830509186}
2025-01-13 03:49:26,583 [INFO] Label accuracies statistics:
2025-01-13 03:49:26,583 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 0.75, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.5, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.25, 238: 1.0, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.5, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 1.0, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 03:49:26,585 [INFO] [62] TRAIN  loss: 0.9460571261951504 acc: 0.9997542695662858
2025-01-13 03:49:26,585 [INFO] [62] TRAIN  loss dict: {'classification_loss': 0.9460571261951504}
2025-01-13 03:49:26,585 [INFO] [62] VALIDATION loss: 1.6797302057196324 VALIDATION acc: 0.8307210031347962
2025-01-13 03:49:26,585 [INFO] [62] VALIDATION loss dict: {'classification_loss': 1.6797302057196324}
2025-01-13 03:49:26,586 [INFO] 
2025-01-13 03:49:44,504 [INFO] Step[50/2713]: training loss : 0.9422737801074982 TRAIN  loss dict:  {'classification_loss': 0.9422737801074982}
2025-01-13 03:49:56,444 [INFO] Step[100/2713]: training loss : 0.9454686343669891 TRAIN  loss dict:  {'classification_loss': 0.9454686343669891}
2025-01-13 03:50:08,355 [INFO] Step[150/2713]: training loss : 0.9474308490753174 TRAIN  loss dict:  {'classification_loss': 0.9474308490753174}
2025-01-13 03:50:20,264 [INFO] Step[200/2713]: training loss : 0.9473695147037506 TRAIN  loss dict:  {'classification_loss': 0.9473695147037506}
2025-01-13 03:50:32,197 [INFO] Step[250/2713]: training loss : 0.945158109664917 TRAIN  loss dict:  {'classification_loss': 0.945158109664917}
2025-01-13 03:50:44,125 [INFO] Step[300/2713]: training loss : 0.9501155161857605 TRAIN  loss dict:  {'classification_loss': 0.9501155161857605}
2025-01-13 03:50:56,055 [INFO] Step[350/2713]: training loss : 0.9442439305782319 TRAIN  loss dict:  {'classification_loss': 0.9442439305782319}
2025-01-13 03:51:07,915 [INFO] Step[400/2713]: training loss : 0.9441763615608215 TRAIN  loss dict:  {'classification_loss': 0.9441763615608215}
2025-01-13 03:51:19,826 [INFO] Step[450/2713]: training loss : 0.945332282781601 TRAIN  loss dict:  {'classification_loss': 0.945332282781601}
2025-01-13 03:51:31,761 [INFO] Step[500/2713]: training loss : 0.9446060681343078 TRAIN  loss dict:  {'classification_loss': 0.9446060681343078}
2025-01-13 03:51:43,683 [INFO] Step[550/2713]: training loss : 0.9443762362003326 TRAIN  loss dict:  {'classification_loss': 0.9443762362003326}
2025-01-13 03:51:55,569 [INFO] Step[600/2713]: training loss : 0.9433114755153656 TRAIN  loss dict:  {'classification_loss': 0.9433114755153656}
2025-01-13 03:52:07,527 [INFO] Step[650/2713]: training loss : 0.9537725377082825 TRAIN  loss dict:  {'classification_loss': 0.9537725377082825}
2025-01-13 03:52:19,453 [INFO] Step[700/2713]: training loss : 0.9437617480754852 TRAIN  loss dict:  {'classification_loss': 0.9437617480754852}
2025-01-13 03:52:31,363 [INFO] Step[750/2713]: training loss : 0.9473910558223725 TRAIN  loss dict:  {'classification_loss': 0.9473910558223725}
2025-01-13 03:52:43,264 [INFO] Step[800/2713]: training loss : 0.9469623863697052 TRAIN  loss dict:  {'classification_loss': 0.9469623863697052}
2025-01-13 03:52:55,189 [INFO] Step[850/2713]: training loss : 0.942071579694748 TRAIN  loss dict:  {'classification_loss': 0.942071579694748}
2025-01-13 03:53:07,122 [INFO] Step[900/2713]: training loss : 0.9431210744380951 TRAIN  loss dict:  {'classification_loss': 0.9431210744380951}
2025-01-13 03:53:19,035 [INFO] Step[950/2713]: training loss : 0.9490627646446228 TRAIN  loss dict:  {'classification_loss': 0.9490627646446228}
2025-01-13 03:53:30,886 [INFO] Step[1000/2713]: training loss : 0.9442672407627106 TRAIN  loss dict:  {'classification_loss': 0.9442672407627106}
2025-01-13 03:53:42,800 [INFO] Step[1050/2713]: training loss : 0.9484675645828247 TRAIN  loss dict:  {'classification_loss': 0.9484675645828247}
2025-01-13 03:53:54,705 [INFO] Step[1100/2713]: training loss : 0.9391927790641784 TRAIN  loss dict:  {'classification_loss': 0.9391927790641784}
2025-01-13 03:54:06,645 [INFO] Step[1150/2713]: training loss : 0.9432633066177368 TRAIN  loss dict:  {'classification_loss': 0.9432633066177368}
2025-01-13 03:54:18,548 [INFO] Step[1200/2713]: training loss : 0.9410760760307312 TRAIN  loss dict:  {'classification_loss': 0.9410760760307312}
2025-01-13 03:54:30,434 [INFO] Step[1250/2713]: training loss : 0.9444239342212677 TRAIN  loss dict:  {'classification_loss': 0.9444239342212677}
2025-01-13 03:54:42,342 [INFO] Step[1300/2713]: training loss : 0.9449958765506744 TRAIN  loss dict:  {'classification_loss': 0.9449958765506744}
2025-01-13 03:54:54,269 [INFO] Step[1350/2713]: training loss : 0.9722612690925598 TRAIN  loss dict:  {'classification_loss': 0.9722612690925598}
2025-01-13 03:55:06,204 [INFO] Step[1400/2713]: training loss : 0.9419497215747833 TRAIN  loss dict:  {'classification_loss': 0.9419497215747833}
2025-01-13 03:55:18,138 [INFO] Step[1450/2713]: training loss : 0.9453056156635284 TRAIN  loss dict:  {'classification_loss': 0.9453056156635284}
2025-01-13 03:55:29,986 [INFO] Step[1500/2713]: training loss : 0.9468740367889404 TRAIN  loss dict:  {'classification_loss': 0.9468740367889404}
2025-01-13 03:55:41,894 [INFO] Step[1550/2713]: training loss : 0.948446329832077 TRAIN  loss dict:  {'classification_loss': 0.948446329832077}
2025-01-13 03:55:53,795 [INFO] Step[1600/2713]: training loss : 0.9457987368106842 TRAIN  loss dict:  {'classification_loss': 0.9457987368106842}
2025-01-13 03:56:05,776 [INFO] Step[1650/2713]: training loss : 0.9452103006839753 TRAIN  loss dict:  {'classification_loss': 0.9452103006839753}
2025-01-13 03:56:17,701 [INFO] Step[1700/2713]: training loss : 0.9421609687805176 TRAIN  loss dict:  {'classification_loss': 0.9421609687805176}
2025-01-13 03:56:29,657 [INFO] Step[1750/2713]: training loss : 0.9412014281749725 TRAIN  loss dict:  {'classification_loss': 0.9412014281749725}
2025-01-13 03:56:41,549 [INFO] Step[1800/2713]: training loss : 0.9433329784870148 TRAIN  loss dict:  {'classification_loss': 0.9433329784870148}
2025-01-13 03:56:53,445 [INFO] Step[1850/2713]: training loss : 0.9471983885765076 TRAIN  loss dict:  {'classification_loss': 0.9471983885765076}
2025-01-13 03:57:05,375 [INFO] Step[1900/2713]: training loss : 0.9446716296672821 TRAIN  loss dict:  {'classification_loss': 0.9446716296672821}
2025-01-13 03:57:17,280 [INFO] Step[1950/2713]: training loss : 0.9477317953109741 TRAIN  loss dict:  {'classification_loss': 0.9477317953109741}
2025-01-13 03:57:29,190 [INFO] Step[2000/2713]: training loss : 0.9509546279907226 TRAIN  loss dict:  {'classification_loss': 0.9509546279907226}
2025-01-13 03:57:41,068 [INFO] Step[2050/2713]: training loss : 0.9447630631923676 TRAIN  loss dict:  {'classification_loss': 0.9447630631923676}
2025-01-13 03:57:52,942 [INFO] Step[2100/2713]: training loss : 0.959783593416214 TRAIN  loss dict:  {'classification_loss': 0.959783593416214}
2025-01-13 03:58:04,838 [INFO] Step[2150/2713]: training loss : 0.9430344259738922 TRAIN  loss dict:  {'classification_loss': 0.9430344259738922}
2025-01-13 03:58:16,715 [INFO] Step[2200/2713]: training loss : 0.9447070574760437 TRAIN  loss dict:  {'classification_loss': 0.9447070574760437}
2025-01-13 03:58:28,648 [INFO] Step[2250/2713]: training loss : 0.9410379147529602 TRAIN  loss dict:  {'classification_loss': 0.9410379147529602}
2025-01-13 03:58:40,590 [INFO] Step[2300/2713]: training loss : 0.9462772381305694 TRAIN  loss dict:  {'classification_loss': 0.9462772381305694}
2025-01-13 03:58:52,468 [INFO] Step[2350/2713]: training loss : 0.9446034049987793 TRAIN  loss dict:  {'classification_loss': 0.9446034049987793}
2025-01-13 03:59:04,363 [INFO] Step[2400/2713]: training loss : 0.9433513450622558 TRAIN  loss dict:  {'classification_loss': 0.9433513450622558}
2025-01-13 03:59:16,295 [INFO] Step[2450/2713]: training loss : 0.9453013908863067 TRAIN  loss dict:  {'classification_loss': 0.9453013908863067}
2025-01-13 03:59:28,193 [INFO] Step[2500/2713]: training loss : 0.9432072198390961 TRAIN  loss dict:  {'classification_loss': 0.9432072198390961}
2025-01-13 03:59:40,066 [INFO] Step[2550/2713]: training loss : 0.9488363647460938 TRAIN  loss dict:  {'classification_loss': 0.9488363647460938}
2025-01-13 03:59:52,002 [INFO] Step[2600/2713]: training loss : 0.9438312530517579 TRAIN  loss dict:  {'classification_loss': 0.9438312530517579}
2025-01-13 04:00:03,918 [INFO] Step[2650/2713]: training loss : 0.9495724260807037 TRAIN  loss dict:  {'classification_loss': 0.9495724260807037}
2025-01-13 04:00:15,759 [INFO] Step[2700/2713]: training loss : 0.9444257915019989 TRAIN  loss dict:  {'classification_loss': 0.9444257915019989}
2025-01-13 04:01:43,687 [INFO] Label accuracies statistics:
2025-01-13 04:01:43,687 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.25, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 1.0, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.5, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 1.0, 239: 1.0, 240: 0.75, 241: 1.0, 242: 1.0, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 04:01:43,689 [INFO] [63] TRAIN  loss: 0.9459545122623619 acc: 0.9996314043494287
2025-01-13 04:01:43,689 [INFO] [63] TRAIN  loss dict: {'classification_loss': 0.9459545122623619}
2025-01-13 04:01:43,690 [INFO] [63] VALIDATION loss: 1.688078719534372 VALIDATION acc: 0.8300940438871474
2025-01-13 04:01:43,690 [INFO] [63] VALIDATION loss dict: {'classification_loss': 1.688078719534372}
2025-01-13 04:01:43,690 [INFO] 
2025-01-13 04:02:00,792 [INFO] Step[50/2713]: training loss : 0.9434924519062042 TRAIN  loss dict:  {'classification_loss': 0.9434924519062042}
2025-01-13 04:02:12,701 [INFO] Step[100/2713]: training loss : 0.9458953642845154 TRAIN  loss dict:  {'classification_loss': 0.9458953642845154}
2025-01-13 04:02:24,651 [INFO] Step[150/2713]: training loss : 0.9480775368213653 TRAIN  loss dict:  {'classification_loss': 0.9480775368213653}
2025-01-13 04:02:36,543 [INFO] Step[200/2713]: training loss : 0.9441271293163299 TRAIN  loss dict:  {'classification_loss': 0.9441271293163299}
2025-01-13 04:02:48,459 [INFO] Step[250/2713]: training loss : 0.9407315051555634 TRAIN  loss dict:  {'classification_loss': 0.9407315051555634}
2025-01-13 04:03:00,410 [INFO] Step[300/2713]: training loss : 0.9429029130935669 TRAIN  loss dict:  {'classification_loss': 0.9429029130935669}
2025-01-13 04:03:12,343 [INFO] Step[350/2713]: training loss : 0.9432181513309479 TRAIN  loss dict:  {'classification_loss': 0.9432181513309479}
2025-01-13 04:03:24,284 [INFO] Step[400/2713]: training loss : 0.9413452005386352 TRAIN  loss dict:  {'classification_loss': 0.9413452005386352}
2025-01-13 04:03:36,249 [INFO] Step[450/2713]: training loss : 0.940782105922699 TRAIN  loss dict:  {'classification_loss': 0.940782105922699}
2025-01-13 04:03:48,121 [INFO] Step[500/2713]: training loss : 0.9429220247268677 TRAIN  loss dict:  {'classification_loss': 0.9429220247268677}
2025-01-13 04:04:00,079 [INFO] Step[550/2713]: training loss : 0.9421760368347168 TRAIN  loss dict:  {'classification_loss': 0.9421760368347168}
2025-01-13 04:04:12,037 [INFO] Step[600/2713]: training loss : 0.9448073256015778 TRAIN  loss dict:  {'classification_loss': 0.9448073256015778}
2025-01-13 04:04:23,981 [INFO] Step[650/2713]: training loss : 0.9426504409313202 TRAIN  loss dict:  {'classification_loss': 0.9426504409313202}
2025-01-13 04:04:35,886 [INFO] Step[700/2713]: training loss : 0.9455838358402252 TRAIN  loss dict:  {'classification_loss': 0.9455838358402252}
2025-01-13 04:04:47,853 [INFO] Step[750/2713]: training loss : 0.9451668858528137 TRAIN  loss dict:  {'classification_loss': 0.9451668858528137}
2025-01-13 04:05:00,078 [INFO] Step[800/2713]: training loss : 0.9422030866146087 TRAIN  loss dict:  {'classification_loss': 0.9422030866146087}
2025-01-13 04:05:12,583 [INFO] Step[850/2713]: training loss : 0.944846646785736 TRAIN  loss dict:  {'classification_loss': 0.944846646785736}
2025-01-13 04:05:25,062 [INFO] Step[900/2713]: training loss : 0.9575168216228485 TRAIN  loss dict:  {'classification_loss': 0.9575168216228485}
2025-01-13 04:05:37,343 [INFO] Step[950/2713]: training loss : 0.9418978381156922 TRAIN  loss dict:  {'classification_loss': 0.9418978381156922}
2025-01-13 04:05:49,929 [INFO] Step[1000/2713]: training loss : 0.9433410727977752 TRAIN  loss dict:  {'classification_loss': 0.9433410727977752}
2025-01-13 04:06:02,177 [INFO] Step[1050/2713]: training loss : 0.9431279265880584 TRAIN  loss dict:  {'classification_loss': 0.9431279265880584}
2025-01-13 04:06:14,884 [INFO] Step[1100/2713]: training loss : 0.9455697226524353 TRAIN  loss dict:  {'classification_loss': 0.9455697226524353}
2025-01-13 04:06:28,559 [INFO] Step[1150/2713]: training loss : 0.9433558368682862 TRAIN  loss dict:  {'classification_loss': 0.9433558368682862}
2025-01-13 04:06:41,960 [INFO] Step[1200/2713]: training loss : 0.9432910907268525 TRAIN  loss dict:  {'classification_loss': 0.9432910907268525}
2025-01-13 04:06:54,108 [INFO] Step[1250/2713]: training loss : 0.9488574755191803 TRAIN  loss dict:  {'classification_loss': 0.9488574755191803}
2025-01-13 04:07:06,007 [INFO] Step[1300/2713]: training loss : 0.948208167552948 TRAIN  loss dict:  {'classification_loss': 0.948208167552948}
2025-01-13 04:07:17,866 [INFO] Step[1350/2713]: training loss : 0.9495454680919647 TRAIN  loss dict:  {'classification_loss': 0.9495454680919647}
2025-01-13 04:07:29,718 [INFO] Step[1400/2713]: training loss : 0.9428820705413818 TRAIN  loss dict:  {'classification_loss': 0.9428820705413818}
2025-01-13 04:07:41,617 [INFO] Step[1450/2713]: training loss : 0.9456361329555512 TRAIN  loss dict:  {'classification_loss': 0.9456361329555512}
2025-01-13 04:07:53,473 [INFO] Step[1500/2713]: training loss : 0.9454494774341583 TRAIN  loss dict:  {'classification_loss': 0.9454494774341583}
2025-01-13 04:08:05,358 [INFO] Step[1550/2713]: training loss : 0.9459024035930633 TRAIN  loss dict:  {'classification_loss': 0.9459024035930633}
2025-01-13 04:08:17,236 [INFO] Step[1600/2713]: training loss : 0.9461459648609162 TRAIN  loss dict:  {'classification_loss': 0.9461459648609162}
2025-01-13 04:08:29,096 [INFO] Step[1650/2713]: training loss : 0.942797030210495 TRAIN  loss dict:  {'classification_loss': 0.942797030210495}
2025-01-13 04:08:40,947 [INFO] Step[1700/2713]: training loss : 0.9465705442428589 TRAIN  loss dict:  {'classification_loss': 0.9465705442428589}
2025-01-13 04:08:52,803 [INFO] Step[1750/2713]: training loss : 0.940496279001236 TRAIN  loss dict:  {'classification_loss': 0.940496279001236}
2025-01-13 04:09:04,742 [INFO] Step[1800/2713]: training loss : 0.9441559088230133 TRAIN  loss dict:  {'classification_loss': 0.9441559088230133}
2025-01-13 04:09:16,636 [INFO] Step[1850/2713]: training loss : 0.944033864736557 TRAIN  loss dict:  {'classification_loss': 0.944033864736557}
2025-01-13 04:09:28,503 [INFO] Step[1900/2713]: training loss : 0.949102942943573 TRAIN  loss dict:  {'classification_loss': 0.949102942943573}
2025-01-13 04:09:40,419 [INFO] Step[1950/2713]: training loss : 0.9436459016799926 TRAIN  loss dict:  {'classification_loss': 0.9436459016799926}
2025-01-13 04:09:52,314 [INFO] Step[2000/2713]: training loss : 0.9498285257816315 TRAIN  loss dict:  {'classification_loss': 0.9498285257816315}
2025-01-13 04:10:04,213 [INFO] Step[2050/2713]: training loss : 0.9495505928993225 TRAIN  loss dict:  {'classification_loss': 0.9495505928993225}
2025-01-13 04:10:16,107 [INFO] Step[2100/2713]: training loss : 0.9481458914279938 TRAIN  loss dict:  {'classification_loss': 0.9481458914279938}
2025-01-13 04:10:28,018 [INFO] Step[2150/2713]: training loss : 0.9418246722221375 TRAIN  loss dict:  {'classification_loss': 0.9418246722221375}
2025-01-13 04:10:39,896 [INFO] Step[2200/2713]: training loss : 0.9412181222438812 TRAIN  loss dict:  {'classification_loss': 0.9412181222438812}
2025-01-13 04:10:51,795 [INFO] Step[2250/2713]: training loss : 0.9451360392570496 TRAIN  loss dict:  {'classification_loss': 0.9451360392570496}
2025-01-13 04:11:03,628 [INFO] Step[2300/2713]: training loss : 0.9475429558753967 TRAIN  loss dict:  {'classification_loss': 0.9475429558753967}
2025-01-13 04:11:15,486 [INFO] Step[2350/2713]: training loss : 0.9429017758369446 TRAIN  loss dict:  {'classification_loss': 0.9429017758369446}
2025-01-13 04:11:27,391 [INFO] Step[2400/2713]: training loss : 0.9455620491504669 TRAIN  loss dict:  {'classification_loss': 0.9455620491504669}
2025-01-13 04:11:39,264 [INFO] Step[2450/2713]: training loss : 0.9454355871677399 TRAIN  loss dict:  {'classification_loss': 0.9454355871677399}
2025-01-13 04:11:51,116 [INFO] Step[2500/2713]: training loss : 0.9441308033466339 TRAIN  loss dict:  {'classification_loss': 0.9441308033466339}
2025-01-13 04:12:03,060 [INFO] Step[2550/2713]: training loss : 0.9438597702980042 TRAIN  loss dict:  {'classification_loss': 0.9438597702980042}
2025-01-13 04:12:14,930 [INFO] Step[2600/2713]: training loss : 0.9411624670028687 TRAIN  loss dict:  {'classification_loss': 0.9411624670028687}
2025-01-13 04:12:26,865 [INFO] Step[2650/2713]: training loss : 0.9480110824108123 TRAIN  loss dict:  {'classification_loss': 0.9480110824108123}
2025-01-13 04:12:38,763 [INFO] Step[2700/2713]: training loss : 0.9417061245441437 TRAIN  loss dict:  {'classification_loss': 0.9417061245441437}
2025-01-13 04:14:06,386 [INFO] Label accuracies statistics:
2025-01-13 04:14:06,386 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.5, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.75, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 0.75, 281: 1.0, 282: 1.0, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 0.75, 381: 0.25, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 04:14:06,388 [INFO] [64] TRAIN  loss: 0.9447769568584258 acc: 0.9997542695662858
2025-01-13 04:14:06,388 [INFO] [64] TRAIN  loss dict: {'classification_loss': 0.9447769568584258}
2025-01-13 04:14:06,388 [INFO] [64] VALIDATION loss: 1.6617037750276409 VALIDATION acc: 0.8401253918495298
2025-01-13 04:14:06,388 [INFO] [64] VALIDATION loss dict: {'classification_loss': 1.6617037750276409}
2025-01-13 04:14:06,389 [INFO] 
2025-01-13 04:14:24,511 [INFO] Step[50/2713]: training loss : 0.9421372318267822 TRAIN  loss dict:  {'classification_loss': 0.9421372318267822}
2025-01-13 04:14:36,392 [INFO] Step[100/2713]: training loss : 0.946690092086792 TRAIN  loss dict:  {'classification_loss': 0.946690092086792}
2025-01-13 04:14:48,291 [INFO] Step[150/2713]: training loss : 0.9408232963085175 TRAIN  loss dict:  {'classification_loss': 0.9408232963085175}
2025-01-13 04:15:00,189 [INFO] Step[200/2713]: training loss : 0.9448830842971802 TRAIN  loss dict:  {'classification_loss': 0.9448830842971802}
2025-01-13 04:15:12,071 [INFO] Step[250/2713]: training loss : 0.9459810948371887 TRAIN  loss dict:  {'classification_loss': 0.9459810948371887}
2025-01-13 04:15:23,942 [INFO] Step[300/2713]: training loss : 0.9423563814163208 TRAIN  loss dict:  {'classification_loss': 0.9423563814163208}
2025-01-13 04:15:35,864 [INFO] Step[350/2713]: training loss : 0.9451834905147553 TRAIN  loss dict:  {'classification_loss': 0.9451834905147553}
2025-01-13 04:15:47,788 [INFO] Step[400/2713]: training loss : 0.9441680943965912 TRAIN  loss dict:  {'classification_loss': 0.9441680943965912}
2025-01-13 04:15:59,735 [INFO] Step[450/2713]: training loss : 0.9406907308101654 TRAIN  loss dict:  {'classification_loss': 0.9406907308101654}
2025-01-13 04:16:11,667 [INFO] Step[500/2713]: training loss : 0.9457688975334168 TRAIN  loss dict:  {'classification_loss': 0.9457688975334168}
2025-01-13 04:16:23,575 [INFO] Step[550/2713]: training loss : 0.9426725077629089 TRAIN  loss dict:  {'classification_loss': 0.9426725077629089}
2025-01-13 04:16:35,569 [INFO] Step[600/2713]: training loss : 0.9419944834709167 TRAIN  loss dict:  {'classification_loss': 0.9419944834709167}
2025-01-13 04:16:47,474 [INFO] Step[650/2713]: training loss : 0.9470739436149597 TRAIN  loss dict:  {'classification_loss': 0.9470739436149597}
2025-01-13 04:16:59,361 [INFO] Step[700/2713]: training loss : 0.9435280215740204 TRAIN  loss dict:  {'classification_loss': 0.9435280215740204}
2025-01-13 04:17:11,237 [INFO] Step[750/2713]: training loss : 0.9408743643760681 TRAIN  loss dict:  {'classification_loss': 0.9408743643760681}
2025-01-13 04:17:23,153 [INFO] Step[800/2713]: training loss : 0.9428471529483795 TRAIN  loss dict:  {'classification_loss': 0.9428471529483795}
2025-01-13 04:17:35,090 [INFO] Step[850/2713]: training loss : 0.9437614679336548 TRAIN  loss dict:  {'classification_loss': 0.9437614679336548}
2025-01-13 04:17:46,970 [INFO] Step[900/2713]: training loss : 0.949999384880066 TRAIN  loss dict:  {'classification_loss': 0.949999384880066}
2025-01-13 04:17:58,840 [INFO] Step[950/2713]: training loss : 0.9443278539180756 TRAIN  loss dict:  {'classification_loss': 0.9443278539180756}
2025-01-13 04:18:10,792 [INFO] Step[1000/2713]: training loss : 0.9470737838745117 TRAIN  loss dict:  {'classification_loss': 0.9470737838745117}
2025-01-13 04:18:22,701 [INFO] Step[1050/2713]: training loss : 0.9450025188922883 TRAIN  loss dict:  {'classification_loss': 0.9450025188922883}
2025-01-13 04:18:34,583 [INFO] Step[1100/2713]: training loss : 0.9469552278518677 TRAIN  loss dict:  {'classification_loss': 0.9469552278518677}
2025-01-13 04:18:46,497 [INFO] Step[1150/2713]: training loss : 0.9419010758399964 TRAIN  loss dict:  {'classification_loss': 0.9419010758399964}
2025-01-13 04:18:58,407 [INFO] Step[1200/2713]: training loss : 0.94346395611763 TRAIN  loss dict:  {'classification_loss': 0.94346395611763}
2025-01-13 04:19:10,358 [INFO] Step[1250/2713]: training loss : 0.946783344745636 TRAIN  loss dict:  {'classification_loss': 0.946783344745636}
2025-01-13 04:19:22,251 [INFO] Step[1300/2713]: training loss : 0.9418669641017914 TRAIN  loss dict:  {'classification_loss': 0.9418669641017914}
2025-01-13 04:19:34,156 [INFO] Step[1350/2713]: training loss : 0.9482454121112823 TRAIN  loss dict:  {'classification_loss': 0.9482454121112823}
2025-01-13 04:19:46,104 [INFO] Step[1400/2713]: training loss : 0.942612771987915 TRAIN  loss dict:  {'classification_loss': 0.942612771987915}
2025-01-13 04:19:57,998 [INFO] Step[1450/2713]: training loss : 0.9454591512680054 TRAIN  loss dict:  {'classification_loss': 0.9454591512680054}
2025-01-13 04:20:09,874 [INFO] Step[1500/2713]: training loss : 0.9442190086841583 TRAIN  loss dict:  {'classification_loss': 0.9442190086841583}
2025-01-13 04:20:21,818 [INFO] Step[1550/2713]: training loss : 0.9389378583431244 TRAIN  loss dict:  {'classification_loss': 0.9389378583431244}
2025-01-13 04:20:33,751 [INFO] Step[1600/2713]: training loss : 0.9664541888236999 TRAIN  loss dict:  {'classification_loss': 0.9664541888236999}
2025-01-13 04:20:45,659 [INFO] Step[1650/2713]: training loss : 0.9436142146587372 TRAIN  loss dict:  {'classification_loss': 0.9436142146587372}
2025-01-13 04:20:57,564 [INFO] Step[1700/2713]: training loss : 0.9438397920131684 TRAIN  loss dict:  {'classification_loss': 0.9438397920131684}
2025-01-13 04:21:09,459 [INFO] Step[1750/2713]: training loss : 0.9416802871227264 TRAIN  loss dict:  {'classification_loss': 0.9416802871227264}
2025-01-13 04:21:21,341 [INFO] Step[1800/2713]: training loss : 0.9418688201904297 TRAIN  loss dict:  {'classification_loss': 0.9418688201904297}
2025-01-13 04:21:33,285 [INFO] Step[1850/2713]: training loss : 0.9414327347278595 TRAIN  loss dict:  {'classification_loss': 0.9414327347278595}
2025-01-13 04:21:45,202 [INFO] Step[1900/2713]: training loss : 0.9494290912151336 TRAIN  loss dict:  {'classification_loss': 0.9494290912151336}
2025-01-13 04:21:57,149 [INFO] Step[1950/2713]: training loss : 0.9458352160453797 TRAIN  loss dict:  {'classification_loss': 0.9458352160453797}
2025-01-13 04:22:09,081 [INFO] Step[2000/2713]: training loss : 0.9446473670005798 TRAIN  loss dict:  {'classification_loss': 0.9446473670005798}
2025-01-13 04:22:20,998 [INFO] Step[2050/2713]: training loss : 0.9424890840053558 TRAIN  loss dict:  {'classification_loss': 0.9424890840053558}
2025-01-13 04:22:32,884 [INFO] Step[2100/2713]: training loss : 0.9418169224262237 TRAIN  loss dict:  {'classification_loss': 0.9418169224262237}
2025-01-13 04:22:44,791 [INFO] Step[2150/2713]: training loss : 0.9425152719020844 TRAIN  loss dict:  {'classification_loss': 0.9425152719020844}
2025-01-13 04:22:56,661 [INFO] Step[2200/2713]: training loss : 0.9471899437904358 TRAIN  loss dict:  {'classification_loss': 0.9471899437904358}
2025-01-13 04:23:08,610 [INFO] Step[2250/2713]: training loss : 0.9541497719287872 TRAIN  loss dict:  {'classification_loss': 0.9541497719287872}
2025-01-13 04:23:20,523 [INFO] Step[2300/2713]: training loss : 0.9446414017677307 TRAIN  loss dict:  {'classification_loss': 0.9446414017677307}
2025-01-13 04:23:32,391 [INFO] Step[2350/2713]: training loss : 0.9422459721565246 TRAIN  loss dict:  {'classification_loss': 0.9422459721565246}
2025-01-13 04:23:44,392 [INFO] Step[2400/2713]: training loss : 0.9416805565357208 TRAIN  loss dict:  {'classification_loss': 0.9416805565357208}
2025-01-13 04:23:56,670 [INFO] Step[2450/2713]: training loss : 0.9461978781223297 TRAIN  loss dict:  {'classification_loss': 0.9461978781223297}
2025-01-13 04:24:09,125 [INFO] Step[2500/2713]: training loss : 0.9413749074935913 TRAIN  loss dict:  {'classification_loss': 0.9413749074935913}
2025-01-13 04:24:21,398 [INFO] Step[2550/2713]: training loss : 0.9461740934848786 TRAIN  loss dict:  {'classification_loss': 0.9461740934848786}
2025-01-13 04:24:33,882 [INFO] Step[2600/2713]: training loss : 0.9450071907043457 TRAIN  loss dict:  {'classification_loss': 0.9450071907043457}
2025-01-13 04:24:46,350 [INFO] Step[2650/2713]: training loss : 0.9433583414554596 TRAIN  loss dict:  {'classification_loss': 0.9433583414554596}
2025-01-13 04:24:58,731 [INFO] Step[2700/2713]: training loss : 0.9421637058258057 TRAIN  loss dict:  {'classification_loss': 0.9421637058258057}
2025-01-13 04:26:42,122 [INFO] Label accuracies statistics:
2025-01-13 04:26:42,122 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 1.0, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 1.0, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.5, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 04:26:42,125 [INFO] [65] TRAIN  loss: 0.9445782380485465 acc: 0.9997542695662858
2025-01-13 04:26:42,125 [INFO] [65] TRAIN  loss dict: {'classification_loss': 0.9445782380485465}
2025-01-13 04:26:42,125 [INFO] [65] VALIDATION loss: 1.7167247633512754 VALIDATION acc: 0.8288401253918495
2025-01-13 04:26:42,125 [INFO] [65] VALIDATION loss dict: {'classification_loss': 1.7167247633512754}
2025-01-13 04:26:42,125 [INFO] 
2025-01-13 04:26:59,810 [INFO] Step[50/2713]: training loss : 0.9406332182884216 TRAIN  loss dict:  {'classification_loss': 0.9406332182884216}
2025-01-13 04:27:11,656 [INFO] Step[100/2713]: training loss : 0.9483337128162384 TRAIN  loss dict:  {'classification_loss': 0.9483337128162384}
2025-01-13 04:27:23,579 [INFO] Step[150/2713]: training loss : 0.9438214600086212 TRAIN  loss dict:  {'classification_loss': 0.9438214600086212}
2025-01-13 04:27:35,483 [INFO] Step[200/2713]: training loss : 0.943789176940918 TRAIN  loss dict:  {'classification_loss': 0.943789176940918}
2025-01-13 04:27:47,365 [INFO] Step[250/2713]: training loss : 0.9422909259796143 TRAIN  loss dict:  {'classification_loss': 0.9422909259796143}
2025-01-13 04:27:59,267 [INFO] Step[300/2713]: training loss : 0.9448567950725555 TRAIN  loss dict:  {'classification_loss': 0.9448567950725555}
2025-01-13 04:28:11,168 [INFO] Step[350/2713]: training loss : 0.9404490709304809 TRAIN  loss dict:  {'classification_loss': 0.9404490709304809}
2025-01-13 04:28:23,057 [INFO] Step[400/2713]: training loss : 0.9429706525802612 TRAIN  loss dict:  {'classification_loss': 0.9429706525802612}
2025-01-13 04:28:35,024 [INFO] Step[450/2713]: training loss : 0.9423394656181335 TRAIN  loss dict:  {'classification_loss': 0.9423394656181335}
2025-01-13 04:28:46,890 [INFO] Step[500/2713]: training loss : 0.9432133936882019 TRAIN  loss dict:  {'classification_loss': 0.9432133936882019}
2025-01-13 04:28:58,773 [INFO] Step[550/2713]: training loss : 0.9486154353618622 TRAIN  loss dict:  {'classification_loss': 0.9486154353618622}
2025-01-13 04:29:10,673 [INFO] Step[600/2713]: training loss : 0.944321916103363 TRAIN  loss dict:  {'classification_loss': 0.944321916103363}
2025-01-13 04:29:22,600 [INFO] Step[650/2713]: training loss : 0.9445130467414856 TRAIN  loss dict:  {'classification_loss': 0.9445130467414856}
2025-01-13 04:29:34,508 [INFO] Step[700/2713]: training loss : 0.9408131885528564 TRAIN  loss dict:  {'classification_loss': 0.9408131885528564}
2025-01-13 04:29:46,372 [INFO] Step[750/2713]: training loss : 0.9429894483089447 TRAIN  loss dict:  {'classification_loss': 0.9429894483089447}
2025-01-13 04:29:58,269 [INFO] Step[800/2713]: training loss : 0.9436857807636261 TRAIN  loss dict:  {'classification_loss': 0.9436857807636261}
2025-01-13 04:30:10,216 [INFO] Step[850/2713]: training loss : 0.959400178194046 TRAIN  loss dict:  {'classification_loss': 0.959400178194046}
2025-01-13 04:30:22,127 [INFO] Step[900/2713]: training loss : 0.9428419864177704 TRAIN  loss dict:  {'classification_loss': 0.9428419864177704}
2025-01-13 04:30:33,993 [INFO] Step[950/2713]: training loss : 0.943652275800705 TRAIN  loss dict:  {'classification_loss': 0.943652275800705}
2025-01-13 04:30:45,904 [INFO] Step[1000/2713]: training loss : 0.948450231552124 TRAIN  loss dict:  {'classification_loss': 0.948450231552124}
2025-01-13 04:30:57,808 [INFO] Step[1050/2713]: training loss : 0.9431673610210418 TRAIN  loss dict:  {'classification_loss': 0.9431673610210418}
2025-01-13 04:31:09,729 [INFO] Step[1100/2713]: training loss : 0.941217623949051 TRAIN  loss dict:  {'classification_loss': 0.941217623949051}
2025-01-13 04:31:21,637 [INFO] Step[1150/2713]: training loss : 0.9429262387752533 TRAIN  loss dict:  {'classification_loss': 0.9429262387752533}
2025-01-13 04:31:33,545 [INFO] Step[1200/2713]: training loss : 0.9437967014312744 TRAIN  loss dict:  {'classification_loss': 0.9437967014312744}
2025-01-13 04:31:45,437 [INFO] Step[1250/2713]: training loss : 0.9413876962661744 TRAIN  loss dict:  {'classification_loss': 0.9413876962661744}
2025-01-13 04:31:57,318 [INFO] Step[1300/2713]: training loss : 0.9438501131534577 TRAIN  loss dict:  {'classification_loss': 0.9438501131534577}
2025-01-13 04:32:09,244 [INFO] Step[1350/2713]: training loss : 0.9429148292541504 TRAIN  loss dict:  {'classification_loss': 0.9429148292541504}
2025-01-13 04:32:21,137 [INFO] Step[1400/2713]: training loss : 0.9409557712078095 TRAIN  loss dict:  {'classification_loss': 0.9409557712078095}
2025-01-13 04:32:33,069 [INFO] Step[1450/2713]: training loss : 0.9417991077899933 TRAIN  loss dict:  {'classification_loss': 0.9417991077899933}
2025-01-13 04:32:44,960 [INFO] Step[1500/2713]: training loss : 0.9430289161205292 TRAIN  loss dict:  {'classification_loss': 0.9430289161205292}
2025-01-13 04:32:56,860 [INFO] Step[1550/2713]: training loss : 0.9512733364105225 TRAIN  loss dict:  {'classification_loss': 0.9512733364105225}
2025-01-13 04:33:08,766 [INFO] Step[1600/2713]: training loss : 0.9412942707538605 TRAIN  loss dict:  {'classification_loss': 0.9412942707538605}
2025-01-13 04:33:20,703 [INFO] Step[1650/2713]: training loss : 0.9410502362251282 TRAIN  loss dict:  {'classification_loss': 0.9410502362251282}
2025-01-13 04:33:32,596 [INFO] Step[1700/2713]: training loss : 0.9409374678134919 TRAIN  loss dict:  {'classification_loss': 0.9409374678134919}
2025-01-13 04:33:44,480 [INFO] Step[1750/2713]: training loss : 0.9440962374210358 TRAIN  loss dict:  {'classification_loss': 0.9440962374210358}
2025-01-13 04:33:56,377 [INFO] Step[1800/2713]: training loss : 0.9395153832435608 TRAIN  loss dict:  {'classification_loss': 0.9395153832435608}
2025-01-13 04:34:08,314 [INFO] Step[1850/2713]: training loss : 0.9471009075641632 TRAIN  loss dict:  {'classification_loss': 0.9471009075641632}
2025-01-13 04:34:20,218 [INFO] Step[1900/2713]: training loss : 0.943262654542923 TRAIN  loss dict:  {'classification_loss': 0.943262654542923}
2025-01-13 04:34:32,154 [INFO] Step[1950/2713]: training loss : 0.9426511454582215 TRAIN  loss dict:  {'classification_loss': 0.9426511454582215}
2025-01-13 04:34:44,033 [INFO] Step[2000/2713]: training loss : 0.9395505213737487 TRAIN  loss dict:  {'classification_loss': 0.9395505213737487}
2025-01-13 04:34:55,919 [INFO] Step[2050/2713]: training loss : 0.9432281303405762 TRAIN  loss dict:  {'classification_loss': 0.9432281303405762}
2025-01-13 04:35:07,818 [INFO] Step[2100/2713]: training loss : 0.9448980438709259 TRAIN  loss dict:  {'classification_loss': 0.9448980438709259}
2025-01-13 04:35:19,727 [INFO] Step[2150/2713]: training loss : 0.9436226761341096 TRAIN  loss dict:  {'classification_loss': 0.9436226761341096}
2025-01-13 04:35:31,646 [INFO] Step[2200/2713]: training loss : 0.9505899000167847 TRAIN  loss dict:  {'classification_loss': 0.9505899000167847}
2025-01-13 04:35:43,553 [INFO] Step[2250/2713]: training loss : 0.9440485465526581 TRAIN  loss dict:  {'classification_loss': 0.9440485465526581}
2025-01-13 04:35:55,500 [INFO] Step[2300/2713]: training loss : 0.9413369679450989 TRAIN  loss dict:  {'classification_loss': 0.9413369679450989}
2025-01-13 04:36:07,426 [INFO] Step[2350/2713]: training loss : 0.9520953559875488 TRAIN  loss dict:  {'classification_loss': 0.9520953559875488}
2025-01-13 04:36:19,307 [INFO] Step[2400/2713]: training loss : 0.9525351905822754 TRAIN  loss dict:  {'classification_loss': 0.9525351905822754}
2025-01-13 04:36:31,255 [INFO] Step[2450/2713]: training loss : 0.9394435954093933 TRAIN  loss dict:  {'classification_loss': 0.9394435954093933}
2025-01-13 04:36:43,149 [INFO] Step[2500/2713]: training loss : 0.9418430781364441 TRAIN  loss dict:  {'classification_loss': 0.9418430781364441}
2025-01-13 04:36:55,095 [INFO] Step[2550/2713]: training loss : 0.9445863163471222 TRAIN  loss dict:  {'classification_loss': 0.9445863163471222}
2025-01-13 04:37:06,981 [INFO] Step[2600/2713]: training loss : 0.9437275004386901 TRAIN  loss dict:  {'classification_loss': 0.9437275004386901}
2025-01-13 04:37:18,861 [INFO] Step[2650/2713]: training loss : 0.9428248691558838 TRAIN  loss dict:  {'classification_loss': 0.9428248691558838}
2025-01-13 04:37:30,733 [INFO] Step[2700/2713]: training loss : 0.947617484331131 TRAIN  loss dict:  {'classification_loss': 0.947617484331131}
2025-01-13 04:38:57,913 [INFO] Label accuracies statistics:
2025-01-13 04:38:57,914 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.75, 58: 1.0, 59: 1.0, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 0.75, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.25, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 04:38:57,915 [INFO] [66] TRAIN  loss: 0.944050902881791 acc: 0.9995085391325715
2025-01-13 04:38:57,915 [INFO] [66] TRAIN  loss dict: {'classification_loss': 0.944050902881791}
2025-01-13 04:38:57,916 [INFO] [66] VALIDATION loss: 1.6986038879792493 VALIDATION acc: 0.8357366771159874
2025-01-13 04:38:57,916 [INFO] [66] VALIDATION loss dict: {'classification_loss': 1.6986038879792493}
2025-01-13 04:38:57,916 [INFO] 
2025-01-13 04:39:15,648 [INFO] Step[50/2713]: training loss : 0.9428147912025452 TRAIN  loss dict:  {'classification_loss': 0.9428147912025452}
2025-01-13 04:39:27,588 [INFO] Step[100/2713]: training loss : 0.9439723217487335 TRAIN  loss dict:  {'classification_loss': 0.9439723217487335}
2025-01-13 04:39:39,533 [INFO] Step[150/2713]: training loss : 0.9418407237529755 TRAIN  loss dict:  {'classification_loss': 0.9418407237529755}
2025-01-13 04:39:51,462 [INFO] Step[200/2713]: training loss : 0.9433681261539459 TRAIN  loss dict:  {'classification_loss': 0.9433681261539459}
2025-01-13 04:40:03,372 [INFO] Step[250/2713]: training loss : 0.9466039752960205 TRAIN  loss dict:  {'classification_loss': 0.9466039752960205}
2025-01-13 04:40:15,308 [INFO] Step[300/2713]: training loss : 0.9422585260868073 TRAIN  loss dict:  {'classification_loss': 0.9422585260868073}
2025-01-13 04:40:27,229 [INFO] Step[350/2713]: training loss : 0.9464745557308197 TRAIN  loss dict:  {'classification_loss': 0.9464745557308197}
2025-01-13 04:40:39,151 [INFO] Step[400/2713]: training loss : 0.9424190592765808 TRAIN  loss dict:  {'classification_loss': 0.9424190592765808}
2025-01-13 04:40:51,036 [INFO] Step[450/2713]: training loss : 0.9452573442459107 TRAIN  loss dict:  {'classification_loss': 0.9452573442459107}
2025-01-13 04:41:02,941 [INFO] Step[500/2713]: training loss : 0.9442321288585663 TRAIN  loss dict:  {'classification_loss': 0.9442321288585663}
2025-01-13 04:41:14,893 [INFO] Step[550/2713]: training loss : 0.9531263899803162 TRAIN  loss dict:  {'classification_loss': 0.9531263899803162}
2025-01-13 04:41:26,819 [INFO] Step[600/2713]: training loss : 0.9492225658893585 TRAIN  loss dict:  {'classification_loss': 0.9492225658893585}
2025-01-13 04:41:38,715 [INFO] Step[650/2713]: training loss : 0.9397964298725128 TRAIN  loss dict:  {'classification_loss': 0.9397964298725128}
2025-01-13 04:41:50,605 [INFO] Step[700/2713]: training loss : 0.9431462359428405 TRAIN  loss dict:  {'classification_loss': 0.9431462359428405}
2025-01-13 04:42:02,520 [INFO] Step[750/2713]: training loss : 0.9443883311748504 TRAIN  loss dict:  {'classification_loss': 0.9443883311748504}
2025-01-13 04:42:14,459 [INFO] Step[800/2713]: training loss : 0.9439112830162049 TRAIN  loss dict:  {'classification_loss': 0.9439112830162049}
2025-01-13 04:42:26,369 [INFO] Step[850/2713]: training loss : 0.9431169939041137 TRAIN  loss dict:  {'classification_loss': 0.9431169939041137}
2025-01-13 04:42:38,279 [INFO] Step[900/2713]: training loss : 0.944090473651886 TRAIN  loss dict:  {'classification_loss': 0.944090473651886}
2025-01-13 04:42:50,517 [INFO] Step[950/2713]: training loss : 0.9416776919364929 TRAIN  loss dict:  {'classification_loss': 0.9416776919364929}
2025-01-13 04:43:02,976 [INFO] Step[1000/2713]: training loss : 0.9450533831119537 TRAIN  loss dict:  {'classification_loss': 0.9450533831119537}
2025-01-13 04:43:15,387 [INFO] Step[1050/2713]: training loss : 0.9440036475658417 TRAIN  loss dict:  {'classification_loss': 0.9440036475658417}
2025-01-13 04:43:27,849 [INFO] Step[1100/2713]: training loss : 0.9448388493061066 TRAIN  loss dict:  {'classification_loss': 0.9448388493061066}
2025-01-13 04:43:40,430 [INFO] Step[1150/2713]: training loss : 0.9428563356399536 TRAIN  loss dict:  {'classification_loss': 0.9428563356399536}
2025-01-13 04:43:52,833 [INFO] Step[1200/2713]: training loss : 0.9434522724151612 TRAIN  loss dict:  {'classification_loss': 0.9434522724151612}
2025-01-13 04:44:05,811 [INFO] Step[1250/2713]: training loss : 0.9420778977870942 TRAIN  loss dict:  {'classification_loss': 0.9420778977870942}
2025-01-13 04:44:19,933 [INFO] Step[1300/2713]: training loss : 0.9447785222530365 TRAIN  loss dict:  {'classification_loss': 0.9447785222530365}
2025-01-13 04:44:32,963 [INFO] Step[1350/2713]: training loss : 0.9479694604873657 TRAIN  loss dict:  {'classification_loss': 0.9479694604873657}
2025-01-13 04:44:45,023 [INFO] Step[1400/2713]: training loss : 0.9439054334163666 TRAIN  loss dict:  {'classification_loss': 0.9439054334163666}
2025-01-13 04:44:56,946 [INFO] Step[1450/2713]: training loss : 0.9429217064380646 TRAIN  loss dict:  {'classification_loss': 0.9429217064380646}
2025-01-13 04:45:08,897 [INFO] Step[1500/2713]: training loss : 0.9583280062675477 TRAIN  loss dict:  {'classification_loss': 0.9583280062675477}
2025-01-13 04:45:20,833 [INFO] Step[1550/2713]: training loss : 0.9460924077033996 TRAIN  loss dict:  {'classification_loss': 0.9460924077033996}
2025-01-13 04:45:32,763 [INFO] Step[1600/2713]: training loss : 0.943490263223648 TRAIN  loss dict:  {'classification_loss': 0.943490263223648}
2025-01-13 04:45:44,721 [INFO] Step[1650/2713]: training loss : 0.9425059950351715 TRAIN  loss dict:  {'classification_loss': 0.9425059950351715}
2025-01-13 04:45:56,654 [INFO] Step[1700/2713]: training loss : 0.9395474457740783 TRAIN  loss dict:  {'classification_loss': 0.9395474457740783}
2025-01-13 04:46:08,602 [INFO] Step[1750/2713]: training loss : 0.9496718692779541 TRAIN  loss dict:  {'classification_loss': 0.9496718692779541}
2025-01-13 04:46:20,552 [INFO] Step[1800/2713]: training loss : 0.9410332202911377 TRAIN  loss dict:  {'classification_loss': 0.9410332202911377}
2025-01-13 04:46:32,541 [INFO] Step[1850/2713]: training loss : 0.94167271733284 TRAIN  loss dict:  {'classification_loss': 0.94167271733284}
2025-01-13 04:46:44,497 [INFO] Step[1900/2713]: training loss : 0.9427541983127594 TRAIN  loss dict:  {'classification_loss': 0.9427541983127594}
2025-01-13 04:46:56,440 [INFO] Step[1950/2713]: training loss : 0.9425780856609345 TRAIN  loss dict:  {'classification_loss': 0.9425780856609345}
2025-01-13 04:47:08,367 [INFO] Step[2000/2713]: training loss : 0.9425333428382874 TRAIN  loss dict:  {'classification_loss': 0.9425333428382874}
2025-01-13 04:47:20,317 [INFO] Step[2050/2713]: training loss : 0.9439046061038971 TRAIN  loss dict:  {'classification_loss': 0.9439046061038971}
2025-01-13 04:47:32,266 [INFO] Step[2100/2713]: training loss : 0.9398235881328583 TRAIN  loss dict:  {'classification_loss': 0.9398235881328583}
2025-01-13 04:47:44,246 [INFO] Step[2150/2713]: training loss : 0.9435966730117797 TRAIN  loss dict:  {'classification_loss': 0.9435966730117797}
2025-01-13 04:47:56,241 [INFO] Step[2200/2713]: training loss : 0.9468388724327087 TRAIN  loss dict:  {'classification_loss': 0.9468388724327087}
2025-01-13 04:48:08,210 [INFO] Step[2250/2713]: training loss : 0.9485942375659943 TRAIN  loss dict:  {'classification_loss': 0.9485942375659943}
2025-01-13 04:48:20,212 [INFO] Step[2300/2713]: training loss : 0.9424683272838592 TRAIN  loss dict:  {'classification_loss': 0.9424683272838592}
2025-01-13 04:48:32,178 [INFO] Step[2350/2713]: training loss : 0.94247638463974 TRAIN  loss dict:  {'classification_loss': 0.94247638463974}
2025-01-13 04:48:44,116 [INFO] Step[2400/2713]: training loss : 0.9429566621780395 TRAIN  loss dict:  {'classification_loss': 0.9429566621780395}
2025-01-13 04:48:56,048 [INFO] Step[2450/2713]: training loss : 0.94495964884758 TRAIN  loss dict:  {'classification_loss': 0.94495964884758}
2025-01-13 04:49:07,989 [INFO] Step[2500/2713]: training loss : 0.9504704523086548 TRAIN  loss dict:  {'classification_loss': 0.9504704523086548}
2025-01-13 04:49:19,953 [INFO] Step[2550/2713]: training loss : 0.9435440266132354 TRAIN  loss dict:  {'classification_loss': 0.9435440266132354}
2025-01-13 04:49:31,925 [INFO] Step[2600/2713]: training loss : 0.9452308559417725 TRAIN  loss dict:  {'classification_loss': 0.9452308559417725}
2025-01-13 04:49:43,866 [INFO] Step[2650/2713]: training loss : 0.9442506325244904 TRAIN  loss dict:  {'classification_loss': 0.9442506325244904}
2025-01-13 04:49:55,804 [INFO] Step[2700/2713]: training loss : 0.9456320965290069 TRAIN  loss dict:  {'classification_loss': 0.9456320965290069}
2025-01-13 04:51:22,030 [INFO] Label accuracies statistics:
2025-01-13 04:51:22,030 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 0.75, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 1.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 1.0, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 04:51:22,033 [INFO] [67] TRAIN  loss: 0.9443940336617253 acc: 0.9996314043494287
2025-01-13 04:51:22,033 [INFO] [67] TRAIN  loss dict: {'classification_loss': 0.9443940336617253}
2025-01-13 04:51:22,033 [INFO] [67] VALIDATION loss: 1.7001886250157106 VALIDATION acc: 0.8401253918495298
2025-01-13 04:51:22,033 [INFO] [67] VALIDATION loss dict: {'classification_loss': 1.7001886250157106}
2025-01-13 04:51:22,034 [INFO] 
2025-01-13 04:51:39,450 [INFO] Step[50/2713]: training loss : 0.9428335356712342 TRAIN  loss dict:  {'classification_loss': 0.9428335356712342}
2025-01-13 04:51:51,353 [INFO] Step[100/2713]: training loss : 0.9423019361495971 TRAIN  loss dict:  {'classification_loss': 0.9423019361495971}
2025-01-13 04:52:03,294 [INFO] Step[150/2713]: training loss : 0.943956995010376 TRAIN  loss dict:  {'classification_loss': 0.943956995010376}
2025-01-13 04:52:15,196 [INFO] Step[200/2713]: training loss : 0.9407552349567413 TRAIN  loss dict:  {'classification_loss': 0.9407552349567413}
2025-01-13 04:52:27,161 [INFO] Step[250/2713]: training loss : 0.9440333950519562 TRAIN  loss dict:  {'classification_loss': 0.9440333950519562}
2025-01-13 04:52:39,109 [INFO] Step[300/2713]: training loss : 0.9436131310462952 TRAIN  loss dict:  {'classification_loss': 0.9436131310462952}
2025-01-13 04:52:51,019 [INFO] Step[350/2713]: training loss : 0.9426655459403992 TRAIN  loss dict:  {'classification_loss': 0.9426655459403992}
2025-01-13 04:53:02,939 [INFO] Step[400/2713]: training loss : 0.9430540919303894 TRAIN  loss dict:  {'classification_loss': 0.9430540919303894}
2025-01-13 04:53:14,884 [INFO] Step[450/2713]: training loss : 0.9456185042858124 TRAIN  loss dict:  {'classification_loss': 0.9456185042858124}
2025-01-13 04:53:26,807 [INFO] Step[500/2713]: training loss : 0.943603435754776 TRAIN  loss dict:  {'classification_loss': 0.943603435754776}
2025-01-13 04:53:38,715 [INFO] Step[550/2713]: training loss : 0.948118736743927 TRAIN  loss dict:  {'classification_loss': 0.948118736743927}
2025-01-13 04:53:50,610 [INFO] Step[600/2713]: training loss : 0.9420289599895477 TRAIN  loss dict:  {'classification_loss': 0.9420289599895477}
2025-01-13 04:54:02,554 [INFO] Step[650/2713]: training loss : 0.9451676499843598 TRAIN  loss dict:  {'classification_loss': 0.9451676499843598}
2025-01-13 04:54:14,484 [INFO] Step[700/2713]: training loss : 0.9441052842140197 TRAIN  loss dict:  {'classification_loss': 0.9441052842140197}
2025-01-13 04:54:26,398 [INFO] Step[750/2713]: training loss : 0.9427803838253022 TRAIN  loss dict:  {'classification_loss': 0.9427803838253022}
2025-01-13 04:54:38,318 [INFO] Step[800/2713]: training loss : 0.9444907331466674 TRAIN  loss dict:  {'classification_loss': 0.9444907331466674}
2025-01-13 04:54:50,249 [INFO] Step[850/2713]: training loss : 0.94160698056221 TRAIN  loss dict:  {'classification_loss': 0.94160698056221}
2025-01-13 04:55:02,175 [INFO] Step[900/2713]: training loss : 0.9429428136348724 TRAIN  loss dict:  {'classification_loss': 0.9429428136348724}
2025-01-13 04:55:14,092 [INFO] Step[950/2713]: training loss : 0.9416408383846283 TRAIN  loss dict:  {'classification_loss': 0.9416408383846283}
2025-01-13 04:55:26,021 [INFO] Step[1000/2713]: training loss : 0.9462491822242737 TRAIN  loss dict:  {'classification_loss': 0.9462491822242737}
2025-01-13 04:55:38,022 [INFO] Step[1050/2713]: training loss : 0.9550960648059845 TRAIN  loss dict:  {'classification_loss': 0.9550960648059845}
2025-01-13 04:55:49,910 [INFO] Step[1100/2713]: training loss : 0.943157685995102 TRAIN  loss dict:  {'classification_loss': 0.943157685995102}
2025-01-13 04:56:01,866 [INFO] Step[1150/2713]: training loss : 0.9459561228752136 TRAIN  loss dict:  {'classification_loss': 0.9459561228752136}
2025-01-13 04:56:13,758 [INFO] Step[1200/2713]: training loss : 0.9434057676792145 TRAIN  loss dict:  {'classification_loss': 0.9434057676792145}
2025-01-13 04:56:25,721 [INFO] Step[1250/2713]: training loss : 0.9407266986370086 TRAIN  loss dict:  {'classification_loss': 0.9407266986370086}
2025-01-13 04:56:37,617 [INFO] Step[1300/2713]: training loss : 0.9422743892669678 TRAIN  loss dict:  {'classification_loss': 0.9422743892669678}
2025-01-13 04:56:49,555 [INFO] Step[1350/2713]: training loss : 0.9432608139514923 TRAIN  loss dict:  {'classification_loss': 0.9432608139514923}
2025-01-13 04:57:01,503 [INFO] Step[1400/2713]: training loss : 0.9451731872558594 TRAIN  loss dict:  {'classification_loss': 0.9451731872558594}
2025-01-13 04:57:13,430 [INFO] Step[1450/2713]: training loss : 0.9511391913890839 TRAIN  loss dict:  {'classification_loss': 0.9511391913890839}
2025-01-13 04:57:25,358 [INFO] Step[1500/2713]: training loss : 0.9551510000228882 TRAIN  loss dict:  {'classification_loss': 0.9551510000228882}
2025-01-13 04:57:37,297 [INFO] Step[1550/2713]: training loss : 0.9424190962314606 TRAIN  loss dict:  {'classification_loss': 0.9424190962314606}
2025-01-13 04:57:49,203 [INFO] Step[1600/2713]: training loss : 0.9433141136169434 TRAIN  loss dict:  {'classification_loss': 0.9433141136169434}
2025-01-13 04:58:01,145 [INFO] Step[1650/2713]: training loss : 0.941958224773407 TRAIN  loss dict:  {'classification_loss': 0.941958224773407}
2025-01-13 04:58:13,062 [INFO] Step[1700/2713]: training loss : 0.9411259913444519 TRAIN  loss dict:  {'classification_loss': 0.9411259913444519}
2025-01-13 04:58:25,008 [INFO] Step[1750/2713]: training loss : 0.9456495618820191 TRAIN  loss dict:  {'classification_loss': 0.9456495618820191}
2025-01-13 04:58:36,935 [INFO] Step[1800/2713]: training loss : 0.9400992345809936 TRAIN  loss dict:  {'classification_loss': 0.9400992345809936}
2025-01-13 04:58:48,876 [INFO] Step[1850/2713]: training loss : 0.9428310143947601 TRAIN  loss dict:  {'classification_loss': 0.9428310143947601}
2025-01-13 04:59:00,768 [INFO] Step[1900/2713]: training loss : 0.943298157453537 TRAIN  loss dict:  {'classification_loss': 0.943298157453537}
2025-01-13 04:59:12,716 [INFO] Step[1950/2713]: training loss : 0.9424359548091888 TRAIN  loss dict:  {'classification_loss': 0.9424359548091888}
2025-01-13 04:59:24,622 [INFO] Step[2000/2713]: training loss : 0.946820456981659 TRAIN  loss dict:  {'classification_loss': 0.946820456981659}
2025-01-13 04:59:36,587 [INFO] Step[2050/2713]: training loss : 0.9452498030662536 TRAIN  loss dict:  {'classification_loss': 0.9452498030662536}
2025-01-13 04:59:48,530 [INFO] Step[2100/2713]: training loss : 0.947346317768097 TRAIN  loss dict:  {'classification_loss': 0.947346317768097}
2025-01-13 05:00:00,454 [INFO] Step[2150/2713]: training loss : 0.941731173992157 TRAIN  loss dict:  {'classification_loss': 0.941731173992157}
2025-01-13 05:00:12,401 [INFO] Step[2200/2713]: training loss : 0.9451596903800964 TRAIN  loss dict:  {'classification_loss': 0.9451596903800964}
2025-01-13 05:00:24,406 [INFO] Step[2250/2713]: training loss : 0.9425451624393463 TRAIN  loss dict:  {'classification_loss': 0.9425451624393463}
2025-01-13 05:00:36,375 [INFO] Step[2300/2713]: training loss : 0.942596127986908 TRAIN  loss dict:  {'classification_loss': 0.942596127986908}
2025-01-13 05:00:48,335 [INFO] Step[2350/2713]: training loss : 0.9400714361667633 TRAIN  loss dict:  {'classification_loss': 0.9400714361667633}
2025-01-13 05:01:00,300 [INFO] Step[2400/2713]: training loss : 0.9426848542690277 TRAIN  loss dict:  {'classification_loss': 0.9426848542690277}
2025-01-13 05:01:12,240 [INFO] Step[2450/2713]: training loss : 0.9428099036216736 TRAIN  loss dict:  {'classification_loss': 0.9428099036216736}
2025-01-13 05:01:24,201 [INFO] Step[2500/2713]: training loss : 0.9431481909751892 TRAIN  loss dict:  {'classification_loss': 0.9431481909751892}
2025-01-13 05:01:36,278 [INFO] Step[2550/2713]: training loss : 0.9428282272815705 TRAIN  loss dict:  {'classification_loss': 0.9428282272815705}
2025-01-13 05:01:48,697 [INFO] Step[2600/2713]: training loss : 0.9492664182186127 TRAIN  loss dict:  {'classification_loss': 0.9492664182186127}
2025-01-13 05:02:01,054 [INFO] Step[2650/2713]: training loss : 0.9446188080310821 TRAIN  loss dict:  {'classification_loss': 0.9446188080310821}
2025-01-13 05:02:13,195 [INFO] Step[2700/2713]: training loss : 0.9468775165081024 TRAIN  loss dict:  {'classification_loss': 0.9468775165081024}
2025-01-13 05:04:04,829 [INFO] Label accuracies statistics:
2025-01-13 05:04:04,829 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.5, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 1.0, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 1.0, 335: 0.75, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 05:04:04,831 [INFO] [68] TRAIN  loss: 0.9441484216390569 acc: 0.9998771347831429
2025-01-13 05:04:04,832 [INFO] [68] TRAIN  loss dict: {'classification_loss': 0.9441484216390569}
2025-01-13 05:04:04,832 [INFO] [68] VALIDATION loss: 1.7149743277551537 VALIDATION acc: 0.8225705329153605
2025-01-13 05:04:04,832 [INFO] [68] VALIDATION loss dict: {'classification_loss': 1.7149743277551537}
2025-01-13 05:04:04,832 [INFO] 
2025-01-13 05:04:22,734 [INFO] Step[50/2713]: training loss : 0.942317316532135 TRAIN  loss dict:  {'classification_loss': 0.942317316532135}
2025-01-13 05:04:34,674 [INFO] Step[100/2713]: training loss : 0.9476742577552796 TRAIN  loss dict:  {'classification_loss': 0.9476742577552796}
2025-01-13 05:04:46,572 [INFO] Step[150/2713]: training loss : 0.94408487200737 TRAIN  loss dict:  {'classification_loss': 0.94408487200737}
2025-01-13 05:04:58,503 [INFO] Step[200/2713]: training loss : 0.9408356046676636 TRAIN  loss dict:  {'classification_loss': 0.9408356046676636}
2025-01-13 05:05:10,459 [INFO] Step[250/2713]: training loss : 0.9398612928390503 TRAIN  loss dict:  {'classification_loss': 0.9398612928390503}
2025-01-13 05:05:22,399 [INFO] Step[300/2713]: training loss : 0.9434895730018615 TRAIN  loss dict:  {'classification_loss': 0.9434895730018615}
2025-01-13 05:05:34,328 [INFO] Step[350/2713]: training loss : 0.9403461992740632 TRAIN  loss dict:  {'classification_loss': 0.9403461992740632}
2025-01-13 05:05:46,255 [INFO] Step[400/2713]: training loss : 0.9471665585041046 TRAIN  loss dict:  {'classification_loss': 0.9471665585041046}
2025-01-13 05:05:58,203 [INFO] Step[450/2713]: training loss : 0.9431288063526153 TRAIN  loss dict:  {'classification_loss': 0.9431288063526153}
2025-01-13 05:06:10,116 [INFO] Step[500/2713]: training loss : 0.9398140621185302 TRAIN  loss dict:  {'classification_loss': 0.9398140621185302}
2025-01-13 05:06:22,085 [INFO] Step[550/2713]: training loss : 0.9410089445114136 TRAIN  loss dict:  {'classification_loss': 0.9410089445114136}
2025-01-13 05:06:34,029 [INFO] Step[600/2713]: training loss : 0.9430018508434296 TRAIN  loss dict:  {'classification_loss': 0.9430018508434296}
2025-01-13 05:06:45,972 [INFO] Step[650/2713]: training loss : 0.9408588182926177 TRAIN  loss dict:  {'classification_loss': 0.9408588182926177}
2025-01-13 05:06:57,970 [INFO] Step[700/2713]: training loss : 0.9448839700222016 TRAIN  loss dict:  {'classification_loss': 0.9448839700222016}
2025-01-13 05:07:09,936 [INFO] Step[750/2713]: training loss : 0.9400239586830139 TRAIN  loss dict:  {'classification_loss': 0.9400239586830139}
2025-01-13 05:07:21,907 [INFO] Step[800/2713]: training loss : 0.9415433621406555 TRAIN  loss dict:  {'classification_loss': 0.9415433621406555}
2025-01-13 05:07:33,857 [INFO] Step[850/2713]: training loss : 0.9438514411449432 TRAIN  loss dict:  {'classification_loss': 0.9438514411449432}
2025-01-13 05:07:45,813 [INFO] Step[900/2713]: training loss : 0.9414479458332061 TRAIN  loss dict:  {'classification_loss': 0.9414479458332061}
2025-01-13 05:07:57,754 [INFO] Step[950/2713]: training loss : 0.9424435496330261 TRAIN  loss dict:  {'classification_loss': 0.9424435496330261}
2025-01-13 05:08:09,723 [INFO] Step[1000/2713]: training loss : 0.9401691842079163 TRAIN  loss dict:  {'classification_loss': 0.9401691842079163}
2025-01-13 05:08:21,658 [INFO] Step[1050/2713]: training loss : 0.945995409488678 TRAIN  loss dict:  {'classification_loss': 0.945995409488678}
2025-01-13 05:08:33,631 [INFO] Step[1100/2713]: training loss : 0.9406754362583161 TRAIN  loss dict:  {'classification_loss': 0.9406754362583161}
2025-01-13 05:08:45,556 [INFO] Step[1150/2713]: training loss : 0.949510987997055 TRAIN  loss dict:  {'classification_loss': 0.949510987997055}
2025-01-13 05:08:57,470 [INFO] Step[1200/2713]: training loss : 0.9413789248466492 TRAIN  loss dict:  {'classification_loss': 0.9413789248466492}
2025-01-13 05:09:09,377 [INFO] Step[1250/2713]: training loss : 0.9530211794376373 TRAIN  loss dict:  {'classification_loss': 0.9530211794376373}
2025-01-13 05:09:21,343 [INFO] Step[1300/2713]: training loss : 0.9403974568843841 TRAIN  loss dict:  {'classification_loss': 0.9403974568843841}
2025-01-13 05:09:33,312 [INFO] Step[1350/2713]: training loss : 0.9440678596496582 TRAIN  loss dict:  {'classification_loss': 0.9440678596496582}
2025-01-13 05:09:45,282 [INFO] Step[1400/2713]: training loss : 0.9392653048038483 TRAIN  loss dict:  {'classification_loss': 0.9392653048038483}
2025-01-13 05:09:57,255 [INFO] Step[1450/2713]: training loss : 0.9422359931468963 TRAIN  loss dict:  {'classification_loss': 0.9422359931468963}
2025-01-13 05:10:09,260 [INFO] Step[1500/2713]: training loss : 0.9432843589782715 TRAIN  loss dict:  {'classification_loss': 0.9432843589782715}
2025-01-13 05:10:21,189 [INFO] Step[1550/2713]: training loss : 0.9405965256690979 TRAIN  loss dict:  {'classification_loss': 0.9405965256690979}
2025-01-13 05:10:33,130 [INFO] Step[1600/2713]: training loss : 0.9438499939441681 TRAIN  loss dict:  {'classification_loss': 0.9438499939441681}
2025-01-13 05:10:45,094 [INFO] Step[1650/2713]: training loss : 0.9470164430141449 TRAIN  loss dict:  {'classification_loss': 0.9470164430141449}
2025-01-13 05:10:57,068 [INFO] Step[1700/2713]: training loss : 0.9420461463928222 TRAIN  loss dict:  {'classification_loss': 0.9420461463928222}
2025-01-13 05:11:09,067 [INFO] Step[1750/2713]: training loss : 0.9486348783969879 TRAIN  loss dict:  {'classification_loss': 0.9486348783969879}
2025-01-13 05:11:21,031 [INFO] Step[1800/2713]: training loss : 0.9434159743785858 TRAIN  loss dict:  {'classification_loss': 0.9434159743785858}
2025-01-13 05:11:33,002 [INFO] Step[1850/2713]: training loss : 0.9417952156066894 TRAIN  loss dict:  {'classification_loss': 0.9417952156066894}
2025-01-13 05:11:44,959 [INFO] Step[1900/2713]: training loss : 0.9419166612625122 TRAIN  loss dict:  {'classification_loss': 0.9419166612625122}
2025-01-13 05:11:56,901 [INFO] Step[1950/2713]: training loss : 0.9409225630760193 TRAIN  loss dict:  {'classification_loss': 0.9409225630760193}
2025-01-13 05:12:08,887 [INFO] Step[2000/2713]: training loss : 0.9471993362903595 TRAIN  loss dict:  {'classification_loss': 0.9471993362903595}
2025-01-13 05:12:20,827 [INFO] Step[2050/2713]: training loss : 0.9401988232135773 TRAIN  loss dict:  {'classification_loss': 0.9401988232135773}
2025-01-13 05:12:32,781 [INFO] Step[2100/2713]: training loss : 0.9424213969707489 TRAIN  loss dict:  {'classification_loss': 0.9424213969707489}
2025-01-13 05:12:44,750 [INFO] Step[2150/2713]: training loss : 0.9459983432292938 TRAIN  loss dict:  {'classification_loss': 0.9459983432292938}
2025-01-13 05:12:56,705 [INFO] Step[2200/2713]: training loss : 0.9465216815471649 TRAIN  loss dict:  {'classification_loss': 0.9465216815471649}
2025-01-13 05:13:08,670 [INFO] Step[2250/2713]: training loss : 0.9434335672855377 TRAIN  loss dict:  {'classification_loss': 0.9434335672855377}
2025-01-13 05:13:20,635 [INFO] Step[2300/2713]: training loss : 0.9426315128803253 TRAIN  loss dict:  {'classification_loss': 0.9426315128803253}
2025-01-13 05:13:32,525 [INFO] Step[2350/2713]: training loss : 0.9402809917926789 TRAIN  loss dict:  {'classification_loss': 0.9402809917926789}
2025-01-13 05:13:44,533 [INFO] Step[2400/2713]: training loss : 0.9411120724678039 TRAIN  loss dict:  {'classification_loss': 0.9411120724678039}
2025-01-13 05:13:56,503 [INFO] Step[2450/2713]: training loss : 0.9442277038097382 TRAIN  loss dict:  {'classification_loss': 0.9442277038097382}
2025-01-13 05:14:08,447 [INFO] Step[2500/2713]: training loss : 0.9474130928516388 TRAIN  loss dict:  {'classification_loss': 0.9474130928516388}
2025-01-13 05:14:20,450 [INFO] Step[2550/2713]: training loss : 0.944700448513031 TRAIN  loss dict:  {'classification_loss': 0.944700448513031}
2025-01-13 05:14:32,406 [INFO] Step[2600/2713]: training loss : 0.9423025262355804 TRAIN  loss dict:  {'classification_loss': 0.9423025262355804}
2025-01-13 05:14:44,315 [INFO] Step[2650/2713]: training loss : 0.9409725618362427 TRAIN  loss dict:  {'classification_loss': 0.9409725618362427}
2025-01-13 05:14:56,337 [INFO] Step[2700/2713]: training loss : 0.940186824798584 TRAIN  loss dict:  {'classification_loss': 0.940186824798584}
2025-01-13 05:16:25,335 [INFO] Label accuracies statistics:
2025-01-13 05:16:25,335 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 1.0, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 1.0, 243: 0.75, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.5, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 0.75, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-13 05:16:25,337 [INFO] [69] TRAIN  loss: 0.9430843063970356 acc: 1.0
2025-01-13 05:16:25,337 [INFO] [69] TRAIN  loss dict: {'classification_loss': 0.9430843063970356}
2025-01-13 05:16:25,337 [INFO] [69] VALIDATION loss: 1.656741813385397 VALIDATION acc: 0.8413793103448276
2025-01-13 05:16:25,337 [INFO] [69] VALIDATION loss dict: {'classification_loss': 1.656741813385397}
2025-01-13 05:16:25,338 [INFO] 
2025-01-13 05:16:43,423 [INFO] Step[50/2713]: training loss : 0.9412746119499207 TRAIN  loss dict:  {'classification_loss': 0.9412746119499207}
2025-01-13 05:16:55,301 [INFO] Step[100/2713]: training loss : 0.9398564910888672 TRAIN  loss dict:  {'classification_loss': 0.9398564910888672}
2025-01-13 05:17:07,209 [INFO] Step[150/2713]: training loss : 0.9393804252147675 TRAIN  loss dict:  {'classification_loss': 0.9393804252147675}
2025-01-13 05:17:19,100 [INFO] Step[200/2713]: training loss : 0.9443355512619018 TRAIN  loss dict:  {'classification_loss': 0.9443355512619018}
2025-01-13 05:17:31,047 [INFO] Step[250/2713]: training loss : 0.941516386270523 TRAIN  loss dict:  {'classification_loss': 0.941516386270523}
2025-01-13 05:17:42,993 [INFO] Step[300/2713]: training loss : 0.9429871106147766 TRAIN  loss dict:  {'classification_loss': 0.9429871106147766}
2025-01-13 05:17:54,942 [INFO] Step[350/2713]: training loss : 0.9412056493759156 TRAIN  loss dict:  {'classification_loss': 0.9412056493759156}
2025-01-13 05:18:06,859 [INFO] Step[400/2713]: training loss : 0.9408884489536286 TRAIN  loss dict:  {'classification_loss': 0.9408884489536286}
2025-01-13 05:18:18,754 [INFO] Step[450/2713]: training loss : 0.9478176534175873 TRAIN  loss dict:  {'classification_loss': 0.9478176534175873}
2025-01-13 05:18:30,643 [INFO] Step[500/2713]: training loss : 0.9434988844394684 TRAIN  loss dict:  {'classification_loss': 0.9434988844394684}
2025-01-13 05:18:42,598 [INFO] Step[550/2713]: training loss : 0.943779182434082 TRAIN  loss dict:  {'classification_loss': 0.943779182434082}
2025-01-13 05:18:54,552 [INFO] Step[600/2713]: training loss : 0.94078134059906 TRAIN  loss dict:  {'classification_loss': 0.94078134059906}
2025-01-13 05:19:06,481 [INFO] Step[650/2713]: training loss : 0.9441220331192016 TRAIN  loss dict:  {'classification_loss': 0.9441220331192016}
2025-01-13 05:19:18,367 [INFO] Step[700/2713]: training loss : 0.9414212656021118 TRAIN  loss dict:  {'classification_loss': 0.9414212656021118}
2025-01-13 05:19:30,298 [INFO] Step[750/2713]: training loss : 0.9427291250228882 TRAIN  loss dict:  {'classification_loss': 0.9427291250228882}
2025-01-13 05:19:42,224 [INFO] Step[800/2713]: training loss : 0.9415568077564239 TRAIN  loss dict:  {'classification_loss': 0.9415568077564239}
2025-01-13 05:19:54,118 [INFO] Step[850/2713]: training loss : 0.9538808143138886 TRAIN  loss dict:  {'classification_loss': 0.9538808143138886}
2025-01-13 05:20:06,041 [INFO] Step[900/2713]: training loss : 0.9404467165470123 TRAIN  loss dict:  {'classification_loss': 0.9404467165470123}
2025-01-13 05:20:17,979 [INFO] Step[950/2713]: training loss : 0.9388877594470978 TRAIN  loss dict:  {'classification_loss': 0.9388877594470978}
2025-01-13 05:20:29,885 [INFO] Step[1000/2713]: training loss : 0.9436149561405182 TRAIN  loss dict:  {'classification_loss': 0.9436149561405182}
2025-01-13 05:20:41,770 [INFO] Step[1050/2713]: training loss : 0.9397463500499725 TRAIN  loss dict:  {'classification_loss': 0.9397463500499725}
2025-01-13 05:20:53,895 [INFO] Step[1100/2713]: training loss : 0.9412739551067353 TRAIN  loss dict:  {'classification_loss': 0.9412739551067353}
2025-01-13 05:21:06,318 [INFO] Step[1150/2713]: training loss : 0.9428284347057343 TRAIN  loss dict:  {'classification_loss': 0.9428284347057343}
2025-01-13 05:21:18,687 [INFO] Step[1200/2713]: training loss : 0.941399896144867 TRAIN  loss dict:  {'classification_loss': 0.941399896144867}
2025-01-13 05:21:30,924 [INFO] Step[1250/2713]: training loss : 0.9398943316936493 TRAIN  loss dict:  {'classification_loss': 0.9398943316936493}
2025-01-13 05:21:43,570 [INFO] Step[1300/2713]: training loss : 0.9436008310317994 TRAIN  loss dict:  {'classification_loss': 0.9436008310317994}
2025-01-13 05:21:55,859 [INFO] Step[1350/2713]: training loss : 0.9420325446128845 TRAIN  loss dict:  {'classification_loss': 0.9420325446128845}
2025-01-13 05:22:08,577 [INFO] Step[1400/2713]: training loss : 0.9439005887508393 TRAIN  loss dict:  {'classification_loss': 0.9439005887508393}
2025-01-13 05:22:22,108 [INFO] Step[1450/2713]: training loss : 0.941340571641922 TRAIN  loss dict:  {'classification_loss': 0.941340571641922}
2025-01-13 05:22:35,877 [INFO] Step[1500/2713]: training loss : 0.9394020748138427 TRAIN  loss dict:  {'classification_loss': 0.9394020748138427}
2025-01-13 05:22:48,049 [INFO] Step[1550/2713]: training loss : 0.9404220390319824 TRAIN  loss dict:  {'classification_loss': 0.9404220390319824}
2025-01-13 05:22:59,909 [INFO] Step[1600/2713]: training loss : 0.9432377922534942 TRAIN  loss dict:  {'classification_loss': 0.9432377922534942}
2025-01-13 05:23:11,755 [INFO] Step[1650/2713]: training loss : 0.9409574615955353 TRAIN  loss dict:  {'classification_loss': 0.9409574615955353}
2025-01-13 05:23:23,640 [INFO] Step[1700/2713]: training loss : 0.9415362560749054 TRAIN  loss dict:  {'classification_loss': 0.9415362560749054}
2025-01-13 05:23:35,533 [INFO] Step[1750/2713]: training loss : 0.9455803227424622 TRAIN  loss dict:  {'classification_loss': 0.9455803227424622}
2025-01-13 05:23:47,404 [INFO] Step[1800/2713]: training loss : 0.9456951451301575 TRAIN  loss dict:  {'classification_loss': 0.9456951451301575}
2025-01-13 05:23:59,272 [INFO] Step[1850/2713]: training loss : 0.9411262404918671 TRAIN  loss dict:  {'classification_loss': 0.9411262404918671}
2025-01-13 05:24:11,124 [INFO] Step[1900/2713]: training loss : 0.9448695170879364 TRAIN  loss dict:  {'classification_loss': 0.9448695170879364}
2025-01-13 05:24:23,069 [INFO] Step[1950/2713]: training loss : 0.9430323302745819 TRAIN  loss dict:  {'classification_loss': 0.9430323302745819}
2025-01-13 05:24:35,027 [INFO] Step[2000/2713]: training loss : 0.9409502029418946 TRAIN  loss dict:  {'classification_loss': 0.9409502029418946}
2025-01-13 05:24:46,995 [INFO] Step[2050/2713]: training loss : 0.9426993060112 TRAIN  loss dict:  {'classification_loss': 0.9426993060112}
2025-01-13 05:24:58,917 [INFO] Step[2100/2713]: training loss : 0.9429352509975434 TRAIN  loss dict:  {'classification_loss': 0.9429352509975434}
2025-01-13 05:25:10,814 [INFO] Step[2150/2713]: training loss : 0.9403062999248505 TRAIN  loss dict:  {'classification_loss': 0.9403062999248505}
2025-01-13 05:25:22,705 [INFO] Step[2200/2713]: training loss : 0.9413760423660278 TRAIN  loss dict:  {'classification_loss': 0.9413760423660278}
2025-01-13 05:25:34,639 [INFO] Step[2250/2713]: training loss : 0.9418744277954102 TRAIN  loss dict:  {'classification_loss': 0.9418744277954102}
2025-01-13 05:25:46,599 [INFO] Step[2300/2713]: training loss : 0.9587369680404663 TRAIN  loss dict:  {'classification_loss': 0.9587369680404663}
2025-01-13 05:25:58,533 [INFO] Step[2350/2713]: training loss : 0.9435492467880249 TRAIN  loss dict:  {'classification_loss': 0.9435492467880249}
2025-01-13 05:26:10,392 [INFO] Step[2400/2713]: training loss : 0.9426026248931885 TRAIN  loss dict:  {'classification_loss': 0.9426026248931885}
2025-01-13 05:26:22,324 [INFO] Step[2450/2713]: training loss : 0.9494668877124787 TRAIN  loss dict:  {'classification_loss': 0.9494668877124787}
2025-01-13 05:26:34,235 [INFO] Step[2500/2713]: training loss : 0.9423993039131164 TRAIN  loss dict:  {'classification_loss': 0.9423993039131164}
2025-01-13 05:26:46,142 [INFO] Step[2550/2713]: training loss : 0.9415112733840942 TRAIN  loss dict:  {'classification_loss': 0.9415112733840942}
2025-01-13 05:26:58,029 [INFO] Step[2600/2713]: training loss : 0.9402182567119598 TRAIN  loss dict:  {'classification_loss': 0.9402182567119598}
2025-01-13 05:27:09,955 [INFO] Step[2650/2713]: training loss : 0.9424182009696961 TRAIN  loss dict:  {'classification_loss': 0.9424182009696961}
2025-01-13 05:27:21,894 [INFO] Step[2700/2713]: training loss : 0.9479744911193848 TRAIN  loss dict:  {'classification_loss': 0.9479744911193848}
2025-01-13 05:28:49,168 [INFO] Label accuracies statistics:
2025-01-13 05:28:49,168 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 1.0, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.3333333333333333, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 1.0, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.25, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 05:28:49,169 [INFO] [70] TRAIN  loss: 0.9428502092535452 acc: 0.9996314043494287
2025-01-13 05:28:49,169 [INFO] [70] TRAIN  loss dict: {'classification_loss': 0.9428502092535452}
2025-01-13 05:28:49,169 [INFO] [70] VALIDATION loss: 1.6816402849622238 VALIDATION acc: 0.8332288401253919
2025-01-13 05:28:49,170 [INFO] [70] VALIDATION loss dict: {'classification_loss': 1.6816402849622238}
2025-01-13 05:28:49,170 [INFO] 
2025-01-13 05:29:07,453 [INFO] Step[50/2713]: training loss : 0.9428783988952637 TRAIN  loss dict:  {'classification_loss': 0.9428783988952637}
2025-01-13 05:29:19,351 [INFO] Step[100/2713]: training loss : 0.9393221271038056 TRAIN  loss dict:  {'classification_loss': 0.9393221271038056}
2025-01-13 05:29:31,251 [INFO] Step[150/2713]: training loss : 0.9445396149158478 TRAIN  loss dict:  {'classification_loss': 0.9445396149158478}
2025-01-13 05:29:43,136 [INFO] Step[200/2713]: training loss : 0.9432873451709747 TRAIN  loss dict:  {'classification_loss': 0.9432873451709747}
2025-01-13 05:29:55,046 [INFO] Step[250/2713]: training loss : 0.9430414438247681 TRAIN  loss dict:  {'classification_loss': 0.9430414438247681}
2025-01-13 05:30:06,913 [INFO] Step[300/2713]: training loss : 0.9414470291137695 TRAIN  loss dict:  {'classification_loss': 0.9414470291137695}
2025-01-13 05:30:18,815 [INFO] Step[350/2713]: training loss : 0.9416523909568787 TRAIN  loss dict:  {'classification_loss': 0.9416523909568787}
2025-01-13 05:30:30,713 [INFO] Step[400/2713]: training loss : 0.945729067325592 TRAIN  loss dict:  {'classification_loss': 0.945729067325592}
2025-01-13 05:30:42,626 [INFO] Step[450/2713]: training loss : 0.940144600868225 TRAIN  loss dict:  {'classification_loss': 0.940144600868225}
2025-01-13 05:30:54,446 [INFO] Step[500/2713]: training loss : 0.9434313786029815 TRAIN  loss dict:  {'classification_loss': 0.9434313786029815}
2025-01-13 05:31:06,360 [INFO] Step[550/2713]: training loss : 0.9404504466056823 TRAIN  loss dict:  {'classification_loss': 0.9404504466056823}
2025-01-13 05:31:18,240 [INFO] Step[600/2713]: training loss : 0.9418137729167938 TRAIN  loss dict:  {'classification_loss': 0.9418137729167938}
2025-01-13 05:31:30,120 [INFO] Step[650/2713]: training loss : 0.9406495678424835 TRAIN  loss dict:  {'classification_loss': 0.9406495678424835}
2025-01-13 05:31:42,009 [INFO] Step[700/2713]: training loss : 0.9482474076747894 TRAIN  loss dict:  {'classification_loss': 0.9482474076747894}
2025-01-13 05:31:53,940 [INFO] Step[750/2713]: training loss : 0.9407627606391906 TRAIN  loss dict:  {'classification_loss': 0.9407627606391906}
2025-01-13 05:32:05,828 [INFO] Step[800/2713]: training loss : 0.9419776034355164 TRAIN  loss dict:  {'classification_loss': 0.9419776034355164}
2025-01-13 05:32:17,794 [INFO] Step[850/2713]: training loss : 0.941195878982544 TRAIN  loss dict:  {'classification_loss': 0.941195878982544}
2025-01-13 05:32:29,724 [INFO] Step[900/2713]: training loss : 0.9426167678833007 TRAIN  loss dict:  {'classification_loss': 0.9426167678833007}
2025-01-13 05:32:41,661 [INFO] Step[950/2713]: training loss : 0.9397205471992492 TRAIN  loss dict:  {'classification_loss': 0.9397205471992492}
2025-01-13 05:32:53,588 [INFO] Step[1000/2713]: training loss : 0.9417030203342438 TRAIN  loss dict:  {'classification_loss': 0.9417030203342438}
2025-01-13 05:33:05,474 [INFO] Step[1050/2713]: training loss : 0.9393648469448089 TRAIN  loss dict:  {'classification_loss': 0.9393648469448089}
2025-01-13 05:33:17,387 [INFO] Step[1100/2713]: training loss : 0.943366185426712 TRAIN  loss dict:  {'classification_loss': 0.943366185426712}
2025-01-13 05:33:29,280 [INFO] Step[1150/2713]: training loss : 0.9425500226020813 TRAIN  loss dict:  {'classification_loss': 0.9425500226020813}
2025-01-13 05:33:41,184 [INFO] Step[1200/2713]: training loss : 0.9414021146297454 TRAIN  loss dict:  {'classification_loss': 0.9414021146297454}
2025-01-13 05:33:53,110 [INFO] Step[1250/2713]: training loss : 0.9411392307281494 TRAIN  loss dict:  {'classification_loss': 0.9411392307281494}
2025-01-13 05:34:04,973 [INFO] Step[1300/2713]: training loss : 0.9428194177150726 TRAIN  loss dict:  {'classification_loss': 0.9428194177150726}
2025-01-13 05:34:16,852 [INFO] Step[1350/2713]: training loss : 0.9421696388721466 TRAIN  loss dict:  {'classification_loss': 0.9421696388721466}
2025-01-13 05:34:28,774 [INFO] Step[1400/2713]: training loss : 0.941954436302185 TRAIN  loss dict:  {'classification_loss': 0.941954436302185}
2025-01-13 05:34:40,689 [INFO] Step[1450/2713]: training loss : 0.9385930967330932 TRAIN  loss dict:  {'classification_loss': 0.9385930967330932}
2025-01-13 05:34:52,626 [INFO] Step[1500/2713]: training loss : 0.9405900561809539 TRAIN  loss dict:  {'classification_loss': 0.9405900561809539}
2025-01-13 05:35:04,511 [INFO] Step[1550/2713]: training loss : 0.9428222298622131 TRAIN  loss dict:  {'classification_loss': 0.9428222298622131}
2025-01-13 05:35:16,393 [INFO] Step[1600/2713]: training loss : 0.9427626824378967 TRAIN  loss dict:  {'classification_loss': 0.9427626824378967}
2025-01-13 05:35:28,329 [INFO] Step[1650/2713]: training loss : 0.9520895302295684 TRAIN  loss dict:  {'classification_loss': 0.9520895302295684}
2025-01-13 05:35:40,238 [INFO] Step[1700/2713]: training loss : 0.9387266099452972 TRAIN  loss dict:  {'classification_loss': 0.9387266099452972}
2025-01-13 05:35:52,107 [INFO] Step[1750/2713]: training loss : 0.9396429300308228 TRAIN  loss dict:  {'classification_loss': 0.9396429300308228}
2025-01-13 05:36:04,001 [INFO] Step[1800/2713]: training loss : 0.9411338746547699 TRAIN  loss dict:  {'classification_loss': 0.9411338746547699}
2025-01-13 05:36:15,884 [INFO] Step[1850/2713]: training loss : 0.9438857388496399 TRAIN  loss dict:  {'classification_loss': 0.9438857388496399}
2025-01-13 05:36:27,773 [INFO] Step[1900/2713]: training loss : 0.9403647863864899 TRAIN  loss dict:  {'classification_loss': 0.9403647863864899}
2025-01-13 05:36:39,673 [INFO] Step[1950/2713]: training loss : 0.9389463138580322 TRAIN  loss dict:  {'classification_loss': 0.9389463138580322}
2025-01-13 05:36:51,631 [INFO] Step[2000/2713]: training loss : 0.9414061081409454 TRAIN  loss dict:  {'classification_loss': 0.9414061081409454}
2025-01-13 05:37:03,498 [INFO] Step[2050/2713]: training loss : 0.9388499355316162 TRAIN  loss dict:  {'classification_loss': 0.9388499355316162}
2025-01-13 05:37:15,434 [INFO] Step[2100/2713]: training loss : 0.9380316495895386 TRAIN  loss dict:  {'classification_loss': 0.9380316495895386}
2025-01-13 05:37:27,343 [INFO] Step[2150/2713]: training loss : 0.9472591722011566 TRAIN  loss dict:  {'classification_loss': 0.9472591722011566}
2025-01-13 05:37:39,215 [INFO] Step[2200/2713]: training loss : 0.9421359968185424 TRAIN  loss dict:  {'classification_loss': 0.9421359968185424}
2025-01-13 05:37:51,096 [INFO] Step[2250/2713]: training loss : 0.9410747992992401 TRAIN  loss dict:  {'classification_loss': 0.9410747992992401}
2025-01-13 05:38:03,000 [INFO] Step[2300/2713]: training loss : 0.9395239961147308 TRAIN  loss dict:  {'classification_loss': 0.9395239961147308}
2025-01-13 05:38:14,924 [INFO] Step[2350/2713]: training loss : 0.9405701887607575 TRAIN  loss dict:  {'classification_loss': 0.9405701887607575}
2025-01-13 05:38:26,855 [INFO] Step[2400/2713]: training loss : 0.94259596824646 TRAIN  loss dict:  {'classification_loss': 0.94259596824646}
2025-01-13 05:38:38,777 [INFO] Step[2450/2713]: training loss : 0.9414734876155854 TRAIN  loss dict:  {'classification_loss': 0.9414734876155854}
2025-01-13 05:38:50,675 [INFO] Step[2500/2713]: training loss : 0.9381414258480072 TRAIN  loss dict:  {'classification_loss': 0.9381414258480072}
2025-01-13 05:39:02,613 [INFO] Step[2550/2713]: training loss : 0.9402348828315735 TRAIN  loss dict:  {'classification_loss': 0.9402348828315735}
2025-01-13 05:39:14,504 [INFO] Step[2600/2713]: training loss : 0.9414127457141876 TRAIN  loss dict:  {'classification_loss': 0.9414127457141876}
2025-01-13 05:39:26,400 [INFO] Step[2650/2713]: training loss : 0.9394725966453552 TRAIN  loss dict:  {'classification_loss': 0.9394725966453552}
2025-01-13 05:39:38,342 [INFO] Step[2700/2713]: training loss : 0.9397953641414643 TRAIN  loss dict:  {'classification_loss': 0.9397953641414643}
2025-01-13 05:41:26,990 [INFO] Label accuracies statistics:
2025-01-13 05:41:26,990 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 1.0, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 1.0, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.5, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.25, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 05:41:26,994 [INFO] [71] TRAIN  loss: 0.9417893331164238 acc: 0.9997542695662858
2025-01-13 05:41:26,994 [INFO] [71] TRAIN  loss dict: {'classification_loss': 0.9417893331164238}
2025-01-13 05:41:26,995 [INFO] [71] VALIDATION loss: 1.7163272087735342 VALIDATION acc: 0.831974921630094
2025-01-13 05:41:26,995 [INFO] [71] VALIDATION loss dict: {'classification_loss': 1.7163272087735342}
2025-01-13 05:41:26,995 [INFO] 
2025-01-13 05:41:51,207 [INFO] Step[50/2713]: training loss : 0.9390707731246948 TRAIN  loss dict:  {'classification_loss': 0.9390707731246948}
2025-01-13 05:42:03,820 [INFO] Step[100/2713]: training loss : 0.941317697763443 TRAIN  loss dict:  {'classification_loss': 0.941317697763443}
2025-01-13 05:42:15,770 [INFO] Step[150/2713]: training loss : 0.9386994218826294 TRAIN  loss dict:  {'classification_loss': 0.9386994218826294}
2025-01-13 05:42:27,703 [INFO] Step[200/2713]: training loss : 0.9408928668498993 TRAIN  loss dict:  {'classification_loss': 0.9408928668498993}
2025-01-13 05:42:39,606 [INFO] Step[250/2713]: training loss : 0.9424730849266052 TRAIN  loss dict:  {'classification_loss': 0.9424730849266052}
2025-01-13 05:42:51,448 [INFO] Step[300/2713]: training loss : 0.9417816615104675 TRAIN  loss dict:  {'classification_loss': 0.9417816615104675}
2025-01-13 05:43:03,353 [INFO] Step[350/2713]: training loss : 0.9403185665607452 TRAIN  loss dict:  {'classification_loss': 0.9403185665607452}
2025-01-13 05:43:15,211 [INFO] Step[400/2713]: training loss : 0.9395674633979797 TRAIN  loss dict:  {'classification_loss': 0.9395674633979797}
2025-01-13 05:43:27,169 [INFO] Step[450/2713]: training loss : 0.9415419459342956 TRAIN  loss dict:  {'classification_loss': 0.9415419459342956}
2025-01-13 05:43:39,038 [INFO] Step[500/2713]: training loss : 0.9388732242584229 TRAIN  loss dict:  {'classification_loss': 0.9388732242584229}
2025-01-13 05:43:50,941 [INFO] Step[550/2713]: training loss : 0.9376425278186798 TRAIN  loss dict:  {'classification_loss': 0.9376425278186798}
2025-01-13 05:44:02,784 [INFO] Step[600/2713]: training loss : 0.9412829053401947 TRAIN  loss dict:  {'classification_loss': 0.9412829053401947}
2025-01-13 05:44:14,655 [INFO] Step[650/2713]: training loss : 0.9384850215911865 TRAIN  loss dict:  {'classification_loss': 0.9384850215911865}
2025-01-13 05:44:26,518 [INFO] Step[700/2713]: training loss : 0.9368243241310119 TRAIN  loss dict:  {'classification_loss': 0.9368243241310119}
2025-01-13 05:44:38,425 [INFO] Step[750/2713]: training loss : 0.9391586589813232 TRAIN  loss dict:  {'classification_loss': 0.9391586589813232}
2025-01-13 05:44:50,328 [INFO] Step[800/2713]: training loss : 0.9400794184207917 TRAIN  loss dict:  {'classification_loss': 0.9400794184207917}
2025-01-13 05:45:02,213 [INFO] Step[850/2713]: training loss : 0.9392241740226746 TRAIN  loss dict:  {'classification_loss': 0.9392241740226746}
2025-01-13 05:45:14,053 [INFO] Step[900/2713]: training loss : 0.9417689192295075 TRAIN  loss dict:  {'classification_loss': 0.9417689192295075}
2025-01-13 05:45:25,925 [INFO] Step[950/2713]: training loss : 0.9407650971412659 TRAIN  loss dict:  {'classification_loss': 0.9407650971412659}
2025-01-13 05:45:37,812 [INFO] Step[1000/2713]: training loss : 0.9421665942668915 TRAIN  loss dict:  {'classification_loss': 0.9421665942668915}
2025-01-13 05:45:49,677 [INFO] Step[1050/2713]: training loss : 0.9393927943706513 TRAIN  loss dict:  {'classification_loss': 0.9393927943706513}
2025-01-13 05:46:01,557 [INFO] Step[1100/2713]: training loss : 0.9425467705726623 TRAIN  loss dict:  {'classification_loss': 0.9425467705726623}
2025-01-13 05:46:13,452 [INFO] Step[1150/2713]: training loss : 0.9420276689529419 TRAIN  loss dict:  {'classification_loss': 0.9420276689529419}
2025-01-13 05:46:25,373 [INFO] Step[1200/2713]: training loss : 0.9397337687015533 TRAIN  loss dict:  {'classification_loss': 0.9397337687015533}
2025-01-13 05:46:37,250 [INFO] Step[1250/2713]: training loss : 0.9609032356739045 TRAIN  loss dict:  {'classification_loss': 0.9609032356739045}
2025-01-13 05:46:49,143 [INFO] Step[1300/2713]: training loss : 0.9420977878570557 TRAIN  loss dict:  {'classification_loss': 0.9420977878570557}
2025-01-13 05:47:01,090 [INFO] Step[1350/2713]: training loss : 0.9387994241714478 TRAIN  loss dict:  {'classification_loss': 0.9387994241714478}
2025-01-13 05:47:12,996 [INFO] Step[1400/2713]: training loss : 0.9402213823795319 TRAIN  loss dict:  {'classification_loss': 0.9402213823795319}
2025-01-13 05:47:24,889 [INFO] Step[1450/2713]: training loss : 0.9429848337173462 TRAIN  loss dict:  {'classification_loss': 0.9429848337173462}
2025-01-13 05:47:36,777 [INFO] Step[1500/2713]: training loss : 0.941610015630722 TRAIN  loss dict:  {'classification_loss': 0.941610015630722}
2025-01-13 05:47:48,659 [INFO] Step[1550/2713]: training loss : 0.9401049733161926 TRAIN  loss dict:  {'classification_loss': 0.9401049733161926}
2025-01-13 05:48:00,527 [INFO] Step[1600/2713]: training loss : 0.9404660046100617 TRAIN  loss dict:  {'classification_loss': 0.9404660046100617}
2025-01-13 05:48:12,398 [INFO] Step[1650/2713]: training loss : 0.9560291647911072 TRAIN  loss dict:  {'classification_loss': 0.9560291647911072}
2025-01-13 05:48:24,296 [INFO] Step[1700/2713]: training loss : 0.9398871076107025 TRAIN  loss dict:  {'classification_loss': 0.9398871076107025}
2025-01-13 05:48:36,164 [INFO] Step[1750/2713]: training loss : 0.9394524693489075 TRAIN  loss dict:  {'classification_loss': 0.9394524693489075}
2025-01-13 05:48:48,042 [INFO] Step[1800/2713]: training loss : 0.9413636183738708 TRAIN  loss dict:  {'classification_loss': 0.9413636183738708}
2025-01-13 05:48:59,964 [INFO] Step[1850/2713]: training loss : 0.9443421542644501 TRAIN  loss dict:  {'classification_loss': 0.9443421542644501}
2025-01-13 05:49:11,833 [INFO] Step[1900/2713]: training loss : 0.9435312831401825 TRAIN  loss dict:  {'classification_loss': 0.9435312831401825}
2025-01-13 05:49:23,758 [INFO] Step[1950/2713]: training loss : 0.942004326581955 TRAIN  loss dict:  {'classification_loss': 0.942004326581955}
2025-01-13 05:49:35,686 [INFO] Step[2000/2713]: training loss : 0.9395045685768127 TRAIN  loss dict:  {'classification_loss': 0.9395045685768127}
2025-01-13 05:49:47,585 [INFO] Step[2050/2713]: training loss : 0.9426101112365722 TRAIN  loss dict:  {'classification_loss': 0.9426101112365722}
2025-01-13 05:49:59,445 [INFO] Step[2100/2713]: training loss : 0.9414794278144837 TRAIN  loss dict:  {'classification_loss': 0.9414794278144837}
2025-01-13 05:50:11,350 [INFO] Step[2150/2713]: training loss : 0.9406157505512237 TRAIN  loss dict:  {'classification_loss': 0.9406157505512237}
2025-01-13 05:50:23,275 [INFO] Step[2200/2713]: training loss : 0.9418637561798096 TRAIN  loss dict:  {'classification_loss': 0.9418637561798096}
2025-01-13 05:50:35,167 [INFO] Step[2250/2713]: training loss : 0.9450060474872589 TRAIN  loss dict:  {'classification_loss': 0.9450060474872589}
2025-01-13 05:50:47,095 [INFO] Step[2300/2713]: training loss : 0.9385362160205841 TRAIN  loss dict:  {'classification_loss': 0.9385362160205841}
2025-01-13 05:50:58,997 [INFO] Step[2350/2713]: training loss : 0.939458636045456 TRAIN  loss dict:  {'classification_loss': 0.939458636045456}
2025-01-13 05:51:10,880 [INFO] Step[2400/2713]: training loss : 0.9419211637973786 TRAIN  loss dict:  {'classification_loss': 0.9419211637973786}
2025-01-13 05:51:22,750 [INFO] Step[2450/2713]: training loss : 0.9420517098903656 TRAIN  loss dict:  {'classification_loss': 0.9420517098903656}
2025-01-13 05:51:34,651 [INFO] Step[2500/2713]: training loss : 0.9417352139949798 TRAIN  loss dict:  {'classification_loss': 0.9417352139949798}
2025-01-13 05:51:46,569 [INFO] Step[2550/2713]: training loss : 0.9391133320331574 TRAIN  loss dict:  {'classification_loss': 0.9391133320331574}
2025-01-13 05:51:58,460 [INFO] Step[2600/2713]: training loss : 0.9431017053127289 TRAIN  loss dict:  {'classification_loss': 0.9431017053127289}
2025-01-13 05:52:10,369 [INFO] Step[2650/2713]: training loss : 0.9423307776451111 TRAIN  loss dict:  {'classification_loss': 0.9423307776451111}
2025-01-13 05:52:22,240 [INFO] Step[2700/2713]: training loss : 0.9506204116344452 TRAIN  loss dict:  {'classification_loss': 0.9506204116344452}
2025-01-13 05:53:50,141 [INFO] Label accuracies statistics:
2025-01-13 05:53:50,142 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.5, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.5, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 0.75, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.5, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 05:53:50,143 [INFO] [72] TRAIN  loss: 0.9416500147839907 acc: 0.9995085391325715
2025-01-13 05:53:50,143 [INFO] [72] TRAIN  loss dict: {'classification_loss': 0.9416500147839907}
2025-01-13 05:53:50,144 [INFO] [72] VALIDATION loss: 1.7061968420008968 VALIDATION acc: 0.8369905956112853
2025-01-13 05:53:50,144 [INFO] [72] VALIDATION loss dict: {'classification_loss': 1.7061968420008968}
2025-01-13 05:53:50,144 [INFO] 
2025-01-13 05:54:08,167 [INFO] Step[50/2713]: training loss : 0.9412033116817474 TRAIN  loss dict:  {'classification_loss': 0.9412033116817474}
2025-01-13 05:54:20,067 [INFO] Step[100/2713]: training loss : 0.9388539350032806 TRAIN  loss dict:  {'classification_loss': 0.9388539350032806}
2025-01-13 05:54:31,993 [INFO] Step[150/2713]: training loss : 0.9437054824829102 TRAIN  loss dict:  {'classification_loss': 0.9437054824829102}
2025-01-13 05:54:43,893 [INFO] Step[200/2713]: training loss : 0.9394148099422455 TRAIN  loss dict:  {'classification_loss': 0.9394148099422455}
2025-01-13 05:54:55,833 [INFO] Step[250/2713]: training loss : 0.9387512981891633 TRAIN  loss dict:  {'classification_loss': 0.9387512981891633}
2025-01-13 05:55:07,735 [INFO] Step[300/2713]: training loss : 0.9409209275245667 TRAIN  loss dict:  {'classification_loss': 0.9409209275245667}
2025-01-13 05:55:19,662 [INFO] Step[350/2713]: training loss : 0.9458703219890594 TRAIN  loss dict:  {'classification_loss': 0.9458703219890594}
2025-01-13 05:55:31,533 [INFO] Step[400/2713]: training loss : 0.9405180037021637 TRAIN  loss dict:  {'classification_loss': 0.9405180037021637}
2025-01-13 05:55:43,495 [INFO] Step[450/2713]: training loss : 0.9401787984371185 TRAIN  loss dict:  {'classification_loss': 0.9401787984371185}
2025-01-13 05:55:55,387 [INFO] Step[500/2713]: training loss : 0.9399725592136383 TRAIN  loss dict:  {'classification_loss': 0.9399725592136383}
2025-01-13 05:56:07,326 [INFO] Step[550/2713]: training loss : 0.9569358479976654 TRAIN  loss dict:  {'classification_loss': 0.9569358479976654}
2025-01-13 05:56:19,243 [INFO] Step[600/2713]: training loss : 0.9446631693840026 TRAIN  loss dict:  {'classification_loss': 0.9446631693840026}
2025-01-13 05:56:31,167 [INFO] Step[650/2713]: training loss : 0.9406913185119629 TRAIN  loss dict:  {'classification_loss': 0.9406913185119629}
2025-01-13 05:56:43,076 [INFO] Step[700/2713]: training loss : 0.9402903878688812 TRAIN  loss dict:  {'classification_loss': 0.9402903878688812}
2025-01-13 05:56:54,984 [INFO] Step[750/2713]: training loss : 0.9400658857822418 TRAIN  loss dict:  {'classification_loss': 0.9400658857822418}
2025-01-13 05:57:06,884 [INFO] Step[800/2713]: training loss : 0.9405442190170288 TRAIN  loss dict:  {'classification_loss': 0.9405442190170288}
2025-01-13 05:57:18,823 [INFO] Step[850/2713]: training loss : 0.9387584102153778 TRAIN  loss dict:  {'classification_loss': 0.9387584102153778}
2025-01-13 05:57:30,731 [INFO] Step[900/2713]: training loss : 0.9399306988716125 TRAIN  loss dict:  {'classification_loss': 0.9399306988716125}
2025-01-13 05:57:42,698 [INFO] Step[950/2713]: training loss : 0.9414663124084472 TRAIN  loss dict:  {'classification_loss': 0.9414663124084472}
2025-01-13 05:57:54,576 [INFO] Step[1000/2713]: training loss : 0.939990085363388 TRAIN  loss dict:  {'classification_loss': 0.939990085363388}
2025-01-13 05:58:06,484 [INFO] Step[1050/2713]: training loss : 0.9492484891414642 TRAIN  loss dict:  {'classification_loss': 0.9492484891414642}
2025-01-13 05:58:18,419 [INFO] Step[1100/2713]: training loss : 0.9441419351100921 TRAIN  loss dict:  {'classification_loss': 0.9441419351100921}
2025-01-13 05:58:30,353 [INFO] Step[1150/2713]: training loss : 0.939833208322525 TRAIN  loss dict:  {'classification_loss': 0.939833208322525}
2025-01-13 05:58:42,302 [INFO] Step[1200/2713]: training loss : 0.940201575756073 TRAIN  loss dict:  {'classification_loss': 0.940201575756073}
2025-01-13 05:58:54,233 [INFO] Step[1250/2713]: training loss : 0.9399076342582703 TRAIN  loss dict:  {'classification_loss': 0.9399076342582703}
2025-01-13 05:59:06,434 [INFO] Step[1300/2713]: training loss : 0.9430244815349579 TRAIN  loss dict:  {'classification_loss': 0.9430244815349579}
2025-01-13 05:59:18,847 [INFO] Step[1350/2713]: training loss : 0.9452342367172242 TRAIN  loss dict:  {'classification_loss': 0.9452342367172242}
2025-01-13 05:59:31,268 [INFO] Step[1400/2713]: training loss : 0.9386682820320129 TRAIN  loss dict:  {'classification_loss': 0.9386682820320129}
2025-01-13 05:59:43,566 [INFO] Step[1450/2713]: training loss : 0.9406087291240692 TRAIN  loss dict:  {'classification_loss': 0.9406087291240692}
2025-01-13 05:59:56,206 [INFO] Step[1500/2713]: training loss : 0.9417595791816712 TRAIN  loss dict:  {'classification_loss': 0.9417595791816712}
2025-01-13 06:00:08,553 [INFO] Step[1550/2713]: training loss : 0.9418343520164489 TRAIN  loss dict:  {'classification_loss': 0.9418343520164489}
2025-01-13 06:00:21,354 [INFO] Step[1600/2713]: training loss : 0.9381208920478821 TRAIN  loss dict:  {'classification_loss': 0.9381208920478821}
2025-01-13 06:00:35,081 [INFO] Step[1650/2713]: training loss : 0.939067839384079 TRAIN  loss dict:  {'classification_loss': 0.939067839384079}
2025-01-13 06:00:48,707 [INFO] Step[1700/2713]: training loss : 0.9408444619178772 TRAIN  loss dict:  {'classification_loss': 0.9408444619178772}
2025-01-13 06:01:00,881 [INFO] Step[1750/2713]: training loss : 0.9405197191238404 TRAIN  loss dict:  {'classification_loss': 0.9405197191238404}
2025-01-13 06:01:12,731 [INFO] Step[1800/2713]: training loss : 0.9466755020618439 TRAIN  loss dict:  {'classification_loss': 0.9466755020618439}
2025-01-13 06:01:24,540 [INFO] Step[1850/2713]: training loss : 0.9410832440853119 TRAIN  loss dict:  {'classification_loss': 0.9410832440853119}
2025-01-13 06:01:36,433 [INFO] Step[1900/2713]: training loss : 0.9415340685844421 TRAIN  loss dict:  {'classification_loss': 0.9415340685844421}
2025-01-13 06:01:48,305 [INFO] Step[1950/2713]: training loss : 0.9402621126174927 TRAIN  loss dict:  {'classification_loss': 0.9402621126174927}
2025-01-13 06:02:00,164 [INFO] Step[2000/2713]: training loss : 0.9472693037986756 TRAIN  loss dict:  {'classification_loss': 0.9472693037986756}
2025-01-13 06:02:12,033 [INFO] Step[2050/2713]: training loss : 0.9424597322940826 TRAIN  loss dict:  {'classification_loss': 0.9424597322940826}
2025-01-13 06:02:23,888 [INFO] Step[2100/2713]: training loss : 0.9417660510540009 TRAIN  loss dict:  {'classification_loss': 0.9417660510540009}
2025-01-13 06:02:35,795 [INFO] Step[2150/2713]: training loss : 0.9398661363124847 TRAIN  loss dict:  {'classification_loss': 0.9398661363124847}
2025-01-13 06:02:47,714 [INFO] Step[2200/2713]: training loss : 0.9434997689723968 TRAIN  loss dict:  {'classification_loss': 0.9434997689723968}
2025-01-13 06:02:59,611 [INFO] Step[2250/2713]: training loss : 0.9432984006404876 TRAIN  loss dict:  {'classification_loss': 0.9432984006404876}
2025-01-13 06:03:11,509 [INFO] Step[2300/2713]: training loss : 0.9371707451343536 TRAIN  loss dict:  {'classification_loss': 0.9371707451343536}
2025-01-13 06:03:23,458 [INFO] Step[2350/2713]: training loss : 0.9435297977924347 TRAIN  loss dict:  {'classification_loss': 0.9435297977924347}
2025-01-13 06:03:35,358 [INFO] Step[2400/2713]: training loss : 0.9430750167369842 TRAIN  loss dict:  {'classification_loss': 0.9430750167369842}
2025-01-13 06:03:47,256 [INFO] Step[2450/2713]: training loss : 0.9417311608791351 TRAIN  loss dict:  {'classification_loss': 0.9417311608791351}
2025-01-13 06:03:59,161 [INFO] Step[2500/2713]: training loss : 0.9416190028190613 TRAIN  loss dict:  {'classification_loss': 0.9416190028190613}
2025-01-13 06:04:11,038 [INFO] Step[2550/2713]: training loss : 0.9403189671039581 TRAIN  loss dict:  {'classification_loss': 0.9403189671039581}
2025-01-13 06:04:22,947 [INFO] Step[2600/2713]: training loss : 0.9433041501045227 TRAIN  loss dict:  {'classification_loss': 0.9433041501045227}
2025-01-13 06:04:34,873 [INFO] Step[2650/2713]: training loss : 0.9512959921360016 TRAIN  loss dict:  {'classification_loss': 0.9512959921360016}
2025-01-13 06:04:46,792 [INFO] Step[2700/2713]: training loss : 0.9385702264308929 TRAIN  loss dict:  {'classification_loss': 0.9385702264308929}
2025-01-13 06:06:14,994 [INFO] Label accuracies statistics:
2025-01-13 06:06:14,994 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 1.0, 205: 0.75, 206: 0.5, 207: 0.75, 208: 1.0, 209: 0.75, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.75, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 0.75, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 06:06:14,997 [INFO] [73] TRAIN  loss: 0.9419267856674883 acc: 0.9996314043494287
2025-01-13 06:06:14,997 [INFO] [73] TRAIN  loss dict: {'classification_loss': 0.9419267856674883}
2025-01-13 06:06:14,997 [INFO] [73] VALIDATION loss: 1.6907432199196708 VALIDATION acc: 0.8382445141065831
2025-01-13 06:06:14,997 [INFO] [73] VALIDATION loss dict: {'classification_loss': 1.6907432199196708}
2025-01-13 06:06:14,997 [INFO] 
2025-01-13 06:06:32,418 [INFO] Step[50/2713]: training loss : 0.9381691300868988 TRAIN  loss dict:  {'classification_loss': 0.9381691300868988}
2025-01-13 06:06:44,351 [INFO] Step[100/2713]: training loss : 0.9413157093524933 TRAIN  loss dict:  {'classification_loss': 0.9413157093524933}
2025-01-13 06:06:56,365 [INFO] Step[150/2713]: training loss : 0.9396760094165802 TRAIN  loss dict:  {'classification_loss': 0.9396760094165802}
2025-01-13 06:07:08,305 [INFO] Step[200/2713]: training loss : 0.9410538733005523 TRAIN  loss dict:  {'classification_loss': 0.9410538733005523}
2025-01-13 06:07:20,286 [INFO] Step[250/2713]: training loss : 0.9490290808677674 TRAIN  loss dict:  {'classification_loss': 0.9490290808677674}
2025-01-13 06:07:32,234 [INFO] Step[300/2713]: training loss : 0.9416080343723298 TRAIN  loss dict:  {'classification_loss': 0.9416080343723298}
2025-01-13 06:07:44,205 [INFO] Step[350/2713]: training loss : 0.9399904358386993 TRAIN  loss dict:  {'classification_loss': 0.9399904358386993}
2025-01-13 06:07:56,121 [INFO] Step[400/2713]: training loss : 0.9405058932304382 TRAIN  loss dict:  {'classification_loss': 0.9405058932304382}
2025-01-13 06:08:08,068 [INFO] Step[450/2713]: training loss : 0.9416879439353942 TRAIN  loss dict:  {'classification_loss': 0.9416879439353942}
2025-01-13 06:08:19,968 [INFO] Step[500/2713]: training loss : 0.9427214503288269 TRAIN  loss dict:  {'classification_loss': 0.9427214503288269}
2025-01-13 06:08:31,928 [INFO] Step[550/2713]: training loss : 0.9393682312965393 TRAIN  loss dict:  {'classification_loss': 0.9393682312965393}
2025-01-13 06:08:43,884 [INFO] Step[600/2713]: training loss : 0.9387647008895874 TRAIN  loss dict:  {'classification_loss': 0.9387647008895874}
2025-01-13 06:08:55,865 [INFO] Step[650/2713]: training loss : 0.9389429402351379 TRAIN  loss dict:  {'classification_loss': 0.9389429402351379}
2025-01-13 06:09:07,806 [INFO] Step[700/2713]: training loss : 0.9418381571769714 TRAIN  loss dict:  {'classification_loss': 0.9418381571769714}
2025-01-13 06:09:19,772 [INFO] Step[750/2713]: training loss : 0.9402892124652863 TRAIN  loss dict:  {'classification_loss': 0.9402892124652863}
2025-01-13 06:09:31,748 [INFO] Step[800/2713]: training loss : 0.9410503733158112 TRAIN  loss dict:  {'classification_loss': 0.9410503733158112}
2025-01-13 06:09:43,700 [INFO] Step[850/2713]: training loss : 0.9407244765758515 TRAIN  loss dict:  {'classification_loss': 0.9407244765758515}
2025-01-13 06:09:55,583 [INFO] Step[900/2713]: training loss : 0.938505048751831 TRAIN  loss dict:  {'classification_loss': 0.938505048751831}
2025-01-13 06:10:07,502 [INFO] Step[950/2713]: training loss : 0.943521579504013 TRAIN  loss dict:  {'classification_loss': 0.943521579504013}
2025-01-13 06:10:19,433 [INFO] Step[1000/2713]: training loss : 0.9417872273921967 TRAIN  loss dict:  {'classification_loss': 0.9417872273921967}
2025-01-13 06:10:31,378 [INFO] Step[1050/2713]: training loss : 0.9402973258495331 TRAIN  loss dict:  {'classification_loss': 0.9402973258495331}
2025-01-13 06:10:43,298 [INFO] Step[1100/2713]: training loss : 0.9389237713813782 TRAIN  loss dict:  {'classification_loss': 0.9389237713813782}
2025-01-13 06:10:55,273 [INFO] Step[1150/2713]: training loss : 0.9381247675418853 TRAIN  loss dict:  {'classification_loss': 0.9381247675418853}
2025-01-13 06:11:07,212 [INFO] Step[1200/2713]: training loss : 0.939701327085495 TRAIN  loss dict:  {'classification_loss': 0.939701327085495}
2025-01-13 06:11:19,180 [INFO] Step[1250/2713]: training loss : 0.9435255253314971 TRAIN  loss dict:  {'classification_loss': 0.9435255253314971}
2025-01-13 06:11:31,093 [INFO] Step[1300/2713]: training loss : 0.9452198231220246 TRAIN  loss dict:  {'classification_loss': 0.9452198231220246}
2025-01-13 06:11:43,062 [INFO] Step[1350/2713]: training loss : 0.9454213690757751 TRAIN  loss dict:  {'classification_loss': 0.9454213690757751}
2025-01-13 06:11:55,033 [INFO] Step[1400/2713]: training loss : 0.9396034908294678 TRAIN  loss dict:  {'classification_loss': 0.9396034908294678}
2025-01-13 06:12:06,979 [INFO] Step[1450/2713]: training loss : 0.9399504089355468 TRAIN  loss dict:  {'classification_loss': 0.9399504089355468}
2025-01-13 06:12:18,891 [INFO] Step[1500/2713]: training loss : 0.9437262332439422 TRAIN  loss dict:  {'classification_loss': 0.9437262332439422}
2025-01-13 06:12:30,807 [INFO] Step[1550/2713]: training loss : 0.941935828924179 TRAIN  loss dict:  {'classification_loss': 0.941935828924179}
2025-01-13 06:12:42,773 [INFO] Step[1600/2713]: training loss : 0.9396936738491058 TRAIN  loss dict:  {'classification_loss': 0.9396936738491058}
2025-01-13 06:12:54,734 [INFO] Step[1650/2713]: training loss : 0.9383478319644928 TRAIN  loss dict:  {'classification_loss': 0.9383478319644928}
2025-01-13 06:13:06,696 [INFO] Step[1700/2713]: training loss : 0.9404838061332703 TRAIN  loss dict:  {'classification_loss': 0.9404838061332703}
2025-01-13 06:13:18,669 [INFO] Step[1750/2713]: training loss : 0.9417166948318482 TRAIN  loss dict:  {'classification_loss': 0.9417166948318482}
2025-01-13 06:13:30,623 [INFO] Step[1800/2713]: training loss : 0.9421345031261444 TRAIN  loss dict:  {'classification_loss': 0.9421345031261444}
2025-01-13 06:13:42,622 [INFO] Step[1850/2713]: training loss : 0.9409639239311218 TRAIN  loss dict:  {'classification_loss': 0.9409639239311218}
2025-01-13 06:13:54,527 [INFO] Step[1900/2713]: training loss : 0.9401312887668609 TRAIN  loss dict:  {'classification_loss': 0.9401312887668609}
2025-01-13 06:14:06,527 [INFO] Step[1950/2713]: training loss : 0.9397810578346253 TRAIN  loss dict:  {'classification_loss': 0.9397810578346253}
2025-01-13 06:14:18,460 [INFO] Step[2000/2713]: training loss : 0.9517804336547852 TRAIN  loss dict:  {'classification_loss': 0.9517804336547852}
2025-01-13 06:14:30,381 [INFO] Step[2050/2713]: training loss : 0.9388577723503113 TRAIN  loss dict:  {'classification_loss': 0.9388577723503113}
2025-01-13 06:14:42,381 [INFO] Step[2100/2713]: training loss : 0.950186800956726 TRAIN  loss dict:  {'classification_loss': 0.950186800956726}
2025-01-13 06:14:54,314 [INFO] Step[2150/2713]: training loss : 0.9380441093444825 TRAIN  loss dict:  {'classification_loss': 0.9380441093444825}
2025-01-13 06:15:06,247 [INFO] Step[2200/2713]: training loss : 0.9403834486007691 TRAIN  loss dict:  {'classification_loss': 0.9403834486007691}
2025-01-13 06:15:18,199 [INFO] Step[2250/2713]: training loss : 0.9437346482276916 TRAIN  loss dict:  {'classification_loss': 0.9437346482276916}
2025-01-13 06:15:30,137 [INFO] Step[2300/2713]: training loss : 0.9410524940490723 TRAIN  loss dict:  {'classification_loss': 0.9410524940490723}
2025-01-13 06:15:42,092 [INFO] Step[2350/2713]: training loss : 0.9426360166072846 TRAIN  loss dict:  {'classification_loss': 0.9426360166072846}
2025-01-13 06:15:54,058 [INFO] Step[2400/2713]: training loss : 0.9402733016014099 TRAIN  loss dict:  {'classification_loss': 0.9402733016014099}
2025-01-13 06:16:06,005 [INFO] Step[2450/2713]: training loss : 0.9390782582759857 TRAIN  loss dict:  {'classification_loss': 0.9390782582759857}
2025-01-13 06:16:17,921 [INFO] Step[2500/2713]: training loss : 0.9385808634757996 TRAIN  loss dict:  {'classification_loss': 0.9385808634757996}
2025-01-13 06:16:29,889 [INFO] Step[2550/2713]: training loss : 0.9403647756576539 TRAIN  loss dict:  {'classification_loss': 0.9403647756576539}
2025-01-13 06:16:41,811 [INFO] Step[2600/2713]: training loss : 0.9395064079761505 TRAIN  loss dict:  {'classification_loss': 0.9395064079761505}
2025-01-13 06:16:53,794 [INFO] Step[2650/2713]: training loss : 0.939672269821167 TRAIN  loss dict:  {'classification_loss': 0.939672269821167}
2025-01-13 06:17:05,803 [INFO] Step[2700/2713]: training loss : 0.9395010113716126 TRAIN  loss dict:  {'classification_loss': 0.9395010113716126}
2025-01-13 06:18:37,339 [INFO] Label accuracies statistics:
2025-01-13 06:18:37,339 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 1.0, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 1.0, 205: 1.0, 206: 1.0, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 1.0, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 0.75, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 06:18:38,509 [INFO] [74] TRAIN  loss: 0.9412017945781037 acc: 0.9997542695662858
2025-01-13 06:18:38,509 [INFO] [74] TRAIN  loss dict: {'classification_loss': 0.9412017945781037}
2025-01-13 06:18:38,510 [INFO] [74] VALIDATION loss: 1.6742985782990778 VALIDATION acc: 0.8463949843260188
2025-01-13 06:18:38,510 [INFO] [74] VALIDATION loss dict: {'classification_loss': 1.6742985782990778}
2025-01-13 06:18:38,510 [INFO] 
2025-01-13 06:18:58,334 [INFO] Step[50/2713]: training loss : 0.9382246041297913 TRAIN  loss dict:  {'classification_loss': 0.9382246041297913}
2025-01-13 06:19:10,746 [INFO] Step[100/2713]: training loss : 0.9385069787502289 TRAIN  loss dict:  {'classification_loss': 0.9385069787502289}
2025-01-13 06:19:23,205 [INFO] Step[150/2713]: training loss : 0.9398637187480926 TRAIN  loss dict:  {'classification_loss': 0.9398637187480926}
2025-01-13 06:19:36,073 [INFO] Step[200/2713]: training loss : 0.9400556397438049 TRAIN  loss dict:  {'classification_loss': 0.9400556397438049}
2025-01-13 06:19:49,605 [INFO] Step[250/2713]: training loss : 0.9402069222927093 TRAIN  loss dict:  {'classification_loss': 0.9402069222927093}
2025-01-13 06:20:02,497 [INFO] Step[300/2713]: training loss : 0.9410725212097169 TRAIN  loss dict:  {'classification_loss': 0.9410725212097169}
2025-01-13 06:20:14,707 [INFO] Step[350/2713]: training loss : 0.9416251051425933 TRAIN  loss dict:  {'classification_loss': 0.9416251051425933}
2025-01-13 06:20:26,579 [INFO] Step[400/2713]: training loss : 0.9386250305175782 TRAIN  loss dict:  {'classification_loss': 0.9386250305175782}
2025-01-13 06:20:38,473 [INFO] Step[450/2713]: training loss : 0.9406450009346008 TRAIN  loss dict:  {'classification_loss': 0.9406450009346008}
2025-01-13 06:20:50,332 [INFO] Step[500/2713]: training loss : 0.9401807880401611 TRAIN  loss dict:  {'classification_loss': 0.9401807880401611}
2025-01-13 06:21:02,273 [INFO] Step[550/2713]: training loss : 0.9420434033870697 TRAIN  loss dict:  {'classification_loss': 0.9420434033870697}
2025-01-13 06:21:14,153 [INFO] Step[600/2713]: training loss : 0.9389435994625092 TRAIN  loss dict:  {'classification_loss': 0.9389435994625092}
2025-01-13 06:21:26,094 [INFO] Step[650/2713]: training loss : 0.9397123050689697 TRAIN  loss dict:  {'classification_loss': 0.9397123050689697}
2025-01-13 06:21:38,009 [INFO] Step[700/2713]: training loss : 0.9394055616855621 TRAIN  loss dict:  {'classification_loss': 0.9394055616855621}
2025-01-13 06:21:49,942 [INFO] Step[750/2713]: training loss : 0.9394961285591126 TRAIN  loss dict:  {'classification_loss': 0.9394961285591126}
2025-01-13 06:22:01,859 [INFO] Step[800/2713]: training loss : 0.9382414615154266 TRAIN  loss dict:  {'classification_loss': 0.9382414615154266}
2025-01-13 06:22:13,779 [INFO] Step[850/2713]: training loss : 0.9428997623920441 TRAIN  loss dict:  {'classification_loss': 0.9428997623920441}
2025-01-13 06:22:25,680 [INFO] Step[900/2713]: training loss : 0.9414666175842286 TRAIN  loss dict:  {'classification_loss': 0.9414666175842286}
2025-01-13 06:22:37,571 [INFO] Step[950/2713]: training loss : 0.9393003594875335 TRAIN  loss dict:  {'classification_loss': 0.9393003594875335}
2025-01-13 06:22:49,523 [INFO] Step[1000/2713]: training loss : 0.9456828212738038 TRAIN  loss dict:  {'classification_loss': 0.9456828212738038}
2025-01-13 06:23:01,439 [INFO] Step[1050/2713]: training loss : 0.9400971853733062 TRAIN  loss dict:  {'classification_loss': 0.9400971853733062}
2025-01-13 06:23:13,313 [INFO] Step[1100/2713]: training loss : 0.9402576816082001 TRAIN  loss dict:  {'classification_loss': 0.9402576816082001}
2025-01-13 06:23:25,235 [INFO] Step[1150/2713]: training loss : 0.9403836250305175 TRAIN  loss dict:  {'classification_loss': 0.9403836250305175}
2025-01-13 06:23:37,159 [INFO] Step[1200/2713]: training loss : 0.9404204857349395 TRAIN  loss dict:  {'classification_loss': 0.9404204857349395}
2025-01-13 06:23:49,054 [INFO] Step[1250/2713]: training loss : 0.9381727695465087 TRAIN  loss dict:  {'classification_loss': 0.9381727695465087}
2025-01-13 06:24:00,984 [INFO] Step[1300/2713]: training loss : 0.939622529745102 TRAIN  loss dict:  {'classification_loss': 0.939622529745102}
2025-01-13 06:24:12,937 [INFO] Step[1350/2713]: training loss : 0.9380035245418549 TRAIN  loss dict:  {'classification_loss': 0.9380035245418549}
2025-01-13 06:24:24,901 [INFO] Step[1400/2713]: training loss : 0.938170999288559 TRAIN  loss dict:  {'classification_loss': 0.938170999288559}
2025-01-13 06:24:36,804 [INFO] Step[1450/2713]: training loss : 0.9394434928894043 TRAIN  loss dict:  {'classification_loss': 0.9394434928894043}
2025-01-13 06:24:48,734 [INFO] Step[1500/2713]: training loss : 0.9400023186206817 TRAIN  loss dict:  {'classification_loss': 0.9400023186206817}
2025-01-13 06:25:00,682 [INFO] Step[1550/2713]: training loss : 0.9380137121677399 TRAIN  loss dict:  {'classification_loss': 0.9380137121677399}
2025-01-13 06:25:12,619 [INFO] Step[1600/2713]: training loss : 0.9394953310489654 TRAIN  loss dict:  {'classification_loss': 0.9394953310489654}
2025-01-13 06:25:24,562 [INFO] Step[1650/2713]: training loss : 0.9434304404258728 TRAIN  loss dict:  {'classification_loss': 0.9434304404258728}
2025-01-13 06:25:36,527 [INFO] Step[1700/2713]: training loss : 0.941318827867508 TRAIN  loss dict:  {'classification_loss': 0.941318827867508}
2025-01-13 06:25:48,439 [INFO] Step[1750/2713]: training loss : 0.9393755602836609 TRAIN  loss dict:  {'classification_loss': 0.9393755602836609}
2025-01-13 06:26:00,350 [INFO] Step[1800/2713]: training loss : 0.9400818228721619 TRAIN  loss dict:  {'classification_loss': 0.9400818228721619}
2025-01-13 06:26:12,311 [INFO] Step[1850/2713]: training loss : 0.9376099181175231 TRAIN  loss dict:  {'classification_loss': 0.9376099181175231}
2025-01-13 06:26:24,230 [INFO] Step[1900/2713]: training loss : 0.9407728409767151 TRAIN  loss dict:  {'classification_loss': 0.9407728409767151}
2025-01-13 06:26:36,156 [INFO] Step[1950/2713]: training loss : 0.9421144151687622 TRAIN  loss dict:  {'classification_loss': 0.9421144151687622}
2025-01-13 06:26:48,111 [INFO] Step[2000/2713]: training loss : 0.9394852209091187 TRAIN  loss dict:  {'classification_loss': 0.9394852209091187}
2025-01-13 06:27:00,033 [INFO] Step[2050/2713]: training loss : 0.9395022296905517 TRAIN  loss dict:  {'classification_loss': 0.9395022296905517}
2025-01-13 06:27:11,935 [INFO] Step[2100/2713]: training loss : 0.9401884603500367 TRAIN  loss dict:  {'classification_loss': 0.9401884603500367}
2025-01-13 06:27:23,850 [INFO] Step[2150/2713]: training loss : 0.9404599940776825 TRAIN  loss dict:  {'classification_loss': 0.9404599940776825}
2025-01-13 06:27:35,768 [INFO] Step[2200/2713]: training loss : 0.9401370859146119 TRAIN  loss dict:  {'classification_loss': 0.9401370859146119}
2025-01-13 06:27:47,689 [INFO] Step[2250/2713]: training loss : 0.939251892566681 TRAIN  loss dict:  {'classification_loss': 0.939251892566681}
2025-01-13 06:27:59,609 [INFO] Step[2300/2713]: training loss : 0.9415360045433044 TRAIN  loss dict:  {'classification_loss': 0.9415360045433044}
2025-01-13 06:28:11,484 [INFO] Step[2350/2713]: training loss : 0.9382559669017791 TRAIN  loss dict:  {'classification_loss': 0.9382559669017791}
2025-01-13 06:28:23,412 [INFO] Step[2400/2713]: training loss : 0.9378192293643951 TRAIN  loss dict:  {'classification_loss': 0.9378192293643951}
2025-01-13 06:28:35,354 [INFO] Step[2450/2713]: training loss : 0.9404327464103699 TRAIN  loss dict:  {'classification_loss': 0.9404327464103699}
2025-01-13 06:28:47,275 [INFO] Step[2500/2713]: training loss : 0.9407091677188874 TRAIN  loss dict:  {'classification_loss': 0.9407091677188874}
2025-01-13 06:28:59,200 [INFO] Step[2550/2713]: training loss : 0.939028594493866 TRAIN  loss dict:  {'classification_loss': 0.939028594493866}
2025-01-13 06:29:11,170 [INFO] Step[2600/2713]: training loss : 0.9404104721546173 TRAIN  loss dict:  {'classification_loss': 0.9404104721546173}
2025-01-13 06:29:23,114 [INFO] Step[2650/2713]: training loss : 0.9435613763332367 TRAIN  loss dict:  {'classification_loss': 0.9435613763332367}
2025-01-13 06:29:35,048 [INFO] Step[2700/2713]: training loss : 0.9388577198982239 TRAIN  loss dict:  {'classification_loss': 0.9388577198982239}
2025-01-13 06:31:02,604 [INFO] Label accuracies statistics:
2025-01-13 06:31:02,604 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.5, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 06:31:02,606 [INFO] [75] TRAIN  loss: 0.9400419655911297 acc: 1.0
2025-01-13 06:31:02,606 [INFO] [75] TRAIN  loss dict: {'classification_loss': 0.9400419655911297}
2025-01-13 06:31:02,607 [INFO] [75] VALIDATION loss: 1.730338149949124 VALIDATION acc: 0.8294670846394985
2025-01-13 06:31:02,607 [INFO] [75] VALIDATION loss dict: {'classification_loss': 1.730338149949124}
2025-01-13 06:31:02,607 [INFO] 
2025-01-13 06:31:19,995 [INFO] Step[50/2713]: training loss : 0.9376555824279785 TRAIN  loss dict:  {'classification_loss': 0.9376555824279785}
2025-01-13 06:31:31,901 [INFO] Step[100/2713]: training loss : 0.9397896742820739 TRAIN  loss dict:  {'classification_loss': 0.9397896742820739}
2025-01-13 06:31:43,808 [INFO] Step[150/2713]: training loss : 0.9411783969402313 TRAIN  loss dict:  {'classification_loss': 0.9411783969402313}
2025-01-13 06:31:55,771 [INFO] Step[200/2713]: training loss : 0.9373727083206177 TRAIN  loss dict:  {'classification_loss': 0.9373727083206177}
2025-01-13 06:32:07,713 [INFO] Step[250/2713]: training loss : 0.9382456886768341 TRAIN  loss dict:  {'classification_loss': 0.9382456886768341}
2025-01-13 06:32:19,620 [INFO] Step[300/2713]: training loss : 0.939093006849289 TRAIN  loss dict:  {'classification_loss': 0.939093006849289}
2025-01-13 06:32:31,529 [INFO] Step[350/2713]: training loss : 0.9379797625541687 TRAIN  loss dict:  {'classification_loss': 0.9379797625541687}
2025-01-13 06:32:43,466 [INFO] Step[400/2713]: training loss : 0.939835000038147 TRAIN  loss dict:  {'classification_loss': 0.939835000038147}
2025-01-13 06:32:55,390 [INFO] Step[450/2713]: training loss : 0.9423282194137573 TRAIN  loss dict:  {'classification_loss': 0.9423282194137573}
2025-01-13 06:33:07,306 [INFO] Step[500/2713]: training loss : 0.9406806910037995 TRAIN  loss dict:  {'classification_loss': 0.9406806910037995}
2025-01-13 06:33:19,218 [INFO] Step[550/2713]: training loss : 0.9380336511135101 TRAIN  loss dict:  {'classification_loss': 0.9380336511135101}
2025-01-13 06:33:31,138 [INFO] Step[600/2713]: training loss : 0.9372839939594269 TRAIN  loss dict:  {'classification_loss': 0.9372839939594269}
2025-01-13 06:33:43,071 [INFO] Step[650/2713]: training loss : 0.9397879326343537 TRAIN  loss dict:  {'classification_loss': 0.9397879326343537}
2025-01-13 06:33:54,981 [INFO] Step[700/2713]: training loss : 0.9377950525283814 TRAIN  loss dict:  {'classification_loss': 0.9377950525283814}
2025-01-13 06:34:06,885 [INFO] Step[750/2713]: training loss : 0.9377523910999298 TRAIN  loss dict:  {'classification_loss': 0.9377523910999298}
2025-01-13 06:34:18,827 [INFO] Step[800/2713]: training loss : 0.9381831872463227 TRAIN  loss dict:  {'classification_loss': 0.9381831872463227}
2025-01-13 06:34:30,773 [INFO] Step[850/2713]: training loss : 0.9399939358234406 TRAIN  loss dict:  {'classification_loss': 0.9399939358234406}
2025-01-13 06:34:42,644 [INFO] Step[900/2713]: training loss : 0.9394127488136291 TRAIN  loss dict:  {'classification_loss': 0.9394127488136291}
2025-01-13 06:34:54,592 [INFO] Step[950/2713]: training loss : 0.9426400709152222 TRAIN  loss dict:  {'classification_loss': 0.9426400709152222}
2025-01-13 06:35:06,508 [INFO] Step[1000/2713]: training loss : 0.9421606147289276 TRAIN  loss dict:  {'classification_loss': 0.9421606147289276}
2025-01-13 06:35:18,429 [INFO] Step[1050/2713]: training loss : 0.9407461774349213 TRAIN  loss dict:  {'classification_loss': 0.9407461774349213}
2025-01-13 06:35:30,337 [INFO] Step[1100/2713]: training loss : 0.9405663871765136 TRAIN  loss dict:  {'classification_loss': 0.9405663871765136}
2025-01-13 06:35:42,264 [INFO] Step[1150/2713]: training loss : 0.938510035276413 TRAIN  loss dict:  {'classification_loss': 0.938510035276413}
2025-01-13 06:35:54,165 [INFO] Step[1200/2713]: training loss : 0.9383585965633392 TRAIN  loss dict:  {'classification_loss': 0.9383585965633392}
2025-01-13 06:36:06,097 [INFO] Step[1250/2713]: training loss : 0.9456012845039368 TRAIN  loss dict:  {'classification_loss': 0.9456012845039368}
2025-01-13 06:36:17,977 [INFO] Step[1300/2713]: training loss : 0.9390954840183258 TRAIN  loss dict:  {'classification_loss': 0.9390954840183258}
2025-01-13 06:36:29,926 [INFO] Step[1350/2713]: training loss : 0.9428242826461792 TRAIN  loss dict:  {'classification_loss': 0.9428242826461792}
2025-01-13 06:36:41,895 [INFO] Step[1400/2713]: training loss : 0.9388886678218842 TRAIN  loss dict:  {'classification_loss': 0.9388886678218842}
2025-01-13 06:36:53,779 [INFO] Step[1450/2713]: training loss : 0.9416711044311523 TRAIN  loss dict:  {'classification_loss': 0.9416711044311523}
2025-01-13 06:37:05,867 [INFO] Step[1500/2713]: training loss : 0.9376798903942108 TRAIN  loss dict:  {'classification_loss': 0.9376798903942108}
2025-01-13 06:37:18,342 [INFO] Step[1550/2713]: training loss : 0.9398702132701874 TRAIN  loss dict:  {'classification_loss': 0.9398702132701874}
2025-01-13 06:37:30,716 [INFO] Step[1600/2713]: training loss : 0.9406902980804444 TRAIN  loss dict:  {'classification_loss': 0.9406902980804444}
2025-01-13 06:37:42,921 [INFO] Step[1650/2713]: training loss : 0.9393166947364807 TRAIN  loss dict:  {'classification_loss': 0.9393166947364807}
2025-01-13 06:37:55,663 [INFO] Step[1700/2713]: training loss : 0.9377810537815094 TRAIN  loss dict:  {'classification_loss': 0.9377810537815094}
2025-01-13 06:38:08,045 [INFO] Step[1750/2713]: training loss : 0.9384954822063446 TRAIN  loss dict:  {'classification_loss': 0.9384954822063446}
2025-01-13 06:38:20,732 [INFO] Step[1800/2713]: training loss : 0.9388636171817779 TRAIN  loss dict:  {'classification_loss': 0.9388636171817779}
2025-01-13 06:38:34,207 [INFO] Step[1850/2713]: training loss : 0.9444135046005249 TRAIN  loss dict:  {'classification_loss': 0.9444135046005249}
2025-01-13 06:38:47,773 [INFO] Step[1900/2713]: training loss : 0.9384258723258972 TRAIN  loss dict:  {'classification_loss': 0.9384258723258972}
2025-01-13 06:39:00,084 [INFO] Step[1950/2713]: training loss : 0.9411130285263062 TRAIN  loss dict:  {'classification_loss': 0.9411130285263062}
2025-01-13 06:39:11,946 [INFO] Step[2000/2713]: training loss : 0.9401748538017273 TRAIN  loss dict:  {'classification_loss': 0.9401748538017273}
2025-01-13 06:39:23,791 [INFO] Step[2050/2713]: training loss : 0.9399782550334931 TRAIN  loss dict:  {'classification_loss': 0.9399782550334931}
2025-01-13 06:39:35,645 [INFO] Step[2100/2713]: training loss : 0.9400009751319885 TRAIN  loss dict:  {'classification_loss': 0.9400009751319885}
2025-01-13 06:39:47,523 [INFO] Step[2150/2713]: training loss : 0.9415381300449371 TRAIN  loss dict:  {'classification_loss': 0.9415381300449371}
2025-01-13 06:39:59,365 [INFO] Step[2200/2713]: training loss : 0.9387216567993164 TRAIN  loss dict:  {'classification_loss': 0.9387216567993164}
2025-01-13 06:40:11,303 [INFO] Step[2250/2713]: training loss : 0.9385804188251495 TRAIN  loss dict:  {'classification_loss': 0.9385804188251495}
2025-01-13 06:40:23,142 [INFO] Step[2300/2713]: training loss : 0.9392148327827453 TRAIN  loss dict:  {'classification_loss': 0.9392148327827453}
2025-01-13 06:40:35,096 [INFO] Step[2350/2713]: training loss : 0.9404105198383331 TRAIN  loss dict:  {'classification_loss': 0.9404105198383331}
2025-01-13 06:40:46,990 [INFO] Step[2400/2713]: training loss : 0.9408241951465607 TRAIN  loss dict:  {'classification_loss': 0.9408241951465607}
2025-01-13 06:40:58,900 [INFO] Step[2450/2713]: training loss : 0.9380746340751648 TRAIN  loss dict:  {'classification_loss': 0.9380746340751648}
2025-01-13 06:41:10,793 [INFO] Step[2500/2713]: training loss : 0.9403203356266022 TRAIN  loss dict:  {'classification_loss': 0.9403203356266022}
2025-01-13 06:41:22,724 [INFO] Step[2550/2713]: training loss : 0.9380018925666809 TRAIN  loss dict:  {'classification_loss': 0.9380018925666809}
2025-01-13 06:41:34,657 [INFO] Step[2600/2713]: training loss : 0.9401542913913726 TRAIN  loss dict:  {'classification_loss': 0.9401542913913726}
2025-01-13 06:41:46,608 [INFO] Step[2650/2713]: training loss : 0.9442181289196014 TRAIN  loss dict:  {'classification_loss': 0.9442181289196014}
2025-01-13 06:41:58,526 [INFO] Step[2700/2713]: training loss : 0.937244610786438 TRAIN  loss dict:  {'classification_loss': 0.937244610786438}
2025-01-13 06:43:25,444 [INFO] Label accuracies statistics:
2025-01-13 06:43:25,444 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 1.0, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.5, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.25, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 1.0, 205: 0.75, 206: 1.0, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 1.0, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.5, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 06:43:25,446 [INFO] [76] TRAIN  loss: 0.9397543706535371 acc: 1.0
2025-01-13 06:43:25,446 [INFO] [76] TRAIN  loss dict: {'classification_loss': 0.9397543706535371}
2025-01-13 06:43:25,446 [INFO] [76] VALIDATION loss: 1.7162092062539624 VALIDATION acc: 0.8257053291536051
2025-01-13 06:43:25,446 [INFO] [76] VALIDATION loss dict: {'classification_loss': 1.7162092062539624}
2025-01-13 06:43:25,446 [INFO] 
2025-01-13 06:43:42,678 [INFO] Step[50/2713]: training loss : 0.9405358147621155 TRAIN  loss dict:  {'classification_loss': 0.9405358147621155}
2025-01-13 06:43:54,594 [INFO] Step[100/2713]: training loss : 0.9383632695674896 TRAIN  loss dict:  {'classification_loss': 0.9383632695674896}
2025-01-13 06:44:06,581 [INFO] Step[150/2713]: training loss : 0.9404011750221253 TRAIN  loss dict:  {'classification_loss': 0.9404011750221253}
2025-01-13 06:44:18,501 [INFO] Step[200/2713]: training loss : 0.9394039368629455 TRAIN  loss dict:  {'classification_loss': 0.9394039368629455}
2025-01-13 06:44:30,467 [INFO] Step[250/2713]: training loss : 0.9451138281822205 TRAIN  loss dict:  {'classification_loss': 0.9451138281822205}
2025-01-13 06:44:42,366 [INFO] Step[300/2713]: training loss : 0.940104238986969 TRAIN  loss dict:  {'classification_loss': 0.940104238986969}
2025-01-13 06:44:54,269 [INFO] Step[350/2713]: training loss : 0.9381859862804413 TRAIN  loss dict:  {'classification_loss': 0.9381859862804413}
2025-01-13 06:45:06,228 [INFO] Step[400/2713]: training loss : 0.9376598298549652 TRAIN  loss dict:  {'classification_loss': 0.9376598298549652}
2025-01-13 06:45:18,147 [INFO] Step[450/2713]: training loss : 0.9396424043178558 TRAIN  loss dict:  {'classification_loss': 0.9396424043178558}
2025-01-13 06:45:30,058 [INFO] Step[500/2713]: training loss : 0.9390137577056885 TRAIN  loss dict:  {'classification_loss': 0.9390137577056885}
2025-01-13 06:45:41,975 [INFO] Step[550/2713]: training loss : 0.9428982102870941 TRAIN  loss dict:  {'classification_loss': 0.9428982102870941}
2025-01-13 06:45:53,889 [INFO] Step[600/2713]: training loss : 0.944316977262497 TRAIN  loss dict:  {'classification_loss': 0.944316977262497}
2025-01-13 06:46:05,819 [INFO] Step[650/2713]: training loss : 0.9384960269927979 TRAIN  loss dict:  {'classification_loss': 0.9384960269927979}
2025-01-13 06:46:17,717 [INFO] Step[700/2713]: training loss : 0.9389583551883698 TRAIN  loss dict:  {'classification_loss': 0.9389583551883698}
2025-01-13 06:46:29,639 [INFO] Step[750/2713]: training loss : 0.9375469851493835 TRAIN  loss dict:  {'classification_loss': 0.9375469851493835}
2025-01-13 06:46:41,525 [INFO] Step[800/2713]: training loss : 0.9386344969272613 TRAIN  loss dict:  {'classification_loss': 0.9386344969272613}
2025-01-13 06:46:53,470 [INFO] Step[850/2713]: training loss : 0.937714867591858 TRAIN  loss dict:  {'classification_loss': 0.937714867591858}
2025-01-13 06:47:05,356 [INFO] Step[900/2713]: training loss : 0.9461053597927094 TRAIN  loss dict:  {'classification_loss': 0.9461053597927094}
2025-01-13 06:47:17,261 [INFO] Step[950/2713]: training loss : 0.9375849545001984 TRAIN  loss dict:  {'classification_loss': 0.9375849545001984}
2025-01-13 06:47:29,199 [INFO] Step[1000/2713]: training loss : 0.9379117929935455 TRAIN  loss dict:  {'classification_loss': 0.9379117929935455}
2025-01-13 06:47:41,144 [INFO] Step[1050/2713]: training loss : 0.9408608698844909 TRAIN  loss dict:  {'classification_loss': 0.9408608698844909}
2025-01-13 06:47:53,053 [INFO] Step[1100/2713]: training loss : 0.9374442553520203 TRAIN  loss dict:  {'classification_loss': 0.9374442553520203}
2025-01-13 06:48:04,994 [INFO] Step[1150/2713]: training loss : 0.9416489732265473 TRAIN  loss dict:  {'classification_loss': 0.9416489732265473}
2025-01-13 06:48:16,928 [INFO] Step[1200/2713]: training loss : 0.9493235266208648 TRAIN  loss dict:  {'classification_loss': 0.9493235266208648}
2025-01-13 06:48:28,890 [INFO] Step[1250/2713]: training loss : 0.9376136410236359 TRAIN  loss dict:  {'classification_loss': 0.9376136410236359}
2025-01-13 06:48:40,826 [INFO] Step[1300/2713]: training loss : 0.9374307620525361 TRAIN  loss dict:  {'classification_loss': 0.9374307620525361}
2025-01-13 06:48:52,708 [INFO] Step[1350/2713]: training loss : 0.9386810827255249 TRAIN  loss dict:  {'classification_loss': 0.9386810827255249}
2025-01-13 06:49:04,643 [INFO] Step[1400/2713]: training loss : 0.9385631084442139 TRAIN  loss dict:  {'classification_loss': 0.9385631084442139}
2025-01-13 06:49:16,567 [INFO] Step[1450/2713]: training loss : 0.9379961812496185 TRAIN  loss dict:  {'classification_loss': 0.9379961812496185}
2025-01-13 06:49:28,460 [INFO] Step[1500/2713]: training loss : 0.9409574890136718 TRAIN  loss dict:  {'classification_loss': 0.9409574890136718}
2025-01-13 06:49:40,348 [INFO] Step[1550/2713]: training loss : 0.9380881059169769 TRAIN  loss dict:  {'classification_loss': 0.9380881059169769}
2025-01-13 06:49:52,264 [INFO] Step[1600/2713]: training loss : 0.9395380747318268 TRAIN  loss dict:  {'classification_loss': 0.9395380747318268}
2025-01-13 06:50:04,208 [INFO] Step[1650/2713]: training loss : 0.9399406576156616 TRAIN  loss dict:  {'classification_loss': 0.9399406576156616}
2025-01-13 06:50:16,127 [INFO] Step[1700/2713]: training loss : 0.9395449268817901 TRAIN  loss dict:  {'classification_loss': 0.9395449268817901}
2025-01-13 06:50:28,058 [INFO] Step[1750/2713]: training loss : 0.9480579829216004 TRAIN  loss dict:  {'classification_loss': 0.9480579829216004}
2025-01-13 06:50:39,959 [INFO] Step[1800/2713]: training loss : 0.9409656631946564 TRAIN  loss dict:  {'classification_loss': 0.9409656631946564}
2025-01-13 06:50:51,881 [INFO] Step[1850/2713]: training loss : 0.9367671227455139 TRAIN  loss dict:  {'classification_loss': 0.9367671227455139}
2025-01-13 06:51:03,795 [INFO] Step[1900/2713]: training loss : 0.9373277592658996 TRAIN  loss dict:  {'classification_loss': 0.9373277592658996}
2025-01-13 06:51:15,723 [INFO] Step[1950/2713]: training loss : 0.937917867898941 TRAIN  loss dict:  {'classification_loss': 0.937917867898941}
2025-01-13 06:51:27,641 [INFO] Step[2000/2713]: training loss : 0.9386473655700683 TRAIN  loss dict:  {'classification_loss': 0.9386473655700683}
2025-01-13 06:51:39,545 [INFO] Step[2050/2713]: training loss : 0.9402604615688324 TRAIN  loss dict:  {'classification_loss': 0.9402604615688324}
2025-01-13 06:51:51,471 [INFO] Step[2100/2713]: training loss : 0.9371465241909027 TRAIN  loss dict:  {'classification_loss': 0.9371465241909027}
2025-01-13 06:52:03,386 [INFO] Step[2150/2713]: training loss : 0.9391302597522736 TRAIN  loss dict:  {'classification_loss': 0.9391302597522736}
2025-01-13 06:52:15,296 [INFO] Step[2200/2713]: training loss : 0.9388864243030548 TRAIN  loss dict:  {'classification_loss': 0.9388864243030548}
2025-01-13 06:52:27,212 [INFO] Step[2250/2713]: training loss : 0.9426776587963104 TRAIN  loss dict:  {'classification_loss': 0.9426776587963104}
2025-01-13 06:52:39,171 [INFO] Step[2300/2713]: training loss : 0.9389626324176789 TRAIN  loss dict:  {'classification_loss': 0.9389626324176789}
2025-01-13 06:52:51,102 [INFO] Step[2350/2713]: training loss : 0.937033531665802 TRAIN  loss dict:  {'classification_loss': 0.937033531665802}
2025-01-13 06:53:03,030 [INFO] Step[2400/2713]: training loss : 0.9388902914524079 TRAIN  loss dict:  {'classification_loss': 0.9388902914524079}
2025-01-13 06:53:14,999 [INFO] Step[2450/2713]: training loss : 0.940497281551361 TRAIN  loss dict:  {'classification_loss': 0.940497281551361}
2025-01-13 06:53:26,950 [INFO] Step[2500/2713]: training loss : 0.9396362280845643 TRAIN  loss dict:  {'classification_loss': 0.9396362280845643}
2025-01-13 06:53:38,921 [INFO] Step[2550/2713]: training loss : 0.9364985466003418 TRAIN  loss dict:  {'classification_loss': 0.9364985466003418}
2025-01-13 06:53:50,807 [INFO] Step[2600/2713]: training loss : 0.9407159173488617 TRAIN  loss dict:  {'classification_loss': 0.9407159173488617}
2025-01-13 06:54:02,753 [INFO] Step[2650/2713]: training loss : 0.9400951373577118 TRAIN  loss dict:  {'classification_loss': 0.9400951373577118}
2025-01-13 06:54:14,648 [INFO] Step[2700/2713]: training loss : 0.9406991815567016 TRAIN  loss dict:  {'classification_loss': 0.9406991815567016}
2025-01-13 06:55:41,662 [INFO] Label accuracies statistics:
2025-01-13 06:55:41,663 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.5, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.5, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.5, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 1.0, 205: 1.0, 206: 0.5, 207: 0.5, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.5, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.5, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 06:55:41,665 [INFO] [77] TRAIN  loss: 0.9397654765169866 acc: 0.9997542695662858
2025-01-13 06:55:41,665 [INFO] [77] TRAIN  loss dict: {'classification_loss': 0.9397654765169866}
2025-01-13 06:55:41,665 [INFO] [77] VALIDATION loss: 1.7249361622826498 VALIDATION acc: 0.8275862068965517
2025-01-13 06:55:41,665 [INFO] [77] VALIDATION loss dict: {'classification_loss': 1.7249361622826498}
2025-01-13 06:55:41,665 [INFO] 
2025-01-13 06:55:58,799 [INFO] Step[50/2713]: training loss : 0.9411446428298951 TRAIN  loss dict:  {'classification_loss': 0.9411446428298951}
2025-01-13 06:56:11,152 [INFO] Step[100/2713]: training loss : 0.9381364929676056 TRAIN  loss dict:  {'classification_loss': 0.9381364929676056}
2025-01-13 06:56:23,548 [INFO] Step[150/2713]: training loss : 0.939824982881546 TRAIN  loss dict:  {'classification_loss': 0.939824982881546}
2025-01-13 06:56:35,797 [INFO] Step[200/2713]: training loss : 0.9378732180595398 TRAIN  loss dict:  {'classification_loss': 0.9378732180595398}
2025-01-13 06:56:48,429 [INFO] Step[250/2713]: training loss : 0.9415929913520813 TRAIN  loss dict:  {'classification_loss': 0.9415929913520813}
2025-01-13 06:57:00,743 [INFO] Step[300/2713]: training loss : 0.9392712867259979 TRAIN  loss dict:  {'classification_loss': 0.9392712867259979}
2025-01-13 06:57:13,408 [INFO] Step[350/2713]: training loss : 0.9392455613613129 TRAIN  loss dict:  {'classification_loss': 0.9392455613613129}
2025-01-13 06:57:26,792 [INFO] Step[400/2713]: training loss : 0.9389146435260772 TRAIN  loss dict:  {'classification_loss': 0.9389146435260772}
2025-01-13 06:57:40,758 [INFO] Step[450/2713]: training loss : 0.9432708549499512 TRAIN  loss dict:  {'classification_loss': 0.9432708549499512}
2025-01-13 06:57:53,418 [INFO] Step[500/2713]: training loss : 0.9380673158168793 TRAIN  loss dict:  {'classification_loss': 0.9380673158168793}
2025-01-13 06:58:05,351 [INFO] Step[550/2713]: training loss : 0.9374219858646393 TRAIN  loss dict:  {'classification_loss': 0.9374219858646393}
2025-01-13 06:58:17,223 [INFO] Step[600/2713]: training loss : 0.9394325995445252 TRAIN  loss dict:  {'classification_loss': 0.9394325995445252}
2025-01-13 06:58:29,086 [INFO] Step[650/2713]: training loss : 0.9385362398624421 TRAIN  loss dict:  {'classification_loss': 0.9385362398624421}
2025-01-13 06:58:40,961 [INFO] Step[700/2713]: training loss : 0.940593296289444 TRAIN  loss dict:  {'classification_loss': 0.940593296289444}
2025-01-13 06:58:52,886 [INFO] Step[750/2713]: training loss : 0.9376268231868744 TRAIN  loss dict:  {'classification_loss': 0.9376268231868744}
2025-01-13 06:59:04,730 [INFO] Step[800/2713]: training loss : 0.9385435450077056 TRAIN  loss dict:  {'classification_loss': 0.9385435450077056}
2025-01-13 06:59:16,636 [INFO] Step[850/2713]: training loss : 0.9380822479724884 TRAIN  loss dict:  {'classification_loss': 0.9380822479724884}
2025-01-13 06:59:28,542 [INFO] Step[900/2713]: training loss : 0.9407541584968567 TRAIN  loss dict:  {'classification_loss': 0.9407541584968567}
2025-01-13 06:59:40,431 [INFO] Step[950/2713]: training loss : 0.9390271103382111 TRAIN  loss dict:  {'classification_loss': 0.9390271103382111}
2025-01-13 06:59:52,382 [INFO] Step[1000/2713]: training loss : 0.940964766740799 TRAIN  loss dict:  {'classification_loss': 0.940964766740799}
2025-01-13 07:00:04,308 [INFO] Step[1050/2713]: training loss : 0.9385079634189606 TRAIN  loss dict:  {'classification_loss': 0.9385079634189606}
2025-01-13 07:00:16,280 [INFO] Step[1100/2713]: training loss : 0.9366897737979889 TRAIN  loss dict:  {'classification_loss': 0.9366897737979889}
2025-01-13 07:00:28,169 [INFO] Step[1150/2713]: training loss : 0.9388786268234253 TRAIN  loss dict:  {'classification_loss': 0.9388786268234253}
2025-01-13 07:00:40,069 [INFO] Step[1200/2713]: training loss : 0.9464544355869293 TRAIN  loss dict:  {'classification_loss': 0.9464544355869293}
2025-01-13 07:00:52,007 [INFO] Step[1250/2713]: training loss : 0.939638694524765 TRAIN  loss dict:  {'classification_loss': 0.939638694524765}
2025-01-13 07:01:03,887 [INFO] Step[1300/2713]: training loss : 0.9401122140884399 TRAIN  loss dict:  {'classification_loss': 0.9401122140884399}
2025-01-13 07:01:15,824 [INFO] Step[1350/2713]: training loss : 0.9404054737091064 TRAIN  loss dict:  {'classification_loss': 0.9404054737091064}
2025-01-13 07:01:27,703 [INFO] Step[1400/2713]: training loss : 0.939498542547226 TRAIN  loss dict:  {'classification_loss': 0.939498542547226}
2025-01-13 07:01:39,609 [INFO] Step[1450/2713]: training loss : 0.9389989340305328 TRAIN  loss dict:  {'classification_loss': 0.9389989340305328}
2025-01-13 07:01:51,535 [INFO] Step[1500/2713]: training loss : 0.9392486989498139 TRAIN  loss dict:  {'classification_loss': 0.9392486989498139}
2025-01-13 07:02:03,415 [INFO] Step[1550/2713]: training loss : 0.9413206863403321 TRAIN  loss dict:  {'classification_loss': 0.9413206863403321}
2025-01-13 07:02:15,337 [INFO] Step[1600/2713]: training loss : 0.938081088066101 TRAIN  loss dict:  {'classification_loss': 0.938081088066101}
2025-01-13 07:02:27,213 [INFO] Step[1650/2713]: training loss : 0.9383103334903717 TRAIN  loss dict:  {'classification_loss': 0.9383103334903717}
2025-01-13 07:02:39,137 [INFO] Step[1700/2713]: training loss : 0.9392608165740967 TRAIN  loss dict:  {'classification_loss': 0.9392608165740967}
2025-01-13 07:02:51,016 [INFO] Step[1750/2713]: training loss : 0.9412942254543304 TRAIN  loss dict:  {'classification_loss': 0.9412942254543304}
2025-01-13 07:03:02,909 [INFO] Step[1800/2713]: training loss : 0.9382289111614227 TRAIN  loss dict:  {'classification_loss': 0.9382289111614227}
2025-01-13 07:03:14,851 [INFO] Step[1850/2713]: training loss : 0.9389823174476624 TRAIN  loss dict:  {'classification_loss': 0.9389823174476624}
2025-01-13 07:03:26,786 [INFO] Step[1900/2713]: training loss : 0.9412892508506775 TRAIN  loss dict:  {'classification_loss': 0.9412892508506775}
2025-01-13 07:03:38,715 [INFO] Step[1950/2713]: training loss : 0.9394394659996033 TRAIN  loss dict:  {'classification_loss': 0.9394394659996033}
2025-01-13 07:03:50,599 [INFO] Step[2000/2713]: training loss : 0.9409687578678131 TRAIN  loss dict:  {'classification_loss': 0.9409687578678131}
2025-01-13 07:04:02,526 [INFO] Step[2050/2713]: training loss : 0.9417358219623566 TRAIN  loss dict:  {'classification_loss': 0.9417358219623566}
2025-01-13 07:04:14,446 [INFO] Step[2100/2713]: training loss : 0.9400620627403259 TRAIN  loss dict:  {'classification_loss': 0.9400620627403259}
2025-01-13 07:04:26,369 [INFO] Step[2150/2713]: training loss : 0.9397793126106262 TRAIN  loss dict:  {'classification_loss': 0.9397793126106262}
2025-01-13 07:04:38,245 [INFO] Step[2200/2713]: training loss : 0.9385815668106079 TRAIN  loss dict:  {'classification_loss': 0.9385815668106079}
2025-01-13 07:04:50,185 [INFO] Step[2250/2713]: training loss : 0.9421879398822784 TRAIN  loss dict:  {'classification_loss': 0.9421879398822784}
2025-01-13 07:05:02,111 [INFO] Step[2300/2713]: training loss : 0.9403003525733947 TRAIN  loss dict:  {'classification_loss': 0.9403003525733947}
2025-01-13 07:05:14,057 [INFO] Step[2350/2713]: training loss : 0.9399529004096985 TRAIN  loss dict:  {'classification_loss': 0.9399529004096985}
2025-01-13 07:05:25,932 [INFO] Step[2400/2713]: training loss : 0.9398688352108002 TRAIN  loss dict:  {'classification_loss': 0.9398688352108002}
2025-01-13 07:05:37,874 [INFO] Step[2450/2713]: training loss : 0.937952036857605 TRAIN  loss dict:  {'classification_loss': 0.937952036857605}
2025-01-13 07:05:49,785 [INFO] Step[2500/2713]: training loss : 0.9405290460586548 TRAIN  loss dict:  {'classification_loss': 0.9405290460586548}
2025-01-13 07:06:01,678 [INFO] Step[2550/2713]: training loss : 0.9380193042755127 TRAIN  loss dict:  {'classification_loss': 0.9380193042755127}
2025-01-13 07:06:13,676 [INFO] Step[2600/2713]: training loss : 0.9388438105583191 TRAIN  loss dict:  {'classification_loss': 0.9388438105583191}
2025-01-13 07:06:25,609 [INFO] Step[2650/2713]: training loss : 0.9394120514392853 TRAIN  loss dict:  {'classification_loss': 0.9394120514392853}
2025-01-13 07:06:37,529 [INFO] Step[2700/2713]: training loss : 0.938275887966156 TRAIN  loss dict:  {'classification_loss': 0.938275887966156}
2025-01-13 07:08:04,036 [INFO] Label accuracies statistics:
2025-01-13 07:08:04,036 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 1.0, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.25, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.75, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 1.0, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 07:08:04,038 [INFO] [78] TRAIN  loss: 0.9396272701308422 acc: 1.0
2025-01-13 07:08:04,038 [INFO] [78] TRAIN  loss dict: {'classification_loss': 0.9396272701308422}
2025-01-13 07:08:04,038 [INFO] [78] VALIDATION loss: 1.6971770752417414 VALIDATION acc: 0.8307210031347962
2025-01-13 07:08:04,038 [INFO] [78] VALIDATION loss dict: {'classification_loss': 1.6971770752417414}
2025-01-13 07:08:04,038 [INFO] 
2025-01-13 07:08:21,654 [INFO] Step[50/2713]: training loss : 0.9390928244590759 TRAIN  loss dict:  {'classification_loss': 0.9390928244590759}
2025-01-13 07:08:33,546 [INFO] Step[100/2713]: training loss : 0.9409874367713928 TRAIN  loss dict:  {'classification_loss': 0.9409874367713928}
2025-01-13 07:08:45,510 [INFO] Step[150/2713]: training loss : 0.9397934544086456 TRAIN  loss dict:  {'classification_loss': 0.9397934544086456}
2025-01-13 07:08:57,424 [INFO] Step[200/2713]: training loss : 0.9386499118804932 TRAIN  loss dict:  {'classification_loss': 0.9386499118804932}
2025-01-13 07:09:09,381 [INFO] Step[250/2713]: training loss : 0.9400724804401398 TRAIN  loss dict:  {'classification_loss': 0.9400724804401398}
2025-01-13 07:09:21,265 [INFO] Step[300/2713]: training loss : 0.9389790058135986 TRAIN  loss dict:  {'classification_loss': 0.9389790058135986}
2025-01-13 07:09:33,240 [INFO] Step[350/2713]: training loss : 0.9384266340732574 TRAIN  loss dict:  {'classification_loss': 0.9384266340732574}
2025-01-13 07:09:45,176 [INFO] Step[400/2713]: training loss : 0.937570971250534 TRAIN  loss dict:  {'classification_loss': 0.937570971250534}
2025-01-13 07:09:57,107 [INFO] Step[450/2713]: training loss : 0.9381332647800446 TRAIN  loss dict:  {'classification_loss': 0.9381332647800446}
2025-01-13 07:10:08,988 [INFO] Step[500/2713]: training loss : 0.9376854586601258 TRAIN  loss dict:  {'classification_loss': 0.9376854586601258}
2025-01-13 07:10:20,870 [INFO] Step[550/2713]: training loss : 0.939851781129837 TRAIN  loss dict:  {'classification_loss': 0.939851781129837}
2025-01-13 07:10:32,768 [INFO] Step[600/2713]: training loss : 0.9436107516288758 TRAIN  loss dict:  {'classification_loss': 0.9436107516288758}
2025-01-13 07:10:44,691 [INFO] Step[650/2713]: training loss : 0.9403107333183288 TRAIN  loss dict:  {'classification_loss': 0.9403107333183288}
2025-01-13 07:10:56,642 [INFO] Step[700/2713]: training loss : 0.9400291264057159 TRAIN  loss dict:  {'classification_loss': 0.9400291264057159}
2025-01-13 07:11:08,555 [INFO] Step[750/2713]: training loss : 0.9384266054630279 TRAIN  loss dict:  {'classification_loss': 0.9384266054630279}
2025-01-13 07:11:20,493 [INFO] Step[800/2713]: training loss : 0.939071558713913 TRAIN  loss dict:  {'classification_loss': 0.939071558713913}
2025-01-13 07:11:32,402 [INFO] Step[850/2713]: training loss : 0.9378389346599579 TRAIN  loss dict:  {'classification_loss': 0.9378389346599579}
2025-01-13 07:11:44,283 [INFO] Step[900/2713]: training loss : 0.937689037322998 TRAIN  loss dict:  {'classification_loss': 0.937689037322998}
2025-01-13 07:11:56,197 [INFO] Step[950/2713]: training loss : 0.9366040003299713 TRAIN  loss dict:  {'classification_loss': 0.9366040003299713}
2025-01-13 07:12:08,076 [INFO] Step[1000/2713]: training loss : 0.9388881659507752 TRAIN  loss dict:  {'classification_loss': 0.9388881659507752}
2025-01-13 07:12:19,999 [INFO] Step[1050/2713]: training loss : 0.9388250255584717 TRAIN  loss dict:  {'classification_loss': 0.9388250255584717}
2025-01-13 07:12:31,923 [INFO] Step[1100/2713]: training loss : 0.9385106372833252 TRAIN  loss dict:  {'classification_loss': 0.9385106372833252}
2025-01-13 07:12:43,820 [INFO] Step[1150/2713]: training loss : 0.9377806222438813 TRAIN  loss dict:  {'classification_loss': 0.9377806222438813}
2025-01-13 07:12:55,679 [INFO] Step[1200/2713]: training loss : 0.9388236021995544 TRAIN  loss dict:  {'classification_loss': 0.9388236021995544}
2025-01-13 07:13:07,585 [INFO] Step[1250/2713]: training loss : 0.9395096004009247 TRAIN  loss dict:  {'classification_loss': 0.9395096004009247}
2025-01-13 07:13:19,493 [INFO] Step[1300/2713]: training loss : 0.9369295024871827 TRAIN  loss dict:  {'classification_loss': 0.9369295024871827}
2025-01-13 07:13:31,375 [INFO] Step[1350/2713]: training loss : 0.9388230168819427 TRAIN  loss dict:  {'classification_loss': 0.9388230168819427}
2025-01-13 07:13:43,254 [INFO] Step[1400/2713]: training loss : 0.9389789855480194 TRAIN  loss dict:  {'classification_loss': 0.9389789855480194}
2025-01-13 07:13:55,205 [INFO] Step[1450/2713]: training loss : 0.9395660269260406 TRAIN  loss dict:  {'classification_loss': 0.9395660269260406}
2025-01-13 07:14:07,107 [INFO] Step[1500/2713]: training loss : 0.9366947138309478 TRAIN  loss dict:  {'classification_loss': 0.9366947138309478}
2025-01-13 07:14:19,014 [INFO] Step[1550/2713]: training loss : 0.941664868593216 TRAIN  loss dict:  {'classification_loss': 0.941664868593216}
2025-01-13 07:14:30,937 [INFO] Step[1600/2713]: training loss : 0.9406242537498474 TRAIN  loss dict:  {'classification_loss': 0.9406242537498474}
2025-01-13 07:14:42,862 [INFO] Step[1650/2713]: training loss : 0.942258198261261 TRAIN  loss dict:  {'classification_loss': 0.942258198261261}
2025-01-13 07:14:54,997 [INFO] Step[1700/2713]: training loss : 0.9375534462928772 TRAIN  loss dict:  {'classification_loss': 0.9375534462928772}
2025-01-13 07:15:07,394 [INFO] Step[1750/2713]: training loss : 0.940153284072876 TRAIN  loss dict:  {'classification_loss': 0.940153284072876}
2025-01-13 07:15:19,781 [INFO] Step[1800/2713]: training loss : 0.9401286447048187 TRAIN  loss dict:  {'classification_loss': 0.9401286447048187}
2025-01-13 07:15:32,084 [INFO] Step[1850/2713]: training loss : 0.9377425968647003 TRAIN  loss dict:  {'classification_loss': 0.9377425968647003}
2025-01-13 07:15:44,724 [INFO] Step[1900/2713]: training loss : 0.9372014927864075 TRAIN  loss dict:  {'classification_loss': 0.9372014927864075}
2025-01-13 07:15:57,022 [INFO] Step[1950/2713]: training loss : 0.9385475265979767 TRAIN  loss dict:  {'classification_loss': 0.9385475265979767}
2025-01-13 07:16:09,793 [INFO] Step[2000/2713]: training loss : 0.9365558052062988 TRAIN  loss dict:  {'classification_loss': 0.9365558052062988}
2025-01-13 07:16:23,446 [INFO] Step[2050/2713]: training loss : 0.9403474605083466 TRAIN  loss dict:  {'classification_loss': 0.9403474605083466}
2025-01-13 07:16:36,766 [INFO] Step[2100/2713]: training loss : 0.9404142606258392 TRAIN  loss dict:  {'classification_loss': 0.9404142606258392}
2025-01-13 07:16:48,904 [INFO] Step[2150/2713]: training loss : 0.941417680978775 TRAIN  loss dict:  {'classification_loss': 0.941417680978775}
2025-01-13 07:17:00,744 [INFO] Step[2200/2713]: training loss : 0.9432071208953857 TRAIN  loss dict:  {'classification_loss': 0.9432071208953857}
2025-01-13 07:17:12,639 [INFO] Step[2250/2713]: training loss : 0.9404411089420318 TRAIN  loss dict:  {'classification_loss': 0.9404411089420318}
2025-01-13 07:17:24,490 [INFO] Step[2300/2713]: training loss : 0.9383033335208892 TRAIN  loss dict:  {'classification_loss': 0.9383033335208892}
2025-01-13 07:17:36,398 [INFO] Step[2350/2713]: training loss : 0.9395802009105683 TRAIN  loss dict:  {'classification_loss': 0.9395802009105683}
2025-01-13 07:17:48,291 [INFO] Step[2400/2713]: training loss : 0.9403165304660797 TRAIN  loss dict:  {'classification_loss': 0.9403165304660797}
2025-01-13 07:18:00,159 [INFO] Step[2450/2713]: training loss : 0.9376386964321136 TRAIN  loss dict:  {'classification_loss': 0.9376386964321136}
2025-01-13 07:18:12,022 [INFO] Step[2500/2713]: training loss : 0.9497056734561921 TRAIN  loss dict:  {'classification_loss': 0.9497056734561921}
2025-01-13 07:18:23,944 [INFO] Step[2550/2713]: training loss : 0.9371561944484711 TRAIN  loss dict:  {'classification_loss': 0.9371561944484711}
2025-01-13 07:18:35,862 [INFO] Step[2600/2713]: training loss : 0.9403949236869812 TRAIN  loss dict:  {'classification_loss': 0.9403949236869812}
2025-01-13 07:18:47,762 [INFO] Step[2650/2713]: training loss : 0.9398457205295563 TRAIN  loss dict:  {'classification_loss': 0.9398457205295563}
2025-01-13 07:18:59,657 [INFO] Step[2700/2713]: training loss : 0.9384309220314025 TRAIN  loss dict:  {'classification_loss': 0.9384309220314025}
2025-01-13 07:20:26,188 [INFO] Label accuracies statistics:
2025-01-13 07:20:26,188 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 0.75, 206: 0.0, 207: 0.75, 208: 0.75, 209: 0.75, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.25, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 1.0, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 07:20:26,190 [INFO] [79] TRAIN  loss: 0.9393792334096791 acc: 0.9998771347831429
2025-01-13 07:20:26,190 [INFO] [79] TRAIN  loss dict: {'classification_loss': 0.9393792334096791}
2025-01-13 07:20:26,191 [INFO] [79] VALIDATION loss: 1.7011713718337225 VALIDATION acc: 0.8294670846394985
2025-01-13 07:20:26,191 [INFO] [79] VALIDATION loss dict: {'classification_loss': 1.7011713718337225}
2025-01-13 07:20:26,191 [INFO] 
2025-01-13 07:20:43,771 [INFO] Step[50/2713]: training loss : 0.9426151883602142 TRAIN  loss dict:  {'classification_loss': 0.9426151883602142}
2025-01-13 07:20:55,688 [INFO] Step[100/2713]: training loss : 0.9393898129463196 TRAIN  loss dict:  {'classification_loss': 0.9393898129463196}
2025-01-13 07:21:07,651 [INFO] Step[150/2713]: training loss : 0.9373882329463958 TRAIN  loss dict:  {'classification_loss': 0.9373882329463958}
2025-01-13 07:21:19,543 [INFO] Step[200/2713]: training loss : 0.9401584374904632 TRAIN  loss dict:  {'classification_loss': 0.9401584374904632}
2025-01-13 07:21:31,464 [INFO] Step[250/2713]: training loss : 0.9367101120948792 TRAIN  loss dict:  {'classification_loss': 0.9367101120948792}
2025-01-13 07:21:43,363 [INFO] Step[300/2713]: training loss : 0.9364377522468567 TRAIN  loss dict:  {'classification_loss': 0.9364377522468567}
2025-01-13 07:21:55,309 [INFO] Step[350/2713]: training loss : 0.9405766224861145 TRAIN  loss dict:  {'classification_loss': 0.9405766224861145}
2025-01-13 07:22:07,134 [INFO] Step[400/2713]: training loss : 0.9375989246368408 TRAIN  loss dict:  {'classification_loss': 0.9375989246368408}
2025-01-13 07:22:19,037 [INFO] Step[450/2713]: training loss : 0.9385104095935821 TRAIN  loss dict:  {'classification_loss': 0.9385104095935821}
2025-01-13 07:22:30,937 [INFO] Step[500/2713]: training loss : 0.9398245453834534 TRAIN  loss dict:  {'classification_loss': 0.9398245453834534}
2025-01-13 07:22:42,835 [INFO] Step[550/2713]: training loss : 0.93874014377594 TRAIN  loss dict:  {'classification_loss': 0.93874014377594}
2025-01-13 07:22:54,763 [INFO] Step[600/2713]: training loss : 0.9381567013263702 TRAIN  loss dict:  {'classification_loss': 0.9381567013263702}
2025-01-13 07:23:06,694 [INFO] Step[650/2713]: training loss : 0.9388705563545227 TRAIN  loss dict:  {'classification_loss': 0.9388705563545227}
2025-01-13 07:23:18,593 [INFO] Step[700/2713]: training loss : 0.9400311160087585 TRAIN  loss dict:  {'classification_loss': 0.9400311160087585}
2025-01-13 07:23:30,482 [INFO] Step[750/2713]: training loss : 0.9387136662006378 TRAIN  loss dict:  {'classification_loss': 0.9387136662006378}
2025-01-13 07:23:42,438 [INFO] Step[800/2713]: training loss : 0.9392178189754486 TRAIN  loss dict:  {'classification_loss': 0.9392178189754486}
2025-01-13 07:23:54,399 [INFO] Step[850/2713]: training loss : 0.9460995817184448 TRAIN  loss dict:  {'classification_loss': 0.9460995817184448}
2025-01-13 07:24:06,286 [INFO] Step[900/2713]: training loss : 0.9504231977462768 TRAIN  loss dict:  {'classification_loss': 0.9504231977462768}
2025-01-13 07:24:18,187 [INFO] Step[950/2713]: training loss : 0.9378862059116364 TRAIN  loss dict:  {'classification_loss': 0.9378862059116364}
2025-01-13 07:24:30,073 [INFO] Step[1000/2713]: training loss : 0.9367484140396118 TRAIN  loss dict:  {'classification_loss': 0.9367484140396118}
2025-01-13 07:24:41,972 [INFO] Step[1050/2713]: training loss : 0.9376832544803619 TRAIN  loss dict:  {'classification_loss': 0.9376832544803619}
2025-01-13 07:24:53,857 [INFO] Step[1100/2713]: training loss : 0.947912095785141 TRAIN  loss dict:  {'classification_loss': 0.947912095785141}
2025-01-13 07:25:05,761 [INFO] Step[1150/2713]: training loss : 0.9399732434749604 TRAIN  loss dict:  {'classification_loss': 0.9399732434749604}
2025-01-13 07:25:17,650 [INFO] Step[1200/2713]: training loss : 0.9371155881881714 TRAIN  loss dict:  {'classification_loss': 0.9371155881881714}
2025-01-13 07:25:29,546 [INFO] Step[1250/2713]: training loss : 0.9391158521175385 TRAIN  loss dict:  {'classification_loss': 0.9391158521175385}
2025-01-13 07:25:41,463 [INFO] Step[1300/2713]: training loss : 0.9369045507907867 TRAIN  loss dict:  {'classification_loss': 0.9369045507907867}
2025-01-13 07:25:53,344 [INFO] Step[1350/2713]: training loss : 0.9407748198509216 TRAIN  loss dict:  {'classification_loss': 0.9407748198509216}
2025-01-13 07:26:05,249 [INFO] Step[1400/2713]: training loss : 0.9384885621070862 TRAIN  loss dict:  {'classification_loss': 0.9384885621070862}
2025-01-13 07:26:17,164 [INFO] Step[1450/2713]: training loss : 0.9362460243701934 TRAIN  loss dict:  {'classification_loss': 0.9362460243701934}
2025-01-13 07:26:29,052 [INFO] Step[1500/2713]: training loss : 0.9395453095436096 TRAIN  loss dict:  {'classification_loss': 0.9395453095436096}
2025-01-13 07:26:40,979 [INFO] Step[1550/2713]: training loss : 0.9397320926189423 TRAIN  loss dict:  {'classification_loss': 0.9397320926189423}
2025-01-13 07:26:52,902 [INFO] Step[1600/2713]: training loss : 0.9377121400833129 TRAIN  loss dict:  {'classification_loss': 0.9377121400833129}
2025-01-13 07:27:04,849 [INFO] Step[1650/2713]: training loss : 0.9372370100021362 TRAIN  loss dict:  {'classification_loss': 0.9372370100021362}
2025-01-13 07:27:16,748 [INFO] Step[1700/2713]: training loss : 0.9427395057678223 TRAIN  loss dict:  {'classification_loss': 0.9427395057678223}
2025-01-13 07:27:28,693 [INFO] Step[1750/2713]: training loss : 0.9389622795581818 TRAIN  loss dict:  {'classification_loss': 0.9389622795581818}
2025-01-13 07:27:40,607 [INFO] Step[1800/2713]: training loss : 0.9390046215057373 TRAIN  loss dict:  {'classification_loss': 0.9390046215057373}
2025-01-13 07:27:52,482 [INFO] Step[1850/2713]: training loss : 0.936487991809845 TRAIN  loss dict:  {'classification_loss': 0.936487991809845}
2025-01-13 07:28:04,404 [INFO] Step[1900/2713]: training loss : 0.9396690773963928 TRAIN  loss dict:  {'classification_loss': 0.9396690773963928}
2025-01-13 07:28:16,302 [INFO] Step[1950/2713]: training loss : 0.9384580111503601 TRAIN  loss dict:  {'classification_loss': 0.9384580111503601}
2025-01-13 07:28:28,201 [INFO] Step[2000/2713]: training loss : 0.938919085264206 TRAIN  loss dict:  {'classification_loss': 0.938919085264206}
2025-01-13 07:28:40,077 [INFO] Step[2050/2713]: training loss : 0.9430991208553314 TRAIN  loss dict:  {'classification_loss': 0.9430991208553314}
2025-01-13 07:28:52,023 [INFO] Step[2100/2713]: training loss : 0.9385204255580902 TRAIN  loss dict:  {'classification_loss': 0.9385204255580902}
2025-01-13 07:29:03,899 [INFO] Step[2150/2713]: training loss : 0.9397417378425598 TRAIN  loss dict:  {'classification_loss': 0.9397417378425598}
2025-01-13 07:29:15,816 [INFO] Step[2200/2713]: training loss : 0.9397359931468964 TRAIN  loss dict:  {'classification_loss': 0.9397359931468964}
2025-01-13 07:29:27,783 [INFO] Step[2250/2713]: training loss : 0.9433403897285462 TRAIN  loss dict:  {'classification_loss': 0.9433403897285462}
2025-01-13 07:29:39,715 [INFO] Step[2300/2713]: training loss : 0.9392583870887756 TRAIN  loss dict:  {'classification_loss': 0.9392583870887756}
2025-01-13 07:29:51,591 [INFO] Step[2350/2713]: training loss : 0.9390649497509003 TRAIN  loss dict:  {'classification_loss': 0.9390649497509003}
2025-01-13 07:30:03,451 [INFO] Step[2400/2713]: training loss : 0.9384867799282074 TRAIN  loss dict:  {'classification_loss': 0.9384867799282074}
2025-01-13 07:30:15,386 [INFO] Step[2450/2713]: training loss : 0.9395340526103974 TRAIN  loss dict:  {'classification_loss': 0.9395340526103974}
2025-01-13 07:30:27,287 [INFO] Step[2500/2713]: training loss : 0.9469635450839996 TRAIN  loss dict:  {'classification_loss': 0.9469635450839996}
2025-01-13 07:30:39,184 [INFO] Step[2550/2713]: training loss : 0.9367804193496704 TRAIN  loss dict:  {'classification_loss': 0.9367804193496704}
2025-01-13 07:30:51,095 [INFO] Step[2600/2713]: training loss : 0.9401288175582886 TRAIN  loss dict:  {'classification_loss': 0.9401288175582886}
2025-01-13 07:31:02,995 [INFO] Step[2650/2713]: training loss : 0.9381262874603271 TRAIN  loss dict:  {'classification_loss': 0.9381262874603271}
2025-01-13 07:31:14,942 [INFO] Step[2700/2713]: training loss : 0.937499840259552 TRAIN  loss dict:  {'classification_loss': 0.937499840259552}
2025-01-13 07:32:41,748 [INFO] Label accuracies statistics:
2025-01-13 07:32:41,749 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.5, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.75, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.25, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 1.0, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.5, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.25, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.75, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 07:32:41,750 [INFO] [80] TRAIN  loss: 0.9395804169254465 acc: 0.9998771347831429
2025-01-13 07:32:41,750 [INFO] [80] TRAIN  loss dict: {'classification_loss': 0.9395804169254465}
2025-01-13 07:32:41,750 [INFO] [80] VALIDATION loss: 1.717801724497537 VALIDATION acc: 0.8326018808777429
2025-01-13 07:32:41,751 [INFO] [80] VALIDATION loss dict: {'classification_loss': 1.717801724497537}
2025-01-13 07:32:41,751 [INFO] 
2025-01-13 07:32:59,482 [INFO] Step[50/2713]: training loss : 0.939234504699707 TRAIN  loss dict:  {'classification_loss': 0.939234504699707}
2025-01-13 07:33:11,378 [INFO] Step[100/2713]: training loss : 0.938569974899292 TRAIN  loss dict:  {'classification_loss': 0.938569974899292}
2025-01-13 07:33:23,316 [INFO] Step[150/2713]: training loss : 0.9370966541767121 TRAIN  loss dict:  {'classification_loss': 0.9370966541767121}
2025-01-13 07:33:35,246 [INFO] Step[200/2713]: training loss : 0.9380523157119751 TRAIN  loss dict:  {'classification_loss': 0.9380523157119751}
2025-01-13 07:33:47,207 [INFO] Step[250/2713]: training loss : 0.9378216624259949 TRAIN  loss dict:  {'classification_loss': 0.9378216624259949}
2025-01-13 07:33:59,530 [INFO] Step[300/2713]: training loss : 0.9430507695674897 TRAIN  loss dict:  {'classification_loss': 0.9430507695674897}
2025-01-13 07:34:11,888 [INFO] Step[350/2713]: training loss : 0.9387012934684753 TRAIN  loss dict:  {'classification_loss': 0.9387012934684753}
2025-01-13 07:34:24,149 [INFO] Step[400/2713]: training loss : 0.939382164478302 TRAIN  loss dict:  {'classification_loss': 0.939382164478302}
2025-01-13 07:34:36,688 [INFO] Step[450/2713]: training loss : 0.937671526670456 TRAIN  loss dict:  {'classification_loss': 0.937671526670456}
2025-01-13 07:34:49,000 [INFO] Step[500/2713]: training loss : 0.9375772988796234 TRAIN  loss dict:  {'classification_loss': 0.9375772988796234}
2025-01-13 07:35:01,502 [INFO] Step[550/2713]: training loss : 0.9387775027751922 TRAIN  loss dict:  {'classification_loss': 0.9387775027751922}
2025-01-13 07:35:14,521 [INFO] Step[600/2713]: training loss : 0.9388938736915589 TRAIN  loss dict:  {'classification_loss': 0.9388938736915589}
2025-01-13 07:35:28,105 [INFO] Step[650/2713]: training loss : 0.938221595287323 TRAIN  loss dict:  {'classification_loss': 0.938221595287323}
2025-01-13 07:35:40,957 [INFO] Step[700/2713]: training loss : 0.9394508457183838 TRAIN  loss dict:  {'classification_loss': 0.9394508457183838}
2025-01-13 07:35:52,961 [INFO] Step[750/2713]: training loss : 0.939974479675293 TRAIN  loss dict:  {'classification_loss': 0.939974479675293}
2025-01-13 07:36:04,830 [INFO] Step[800/2713]: training loss : 0.9369625878334046 TRAIN  loss dict:  {'classification_loss': 0.9369625878334046}
2025-01-13 07:36:16,696 [INFO] Step[850/2713]: training loss : 0.9372531676292419 TRAIN  loss dict:  {'classification_loss': 0.9372531676292419}
2025-01-13 07:36:28,622 [INFO] Step[900/2713]: training loss : 0.9407957506179809 TRAIN  loss dict:  {'classification_loss': 0.9407957506179809}
2025-01-13 07:36:40,532 [INFO] Step[950/2713]: training loss : 0.9384275388717651 TRAIN  loss dict:  {'classification_loss': 0.9384275388717651}
2025-01-13 07:36:52,397 [INFO] Step[1000/2713]: training loss : 0.9372738766670227 TRAIN  loss dict:  {'classification_loss': 0.9372738766670227}
2025-01-13 07:37:04,304 [INFO] Step[1050/2713]: training loss : 0.9372662842273712 TRAIN  loss dict:  {'classification_loss': 0.9372662842273712}
2025-01-13 07:37:16,156 [INFO] Step[1100/2713]: training loss : 0.9381674933433533 TRAIN  loss dict:  {'classification_loss': 0.9381674933433533}
2025-01-13 07:37:28,070 [INFO] Step[1150/2713]: training loss : 0.9390028071403503 TRAIN  loss dict:  {'classification_loss': 0.9390028071403503}
2025-01-13 07:37:39,966 [INFO] Step[1200/2713]: training loss : 0.937542415857315 TRAIN  loss dict:  {'classification_loss': 0.937542415857315}
2025-01-13 07:37:51,886 [INFO] Step[1250/2713]: training loss : 0.9375948345661164 TRAIN  loss dict:  {'classification_loss': 0.9375948345661164}
2025-01-13 07:38:03,777 [INFO] Step[1300/2713]: training loss : 0.9383953976631164 TRAIN  loss dict:  {'classification_loss': 0.9383953976631164}
2025-01-13 07:38:15,675 [INFO] Step[1350/2713]: training loss : 0.9397261655330658 TRAIN  loss dict:  {'classification_loss': 0.9397261655330658}
2025-01-13 07:38:27,572 [INFO] Step[1400/2713]: training loss : 0.9363968110084534 TRAIN  loss dict:  {'classification_loss': 0.9363968110084534}
2025-01-13 07:38:39,476 [INFO] Step[1450/2713]: training loss : 0.9367139279842377 TRAIN  loss dict:  {'classification_loss': 0.9367139279842377}
2025-01-13 07:38:51,378 [INFO] Step[1500/2713]: training loss : 0.9383117353916168 TRAIN  loss dict:  {'classification_loss': 0.9383117353916168}
2025-01-13 07:39:03,286 [INFO] Step[1550/2713]: training loss : 0.9376463413238525 TRAIN  loss dict:  {'classification_loss': 0.9376463413238525}
2025-01-13 07:39:15,186 [INFO] Step[1600/2713]: training loss : 0.9379425191879273 TRAIN  loss dict:  {'classification_loss': 0.9379425191879273}
2025-01-13 07:39:27,116 [INFO] Step[1650/2713]: training loss : 0.9367445981502533 TRAIN  loss dict:  {'classification_loss': 0.9367445981502533}
2025-01-13 07:39:38,997 [INFO] Step[1700/2713]: training loss : 0.9386644530296325 TRAIN  loss dict:  {'classification_loss': 0.9386644530296325}
2025-01-13 07:39:50,947 [INFO] Step[1750/2713]: training loss : 0.9371837913990021 TRAIN  loss dict:  {'classification_loss': 0.9371837913990021}
2025-01-13 07:40:02,906 [INFO] Step[1800/2713]: training loss : 0.9366973471641541 TRAIN  loss dict:  {'classification_loss': 0.9366973471641541}
2025-01-13 07:40:14,858 [INFO] Step[1850/2713]: training loss : 0.9376776802539826 TRAIN  loss dict:  {'classification_loss': 0.9376776802539826}
2025-01-13 07:40:26,713 [INFO] Step[1900/2713]: training loss : 0.9396463823318482 TRAIN  loss dict:  {'classification_loss': 0.9396463823318482}
2025-01-13 07:40:38,630 [INFO] Step[1950/2713]: training loss : 0.9399565744400025 TRAIN  loss dict:  {'classification_loss': 0.9399565744400025}
2025-01-13 07:40:50,567 [INFO] Step[2000/2713]: training loss : 0.9407388150691987 TRAIN  loss dict:  {'classification_loss': 0.9407388150691987}
2025-01-13 07:41:02,464 [INFO] Step[2050/2713]: training loss : 0.9366502416133881 TRAIN  loss dict:  {'classification_loss': 0.9366502416133881}
2025-01-13 07:41:14,377 [INFO] Step[2100/2713]: training loss : 0.9361035585403442 TRAIN  loss dict:  {'classification_loss': 0.9361035585403442}
2025-01-13 07:41:26,281 [INFO] Step[2150/2713]: training loss : 0.9383931493759156 TRAIN  loss dict:  {'classification_loss': 0.9383931493759156}
2025-01-13 07:41:38,170 [INFO] Step[2200/2713]: training loss : 0.9382384836673736 TRAIN  loss dict:  {'classification_loss': 0.9382384836673736}
2025-01-13 07:41:50,095 [INFO] Step[2250/2713]: training loss : 0.9428086721897125 TRAIN  loss dict:  {'classification_loss': 0.9428086721897125}
2025-01-13 07:42:01,990 [INFO] Step[2300/2713]: training loss : 0.9366275310516358 TRAIN  loss dict:  {'classification_loss': 0.9366275310516358}
2025-01-13 07:42:13,936 [INFO] Step[2350/2713]: training loss : 0.9386032092571258 TRAIN  loss dict:  {'classification_loss': 0.9386032092571258}
2025-01-13 07:42:25,828 [INFO] Step[2400/2713]: training loss : 0.937209769487381 TRAIN  loss dict:  {'classification_loss': 0.937209769487381}
2025-01-13 07:42:37,716 [INFO] Step[2450/2713]: training loss : 0.9380670690536499 TRAIN  loss dict:  {'classification_loss': 0.9380670690536499}
2025-01-13 07:42:49,576 [INFO] Step[2500/2713]: training loss : 0.9399640595912934 TRAIN  loss dict:  {'classification_loss': 0.9399640595912934}
2025-01-13 07:43:01,497 [INFO] Step[2550/2713]: training loss : 0.9367500412464141 TRAIN  loss dict:  {'classification_loss': 0.9367500412464141}
2025-01-13 07:43:13,429 [INFO] Step[2600/2713]: training loss : 0.9359390604496002 TRAIN  loss dict:  {'classification_loss': 0.9359390604496002}
2025-01-13 07:43:25,320 [INFO] Step[2650/2713]: training loss : 0.9386351537704468 TRAIN  loss dict:  {'classification_loss': 0.9386351537704468}
2025-01-13 07:43:37,223 [INFO] Step[2700/2713]: training loss : 0.9371882963180542 TRAIN  loss dict:  {'classification_loss': 0.9371882963180542}
2025-01-13 07:45:05,530 [INFO] Label accuracies statistics:
2025-01-13 07:45:05,530 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 1.0, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 07:45:05,532 [INFO] [81] TRAIN  loss: 0.9382888510620028 acc: 1.0
2025-01-13 07:45:05,532 [INFO] [81] TRAIN  loss dict: {'classification_loss': 0.9382888510620028}
2025-01-13 07:45:05,532 [INFO] [81] VALIDATION loss: 1.683244756177852 VALIDATION acc: 0.8351097178683385
2025-01-13 07:45:05,532 [INFO] [81] VALIDATION loss dict: {'classification_loss': 1.683244756177852}
2025-01-13 07:45:05,532 [INFO] 
2025-01-13 07:45:23,096 [INFO] Step[50/2713]: training loss : 0.9362050747871399 TRAIN  loss dict:  {'classification_loss': 0.9362050747871399}
2025-01-13 07:45:35,014 [INFO] Step[100/2713]: training loss : 0.9372558748722076 TRAIN  loss dict:  {'classification_loss': 0.9372558748722076}
2025-01-13 07:45:46,966 [INFO] Step[150/2713]: training loss : 0.9379833006858825 TRAIN  loss dict:  {'classification_loss': 0.9379833006858825}
2025-01-13 07:45:58,924 [INFO] Step[200/2713]: training loss : 0.9372218978404999 TRAIN  loss dict:  {'classification_loss': 0.9372218978404999}
2025-01-13 07:46:10,854 [INFO] Step[250/2713]: training loss : 0.9373654675483704 TRAIN  loss dict:  {'classification_loss': 0.9373654675483704}
2025-01-13 07:46:22,778 [INFO] Step[300/2713]: training loss : 0.9359860789775848 TRAIN  loss dict:  {'classification_loss': 0.9359860789775848}
2025-01-13 07:46:34,725 [INFO] Step[350/2713]: training loss : 0.9369829189777374 TRAIN  loss dict:  {'classification_loss': 0.9369829189777374}
2025-01-13 07:46:46,616 [INFO] Step[400/2713]: training loss : 0.9370949363708496 TRAIN  loss dict:  {'classification_loss': 0.9370949363708496}
2025-01-13 07:46:58,551 [INFO] Step[450/2713]: training loss : 0.9373723328113556 TRAIN  loss dict:  {'classification_loss': 0.9373723328113556}
2025-01-13 07:47:10,465 [INFO] Step[500/2713]: training loss : 0.9367885184288025 TRAIN  loss dict:  {'classification_loss': 0.9367885184288025}
2025-01-13 07:47:22,357 [INFO] Step[550/2713]: training loss : 0.9366672194004059 TRAIN  loss dict:  {'classification_loss': 0.9366672194004059}
2025-01-13 07:47:34,292 [INFO] Step[600/2713]: training loss : 0.9390317130088807 TRAIN  loss dict:  {'classification_loss': 0.9390317130088807}
2025-01-13 07:47:46,202 [INFO] Step[650/2713]: training loss : 0.9379250383377076 TRAIN  loss dict:  {'classification_loss': 0.9379250383377076}
2025-01-13 07:47:58,092 [INFO] Step[700/2713]: training loss : 0.9380192792415619 TRAIN  loss dict:  {'classification_loss': 0.9380192792415619}
2025-01-13 07:48:09,956 [INFO] Step[750/2713]: training loss : 0.9385975646972656 TRAIN  loss dict:  {'classification_loss': 0.9385975646972656}
2025-01-13 07:48:21,869 [INFO] Step[800/2713]: training loss : 0.9378139436244964 TRAIN  loss dict:  {'classification_loss': 0.9378139436244964}
2025-01-13 07:48:33,800 [INFO] Step[850/2713]: training loss : 0.9389878022670746 TRAIN  loss dict:  {'classification_loss': 0.9389878022670746}
2025-01-13 07:48:45,735 [INFO] Step[900/2713]: training loss : 0.937364649772644 TRAIN  loss dict:  {'classification_loss': 0.937364649772644}
2025-01-13 07:48:57,665 [INFO] Step[950/2713]: training loss : 0.9375342738628387 TRAIN  loss dict:  {'classification_loss': 0.9375342738628387}
2025-01-13 07:49:09,547 [INFO] Step[1000/2713]: training loss : 0.9379906928539277 TRAIN  loss dict:  {'classification_loss': 0.9379906928539277}
2025-01-13 07:49:21,498 [INFO] Step[1050/2713]: training loss : 0.9404435896873474 TRAIN  loss dict:  {'classification_loss': 0.9404435896873474}
2025-01-13 07:49:33,416 [INFO] Step[1100/2713]: training loss : 0.9387972700595856 TRAIN  loss dict:  {'classification_loss': 0.9387972700595856}
2025-01-13 07:49:45,340 [INFO] Step[1150/2713]: training loss : 0.9400340270996094 TRAIN  loss dict:  {'classification_loss': 0.9400340270996094}
2025-01-13 07:49:57,239 [INFO] Step[1200/2713]: training loss : 0.9365240359306335 TRAIN  loss dict:  {'classification_loss': 0.9365240359306335}
2025-01-13 07:50:09,156 [INFO] Step[1250/2713]: training loss : 0.9373317992687226 TRAIN  loss dict:  {'classification_loss': 0.9373317992687226}
2025-01-13 07:50:21,051 [INFO] Step[1300/2713]: training loss : 0.9385401809215546 TRAIN  loss dict:  {'classification_loss': 0.9385401809215546}
2025-01-13 07:50:32,968 [INFO] Step[1350/2713]: training loss : 0.9362279546260833 TRAIN  loss dict:  {'classification_loss': 0.9362279546260833}
2025-01-13 07:50:44,871 [INFO] Step[1400/2713]: training loss : 0.939521347284317 TRAIN  loss dict:  {'classification_loss': 0.939521347284317}
2025-01-13 07:50:56,768 [INFO] Step[1450/2713]: training loss : 0.9378156685829162 TRAIN  loss dict:  {'classification_loss': 0.9378156685829162}
2025-01-13 07:51:08,648 [INFO] Step[1500/2713]: training loss : 0.9381390511989594 TRAIN  loss dict:  {'classification_loss': 0.9381390511989594}
2025-01-13 07:51:20,527 [INFO] Step[1550/2713]: training loss : 0.9377683782577515 TRAIN  loss dict:  {'classification_loss': 0.9377683782577515}
2025-01-13 07:51:32,427 [INFO] Step[1600/2713]: training loss : 0.936642951965332 TRAIN  loss dict:  {'classification_loss': 0.936642951965332}
2025-01-13 07:51:44,314 [INFO] Step[1650/2713]: training loss : 0.9383141386508942 TRAIN  loss dict:  {'classification_loss': 0.9383141386508942}
2025-01-13 07:51:56,201 [INFO] Step[1700/2713]: training loss : 0.9388155949115753 TRAIN  loss dict:  {'classification_loss': 0.9388155949115753}
2025-01-13 07:52:08,089 [INFO] Step[1750/2713]: training loss : 0.9386424136161804 TRAIN  loss dict:  {'classification_loss': 0.9386424136161804}
2025-01-13 07:52:19,994 [INFO] Step[1800/2713]: training loss : 0.9398608708381653 TRAIN  loss dict:  {'classification_loss': 0.9398608708381653}
2025-01-13 07:52:31,881 [INFO] Step[1850/2713]: training loss : 0.9379052817821503 TRAIN  loss dict:  {'classification_loss': 0.9379052817821503}
2025-01-13 07:52:43,972 [INFO] Step[1900/2713]: training loss : 0.9383372950553894 TRAIN  loss dict:  {'classification_loss': 0.9383372950553894}
2025-01-13 07:52:56,430 [INFO] Step[1950/2713]: training loss : 0.9389461958408356 TRAIN  loss dict:  {'classification_loss': 0.9389461958408356}
2025-01-13 07:53:08,916 [INFO] Step[2000/2713]: training loss : 0.9414693975448608 TRAIN  loss dict:  {'classification_loss': 0.9414693975448608}
2025-01-13 07:53:21,204 [INFO] Step[2050/2713]: training loss : 0.9405475306510925 TRAIN  loss dict:  {'classification_loss': 0.9405475306510925}
2025-01-13 07:53:34,106 [INFO] Step[2100/2713]: training loss : 0.9404951786994934 TRAIN  loss dict:  {'classification_loss': 0.9404951786994934}
2025-01-13 07:53:46,340 [INFO] Step[2150/2713]: training loss : 0.9362682247161865 TRAIN  loss dict:  {'classification_loss': 0.9362682247161865}
2025-01-13 07:53:59,035 [INFO] Step[2200/2713]: training loss : 0.937321788072586 TRAIN  loss dict:  {'classification_loss': 0.937321788072586}
2025-01-13 07:54:12,748 [INFO] Step[2250/2713]: training loss : 0.9389160311222077 TRAIN  loss dict:  {'classification_loss': 0.9389160311222077}
2025-01-13 07:54:26,203 [INFO] Step[2300/2713]: training loss : 0.9576946771144867 TRAIN  loss dict:  {'classification_loss': 0.9576946771144867}
2025-01-13 07:54:38,309 [INFO] Step[2350/2713]: training loss : 0.9398361229896546 TRAIN  loss dict:  {'classification_loss': 0.9398361229896546}
2025-01-13 07:54:50,207 [INFO] Step[2400/2713]: training loss : 0.9382390081882477 TRAIN  loss dict:  {'classification_loss': 0.9382390081882477}
2025-01-13 07:55:02,103 [INFO] Step[2450/2713]: training loss : 0.939585348367691 TRAIN  loss dict:  {'classification_loss': 0.939585348367691}
2025-01-13 07:55:13,946 [INFO] Step[2500/2713]: training loss : 0.9377121984958648 TRAIN  loss dict:  {'classification_loss': 0.9377121984958648}
2025-01-13 07:55:25,782 [INFO] Step[2550/2713]: training loss : 0.9356288516521454 TRAIN  loss dict:  {'classification_loss': 0.9356288516521454}
2025-01-13 07:55:37,647 [INFO] Step[2600/2713]: training loss : 0.9374329149723053 TRAIN  loss dict:  {'classification_loss': 0.9374329149723053}
2025-01-13 07:55:49,518 [INFO] Step[2650/2713]: training loss : 0.93765345454216 TRAIN  loss dict:  {'classification_loss': 0.93765345454216}
2025-01-13 07:56:01,396 [INFO] Step[2700/2713]: training loss : 0.9376157021522522 TRAIN  loss dict:  {'classification_loss': 0.9376157021522522}
2025-01-13 07:57:27,757 [INFO] Label accuracies statistics:
2025-01-13 07:57:27,757 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 1.0, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 1.0, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.25, 204: 1.0, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 0.75, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.5, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.5, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 07:57:28,850 [INFO] [82] TRAIN  loss: 0.9384444892692074 acc: 0.9998771347831429
2025-01-13 07:57:28,850 [INFO] [82] TRAIN  loss dict: {'classification_loss': 0.9384444892692074}
2025-01-13 07:57:28,850 [INFO] [82] VALIDATION loss: 1.66926274936002 VALIDATION acc: 0.8470219435736677
2025-01-13 07:57:28,850 [INFO] [82] VALIDATION loss dict: {'classification_loss': 1.66926274936002}
2025-01-13 07:57:28,851 [INFO] 
2025-01-13 07:57:46,156 [INFO] Step[50/2713]: training loss : 0.939430867433548 TRAIN  loss dict:  {'classification_loss': 0.939430867433548}
2025-01-13 07:57:58,017 [INFO] Step[100/2713]: training loss : 0.9375990653038024 TRAIN  loss dict:  {'classification_loss': 0.9375990653038024}
2025-01-13 07:58:09,937 [INFO] Step[150/2713]: training loss : 0.9365239799022674 TRAIN  loss dict:  {'classification_loss': 0.9365239799022674}
2025-01-13 07:58:21,912 [INFO] Step[200/2713]: training loss : 0.9424057030677795 TRAIN  loss dict:  {'classification_loss': 0.9424057030677795}
2025-01-13 07:58:33,862 [INFO] Step[250/2713]: training loss : 0.9369864451885224 TRAIN  loss dict:  {'classification_loss': 0.9369864451885224}
2025-01-13 07:58:45,789 [INFO] Step[300/2713]: training loss : 0.9355648684501648 TRAIN  loss dict:  {'classification_loss': 0.9355648684501648}
2025-01-13 07:58:57,725 [INFO] Step[350/2713]: training loss : 0.9370054674148559 TRAIN  loss dict:  {'classification_loss': 0.9370054674148559}
2025-01-13 07:59:09,643 [INFO] Step[400/2713]: training loss : 0.9575888478755951 TRAIN  loss dict:  {'classification_loss': 0.9575888478755951}
2025-01-13 07:59:21,594 [INFO] Step[450/2713]: training loss : 0.9392297053337098 TRAIN  loss dict:  {'classification_loss': 0.9392297053337098}
2025-01-13 07:59:33,508 [INFO] Step[500/2713]: training loss : 0.9414646017551422 TRAIN  loss dict:  {'classification_loss': 0.9414646017551422}
2025-01-13 07:59:45,433 [INFO] Step[550/2713]: training loss : 0.9365322208404541 TRAIN  loss dict:  {'classification_loss': 0.9365322208404541}
2025-01-13 07:59:57,334 [INFO] Step[600/2713]: training loss : 0.9370212268829345 TRAIN  loss dict:  {'classification_loss': 0.9370212268829345}
2025-01-13 08:00:09,236 [INFO] Step[650/2713]: training loss : 0.9375691413879395 TRAIN  loss dict:  {'classification_loss': 0.9375691413879395}
2025-01-13 08:00:21,166 [INFO] Step[700/2713]: training loss : 0.9365224766731263 TRAIN  loss dict:  {'classification_loss': 0.9365224766731263}
2025-01-13 08:00:33,118 [INFO] Step[750/2713]: training loss : 0.9376049196720123 TRAIN  loss dict:  {'classification_loss': 0.9376049196720123}
2025-01-13 08:00:45,040 [INFO] Step[800/2713]: training loss : 0.936849811077118 TRAIN  loss dict:  {'classification_loss': 0.936849811077118}
2025-01-13 08:00:56,932 [INFO] Step[850/2713]: training loss : 0.9362310779094696 TRAIN  loss dict:  {'classification_loss': 0.9362310779094696}
2025-01-13 08:01:08,868 [INFO] Step[900/2713]: training loss : 0.936738213300705 TRAIN  loss dict:  {'classification_loss': 0.936738213300705}
2025-01-13 08:01:20,790 [INFO] Step[950/2713]: training loss : 0.9370891225337982 TRAIN  loss dict:  {'classification_loss': 0.9370891225337982}
2025-01-13 08:01:32,730 [INFO] Step[1000/2713]: training loss : 0.9374033796787262 TRAIN  loss dict:  {'classification_loss': 0.9374033796787262}
2025-01-13 08:01:44,635 [INFO] Step[1050/2713]: training loss : 0.9374205541610717 TRAIN  loss dict:  {'classification_loss': 0.9374205541610717}
2025-01-13 08:01:56,556 [INFO] Step[1100/2713]: training loss : 0.9391097664833069 TRAIN  loss dict:  {'classification_loss': 0.9391097664833069}
2025-01-13 08:02:08,507 [INFO] Step[1150/2713]: training loss : 0.9369124245643615 TRAIN  loss dict:  {'classification_loss': 0.9369124245643615}
2025-01-13 08:02:20,395 [INFO] Step[1200/2713]: training loss : 0.9380467927455902 TRAIN  loss dict:  {'classification_loss': 0.9380467927455902}
2025-01-13 08:02:32,341 [INFO] Step[1250/2713]: training loss : 0.9371109938621521 TRAIN  loss dict:  {'classification_loss': 0.9371109938621521}
2025-01-13 08:02:44,255 [INFO] Step[1300/2713]: training loss : 0.9371076679229736 TRAIN  loss dict:  {'classification_loss': 0.9371076679229736}
2025-01-13 08:02:56,127 [INFO] Step[1350/2713]: training loss : 0.9369783675670624 TRAIN  loss dict:  {'classification_loss': 0.9369783675670624}
2025-01-13 08:03:07,991 [INFO] Step[1400/2713]: training loss : 0.9418110620975494 TRAIN  loss dict:  {'classification_loss': 0.9418110620975494}
2025-01-13 08:03:19,909 [INFO] Step[1450/2713]: training loss : 0.9369776058197021 TRAIN  loss dict:  {'classification_loss': 0.9369776058197021}
2025-01-13 08:03:31,811 [INFO] Step[1500/2713]: training loss : 0.9412250256538391 TRAIN  loss dict:  {'classification_loss': 0.9412250256538391}
2025-01-13 08:03:43,728 [INFO] Step[1550/2713]: training loss : 0.9402484464645385 TRAIN  loss dict:  {'classification_loss': 0.9402484464645385}
2025-01-13 08:03:55,646 [INFO] Step[1600/2713]: training loss : 0.9384556102752686 TRAIN  loss dict:  {'classification_loss': 0.9384556102752686}
2025-01-13 08:04:07,563 [INFO] Step[1650/2713]: training loss : 0.9374104809761047 TRAIN  loss dict:  {'classification_loss': 0.9374104809761047}
2025-01-13 08:04:19,448 [INFO] Step[1700/2713]: training loss : 0.9365816962718964 TRAIN  loss dict:  {'classification_loss': 0.9365816962718964}
2025-01-13 08:04:31,331 [INFO] Step[1750/2713]: training loss : 0.9533986413478851 TRAIN  loss dict:  {'classification_loss': 0.9533986413478851}
2025-01-13 08:04:43,248 [INFO] Step[1800/2713]: training loss : 0.9374999046325684 TRAIN  loss dict:  {'classification_loss': 0.9374999046325684}
2025-01-13 08:04:55,153 [INFO] Step[1850/2713]: training loss : 0.9371271729469299 TRAIN  loss dict:  {'classification_loss': 0.9371271729469299}
2025-01-13 08:05:07,117 [INFO] Step[1900/2713]: training loss : 0.9366726338863373 TRAIN  loss dict:  {'classification_loss': 0.9366726338863373}
2025-01-13 08:05:19,037 [INFO] Step[1950/2713]: training loss : 0.9361157011985779 TRAIN  loss dict:  {'classification_loss': 0.9361157011985779}
2025-01-13 08:05:30,962 [INFO] Step[2000/2713]: training loss : 0.9368596684932708 TRAIN  loss dict:  {'classification_loss': 0.9368596684932708}
2025-01-13 08:05:42,900 [INFO] Step[2050/2713]: training loss : 0.9374979138374329 TRAIN  loss dict:  {'classification_loss': 0.9374979138374329}
2025-01-13 08:05:54,811 [INFO] Step[2100/2713]: training loss : 0.9370607590675354 TRAIN  loss dict:  {'classification_loss': 0.9370607590675354}
2025-01-13 08:06:06,689 [INFO] Step[2150/2713]: training loss : 0.9381833577156067 TRAIN  loss dict:  {'classification_loss': 0.9381833577156067}
2025-01-13 08:06:18,572 [INFO] Step[2200/2713]: training loss : 0.9359959661960602 TRAIN  loss dict:  {'classification_loss': 0.9359959661960602}
2025-01-13 08:06:30,526 [INFO] Step[2250/2713]: training loss : 0.9370846593379974 TRAIN  loss dict:  {'classification_loss': 0.9370846593379974}
2025-01-13 08:06:42,481 [INFO] Step[2300/2713]: training loss : 0.9372276246547699 TRAIN  loss dict:  {'classification_loss': 0.9372276246547699}
2025-01-13 08:06:54,370 [INFO] Step[2350/2713]: training loss : 0.9385322904586793 TRAIN  loss dict:  {'classification_loss': 0.9385322904586793}
2025-01-13 08:07:06,301 [INFO] Step[2400/2713]: training loss : 0.9383196115493775 TRAIN  loss dict:  {'classification_loss': 0.9383196115493775}
2025-01-13 08:07:18,254 [INFO] Step[2450/2713]: training loss : 0.9390993118286133 TRAIN  loss dict:  {'classification_loss': 0.9390993118286133}
2025-01-13 08:07:30,151 [INFO] Step[2500/2713]: training loss : 0.9367637050151825 TRAIN  loss dict:  {'classification_loss': 0.9367637050151825}
2025-01-13 08:07:42,116 [INFO] Step[2550/2713]: training loss : 0.9380524933338166 TRAIN  loss dict:  {'classification_loss': 0.9380524933338166}
2025-01-13 08:07:54,016 [INFO] Step[2600/2713]: training loss : 0.9397155308723449 TRAIN  loss dict:  {'classification_loss': 0.9397155308723449}
2025-01-13 08:08:05,924 [INFO] Step[2650/2713]: training loss : 0.9414588797092438 TRAIN  loss dict:  {'classification_loss': 0.9414588797092438}
2025-01-13 08:08:17,857 [INFO] Step[2700/2713]: training loss : 0.9358975422382355 TRAIN  loss dict:  {'classification_loss': 0.9358975422382355}
2025-01-13 08:09:43,979 [INFO] Label accuracies statistics:
2025-01-13 08:09:43,979 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.5, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.5, 204: 1.0, 205: 1.0, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.75, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.25, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 0.6666666666666666, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 08:09:43,981 [INFO] [83] TRAIN  loss: 0.9384754795321582 acc: 0.9998771347831429
2025-01-13 08:09:43,981 [INFO] [83] TRAIN  loss dict: {'classification_loss': 0.9384754795321582}
2025-01-13 08:09:43,981 [INFO] [83] VALIDATION loss: 1.7095083240279578 VALIDATION acc: 0.8326018808777429
2025-01-13 08:09:43,981 [INFO] [83] VALIDATION loss dict: {'classification_loss': 1.7095083240279578}
2025-01-13 08:09:43,981 [INFO] 
2025-01-13 08:10:01,968 [INFO] Step[50/2713]: training loss : 0.9416355872154236 TRAIN  loss dict:  {'classification_loss': 0.9416355872154236}
2025-01-13 08:10:13,848 [INFO] Step[100/2713]: training loss : 0.9381320607662201 TRAIN  loss dict:  {'classification_loss': 0.9381320607662201}
2025-01-13 08:10:25,721 [INFO] Step[150/2713]: training loss : 0.9367024850845337 TRAIN  loss dict:  {'classification_loss': 0.9367024850845337}
2025-01-13 08:10:37,647 [INFO] Step[200/2713]: training loss : 0.9367700564861298 TRAIN  loss dict:  {'classification_loss': 0.9367700564861298}
2025-01-13 08:10:49,577 [INFO] Step[250/2713]: training loss : 0.9377831542491912 TRAIN  loss dict:  {'classification_loss': 0.9377831542491912}
2025-01-13 08:11:01,492 [INFO] Step[300/2713]: training loss : 0.9469554662704468 TRAIN  loss dict:  {'classification_loss': 0.9469554662704468}
2025-01-13 08:11:13,396 [INFO] Step[350/2713]: training loss : 0.9393940532207489 TRAIN  loss dict:  {'classification_loss': 0.9393940532207489}
2025-01-13 08:11:25,296 [INFO] Step[400/2713]: training loss : 0.9371515774726867 TRAIN  loss dict:  {'classification_loss': 0.9371515774726867}
2025-01-13 08:11:37,308 [INFO] Step[450/2713]: training loss : 0.9396669805049896 TRAIN  loss dict:  {'classification_loss': 0.9396669805049896}
2025-01-13 08:11:49,592 [INFO] Step[500/2713]: training loss : 0.9389435088634491 TRAIN  loss dict:  {'classification_loss': 0.9389435088634491}
2025-01-13 08:12:01,991 [INFO] Step[550/2713]: training loss : 0.9367648577690124 TRAIN  loss dict:  {'classification_loss': 0.9367648577690124}
2025-01-13 08:12:14,149 [INFO] Step[600/2713]: training loss : 0.9395744657516479 TRAIN  loss dict:  {'classification_loss': 0.9395744657516479}
2025-01-13 08:12:26,798 [INFO] Step[650/2713]: training loss : 0.9381851994991303 TRAIN  loss dict:  {'classification_loss': 0.9381851994991303}
2025-01-13 08:12:39,085 [INFO] Step[700/2713]: training loss : 0.9384090435504914 TRAIN  loss dict:  {'classification_loss': 0.9384090435504914}
2025-01-13 08:12:51,584 [INFO] Step[750/2713]: training loss : 0.9368009769916534 TRAIN  loss dict:  {'classification_loss': 0.9368009769916534}
2025-01-13 08:13:04,865 [INFO] Step[800/2713]: training loss : 0.9362460207939148 TRAIN  loss dict:  {'classification_loss': 0.9362460207939148}
2025-01-13 08:13:18,761 [INFO] Step[850/2713]: training loss : 0.9353700947761535 TRAIN  loss dict:  {'classification_loss': 0.9353700947761535}
2025-01-13 08:13:31,492 [INFO] Step[900/2713]: training loss : 0.9376024878025055 TRAIN  loss dict:  {'classification_loss': 0.9376024878025055}
2025-01-13 08:13:43,402 [INFO] Step[950/2713]: training loss : 0.9376802313327789 TRAIN  loss dict:  {'classification_loss': 0.9376802313327789}
2025-01-13 08:13:55,356 [INFO] Step[1000/2713]: training loss : 0.9364858746528626 TRAIN  loss dict:  {'classification_loss': 0.9364858746528626}
2025-01-13 08:14:07,230 [INFO] Step[1050/2713]: training loss : 0.9387404251098633 TRAIN  loss dict:  {'classification_loss': 0.9387404251098633}
2025-01-13 08:14:19,099 [INFO] Step[1100/2713]: training loss : 0.9380342030525207 TRAIN  loss dict:  {'classification_loss': 0.9380342030525207}
2025-01-13 08:14:30,974 [INFO] Step[1150/2713]: training loss : 0.9362168657779694 TRAIN  loss dict:  {'classification_loss': 0.9362168657779694}
2025-01-13 08:14:42,855 [INFO] Step[1200/2713]: training loss : 0.9358499610424041 TRAIN  loss dict:  {'classification_loss': 0.9358499610424041}
2025-01-13 08:14:54,732 [INFO] Step[1250/2713]: training loss : 0.9361979556083679 TRAIN  loss dict:  {'classification_loss': 0.9361979556083679}
2025-01-13 08:15:06,618 [INFO] Step[1300/2713]: training loss : 0.9376824259757995 TRAIN  loss dict:  {'classification_loss': 0.9376824259757995}
2025-01-13 08:15:18,493 [INFO] Step[1350/2713]: training loss : 0.9371435642242432 TRAIN  loss dict:  {'classification_loss': 0.9371435642242432}
2025-01-13 08:15:30,371 [INFO] Step[1400/2713]: training loss : 0.9375279235839844 TRAIN  loss dict:  {'classification_loss': 0.9375279235839844}
2025-01-13 08:15:42,267 [INFO] Step[1450/2713]: training loss : 0.9367143881320953 TRAIN  loss dict:  {'classification_loss': 0.9367143881320953}
2025-01-13 08:15:54,135 [INFO] Step[1500/2713]: training loss : 0.9366558754444122 TRAIN  loss dict:  {'classification_loss': 0.9366558754444122}
2025-01-13 08:16:06,054 [INFO] Step[1550/2713]: training loss : 0.9373433923721314 TRAIN  loss dict:  {'classification_loss': 0.9373433923721314}
2025-01-13 08:16:17,984 [INFO] Step[1600/2713]: training loss : 0.9364794588088989 TRAIN  loss dict:  {'classification_loss': 0.9364794588088989}
2025-01-13 08:16:29,901 [INFO] Step[1650/2713]: training loss : 0.9391752183437347 TRAIN  loss dict:  {'classification_loss': 0.9391752183437347}
2025-01-13 08:16:41,813 [INFO] Step[1700/2713]: training loss : 0.9367640006542206 TRAIN  loss dict:  {'classification_loss': 0.9367640006542206}
2025-01-13 08:16:53,729 [INFO] Step[1750/2713]: training loss : 0.9370281386375428 TRAIN  loss dict:  {'classification_loss': 0.9370281386375428}
2025-01-13 08:17:05,619 [INFO] Step[1800/2713]: training loss : 0.9362178599834442 TRAIN  loss dict:  {'classification_loss': 0.9362178599834442}
2025-01-13 08:17:17,543 [INFO] Step[1850/2713]: training loss : 0.9347397530078888 TRAIN  loss dict:  {'classification_loss': 0.9347397530078888}
2025-01-13 08:17:29,477 [INFO] Step[1900/2713]: training loss : 0.9368482339382171 TRAIN  loss dict:  {'classification_loss': 0.9368482339382171}
2025-01-13 08:17:41,328 [INFO] Step[1950/2713]: training loss : 0.9356722259521484 TRAIN  loss dict:  {'classification_loss': 0.9356722259521484}
2025-01-13 08:17:53,193 [INFO] Step[2000/2713]: training loss : 0.9373367631435394 TRAIN  loss dict:  {'classification_loss': 0.9373367631435394}
2025-01-13 08:18:05,076 [INFO] Step[2050/2713]: training loss : 0.936686635017395 TRAIN  loss dict:  {'classification_loss': 0.936686635017395}
2025-01-13 08:18:17,034 [INFO] Step[2100/2713]: training loss : 0.9375254619121551 TRAIN  loss dict:  {'classification_loss': 0.9375254619121551}
2025-01-13 08:18:28,888 [INFO] Step[2150/2713]: training loss : 0.9383341562747955 TRAIN  loss dict:  {'classification_loss': 0.9383341562747955}
2025-01-13 08:18:40,787 [INFO] Step[2200/2713]: training loss : 0.9369404995441437 TRAIN  loss dict:  {'classification_loss': 0.9369404995441437}
2025-01-13 08:18:52,752 [INFO] Step[2250/2713]: training loss : 0.9373512434959411 TRAIN  loss dict:  {'classification_loss': 0.9373512434959411}
2025-01-13 08:19:04,666 [INFO] Step[2300/2713]: training loss : 0.9355022919178009 TRAIN  loss dict:  {'classification_loss': 0.9355022919178009}
2025-01-13 08:19:16,526 [INFO] Step[2350/2713]: training loss : 0.9367992544174194 TRAIN  loss dict:  {'classification_loss': 0.9367992544174194}
2025-01-13 08:19:28,382 [INFO] Step[2400/2713]: training loss : 0.9364792656898498 TRAIN  loss dict:  {'classification_loss': 0.9364792656898498}
2025-01-13 08:19:40,247 [INFO] Step[2450/2713]: training loss : 0.936548775434494 TRAIN  loss dict:  {'classification_loss': 0.936548775434494}
2025-01-13 08:19:52,160 [INFO] Step[2500/2713]: training loss : 0.9389754521846772 TRAIN  loss dict:  {'classification_loss': 0.9389754521846772}
2025-01-13 08:20:04,058 [INFO] Step[2550/2713]: training loss : 0.9382713210582733 TRAIN  loss dict:  {'classification_loss': 0.9382713210582733}
2025-01-13 08:20:15,956 [INFO] Step[2600/2713]: training loss : 0.9368511378765106 TRAIN  loss dict:  {'classification_loss': 0.9368511378765106}
2025-01-13 08:20:27,854 [INFO] Step[2650/2713]: training loss : 0.9355973744392395 TRAIN  loss dict:  {'classification_loss': 0.9355973744392395}
2025-01-13 08:20:39,768 [INFO] Step[2700/2713]: training loss : 0.9391321837902069 TRAIN  loss dict:  {'classification_loss': 0.9391321837902069}
2025-01-13 08:22:08,432 [INFO] Label accuracies statistics:
2025-01-13 08:22:08,432 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 1.0, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.25, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 1.0, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 08:22:08,434 [INFO] [84] TRAIN  loss: 0.9375117225836793 acc: 1.0
2025-01-13 08:22:08,434 [INFO] [84] TRAIN  loss dict: {'classification_loss': 0.9375117225836793}
2025-01-13 08:22:08,434 [INFO] [84] VALIDATION loss: 1.6690557897090912 VALIDATION acc: 0.8445141065830721
2025-01-13 08:22:08,434 [INFO] [84] VALIDATION loss dict: {'classification_loss': 1.6690557897090912}
2025-01-13 08:22:08,434 [INFO] 
2025-01-13 08:22:26,414 [INFO] Step[50/2713]: training loss : 0.9398111987113953 TRAIN  loss dict:  {'classification_loss': 0.9398111987113953}
2025-01-13 08:22:38,283 [INFO] Step[100/2713]: training loss : 0.9365633463859558 TRAIN  loss dict:  {'classification_loss': 0.9365633463859558}
2025-01-13 08:22:50,186 [INFO] Step[150/2713]: training loss : 0.9371732676029205 TRAIN  loss dict:  {'classification_loss': 0.9371732676029205}
2025-01-13 08:23:02,128 [INFO] Step[200/2713]: training loss : 0.9359458470344544 TRAIN  loss dict:  {'classification_loss': 0.9359458470344544}
2025-01-13 08:23:14,028 [INFO] Step[250/2713]: training loss : 0.9365864634513855 TRAIN  loss dict:  {'classification_loss': 0.9365864634513855}
2025-01-13 08:23:25,901 [INFO] Step[300/2713]: training loss : 0.9391753244400024 TRAIN  loss dict:  {'classification_loss': 0.9391753244400024}
2025-01-13 08:23:37,819 [INFO] Step[350/2713]: training loss : 0.9369305562973023 TRAIN  loss dict:  {'classification_loss': 0.9369305562973023}
2025-01-13 08:23:49,761 [INFO] Step[400/2713]: training loss : 0.9359164714813233 TRAIN  loss dict:  {'classification_loss': 0.9359164714813233}
2025-01-13 08:24:01,664 [INFO] Step[450/2713]: training loss : 0.9367108798027038 TRAIN  loss dict:  {'classification_loss': 0.9367108798027038}
2025-01-13 08:24:13,551 [INFO] Step[500/2713]: training loss : 0.9382134711742401 TRAIN  loss dict:  {'classification_loss': 0.9382134711742401}
2025-01-13 08:24:25,531 [INFO] Step[550/2713]: training loss : 0.9348310256004333 TRAIN  loss dict:  {'classification_loss': 0.9348310256004333}
2025-01-13 08:24:37,458 [INFO] Step[600/2713]: training loss : 0.9360833156108856 TRAIN  loss dict:  {'classification_loss': 0.9360833156108856}
2025-01-13 08:24:49,348 [INFO] Step[650/2713]: training loss : 0.9370338332653045 TRAIN  loss dict:  {'classification_loss': 0.9370338332653045}
2025-01-13 08:25:01,253 [INFO] Step[700/2713]: training loss : 0.9381676387786865 TRAIN  loss dict:  {'classification_loss': 0.9381676387786865}
2025-01-13 08:25:13,161 [INFO] Step[750/2713]: training loss : 0.938339717388153 TRAIN  loss dict:  {'classification_loss': 0.938339717388153}
2025-01-13 08:25:25,038 [INFO] Step[800/2713]: training loss : 0.9373186600208282 TRAIN  loss dict:  {'classification_loss': 0.9373186600208282}
2025-01-13 08:25:36,930 [INFO] Step[850/2713]: training loss : 0.9365924489498139 TRAIN  loss dict:  {'classification_loss': 0.9365924489498139}
2025-01-13 08:25:48,848 [INFO] Step[900/2713]: training loss : 0.9415803563594818 TRAIN  loss dict:  {'classification_loss': 0.9415803563594818}
2025-01-13 08:26:00,765 [INFO] Step[950/2713]: training loss : 0.9380313098430634 TRAIN  loss dict:  {'classification_loss': 0.9380313098430634}
2025-01-13 08:26:12,617 [INFO] Step[1000/2713]: training loss : 0.9388704419136047 TRAIN  loss dict:  {'classification_loss': 0.9388704419136047}
2025-01-13 08:26:24,554 [INFO] Step[1050/2713]: training loss : 0.9352400410175323 TRAIN  loss dict:  {'classification_loss': 0.9352400410175323}
2025-01-13 08:26:36,459 [INFO] Step[1100/2713]: training loss : 0.9365873527526856 TRAIN  loss dict:  {'classification_loss': 0.9365873527526856}
2025-01-13 08:26:48,352 [INFO] Step[1150/2713]: training loss : 0.9360596430301666 TRAIN  loss dict:  {'classification_loss': 0.9360596430301666}
2025-01-13 08:27:00,278 [INFO] Step[1200/2713]: training loss : 0.9372676026821136 TRAIN  loss dict:  {'classification_loss': 0.9372676026821136}
2025-01-13 08:27:12,182 [INFO] Step[1250/2713]: training loss : 0.9353005087375641 TRAIN  loss dict:  {'classification_loss': 0.9353005087375641}
2025-01-13 08:27:24,064 [INFO] Step[1300/2713]: training loss : 0.9365922904014587 TRAIN  loss dict:  {'classification_loss': 0.9365922904014587}
2025-01-13 08:27:35,905 [INFO] Step[1350/2713]: training loss : 0.9371892583370208 TRAIN  loss dict:  {'classification_loss': 0.9371892583370208}
2025-01-13 08:27:47,782 [INFO] Step[1400/2713]: training loss : 0.9405463480949402 TRAIN  loss dict:  {'classification_loss': 0.9405463480949402}
2025-01-13 08:27:59,691 [INFO] Step[1450/2713]: training loss : 0.9366514265537262 TRAIN  loss dict:  {'classification_loss': 0.9366514265537262}
2025-01-13 08:28:11,597 [INFO] Step[1500/2713]: training loss : 0.9374309480190277 TRAIN  loss dict:  {'classification_loss': 0.9374309480190277}
2025-01-13 08:28:23,496 [INFO] Step[1550/2713]: training loss : 0.9371210086345673 TRAIN  loss dict:  {'classification_loss': 0.9371210086345673}
2025-01-13 08:28:35,404 [INFO] Step[1600/2713]: training loss : 0.9361818647384643 TRAIN  loss dict:  {'classification_loss': 0.9361818647384643}
2025-01-13 08:28:47,332 [INFO] Step[1650/2713]: training loss : 0.9429627645015717 TRAIN  loss dict:  {'classification_loss': 0.9429627645015717}
2025-01-13 08:28:59,223 [INFO] Step[1700/2713]: training loss : 0.9402251195907593 TRAIN  loss dict:  {'classification_loss': 0.9402251195907593}
2025-01-13 08:29:11,107 [INFO] Step[1750/2713]: training loss : 0.9411963403224946 TRAIN  loss dict:  {'classification_loss': 0.9411963403224946}
2025-01-13 08:29:23,013 [INFO] Step[1800/2713]: training loss : 0.9376186096668243 TRAIN  loss dict:  {'classification_loss': 0.9376186096668243}
2025-01-13 08:29:34,935 [INFO] Step[1850/2713]: training loss : 0.9374291622638702 TRAIN  loss dict:  {'classification_loss': 0.9374291622638702}
2025-01-13 08:29:46,862 [INFO] Step[1900/2713]: training loss : 0.938735282421112 TRAIN  loss dict:  {'classification_loss': 0.938735282421112}
2025-01-13 08:29:58,795 [INFO] Step[1950/2713]: training loss : 0.9392001235485077 TRAIN  loss dict:  {'classification_loss': 0.9392001235485077}
2025-01-13 08:30:10,717 [INFO] Step[2000/2713]: training loss : 0.9399466526508331 TRAIN  loss dict:  {'classification_loss': 0.9399466526508331}
2025-01-13 08:30:22,628 [INFO] Step[2050/2713]: training loss : 0.9485564601421356 TRAIN  loss dict:  {'classification_loss': 0.9485564601421356}
2025-01-13 08:30:34,731 [INFO] Step[2100/2713]: training loss : 0.9435246503353119 TRAIN  loss dict:  {'classification_loss': 0.9435246503353119}
2025-01-13 08:30:47,185 [INFO] Step[2150/2713]: training loss : 0.9383552432060241 TRAIN  loss dict:  {'classification_loss': 0.9383552432060241}
2025-01-13 08:30:59,606 [INFO] Step[2200/2713]: training loss : 0.9388654446601867 TRAIN  loss dict:  {'classification_loss': 0.9388654446601867}
2025-01-13 08:31:11,841 [INFO] Step[2250/2713]: training loss : 0.9372420692443848 TRAIN  loss dict:  {'classification_loss': 0.9372420692443848}
2025-01-13 08:31:24,587 [INFO] Step[2300/2713]: training loss : 0.9363717675209046 TRAIN  loss dict:  {'classification_loss': 0.9363717675209046}
2025-01-13 08:31:36,792 [INFO] Step[2350/2713]: training loss : 0.9363901138305664 TRAIN  loss dict:  {'classification_loss': 0.9363901138305664}
2025-01-13 08:31:49,471 [INFO] Step[2400/2713]: training loss : 0.9370309937000275 TRAIN  loss dict:  {'classification_loss': 0.9370309937000275}
2025-01-13 08:32:02,869 [INFO] Step[2450/2713]: training loss : 0.9362105441093445 TRAIN  loss dict:  {'classification_loss': 0.9362105441093445}
2025-01-13 08:32:16,492 [INFO] Step[2500/2713]: training loss : 0.9382471489906311 TRAIN  loss dict:  {'classification_loss': 0.9382471489906311}
2025-01-13 08:32:28,757 [INFO] Step[2550/2713]: training loss : 0.9363355004787445 TRAIN  loss dict:  {'classification_loss': 0.9363355004787445}
2025-01-13 08:32:40,643 [INFO] Step[2600/2713]: training loss : 0.9368149149417877 TRAIN  loss dict:  {'classification_loss': 0.9368149149417877}
2025-01-13 08:32:52,509 [INFO] Step[2650/2713]: training loss : 0.9415080320835113 TRAIN  loss dict:  {'classification_loss': 0.9415080320835113}
2025-01-13 08:33:04,366 [INFO] Step[2700/2713]: training loss : 0.9413166117668151 TRAIN  loss dict:  {'classification_loss': 0.9413166117668151}
2025-01-13 08:34:30,576 [INFO] Label accuracies statistics:
2025-01-13 08:34:30,576 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 0.75, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 1.0, 261: 1.0, 262: 1.0, 263: 1.0, 264: 0.75, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 0.75, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 08:34:30,578 [INFO] [85] TRAIN  loss: 0.9380746509947111 acc: 0.9997542695662858
2025-01-13 08:34:30,578 [INFO] [85] TRAIN  loss dict: {'classification_loss': 0.9380746509947111}
2025-01-13 08:34:30,578 [INFO] [85] VALIDATION loss: 1.712471099052214 VALIDATION acc: 0.8332288401253919
2025-01-13 08:34:30,578 [INFO] [85] VALIDATION loss dict: {'classification_loss': 1.712471099052214}
2025-01-13 08:34:30,578 [INFO] 
2025-01-13 08:34:48,712 [INFO] Step[50/2713]: training loss : 0.9363607490062713 TRAIN  loss dict:  {'classification_loss': 0.9363607490062713}
2025-01-13 08:35:00,605 [INFO] Step[100/2713]: training loss : 0.9366438782215118 TRAIN  loss dict:  {'classification_loss': 0.9366438782215118}
2025-01-13 08:35:12,573 [INFO] Step[150/2713]: training loss : 0.9368303143978118 TRAIN  loss dict:  {'classification_loss': 0.9368303143978118}
2025-01-13 08:35:24,494 [INFO] Step[200/2713]: training loss : 0.936131409406662 TRAIN  loss dict:  {'classification_loss': 0.936131409406662}
2025-01-13 08:35:36,407 [INFO] Step[250/2713]: training loss : 0.9383729636669159 TRAIN  loss dict:  {'classification_loss': 0.9383729636669159}
2025-01-13 08:35:48,345 [INFO] Step[300/2713]: training loss : 0.9400338685512543 TRAIN  loss dict:  {'classification_loss': 0.9400338685512543}
2025-01-13 08:36:00,261 [INFO] Step[350/2713]: training loss : 0.9368175160884857 TRAIN  loss dict:  {'classification_loss': 0.9368175160884857}
2025-01-13 08:36:12,178 [INFO] Step[400/2713]: training loss : 0.9380769681930542 TRAIN  loss dict:  {'classification_loss': 0.9380769681930542}
2025-01-13 08:36:24,116 [INFO] Step[450/2713]: training loss : 0.937227121591568 TRAIN  loss dict:  {'classification_loss': 0.937227121591568}
2025-01-13 08:36:36,010 [INFO] Step[500/2713]: training loss : 0.9364369475841522 TRAIN  loss dict:  {'classification_loss': 0.9364369475841522}
2025-01-13 08:36:47,880 [INFO] Step[550/2713]: training loss : 0.9371509075164794 TRAIN  loss dict:  {'classification_loss': 0.9371509075164794}
2025-01-13 08:36:59,794 [INFO] Step[600/2713]: training loss : 0.9369492793083191 TRAIN  loss dict:  {'classification_loss': 0.9369492793083191}
2025-01-13 08:37:11,758 [INFO] Step[650/2713]: training loss : 0.9376029741764068 TRAIN  loss dict:  {'classification_loss': 0.9376029741764068}
2025-01-13 08:37:23,653 [INFO] Step[700/2713]: training loss : 0.937812386751175 TRAIN  loss dict:  {'classification_loss': 0.937812386751175}
2025-01-13 08:37:35,537 [INFO] Step[750/2713]: training loss : 0.9382251334190369 TRAIN  loss dict:  {'classification_loss': 0.9382251334190369}
2025-01-13 08:37:47,423 [INFO] Step[800/2713]: training loss : 0.9383094561100006 TRAIN  loss dict:  {'classification_loss': 0.9383094561100006}
2025-01-13 08:37:59,332 [INFO] Step[850/2713]: training loss : 0.9382784569263458 TRAIN  loss dict:  {'classification_loss': 0.9382784569263458}
2025-01-13 08:38:11,251 [INFO] Step[900/2713]: training loss : 0.9356841647624969 TRAIN  loss dict:  {'classification_loss': 0.9356841647624969}
2025-01-13 08:38:23,190 [INFO] Step[950/2713]: training loss : 0.9355648982524872 TRAIN  loss dict:  {'classification_loss': 0.9355648982524872}
2025-01-13 08:38:35,153 [INFO] Step[1000/2713]: training loss : 0.9353632009029389 TRAIN  loss dict:  {'classification_loss': 0.9353632009029389}
2025-01-13 08:38:47,059 [INFO] Step[1050/2713]: training loss : 0.9376630818843842 TRAIN  loss dict:  {'classification_loss': 0.9376630818843842}
2025-01-13 08:38:58,952 [INFO] Step[1100/2713]: training loss : 0.9368760597705841 TRAIN  loss dict:  {'classification_loss': 0.9368760597705841}
2025-01-13 08:39:10,862 [INFO] Step[1150/2713]: training loss : 0.9399757218360901 TRAIN  loss dict:  {'classification_loss': 0.9399757218360901}
2025-01-13 08:39:22,817 [INFO] Step[1200/2713]: training loss : 0.9339254021644592 TRAIN  loss dict:  {'classification_loss': 0.9339254021644592}
2025-01-13 08:39:34,740 [INFO] Step[1250/2713]: training loss : 0.9411931645870208 TRAIN  loss dict:  {'classification_loss': 0.9411931645870208}
2025-01-13 08:39:46,661 [INFO] Step[1300/2713]: training loss : 0.9385179817676544 TRAIN  loss dict:  {'classification_loss': 0.9385179817676544}
2025-01-13 08:39:58,583 [INFO] Step[1350/2713]: training loss : 0.9369717645645141 TRAIN  loss dict:  {'classification_loss': 0.9369717645645141}
2025-01-13 08:40:10,494 [INFO] Step[1400/2713]: training loss : 0.9374456560611725 TRAIN  loss dict:  {'classification_loss': 0.9374456560611725}
2025-01-13 08:40:22,404 [INFO] Step[1450/2713]: training loss : 0.9405150139331817 TRAIN  loss dict:  {'classification_loss': 0.9405150139331817}
2025-01-13 08:40:34,303 [INFO] Step[1500/2713]: training loss : 0.9377866744995117 TRAIN  loss dict:  {'classification_loss': 0.9377866744995117}
2025-01-13 08:40:46,233 [INFO] Step[1550/2713]: training loss : 0.9395507323741913 TRAIN  loss dict:  {'classification_loss': 0.9395507323741913}
2025-01-13 08:40:58,063 [INFO] Step[1600/2713]: training loss : 0.9351551556587219 TRAIN  loss dict:  {'classification_loss': 0.9351551556587219}
2025-01-13 08:41:09,956 [INFO] Step[1650/2713]: training loss : 0.9406303596496582 TRAIN  loss dict:  {'classification_loss': 0.9406303596496582}
2025-01-13 08:41:21,878 [INFO] Step[1700/2713]: training loss : 0.9350882601737976 TRAIN  loss dict:  {'classification_loss': 0.9350882601737976}
2025-01-13 08:41:33,770 [INFO] Step[1750/2713]: training loss : 0.9380243611335755 TRAIN  loss dict:  {'classification_loss': 0.9380243611335755}
2025-01-13 08:41:45,666 [INFO] Step[1800/2713]: training loss : 0.9356879663467407 TRAIN  loss dict:  {'classification_loss': 0.9356879663467407}
2025-01-13 08:41:57,556 [INFO] Step[1850/2713]: training loss : 0.9373444139957428 TRAIN  loss dict:  {'classification_loss': 0.9373444139957428}
2025-01-13 08:42:09,438 [INFO] Step[1900/2713]: training loss : 0.935231351852417 TRAIN  loss dict:  {'classification_loss': 0.935231351852417}
2025-01-13 08:42:21,367 [INFO] Step[1950/2713]: training loss : 0.937205274105072 TRAIN  loss dict:  {'classification_loss': 0.937205274105072}
2025-01-13 08:42:33,287 [INFO] Step[2000/2713]: training loss : 0.9383226954936981 TRAIN  loss dict:  {'classification_loss': 0.9383226954936981}
2025-01-13 08:42:45,206 [INFO] Step[2050/2713]: training loss : 0.9362177979946137 TRAIN  loss dict:  {'classification_loss': 0.9362177979946137}
2025-01-13 08:42:57,118 [INFO] Step[2100/2713]: training loss : 0.9389304971694946 TRAIN  loss dict:  {'classification_loss': 0.9389304971694946}
2025-01-13 08:43:09,008 [INFO] Step[2150/2713]: training loss : 0.935612848997116 TRAIN  loss dict:  {'classification_loss': 0.935612848997116}
2025-01-13 08:43:20,929 [INFO] Step[2200/2713]: training loss : 0.9392311239242553 TRAIN  loss dict:  {'classification_loss': 0.9392311239242553}
2025-01-13 08:43:32,874 [INFO] Step[2250/2713]: training loss : 0.939037891626358 TRAIN  loss dict:  {'classification_loss': 0.939037891626358}
2025-01-13 08:43:44,774 [INFO] Step[2300/2713]: training loss : 0.9354376041889191 TRAIN  loss dict:  {'classification_loss': 0.9354376041889191}
2025-01-13 08:43:56,660 [INFO] Step[2350/2713]: training loss : 0.9377592432498932 TRAIN  loss dict:  {'classification_loss': 0.9377592432498932}
2025-01-13 08:44:08,559 [INFO] Step[2400/2713]: training loss : 0.9345230197906494 TRAIN  loss dict:  {'classification_loss': 0.9345230197906494}
2025-01-13 08:44:20,481 [INFO] Step[2450/2713]: training loss : 0.936887298822403 TRAIN  loss dict:  {'classification_loss': 0.936887298822403}
2025-01-13 08:44:32,376 [INFO] Step[2500/2713]: training loss : 0.9384326934814453 TRAIN  loss dict:  {'classification_loss': 0.9384326934814453}
2025-01-13 08:44:44,292 [INFO] Step[2550/2713]: training loss : 0.9399010348320007 TRAIN  loss dict:  {'classification_loss': 0.9399010348320007}
2025-01-13 08:44:56,189 [INFO] Step[2600/2713]: training loss : 0.9413727915287018 TRAIN  loss dict:  {'classification_loss': 0.9413727915287018}
2025-01-13 08:45:08,124 [INFO] Step[2650/2713]: training loss : 0.9462029814720154 TRAIN  loss dict:  {'classification_loss': 0.9462029814720154}
2025-01-13 08:45:19,984 [INFO] Step[2700/2713]: training loss : 0.9364560294151306 TRAIN  loss dict:  {'classification_loss': 0.9364560294151306}
2025-01-13 08:46:45,347 [INFO] Label accuracies statistics:
2025-01-13 08:46:45,347 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 0.75, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 1.0, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.5, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.5, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 08:46:45,349 [INFO] [86] TRAIN  loss: 0.9377092603735496 acc: 0.9998771347831429
2025-01-13 08:46:45,349 [INFO] [86] TRAIN  loss dict: {'classification_loss': 0.9377092603735496}
2025-01-13 08:46:45,349 [INFO] [86] VALIDATION loss: 1.6840202375910336 VALIDATION acc: 0.8313479623824451
2025-01-13 08:46:45,349 [INFO] [86] VALIDATION loss dict: {'classification_loss': 1.6840202375910336}
2025-01-13 08:46:45,350 [INFO] 
2025-01-13 08:47:03,151 [INFO] Step[50/2713]: training loss : 0.9361221742630005 TRAIN  loss dict:  {'classification_loss': 0.9361221742630005}
2025-01-13 08:47:15,064 [INFO] Step[100/2713]: training loss : 0.9377032923698425 TRAIN  loss dict:  {'classification_loss': 0.9377032923698425}
2025-01-13 08:47:26,949 [INFO] Step[150/2713]: training loss : 0.9381811583042144 TRAIN  loss dict:  {'classification_loss': 0.9381811583042144}
2025-01-13 08:47:38,868 [INFO] Step[200/2713]: training loss : 0.9388969123363495 TRAIN  loss dict:  {'classification_loss': 0.9388969123363495}
2025-01-13 08:47:50,802 [INFO] Step[250/2713]: training loss : 0.9358590757846832 TRAIN  loss dict:  {'classification_loss': 0.9358590757846832}
2025-01-13 08:48:02,686 [INFO] Step[300/2713]: training loss : 0.9510043096542359 TRAIN  loss dict:  {'classification_loss': 0.9510043096542359}
2025-01-13 08:48:14,584 [INFO] Step[350/2713]: training loss : 0.9367864847183227 TRAIN  loss dict:  {'classification_loss': 0.9367864847183227}
2025-01-13 08:48:26,463 [INFO] Step[400/2713]: training loss : 0.9384334993362426 TRAIN  loss dict:  {'classification_loss': 0.9384334993362426}
2025-01-13 08:48:38,398 [INFO] Step[450/2713]: training loss : 0.9370770835876465 TRAIN  loss dict:  {'classification_loss': 0.9370770835876465}
2025-01-13 08:48:50,324 [INFO] Step[500/2713]: training loss : 0.9360024011135102 TRAIN  loss dict:  {'classification_loss': 0.9360024011135102}
2025-01-13 08:49:02,288 [INFO] Step[550/2713]: training loss : 0.9379335355758667 TRAIN  loss dict:  {'classification_loss': 0.9379335355758667}
2025-01-13 08:49:14,187 [INFO] Step[600/2713]: training loss : 0.9369089424610137 TRAIN  loss dict:  {'classification_loss': 0.9369089424610137}
2025-01-13 08:49:26,107 [INFO] Step[650/2713]: training loss : 0.9371292686462402 TRAIN  loss dict:  {'classification_loss': 0.9371292686462402}
2025-01-13 08:49:38,177 [INFO] Step[700/2713]: training loss : 0.9374002587795257 TRAIN  loss dict:  {'classification_loss': 0.9374002587795257}
2025-01-13 08:49:50,599 [INFO] Step[750/2713]: training loss : 0.9365742862224579 TRAIN  loss dict:  {'classification_loss': 0.9365742862224579}
2025-01-13 08:50:02,948 [INFO] Step[800/2713]: training loss : 0.9364747631549836 TRAIN  loss dict:  {'classification_loss': 0.9364747631549836}
2025-01-13 08:50:15,166 [INFO] Step[850/2713]: training loss : 0.9370970308780671 TRAIN  loss dict:  {'classification_loss': 0.9370970308780671}
2025-01-13 08:50:27,892 [INFO] Step[900/2713]: training loss : 0.9386350095272065 TRAIN  loss dict:  {'classification_loss': 0.9386350095272065}
2025-01-13 08:50:40,209 [INFO] Step[950/2713]: training loss : 0.9381940996646881 TRAIN  loss dict:  {'classification_loss': 0.9381940996646881}
2025-01-13 08:50:52,884 [INFO] Step[1000/2713]: training loss : 0.9363456892967225 TRAIN  loss dict:  {'classification_loss': 0.9363456892967225}
2025-01-13 08:51:06,379 [INFO] Step[1050/2713]: training loss : 0.9354425048828126 TRAIN  loss dict:  {'classification_loss': 0.9354425048828126}
2025-01-13 08:51:20,273 [INFO] Step[1100/2713]: training loss : 0.9357102847099305 TRAIN  loss dict:  {'classification_loss': 0.9357102847099305}
2025-01-13 08:51:32,456 [INFO] Step[1150/2713]: training loss : 0.9370703816413879 TRAIN  loss dict:  {'classification_loss': 0.9370703816413879}
2025-01-13 08:51:44,305 [INFO] Step[1200/2713]: training loss : 0.9360523331165314 TRAIN  loss dict:  {'classification_loss': 0.9360523331165314}
2025-01-13 08:51:56,251 [INFO] Step[1250/2713]: training loss : 0.9371902656555176 TRAIN  loss dict:  {'classification_loss': 0.9371902656555176}
2025-01-13 08:52:08,090 [INFO] Step[1300/2713]: training loss : 0.9369609928131104 TRAIN  loss dict:  {'classification_loss': 0.9369609928131104}
2025-01-13 08:52:20,001 [INFO] Step[1350/2713]: training loss : 0.9369672501087188 TRAIN  loss dict:  {'classification_loss': 0.9369672501087188}
2025-01-13 08:52:31,911 [INFO] Step[1400/2713]: training loss : 0.9353176712989807 TRAIN  loss dict:  {'classification_loss': 0.9353176712989807}
2025-01-13 08:52:43,799 [INFO] Step[1450/2713]: training loss : 0.9379614198207855 TRAIN  loss dict:  {'classification_loss': 0.9379614198207855}
2025-01-13 08:52:55,651 [INFO] Step[1500/2713]: training loss : 0.9371486568450927 TRAIN  loss dict:  {'classification_loss': 0.9371486568450927}
2025-01-13 08:53:07,529 [INFO] Step[1550/2713]: training loss : 0.9374299287796021 TRAIN  loss dict:  {'classification_loss': 0.9374299287796021}
2025-01-13 08:53:19,388 [INFO] Step[1600/2713]: training loss : 0.9359664392471313 TRAIN  loss dict:  {'classification_loss': 0.9359664392471313}
2025-01-13 08:53:31,323 [INFO] Step[1650/2713]: training loss : 0.9375175309181213 TRAIN  loss dict:  {'classification_loss': 0.9375175309181213}
2025-01-13 08:53:43,264 [INFO] Step[1700/2713]: training loss : 0.9359143257141114 TRAIN  loss dict:  {'classification_loss': 0.9359143257141114}
2025-01-13 08:53:55,141 [INFO] Step[1750/2713]: training loss : 0.936336532831192 TRAIN  loss dict:  {'classification_loss': 0.936336532831192}
2025-01-13 08:54:07,112 [INFO] Step[1800/2713]: training loss : 0.936792014837265 TRAIN  loss dict:  {'classification_loss': 0.936792014837265}
2025-01-13 08:54:19,056 [INFO] Step[1850/2713]: training loss : 0.9380359375476837 TRAIN  loss dict:  {'classification_loss': 0.9380359375476837}
2025-01-13 08:54:30,979 [INFO] Step[1900/2713]: training loss : 0.9377222394943238 TRAIN  loss dict:  {'classification_loss': 0.9377222394943238}
2025-01-13 08:54:42,886 [INFO] Step[1950/2713]: training loss : 0.9369375169277191 TRAIN  loss dict:  {'classification_loss': 0.9369375169277191}
2025-01-13 08:54:54,832 [INFO] Step[2000/2713]: training loss : 0.9359896922111511 TRAIN  loss dict:  {'classification_loss': 0.9359896922111511}
2025-01-13 08:55:06,811 [INFO] Step[2050/2713]: training loss : 0.9384532928466797 TRAIN  loss dict:  {'classification_loss': 0.9384532928466797}
2025-01-13 08:55:18,713 [INFO] Step[2100/2713]: training loss : 0.9368963420391083 TRAIN  loss dict:  {'classification_loss': 0.9368963420391083}
2025-01-13 08:55:30,622 [INFO] Step[2150/2713]: training loss : 0.9353672635555267 TRAIN  loss dict:  {'classification_loss': 0.9353672635555267}
2025-01-13 08:55:42,536 [INFO] Step[2200/2713]: training loss : 0.9359768581390381 TRAIN  loss dict:  {'classification_loss': 0.9359768581390381}
2025-01-13 08:55:54,440 [INFO] Step[2250/2713]: training loss : 0.9365587103366851 TRAIN  loss dict:  {'classification_loss': 0.9365587103366851}
2025-01-13 08:56:06,358 [INFO] Step[2300/2713]: training loss : 0.9432742810249328 TRAIN  loss dict:  {'classification_loss': 0.9432742810249328}
2025-01-13 08:56:18,269 [INFO] Step[2350/2713]: training loss : 0.9369521450996399 TRAIN  loss dict:  {'classification_loss': 0.9369521450996399}
2025-01-13 08:56:30,143 [INFO] Step[2400/2713]: training loss : 0.9361071884632111 TRAIN  loss dict:  {'classification_loss': 0.9361071884632111}
2025-01-13 08:56:42,072 [INFO] Step[2450/2713]: training loss : 0.9406668996810913 TRAIN  loss dict:  {'classification_loss': 0.9406668996810913}
2025-01-13 08:56:53,960 [INFO] Step[2500/2713]: training loss : 0.9357470870018005 TRAIN  loss dict:  {'classification_loss': 0.9357470870018005}
2025-01-13 08:57:05,886 [INFO] Step[2550/2713]: training loss : 0.9373253393173218 TRAIN  loss dict:  {'classification_loss': 0.9373253393173218}
2025-01-13 08:57:17,791 [INFO] Step[2600/2713]: training loss : 0.9370999896526336 TRAIN  loss dict:  {'classification_loss': 0.9370999896526336}
2025-01-13 08:57:29,658 [INFO] Step[2650/2713]: training loss : 0.9507883489131927 TRAIN  loss dict:  {'classification_loss': 0.9507883489131927}
2025-01-13 08:57:41,525 [INFO] Step[2700/2713]: training loss : 0.9359354197978973 TRAIN  loss dict:  {'classification_loss': 0.9359354197978973}
2025-01-13 08:59:08,777 [INFO] Label accuracies statistics:
2025-01-13 08:59:08,777 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 0.75, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.0, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 08:59:08,778 [INFO] [87] TRAIN  loss: 0.9375996341619199 acc: 0.9998771347831429
2025-01-13 08:59:08,778 [INFO] [87] TRAIN  loss dict: {'classification_loss': 0.9375996341619199}
2025-01-13 08:59:08,779 [INFO] [87] VALIDATION loss: 1.6842417349492698 VALIDATION acc: 0.8307210031347962
2025-01-13 08:59:08,779 [INFO] [87] VALIDATION loss dict: {'classification_loss': 1.6842417349492698}
2025-01-13 08:59:08,779 [INFO] 
2025-01-13 08:59:26,102 [INFO] Step[50/2713]: training loss : 0.9361283552646636 TRAIN  loss dict:  {'classification_loss': 0.9361283552646636}
2025-01-13 08:59:38,008 [INFO] Step[100/2713]: training loss : 0.9367000842094422 TRAIN  loss dict:  {'classification_loss': 0.9367000842094422}
2025-01-13 08:59:49,942 [INFO] Step[150/2713]: training loss : 0.9372195029258727 TRAIN  loss dict:  {'classification_loss': 0.9372195029258727}
2025-01-13 09:00:01,807 [INFO] Step[200/2713]: training loss : 0.9381140851974488 TRAIN  loss dict:  {'classification_loss': 0.9381140851974488}
2025-01-13 09:00:13,800 [INFO] Step[250/2713]: training loss : 0.9376654946804046 TRAIN  loss dict:  {'classification_loss': 0.9376654946804046}
2025-01-13 09:00:25,734 [INFO] Step[300/2713]: training loss : 0.9361719393730163 TRAIN  loss dict:  {'classification_loss': 0.9361719393730163}
2025-01-13 09:00:37,642 [INFO] Step[350/2713]: training loss : 0.938389481306076 TRAIN  loss dict:  {'classification_loss': 0.938389481306076}
2025-01-13 09:00:49,580 [INFO] Step[400/2713]: training loss : 0.9360759425163269 TRAIN  loss dict:  {'classification_loss': 0.9360759425163269}
2025-01-13 09:01:01,498 [INFO] Step[450/2713]: training loss : 0.9456793880462646 TRAIN  loss dict:  {'classification_loss': 0.9456793880462646}
2025-01-13 09:01:13,459 [INFO] Step[500/2713]: training loss : 0.9351112067699432 TRAIN  loss dict:  {'classification_loss': 0.9351112067699432}
2025-01-13 09:01:25,333 [INFO] Step[550/2713]: training loss : 0.9358709156513214 TRAIN  loss dict:  {'classification_loss': 0.9358709156513214}
2025-01-13 09:01:37,217 [INFO] Step[600/2713]: training loss : 0.9349510860443115 TRAIN  loss dict:  {'classification_loss': 0.9349510860443115}
2025-01-13 09:01:49,137 [INFO] Step[650/2713]: training loss : 0.9375384712219238 TRAIN  loss dict:  {'classification_loss': 0.9375384712219238}
2025-01-13 09:02:01,020 [INFO] Step[700/2713]: training loss : 0.9363590216636658 TRAIN  loss dict:  {'classification_loss': 0.9363590216636658}
2025-01-13 09:02:12,935 [INFO] Step[750/2713]: training loss : 0.9379297161102295 TRAIN  loss dict:  {'classification_loss': 0.9379297161102295}
2025-01-13 09:02:24,832 [INFO] Step[800/2713]: training loss : 0.9383301424980164 TRAIN  loss dict:  {'classification_loss': 0.9383301424980164}
2025-01-13 09:02:36,744 [INFO] Step[850/2713]: training loss : 0.9361118519306183 TRAIN  loss dict:  {'classification_loss': 0.9361118519306183}
2025-01-13 09:02:48,668 [INFO] Step[900/2713]: training loss : 0.9387217748165131 TRAIN  loss dict:  {'classification_loss': 0.9387217748165131}
2025-01-13 09:03:00,586 [INFO] Step[950/2713]: training loss : 0.936606924533844 TRAIN  loss dict:  {'classification_loss': 0.936606924533844}
2025-01-13 09:03:12,549 [INFO] Step[1000/2713]: training loss : 0.9448454105854034 TRAIN  loss dict:  {'classification_loss': 0.9448454105854034}
2025-01-13 09:03:24,460 [INFO] Step[1050/2713]: training loss : 0.9357320368289948 TRAIN  loss dict:  {'classification_loss': 0.9357320368289948}
2025-01-13 09:03:36,354 [INFO] Step[1100/2713]: training loss : 0.9366292345523834 TRAIN  loss dict:  {'classification_loss': 0.9366292345523834}
2025-01-13 09:03:48,327 [INFO] Step[1150/2713]: training loss : 0.936329653263092 TRAIN  loss dict:  {'classification_loss': 0.936329653263092}
2025-01-13 09:04:00,239 [INFO] Step[1200/2713]: training loss : 0.9356346106529236 TRAIN  loss dict:  {'classification_loss': 0.9356346106529236}
2025-01-13 09:04:12,117 [INFO] Step[1250/2713]: training loss : 0.9364761674404144 TRAIN  loss dict:  {'classification_loss': 0.9364761674404144}
2025-01-13 09:04:24,012 [INFO] Step[1300/2713]: training loss : 0.9399626338481903 TRAIN  loss dict:  {'classification_loss': 0.9399626338481903}
2025-01-13 09:04:35,915 [INFO] Step[1350/2713]: training loss : 0.935168673992157 TRAIN  loss dict:  {'classification_loss': 0.935168673992157}
2025-01-13 09:04:47,826 [INFO] Step[1400/2713]: training loss : 0.9351249921321869 TRAIN  loss dict:  {'classification_loss': 0.9351249921321869}
2025-01-13 09:04:59,737 [INFO] Step[1450/2713]: training loss : 0.9366643536090851 TRAIN  loss dict:  {'classification_loss': 0.9366643536090851}
2025-01-13 09:05:11,595 [INFO] Step[1500/2713]: training loss : 0.935051656961441 TRAIN  loss dict:  {'classification_loss': 0.935051656961441}
2025-01-13 09:05:23,518 [INFO] Step[1550/2713]: training loss : 0.9374946272373199 TRAIN  loss dict:  {'classification_loss': 0.9374946272373199}
2025-01-13 09:05:35,427 [INFO] Step[1600/2713]: training loss : 0.9367908966541291 TRAIN  loss dict:  {'classification_loss': 0.9367908966541291}
2025-01-13 09:05:47,305 [INFO] Step[1650/2713]: training loss : 0.9366619801521301 TRAIN  loss dict:  {'classification_loss': 0.9366619801521301}
2025-01-13 09:05:59,203 [INFO] Step[1700/2713]: training loss : 0.9367918872833252 TRAIN  loss dict:  {'classification_loss': 0.9367918872833252}
2025-01-13 09:06:11,138 [INFO] Step[1750/2713]: training loss : 0.9348594760894775 TRAIN  loss dict:  {'classification_loss': 0.9348594760894775}
2025-01-13 09:06:23,024 [INFO] Step[1800/2713]: training loss : 0.9385190117359161 TRAIN  loss dict:  {'classification_loss': 0.9385190117359161}
2025-01-13 09:06:34,927 [INFO] Step[1850/2713]: training loss : 0.9366929543018341 TRAIN  loss dict:  {'classification_loss': 0.9366929543018341}
2025-01-13 09:06:46,833 [INFO] Step[1900/2713]: training loss : 0.9366253328323364 TRAIN  loss dict:  {'classification_loss': 0.9366253328323364}
2025-01-13 09:06:58,776 [INFO] Step[1950/2713]: training loss : 0.9364527881145477 TRAIN  loss dict:  {'classification_loss': 0.9364527881145477}
2025-01-13 09:07:10,685 [INFO] Step[2000/2713]: training loss : 0.9385768568515778 TRAIN  loss dict:  {'classification_loss': 0.9385768568515778}
2025-01-13 09:07:22,621 [INFO] Step[2050/2713]: training loss : 0.9354117643833161 TRAIN  loss dict:  {'classification_loss': 0.9354117643833161}
2025-01-13 09:07:34,549 [INFO] Step[2100/2713]: training loss : 0.9360023498535156 TRAIN  loss dict:  {'classification_loss': 0.9360023498535156}
2025-01-13 09:07:46,490 [INFO] Step[2150/2713]: training loss : 0.9346008360385895 TRAIN  loss dict:  {'classification_loss': 0.9346008360385895}
2025-01-13 09:07:58,366 [INFO] Step[2200/2713]: training loss : 0.9357358288764953 TRAIN  loss dict:  {'classification_loss': 0.9357358288764953}
2025-01-13 09:08:10,286 [INFO] Step[2250/2713]: training loss : 0.9370819401741027 TRAIN  loss dict:  {'classification_loss': 0.9370819401741027}
2025-01-13 09:08:22,224 [INFO] Step[2300/2713]: training loss : 0.9383683907985687 TRAIN  loss dict:  {'classification_loss': 0.9383683907985687}
2025-01-13 09:08:34,453 [INFO] Step[2350/2713]: training loss : 0.9368364572525024 TRAIN  loss dict:  {'classification_loss': 0.9368364572525024}
2025-01-13 09:08:46,800 [INFO] Step[2400/2713]: training loss : 0.9352635908126831 TRAIN  loss dict:  {'classification_loss': 0.9352635908126831}
2025-01-13 09:08:59,260 [INFO] Step[2450/2713]: training loss : 0.9357728707790375 TRAIN  loss dict:  {'classification_loss': 0.9357728707790375}
2025-01-13 09:09:11,581 [INFO] Step[2500/2713]: training loss : 0.935987013578415 TRAIN  loss dict:  {'classification_loss': 0.935987013578415}
2025-01-13 09:09:24,172 [INFO] Step[2550/2713]: training loss : 0.9366783583164215 TRAIN  loss dict:  {'classification_loss': 0.9366783583164215}
2025-01-13 09:09:36,479 [INFO] Step[2600/2713]: training loss : 0.93507399559021 TRAIN  loss dict:  {'classification_loss': 0.93507399559021}
2025-01-13 09:09:49,310 [INFO] Step[2650/2713]: training loss : 0.9373398864269257 TRAIN  loss dict:  {'classification_loss': 0.9373398864269257}
2025-01-13 09:10:03,041 [INFO] Step[2700/2713]: training loss : 0.9391146755218506 TRAIN  loss dict:  {'classification_loss': 0.9391146755218506}
2025-01-13 09:11:35,566 [INFO] Label accuracies statistics:
2025-01-13 09:11:35,567 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.5, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.75, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.5, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 1.0, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 1.0, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 0.75, 343: 1.0, 344: 1.0, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 09:11:35,568 [INFO] [88] TRAIN  loss: 0.9369677283471245 acc: 1.0
2025-01-13 09:11:35,568 [INFO] [88] TRAIN  loss dict: {'classification_loss': 0.9369677283471245}
2025-01-13 09:11:35,568 [INFO] [88] VALIDATION loss: 1.6824163275777846 VALIDATION acc: 0.831974921630094
2025-01-13 09:11:35,568 [INFO] [88] VALIDATION loss dict: {'classification_loss': 1.6824163275777846}
2025-01-13 09:11:35,569 [INFO] 
2025-01-13 09:11:53,009 [INFO] Step[50/2713]: training loss : 0.9365444731712341 TRAIN  loss dict:  {'classification_loss': 0.9365444731712341}
2025-01-13 09:12:04,876 [INFO] Step[100/2713]: training loss : 0.9367594754695893 TRAIN  loss dict:  {'classification_loss': 0.9367594754695893}
2025-01-13 09:12:16,796 [INFO] Step[150/2713]: training loss : 0.9354796195030213 TRAIN  loss dict:  {'classification_loss': 0.9354796195030213}
2025-01-13 09:12:28,745 [INFO] Step[200/2713]: training loss : 0.937889769077301 TRAIN  loss dict:  {'classification_loss': 0.937889769077301}
2025-01-13 09:12:40,712 [INFO] Step[250/2713]: training loss : 0.9370241022109985 TRAIN  loss dict:  {'classification_loss': 0.9370241022109985}
2025-01-13 09:12:52,624 [INFO] Step[300/2713]: training loss : 0.9356790339946747 TRAIN  loss dict:  {'classification_loss': 0.9356790339946747}
2025-01-13 09:13:04,558 [INFO] Step[350/2713]: training loss : 0.9358044230937957 TRAIN  loss dict:  {'classification_loss': 0.9358044230937957}
2025-01-13 09:13:16,449 [INFO] Step[400/2713]: training loss : 0.9362648057937623 TRAIN  loss dict:  {'classification_loss': 0.9362648057937623}
2025-01-13 09:13:28,373 [INFO] Step[450/2713]: training loss : 0.9414987409114838 TRAIN  loss dict:  {'classification_loss': 0.9414987409114838}
2025-01-13 09:13:40,312 [INFO] Step[500/2713]: training loss : 0.9370729863643646 TRAIN  loss dict:  {'classification_loss': 0.9370729863643646}
2025-01-13 09:13:52,245 [INFO] Step[550/2713]: training loss : 0.9347298181056977 TRAIN  loss dict:  {'classification_loss': 0.9347298181056977}
2025-01-13 09:14:04,121 [INFO] Step[600/2713]: training loss : 0.9359404301643371 TRAIN  loss dict:  {'classification_loss': 0.9359404301643371}
2025-01-13 09:14:16,063 [INFO] Step[650/2713]: training loss : 0.9354661738872528 TRAIN  loss dict:  {'classification_loss': 0.9354661738872528}
2025-01-13 09:14:28,015 [INFO] Step[700/2713]: training loss : 0.937237617969513 TRAIN  loss dict:  {'classification_loss': 0.937237617969513}
2025-01-13 09:14:39,930 [INFO] Step[750/2713]: training loss : 0.9354423713684082 TRAIN  loss dict:  {'classification_loss': 0.9354423713684082}
2025-01-13 09:14:51,793 [INFO] Step[800/2713]: training loss : 0.9353123438358307 TRAIN  loss dict:  {'classification_loss': 0.9353123438358307}
2025-01-13 09:15:03,722 [INFO] Step[850/2713]: training loss : 0.9355129563808441 TRAIN  loss dict:  {'classification_loss': 0.9355129563808441}
2025-01-13 09:15:15,632 [INFO] Step[900/2713]: training loss : 0.9453845357894898 TRAIN  loss dict:  {'classification_loss': 0.9453845357894898}
2025-01-13 09:15:27,586 [INFO] Step[950/2713]: training loss : 0.937203060388565 TRAIN  loss dict:  {'classification_loss': 0.937203060388565}
2025-01-13 09:15:39,478 [INFO] Step[1000/2713]: training loss : 0.934908230304718 TRAIN  loss dict:  {'classification_loss': 0.934908230304718}
2025-01-13 09:15:51,408 [INFO] Step[1050/2713]: training loss : 0.936692385673523 TRAIN  loss dict:  {'classification_loss': 0.936692385673523}
2025-01-13 09:16:03,323 [INFO] Step[1100/2713]: training loss : 0.9364078617095948 TRAIN  loss dict:  {'classification_loss': 0.9364078617095948}
2025-01-13 09:16:15,260 [INFO] Step[1150/2713]: training loss : 0.9380883932113647 TRAIN  loss dict:  {'classification_loss': 0.9380883932113647}
2025-01-13 09:16:27,187 [INFO] Step[1200/2713]: training loss : 0.9369406044483185 TRAIN  loss dict:  {'classification_loss': 0.9369406044483185}
2025-01-13 09:16:39,124 [INFO] Step[1250/2713]: training loss : 0.9365674889087677 TRAIN  loss dict:  {'classification_loss': 0.9365674889087677}
2025-01-13 09:16:51,003 [INFO] Step[1300/2713]: training loss : 0.9439014947414398 TRAIN  loss dict:  {'classification_loss': 0.9439014947414398}
2025-01-13 09:17:02,941 [INFO] Step[1350/2713]: training loss : 0.9351855802536011 TRAIN  loss dict:  {'classification_loss': 0.9351855802536011}
2025-01-13 09:17:14,876 [INFO] Step[1400/2713]: training loss : 0.9356997573375702 TRAIN  loss dict:  {'classification_loss': 0.9356997573375702}
2025-01-13 09:17:26,767 [INFO] Step[1450/2713]: training loss : 0.970264310836792 TRAIN  loss dict:  {'classification_loss': 0.970264310836792}
2025-01-13 09:17:38,673 [INFO] Step[1500/2713]: training loss : 0.9394085228443145 TRAIN  loss dict:  {'classification_loss': 0.9394085228443145}
2025-01-13 09:17:50,612 [INFO] Step[1550/2713]: training loss : 0.9350338006019592 TRAIN  loss dict:  {'classification_loss': 0.9350338006019592}
2025-01-13 09:18:02,521 [INFO] Step[1600/2713]: training loss : 0.936774057149887 TRAIN  loss dict:  {'classification_loss': 0.936774057149887}
2025-01-13 09:18:14,425 [INFO] Step[1650/2713]: training loss : 0.9418155443668366 TRAIN  loss dict:  {'classification_loss': 0.9418155443668366}
2025-01-13 09:18:26,376 [INFO] Step[1700/2713]: training loss : 0.9349469864368438 TRAIN  loss dict:  {'classification_loss': 0.9349469864368438}
2025-01-13 09:18:38,271 [INFO] Step[1750/2713]: training loss : 0.9381994533538819 TRAIN  loss dict:  {'classification_loss': 0.9381994533538819}
2025-01-13 09:18:50,166 [INFO] Step[1800/2713]: training loss : 0.9354883778095245 TRAIN  loss dict:  {'classification_loss': 0.9354883778095245}
2025-01-13 09:19:02,109 [INFO] Step[1850/2713]: training loss : 0.9364412295818328 TRAIN  loss dict:  {'classification_loss': 0.9364412295818328}
2025-01-13 09:19:14,078 [INFO] Step[1900/2713]: training loss : 0.9378301346302033 TRAIN  loss dict:  {'classification_loss': 0.9378301346302033}
2025-01-13 09:19:26,001 [INFO] Step[1950/2713]: training loss : 0.937213739156723 TRAIN  loss dict:  {'classification_loss': 0.937213739156723}
2025-01-13 09:19:37,872 [INFO] Step[2000/2713]: training loss : 0.9340477192401886 TRAIN  loss dict:  {'classification_loss': 0.9340477192401886}
2025-01-13 09:19:49,833 [INFO] Step[2050/2713]: training loss : 0.9373242390155793 TRAIN  loss dict:  {'classification_loss': 0.9373242390155793}
2025-01-13 09:20:01,705 [INFO] Step[2100/2713]: training loss : 0.9349787282943726 TRAIN  loss dict:  {'classification_loss': 0.9349787282943726}
2025-01-13 09:20:13,608 [INFO] Step[2150/2713]: training loss : 0.9340120232105256 TRAIN  loss dict:  {'classification_loss': 0.9340120232105256}
2025-01-13 09:20:25,525 [INFO] Step[2200/2713]: training loss : 0.9384241223335266 TRAIN  loss dict:  {'classification_loss': 0.9384241223335266}
2025-01-13 09:20:37,470 [INFO] Step[2250/2713]: training loss : 0.937085280418396 TRAIN  loss dict:  {'classification_loss': 0.937085280418396}
2025-01-13 09:20:49,410 [INFO] Step[2300/2713]: training loss : 0.9355977618694306 TRAIN  loss dict:  {'classification_loss': 0.9355977618694306}
2025-01-13 09:21:01,289 [INFO] Step[2350/2713]: training loss : 0.9350470471382141 TRAIN  loss dict:  {'classification_loss': 0.9350470471382141}
2025-01-13 09:21:13,185 [INFO] Step[2400/2713]: training loss : 0.9349475252628326 TRAIN  loss dict:  {'classification_loss': 0.9349475252628326}
2025-01-13 09:21:25,154 [INFO] Step[2450/2713]: training loss : 0.9347196078300476 TRAIN  loss dict:  {'classification_loss': 0.9347196078300476}
2025-01-13 09:21:37,062 [INFO] Step[2500/2713]: training loss : 0.9376161158084869 TRAIN  loss dict:  {'classification_loss': 0.9376161158084869}
2025-01-13 09:21:48,978 [INFO] Step[2550/2713]: training loss : 0.9366331160068512 TRAIN  loss dict:  {'classification_loss': 0.9366331160068512}
2025-01-13 09:22:00,912 [INFO] Step[2600/2713]: training loss : 0.9351544654369355 TRAIN  loss dict:  {'classification_loss': 0.9351544654369355}
2025-01-13 09:22:12,818 [INFO] Step[2650/2713]: training loss : 0.9351620852947236 TRAIN  loss dict:  {'classification_loss': 0.9351620852947236}
2025-01-13 09:22:24,765 [INFO] Step[2700/2713]: training loss : 0.9367307722568512 TRAIN  loss dict:  {'classification_loss': 0.9367307722568512}
2025-01-13 09:23:50,728 [INFO] Label accuracies statistics:
2025-01-13 09:23:50,728 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 1.0, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 0.75, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 1.0, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 0.75, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.5, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 09:23:50,731 [INFO] [89] TRAIN  loss: 0.9373750127948082 acc: 0.9996314043494287
2025-01-13 09:23:50,731 [INFO] [89] TRAIN  loss dict: {'classification_loss': 0.9373750127948082}
2025-01-13 09:23:50,731 [INFO] [89] VALIDATION loss: 1.695845834294656 VALIDATION acc: 0.8307210031347962
2025-01-13 09:23:50,731 [INFO] [89] VALIDATION loss dict: {'classification_loss': 1.695845834294656}
2025-01-13 09:23:50,731 [INFO] 
2025-01-13 09:24:08,081 [INFO] Step[50/2713]: training loss : 0.9367784249782563 TRAIN  loss dict:  {'classification_loss': 0.9367784249782563}
2025-01-13 09:24:19,991 [INFO] Step[100/2713]: training loss : 0.9426098608970642 TRAIN  loss dict:  {'classification_loss': 0.9426098608970642}
2025-01-13 09:24:31,941 [INFO] Step[150/2713]: training loss : 0.9374903333187103 TRAIN  loss dict:  {'classification_loss': 0.9374903333187103}
2025-01-13 09:24:43,877 [INFO] Step[200/2713]: training loss : 0.9360985350608826 TRAIN  loss dict:  {'classification_loss': 0.9360985350608826}
2025-01-13 09:24:55,809 [INFO] Step[250/2713]: training loss : 0.9380154836177826 TRAIN  loss dict:  {'classification_loss': 0.9380154836177826}
2025-01-13 09:25:07,714 [INFO] Step[300/2713]: training loss : 0.9372236549854278 TRAIN  loss dict:  {'classification_loss': 0.9372236549854278}
2025-01-13 09:25:19,658 [INFO] Step[350/2713]: training loss : 0.9360162746906281 TRAIN  loss dict:  {'classification_loss': 0.9360162746906281}
2025-01-13 09:25:31,568 [INFO] Step[400/2713]: training loss : 0.9377805757522583 TRAIN  loss dict:  {'classification_loss': 0.9377805757522583}
2025-01-13 09:25:43,467 [INFO] Step[450/2713]: training loss : 0.9357539546489716 TRAIN  loss dict:  {'classification_loss': 0.9357539546489716}
2025-01-13 09:25:55,389 [INFO] Step[500/2713]: training loss : 0.9359465277194977 TRAIN  loss dict:  {'classification_loss': 0.9359465277194977}
2025-01-13 09:26:07,324 [INFO] Step[550/2713]: training loss : 0.9376039159297943 TRAIN  loss dict:  {'classification_loss': 0.9376039159297943}
2025-01-13 09:26:19,282 [INFO] Step[600/2713]: training loss : 0.9354056251049042 TRAIN  loss dict:  {'classification_loss': 0.9354056251049042}
2025-01-13 09:26:31,182 [INFO] Step[650/2713]: training loss : 0.9383902990818024 TRAIN  loss dict:  {'classification_loss': 0.9383902990818024}
2025-01-13 09:26:43,059 [INFO] Step[700/2713]: training loss : 0.9370810687541962 TRAIN  loss dict:  {'classification_loss': 0.9370810687541962}
2025-01-13 09:26:54,979 [INFO] Step[750/2713]: training loss : 0.9343575143814087 TRAIN  loss dict:  {'classification_loss': 0.9343575143814087}
2025-01-13 09:27:06,937 [INFO] Step[800/2713]: training loss : 0.9406558144092559 TRAIN  loss dict:  {'classification_loss': 0.9406558144092559}
2025-01-13 09:27:18,844 [INFO] Step[850/2713]: training loss : 0.9355503845214844 TRAIN  loss dict:  {'classification_loss': 0.9355503845214844}
2025-01-13 09:27:30,972 [INFO] Step[900/2713]: training loss : 0.9367391037940979 TRAIN  loss dict:  {'classification_loss': 0.9367391037940979}
2025-01-13 09:27:43,430 [INFO] Step[950/2713]: training loss : 0.9366963708400726 TRAIN  loss dict:  {'classification_loss': 0.9366963708400726}
2025-01-13 09:27:55,853 [INFO] Step[1000/2713]: training loss : 0.9354086768627167 TRAIN  loss dict:  {'classification_loss': 0.9354086768627167}
2025-01-13 09:28:08,102 [INFO] Step[1050/2713]: training loss : 0.9348870837688446 TRAIN  loss dict:  {'classification_loss': 0.9348870837688446}
2025-01-13 09:28:20,885 [INFO] Step[1100/2713]: training loss : 0.9345648574829102 TRAIN  loss dict:  {'classification_loss': 0.9345648574829102}
2025-01-13 09:28:33,122 [INFO] Step[1150/2713]: training loss : 0.9351378691196441 TRAIN  loss dict:  {'classification_loss': 0.9351378691196441}
2025-01-13 09:28:45,828 [INFO] Step[1200/2713]: training loss : 0.9364998054504394 TRAIN  loss dict:  {'classification_loss': 0.9364998054504394}
2025-01-13 09:28:59,421 [INFO] Step[1250/2713]: training loss : 0.9382217729091644 TRAIN  loss dict:  {'classification_loss': 0.9382217729091644}
2025-01-13 09:29:12,955 [INFO] Step[1300/2713]: training loss : 0.9364997386932373 TRAIN  loss dict:  {'classification_loss': 0.9364997386932373}
2025-01-13 09:29:25,139 [INFO] Step[1350/2713]: training loss : 0.9359359920024872 TRAIN  loss dict:  {'classification_loss': 0.9359359920024872}
2025-01-13 09:29:37,050 [INFO] Step[1400/2713]: training loss : 0.9379176199436188 TRAIN  loss dict:  {'classification_loss': 0.9379176199436188}
2025-01-13 09:29:48,913 [INFO] Step[1450/2713]: training loss : 0.9357897126674652 TRAIN  loss dict:  {'classification_loss': 0.9357897126674652}
2025-01-13 09:30:00,773 [INFO] Step[1500/2713]: training loss : 0.9358186936378479 TRAIN  loss dict:  {'classification_loss': 0.9358186936378479}
2025-01-13 09:30:12,649 [INFO] Step[1550/2713]: training loss : 0.937912039756775 TRAIN  loss dict:  {'classification_loss': 0.937912039756775}
2025-01-13 09:30:24,527 [INFO] Step[1600/2713]: training loss : 0.9355865740776061 TRAIN  loss dict:  {'classification_loss': 0.9355865740776061}
2025-01-13 09:30:36,448 [INFO] Step[1650/2713]: training loss : 0.9355391693115235 TRAIN  loss dict:  {'classification_loss': 0.9355391693115235}
2025-01-13 09:30:48,348 [INFO] Step[1700/2713]: training loss : 0.9360030901432037 TRAIN  loss dict:  {'classification_loss': 0.9360030901432037}
2025-01-13 09:31:00,259 [INFO] Step[1750/2713]: training loss : 0.9364805817604065 TRAIN  loss dict:  {'classification_loss': 0.9364805817604065}
2025-01-13 09:31:12,150 [INFO] Step[1800/2713]: training loss : 0.935634229183197 TRAIN  loss dict:  {'classification_loss': 0.935634229183197}
2025-01-13 09:31:24,081 [INFO] Step[1850/2713]: training loss : 0.9351970016956329 TRAIN  loss dict:  {'classification_loss': 0.9351970016956329}
2025-01-13 09:31:36,022 [INFO] Step[1900/2713]: training loss : 0.9350320792198181 TRAIN  loss dict:  {'classification_loss': 0.9350320792198181}
2025-01-13 09:31:47,936 [INFO] Step[1950/2713]: training loss : 0.9352908945083618 TRAIN  loss dict:  {'classification_loss': 0.9352908945083618}
2025-01-13 09:31:59,891 [INFO] Step[2000/2713]: training loss : 0.9349138295650482 TRAIN  loss dict:  {'classification_loss': 0.9349138295650482}
2025-01-13 09:32:11,839 [INFO] Step[2050/2713]: training loss : 0.9362164258956909 TRAIN  loss dict:  {'classification_loss': 0.9362164258956909}
2025-01-13 09:32:23,752 [INFO] Step[2100/2713]: training loss : 0.9372468328475952 TRAIN  loss dict:  {'classification_loss': 0.9372468328475952}
2025-01-13 09:32:35,703 [INFO] Step[2150/2713]: training loss : 0.9354989492893219 TRAIN  loss dict:  {'classification_loss': 0.9354989492893219}
2025-01-13 09:32:47,644 [INFO] Step[2200/2713]: training loss : 0.9368513524532318 TRAIN  loss dict:  {'classification_loss': 0.9368513524532318}
2025-01-13 09:32:59,579 [INFO] Step[2250/2713]: training loss : 0.9350567066669464 TRAIN  loss dict:  {'classification_loss': 0.9350567066669464}
2025-01-13 09:33:11,491 [INFO] Step[2300/2713]: training loss : 0.9362116456031799 TRAIN  loss dict:  {'classification_loss': 0.9362116456031799}
2025-01-13 09:33:23,404 [INFO] Step[2350/2713]: training loss : 0.9355106401443481 TRAIN  loss dict:  {'classification_loss': 0.9355106401443481}
2025-01-13 09:33:35,347 [INFO] Step[2400/2713]: training loss : 0.9360863244533539 TRAIN  loss dict:  {'classification_loss': 0.9360863244533539}
2025-01-13 09:33:47,278 [INFO] Step[2450/2713]: training loss : 0.9352623128890991 TRAIN  loss dict:  {'classification_loss': 0.9352623128890991}
2025-01-13 09:33:59,165 [INFO] Step[2500/2713]: training loss : 0.9355909311771393 TRAIN  loss dict:  {'classification_loss': 0.9355909311771393}
2025-01-13 09:34:11,118 [INFO] Step[2550/2713]: training loss : 0.9361118865013123 TRAIN  loss dict:  {'classification_loss': 0.9361118865013123}
2025-01-13 09:34:22,990 [INFO] Step[2600/2713]: training loss : 0.9377790224552155 TRAIN  loss dict:  {'classification_loss': 0.9377790224552155}
2025-01-13 09:34:34,922 [INFO] Step[2650/2713]: training loss : 0.9366673576831818 TRAIN  loss dict:  {'classification_loss': 0.9366673576831818}
2025-01-13 09:34:46,795 [INFO] Step[2700/2713]: training loss : 0.9361730515956879 TRAIN  loss dict:  {'classification_loss': 0.9361730515956879}
2025-01-13 09:36:13,891 [INFO] Label accuracies statistics:
2025-01-13 09:36:13,892 [INFO] {0: 0.6666666666666666, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.5, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 1.0, 260: 0.5, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.5, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 09:36:13,893 [INFO] [90] TRAIN  loss: 0.9364481370166972 acc: 1.0
2025-01-13 09:36:13,893 [INFO] [90] TRAIN  loss dict: {'classification_loss': 0.9364481370166972}
2025-01-13 09:36:13,894 [INFO] [90] VALIDATION loss: 1.68084480338975 VALIDATION acc: 0.8432601880877743
2025-01-13 09:36:13,894 [INFO] [90] VALIDATION loss dict: {'classification_loss': 1.68084480338975}
2025-01-13 09:36:13,894 [INFO] 
2025-01-13 09:36:31,234 [INFO] Step[50/2713]: training loss : 0.9364325892925263 TRAIN  loss dict:  {'classification_loss': 0.9364325892925263}
2025-01-13 09:36:43,187 [INFO] Step[100/2713]: training loss : 0.9368430233001709 TRAIN  loss dict:  {'classification_loss': 0.9368430233001709}
2025-01-13 09:36:55,101 [INFO] Step[150/2713]: training loss : 0.935038514137268 TRAIN  loss dict:  {'classification_loss': 0.935038514137268}
2025-01-13 09:37:07,028 [INFO] Step[200/2713]: training loss : 0.9350119769573212 TRAIN  loss dict:  {'classification_loss': 0.9350119769573212}
2025-01-13 09:37:18,989 [INFO] Step[250/2713]: training loss : 0.93332639336586 TRAIN  loss dict:  {'classification_loss': 0.93332639336586}
2025-01-13 09:37:30,936 [INFO] Step[300/2713]: training loss : 0.934224717617035 TRAIN  loss dict:  {'classification_loss': 0.934224717617035}
2025-01-13 09:37:42,875 [INFO] Step[350/2713]: training loss : 0.9381748473644257 TRAIN  loss dict:  {'classification_loss': 0.9381748473644257}
2025-01-13 09:37:54,781 [INFO] Step[400/2713]: training loss : 0.9363112545013428 TRAIN  loss dict:  {'classification_loss': 0.9363112545013428}
2025-01-13 09:38:06,684 [INFO] Step[450/2713]: training loss : 0.9350549638271332 TRAIN  loss dict:  {'classification_loss': 0.9350549638271332}
2025-01-13 09:38:18,631 [INFO] Step[500/2713]: training loss : 0.9358853554725647 TRAIN  loss dict:  {'classification_loss': 0.9358853554725647}
2025-01-13 09:38:30,536 [INFO] Step[550/2713]: training loss : 0.9356291449069977 TRAIN  loss dict:  {'classification_loss': 0.9356291449069977}
2025-01-13 09:38:42,419 [INFO] Step[600/2713]: training loss : 0.9358114027976989 TRAIN  loss dict:  {'classification_loss': 0.9358114027976989}
2025-01-13 09:38:54,322 [INFO] Step[650/2713]: training loss : 0.9372168731689453 TRAIN  loss dict:  {'classification_loss': 0.9372168731689453}
2025-01-13 09:39:06,250 [INFO] Step[700/2713]: training loss : 0.9365563356876373 TRAIN  loss dict:  {'classification_loss': 0.9365563356876373}
2025-01-13 09:39:18,234 [INFO] Step[750/2713]: training loss : 0.9356013071537018 TRAIN  loss dict:  {'classification_loss': 0.9356013071537018}
2025-01-13 09:39:30,156 [INFO] Step[800/2713]: training loss : 0.9350009405612946 TRAIN  loss dict:  {'classification_loss': 0.9350009405612946}
2025-01-13 09:39:42,052 [INFO] Step[850/2713]: training loss : 0.9364877271652222 TRAIN  loss dict:  {'classification_loss': 0.9364877271652222}
2025-01-13 09:39:53,924 [INFO] Step[900/2713]: training loss : 0.9377502226829528 TRAIN  loss dict:  {'classification_loss': 0.9377502226829528}
2025-01-13 09:40:05,843 [INFO] Step[950/2713]: training loss : 0.9358494889736175 TRAIN  loss dict:  {'classification_loss': 0.9358494889736175}
2025-01-13 09:40:17,788 [INFO] Step[1000/2713]: training loss : 0.9369700646400452 TRAIN  loss dict:  {'classification_loss': 0.9369700646400452}
2025-01-13 09:40:29,689 [INFO] Step[1050/2713]: training loss : 0.9359104478359223 TRAIN  loss dict:  {'classification_loss': 0.9359104478359223}
2025-01-13 09:40:41,577 [INFO] Step[1100/2713]: training loss : 0.9338222646713257 TRAIN  loss dict:  {'classification_loss': 0.9338222646713257}
2025-01-13 09:40:53,517 [INFO] Step[1150/2713]: training loss : 0.9355075764656067 TRAIN  loss dict:  {'classification_loss': 0.9355075764656067}
2025-01-13 09:41:05,447 [INFO] Step[1200/2713]: training loss : 0.9358537447452545 TRAIN  loss dict:  {'classification_loss': 0.9358537447452545}
2025-01-13 09:41:17,374 [INFO] Step[1250/2713]: training loss : 0.9360794913768768 TRAIN  loss dict:  {'classification_loss': 0.9360794913768768}
2025-01-13 09:41:29,298 [INFO] Step[1300/2713]: training loss : 0.9370684957504273 TRAIN  loss dict:  {'classification_loss': 0.9370684957504273}
2025-01-13 09:41:41,250 [INFO] Step[1350/2713]: training loss : 0.9369574213027954 TRAIN  loss dict:  {'classification_loss': 0.9369574213027954}
2025-01-13 09:41:53,151 [INFO] Step[1400/2713]: training loss : 0.9360775935649872 TRAIN  loss dict:  {'classification_loss': 0.9360775935649872}
2025-01-13 09:42:05,077 [INFO] Step[1450/2713]: training loss : 0.9361790740489959 TRAIN  loss dict:  {'classification_loss': 0.9361790740489959}
2025-01-13 09:42:17,020 [INFO] Step[1500/2713]: training loss : 0.9361801493167877 TRAIN  loss dict:  {'classification_loss': 0.9361801493167877}
2025-01-13 09:42:28,971 [INFO] Step[1550/2713]: training loss : 0.9353219211101532 TRAIN  loss dict:  {'classification_loss': 0.9353219211101532}
2025-01-13 09:42:40,913 [INFO] Step[1600/2713]: training loss : 0.9361354804039002 TRAIN  loss dict:  {'classification_loss': 0.9361354804039002}
2025-01-13 09:42:52,799 [INFO] Step[1650/2713]: training loss : 0.9364160203933716 TRAIN  loss dict:  {'classification_loss': 0.9364160203933716}
2025-01-13 09:43:04,729 [INFO] Step[1700/2713]: training loss : 0.9357154595851899 TRAIN  loss dict:  {'classification_loss': 0.9357154595851899}
2025-01-13 09:43:16,671 [INFO] Step[1750/2713]: training loss : 0.9346064293384552 TRAIN  loss dict:  {'classification_loss': 0.9346064293384552}
2025-01-13 09:43:28,609 [INFO] Step[1800/2713]: training loss : 0.9345065498352051 TRAIN  loss dict:  {'classification_loss': 0.9345065498352051}
2025-01-13 09:43:40,565 [INFO] Step[1850/2713]: training loss : 0.9358607685565948 TRAIN  loss dict:  {'classification_loss': 0.9358607685565948}
2025-01-13 09:43:52,511 [INFO] Step[1900/2713]: training loss : 0.9340620374679566 TRAIN  loss dict:  {'classification_loss': 0.9340620374679566}
2025-01-13 09:44:04,452 [INFO] Step[1950/2713]: training loss : 0.9342489564418792 TRAIN  loss dict:  {'classification_loss': 0.9342489564418792}
2025-01-13 09:44:16,384 [INFO] Step[2000/2713]: training loss : 0.9335795164108276 TRAIN  loss dict:  {'classification_loss': 0.9335795164108276}
2025-01-13 09:44:28,354 [INFO] Step[2050/2713]: training loss : 0.9376842260360718 TRAIN  loss dict:  {'classification_loss': 0.9376842260360718}
2025-01-13 09:44:40,313 [INFO] Step[2100/2713]: training loss : 0.936844470500946 TRAIN  loss dict:  {'classification_loss': 0.936844470500946}
2025-01-13 09:44:52,282 [INFO] Step[2150/2713]: training loss : 0.935036015510559 TRAIN  loss dict:  {'classification_loss': 0.935036015510559}
2025-01-13 09:45:04,219 [INFO] Step[2200/2713]: training loss : 0.9353856313228607 TRAIN  loss dict:  {'classification_loss': 0.9353856313228607}
2025-01-13 09:45:16,154 [INFO] Step[2250/2713]: training loss : 0.9357153296470642 TRAIN  loss dict:  {'classification_loss': 0.9357153296470642}
2025-01-13 09:45:28,082 [INFO] Step[2300/2713]: training loss : 0.9347871792316437 TRAIN  loss dict:  {'classification_loss': 0.9347871792316437}
2025-01-13 09:45:40,048 [INFO] Step[2350/2713]: training loss : 0.9346655189990998 TRAIN  loss dict:  {'classification_loss': 0.9346655189990998}
2025-01-13 09:45:52,016 [INFO] Step[2400/2713]: training loss : 0.9371822571754456 TRAIN  loss dict:  {'classification_loss': 0.9371822571754456}
2025-01-13 09:46:03,956 [INFO] Step[2450/2713]: training loss : 0.9355452942848206 TRAIN  loss dict:  {'classification_loss': 0.9355452942848206}
2025-01-13 09:46:15,873 [INFO] Step[2500/2713]: training loss : 0.9341706681251526 TRAIN  loss dict:  {'classification_loss': 0.9341706681251526}
2025-01-13 09:46:28,132 [INFO] Step[2550/2713]: training loss : 0.936380705833435 TRAIN  loss dict:  {'classification_loss': 0.936380705833435}
2025-01-13 09:46:40,599 [INFO] Step[2600/2713]: training loss : 0.9345621311664581 TRAIN  loss dict:  {'classification_loss': 0.9345621311664581}
2025-01-13 09:46:53,036 [INFO] Step[2650/2713]: training loss : 0.9373851394653321 TRAIN  loss dict:  {'classification_loss': 0.9373851394653321}
2025-01-13 09:47:05,397 [INFO] Step[2700/2713]: training loss : 0.9352004086971283 TRAIN  loss dict:  {'classification_loss': 0.9352004086971283}
2025-01-13 09:48:53,773 [INFO] Label accuracies statistics:
2025-01-13 09:48:53,773 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.25, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.5, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.5, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.5, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 1.0, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 1.0, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 09:48:53,775 [INFO] [91] TRAIN  loss: 0.935768181927038 acc: 1.0
2025-01-13 09:48:53,775 [INFO] [91] TRAIN  loss dict: {'classification_loss': 0.935768181927038}
2025-01-13 09:48:53,775 [INFO] [91] VALIDATION loss: 1.7042532791768699 VALIDATION acc: 0.8326018808777429
2025-01-13 09:48:53,775 [INFO] [91] VALIDATION loss dict: {'classification_loss': 1.7042532791768699}
2025-01-13 09:48:53,775 [INFO] 
2025-01-13 09:49:11,629 [INFO] Step[50/2713]: training loss : 0.9352252793312072 TRAIN  loss dict:  {'classification_loss': 0.9352252793312072}
2025-01-13 09:49:23,511 [INFO] Step[100/2713]: training loss : 0.9372419703006745 TRAIN  loss dict:  {'classification_loss': 0.9372419703006745}
2025-01-13 09:49:35,463 [INFO] Step[150/2713]: training loss : 0.9392984068393707 TRAIN  loss dict:  {'classification_loss': 0.9392984068393707}
2025-01-13 09:49:47,352 [INFO] Step[200/2713]: training loss : 0.9351540315151214 TRAIN  loss dict:  {'classification_loss': 0.9351540315151214}
2025-01-13 09:49:59,311 [INFO] Step[250/2713]: training loss : 0.9356463754177093 TRAIN  loss dict:  {'classification_loss': 0.9356463754177093}
2025-01-13 09:50:11,163 [INFO] Step[300/2713]: training loss : 0.936007490158081 TRAIN  loss dict:  {'classification_loss': 0.936007490158081}
2025-01-13 09:50:23,066 [INFO] Step[350/2713]: training loss : 0.9344044888019561 TRAIN  loss dict:  {'classification_loss': 0.9344044888019561}
2025-01-13 09:50:35,034 [INFO] Step[400/2713]: training loss : 0.9363902008533478 TRAIN  loss dict:  {'classification_loss': 0.9363902008533478}
2025-01-13 09:50:46,927 [INFO] Step[450/2713]: training loss : 0.9353213834762574 TRAIN  loss dict:  {'classification_loss': 0.9353213834762574}
2025-01-13 09:50:58,844 [INFO] Step[500/2713]: training loss : 0.9349897968769073 TRAIN  loss dict:  {'classification_loss': 0.9349897968769073}
2025-01-13 09:51:10,748 [INFO] Step[550/2713]: training loss : 0.9350317800045014 TRAIN  loss dict:  {'classification_loss': 0.9350317800045014}
2025-01-13 09:51:22,657 [INFO] Step[600/2713]: training loss : 0.9362294626235962 TRAIN  loss dict:  {'classification_loss': 0.9362294626235962}
2025-01-13 09:51:34,583 [INFO] Step[650/2713]: training loss : 0.9371505010128022 TRAIN  loss dict:  {'classification_loss': 0.9371505010128022}
2025-01-13 09:51:46,509 [INFO] Step[700/2713]: training loss : 0.9368066549301147 TRAIN  loss dict:  {'classification_loss': 0.9368066549301147}
2025-01-13 09:51:58,457 [INFO] Step[750/2713]: training loss : 0.9415537178516388 TRAIN  loss dict:  {'classification_loss': 0.9415537178516388}
2025-01-13 09:52:10,356 [INFO] Step[800/2713]: training loss : 0.9358496081829071 TRAIN  loss dict:  {'classification_loss': 0.9358496081829071}
2025-01-13 09:52:22,252 [INFO] Step[850/2713]: training loss : 0.9357416558265687 TRAIN  loss dict:  {'classification_loss': 0.9357416558265687}
2025-01-13 09:52:34,176 [INFO] Step[900/2713]: training loss : 0.940971702337265 TRAIN  loss dict:  {'classification_loss': 0.940971702337265}
2025-01-13 09:52:46,120 [INFO] Step[950/2713]: training loss : 0.9366748046875 TRAIN  loss dict:  {'classification_loss': 0.9366748046875}
2025-01-13 09:52:58,007 [INFO] Step[1000/2713]: training loss : 0.9343406188488007 TRAIN  loss dict:  {'classification_loss': 0.9343406188488007}
2025-01-13 09:53:09,928 [INFO] Step[1050/2713]: training loss : 0.9365368592739105 TRAIN  loss dict:  {'classification_loss': 0.9365368592739105}
2025-01-13 09:53:21,815 [INFO] Step[1100/2713]: training loss : 0.936392970085144 TRAIN  loss dict:  {'classification_loss': 0.936392970085144}
2025-01-13 09:53:33,745 [INFO] Step[1150/2713]: training loss : 0.9351249182224274 TRAIN  loss dict:  {'classification_loss': 0.9351249182224274}
2025-01-13 09:53:45,665 [INFO] Step[1200/2713]: training loss : 0.9373500978946686 TRAIN  loss dict:  {'classification_loss': 0.9373500978946686}
2025-01-13 09:53:57,581 [INFO] Step[1250/2713]: training loss : 0.9345215737819672 TRAIN  loss dict:  {'classification_loss': 0.9345215737819672}
2025-01-13 09:54:09,524 [INFO] Step[1300/2713]: training loss : 0.9354836893081665 TRAIN  loss dict:  {'classification_loss': 0.9354836893081665}
2025-01-13 09:54:21,458 [INFO] Step[1350/2713]: training loss : 0.9360913252830505 TRAIN  loss dict:  {'classification_loss': 0.9360913252830505}
2025-01-13 09:54:33,394 [INFO] Step[1400/2713]: training loss : 0.9350554621219636 TRAIN  loss dict:  {'classification_loss': 0.9350554621219636}
2025-01-13 09:54:45,344 [INFO] Step[1450/2713]: training loss : 0.9364958047866822 TRAIN  loss dict:  {'classification_loss': 0.9364958047866822}
2025-01-13 09:54:57,276 [INFO] Step[1500/2713]: training loss : 0.9396309733390809 TRAIN  loss dict:  {'classification_loss': 0.9396309733390809}
2025-01-13 09:55:09,207 [INFO] Step[1550/2713]: training loss : 0.9345198321342468 TRAIN  loss dict:  {'classification_loss': 0.9345198321342468}
2025-01-13 09:55:21,100 [INFO] Step[1600/2713]: training loss : 0.9347129881381988 TRAIN  loss dict:  {'classification_loss': 0.9347129881381988}
2025-01-13 09:55:32,968 [INFO] Step[1650/2713]: training loss : 0.9344047951698303 TRAIN  loss dict:  {'classification_loss': 0.9344047951698303}
2025-01-13 09:55:44,860 [INFO] Step[1700/2713]: training loss : 0.9348527646064758 TRAIN  loss dict:  {'classification_loss': 0.9348527646064758}
2025-01-13 09:55:56,792 [INFO] Step[1750/2713]: training loss : 0.9367788326740265 TRAIN  loss dict:  {'classification_loss': 0.9367788326740265}
2025-01-13 09:56:08,747 [INFO] Step[1800/2713]: training loss : 0.934450957775116 TRAIN  loss dict:  {'classification_loss': 0.934450957775116}
2025-01-13 09:56:20,635 [INFO] Step[1850/2713]: training loss : 0.9345264542102814 TRAIN  loss dict:  {'classification_loss': 0.9345264542102814}
2025-01-13 09:56:32,566 [INFO] Step[1900/2713]: training loss : 0.9353542530536652 TRAIN  loss dict:  {'classification_loss': 0.9353542530536652}
2025-01-13 09:56:44,498 [INFO] Step[1950/2713]: training loss : 0.9338088464736939 TRAIN  loss dict:  {'classification_loss': 0.9338088464736939}
2025-01-13 09:56:56,461 [INFO] Step[2000/2713]: training loss : 0.9347510015964509 TRAIN  loss dict:  {'classification_loss': 0.9347510015964509}
2025-01-13 09:57:08,413 [INFO] Step[2050/2713]: training loss : 0.934358891248703 TRAIN  loss dict:  {'classification_loss': 0.934358891248703}
2025-01-13 09:57:20,311 [INFO] Step[2100/2713]: training loss : 0.9350712919235229 TRAIN  loss dict:  {'classification_loss': 0.9350712919235229}
2025-01-13 09:57:32,290 [INFO] Step[2150/2713]: training loss : 0.9345719504356385 TRAIN  loss dict:  {'classification_loss': 0.9345719504356385}
2025-01-13 09:57:44,199 [INFO] Step[2200/2713]: training loss : 0.9363950300216675 TRAIN  loss dict:  {'classification_loss': 0.9363950300216675}
2025-01-13 09:57:56,136 [INFO] Step[2250/2713]: training loss : 0.9354441940784455 TRAIN  loss dict:  {'classification_loss': 0.9354441940784455}
2025-01-13 09:58:08,064 [INFO] Step[2300/2713]: training loss : 0.9353246140480042 TRAIN  loss dict:  {'classification_loss': 0.9353246140480042}
2025-01-13 09:58:20,022 [INFO] Step[2350/2713]: training loss : 0.9342056715488434 TRAIN  loss dict:  {'classification_loss': 0.9342056715488434}
2025-01-13 09:58:31,899 [INFO] Step[2400/2713]: training loss : 0.933436564207077 TRAIN  loss dict:  {'classification_loss': 0.933436564207077}
2025-01-13 09:58:43,823 [INFO] Step[2450/2713]: training loss : 0.9343935000896454 TRAIN  loss dict:  {'classification_loss': 0.9343935000896454}
2025-01-13 09:58:55,744 [INFO] Step[2500/2713]: training loss : 0.9363578844070435 TRAIN  loss dict:  {'classification_loss': 0.9363578844070435}
2025-01-13 09:59:07,589 [INFO] Step[2550/2713]: training loss : 0.934415557384491 TRAIN  loss dict:  {'classification_loss': 0.934415557384491}
2025-01-13 09:59:19,502 [INFO] Step[2600/2713]: training loss : 0.937477525472641 TRAIN  loss dict:  {'classification_loss': 0.937477525472641}
2025-01-13 09:59:31,418 [INFO] Step[2650/2713]: training loss : 0.9365258657932282 TRAIN  loss dict:  {'classification_loss': 0.9365258657932282}
2025-01-13 09:59:43,334 [INFO] Step[2700/2713]: training loss : 0.9347645699977875 TRAIN  loss dict:  {'classification_loss': 0.9347645699977875}
2025-01-13 10:01:11,119 [INFO] Label accuracies statistics:
2025-01-13 10:01:11,119 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 10:01:11,121 [INFO] [92] TRAIN  loss: 0.9357961530049174 acc: 1.0
2025-01-13 10:01:11,121 [INFO] [92] TRAIN  loss dict: {'classification_loss': 0.9357961530049174}
2025-01-13 10:01:11,121 [INFO] [92] VALIDATION loss: 1.7150332888490276 VALIDATION acc: 0.8275862068965517
2025-01-13 10:01:11,121 [INFO] [92] VALIDATION loss dict: {'classification_loss': 1.7150332888490276}
2025-01-13 10:01:11,121 [INFO] 
2025-01-13 10:01:29,226 [INFO] Step[50/2713]: training loss : 0.935078135728836 TRAIN  loss dict:  {'classification_loss': 0.935078135728836}
2025-01-13 10:01:41,133 [INFO] Step[100/2713]: training loss : 0.9353574037551879 TRAIN  loss dict:  {'classification_loss': 0.9353574037551879}
2025-01-13 10:01:53,042 [INFO] Step[150/2713]: training loss : 0.9358690440654754 TRAIN  loss dict:  {'classification_loss': 0.9358690440654754}
2025-01-13 10:02:05,008 [INFO] Step[200/2713]: training loss : 0.9362085342407227 TRAIN  loss dict:  {'classification_loss': 0.9362085342407227}
2025-01-13 10:02:16,953 [INFO] Step[250/2713]: training loss : 0.935294644832611 TRAIN  loss dict:  {'classification_loss': 0.935294644832611}
2025-01-13 10:02:28,891 [INFO] Step[300/2713]: training loss : 0.9346710753440857 TRAIN  loss dict:  {'classification_loss': 0.9346710753440857}
2025-01-13 10:02:40,841 [INFO] Step[350/2713]: training loss : 0.9410350489616394 TRAIN  loss dict:  {'classification_loss': 0.9410350489616394}
2025-01-13 10:02:52,768 [INFO] Step[400/2713]: training loss : 0.9355610358715057 TRAIN  loss dict:  {'classification_loss': 0.9355610358715057}
2025-01-13 10:03:04,715 [INFO] Step[450/2713]: training loss : 0.934731913805008 TRAIN  loss dict:  {'classification_loss': 0.934731913805008}
2025-01-13 10:03:16,625 [INFO] Step[500/2713]: training loss : 0.9337513446807861 TRAIN  loss dict:  {'classification_loss': 0.9337513446807861}
2025-01-13 10:03:28,620 [INFO] Step[550/2713]: training loss : 0.9369873583316803 TRAIN  loss dict:  {'classification_loss': 0.9369873583316803}
2025-01-13 10:03:40,543 [INFO] Step[600/2713]: training loss : 0.9371013379096985 TRAIN  loss dict:  {'classification_loss': 0.9371013379096985}
2025-01-13 10:03:52,484 [INFO] Step[650/2713]: training loss : 0.9354180538654328 TRAIN  loss dict:  {'classification_loss': 0.9354180538654328}
2025-01-13 10:04:04,450 [INFO] Step[700/2713]: training loss : 0.9347849214076995 TRAIN  loss dict:  {'classification_loss': 0.9347849214076995}
2025-01-13 10:04:16,364 [INFO] Step[750/2713]: training loss : 0.9356137335300445 TRAIN  loss dict:  {'classification_loss': 0.9356137335300445}
2025-01-13 10:04:28,298 [INFO] Step[800/2713]: training loss : 0.9351941394805908 TRAIN  loss dict:  {'classification_loss': 0.9351941394805908}
2025-01-13 10:04:40,228 [INFO] Step[850/2713]: training loss : 0.9347546565532684 TRAIN  loss dict:  {'classification_loss': 0.9347546565532684}
2025-01-13 10:04:52,161 [INFO] Step[900/2713]: training loss : 0.9351193630695342 TRAIN  loss dict:  {'classification_loss': 0.9351193630695342}
2025-01-13 10:05:04,105 [INFO] Step[950/2713]: training loss : 0.9340710461139679 TRAIN  loss dict:  {'classification_loss': 0.9340710461139679}
2025-01-13 10:05:16,015 [INFO] Step[1000/2713]: training loss : 0.9357665967941284 TRAIN  loss dict:  {'classification_loss': 0.9357665967941284}
2025-01-13 10:05:28,010 [INFO] Step[1050/2713]: training loss : 0.9353940165042878 TRAIN  loss dict:  {'classification_loss': 0.9353940165042878}
2025-01-13 10:05:40,255 [INFO] Step[1100/2713]: training loss : 0.9374281275272369 TRAIN  loss dict:  {'classification_loss': 0.9374281275272369}
2025-01-13 10:05:52,659 [INFO] Step[1150/2713]: training loss : 0.9352303576469422 TRAIN  loss dict:  {'classification_loss': 0.9352303576469422}
2025-01-13 10:06:05,045 [INFO] Step[1200/2713]: training loss : 0.9396018970012665 TRAIN  loss dict:  {'classification_loss': 0.9396018970012665}
2025-01-13 10:06:17,453 [INFO] Step[1250/2713]: training loss : 0.9358885025978089 TRAIN  loss dict:  {'classification_loss': 0.9358885025978089}
2025-01-13 10:06:29,952 [INFO] Step[1300/2713]: training loss : 0.9351856231689453 TRAIN  loss dict:  {'classification_loss': 0.9351856231689453}
2025-01-13 10:06:42,273 [INFO] Step[1350/2713]: training loss : 0.9383983194828034 TRAIN  loss dict:  {'classification_loss': 0.9383983194828034}
2025-01-13 10:06:55,069 [INFO] Step[1400/2713]: training loss : 0.9344588911533356 TRAIN  loss dict:  {'classification_loss': 0.9344588911533356}
2025-01-13 10:07:08,739 [INFO] Step[1450/2713]: training loss : 0.9346140646934509 TRAIN  loss dict:  {'classification_loss': 0.9346140646934509}
2025-01-13 10:07:22,077 [INFO] Step[1500/2713]: training loss : 0.939111864566803 TRAIN  loss dict:  {'classification_loss': 0.939111864566803}
2025-01-13 10:07:34,228 [INFO] Step[1550/2713]: training loss : 0.9351261389255524 TRAIN  loss dict:  {'classification_loss': 0.9351261389255524}
2025-01-13 10:07:46,122 [INFO] Step[1600/2713]: training loss : 0.935881028175354 TRAIN  loss dict:  {'classification_loss': 0.935881028175354}
2025-01-13 10:07:58,023 [INFO] Step[1650/2713]: training loss : 0.9356567370891571 TRAIN  loss dict:  {'classification_loss': 0.9356567370891571}
2025-01-13 10:08:09,897 [INFO] Step[1700/2713]: training loss : 0.9365536856651306 TRAIN  loss dict:  {'classification_loss': 0.9365536856651306}
2025-01-13 10:08:21,894 [INFO] Step[1750/2713]: training loss : 0.9354659783840179 TRAIN  loss dict:  {'classification_loss': 0.9354659783840179}
2025-01-13 10:08:33,787 [INFO] Step[1800/2713]: training loss : 0.9339258432388305 TRAIN  loss dict:  {'classification_loss': 0.9339258432388305}
2025-01-13 10:08:45,641 [INFO] Step[1850/2713]: training loss : 0.934474664926529 TRAIN  loss dict:  {'classification_loss': 0.934474664926529}
2025-01-13 10:08:57,543 [INFO] Step[1900/2713]: training loss : 0.9347507011890411 TRAIN  loss dict:  {'classification_loss': 0.9347507011890411}
2025-01-13 10:09:09,482 [INFO] Step[1950/2713]: training loss : 0.9343860125541688 TRAIN  loss dict:  {'classification_loss': 0.9343860125541688}
2025-01-13 10:09:21,381 [INFO] Step[2000/2713]: training loss : 0.9362677609920502 TRAIN  loss dict:  {'classification_loss': 0.9362677609920502}
2025-01-13 10:09:33,251 [INFO] Step[2050/2713]: training loss : 0.9347569262981414 TRAIN  loss dict:  {'classification_loss': 0.9347569262981414}
2025-01-13 10:09:45,161 [INFO] Step[2100/2713]: training loss : 0.9347158479690552 TRAIN  loss dict:  {'classification_loss': 0.9347158479690552}
2025-01-13 10:09:57,060 [INFO] Step[2150/2713]: training loss : 0.9340971064567566 TRAIN  loss dict:  {'classification_loss': 0.9340971064567566}
2025-01-13 10:10:08,965 [INFO] Step[2200/2713]: training loss : 0.9346451222896576 TRAIN  loss dict:  {'classification_loss': 0.9346451222896576}
2025-01-13 10:10:20,906 [INFO] Step[2250/2713]: training loss : 0.9381530141830444 TRAIN  loss dict:  {'classification_loss': 0.9381530141830444}
2025-01-13 10:10:32,790 [INFO] Step[2300/2713]: training loss : 0.9343009507656097 TRAIN  loss dict:  {'classification_loss': 0.9343009507656097}
2025-01-13 10:10:44,712 [INFO] Step[2350/2713]: training loss : 0.9348863053321839 TRAIN  loss dict:  {'classification_loss': 0.9348863053321839}
2025-01-13 10:10:56,625 [INFO] Step[2400/2713]: training loss : 0.9352825057506561 TRAIN  loss dict:  {'classification_loss': 0.9352825057506561}
2025-01-13 10:11:08,594 [INFO] Step[2450/2713]: training loss : 0.9362046492099761 TRAIN  loss dict:  {'classification_loss': 0.9362046492099761}
2025-01-13 10:11:20,497 [INFO] Step[2500/2713]: training loss : 0.9339790344238281 TRAIN  loss dict:  {'classification_loss': 0.9339790344238281}
2025-01-13 10:11:32,384 [INFO] Step[2550/2713]: training loss : 0.9380200612545013 TRAIN  loss dict:  {'classification_loss': 0.9380200612545013}
2025-01-13 10:11:44,248 [INFO] Step[2600/2713]: training loss : 0.9362511205673217 TRAIN  loss dict:  {'classification_loss': 0.9362511205673217}
2025-01-13 10:11:56,129 [INFO] Step[2650/2713]: training loss : 0.9333487164974212 TRAIN  loss dict:  {'classification_loss': 0.9333487164974212}
2025-01-13 10:12:08,043 [INFO] Step[2700/2713]: training loss : 0.9344010484218598 TRAIN  loss dict:  {'classification_loss': 0.9344010484218598}
2025-01-13 10:13:35,069 [INFO] Label accuracies statistics:
2025-01-13 10:13:35,069 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 10:13:35,071 [INFO] [93] TRAIN  loss: 0.9355989804884801 acc: 1.0
2025-01-13 10:13:35,071 [INFO] [93] TRAIN  loss dict: {'classification_loss': 0.9355989804884801}
2025-01-13 10:13:35,071 [INFO] [93] VALIDATION loss: 1.6764577462484962 VALIDATION acc: 0.8413793103448276
2025-01-13 10:13:35,071 [INFO] [93] VALIDATION loss dict: {'classification_loss': 1.6764577462484962}
2025-01-13 10:13:35,071 [INFO] 
2025-01-13 10:13:53,753 [INFO] Step[50/2713]: training loss : 0.9358833169937134 TRAIN  loss dict:  {'classification_loss': 0.9358833169937134}
2025-01-13 10:14:05,676 [INFO] Step[100/2713]: training loss : 0.9345277225971222 TRAIN  loss dict:  {'classification_loss': 0.9345277225971222}
2025-01-13 10:14:17,547 [INFO] Step[150/2713]: training loss : 0.9348415637016296 TRAIN  loss dict:  {'classification_loss': 0.9348415637016296}
2025-01-13 10:14:29,451 [INFO] Step[200/2713]: training loss : 0.9344768166542053 TRAIN  loss dict:  {'classification_loss': 0.9344768166542053}
2025-01-13 10:14:41,414 [INFO] Step[250/2713]: training loss : 0.9338286244869232 TRAIN  loss dict:  {'classification_loss': 0.9338286244869232}
2025-01-13 10:14:53,382 [INFO] Step[300/2713]: training loss : 0.9402495884895324 TRAIN  loss dict:  {'classification_loss': 0.9402495884895324}
2025-01-13 10:15:05,319 [INFO] Step[350/2713]: training loss : 0.9360191941261291 TRAIN  loss dict:  {'classification_loss': 0.9360191941261291}
2025-01-13 10:15:17,228 [INFO] Step[400/2713]: training loss : 0.9354103851318359 TRAIN  loss dict:  {'classification_loss': 0.9354103851318359}
2025-01-13 10:15:29,156 [INFO] Step[450/2713]: training loss : 0.933659452199936 TRAIN  loss dict:  {'classification_loss': 0.933659452199936}
2025-01-13 10:15:41,062 [INFO] Step[500/2713]: training loss : 0.9342857134342194 TRAIN  loss dict:  {'classification_loss': 0.9342857134342194}
2025-01-13 10:15:52,956 [INFO] Step[550/2713]: training loss : 0.9353546035289765 TRAIN  loss dict:  {'classification_loss': 0.9353546035289765}
2025-01-13 10:16:04,848 [INFO] Step[600/2713]: training loss : 0.9346040296554565 TRAIN  loss dict:  {'classification_loss': 0.9346040296554565}
2025-01-13 10:16:16,775 [INFO] Step[650/2713]: training loss : 0.9346090757846832 TRAIN  loss dict:  {'classification_loss': 0.9346090757846832}
2025-01-13 10:16:28,695 [INFO] Step[700/2713]: training loss : 0.9366261994838715 TRAIN  loss dict:  {'classification_loss': 0.9366261994838715}
2025-01-13 10:16:40,639 [INFO] Step[750/2713]: training loss : 0.9353321933746338 TRAIN  loss dict:  {'classification_loss': 0.9353321933746338}
2025-01-13 10:16:52,509 [INFO] Step[800/2713]: training loss : 0.9347693371772766 TRAIN  loss dict:  {'classification_loss': 0.9347693371772766}
2025-01-13 10:17:04,474 [INFO] Step[850/2713]: training loss : 0.9345083475112915 TRAIN  loss dict:  {'classification_loss': 0.9345083475112915}
2025-01-13 10:17:16,378 [INFO] Step[900/2713]: training loss : 0.9349538135528564 TRAIN  loss dict:  {'classification_loss': 0.9349538135528564}
2025-01-13 10:17:28,304 [INFO] Step[950/2713]: training loss : 0.937306649684906 TRAIN  loss dict:  {'classification_loss': 0.937306649684906}
2025-01-13 10:17:40,234 [INFO] Step[1000/2713]: training loss : 0.9357256674766541 TRAIN  loss dict:  {'classification_loss': 0.9357256674766541}
2025-01-13 10:17:52,157 [INFO] Step[1050/2713]: training loss : 0.9350668835639954 TRAIN  loss dict:  {'classification_loss': 0.9350668835639954}
2025-01-13 10:18:04,046 [INFO] Step[1100/2713]: training loss : 0.9362531816959381 TRAIN  loss dict:  {'classification_loss': 0.9362531816959381}
2025-01-13 10:18:15,987 [INFO] Step[1150/2713]: training loss : 0.9369729340076447 TRAIN  loss dict:  {'classification_loss': 0.9369729340076447}
2025-01-13 10:18:27,884 [INFO] Step[1200/2713]: training loss : 0.9357304835319519 TRAIN  loss dict:  {'classification_loss': 0.9357304835319519}
2025-01-13 10:18:39,804 [INFO] Step[1250/2713]: training loss : 0.9352886497974395 TRAIN  loss dict:  {'classification_loss': 0.9352886497974395}
2025-01-13 10:18:51,699 [INFO] Step[1300/2713]: training loss : 0.934745579957962 TRAIN  loss dict:  {'classification_loss': 0.934745579957962}
2025-01-13 10:19:03,577 [INFO] Step[1350/2713]: training loss : 0.9335867059230805 TRAIN  loss dict:  {'classification_loss': 0.9335867059230805}
2025-01-13 10:19:15,469 [INFO] Step[1400/2713]: training loss : 0.9358888745307923 TRAIN  loss dict:  {'classification_loss': 0.9358888745307923}
2025-01-13 10:19:27,405 [INFO] Step[1450/2713]: training loss : 0.9355866611003876 TRAIN  loss dict:  {'classification_loss': 0.9355866611003876}
2025-01-13 10:19:39,317 [INFO] Step[1500/2713]: training loss : 0.936132972240448 TRAIN  loss dict:  {'classification_loss': 0.936132972240448}
2025-01-13 10:19:51,191 [INFO] Step[1550/2713]: training loss : 0.9364213371276855 TRAIN  loss dict:  {'classification_loss': 0.9364213371276855}
2025-01-13 10:20:03,093 [INFO] Step[1600/2713]: training loss : 0.9343944668769837 TRAIN  loss dict:  {'classification_loss': 0.9343944668769837}
2025-01-13 10:20:15,027 [INFO] Step[1650/2713]: training loss : 0.9370890414714813 TRAIN  loss dict:  {'classification_loss': 0.9370890414714813}
2025-01-13 10:20:26,946 [INFO] Step[1700/2713]: training loss : 0.9364970326423645 TRAIN  loss dict:  {'classification_loss': 0.9364970326423645}
2025-01-13 10:20:38,892 [INFO] Step[1750/2713]: training loss : 0.9342090845108032 TRAIN  loss dict:  {'classification_loss': 0.9342090845108032}
2025-01-13 10:20:50,772 [INFO] Step[1800/2713]: training loss : 0.9351576554775238 TRAIN  loss dict:  {'classification_loss': 0.9351576554775238}
2025-01-13 10:21:02,699 [INFO] Step[1850/2713]: training loss : 0.934612547159195 TRAIN  loss dict:  {'classification_loss': 0.934612547159195}
2025-01-13 10:21:14,608 [INFO] Step[1900/2713]: training loss : 0.9385651886463166 TRAIN  loss dict:  {'classification_loss': 0.9385651886463166}
2025-01-13 10:21:26,567 [INFO] Step[1950/2713]: training loss : 0.9386191809177399 TRAIN  loss dict:  {'classification_loss': 0.9386191809177399}
2025-01-13 10:21:38,485 [INFO] Step[2000/2713]: training loss : 0.9354232978820801 TRAIN  loss dict:  {'classification_loss': 0.9354232978820801}
2025-01-13 10:21:50,422 [INFO] Step[2050/2713]: training loss : 0.9401812398433685 TRAIN  loss dict:  {'classification_loss': 0.9401812398433685}
2025-01-13 10:22:02,338 [INFO] Step[2100/2713]: training loss : 0.9363083136081696 TRAIN  loss dict:  {'classification_loss': 0.9363083136081696}
2025-01-13 10:22:14,351 [INFO] Step[2150/2713]: training loss : 0.9351572608947754 TRAIN  loss dict:  {'classification_loss': 0.9351572608947754}
2025-01-13 10:22:26,295 [INFO] Step[2200/2713]: training loss : 0.9347714078426361 TRAIN  loss dict:  {'classification_loss': 0.9347714078426361}
2025-01-13 10:22:38,244 [INFO] Step[2250/2713]: training loss : 0.9360740721225739 TRAIN  loss dict:  {'classification_loss': 0.9360740721225739}
2025-01-13 10:22:50,175 [INFO] Step[2300/2713]: training loss : 0.9339323365688323 TRAIN  loss dict:  {'classification_loss': 0.9339323365688323}
2025-01-13 10:23:02,109 [INFO] Step[2350/2713]: training loss : 0.9354592072963714 TRAIN  loss dict:  {'classification_loss': 0.9354592072963714}
2025-01-13 10:23:14,071 [INFO] Step[2400/2713]: training loss : 0.9374964880943298 TRAIN  loss dict:  {'classification_loss': 0.9374964880943298}
2025-01-13 10:23:26,042 [INFO] Step[2450/2713]: training loss : 0.9346989750862121 TRAIN  loss dict:  {'classification_loss': 0.9346989750862121}
2025-01-13 10:23:37,918 [INFO] Step[2500/2713]: training loss : 0.9350260245800018 TRAIN  loss dict:  {'classification_loss': 0.9350260245800018}
2025-01-13 10:23:49,826 [INFO] Step[2550/2713]: training loss : 0.9350518953800201 TRAIN  loss dict:  {'classification_loss': 0.9350518953800201}
2025-01-13 10:24:01,716 [INFO] Step[2600/2713]: training loss : 0.9360839414596558 TRAIN  loss dict:  {'classification_loss': 0.9360839414596558}
2025-01-13 10:24:13,631 [INFO] Step[2650/2713]: training loss : 0.9350371062755585 TRAIN  loss dict:  {'classification_loss': 0.9350371062755585}
2025-01-13 10:24:25,599 [INFO] Step[2700/2713]: training loss : 0.935033712387085 TRAIN  loss dict:  {'classification_loss': 0.935033712387085}
2025-01-13 10:26:14,706 [INFO] Label accuracies statistics:
2025-01-13 10:26:14,707 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 0.75, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 0.75, 185: 0.75, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 1.0, 260: 0.25, 261: 1.0, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 0.75, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 1.0, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 1.0, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 10:26:14,713 [INFO] [94] TRAIN  loss: 0.9356281389137322 acc: 1.0
2025-01-13 10:26:14,713 [INFO] [94] TRAIN  loss dict: {'classification_loss': 0.9356281389137322}
2025-01-13 10:26:14,714 [INFO] [94] VALIDATION loss: 1.6855994455124204 VALIDATION acc: 0.8326018808777429
2025-01-13 10:26:14,715 [INFO] [94] VALIDATION loss dict: {'classification_loss': 1.6855994455124204}
2025-01-13 10:26:14,715 [INFO] 
2025-01-13 10:26:40,691 [INFO] Step[50/2713]: training loss : 0.9356914341449738 TRAIN  loss dict:  {'classification_loss': 0.9356914341449738}
2025-01-13 10:26:52,828 [INFO] Step[100/2713]: training loss : 0.9346578073501587 TRAIN  loss dict:  {'classification_loss': 0.9346578073501587}
2025-01-13 10:27:04,777 [INFO] Step[150/2713]: training loss : 0.9347711408138275 TRAIN  loss dict:  {'classification_loss': 0.9347711408138275}
2025-01-13 10:27:16,737 [INFO] Step[200/2713]: training loss : 0.9359131777286529 TRAIN  loss dict:  {'classification_loss': 0.9359131777286529}
2025-01-13 10:27:28,717 [INFO] Step[250/2713]: training loss : 0.9342339766025544 TRAIN  loss dict:  {'classification_loss': 0.9342339766025544}
2025-01-13 10:27:40,616 [INFO] Step[300/2713]: training loss : 0.9353481805324555 TRAIN  loss dict:  {'classification_loss': 0.9353481805324555}
2025-01-13 10:27:52,575 [INFO] Step[350/2713]: training loss : 0.9371893799304962 TRAIN  loss dict:  {'classification_loss': 0.9371893799304962}
2025-01-13 10:28:04,483 [INFO] Step[400/2713]: training loss : 0.9375074362754822 TRAIN  loss dict:  {'classification_loss': 0.9375074362754822}
2025-01-13 10:28:16,410 [INFO] Step[450/2713]: training loss : 0.937214537858963 TRAIN  loss dict:  {'classification_loss': 0.937214537858963}
2025-01-13 10:28:28,330 [INFO] Step[500/2713]: training loss : 0.9370641303062439 TRAIN  loss dict:  {'classification_loss': 0.9370641303062439}
2025-01-13 10:28:40,272 [INFO] Step[550/2713]: training loss : 0.9344051349163055 TRAIN  loss dict:  {'classification_loss': 0.9344051349163055}
2025-01-13 10:28:52,200 [INFO] Step[600/2713]: training loss : 0.9375702261924743 TRAIN  loss dict:  {'classification_loss': 0.9375702261924743}
2025-01-13 10:29:04,114 [INFO] Step[650/2713]: training loss : 0.9351926755905151 TRAIN  loss dict:  {'classification_loss': 0.9351926755905151}
2025-01-13 10:29:16,045 [INFO] Step[700/2713]: training loss : 0.9350419878959656 TRAIN  loss dict:  {'classification_loss': 0.9350419878959656}
2025-01-13 10:29:27,989 [INFO] Step[750/2713]: training loss : 0.9346339452266693 TRAIN  loss dict:  {'classification_loss': 0.9346339452266693}
2025-01-13 10:29:39,904 [INFO] Step[800/2713]: training loss : 0.9363208723068237 TRAIN  loss dict:  {'classification_loss': 0.9363208723068237}
2025-01-13 10:29:51,839 [INFO] Step[850/2713]: training loss : 0.9349059867858887 TRAIN  loss dict:  {'classification_loss': 0.9349059867858887}
2025-01-13 10:30:03,803 [INFO] Step[900/2713]: training loss : 0.9350493264198303 TRAIN  loss dict:  {'classification_loss': 0.9350493264198303}
2025-01-13 10:30:15,749 [INFO] Step[950/2713]: training loss : 0.9349785017967224 TRAIN  loss dict:  {'classification_loss': 0.9349785017967224}
2025-01-13 10:30:27,693 [INFO] Step[1000/2713]: training loss : 0.936579304933548 TRAIN  loss dict:  {'classification_loss': 0.936579304933548}
2025-01-13 10:30:39,664 [INFO] Step[1050/2713]: training loss : 0.9361833214759827 TRAIN  loss dict:  {'classification_loss': 0.9361833214759827}
2025-01-13 10:30:51,607 [INFO] Step[1100/2713]: training loss : 0.9360909974575042 TRAIN  loss dict:  {'classification_loss': 0.9360909974575042}
2025-01-13 10:31:03,487 [INFO] Step[1150/2713]: training loss : 0.936479068994522 TRAIN  loss dict:  {'classification_loss': 0.936479068994522}
2025-01-13 10:31:15,429 [INFO] Step[1200/2713]: training loss : 0.9353437340259552 TRAIN  loss dict:  {'classification_loss': 0.9353437340259552}
2025-01-13 10:31:27,381 [INFO] Step[1250/2713]: training loss : 0.9350574958324432 TRAIN  loss dict:  {'classification_loss': 0.9350574958324432}
2025-01-13 10:31:39,336 [INFO] Step[1300/2713]: training loss : 0.9346520137786866 TRAIN  loss dict:  {'classification_loss': 0.9346520137786866}
2025-01-13 10:31:51,273 [INFO] Step[1350/2713]: training loss : 0.935079460144043 TRAIN  loss dict:  {'classification_loss': 0.935079460144043}
2025-01-13 10:32:03,217 [INFO] Step[1400/2713]: training loss : 0.9352624297142029 TRAIN  loss dict:  {'classification_loss': 0.9352624297142029}
2025-01-13 10:32:15,147 [INFO] Step[1450/2713]: training loss : 0.9345860886573791 TRAIN  loss dict:  {'classification_loss': 0.9345860886573791}
2025-01-13 10:32:27,102 [INFO] Step[1500/2713]: training loss : 0.9350304162502289 TRAIN  loss dict:  {'classification_loss': 0.9350304162502289}
2025-01-13 10:32:39,041 [INFO] Step[1550/2713]: training loss : 0.9352253901958466 TRAIN  loss dict:  {'classification_loss': 0.9352253901958466}
2025-01-13 10:32:50,973 [INFO] Step[1600/2713]: training loss : 0.9352270710468292 TRAIN  loss dict:  {'classification_loss': 0.9352270710468292}
2025-01-13 10:33:02,918 [INFO] Step[1650/2713]: training loss : 0.9335384106636048 TRAIN  loss dict:  {'classification_loss': 0.9335384106636048}
2025-01-13 10:33:14,815 [INFO] Step[1700/2713]: training loss : 0.9336873328685761 TRAIN  loss dict:  {'classification_loss': 0.9336873328685761}
2025-01-13 10:33:26,812 [INFO] Step[1750/2713]: training loss : 0.9349124491214752 TRAIN  loss dict:  {'classification_loss': 0.9349124491214752}
2025-01-13 10:33:38,744 [INFO] Step[1800/2713]: training loss : 0.936765319108963 TRAIN  loss dict:  {'classification_loss': 0.936765319108963}
2025-01-13 10:33:50,685 [INFO] Step[1850/2713]: training loss : 0.9344560289382935 TRAIN  loss dict:  {'classification_loss': 0.9344560289382935}
2025-01-13 10:34:02,630 [INFO] Step[1900/2713]: training loss : 0.9342344903945923 TRAIN  loss dict:  {'classification_loss': 0.9342344903945923}
2025-01-13 10:34:14,641 [INFO] Step[1950/2713]: training loss : 0.9353762543201447 TRAIN  loss dict:  {'classification_loss': 0.9353762543201447}
2025-01-13 10:34:26,617 [INFO] Step[2000/2713]: training loss : 0.9342644166946411 TRAIN  loss dict:  {'classification_loss': 0.9342644166946411}
2025-01-13 10:34:38,561 [INFO] Step[2050/2713]: training loss : 0.9363621735572815 TRAIN  loss dict:  {'classification_loss': 0.9363621735572815}
2025-01-13 10:34:50,493 [INFO] Step[2100/2713]: training loss : 0.934015941619873 TRAIN  loss dict:  {'classification_loss': 0.934015941619873}
2025-01-13 10:35:02,417 [INFO] Step[2150/2713]: training loss : 0.9367181861400604 TRAIN  loss dict:  {'classification_loss': 0.9367181861400604}
2025-01-13 10:35:14,356 [INFO] Step[2200/2713]: training loss : 0.9367453515529632 TRAIN  loss dict:  {'classification_loss': 0.9367453515529632}
2025-01-13 10:35:26,289 [INFO] Step[2250/2713]: training loss : 0.9358625888824463 TRAIN  loss dict:  {'classification_loss': 0.9358625888824463}
2025-01-13 10:35:38,214 [INFO] Step[2300/2713]: training loss : 0.9348245084285736 TRAIN  loss dict:  {'classification_loss': 0.9348245084285736}
2025-01-13 10:35:50,162 [INFO] Step[2350/2713]: training loss : 0.9340842139720916 TRAIN  loss dict:  {'classification_loss': 0.9340842139720916}
2025-01-13 10:36:02,129 [INFO] Step[2400/2713]: training loss : 0.9360718023777008 TRAIN  loss dict:  {'classification_loss': 0.9360718023777008}
2025-01-13 10:36:14,060 [INFO] Step[2450/2713]: training loss : 0.935535694360733 TRAIN  loss dict:  {'classification_loss': 0.935535694360733}
2025-01-13 10:36:25,971 [INFO] Step[2500/2713]: training loss : 0.9361388111114501 TRAIN  loss dict:  {'classification_loss': 0.9361388111114501}
2025-01-13 10:36:37,878 [INFO] Step[2550/2713]: training loss : 0.9358545708656311 TRAIN  loss dict:  {'classification_loss': 0.9358545708656311}
2025-01-13 10:36:49,817 [INFO] Step[2600/2713]: training loss : 0.9365431261062622 TRAIN  loss dict:  {'classification_loss': 0.9365431261062622}
2025-01-13 10:37:01,746 [INFO] Step[2650/2713]: training loss : 0.933592655658722 TRAIN  loss dict:  {'classification_loss': 0.933592655658722}
2025-01-13 10:37:13,724 [INFO] Step[2700/2713]: training loss : 0.9355120003223419 TRAIN  loss dict:  {'classification_loss': 0.9355120003223419}
2025-01-13 10:38:42,269 [INFO] Label accuracies statistics:
2025-01-13 10:38:42,269 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.5, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 1.0, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 0.75, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.5, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 10:38:42,271 [INFO] [95] TRAIN  loss: 0.9354237098558512 acc: 1.0
2025-01-13 10:38:42,271 [INFO] [95] TRAIN  loss dict: {'classification_loss': 0.9354237098558512}
2025-01-13 10:38:42,271 [INFO] [95] VALIDATION loss: 1.7033728856341284 VALIDATION acc: 0.8344827586206897
2025-01-13 10:38:42,271 [INFO] [95] VALIDATION loss dict: {'classification_loss': 1.7033728856341284}
2025-01-13 10:38:42,271 [INFO] 
2025-01-13 10:38:59,984 [INFO] Step[50/2713]: training loss : 0.9337977826595306 TRAIN  loss dict:  {'classification_loss': 0.9337977826595306}
2025-01-13 10:39:11,915 [INFO] Step[100/2713]: training loss : 0.9340383160114288 TRAIN  loss dict:  {'classification_loss': 0.9340383160114288}
2025-01-13 10:39:23,844 [INFO] Step[150/2713]: training loss : 0.9345664525032044 TRAIN  loss dict:  {'classification_loss': 0.9345664525032044}
2025-01-13 10:39:35,798 [INFO] Step[200/2713]: training loss : 0.93508398771286 TRAIN  loss dict:  {'classification_loss': 0.93508398771286}
2025-01-13 10:39:47,778 [INFO] Step[250/2713]: training loss : 0.9352033543586731 TRAIN  loss dict:  {'classification_loss': 0.9352033543586731}
2025-01-13 10:39:59,715 [INFO] Step[300/2713]: training loss : 0.9342772877216339 TRAIN  loss dict:  {'classification_loss': 0.9342772877216339}
2025-01-13 10:40:11,659 [INFO] Step[350/2713]: training loss : 0.9361567962169647 TRAIN  loss dict:  {'classification_loss': 0.9361567962169647}
2025-01-13 10:40:23,601 [INFO] Step[400/2713]: training loss : 0.9345539081096649 TRAIN  loss dict:  {'classification_loss': 0.9345539081096649}
2025-01-13 10:40:35,567 [INFO] Step[450/2713]: training loss : 0.9357830667495728 TRAIN  loss dict:  {'classification_loss': 0.9357830667495728}
2025-01-13 10:40:47,487 [INFO] Step[500/2713]: training loss : 0.9356916534900666 TRAIN  loss dict:  {'classification_loss': 0.9356916534900666}
2025-01-13 10:40:59,416 [INFO] Step[550/2713]: training loss : 0.934397554397583 TRAIN  loss dict:  {'classification_loss': 0.934397554397583}
2025-01-13 10:41:11,379 [INFO] Step[600/2713]: training loss : 0.9361082530021667 TRAIN  loss dict:  {'classification_loss': 0.9361082530021667}
2025-01-13 10:41:23,327 [INFO] Step[650/2713]: training loss : 0.9344117856025695 TRAIN  loss dict:  {'classification_loss': 0.9344117856025695}
2025-01-13 10:41:35,291 [INFO] Step[700/2713]: training loss : 0.9366302931308746 TRAIN  loss dict:  {'classification_loss': 0.9366302931308746}
2025-01-13 10:41:47,210 [INFO] Step[750/2713]: training loss : 0.9349543070793152 TRAIN  loss dict:  {'classification_loss': 0.9349543070793152}
2025-01-13 10:41:59,171 [INFO] Step[800/2713]: training loss : 0.9351284503936768 TRAIN  loss dict:  {'classification_loss': 0.9351284503936768}
2025-01-13 10:42:11,101 [INFO] Step[850/2713]: training loss : 0.9360297405719757 TRAIN  loss dict:  {'classification_loss': 0.9360297405719757}
2025-01-13 10:42:23,016 [INFO] Step[900/2713]: training loss : 0.9342418026924133 TRAIN  loss dict:  {'classification_loss': 0.9342418026924133}
2025-01-13 10:42:34,945 [INFO] Step[950/2713]: training loss : 0.9366284382343292 TRAIN  loss dict:  {'classification_loss': 0.9366284382343292}
2025-01-13 10:42:46,837 [INFO] Step[1000/2713]: training loss : 0.9344841873645783 TRAIN  loss dict:  {'classification_loss': 0.9344841873645783}
2025-01-13 10:42:58,799 [INFO] Step[1050/2713]: training loss : 0.9359026062488556 TRAIN  loss dict:  {'classification_loss': 0.9359026062488556}
2025-01-13 10:43:10,780 [INFO] Step[1100/2713]: training loss : 0.9334518253803253 TRAIN  loss dict:  {'classification_loss': 0.9334518253803253}
2025-01-13 10:43:22,711 [INFO] Step[1150/2713]: training loss : 0.9352623176574707 TRAIN  loss dict:  {'classification_loss': 0.9352623176574707}
2025-01-13 10:43:34,648 [INFO] Step[1200/2713]: training loss : 0.9335998106002807 TRAIN  loss dict:  {'classification_loss': 0.9335998106002807}
2025-01-13 10:43:46,661 [INFO] Step[1250/2713]: training loss : 0.9342800056934357 TRAIN  loss dict:  {'classification_loss': 0.9342800056934357}
2025-01-13 10:43:59,137 [INFO] Step[1300/2713]: training loss : 0.937932118177414 TRAIN  loss dict:  {'classification_loss': 0.937932118177414}
2025-01-13 10:44:11,562 [INFO] Step[1350/2713]: training loss : 0.9352542734146119 TRAIN  loss dict:  {'classification_loss': 0.9352542734146119}
2025-01-13 10:44:23,824 [INFO] Step[1400/2713]: training loss : 0.9380896770954132 TRAIN  loss dict:  {'classification_loss': 0.9380896770954132}
2025-01-13 10:44:36,563 [INFO] Step[1450/2713]: training loss : 0.9346987402439118 TRAIN  loss dict:  {'classification_loss': 0.9346987402439118}
2025-01-13 10:44:48,933 [INFO] Step[1500/2713]: training loss : 0.9356432330608367 TRAIN  loss dict:  {'classification_loss': 0.9356432330608367}
2025-01-13 10:45:01,628 [INFO] Step[1550/2713]: training loss : 0.9358378040790558 TRAIN  loss dict:  {'classification_loss': 0.9358378040790558}
2025-01-13 10:45:15,079 [INFO] Step[1600/2713]: training loss : 0.9363096046447754 TRAIN  loss dict:  {'classification_loss': 0.9363096046447754}
2025-01-13 10:45:28,687 [INFO] Step[1650/2713]: training loss : 0.9349568831920624 TRAIN  loss dict:  {'classification_loss': 0.9349568831920624}
2025-01-13 10:45:41,266 [INFO] Step[1700/2713]: training loss : 0.9343121385574341 TRAIN  loss dict:  {'classification_loss': 0.9343121385574341}
2025-01-13 10:45:53,276 [INFO] Step[1750/2713]: training loss : 0.9348884177207947 TRAIN  loss dict:  {'classification_loss': 0.9348884177207947}
2025-01-13 10:46:05,222 [INFO] Step[1800/2713]: training loss : 0.9357636904716492 TRAIN  loss dict:  {'classification_loss': 0.9357636904716492}
2025-01-13 10:46:17,139 [INFO] Step[1850/2713]: training loss : 0.9345284974575043 TRAIN  loss dict:  {'classification_loss': 0.9345284974575043}
2025-01-13 10:46:29,040 [INFO] Step[1900/2713]: training loss : 0.9360582554340362 TRAIN  loss dict:  {'classification_loss': 0.9360582554340362}
2025-01-13 10:46:40,980 [INFO] Step[1950/2713]: training loss : 0.9342137551307679 TRAIN  loss dict:  {'classification_loss': 0.9342137551307679}
2025-01-13 10:46:52,884 [INFO] Step[2000/2713]: training loss : 0.9339196848869323 TRAIN  loss dict:  {'classification_loss': 0.9339196848869323}
2025-01-13 10:47:04,809 [INFO] Step[2050/2713]: training loss : 0.934498336315155 TRAIN  loss dict:  {'classification_loss': 0.934498336315155}
2025-01-13 10:47:16,752 [INFO] Step[2100/2713]: training loss : 0.9368995070457459 TRAIN  loss dict:  {'classification_loss': 0.9368995070457459}
2025-01-13 10:47:28,692 [INFO] Step[2150/2713]: training loss : 0.9357729315757751 TRAIN  loss dict:  {'classification_loss': 0.9357729315757751}
2025-01-13 10:47:40,600 [INFO] Step[2200/2713]: training loss : 0.9338831043243409 TRAIN  loss dict:  {'classification_loss': 0.9338831043243409}
2025-01-13 10:47:52,536 [INFO] Step[2250/2713]: training loss : 0.9358670282363891 TRAIN  loss dict:  {'classification_loss': 0.9358670282363891}
2025-01-13 10:48:04,481 [INFO] Step[2300/2713]: training loss : 0.9345748507976532 TRAIN  loss dict:  {'classification_loss': 0.9345748507976532}
2025-01-13 10:48:16,481 [INFO] Step[2350/2713]: training loss : 0.9340580499172211 TRAIN  loss dict:  {'classification_loss': 0.9340580499172211}
2025-01-13 10:48:28,398 [INFO] Step[2400/2713]: training loss : 0.9349207019805909 TRAIN  loss dict:  {'classification_loss': 0.9349207019805909}
2025-01-13 10:48:40,372 [INFO] Step[2450/2713]: training loss : 0.9349101436138153 TRAIN  loss dict:  {'classification_loss': 0.9349101436138153}
2025-01-13 10:48:52,272 [INFO] Step[2500/2713]: training loss : 0.9354598891735076 TRAIN  loss dict:  {'classification_loss': 0.9354598891735076}
2025-01-13 10:49:04,171 [INFO] Step[2550/2713]: training loss : 0.9340473091602326 TRAIN  loss dict:  {'classification_loss': 0.9340473091602326}
2025-01-13 10:49:16,094 [INFO] Step[2600/2713]: training loss : 0.9348681807518006 TRAIN  loss dict:  {'classification_loss': 0.9348681807518006}
2025-01-13 10:49:28,013 [INFO] Step[2650/2713]: training loss : 0.9349729549884797 TRAIN  loss dict:  {'classification_loss': 0.9349729549884797}
2025-01-13 10:49:39,930 [INFO] Step[2700/2713]: training loss : 0.937415953874588 TRAIN  loss dict:  {'classification_loss': 0.937415953874588}
2025-01-13 10:51:06,695 [INFO] Label accuracies statistics:
2025-01-13 10:51:06,695 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 0.5, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 0.75, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 1.0, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.5, 274: 1.0, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 1.0, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 0.75, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.5, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 10:51:06,697 [INFO] [96] TRAIN  loss: 0.9351607118855092 acc: 1.0
2025-01-13 10:51:06,697 [INFO] [96] TRAIN  loss dict: {'classification_loss': 0.9351607118855092}
2025-01-13 10:51:06,697 [INFO] [96] VALIDATION loss: 1.697751606877585 VALIDATION acc: 0.8394984326018808
2025-01-13 10:51:06,697 [INFO] [96] VALIDATION loss dict: {'classification_loss': 1.697751606877585}
2025-01-13 10:51:06,698 [INFO] 
2025-01-13 10:51:24,893 [INFO] Step[50/2713]: training loss : 0.9380907094478608 TRAIN  loss dict:  {'classification_loss': 0.9380907094478608}
2025-01-13 10:51:36,768 [INFO] Step[100/2713]: training loss : 0.9359528553485871 TRAIN  loss dict:  {'classification_loss': 0.9359528553485871}
2025-01-13 10:51:48,766 [INFO] Step[150/2713]: training loss : 0.9370221304893493 TRAIN  loss dict:  {'classification_loss': 0.9370221304893493}
2025-01-13 10:52:00,684 [INFO] Step[200/2713]: training loss : 0.933793568611145 TRAIN  loss dict:  {'classification_loss': 0.933793568611145}
2025-01-13 10:52:12,592 [INFO] Step[250/2713]: training loss : 0.9337141132354736 TRAIN  loss dict:  {'classification_loss': 0.9337141132354736}
2025-01-13 10:52:24,496 [INFO] Step[300/2713]: training loss : 0.9376160001754761 TRAIN  loss dict:  {'classification_loss': 0.9376160001754761}
2025-01-13 10:52:36,418 [INFO] Step[350/2713]: training loss : 0.9348369753360748 TRAIN  loss dict:  {'classification_loss': 0.9348369753360748}
2025-01-13 10:52:48,334 [INFO] Step[400/2713]: training loss : 0.9347726678848267 TRAIN  loss dict:  {'classification_loss': 0.9347726678848267}
2025-01-13 10:53:00,242 [INFO] Step[450/2713]: training loss : 0.9341122090816498 TRAIN  loss dict:  {'classification_loss': 0.9341122090816498}
2025-01-13 10:53:12,141 [INFO] Step[500/2713]: training loss : 0.9341544318199158 TRAIN  loss dict:  {'classification_loss': 0.9341544318199158}
2025-01-13 10:53:24,059 [INFO] Step[550/2713]: training loss : 0.9349442172050476 TRAIN  loss dict:  {'classification_loss': 0.9349442172050476}
2025-01-13 10:53:35,949 [INFO] Step[600/2713]: training loss : 0.9427736878395081 TRAIN  loss dict:  {'classification_loss': 0.9427736878395081}
2025-01-13 10:53:47,845 [INFO] Step[650/2713]: training loss : 0.9366705572605133 TRAIN  loss dict:  {'classification_loss': 0.9366705572605133}
2025-01-13 10:53:59,713 [INFO] Step[700/2713]: training loss : 0.9344327235221863 TRAIN  loss dict:  {'classification_loss': 0.9344327235221863}
2025-01-13 10:54:11,633 [INFO] Step[750/2713]: training loss : 0.9362794280052185 TRAIN  loss dict:  {'classification_loss': 0.9362794280052185}
2025-01-13 10:54:23,534 [INFO] Step[800/2713]: training loss : 0.9375302970409394 TRAIN  loss dict:  {'classification_loss': 0.9375302970409394}
2025-01-13 10:54:35,449 [INFO] Step[850/2713]: training loss : 0.9339734864234924 TRAIN  loss dict:  {'classification_loss': 0.9339734864234924}
2025-01-13 10:54:47,341 [INFO] Step[900/2713]: training loss : 0.935919896364212 TRAIN  loss dict:  {'classification_loss': 0.935919896364212}
2025-01-13 10:54:59,275 [INFO] Step[950/2713]: training loss : 0.9341796851158142 TRAIN  loss dict:  {'classification_loss': 0.9341796851158142}
2025-01-13 10:55:11,196 [INFO] Step[1000/2713]: training loss : 0.9365759873390198 TRAIN  loss dict:  {'classification_loss': 0.9365759873390198}
2025-01-13 10:55:23,155 [INFO] Step[1050/2713]: training loss : 0.939999326467514 TRAIN  loss dict:  {'classification_loss': 0.939999326467514}
2025-01-13 10:55:35,104 [INFO] Step[1100/2713]: training loss : 0.9345500218868256 TRAIN  loss dict:  {'classification_loss': 0.9345500218868256}
2025-01-13 10:55:47,024 [INFO] Step[1150/2713]: training loss : 0.9336400389671325 TRAIN  loss dict:  {'classification_loss': 0.9336400389671325}
2025-01-13 10:55:58,944 [INFO] Step[1200/2713]: training loss : 0.9357117652893067 TRAIN  loss dict:  {'classification_loss': 0.9357117652893067}
2025-01-13 10:56:10,846 [INFO] Step[1250/2713]: training loss : 0.9342880082130433 TRAIN  loss dict:  {'classification_loss': 0.9342880082130433}
2025-01-13 10:56:22,742 [INFO] Step[1300/2713]: training loss : 0.9354950499534607 TRAIN  loss dict:  {'classification_loss': 0.9354950499534607}
2025-01-13 10:56:34,677 [INFO] Step[1350/2713]: training loss : 0.9345065546035767 TRAIN  loss dict:  {'classification_loss': 0.9345065546035767}
2025-01-13 10:56:46,564 [INFO] Step[1400/2713]: training loss : 0.9351366448402405 TRAIN  loss dict:  {'classification_loss': 0.9351366448402405}
2025-01-13 10:56:58,465 [INFO] Step[1450/2713]: training loss : 0.9330367481708527 TRAIN  loss dict:  {'classification_loss': 0.9330367481708527}
2025-01-13 10:57:10,327 [INFO] Step[1500/2713]: training loss : 0.9340652716159821 TRAIN  loss dict:  {'classification_loss': 0.9340652716159821}
2025-01-13 10:57:22,266 [INFO] Step[1550/2713]: training loss : 0.9358011770248413 TRAIN  loss dict:  {'classification_loss': 0.9358011770248413}
2025-01-13 10:57:34,133 [INFO] Step[1600/2713]: training loss : 0.9392761850357055 TRAIN  loss dict:  {'classification_loss': 0.9392761850357055}
2025-01-13 10:57:46,052 [INFO] Step[1650/2713]: training loss : 0.934854862689972 TRAIN  loss dict:  {'classification_loss': 0.934854862689972}
2025-01-13 10:57:57,929 [INFO] Step[1700/2713]: training loss : 0.9347212398052216 TRAIN  loss dict:  {'classification_loss': 0.9347212398052216}
2025-01-13 10:58:09,791 [INFO] Step[1750/2713]: training loss : 0.9360359704494476 TRAIN  loss dict:  {'classification_loss': 0.9360359704494476}
2025-01-13 10:58:21,713 [INFO] Step[1800/2713]: training loss : 0.9380907559394837 TRAIN  loss dict:  {'classification_loss': 0.9380907559394837}
2025-01-13 10:58:33,664 [INFO] Step[1850/2713]: training loss : 0.9336598443984986 TRAIN  loss dict:  {'classification_loss': 0.9336598443984986}
2025-01-13 10:58:45,597 [INFO] Step[1900/2713]: training loss : 0.9348468208312988 TRAIN  loss dict:  {'classification_loss': 0.9348468208312988}
2025-01-13 10:58:57,505 [INFO] Step[1950/2713]: training loss : 0.9344436955451966 TRAIN  loss dict:  {'classification_loss': 0.9344436955451966}
2025-01-13 10:59:09,420 [INFO] Step[2000/2713]: training loss : 0.9359552836418152 TRAIN  loss dict:  {'classification_loss': 0.9359552836418152}
2025-01-13 10:59:21,288 [INFO] Step[2050/2713]: training loss : 0.9345620560646057 TRAIN  loss dict:  {'classification_loss': 0.9345620560646057}
2025-01-13 10:59:33,168 [INFO] Step[2100/2713]: training loss : 0.9340128350257874 TRAIN  loss dict:  {'classification_loss': 0.9340128350257874}
2025-01-13 10:59:45,046 [INFO] Step[2150/2713]: training loss : 0.9347602093219757 TRAIN  loss dict:  {'classification_loss': 0.9347602093219757}
2025-01-13 10:59:56,964 [INFO] Step[2200/2713]: training loss : 0.9354388654232025 TRAIN  loss dict:  {'classification_loss': 0.9354388654232025}
2025-01-13 11:00:08,892 [INFO] Step[2250/2713]: training loss : 0.9345392382144928 TRAIN  loss dict:  {'classification_loss': 0.9345392382144928}
2025-01-13 11:00:20,795 [INFO] Step[2300/2713]: training loss : 0.9356410241127014 TRAIN  loss dict:  {'classification_loss': 0.9356410241127014}
2025-01-13 11:00:32,725 [INFO] Step[2350/2713]: training loss : 0.9364767324924469 TRAIN  loss dict:  {'classification_loss': 0.9364767324924469}
2025-01-13 11:00:44,703 [INFO] Step[2400/2713]: training loss : 0.9352594578266143 TRAIN  loss dict:  {'classification_loss': 0.9352594578266143}
2025-01-13 11:00:56,615 [INFO] Step[2450/2713]: training loss : 0.9352664470672607 TRAIN  loss dict:  {'classification_loss': 0.9352664470672607}
2025-01-13 11:01:08,522 [INFO] Step[2500/2713]: training loss : 0.9364597964286804 TRAIN  loss dict:  {'classification_loss': 0.9364597964286804}
2025-01-13 11:01:20,438 [INFO] Step[2550/2713]: training loss : 0.9384764730930328 TRAIN  loss dict:  {'classification_loss': 0.9384764730930328}
2025-01-13 11:01:32,352 [INFO] Step[2600/2713]: training loss : 0.9372922956943512 TRAIN  loss dict:  {'classification_loss': 0.9372922956943512}
2025-01-13 11:01:44,283 [INFO] Step[2650/2713]: training loss : 0.9337817466259003 TRAIN  loss dict:  {'classification_loss': 0.9337817466259003}
2025-01-13 11:01:56,174 [INFO] Step[2700/2713]: training loss : 0.934394052028656 TRAIN  loss dict:  {'classification_loss': 0.934394052028656}
2025-01-13 11:03:27,236 [INFO] Label accuracies statistics:
2025-01-13 11:03:27,236 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.5, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 1.0, 355: 1.0, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 1.0, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 11:03:27,239 [INFO] [97] TRAIN  loss: 0.9355750268032047 acc: 0.9997542695662858
2025-01-13 11:03:27,239 [INFO] [97] TRAIN  loss dict: {'classification_loss': 0.9355750268032047}
2025-01-13 11:03:27,239 [INFO] [97] VALIDATION loss: 1.7222841905247896 VALIDATION acc: 0.8288401253918495
2025-01-13 11:03:27,239 [INFO] [97] VALIDATION loss dict: {'classification_loss': 1.7222841905247896}
2025-01-13 11:03:27,239 [INFO] 
2025-01-13 11:03:46,790 [INFO] Step[50/2713]: training loss : 0.9343801891803741 TRAIN  loss dict:  {'classification_loss': 0.9343801891803741}
2025-01-13 11:03:59,401 [INFO] Step[100/2713]: training loss : 0.9400724112987519 TRAIN  loss dict:  {'classification_loss': 0.9400724112987519}
2025-01-13 11:04:11,786 [INFO] Step[150/2713]: training loss : 0.9347009217739105 TRAIN  loss dict:  {'classification_loss': 0.9347009217739105}
2025-01-13 11:04:24,707 [INFO] Step[200/2713]: training loss : 0.934429337978363 TRAIN  loss dict:  {'classification_loss': 0.934429337978363}
2025-01-13 11:04:38,289 [INFO] Step[250/2713]: training loss : 0.9347633576393127 TRAIN  loss dict:  {'classification_loss': 0.9347633576393127}
2025-01-13 11:04:51,404 [INFO] Step[300/2713]: training loss : 0.9350014662742615 TRAIN  loss dict:  {'classification_loss': 0.9350014662742615}
2025-01-13 11:05:03,556 [INFO] Step[350/2713]: training loss : 0.9366568505764008 TRAIN  loss dict:  {'classification_loss': 0.9366568505764008}
2025-01-13 11:05:15,432 [INFO] Step[400/2713]: training loss : 0.9336297249794007 TRAIN  loss dict:  {'classification_loss': 0.9336297249794007}
2025-01-13 11:05:27,321 [INFO] Step[450/2713]: training loss : 0.9351326644420623 TRAIN  loss dict:  {'classification_loss': 0.9351326644420623}
2025-01-13 11:05:39,217 [INFO] Step[500/2713]: training loss : 0.9367282474040985 TRAIN  loss dict:  {'classification_loss': 0.9367282474040985}
2025-01-13 11:05:51,148 [INFO] Step[550/2713]: training loss : 0.9386183094978332 TRAIN  loss dict:  {'classification_loss': 0.9386183094978332}
2025-01-13 11:06:03,091 [INFO] Step[600/2713]: training loss : 0.9400642013549805 TRAIN  loss dict:  {'classification_loss': 0.9400642013549805}
2025-01-13 11:06:15,004 [INFO] Step[650/2713]: training loss : 0.9356851875782013 TRAIN  loss dict:  {'classification_loss': 0.9356851875782013}
2025-01-13 11:06:26,919 [INFO] Step[700/2713]: training loss : 0.9351680529117584 TRAIN  loss dict:  {'classification_loss': 0.9351680529117584}
2025-01-13 11:06:38,839 [INFO] Step[750/2713]: training loss : 0.9357103419303894 TRAIN  loss dict:  {'classification_loss': 0.9357103419303894}
2025-01-13 11:06:50,811 [INFO] Step[800/2713]: training loss : 0.9349337136745453 TRAIN  loss dict:  {'classification_loss': 0.9349337136745453}
2025-01-13 11:07:02,740 [INFO] Step[850/2713]: training loss : 0.9343083429336548 TRAIN  loss dict:  {'classification_loss': 0.9343083429336548}
2025-01-13 11:07:14,679 [INFO] Step[900/2713]: training loss : 0.935713164806366 TRAIN  loss dict:  {'classification_loss': 0.935713164806366}
2025-01-13 11:07:26,638 [INFO] Step[950/2713]: training loss : 0.9357427096366883 TRAIN  loss dict:  {'classification_loss': 0.9357427096366883}
2025-01-13 11:07:38,570 [INFO] Step[1000/2713]: training loss : 0.9365973722934723 TRAIN  loss dict:  {'classification_loss': 0.9365973722934723}
2025-01-13 11:07:50,496 [INFO] Step[1050/2713]: training loss : 0.9349835240840911 TRAIN  loss dict:  {'classification_loss': 0.9349835240840911}
2025-01-13 11:08:02,440 [INFO] Step[1100/2713]: training loss : 0.933130898475647 TRAIN  loss dict:  {'classification_loss': 0.933130898475647}
2025-01-13 11:08:14,342 [INFO] Step[1150/2713]: training loss : 0.9350957763195038 TRAIN  loss dict:  {'classification_loss': 0.9350957763195038}
2025-01-13 11:08:26,323 [INFO] Step[1200/2713]: training loss : 0.9363193416595459 TRAIN  loss dict:  {'classification_loss': 0.9363193416595459}
2025-01-13 11:08:38,272 [INFO] Step[1250/2713]: training loss : 0.9336909902095795 TRAIN  loss dict:  {'classification_loss': 0.9336909902095795}
2025-01-13 11:08:50,242 [INFO] Step[1300/2713]: training loss : 0.9348585188388825 TRAIN  loss dict:  {'classification_loss': 0.9348585188388825}
2025-01-13 11:09:02,206 [INFO] Step[1350/2713]: training loss : 0.9349827468395233 TRAIN  loss dict:  {'classification_loss': 0.9349827468395233}
2025-01-13 11:09:14,117 [INFO] Step[1400/2713]: training loss : 0.9339422261714936 TRAIN  loss dict:  {'classification_loss': 0.9339422261714936}
2025-01-13 11:09:26,023 [INFO] Step[1450/2713]: training loss : 0.9351480031013488 TRAIN  loss dict:  {'classification_loss': 0.9351480031013488}
2025-01-13 11:09:37,988 [INFO] Step[1500/2713]: training loss : 0.934505488872528 TRAIN  loss dict:  {'classification_loss': 0.934505488872528}
2025-01-13 11:09:49,892 [INFO] Step[1550/2713]: training loss : 0.9346738457679749 TRAIN  loss dict:  {'classification_loss': 0.9346738457679749}
2025-01-13 11:10:01,838 [INFO] Step[1600/2713]: training loss : 0.9387361717224121 TRAIN  loss dict:  {'classification_loss': 0.9387361717224121}
2025-01-13 11:10:13,795 [INFO] Step[1650/2713]: training loss : 0.9345386183261871 TRAIN  loss dict:  {'classification_loss': 0.9345386183261871}
2025-01-13 11:10:25,719 [INFO] Step[1700/2713]: training loss : 0.9344154822826386 TRAIN  loss dict:  {'classification_loss': 0.9344154822826386}
2025-01-13 11:10:37,667 [INFO] Step[1750/2713]: training loss : 0.9356264352798462 TRAIN  loss dict:  {'classification_loss': 0.9356264352798462}
2025-01-13 11:10:49,579 [INFO] Step[1800/2713]: training loss : 0.933076502084732 TRAIN  loss dict:  {'classification_loss': 0.933076502084732}
2025-01-13 11:11:01,533 [INFO] Step[1850/2713]: training loss : 0.93529487490654 TRAIN  loss dict:  {'classification_loss': 0.93529487490654}
2025-01-13 11:11:13,472 [INFO] Step[1900/2713]: training loss : 0.9392366981506348 TRAIN  loss dict:  {'classification_loss': 0.9392366981506348}
2025-01-13 11:11:25,404 [INFO] Step[1950/2713]: training loss : 0.9357263851165771 TRAIN  loss dict:  {'classification_loss': 0.9357263851165771}
2025-01-13 11:11:37,318 [INFO] Step[2000/2713]: training loss : 0.9348276293277741 TRAIN  loss dict:  {'classification_loss': 0.9348276293277741}
2025-01-13 11:11:49,303 [INFO] Step[2050/2713]: training loss : 0.9340141260623932 TRAIN  loss dict:  {'classification_loss': 0.9340141260623932}
2025-01-13 11:12:01,208 [INFO] Step[2100/2713]: training loss : 0.9348722755908966 TRAIN  loss dict:  {'classification_loss': 0.9348722755908966}
2025-01-13 11:12:13,175 [INFO] Step[2150/2713]: training loss : 0.9351427781581879 TRAIN  loss dict:  {'classification_loss': 0.9351427781581879}
2025-01-13 11:12:25,070 [INFO] Step[2200/2713]: training loss : 0.9358873319625854 TRAIN  loss dict:  {'classification_loss': 0.9358873319625854}
2025-01-13 11:12:37,049 [INFO] Step[2250/2713]: training loss : 0.9339236533641815 TRAIN  loss dict:  {'classification_loss': 0.9339236533641815}
2025-01-13 11:12:48,941 [INFO] Step[2300/2713]: training loss : 0.9341500926017762 TRAIN  loss dict:  {'classification_loss': 0.9341500926017762}
2025-01-13 11:13:00,875 [INFO] Step[2350/2713]: training loss : 0.9349808442592621 TRAIN  loss dict:  {'classification_loss': 0.9349808442592621}
2025-01-13 11:13:12,821 [INFO] Step[2400/2713]: training loss : 0.9341303515434265 TRAIN  loss dict:  {'classification_loss': 0.9341303515434265}
2025-01-13 11:13:24,776 [INFO] Step[2450/2713]: training loss : 0.9369540548324585 TRAIN  loss dict:  {'classification_loss': 0.9369540548324585}
2025-01-13 11:13:36,737 [INFO] Step[2500/2713]: training loss : 0.935609962940216 TRAIN  loss dict:  {'classification_loss': 0.935609962940216}
2025-01-13 11:13:48,638 [INFO] Step[2550/2713]: training loss : 0.9363145124912262 TRAIN  loss dict:  {'classification_loss': 0.9363145124912262}
2025-01-13 11:14:00,511 [INFO] Step[2600/2713]: training loss : 0.935992294549942 TRAIN  loss dict:  {'classification_loss': 0.935992294549942}
2025-01-13 11:14:12,424 [INFO] Step[2650/2713]: training loss : 0.9378403973579407 TRAIN  loss dict:  {'classification_loss': 0.9378403973579407}
2025-01-13 11:14:24,373 [INFO] Step[2700/2713]: training loss : 0.9331244051456451 TRAIN  loss dict:  {'classification_loss': 0.9331244051456451}
2025-01-13 11:15:51,254 [INFO] Label accuracies statistics:
2025-01-13 11:15:51,254 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.0, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 1.0, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 1.0, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 1.0, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 11:15:51,257 [INFO] [98] TRAIN  loss: 0.9354671246004509 acc: 1.0
2025-01-13 11:15:51,257 [INFO] [98] TRAIN  loss dict: {'classification_loss': 0.9354671246004509}
2025-01-13 11:15:51,257 [INFO] [98] VALIDATION loss: 1.7048430208648955 VALIDATION acc: 0.8369905956112853
2025-01-13 11:15:51,257 [INFO] [98] VALIDATION loss dict: {'classification_loss': 1.7048430208648955}
2025-01-13 11:15:51,257 [INFO] 
2025-01-13 11:16:09,300 [INFO] Step[50/2713]: training loss : 0.935366358757019 TRAIN  loss dict:  {'classification_loss': 0.935366358757019}
2025-01-13 11:16:21,223 [INFO] Step[100/2713]: training loss : 0.9350445413589478 TRAIN  loss dict:  {'classification_loss': 0.9350445413589478}
2025-01-13 11:16:33,171 [INFO] Step[150/2713]: training loss : 0.9339830362796784 TRAIN  loss dict:  {'classification_loss': 0.9339830362796784}
2025-01-13 11:16:45,064 [INFO] Step[200/2713]: training loss : 0.9360147440433502 TRAIN  loss dict:  {'classification_loss': 0.9360147440433502}
2025-01-13 11:16:57,012 [INFO] Step[250/2713]: training loss : 0.9340766620635986 TRAIN  loss dict:  {'classification_loss': 0.9340766620635986}
2025-01-13 11:17:08,944 [INFO] Step[300/2713]: training loss : 0.9351441407203674 TRAIN  loss dict:  {'classification_loss': 0.9351441407203674}
2025-01-13 11:17:20,891 [INFO] Step[350/2713]: training loss : 0.933288516998291 TRAIN  loss dict:  {'classification_loss': 0.933288516998291}
2025-01-13 11:17:32,882 [INFO] Step[400/2713]: training loss : 0.9357958889007568 TRAIN  loss dict:  {'classification_loss': 0.9357958889007568}
2025-01-13 11:17:44,843 [INFO] Step[450/2713]: training loss : 0.9341850411891938 TRAIN  loss dict:  {'classification_loss': 0.9341850411891938}
2025-01-13 11:17:56,696 [INFO] Step[500/2713]: training loss : 0.9363471746444703 TRAIN  loss dict:  {'classification_loss': 0.9363471746444703}
2025-01-13 11:18:08,627 [INFO] Step[550/2713]: training loss : 0.9342498219013214 TRAIN  loss dict:  {'classification_loss': 0.9342498219013214}
2025-01-13 11:18:20,594 [INFO] Step[600/2713]: training loss : 0.9344614505767822 TRAIN  loss dict:  {'classification_loss': 0.9344614505767822}
2025-01-13 11:18:32,521 [INFO] Step[650/2713]: training loss : 0.933472843170166 TRAIN  loss dict:  {'classification_loss': 0.933472843170166}
2025-01-13 11:18:44,460 [INFO] Step[700/2713]: training loss : 0.935971657037735 TRAIN  loss dict:  {'classification_loss': 0.935971657037735}
2025-01-13 11:18:56,457 [INFO] Step[750/2713]: training loss : 0.935577951669693 TRAIN  loss dict:  {'classification_loss': 0.935577951669693}
2025-01-13 11:19:08,378 [INFO] Step[800/2713]: training loss : 0.9340684998035431 TRAIN  loss dict:  {'classification_loss': 0.9340684998035431}
2025-01-13 11:19:20,293 [INFO] Step[850/2713]: training loss : 0.9347975122928619 TRAIN  loss dict:  {'classification_loss': 0.9347975122928619}
2025-01-13 11:19:32,234 [INFO] Step[900/2713]: training loss : 0.9351254546642304 TRAIN  loss dict:  {'classification_loss': 0.9351254546642304}
2025-01-13 11:19:44,192 [INFO] Step[950/2713]: training loss : 0.9346913659572601 TRAIN  loss dict:  {'classification_loss': 0.9346913659572601}
2025-01-13 11:19:56,089 [INFO] Step[1000/2713]: training loss : 0.933966017961502 TRAIN  loss dict:  {'classification_loss': 0.933966017961502}
2025-01-13 11:20:08,005 [INFO] Step[1050/2713]: training loss : 0.9347916471958161 TRAIN  loss dict:  {'classification_loss': 0.9347916471958161}
2025-01-13 11:20:19,965 [INFO] Step[1100/2713]: training loss : 0.9332049524784088 TRAIN  loss dict:  {'classification_loss': 0.9332049524784088}
2025-01-13 11:20:31,925 [INFO] Step[1150/2713]: training loss : 0.9355864739418029 TRAIN  loss dict:  {'classification_loss': 0.9355864739418029}
2025-01-13 11:20:43,869 [INFO] Step[1200/2713]: training loss : 0.9343581318855285 TRAIN  loss dict:  {'classification_loss': 0.9343581318855285}
2025-01-13 11:20:55,842 [INFO] Step[1250/2713]: training loss : 0.9340904486179352 TRAIN  loss dict:  {'classification_loss': 0.9340904486179352}
2025-01-13 11:21:07,792 [INFO] Step[1300/2713]: training loss : 0.9368840074539184 TRAIN  loss dict:  {'classification_loss': 0.9368840074539184}
2025-01-13 11:21:19,730 [INFO] Step[1350/2713]: training loss : 0.9370053076744079 TRAIN  loss dict:  {'classification_loss': 0.9370053076744079}
2025-01-13 11:21:31,626 [INFO] Step[1400/2713]: training loss : 0.9357473421096801 TRAIN  loss dict:  {'classification_loss': 0.9357473421096801}
2025-01-13 11:21:43,570 [INFO] Step[1450/2713]: training loss : 0.9354239571094513 TRAIN  loss dict:  {'classification_loss': 0.9354239571094513}
2025-01-13 11:21:55,582 [INFO] Step[1500/2713]: training loss : 0.9344326889514923 TRAIN  loss dict:  {'classification_loss': 0.9344326889514923}
2025-01-13 11:22:08,096 [INFO] Step[1550/2713]: training loss : 0.9366184067726135 TRAIN  loss dict:  {'classification_loss': 0.9366184067726135}
2025-01-13 11:22:20,501 [INFO] Step[1600/2713]: training loss : 0.9364052128791809 TRAIN  loss dict:  {'classification_loss': 0.9364052128791809}
2025-01-13 11:22:32,699 [INFO] Step[1650/2713]: training loss : 0.9335019612312316 TRAIN  loss dict:  {'classification_loss': 0.9335019612312316}
2025-01-13 11:22:45,379 [INFO] Step[1700/2713]: training loss : 0.9335178446769714 TRAIN  loss dict:  {'classification_loss': 0.9335178446769714}
2025-01-13 11:22:57,738 [INFO] Step[1750/2713]: training loss : 0.9353499007225037 TRAIN  loss dict:  {'classification_loss': 0.9353499007225037}
2025-01-13 11:23:10,424 [INFO] Step[1800/2713]: training loss : 0.9340980958938598 TRAIN  loss dict:  {'classification_loss': 0.9340980958938598}
2025-01-13 11:23:23,930 [INFO] Step[1850/2713]: training loss : 0.9351322901248932 TRAIN  loss dict:  {'classification_loss': 0.9351322901248932}
2025-01-13 11:23:37,873 [INFO] Step[1900/2713]: training loss : 0.9347499203681946 TRAIN  loss dict:  {'classification_loss': 0.9347499203681946}
2025-01-13 11:23:50,239 [INFO] Step[1950/2713]: training loss : 0.9374038565158844 TRAIN  loss dict:  {'classification_loss': 0.9374038565158844}
2025-01-13 11:24:02,141 [INFO] Step[2000/2713]: training loss : 0.934008173942566 TRAIN  loss dict:  {'classification_loss': 0.934008173942566}
2025-01-13 11:24:14,015 [INFO] Step[2050/2713]: training loss : 0.9351545822620392 TRAIN  loss dict:  {'classification_loss': 0.9351545822620392}
2025-01-13 11:24:25,911 [INFO] Step[2100/2713]: training loss : 0.9344363188743592 TRAIN  loss dict:  {'classification_loss': 0.9344363188743592}
2025-01-13 11:24:37,777 [INFO] Step[2150/2713]: training loss : 0.9353927290439605 TRAIN  loss dict:  {'classification_loss': 0.9353927290439605}
2025-01-13 11:24:49,679 [INFO] Step[2200/2713]: training loss : 0.9359127569198609 TRAIN  loss dict:  {'classification_loss': 0.9359127569198609}
2025-01-13 11:25:01,632 [INFO] Step[2250/2713]: training loss : 0.9349940240383148 TRAIN  loss dict:  {'classification_loss': 0.9349940240383148}
2025-01-13 11:25:13,500 [INFO] Step[2300/2713]: training loss : 0.9336670184135437 TRAIN  loss dict:  {'classification_loss': 0.9336670184135437}
2025-01-13 11:25:25,375 [INFO] Step[2350/2713]: training loss : 0.9356322455406189 TRAIN  loss dict:  {'classification_loss': 0.9356322455406189}
2025-01-13 11:25:37,290 [INFO] Step[2400/2713]: training loss : 0.9351317739486694 TRAIN  loss dict:  {'classification_loss': 0.9351317739486694}
2025-01-13 11:25:49,189 [INFO] Step[2450/2713]: training loss : 0.9342890810966492 TRAIN  loss dict:  {'classification_loss': 0.9342890810966492}
2025-01-13 11:26:01,111 [INFO] Step[2500/2713]: training loss : 0.9337699782848358 TRAIN  loss dict:  {'classification_loss': 0.9337699782848358}
2025-01-13 11:26:13,020 [INFO] Step[2550/2713]: training loss : 0.9344268357753753 TRAIN  loss dict:  {'classification_loss': 0.9344268357753753}
2025-01-13 11:26:24,998 [INFO] Step[2600/2713]: training loss : 0.9339844048023224 TRAIN  loss dict:  {'classification_loss': 0.9339844048023224}
2025-01-13 11:26:36,893 [INFO] Step[2650/2713]: training loss : 0.9352067303657532 TRAIN  loss dict:  {'classification_loss': 0.9352067303657532}
2025-01-13 11:26:48,841 [INFO] Step[2700/2713]: training loss : 0.9369445252418518 TRAIN  loss dict:  {'classification_loss': 0.9369445252418518}
2025-01-13 11:28:17,034 [INFO] Label accuracies statistics:
2025-01-13 11:28:17,034 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 0.75, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 0.75, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.5, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 11:28:17,036 [INFO] [99] TRAIN  loss: 0.9349329786418419 acc: 1.0
2025-01-13 11:28:17,036 [INFO] [99] TRAIN  loss dict: {'classification_loss': 0.9349329786418419}
2025-01-13 11:28:17,036 [INFO] [99] VALIDATION loss: 1.6836541522490351 VALIDATION acc: 0.8376175548589342
2025-01-13 11:28:17,036 [INFO] [99] VALIDATION loss dict: {'classification_loss': 1.6836541522490351}
2025-01-13 11:28:17,036 [INFO] 
2025-01-13 11:28:35,511 [INFO] Step[50/2713]: training loss : 0.9341129922866821 TRAIN  loss dict:  {'classification_loss': 0.9341129922866821}
2025-01-13 11:28:47,406 [INFO] Step[100/2713]: training loss : 0.9347552800178528 TRAIN  loss dict:  {'classification_loss': 0.9347552800178528}
2025-01-13 11:28:59,340 [INFO] Step[150/2713]: training loss : 0.9341108429431916 TRAIN  loss dict:  {'classification_loss': 0.9341108429431916}
2025-01-13 11:29:11,235 [INFO] Step[200/2713]: training loss : 0.9365989875793457 TRAIN  loss dict:  {'classification_loss': 0.9365989875793457}
2025-01-13 11:29:23,169 [INFO] Step[250/2713]: training loss : 0.9368386018276215 TRAIN  loss dict:  {'classification_loss': 0.9368386018276215}
2025-01-13 11:29:35,074 [INFO] Step[300/2713]: training loss : 0.9348121476173401 TRAIN  loss dict:  {'classification_loss': 0.9348121476173401}
2025-01-13 11:29:47,013 [INFO] Step[350/2713]: training loss : 0.9341306436061859 TRAIN  loss dict:  {'classification_loss': 0.9341306436061859}
2025-01-13 11:29:58,912 [INFO] Step[400/2713]: training loss : 0.9345528316497803 TRAIN  loss dict:  {'classification_loss': 0.9345528316497803}
2025-01-13 11:30:10,861 [INFO] Step[450/2713]: training loss : 0.93639084815979 TRAIN  loss dict:  {'classification_loss': 0.93639084815979}
2025-01-13 11:30:22,788 [INFO] Step[500/2713]: training loss : 0.9350385570526123 TRAIN  loss dict:  {'classification_loss': 0.9350385570526123}
2025-01-13 11:30:34,721 [INFO] Step[550/2713]: training loss : 0.9333615851402283 TRAIN  loss dict:  {'classification_loss': 0.9333615851402283}
2025-01-13 11:30:46,634 [INFO] Step[600/2713]: training loss : 0.9337990450859069 TRAIN  loss dict:  {'classification_loss': 0.9337990450859069}
2025-01-13 11:30:58,535 [INFO] Step[650/2713]: training loss : 0.9342359924316406 TRAIN  loss dict:  {'classification_loss': 0.9342359924316406}
2025-01-13 11:31:10,508 [INFO] Step[700/2713]: training loss : 0.9337616837024689 TRAIN  loss dict:  {'classification_loss': 0.9337616837024689}
2025-01-13 11:31:22,393 [INFO] Step[750/2713]: training loss : 0.9340728008747101 TRAIN  loss dict:  {'classification_loss': 0.9340728008747101}
2025-01-13 11:31:34,352 [INFO] Step[800/2713]: training loss : 0.9353837823867798 TRAIN  loss dict:  {'classification_loss': 0.9353837823867798}
2025-01-13 11:31:46,289 [INFO] Step[850/2713]: training loss : 0.9348491537570953 TRAIN  loss dict:  {'classification_loss': 0.9348491537570953}
2025-01-13 11:31:58,176 [INFO] Step[900/2713]: training loss : 0.934530029296875 TRAIN  loss dict:  {'classification_loss': 0.934530029296875}
2025-01-13 11:32:10,087 [INFO] Step[950/2713]: training loss : 0.9349750900268554 TRAIN  loss dict:  {'classification_loss': 0.9349750900268554}
2025-01-13 11:32:22,018 [INFO] Step[1000/2713]: training loss : 0.9337102746963502 TRAIN  loss dict:  {'classification_loss': 0.9337102746963502}
2025-01-13 11:32:33,939 [INFO] Step[1050/2713]: training loss : 0.9346652138233185 TRAIN  loss dict:  {'classification_loss': 0.9346652138233185}
2025-01-13 11:32:45,877 [INFO] Step[1100/2713]: training loss : 0.9364851438999175 TRAIN  loss dict:  {'classification_loss': 0.9364851438999175}
2025-01-13 11:32:57,783 [INFO] Step[1150/2713]: training loss : 0.9350734078884124 TRAIN  loss dict:  {'classification_loss': 0.9350734078884124}
2025-01-13 11:33:09,774 [INFO] Step[1200/2713]: training loss : 0.9349683809280396 TRAIN  loss dict:  {'classification_loss': 0.9349683809280396}
2025-01-13 11:33:21,714 [INFO] Step[1250/2713]: training loss : 0.9352799630165101 TRAIN  loss dict:  {'classification_loss': 0.9352799630165101}
2025-01-13 11:33:33,658 [INFO] Step[1300/2713]: training loss : 0.9347375702857971 TRAIN  loss dict:  {'classification_loss': 0.9347375702857971}
2025-01-13 11:33:45,584 [INFO] Step[1350/2713]: training loss : 0.9339990150928498 TRAIN  loss dict:  {'classification_loss': 0.9339990150928498}
2025-01-13 11:33:57,481 [INFO] Step[1400/2713]: training loss : 0.9342168307304383 TRAIN  loss dict:  {'classification_loss': 0.9342168307304383}
2025-01-13 11:34:09,397 [INFO] Step[1450/2713]: training loss : 0.9345095193386078 TRAIN  loss dict:  {'classification_loss': 0.9345095193386078}
2025-01-13 11:34:21,308 [INFO] Step[1500/2713]: training loss : 0.9343077182769776 TRAIN  loss dict:  {'classification_loss': 0.9343077182769776}
2025-01-13 11:34:33,233 [INFO] Step[1550/2713]: training loss : 0.9346087205410004 TRAIN  loss dict:  {'classification_loss': 0.9346087205410004}
2025-01-13 11:34:45,150 [INFO] Step[1600/2713]: training loss : 0.9347837519645691 TRAIN  loss dict:  {'classification_loss': 0.9347837519645691}
2025-01-13 11:34:57,109 [INFO] Step[1650/2713]: training loss : 0.9379371321201324 TRAIN  loss dict:  {'classification_loss': 0.9379371321201324}
2025-01-13 11:35:09,046 [INFO] Step[1700/2713]: training loss : 0.9347796869277955 TRAIN  loss dict:  {'classification_loss': 0.9347796869277955}
2025-01-13 11:35:20,957 [INFO] Step[1750/2713]: training loss : 0.9344569265842437 TRAIN  loss dict:  {'classification_loss': 0.9344569265842437}
2025-01-13 11:35:32,931 [INFO] Step[1800/2713]: training loss : 0.9356176245212555 TRAIN  loss dict:  {'classification_loss': 0.9356176245212555}
2025-01-13 11:35:44,855 [INFO] Step[1850/2713]: training loss : 0.9343842315673828 TRAIN  loss dict:  {'classification_loss': 0.9343842315673828}
2025-01-13 11:35:56,787 [INFO] Step[1900/2713]: training loss : 0.9340020442008972 TRAIN  loss dict:  {'classification_loss': 0.9340020442008972}
2025-01-13 11:36:08,710 [INFO] Step[1950/2713]: training loss : 0.9366328811645508 TRAIN  loss dict:  {'classification_loss': 0.9366328811645508}
2025-01-13 11:36:20,639 [INFO] Step[2000/2713]: training loss : 0.9345615124702453 TRAIN  loss dict:  {'classification_loss': 0.9345615124702453}
2025-01-13 11:36:32,530 [INFO] Step[2050/2713]: training loss : 0.9353074312210083 TRAIN  loss dict:  {'classification_loss': 0.9353074312210083}
2025-01-13 11:36:44,450 [INFO] Step[2100/2713]: training loss : 0.9346139681339264 TRAIN  loss dict:  {'classification_loss': 0.9346139681339264}
2025-01-13 11:36:56,396 [INFO] Step[2150/2713]: training loss : 0.9340481531620025 TRAIN  loss dict:  {'classification_loss': 0.9340481531620025}
2025-01-13 11:37:08,500 [INFO] Step[2200/2713]: training loss : 0.9339993703365326 TRAIN  loss dict:  {'classification_loss': 0.9339993703365326}
2025-01-13 11:37:20,604 [INFO] Step[2250/2713]: training loss : 0.9345369791984558 TRAIN  loss dict:  {'classification_loss': 0.9345369791984558}
2025-01-13 11:37:32,557 [INFO] Step[2300/2713]: training loss : 0.9538044953346252 TRAIN  loss dict:  {'classification_loss': 0.9538044953346252}
2025-01-13 11:37:44,633 [INFO] Step[2350/2713]: training loss : 0.9351012349128723 TRAIN  loss dict:  {'classification_loss': 0.9351012349128723}
2025-01-13 11:37:56,533 [INFO] Step[2400/2713]: training loss : 0.9345504724979401 TRAIN  loss dict:  {'classification_loss': 0.9345504724979401}
2025-01-13 11:38:08,491 [INFO] Step[2450/2713]: training loss : 0.934906085729599 TRAIN  loss dict:  {'classification_loss': 0.934906085729599}
2025-01-13 11:38:20,459 [INFO] Step[2500/2713]: training loss : 0.9332265424728393 TRAIN  loss dict:  {'classification_loss': 0.9332265424728393}
2025-01-13 11:38:32,401 [INFO] Step[2550/2713]: training loss : 0.9363449621200561 TRAIN  loss dict:  {'classification_loss': 0.9363449621200561}
2025-01-13 11:38:44,310 [INFO] Step[2600/2713]: training loss : 0.9352415442466736 TRAIN  loss dict:  {'classification_loss': 0.9352415442466736}
2025-01-13 11:38:56,240 [INFO] Step[2650/2713]: training loss : 0.9348744666576385 TRAIN  loss dict:  {'classification_loss': 0.9348744666576385}
2025-01-13 11:39:08,170 [INFO] Step[2700/2713]: training loss : 0.9343223249912262 TRAIN  loss dict:  {'classification_loss': 0.9343223249912262}
2025-01-13 11:40:34,959 [INFO] Label accuracies statistics:
2025-01-13 11:40:34,959 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.75, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 0.75, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.5, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 1.0, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.5, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.5, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 11:40:34,962 [INFO] [100] TRAIN  loss: 0.9351594823117718 acc: 0.9998771347831429
2025-01-13 11:40:34,962 [INFO] [100] TRAIN  loss dict: {'classification_loss': 0.9351594823117718}
2025-01-13 11:40:34,962 [INFO] [100] VALIDATION loss: 1.684570591364588 VALIDATION acc: 0.8357366771159874
2025-01-13 11:40:34,962 [INFO] [100] VALIDATION loss dict: {'classification_loss': 1.684570591364588}
2025-01-13 11:40:34,962 [INFO] 
2025-01-13 11:40:52,580 [INFO] Step[50/2713]: training loss : 0.93434046626091 TRAIN  loss dict:  {'classification_loss': 0.93434046626091}
2025-01-13 11:41:04,988 [INFO] Step[100/2713]: training loss : 0.9348449206352234 TRAIN  loss dict:  {'classification_loss': 0.9348449206352234}
2025-01-13 11:41:17,819 [INFO] Step[150/2713]: training loss : 0.9351337265968322 TRAIN  loss dict:  {'classification_loss': 0.9351337265968322}
2025-01-13 11:41:30,200 [INFO] Step[200/2713]: training loss : 0.9333628630638122 TRAIN  loss dict:  {'classification_loss': 0.9333628630638122}
2025-01-13 11:41:42,454 [INFO] Step[250/2713]: training loss : 0.9329380881786347 TRAIN  loss dict:  {'classification_loss': 0.9329380881786347}
2025-01-13 11:41:55,312 [INFO] Step[300/2713]: training loss : 0.9351313698291779 TRAIN  loss dict:  {'classification_loss': 0.9351313698291779}
2025-01-13 11:42:07,756 [INFO] Step[350/2713]: training loss : 0.935315603017807 TRAIN  loss dict:  {'classification_loss': 0.935315603017807}
2025-01-13 11:42:20,601 [INFO] Step[400/2713]: training loss : 0.9356480240821838 TRAIN  loss dict:  {'classification_loss': 0.9356480240821838}
2025-01-13 11:42:34,244 [INFO] Step[450/2713]: training loss : 0.934339234828949 TRAIN  loss dict:  {'classification_loss': 0.934339234828949}
2025-01-13 11:42:47,674 [INFO] Step[500/2713]: training loss : 0.9345761251449585 TRAIN  loss dict:  {'classification_loss': 0.9345761251449585}
2025-01-13 11:42:59,705 [INFO] Step[550/2713]: training loss : 0.9342001450061798 TRAIN  loss dict:  {'classification_loss': 0.9342001450061798}
2025-01-13 11:43:11,547 [INFO] Step[600/2713]: training loss : 0.9341872191429138 TRAIN  loss dict:  {'classification_loss': 0.9341872191429138}
2025-01-13 11:43:23,341 [INFO] Step[650/2713]: training loss : 0.93338627576828 TRAIN  loss dict:  {'classification_loss': 0.93338627576828}
2025-01-13 11:43:35,150 [INFO] Step[700/2713]: training loss : 0.9346531498432159 TRAIN  loss dict:  {'classification_loss': 0.9346531498432159}
2025-01-13 11:43:47,026 [INFO] Step[750/2713]: training loss : 0.9375759053230286 TRAIN  loss dict:  {'classification_loss': 0.9375759053230286}
2025-01-13 11:43:59,024 [INFO] Step[800/2713]: training loss : 0.9371927833557129 TRAIN  loss dict:  {'classification_loss': 0.9371927833557129}
2025-01-13 11:44:10,887 [INFO] Step[850/2713]: training loss : 0.9355369889736176 TRAIN  loss dict:  {'classification_loss': 0.9355369889736176}
2025-01-13 11:44:22,883 [INFO] Step[900/2713]: training loss : 0.9332271087169647 TRAIN  loss dict:  {'classification_loss': 0.9332271087169647}
2025-01-13 11:44:34,787 [INFO] Step[950/2713]: training loss : 0.9341415548324585 TRAIN  loss dict:  {'classification_loss': 0.9341415548324585}
2025-01-13 11:44:46,592 [INFO] Step[1000/2713]: training loss : 0.934353837966919 TRAIN  loss dict:  {'classification_loss': 0.934353837966919}
2025-01-13 11:44:58,588 [INFO] Step[1050/2713]: training loss : 0.9386546540260315 TRAIN  loss dict:  {'classification_loss': 0.9386546540260315}
2025-01-13 11:45:10,678 [INFO] Step[1100/2713]: training loss : 0.9342232012748718 TRAIN  loss dict:  {'classification_loss': 0.9342232012748718}
2025-01-13 11:45:22,696 [INFO] Step[1150/2713]: training loss : 0.9347762036323547 TRAIN  loss dict:  {'classification_loss': 0.9347762036323547}
2025-01-13 11:45:34,964 [INFO] Step[1200/2713]: training loss : 0.9320818459987641 TRAIN  loss dict:  {'classification_loss': 0.9320818459987641}
2025-01-13 11:45:46,870 [INFO] Step[1250/2713]: training loss : 0.9342813849449157 TRAIN  loss dict:  {'classification_loss': 0.9342813849449157}
2025-01-13 11:45:58,774 [INFO] Step[1300/2713]: training loss : 0.9342426967620849 TRAIN  loss dict:  {'classification_loss': 0.9342426967620849}
2025-01-13 11:46:11,602 [INFO] Step[1350/2713]: training loss : 0.935258059501648 TRAIN  loss dict:  {'classification_loss': 0.935258059501648}
2025-01-13 11:46:24,046 [INFO] Step[1400/2713]: training loss : 0.9354432225227356 TRAIN  loss dict:  {'classification_loss': 0.9354432225227356}
2025-01-13 11:46:37,295 [INFO] Step[1450/2713]: training loss : 0.9335982203483582 TRAIN  loss dict:  {'classification_loss': 0.9335982203483582}
2025-01-13 11:46:50,564 [INFO] Step[1500/2713]: training loss : 0.9352180027961731 TRAIN  loss dict:  {'classification_loss': 0.9352180027961731}
2025-01-13 11:47:03,176 [INFO] Step[1550/2713]: training loss : 0.934624742269516 TRAIN  loss dict:  {'classification_loss': 0.934624742269516}
2025-01-13 11:47:15,520 [INFO] Step[1600/2713]: training loss : 0.934836152791977 TRAIN  loss dict:  {'classification_loss': 0.934836152791977}
2025-01-13 11:47:28,750 [INFO] Step[1650/2713]: training loss : 0.9337900030612946 TRAIN  loss dict:  {'classification_loss': 0.9337900030612946}
2025-01-13 11:47:42,353 [INFO] Step[1700/2713]: training loss : 0.9335858929157257 TRAIN  loss dict:  {'classification_loss': 0.9335858929157257}
2025-01-13 11:47:55,698 [INFO] Step[1750/2713]: training loss : 0.9336043751239776 TRAIN  loss dict:  {'classification_loss': 0.9336043751239776}
2025-01-13 11:48:08,415 [INFO] Step[1800/2713]: training loss : 0.9352031874656678 TRAIN  loss dict:  {'classification_loss': 0.9352031874656678}
2025-01-13 11:48:21,449 [INFO] Step[1850/2713]: training loss : 0.9364889526367187 TRAIN  loss dict:  {'classification_loss': 0.9364889526367187}
2025-01-13 11:48:34,552 [INFO] Step[1900/2713]: training loss : 0.9341122877597808 TRAIN  loss dict:  {'classification_loss': 0.9341122877597808}
2025-01-13 11:48:47,748 [INFO] Step[1950/2713]: training loss : 0.9337060868740081 TRAIN  loss dict:  {'classification_loss': 0.9337060868740081}
2025-01-13 11:49:00,842 [INFO] Step[2000/2713]: training loss : 0.9344731593132019 TRAIN  loss dict:  {'classification_loss': 0.9344731593132019}
2025-01-13 11:49:13,242 [INFO] Step[2050/2713]: training loss : 0.9339110636711121 TRAIN  loss dict:  {'classification_loss': 0.9339110636711121}
2025-01-13 11:49:26,410 [INFO] Step[2100/2713]: training loss : 0.9341754949092865 TRAIN  loss dict:  {'classification_loss': 0.9341754949092865}
2025-01-13 11:49:39,596 [INFO] Step[2150/2713]: training loss : 0.9334954965114594 TRAIN  loss dict:  {'classification_loss': 0.9334954965114594}
2025-01-13 11:49:52,893 [INFO] Step[2200/2713]: training loss : 0.933417454957962 TRAIN  loss dict:  {'classification_loss': 0.933417454957962}
2025-01-13 11:50:05,829 [INFO] Step[2250/2713]: training loss : 0.9331938898563386 TRAIN  loss dict:  {'classification_loss': 0.9331938898563386}
2025-01-13 11:50:19,051 [INFO] Step[2300/2713]: training loss : 0.9339350390434266 TRAIN  loss dict:  {'classification_loss': 0.9339350390434266}
2025-01-13 11:50:32,265 [INFO] Step[2350/2713]: training loss : 0.9330025684833526 TRAIN  loss dict:  {'classification_loss': 0.9330025684833526}
2025-01-13 11:50:45,346 [INFO] Step[2400/2713]: training loss : 0.9350079691410065 TRAIN  loss dict:  {'classification_loss': 0.9350079691410065}
2025-01-13 11:50:58,007 [INFO] Step[2450/2713]: training loss : 0.9483649611473084 TRAIN  loss dict:  {'classification_loss': 0.9483649611473084}
2025-01-13 11:51:10,032 [INFO] Step[2500/2713]: training loss : 0.9350468528270721 TRAIN  loss dict:  {'classification_loss': 0.9350468528270721}
2025-01-13 11:51:23,068 [INFO] Step[2550/2713]: training loss : 0.9340561270713806 TRAIN  loss dict:  {'classification_loss': 0.9340561270713806}
2025-01-13 11:51:36,373 [INFO] Step[2600/2713]: training loss : 0.9334637260437012 TRAIN  loss dict:  {'classification_loss': 0.9334637260437012}
2025-01-13 11:51:49,840 [INFO] Step[2650/2713]: training loss : 0.9352069973945618 TRAIN  loss dict:  {'classification_loss': 0.9352069973945618}
2025-01-13 11:52:02,626 [INFO] Step[2700/2713]: training loss : 0.9344559776782989 TRAIN  loss dict:  {'classification_loss': 0.9344559776782989}
2025-01-13 11:54:29,318 [INFO] Label accuracies statistics:
2025-01-13 11:54:29,318 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 0.75, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 0.75, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.5, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 0.75, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.5, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 11:54:29,327 [INFO] [101] TRAIN  loss: 0.9347578929527809 acc: 1.0
2025-01-13 11:54:29,327 [INFO] [101] TRAIN  loss dict: {'classification_loss': 0.9347578929527809}
2025-01-13 11:54:29,328 [INFO] [101] VALIDATION loss: 1.6975551746169428 VALIDATION acc: 0.8332288401253919
2025-01-13 11:54:29,328 [INFO] [101] VALIDATION loss dict: {'classification_loss': 1.6975551746169428}
2025-01-13 11:54:29,328 [INFO] 
2025-01-13 11:54:52,898 [INFO] Step[50/2713]: training loss : 0.934394121170044 TRAIN  loss dict:  {'classification_loss': 0.934394121170044}
2025-01-13 11:55:06,175 [INFO] Step[100/2713]: training loss : 0.9352649629116059 TRAIN  loss dict:  {'classification_loss': 0.9352649629116059}
2025-01-13 11:55:18,549 [INFO] Step[150/2713]: training loss : 0.9368363785743713 TRAIN  loss dict:  {'classification_loss': 0.9368363785743713}
2025-01-13 11:55:30,863 [INFO] Step[200/2713]: training loss : 0.9338170742988586 TRAIN  loss dict:  {'classification_loss': 0.9338170742988586}
2025-01-13 11:55:44,392 [INFO] Step[250/2713]: training loss : 0.9340320074558258 TRAIN  loss dict:  {'classification_loss': 0.9340320074558258}
2025-01-13 11:55:58,590 [INFO] Step[300/2713]: training loss : 0.9335831904411316 TRAIN  loss dict:  {'classification_loss': 0.9335831904411316}
2025-01-13 11:56:11,951 [INFO] Step[350/2713]: training loss : 0.9360692822933196 TRAIN  loss dict:  {'classification_loss': 0.9360692822933196}
2025-01-13 11:56:24,670 [INFO] Step[400/2713]: training loss : 0.9330145227909088 TRAIN  loss dict:  {'classification_loss': 0.9330145227909088}
2025-01-13 11:56:37,955 [INFO] Step[450/2713]: training loss : 0.934065101146698 TRAIN  loss dict:  {'classification_loss': 0.934065101146698}
2025-01-13 11:56:51,385 [INFO] Step[500/2713]: training loss : 0.9373975777626038 TRAIN  loss dict:  {'classification_loss': 0.9373975777626038}
2025-01-13 11:57:04,407 [INFO] Step[550/2713]: training loss : 0.933716311454773 TRAIN  loss dict:  {'classification_loss': 0.933716311454773}
2025-01-13 11:57:16,638 [INFO] Step[600/2713]: training loss : 0.9347656488418579 TRAIN  loss dict:  {'classification_loss': 0.9347656488418579}
2025-01-13 11:57:29,503 [INFO] Step[650/2713]: training loss : 0.9341638135910034 TRAIN  loss dict:  {'classification_loss': 0.9341638135910034}
2025-01-13 11:57:42,649 [INFO] Step[700/2713]: training loss : 0.9363533329963684 TRAIN  loss dict:  {'classification_loss': 0.9363533329963684}
2025-01-13 11:57:56,233 [INFO] Step[750/2713]: training loss : 0.9330610358715057 TRAIN  loss dict:  {'classification_loss': 0.9330610358715057}
2025-01-13 11:58:09,598 [INFO] Step[800/2713]: training loss : 0.9381556594371796 TRAIN  loss dict:  {'classification_loss': 0.9381556594371796}
2025-01-13 11:58:22,560 [INFO] Step[850/2713]: training loss : 0.9353738057613373 TRAIN  loss dict:  {'classification_loss': 0.9353738057613373}
2025-01-13 11:58:36,146 [INFO] Step[900/2713]: training loss : 0.9330366885662079 TRAIN  loss dict:  {'classification_loss': 0.9330366885662079}
2025-01-13 11:58:49,549 [INFO] Step[950/2713]: training loss : 0.936215763092041 TRAIN  loss dict:  {'classification_loss': 0.936215763092041}
2025-01-13 11:59:02,753 [INFO] Step[1000/2713]: training loss : 0.9322685956954956 TRAIN  loss dict:  {'classification_loss': 0.9322685956954956}
2025-01-13 11:59:15,041 [INFO] Step[1050/2713]: training loss : 0.9334829890727997 TRAIN  loss dict:  {'classification_loss': 0.9334829890727997}
2025-01-13 11:59:27,607 [INFO] Step[1100/2713]: training loss : 0.9357107031345367 TRAIN  loss dict:  {'classification_loss': 0.9357107031345367}
2025-01-13 11:59:40,927 [INFO] Step[1150/2713]: training loss : 0.9346636962890625 TRAIN  loss dict:  {'classification_loss': 0.9346636962890625}
2025-01-13 11:59:54,498 [INFO] Step[1200/2713]: training loss : 0.9343334031105042 TRAIN  loss dict:  {'classification_loss': 0.9343334031105042}
2025-01-13 12:00:07,871 [INFO] Step[1250/2713]: training loss : 0.933577299118042 TRAIN  loss dict:  {'classification_loss': 0.933577299118042}
2025-01-13 12:00:20,660 [INFO] Step[1300/2713]: training loss : 0.9336202001571655 TRAIN  loss dict:  {'classification_loss': 0.9336202001571655}
2025-01-13 12:00:34,061 [INFO] Step[1350/2713]: training loss : 0.9339206802845001 TRAIN  loss dict:  {'classification_loss': 0.9339206802845001}
2025-01-13 12:00:47,099 [INFO] Step[1400/2713]: training loss : 0.9353387486934662 TRAIN  loss dict:  {'classification_loss': 0.9353387486934662}
2025-01-13 12:01:00,193 [INFO] Step[1450/2713]: training loss : 0.9338265097141266 TRAIN  loss dict:  {'classification_loss': 0.9338265097141266}
2025-01-13 12:01:12,363 [INFO] Step[1500/2713]: training loss : 0.9333811557292938 TRAIN  loss dict:  {'classification_loss': 0.9333811557292938}
2025-01-13 12:01:25,093 [INFO] Step[1550/2713]: training loss : 0.9360234522819519 TRAIN  loss dict:  {'classification_loss': 0.9360234522819519}
2025-01-13 12:01:38,596 [INFO] Step[1600/2713]: training loss : 0.9338065540790558 TRAIN  loss dict:  {'classification_loss': 0.9338065540790558}
2025-01-13 12:01:52,687 [INFO] Step[1650/2713]: training loss : 0.9369749987125396 TRAIN  loss dict:  {'classification_loss': 0.9369749987125396}
2025-01-13 12:02:05,952 [INFO] Step[1700/2713]: training loss : 0.934086377620697 TRAIN  loss dict:  {'classification_loss': 0.934086377620697}
2025-01-13 12:02:18,976 [INFO] Step[1750/2713]: training loss : 0.9348405420780181 TRAIN  loss dict:  {'classification_loss': 0.9348405420780181}
2025-01-13 12:02:32,671 [INFO] Step[1800/2713]: training loss : 0.9345467245578766 TRAIN  loss dict:  {'classification_loss': 0.9345467245578766}
2025-01-13 12:02:45,672 [INFO] Step[1850/2713]: training loss : 0.9343029427528381 TRAIN  loss dict:  {'classification_loss': 0.9343029427528381}
2025-01-13 12:02:58,533 [INFO] Step[1900/2713]: training loss : 0.9327682888507843 TRAIN  loss dict:  {'classification_loss': 0.9327682888507843}
2025-01-13 12:03:10,662 [INFO] Step[1950/2713]: training loss : 0.9382349967956543 TRAIN  loss dict:  {'classification_loss': 0.9382349967956543}
2025-01-13 12:03:23,341 [INFO] Step[2000/2713]: training loss : 0.9351938462257385 TRAIN  loss dict:  {'classification_loss': 0.9351938462257385}
2025-01-13 12:03:36,851 [INFO] Step[2050/2713]: training loss : 0.9332396113872528 TRAIN  loss dict:  {'classification_loss': 0.9332396113872528}
2025-01-13 12:03:50,893 [INFO] Step[2100/2713]: training loss : 0.9348718440532684 TRAIN  loss dict:  {'classification_loss': 0.9348718440532684}
2025-01-13 12:04:04,008 [INFO] Step[2150/2713]: training loss : 0.9336959064006806 TRAIN  loss dict:  {'classification_loss': 0.9336959064006806}
2025-01-13 12:04:17,020 [INFO] Step[2200/2713]: training loss : 0.933073103427887 TRAIN  loss dict:  {'classification_loss': 0.933073103427887}
2025-01-13 12:04:30,346 [INFO] Step[2250/2713]: training loss : 0.9350987637042999 TRAIN  loss dict:  {'classification_loss': 0.9350987637042999}
2025-01-13 12:04:43,374 [INFO] Step[2300/2713]: training loss : 0.9326245367527009 TRAIN  loss dict:  {'classification_loss': 0.9326245367527009}
2025-01-13 12:04:56,243 [INFO] Step[2350/2713]: training loss : 0.9328711295127868 TRAIN  loss dict:  {'classification_loss': 0.9328711295127868}
2025-01-13 12:05:08,371 [INFO] Step[2400/2713]: training loss : 0.9345171308517456 TRAIN  loss dict:  {'classification_loss': 0.9345171308517456}
2025-01-13 12:05:21,583 [INFO] Step[2450/2713]: training loss : 0.9330834126472474 TRAIN  loss dict:  {'classification_loss': 0.9330834126472474}
2025-01-13 12:05:35,142 [INFO] Step[2500/2713]: training loss : 0.9340885925292969 TRAIN  loss dict:  {'classification_loss': 0.9340885925292969}
2025-01-13 12:05:48,535 [INFO] Step[2550/2713]: training loss : 0.9340572476387023 TRAIN  loss dict:  {'classification_loss': 0.9340572476387023}
2025-01-13 12:06:01,380 [INFO] Step[2600/2713]: training loss : 0.9358190774917603 TRAIN  loss dict:  {'classification_loss': 0.9358190774917603}
2025-01-13 12:06:13,155 [INFO] Step[2650/2713]: training loss : 0.934017459154129 TRAIN  loss dict:  {'classification_loss': 0.934017459154129}
2025-01-13 12:06:25,039 [INFO] Step[2700/2713]: training loss : 0.9331941890716553 TRAIN  loss dict:  {'classification_loss': 0.9331941890716553}
2025-01-13 12:07:57,240 [INFO] Label accuracies statistics:
2025-01-13 12:07:57,240 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.75, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.5, 161: 1.0, 162: 0.75, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.5, 209: 0.75, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 12:07:57,243 [INFO] [102] TRAIN  loss: 0.9344854455481058 acc: 1.0
2025-01-13 12:07:57,243 [INFO] [102] TRAIN  loss dict: {'classification_loss': 0.9344854455481058}
2025-01-13 12:07:57,243 [INFO] [102] VALIDATION loss: 1.6739128726093393 VALIDATION acc: 0.838871473354232
2025-01-13 12:07:57,243 [INFO] [102] VALIDATION loss dict: {'classification_loss': 1.6739128726093393}
2025-01-13 12:07:57,243 [INFO] 
2025-01-13 12:08:17,222 [INFO] Step[50/2713]: training loss : 0.9341031432151794 TRAIN  loss dict:  {'classification_loss': 0.9341031432151794}
2025-01-13 12:08:29,602 [INFO] Step[100/2713]: training loss : 0.9336450314521789 TRAIN  loss dict:  {'classification_loss': 0.9336450314521789}
2025-01-13 12:08:41,790 [INFO] Step[150/2713]: training loss : 0.9342184388637542 TRAIN  loss dict:  {'classification_loss': 0.9342184388637542}
2025-01-13 12:08:54,501 [INFO] Step[200/2713]: training loss : 0.9409073138237 TRAIN  loss dict:  {'classification_loss': 0.9409073138237}
2025-01-13 12:09:08,137 [INFO] Step[250/2713]: training loss : 0.9333411312103271 TRAIN  loss dict:  {'classification_loss': 0.9333411312103271}
2025-01-13 12:09:21,808 [INFO] Step[300/2713]: training loss : 0.9343945014476777 TRAIN  loss dict:  {'classification_loss': 0.9343945014476777}
2025-01-13 12:09:33,712 [INFO] Step[350/2713]: training loss : 0.93415651679039 TRAIN  loss dict:  {'classification_loss': 0.93415651679039}
2025-01-13 12:09:45,382 [INFO] Step[400/2713]: training loss : 0.9352498137950898 TRAIN  loss dict:  {'classification_loss': 0.9352498137950898}
2025-01-13 12:09:57,059 [INFO] Step[450/2713]: training loss : 0.9330555021762847 TRAIN  loss dict:  {'classification_loss': 0.9330555021762847}
2025-01-13 12:10:08,776 [INFO] Step[500/2713]: training loss : 0.9344410824775696 TRAIN  loss dict:  {'classification_loss': 0.9344410824775696}
2025-01-13 12:10:20,528 [INFO] Step[550/2713]: training loss : 0.9345592474937439 TRAIN  loss dict:  {'classification_loss': 0.9345592474937439}
2025-01-13 12:10:32,238 [INFO] Step[600/2713]: training loss : 0.9363636517524719 TRAIN  loss dict:  {'classification_loss': 0.9363636517524719}
2025-01-13 12:10:44,036 [INFO] Step[650/2713]: training loss : 0.9338786160945892 TRAIN  loss dict:  {'classification_loss': 0.9338786160945892}
2025-01-13 12:10:55,740 [INFO] Step[700/2713]: training loss : 0.9333885991573334 TRAIN  loss dict:  {'classification_loss': 0.9333885991573334}
2025-01-13 12:11:07,505 [INFO] Step[750/2713]: training loss : 0.934641078710556 TRAIN  loss dict:  {'classification_loss': 0.934641078710556}
2025-01-13 12:11:19,220 [INFO] Step[800/2713]: training loss : 0.9357243227958679 TRAIN  loss dict:  {'classification_loss': 0.9357243227958679}
2025-01-13 12:11:30,976 [INFO] Step[850/2713]: training loss : 0.9336051714420318 TRAIN  loss dict:  {'classification_loss': 0.9336051714420318}
2025-01-13 12:11:42,689 [INFO] Step[900/2713]: training loss : 0.9365887796878815 TRAIN  loss dict:  {'classification_loss': 0.9365887796878815}
2025-01-13 12:11:54,512 [INFO] Step[950/2713]: training loss : 0.9338336360454559 TRAIN  loss dict:  {'classification_loss': 0.9338336360454559}
2025-01-13 12:12:06,259 [INFO] Step[1000/2713]: training loss : 0.9398027217388153 TRAIN  loss dict:  {'classification_loss': 0.9398027217388153}
2025-01-13 12:12:18,040 [INFO] Step[1050/2713]: training loss : 0.9344347655773163 TRAIN  loss dict:  {'classification_loss': 0.9344347655773163}
2025-01-13 12:12:29,763 [INFO] Step[1100/2713]: training loss : 0.9328190243244171 TRAIN  loss dict:  {'classification_loss': 0.9328190243244171}
2025-01-13 12:12:41,536 [INFO] Step[1150/2713]: training loss : 0.9338667798042297 TRAIN  loss dict:  {'classification_loss': 0.9338667798042297}
2025-01-13 12:12:53,283 [INFO] Step[1200/2713]: training loss : 0.9352126514911652 TRAIN  loss dict:  {'classification_loss': 0.9352126514911652}
2025-01-13 12:13:05,069 [INFO] Step[1250/2713]: training loss : 0.9330527639389038 TRAIN  loss dict:  {'classification_loss': 0.9330527639389038}
2025-01-13 12:13:16,812 [INFO] Step[1300/2713]: training loss : 0.9332490944862366 TRAIN  loss dict:  {'classification_loss': 0.9332490944862366}
2025-01-13 12:13:28,580 [INFO] Step[1350/2713]: training loss : 0.935538399219513 TRAIN  loss dict:  {'classification_loss': 0.935538399219513}
2025-01-13 12:13:40,322 [INFO] Step[1400/2713]: training loss : 0.9456217217445374 TRAIN  loss dict:  {'classification_loss': 0.9456217217445374}
2025-01-13 12:13:52,064 [INFO] Step[1450/2713]: training loss : 0.9348820900917053 TRAIN  loss dict:  {'classification_loss': 0.9348820900917053}
2025-01-13 12:14:03,804 [INFO] Step[1500/2713]: training loss : 0.9334244048595428 TRAIN  loss dict:  {'classification_loss': 0.9334244048595428}
2025-01-13 12:14:15,561 [INFO] Step[1550/2713]: training loss : 0.935010154247284 TRAIN  loss dict:  {'classification_loss': 0.935010154247284}
2025-01-13 12:14:27,273 [INFO] Step[1600/2713]: training loss : 0.9322119092941284 TRAIN  loss dict:  {'classification_loss': 0.9322119092941284}
2025-01-13 12:14:38,979 [INFO] Step[1650/2713]: training loss : 0.9334854590892792 TRAIN  loss dict:  {'classification_loss': 0.9334854590892792}
2025-01-13 12:14:50,780 [INFO] Step[1700/2713]: training loss : 0.9343633842468262 TRAIN  loss dict:  {'classification_loss': 0.9343633842468262}
2025-01-13 12:15:02,563 [INFO] Step[1750/2713]: training loss : 0.9337897169589996 TRAIN  loss dict:  {'classification_loss': 0.9337897169589996}
2025-01-13 12:15:14,326 [INFO] Step[1800/2713]: training loss : 0.9333207952976227 TRAIN  loss dict:  {'classification_loss': 0.9333207952976227}
2025-01-13 12:15:26,072 [INFO] Step[1850/2713]: training loss : 0.9341812181472778 TRAIN  loss dict:  {'classification_loss': 0.9341812181472778}
2025-01-13 12:15:37,802 [INFO] Step[1900/2713]: training loss : 0.9347428119182587 TRAIN  loss dict:  {'classification_loss': 0.9347428119182587}
2025-01-13 12:15:49,568 [INFO] Step[1950/2713]: training loss : 0.9350329864025116 TRAIN  loss dict:  {'classification_loss': 0.9350329864025116}
2025-01-13 12:16:01,331 [INFO] Step[2000/2713]: training loss : 0.9377889621257782 TRAIN  loss dict:  {'classification_loss': 0.9377889621257782}
2025-01-13 12:16:13,107 [INFO] Step[2050/2713]: training loss : 0.9332340013980865 TRAIN  loss dict:  {'classification_loss': 0.9332340013980865}
2025-01-13 12:16:24,835 [INFO] Step[2100/2713]: training loss : 0.9329949784278869 TRAIN  loss dict:  {'classification_loss': 0.9329949784278869}
2025-01-13 12:16:36,573 [INFO] Step[2150/2713]: training loss : 0.9354250872135162 TRAIN  loss dict:  {'classification_loss': 0.9354250872135162}
2025-01-13 12:16:48,328 [INFO] Step[2200/2713]: training loss : 0.934745306968689 TRAIN  loss dict:  {'classification_loss': 0.934745306968689}
2025-01-13 12:17:00,097 [INFO] Step[2250/2713]: training loss : 0.9346129357814789 TRAIN  loss dict:  {'classification_loss': 0.9346129357814789}
2025-01-13 12:17:11,824 [INFO] Step[2300/2713]: training loss : 0.9348450469970703 TRAIN  loss dict:  {'classification_loss': 0.9348450469970703}
2025-01-13 12:17:23,545 [INFO] Step[2350/2713]: training loss : 0.934148701429367 TRAIN  loss dict:  {'classification_loss': 0.934148701429367}
2025-01-13 12:17:35,287 [INFO] Step[2400/2713]: training loss : 0.9325469148159027 TRAIN  loss dict:  {'classification_loss': 0.9325469148159027}
2025-01-13 12:17:47,051 [INFO] Step[2450/2713]: training loss : 0.934925651550293 TRAIN  loss dict:  {'classification_loss': 0.934925651550293}
2025-01-13 12:17:58,802 [INFO] Step[2500/2713]: training loss : 0.9343233799934387 TRAIN  loss dict:  {'classification_loss': 0.9343233799934387}
2025-01-13 12:18:10,570 [INFO] Step[2550/2713]: training loss : 0.9336124229431152 TRAIN  loss dict:  {'classification_loss': 0.9336124229431152}
2025-01-13 12:18:22,312 [INFO] Step[2600/2713]: training loss : 0.934056875705719 TRAIN  loss dict:  {'classification_loss': 0.934056875705719}
2025-01-13 12:18:34,072 [INFO] Step[2650/2713]: training loss : 0.9357247054576874 TRAIN  loss dict:  {'classification_loss': 0.9357247054576874}
2025-01-13 12:18:45,851 [INFO] Step[2700/2713]: training loss : 0.934388610124588 TRAIN  loss dict:  {'classification_loss': 0.934388610124588}
2025-01-13 12:20:14,475 [INFO] Label accuracies statistics:
2025-01-13 12:20:14,475 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.75, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.5, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.0, 217: 0.5, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 0.75, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 12:20:14,477 [INFO] [103] TRAIN  loss: 0.9347242936409579 acc: 1.0
2025-01-13 12:20:14,477 [INFO] [103] TRAIN  loss dict: {'classification_loss': 0.9347242936409579}
2025-01-13 12:20:14,477 [INFO] [103] VALIDATION loss: 1.6950374411461049 VALIDATION acc: 0.8407523510971787
2025-01-13 12:20:14,477 [INFO] [103] VALIDATION loss dict: {'classification_loss': 1.6950374411461049}
2025-01-13 12:20:14,477 [INFO] 
2025-01-13 12:20:32,011 [INFO] Step[50/2713]: training loss : 0.9335704147815704 TRAIN  loss dict:  {'classification_loss': 0.9335704147815704}
2025-01-13 12:20:43,759 [INFO] Step[100/2713]: training loss : 0.9332634198665619 TRAIN  loss dict:  {'classification_loss': 0.9332634198665619}
2025-01-13 12:20:55,531 [INFO] Step[150/2713]: training loss : 0.9344546282291413 TRAIN  loss dict:  {'classification_loss': 0.9344546282291413}
2025-01-13 12:21:07,266 [INFO] Step[200/2713]: training loss : 0.9340633225440979 TRAIN  loss dict:  {'classification_loss': 0.9340633225440979}
2025-01-13 12:21:19,028 [INFO] Step[250/2713]: training loss : 0.9334105980396271 TRAIN  loss dict:  {'classification_loss': 0.9334105980396271}
2025-01-13 12:21:30,776 [INFO] Step[300/2713]: training loss : 0.9343834543228149 TRAIN  loss dict:  {'classification_loss': 0.9343834543228149}
2025-01-13 12:21:42,568 [INFO] Step[350/2713]: training loss : 0.9349359679222107 TRAIN  loss dict:  {'classification_loss': 0.9349359679222107}
2025-01-13 12:21:54,281 [INFO] Step[400/2713]: training loss : 0.9354549765586853 TRAIN  loss dict:  {'classification_loss': 0.9354549765586853}
2025-01-13 12:22:06,076 [INFO] Step[450/2713]: training loss : 0.9338309836387634 TRAIN  loss dict:  {'classification_loss': 0.9338309836387634}
2025-01-13 12:22:17,833 [INFO] Step[500/2713]: training loss : 0.932832989692688 TRAIN  loss dict:  {'classification_loss': 0.932832989692688}
2025-01-13 12:22:29,595 [INFO] Step[550/2713]: training loss : 0.9333861303329468 TRAIN  loss dict:  {'classification_loss': 0.9333861303329468}
2025-01-13 12:22:41,354 [INFO] Step[600/2713]: training loss : 0.9329995822906494 TRAIN  loss dict:  {'classification_loss': 0.9329995822906494}
2025-01-13 12:22:53,161 [INFO] Step[650/2713]: training loss : 0.9347490084171295 TRAIN  loss dict:  {'classification_loss': 0.9347490084171295}
2025-01-13 12:23:04,906 [INFO] Step[700/2713]: training loss : 0.9325450253486633 TRAIN  loss dict:  {'classification_loss': 0.9325450253486633}
2025-01-13 12:23:16,632 [INFO] Step[750/2713]: training loss : 0.9340248656272888 TRAIN  loss dict:  {'classification_loss': 0.9340248656272888}
2025-01-13 12:23:28,375 [INFO] Step[800/2713]: training loss : 0.9393020331859588 TRAIN  loss dict:  {'classification_loss': 0.9393020331859588}
2025-01-13 12:23:40,150 [INFO] Step[850/2713]: training loss : 0.9331447744369507 TRAIN  loss dict:  {'classification_loss': 0.9331447744369507}
2025-01-13 12:23:51,906 [INFO] Step[900/2713]: training loss : 0.9330641257762909 TRAIN  loss dict:  {'classification_loss': 0.9330641257762909}
2025-01-13 12:24:03,668 [INFO] Step[950/2713]: training loss : 0.9351092290878296 TRAIN  loss dict:  {'classification_loss': 0.9351092290878296}
2025-01-13 12:24:15,435 [INFO] Step[1000/2713]: training loss : 0.9342529153823853 TRAIN  loss dict:  {'classification_loss': 0.9342529153823853}
2025-01-13 12:24:27,207 [INFO] Step[1050/2713]: training loss : 0.9342466974258423 TRAIN  loss dict:  {'classification_loss': 0.9342466974258423}
2025-01-13 12:24:39,009 [INFO] Step[1100/2713]: training loss : 0.9428625428676605 TRAIN  loss dict:  {'classification_loss': 0.9428625428676605}
2025-01-13 12:24:50,806 [INFO] Step[1150/2713]: training loss : 0.9353279757499695 TRAIN  loss dict:  {'classification_loss': 0.9353279757499695}
2025-01-13 12:25:02,534 [INFO] Step[1200/2713]: training loss : 0.9339990186691284 TRAIN  loss dict:  {'classification_loss': 0.9339990186691284}
2025-01-13 12:25:14,325 [INFO] Step[1250/2713]: training loss : 0.9332819306850433 TRAIN  loss dict:  {'classification_loss': 0.9332819306850433}
2025-01-13 12:25:26,128 [INFO] Step[1300/2713]: training loss : 0.9337388956546784 TRAIN  loss dict:  {'classification_loss': 0.9337388956546784}
2025-01-13 12:25:37,897 [INFO] Step[1350/2713]: training loss : 0.9339607012271881 TRAIN  loss dict:  {'classification_loss': 0.9339607012271881}
2025-01-13 12:25:49,986 [INFO] Step[1400/2713]: training loss : 0.9337133753299713 TRAIN  loss dict:  {'classification_loss': 0.9337133753299713}
2025-01-13 12:26:01,855 [INFO] Step[1450/2713]: training loss : 0.9342532777786254 TRAIN  loss dict:  {'classification_loss': 0.9342532777786254}
2025-01-13 12:26:13,749 [INFO] Step[1500/2713]: training loss : 0.9353978574275971 TRAIN  loss dict:  {'classification_loss': 0.9353978574275971}
2025-01-13 12:26:25,926 [INFO] Step[1550/2713]: training loss : 0.9332924032211304 TRAIN  loss dict:  {'classification_loss': 0.9332924032211304}
2025-01-13 12:26:38,322 [INFO] Step[1600/2713]: training loss : 0.9347012841701507 TRAIN  loss dict:  {'classification_loss': 0.9347012841701507}
2025-01-13 12:26:50,590 [INFO] Step[1650/2713]: training loss : 0.9339794850349427 TRAIN  loss dict:  {'classification_loss': 0.9339794850349427}
2025-01-13 12:27:02,744 [INFO] Step[1700/2713]: training loss : 0.9337445330619812 TRAIN  loss dict:  {'classification_loss': 0.9337445330619812}
2025-01-13 12:27:15,467 [INFO] Step[1750/2713]: training loss : 0.9330259931087493 TRAIN  loss dict:  {'classification_loss': 0.9330259931087493}
2025-01-13 12:27:27,672 [INFO] Step[1800/2713]: training loss : 0.9334269940853119 TRAIN  loss dict:  {'classification_loss': 0.9334269940853119}
2025-01-13 12:27:40,233 [INFO] Step[1850/2713]: training loss : 0.933190507888794 TRAIN  loss dict:  {'classification_loss': 0.933190507888794}
2025-01-13 12:27:53,655 [INFO] Step[1900/2713]: training loss : 0.9339326691627502 TRAIN  loss dict:  {'classification_loss': 0.9339326691627502}
2025-01-13 12:28:07,407 [INFO] Step[1950/2713]: training loss : 0.9350293350219726 TRAIN  loss dict:  {'classification_loss': 0.9350293350219726}
2025-01-13 12:28:19,911 [INFO] Step[2000/2713]: training loss : 0.9327880430221558 TRAIN  loss dict:  {'classification_loss': 0.9327880430221558}
2025-01-13 12:28:31,827 [INFO] Step[2050/2713]: training loss : 0.933223408460617 TRAIN  loss dict:  {'classification_loss': 0.933223408460617}
2025-01-13 12:28:44,170 [INFO] Step[2100/2713]: training loss : 0.9334343135356903 TRAIN  loss dict:  {'classification_loss': 0.9334343135356903}
2025-01-13 12:28:56,686 [INFO] Step[2150/2713]: training loss : 0.9343445265293121 TRAIN  loss dict:  {'classification_loss': 0.9343445265293121}
2025-01-13 12:29:09,672 [INFO] Step[2200/2713]: training loss : 0.9355079078674317 TRAIN  loss dict:  {'classification_loss': 0.9355079078674317}
2025-01-13 12:29:22,648 [INFO] Step[2250/2713]: training loss : 0.9336338913440705 TRAIN  loss dict:  {'classification_loss': 0.9336338913440705}
2025-01-13 12:29:35,084 [INFO] Step[2300/2713]: training loss : 0.9343771493434906 TRAIN  loss dict:  {'classification_loss': 0.9343771493434906}
2025-01-13 12:29:47,571 [INFO] Step[2350/2713]: training loss : 0.9321861660480499 TRAIN  loss dict:  {'classification_loss': 0.9321861660480499}
2025-01-13 12:30:00,282 [INFO] Step[2400/2713]: training loss : 0.9356798839569092 TRAIN  loss dict:  {'classification_loss': 0.9356798839569092}
2025-01-13 12:30:13,485 [INFO] Step[2450/2713]: training loss : 0.9327368879318237 TRAIN  loss dict:  {'classification_loss': 0.9327368879318237}
2025-01-13 12:30:26,718 [INFO] Step[2500/2713]: training loss : 0.9350485718250274 TRAIN  loss dict:  {'classification_loss': 0.9350485718250274}
2025-01-13 12:30:39,626 [INFO] Step[2550/2713]: training loss : 0.9333314955234527 TRAIN  loss dict:  {'classification_loss': 0.9333314955234527}
2025-01-13 12:30:52,514 [INFO] Step[2600/2713]: training loss : 0.9331678509712219 TRAIN  loss dict:  {'classification_loss': 0.9331678509712219}
2025-01-13 12:31:05,099 [INFO] Step[2650/2713]: training loss : 0.9339309847354889 TRAIN  loss dict:  {'classification_loss': 0.9339309847354889}
2025-01-13 12:31:17,616 [INFO] Step[2700/2713]: training loss : 0.9335701417922974 TRAIN  loss dict:  {'classification_loss': 0.9335701417922974}
2025-01-13 12:33:01,423 [INFO] Label accuracies statistics:
2025-01-13 12:33:01,423 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 0.5, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 1.0, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 0.75, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 1.0, 200: 0.5, 201: 0.75, 202: 0.75, 203: 0.75, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 1.0, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.75, 275: 0.75, 276: 1.0, 277: 0.75, 278: 0.75, 279: 0.75, 280: 1.0, 281: 0.75, 282: 0.75, 283: 0.75, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.5, 339: 1.0, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.5, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 12:33:01,426 [INFO] [104] TRAIN  loss: 0.9341701357029831 acc: 0.9998771347831429
2025-01-13 12:33:01,426 [INFO] [104] TRAIN  loss dict: {'classification_loss': 0.9341701357029831}
2025-01-13 12:33:01,426 [INFO] [104] VALIDATION loss: 1.673135747138719 VALIDATION acc: 0.8394984326018808
2025-01-13 12:33:01,426 [INFO] [104] VALIDATION loss dict: {'classification_loss': 1.673135747138719}
2025-01-13 12:33:01,426 [INFO] 
2025-01-13 12:33:19,920 [INFO] Step[50/2713]: training loss : 0.9321604490280151 TRAIN  loss dict:  {'classification_loss': 0.9321604490280151}
2025-01-13 12:33:32,437 [INFO] Step[100/2713]: training loss : 0.9341763830184937 TRAIN  loss dict:  {'classification_loss': 0.9341763830184937}
2025-01-13 12:33:45,021 [INFO] Step[150/2713]: training loss : 0.935126223564148 TRAIN  loss dict:  {'classification_loss': 0.935126223564148}
2025-01-13 12:33:57,590 [INFO] Step[200/2713]: training loss : 0.934914802312851 TRAIN  loss dict:  {'classification_loss': 0.934914802312851}
2025-01-13 12:34:10,340 [INFO] Step[250/2713]: training loss : 0.9335516560077667 TRAIN  loss dict:  {'classification_loss': 0.9335516560077667}
2025-01-13 12:34:23,238 [INFO] Step[300/2713]: training loss : 0.9327763295173646 TRAIN  loss dict:  {'classification_loss': 0.9327763295173646}
2025-01-13 12:34:36,151 [INFO] Step[350/2713]: training loss : 0.9346929872035981 TRAIN  loss dict:  {'classification_loss': 0.9346929872035981}
2025-01-13 12:34:49,417 [INFO] Step[400/2713]: training loss : 0.9344885659217834 TRAIN  loss dict:  {'classification_loss': 0.9344885659217834}
2025-01-13 12:35:02,484 [INFO] Step[450/2713]: training loss : 0.9362450039386749 TRAIN  loss dict:  {'classification_loss': 0.9362450039386749}
2025-01-13 12:35:17,158 [INFO] Step[500/2713]: training loss : 0.934155935049057 TRAIN  loss dict:  {'classification_loss': 0.934155935049057}
2025-01-13 12:35:32,256 [INFO] Step[550/2713]: training loss : 0.9342768800258636 TRAIN  loss dict:  {'classification_loss': 0.9342768800258636}
2025-01-13 12:35:45,269 [INFO] Step[600/2713]: training loss : 0.934247921705246 TRAIN  loss dict:  {'classification_loss': 0.934247921705246}
2025-01-13 12:35:58,241 [INFO] Step[650/2713]: training loss : 0.9336319947242737 TRAIN  loss dict:  {'classification_loss': 0.9336319947242737}
2025-01-13 12:36:11,014 [INFO] Step[700/2713]: training loss : 0.9336001515388489 TRAIN  loss dict:  {'classification_loss': 0.9336001515388489}
2025-01-13 12:36:23,773 [INFO] Step[750/2713]: training loss : 0.9333215892314911 TRAIN  loss dict:  {'classification_loss': 0.9333215892314911}
2025-01-13 12:36:36,714 [INFO] Step[800/2713]: training loss : 0.9341323029994965 TRAIN  loss dict:  {'classification_loss': 0.9341323029994965}
2025-01-13 12:36:49,805 [INFO] Step[850/2713]: training loss : 0.9326747798919678 TRAIN  loss dict:  {'classification_loss': 0.9326747798919678}
2025-01-13 12:37:02,607 [INFO] Step[900/2713]: training loss : 0.9328045392036438 TRAIN  loss dict:  {'classification_loss': 0.9328045392036438}
2025-01-13 12:37:15,743 [INFO] Step[950/2713]: training loss : 0.9330994844436645 TRAIN  loss dict:  {'classification_loss': 0.9330994844436645}
2025-01-13 12:37:28,471 [INFO] Step[1000/2713]: training loss : 0.9346174883842469 TRAIN  loss dict:  {'classification_loss': 0.9346174883842469}
2025-01-13 12:37:41,305 [INFO] Step[1050/2713]: training loss : 0.9340562236309051 TRAIN  loss dict:  {'classification_loss': 0.9340562236309051}
2025-01-13 12:37:54,489 [INFO] Step[1100/2713]: training loss : 0.9355541026592255 TRAIN  loss dict:  {'classification_loss': 0.9355541026592255}
2025-01-13 12:38:07,163 [INFO] Step[1150/2713]: training loss : 0.9338816547393799 TRAIN  loss dict:  {'classification_loss': 0.9338816547393799}
2025-01-13 12:38:19,891 [INFO] Step[1200/2713]: training loss : 0.9333772075176239 TRAIN  loss dict:  {'classification_loss': 0.9333772075176239}
2025-01-13 12:38:33,125 [INFO] Step[1250/2713]: training loss : 0.9354002559185028 TRAIN  loss dict:  {'classification_loss': 0.9354002559185028}
2025-01-13 12:38:46,003 [INFO] Step[1300/2713]: training loss : 0.9346925151348114 TRAIN  loss dict:  {'classification_loss': 0.9346925151348114}
2025-01-13 12:38:59,185 [INFO] Step[1350/2713]: training loss : 0.9344688558578491 TRAIN  loss dict:  {'classification_loss': 0.9344688558578491}
2025-01-13 12:39:12,426 [INFO] Step[1400/2713]: training loss : 0.9338698029518128 TRAIN  loss dict:  {'classification_loss': 0.9338698029518128}
2025-01-13 12:39:25,286 [INFO] Step[1450/2713]: training loss : 0.9329701375961303 TRAIN  loss dict:  {'classification_loss': 0.9329701375961303}
2025-01-13 12:39:38,317 [INFO] Step[1500/2713]: training loss : 0.9350507688522339 TRAIN  loss dict:  {'classification_loss': 0.9350507688522339}
2025-01-13 12:39:51,541 [INFO] Step[1550/2713]: training loss : 0.9337348687648773 TRAIN  loss dict:  {'classification_loss': 0.9337348687648773}
2025-01-13 12:40:04,110 [INFO] Step[1600/2713]: training loss : 0.9400505411624909 TRAIN  loss dict:  {'classification_loss': 0.9400505411624909}
2025-01-13 12:40:16,660 [INFO] Step[1650/2713]: training loss : 0.9341670751571656 TRAIN  loss dict:  {'classification_loss': 0.9341670751571656}
2025-01-13 12:40:29,368 [INFO] Step[1700/2713]: training loss : 0.9337910711765289 TRAIN  loss dict:  {'classification_loss': 0.9337910711765289}
2025-01-13 12:40:42,092 [INFO] Step[1750/2713]: training loss : 0.9335163736343384 TRAIN  loss dict:  {'classification_loss': 0.9335163736343384}
2025-01-13 12:40:54,766 [INFO] Step[1800/2713]: training loss : 0.9358207941055298 TRAIN  loss dict:  {'classification_loss': 0.9358207941055298}
2025-01-13 12:41:08,025 [INFO] Step[1850/2713]: training loss : 0.9355934464931488 TRAIN  loss dict:  {'classification_loss': 0.9355934464931488}
2025-01-13 12:41:20,649 [INFO] Step[1900/2713]: training loss : 0.9335261178016663 TRAIN  loss dict:  {'classification_loss': 0.9335261178016663}
2025-01-13 12:41:33,785 [INFO] Step[1950/2713]: training loss : 0.9350741147994995 TRAIN  loss dict:  {'classification_loss': 0.9350741147994995}
2025-01-13 12:41:46,882 [INFO] Step[2000/2713]: training loss : 0.9328470957279206 TRAIN  loss dict:  {'classification_loss': 0.9328470957279206}
2025-01-13 12:41:59,499 [INFO] Step[2050/2713]: training loss : 0.9343513834476471 TRAIN  loss dict:  {'classification_loss': 0.9343513834476471}
2025-01-13 12:42:12,059 [INFO] Step[2100/2713]: training loss : 0.9341048789024353 TRAIN  loss dict:  {'classification_loss': 0.9341048789024353}
2025-01-13 12:42:25,182 [INFO] Step[2150/2713]: training loss : 0.9341061437129974 TRAIN  loss dict:  {'classification_loss': 0.9341061437129974}
2025-01-13 12:42:37,890 [INFO] Step[2200/2713]: training loss : 0.9333947908878326 TRAIN  loss dict:  {'classification_loss': 0.9333947908878326}
2025-01-13 12:42:50,732 [INFO] Step[2250/2713]: training loss : 0.9337337911128998 TRAIN  loss dict:  {'classification_loss': 0.9337337911128998}
2025-01-13 12:43:03,959 [INFO] Step[2300/2713]: training loss : 0.932608630657196 TRAIN  loss dict:  {'classification_loss': 0.932608630657196}
2025-01-13 12:43:18,455 [INFO] Step[2350/2713]: training loss : 0.9328849494457245 TRAIN  loss dict:  {'classification_loss': 0.9328849494457245}
2025-01-13 12:43:33,513 [INFO] Step[2400/2713]: training loss : 0.9333858060836792 TRAIN  loss dict:  {'classification_loss': 0.9333858060836792}
2025-01-13 12:43:46,868 [INFO] Step[2450/2713]: training loss : 0.933407392501831 TRAIN  loss dict:  {'classification_loss': 0.933407392501831}
2025-01-13 12:43:59,450 [INFO] Step[2500/2713]: training loss : 0.9338140678405762 TRAIN  loss dict:  {'classification_loss': 0.9338140678405762}
2025-01-13 12:44:12,667 [INFO] Step[2550/2713]: training loss : 0.9323522078990937 TRAIN  loss dict:  {'classification_loss': 0.9323522078990937}
2025-01-13 12:44:25,507 [INFO] Step[2600/2713]: training loss : 0.9337589764595031 TRAIN  loss dict:  {'classification_loss': 0.9337589764595031}
2025-01-13 12:44:38,587 [INFO] Step[2650/2713]: training loss : 0.9334557604789734 TRAIN  loss dict:  {'classification_loss': 0.9334557604789734}
2025-01-13 12:44:51,713 [INFO] Step[2700/2713]: training loss : 0.9348045706748962 TRAIN  loss dict:  {'classification_loss': 0.9348045706748962}
2025-01-13 12:46:34,285 [INFO] Label accuracies statistics:
2025-01-13 12:46:34,285 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.75, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 0.5, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 0.75, 351: 0.75, 352: 0.75, 353: 0.25, 354: 0.75, 355: 1.0, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 12:46:34,287 [INFO] [105] TRAIN  loss: 0.9340807797941298 acc: 1.0
2025-01-13 12:46:34,287 [INFO] [105] TRAIN  loss dict: {'classification_loss': 0.9340807797941298}
2025-01-13 12:46:34,287 [INFO] [105] VALIDATION loss: 1.7167647645661706 VALIDATION acc: 0.8376175548589342
2025-01-13 12:46:34,287 [INFO] [105] VALIDATION loss dict: {'classification_loss': 1.7167647645661706}
2025-01-13 12:46:34,287 [INFO] 
2025-01-13 12:46:52,461 [INFO] Step[50/2713]: training loss : 0.9338472616672516 TRAIN  loss dict:  {'classification_loss': 0.9338472616672516}
2025-01-13 12:47:05,136 [INFO] Step[100/2713]: training loss : 0.9327654659748077 TRAIN  loss dict:  {'classification_loss': 0.9327654659748077}
2025-01-13 12:47:17,963 [INFO] Step[150/2713]: training loss : 0.9334199321269989 TRAIN  loss dict:  {'classification_loss': 0.9334199321269989}
2025-01-13 12:47:30,545 [INFO] Step[200/2713]: training loss : 0.9344169068336486 TRAIN  loss dict:  {'classification_loss': 0.9344169068336486}
2025-01-13 12:47:43,237 [INFO] Step[250/2713]: training loss : 0.9355347800254822 TRAIN  loss dict:  {'classification_loss': 0.9355347800254822}
2025-01-13 12:47:56,507 [INFO] Step[300/2713]: training loss : 0.9334457528591156 TRAIN  loss dict:  {'classification_loss': 0.9334457528591156}
2025-01-13 12:48:09,718 [INFO] Step[350/2713]: training loss : 0.9327319180965423 TRAIN  loss dict:  {'classification_loss': 0.9327319180965423}
2025-01-13 12:48:22,672 [INFO] Step[400/2713]: training loss : 0.9331100869178772 TRAIN  loss dict:  {'classification_loss': 0.9331100869178772}
2025-01-13 12:48:35,249 [INFO] Step[450/2713]: training loss : 0.9374358201026917 TRAIN  loss dict:  {'classification_loss': 0.9374358201026917}
2025-01-13 12:48:47,855 [INFO] Step[500/2713]: training loss : 0.9342300856113434 TRAIN  loss dict:  {'classification_loss': 0.9342300856113434}
2025-01-13 12:49:00,957 [INFO] Step[550/2713]: training loss : 0.9323873949050904 TRAIN  loss dict:  {'classification_loss': 0.9323873949050904}
2025-01-13 12:49:13,495 [INFO] Step[600/2713]: training loss : 0.9335238802433014 TRAIN  loss dict:  {'classification_loss': 0.9335238802433014}
2025-01-13 12:49:26,286 [INFO] Step[650/2713]: training loss : 0.9338126814365387 TRAIN  loss dict:  {'classification_loss': 0.9338126814365387}
2025-01-13 12:49:39,263 [INFO] Step[700/2713]: training loss : 0.9342558205127716 TRAIN  loss dict:  {'classification_loss': 0.9342558205127716}
2025-01-13 12:49:52,735 [INFO] Step[750/2713]: training loss : 0.9337118470668793 TRAIN  loss dict:  {'classification_loss': 0.9337118470668793}
2025-01-13 12:50:05,655 [INFO] Step[800/2713]: training loss : 0.9335651814937591 TRAIN  loss dict:  {'classification_loss': 0.9335651814937591}
2025-01-13 12:50:18,638 [INFO] Step[850/2713]: training loss : 0.9331622433662414 TRAIN  loss dict:  {'classification_loss': 0.9331622433662414}
2025-01-13 12:50:31,874 [INFO] Step[900/2713]: training loss : 0.9330250906944275 TRAIN  loss dict:  {'classification_loss': 0.9330250906944275}
2025-01-13 12:50:44,780 [INFO] Step[950/2713]: training loss : 0.9346830415725708 TRAIN  loss dict:  {'classification_loss': 0.9346830415725708}
2025-01-13 12:50:58,232 [INFO] Step[1000/2713]: training loss : 0.9331675481796264 TRAIN  loss dict:  {'classification_loss': 0.9331675481796264}
2025-01-13 12:51:11,620 [INFO] Step[1050/2713]: training loss : 0.9346524858474732 TRAIN  loss dict:  {'classification_loss': 0.9346524858474732}
2025-01-13 12:51:24,991 [INFO] Step[1100/2713]: training loss : 0.9330907869338989 TRAIN  loss dict:  {'classification_loss': 0.9330907869338989}
2025-01-13 12:51:38,498 [INFO] Step[1150/2713]: training loss : 0.9344220566749573 TRAIN  loss dict:  {'classification_loss': 0.9344220566749573}
2025-01-13 12:51:51,140 [INFO] Step[1200/2713]: training loss : 0.9342445838451385 TRAIN  loss dict:  {'classification_loss': 0.9342445838451385}
2025-01-13 12:52:03,643 [INFO] Step[1250/2713]: training loss : 0.9319554221630096 TRAIN  loss dict:  {'classification_loss': 0.9319554221630096}
2025-01-13 12:52:16,463 [INFO] Step[1300/2713]: training loss : 0.9321737861633301 TRAIN  loss dict:  {'classification_loss': 0.9321737861633301}
2025-01-13 12:52:29,326 [INFO] Step[1350/2713]: training loss : 0.9334851312637329 TRAIN  loss dict:  {'classification_loss': 0.9334851312637329}
2025-01-13 12:52:41,829 [INFO] Step[1400/2713]: training loss : 0.9363033449649811 TRAIN  loss dict:  {'classification_loss': 0.9363033449649811}
2025-01-13 12:52:54,685 [INFO] Step[1450/2713]: training loss : 0.9351832902431488 TRAIN  loss dict:  {'classification_loss': 0.9351832902431488}
2025-01-13 12:53:07,815 [INFO] Step[1500/2713]: training loss : 0.933146003484726 TRAIN  loss dict:  {'classification_loss': 0.933146003484726}
2025-01-13 12:53:20,511 [INFO] Step[1550/2713]: training loss : 0.9331140267848969 TRAIN  loss dict:  {'classification_loss': 0.9331140267848969}
2025-01-13 12:53:33,480 [INFO] Step[1600/2713]: training loss : 0.9340806150436402 TRAIN  loss dict:  {'classification_loss': 0.9340806150436402}
2025-01-13 12:53:46,018 [INFO] Step[1650/2713]: training loss : 0.9332552039623261 TRAIN  loss dict:  {'classification_loss': 0.9332552039623261}
2025-01-13 12:53:58,556 [INFO] Step[1700/2713]: training loss : 0.9325809288024902 TRAIN  loss dict:  {'classification_loss': 0.9325809288024902}
2025-01-13 12:54:11,498 [INFO] Step[1750/2713]: training loss : 0.9342654609680175 TRAIN  loss dict:  {'classification_loss': 0.9342654609680175}
2025-01-13 12:54:24,700 [INFO] Step[1800/2713]: training loss : 0.9329122769832611 TRAIN  loss dict:  {'classification_loss': 0.9329122769832611}
2025-01-13 12:54:37,616 [INFO] Step[1850/2713]: training loss : 0.9328770387172699 TRAIN  loss dict:  {'classification_loss': 0.9328770387172699}
2025-01-13 12:54:50,109 [INFO] Step[1900/2713]: training loss : 0.9324273455142975 TRAIN  loss dict:  {'classification_loss': 0.9324273455142975}
2025-01-13 12:55:03,272 [INFO] Step[1950/2713]: training loss : 0.9338771975040436 TRAIN  loss dict:  {'classification_loss': 0.9338771975040436}
2025-01-13 12:55:16,140 [INFO] Step[2000/2713]: training loss : 0.9334836149215698 TRAIN  loss dict:  {'classification_loss': 0.9334836149215698}
2025-01-13 12:55:29,341 [INFO] Step[2050/2713]: training loss : 0.9334508013725281 TRAIN  loss dict:  {'classification_loss': 0.9334508013725281}
2025-01-13 12:55:42,249 [INFO] Step[2100/2713]: training loss : 0.9347643911838531 TRAIN  loss dict:  {'classification_loss': 0.9347643911838531}
2025-01-13 12:55:54,809 [INFO] Step[2150/2713]: training loss : 0.9325280332565308 TRAIN  loss dict:  {'classification_loss': 0.9325280332565308}
2025-01-13 12:56:07,811 [INFO] Step[2200/2713]: training loss : 0.9343800675868988 TRAIN  loss dict:  {'classification_loss': 0.9343800675868988}
2025-01-13 12:56:20,433 [INFO] Step[2250/2713]: training loss : 0.9334211194515228 TRAIN  loss dict:  {'classification_loss': 0.9334211194515228}
2025-01-13 12:56:33,090 [INFO] Step[2300/2713]: training loss : 0.9480329823493957 TRAIN  loss dict:  {'classification_loss': 0.9480329823493957}
2025-01-13 12:56:45,943 [INFO] Step[2350/2713]: training loss : 0.9342794108390808 TRAIN  loss dict:  {'classification_loss': 0.9342794108390808}
2025-01-13 12:56:58,969 [INFO] Step[2400/2713]: training loss : 0.9354129338264465 TRAIN  loss dict:  {'classification_loss': 0.9354129338264465}
2025-01-13 12:57:11,560 [INFO] Step[2450/2713]: training loss : 0.9342507421970367 TRAIN  loss dict:  {'classification_loss': 0.9342507421970367}
2025-01-13 12:57:24,107 [INFO] Step[2500/2713]: training loss : 0.9333178544044495 TRAIN  loss dict:  {'classification_loss': 0.9333178544044495}
2025-01-13 12:57:37,100 [INFO] Step[2550/2713]: training loss : 0.9331859683990479 TRAIN  loss dict:  {'classification_loss': 0.9331859683990479}
2025-01-13 12:57:50,032 [INFO] Step[2600/2713]: training loss : 0.9326457405090331 TRAIN  loss dict:  {'classification_loss': 0.9326457405090331}
2025-01-13 12:58:03,150 [INFO] Step[2650/2713]: training loss : 0.9337125205993653 TRAIN  loss dict:  {'classification_loss': 0.9337125205993653}
2025-01-13 12:58:16,369 [INFO] Step[2700/2713]: training loss : 0.9360984206199646 TRAIN  loss dict:  {'classification_loss': 0.9360984206199646}
2025-01-13 12:59:57,942 [INFO] Label accuracies statistics:
2025-01-13 12:59:57,942 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 0.75, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 0.75, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 1.0, 257: 0.75, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.75, 279: 1.0, 280: 0.75, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.5, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 12:59:57,944 [INFO] [106] TRAIN  loss: 0.934023339239557 acc: 0.9998771347831429
2025-01-13 12:59:57,944 [INFO] [106] TRAIN  loss dict: {'classification_loss': 0.934023339239557}
2025-01-13 12:59:57,944 [INFO] [106] VALIDATION loss: 1.690415123687651 VALIDATION acc: 0.8369905956112853
2025-01-13 12:59:57,944 [INFO] [106] VALIDATION loss dict: {'classification_loss': 1.690415123687651}
2025-01-13 12:59:57,944 [INFO] 
2025-01-13 13:00:16,570 [INFO] Step[50/2713]: training loss : 0.9336006832122803 TRAIN  loss dict:  {'classification_loss': 0.9336006832122803}
2025-01-13 13:00:29,131 [INFO] Step[100/2713]: training loss : 0.9332442605495452 TRAIN  loss dict:  {'classification_loss': 0.9332442605495452}
2025-01-13 13:00:41,686 [INFO] Step[150/2713]: training loss : 0.9333556234836579 TRAIN  loss dict:  {'classification_loss': 0.9333556234836579}
2025-01-13 13:00:54,628 [INFO] Step[200/2713]: training loss : 0.9342930710315704 TRAIN  loss dict:  {'classification_loss': 0.9342930710315704}
2025-01-13 13:01:07,185 [INFO] Step[250/2713]: training loss : 0.9330577909946441 TRAIN  loss dict:  {'classification_loss': 0.9330577909946441}
2025-01-13 13:01:19,729 [INFO] Step[300/2713]: training loss : 0.9331543040275574 TRAIN  loss dict:  {'classification_loss': 0.9331543040275574}
2025-01-13 13:01:32,306 [INFO] Step[350/2713]: training loss : 0.9332217562198639 TRAIN  loss dict:  {'classification_loss': 0.9332217562198639}
2025-01-13 13:01:45,081 [INFO] Step[400/2713]: training loss : 0.9346496832370758 TRAIN  loss dict:  {'classification_loss': 0.9346496832370758}
2025-01-13 13:01:57,778 [INFO] Step[450/2713]: training loss : 0.9336045873165131 TRAIN  loss dict:  {'classification_loss': 0.9336045873165131}
2025-01-13 13:02:10,709 [INFO] Step[500/2713]: training loss : 0.9335511076450348 TRAIN  loss dict:  {'classification_loss': 0.9335511076450348}
2025-01-13 13:02:23,911 [INFO] Step[550/2713]: training loss : 0.9339111220836639 TRAIN  loss dict:  {'classification_loss': 0.9339111220836639}
2025-01-13 13:02:36,504 [INFO] Step[600/2713]: training loss : 0.9329262411594391 TRAIN  loss dict:  {'classification_loss': 0.9329262411594391}
2025-01-13 13:02:49,288 [INFO] Step[650/2713]: training loss : 0.9337191784381866 TRAIN  loss dict:  {'classification_loss': 0.9337191784381866}
2025-01-13 13:03:02,102 [INFO] Step[700/2713]: training loss : 0.934293863773346 TRAIN  loss dict:  {'classification_loss': 0.934293863773346}
2025-01-13 13:03:14,987 [INFO] Step[750/2713]: training loss : 0.9330010092258454 TRAIN  loss dict:  {'classification_loss': 0.9330010092258454}
2025-01-13 13:03:27,586 [INFO] Step[800/2713]: training loss : 0.9330559730529785 TRAIN  loss dict:  {'classification_loss': 0.9330559730529785}
2025-01-13 13:03:40,539 [INFO] Step[850/2713]: training loss : 0.9336796808242798 TRAIN  loss dict:  {'classification_loss': 0.9336796808242798}
2025-01-13 13:03:53,336 [INFO] Step[900/2713]: training loss : 0.9331249105930328 TRAIN  loss dict:  {'classification_loss': 0.9331249105930328}
2025-01-13 13:04:06,113 [INFO] Step[950/2713]: training loss : 0.9344209218025208 TRAIN  loss dict:  {'classification_loss': 0.9344209218025208}
2025-01-13 13:04:19,013 [INFO] Step[1000/2713]: training loss : 0.9337380456924439 TRAIN  loss dict:  {'classification_loss': 0.9337380456924439}
2025-01-13 13:04:31,669 [INFO] Step[1050/2713]: training loss : 0.9370954608917237 TRAIN  loss dict:  {'classification_loss': 0.9370954608917237}
2025-01-13 13:04:44,822 [INFO] Step[1100/2713]: training loss : 0.9332559180259704 TRAIN  loss dict:  {'classification_loss': 0.9332559180259704}
2025-01-13 13:04:57,580 [INFO] Step[1150/2713]: training loss : 0.9323456847667694 TRAIN  loss dict:  {'classification_loss': 0.9323456847667694}
2025-01-13 13:05:10,512 [INFO] Step[1200/2713]: training loss : 0.9334162878990173 TRAIN  loss dict:  {'classification_loss': 0.9334162878990173}
2025-01-13 13:05:23,159 [INFO] Step[1250/2713]: training loss : 0.9344046187400817 TRAIN  loss dict:  {'classification_loss': 0.9344046187400817}
2025-01-13 13:05:35,930 [INFO] Step[1300/2713]: training loss : 0.9328517162799835 TRAIN  loss dict:  {'classification_loss': 0.9328517162799835}
2025-01-13 13:05:49,142 [INFO] Step[1350/2713]: training loss : 0.9333468115329743 TRAIN  loss dict:  {'classification_loss': 0.9333468115329743}
2025-01-13 13:06:02,134 [INFO] Step[1400/2713]: training loss : 0.9332219815254211 TRAIN  loss dict:  {'classification_loss': 0.9332219815254211}
2025-01-13 13:06:15,235 [INFO] Step[1450/2713]: training loss : 0.9333344149589539 TRAIN  loss dict:  {'classification_loss': 0.9333344149589539}
2025-01-13 13:06:27,859 [INFO] Step[1500/2713]: training loss : 0.9331510639190674 TRAIN  loss dict:  {'classification_loss': 0.9331510639190674}
2025-01-13 13:06:40,936 [INFO] Step[1550/2713]: training loss : 0.9346168971061707 TRAIN  loss dict:  {'classification_loss': 0.9346168971061707}
2025-01-13 13:06:53,495 [INFO] Step[1600/2713]: training loss : 0.9334736120700836 TRAIN  loss dict:  {'classification_loss': 0.9334736120700836}
2025-01-13 13:07:06,051 [INFO] Step[1650/2713]: training loss : 0.933511620759964 TRAIN  loss dict:  {'classification_loss': 0.933511620759964}
2025-01-13 13:07:18,870 [INFO] Step[1700/2713]: training loss : 0.9338947868347168 TRAIN  loss dict:  {'classification_loss': 0.9338947868347168}
2025-01-13 13:07:32,138 [INFO] Step[1750/2713]: training loss : 0.9330872249603271 TRAIN  loss dict:  {'classification_loss': 0.9330872249603271}
2025-01-13 13:07:45,037 [INFO] Step[1800/2713]: training loss : 0.9343950438499451 TRAIN  loss dict:  {'classification_loss': 0.9343950438499451}
2025-01-13 13:07:57,679 [INFO] Step[1850/2713]: training loss : 0.9336510705947876 TRAIN  loss dict:  {'classification_loss': 0.9336510705947876}
2025-01-13 13:08:10,253 [INFO] Step[1900/2713]: training loss : 0.9340152025222779 TRAIN  loss dict:  {'classification_loss': 0.9340152025222779}
2025-01-13 13:08:22,816 [INFO] Step[1950/2713]: training loss : 0.9335685741901397 TRAIN  loss dict:  {'classification_loss': 0.9335685741901397}
2025-01-13 13:08:35,370 [INFO] Step[2000/2713]: training loss : 0.9330593168735504 TRAIN  loss dict:  {'classification_loss': 0.9330593168735504}
2025-01-13 13:08:48,333 [INFO] Step[2050/2713]: training loss : 0.9343055999279022 TRAIN  loss dict:  {'classification_loss': 0.9343055999279022}
2025-01-13 13:09:00,941 [INFO] Step[2100/2713]: training loss : 0.9329086601734161 TRAIN  loss dict:  {'classification_loss': 0.9329086601734161}
2025-01-13 13:09:13,510 [INFO] Step[2150/2713]: training loss : 0.9335619330406189 TRAIN  loss dict:  {'classification_loss': 0.9335619330406189}
2025-01-13 13:09:26,117 [INFO] Step[2200/2713]: training loss : 0.9337983500957489 TRAIN  loss dict:  {'classification_loss': 0.9337983500957489}
2025-01-13 13:09:39,033 [INFO] Step[2250/2713]: training loss : 0.9335288059711456 TRAIN  loss dict:  {'classification_loss': 0.9335288059711456}
2025-01-13 13:09:54,240 [INFO] Step[2300/2713]: training loss : 0.9345937311649323 TRAIN  loss dict:  {'classification_loss': 0.9345937311649323}
2025-01-13 13:10:08,536 [INFO] Step[2350/2713]: training loss : 0.9342002487182617 TRAIN  loss dict:  {'classification_loss': 0.9342002487182617}
2025-01-13 13:10:21,264 [INFO] Step[2400/2713]: training loss : 0.9376989531517029 TRAIN  loss dict:  {'classification_loss': 0.9376989531517029}
2025-01-13 13:10:34,020 [INFO] Step[2450/2713]: training loss : 0.9327432668209076 TRAIN  loss dict:  {'classification_loss': 0.9327432668209076}
2025-01-13 13:10:46,560 [INFO] Step[2500/2713]: training loss : 0.9330703246593476 TRAIN  loss dict:  {'classification_loss': 0.9330703246593476}
2025-01-13 13:10:59,347 [INFO] Step[2550/2713]: training loss : 0.9338111329078674 TRAIN  loss dict:  {'classification_loss': 0.9338111329078674}
2025-01-13 13:11:12,533 [INFO] Step[2600/2713]: training loss : 0.934309515953064 TRAIN  loss dict:  {'classification_loss': 0.934309515953064}
2025-01-13 13:11:25,144 [INFO] Step[2650/2713]: training loss : 0.9331150054931641 TRAIN  loss dict:  {'classification_loss': 0.9331150054931641}
2025-01-13 13:11:38,196 [INFO] Step[2700/2713]: training loss : 0.9324701070785523 TRAIN  loss dict:  {'classification_loss': 0.9324701070785523}
2025-01-13 13:13:25,282 [INFO] Label accuracies statistics:
2025-01-13 13:13:25,282 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.25, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.25, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 1.0, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 0.75, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 13:13:25,285 [INFO] [107] TRAIN  loss: 0.9337017532208728 acc: 1.0
2025-01-13 13:13:25,285 [INFO] [107] TRAIN  loss dict: {'classification_loss': 0.9337017532208728}
2025-01-13 13:13:25,285 [INFO] [107] VALIDATION loss: 1.6920166166877388 VALIDATION acc: 0.8369905956112853
2025-01-13 13:13:25,285 [INFO] [107] VALIDATION loss dict: {'classification_loss': 1.6920166166877388}
2025-01-13 13:13:25,285 [INFO] 
2025-01-13 13:13:46,659 [INFO] Step[50/2713]: training loss : 0.932774749994278 TRAIN  loss dict:  {'classification_loss': 0.932774749994278}
2025-01-13 13:13:59,696 [INFO] Step[100/2713]: training loss : 0.9329019463062287 TRAIN  loss dict:  {'classification_loss': 0.9329019463062287}
2025-01-13 13:14:12,929 [INFO] Step[150/2713]: training loss : 0.9329590916633606 TRAIN  loss dict:  {'classification_loss': 0.9329590916633606}
2025-01-13 13:14:25,990 [INFO] Step[200/2713]: training loss : 0.93375936627388 TRAIN  loss dict:  {'classification_loss': 0.93375936627388}
2025-01-13 13:14:40,050 [INFO] Step[250/2713]: training loss : 0.9348948574066163 TRAIN  loss dict:  {'classification_loss': 0.9348948574066163}
2025-01-13 13:14:54,501 [INFO] Step[300/2713]: training loss : 0.937531887292862 TRAIN  loss dict:  {'classification_loss': 0.937531887292862}
2025-01-13 13:15:07,918 [INFO] Step[350/2713]: training loss : 0.9346844637393952 TRAIN  loss dict:  {'classification_loss': 0.9346844637393952}
2025-01-13 13:15:20,897 [INFO] Step[400/2713]: training loss : 0.9332505619525909 TRAIN  loss dict:  {'classification_loss': 0.9332505619525909}
2025-01-13 13:15:33,399 [INFO] Step[450/2713]: training loss : 0.9328966391086578 TRAIN  loss dict:  {'classification_loss': 0.9328966391086578}
2025-01-13 13:15:45,975 [INFO] Step[500/2713]: training loss : 0.9354674029350281 TRAIN  loss dict:  {'classification_loss': 0.9354674029350281}
2025-01-13 13:15:59,164 [INFO] Step[550/2713]: training loss : 0.9347231805324554 TRAIN  loss dict:  {'classification_loss': 0.9347231805324554}
2025-01-13 13:16:11,913 [INFO] Step[600/2713]: training loss : 0.9329435658454895 TRAIN  loss dict:  {'classification_loss': 0.9329435658454895}
2025-01-13 13:16:24,716 [INFO] Step[650/2713]: training loss : 0.9344566488265991 TRAIN  loss dict:  {'classification_loss': 0.9344566488265991}
2025-01-13 13:16:37,632 [INFO] Step[700/2713]: training loss : 0.9338951277732849 TRAIN  loss dict:  {'classification_loss': 0.9338951277732849}
2025-01-13 13:16:50,598 [INFO] Step[750/2713]: training loss : 0.9326608431339264 TRAIN  loss dict:  {'classification_loss': 0.9326608431339264}
2025-01-13 13:17:03,414 [INFO] Step[800/2713]: training loss : 0.9333360111713409 TRAIN  loss dict:  {'classification_loss': 0.9333360111713409}
2025-01-13 13:17:15,979 [INFO] Step[850/2713]: training loss : 0.9336673486232757 TRAIN  loss dict:  {'classification_loss': 0.9336673486232757}
2025-01-13 13:17:29,155 [INFO] Step[900/2713]: training loss : 0.9333402907848358 TRAIN  loss dict:  {'classification_loss': 0.9333402907848358}
2025-01-13 13:17:42,379 [INFO] Step[950/2713]: training loss : 0.9319768142700195 TRAIN  loss dict:  {'classification_loss': 0.9319768142700195}
2025-01-13 13:17:55,538 [INFO] Step[1000/2713]: training loss : 0.9324804854393005 TRAIN  loss dict:  {'classification_loss': 0.9324804854393005}
2025-01-13 13:18:08,186 [INFO] Step[1050/2713]: training loss : 0.9348584830760955 TRAIN  loss dict:  {'classification_loss': 0.9348584830760955}
2025-01-13 13:18:21,093 [INFO] Step[1100/2713]: training loss : 0.9328373980522155 TRAIN  loss dict:  {'classification_loss': 0.9328373980522155}
2025-01-13 13:18:34,340 [INFO] Step[1150/2713]: training loss : 0.932846121788025 TRAIN  loss dict:  {'classification_loss': 0.932846121788025}
2025-01-13 13:18:47,528 [INFO] Step[1200/2713]: training loss : 0.9336568892002106 TRAIN  loss dict:  {'classification_loss': 0.9336568892002106}
2025-01-13 13:19:00,293 [INFO] Step[1250/2713]: training loss : 0.9323737621307373 TRAIN  loss dict:  {'classification_loss': 0.9323737621307373}
2025-01-13 13:19:13,257 [INFO] Step[1300/2713]: training loss : 0.9342537808418274 TRAIN  loss dict:  {'classification_loss': 0.9342537808418274}
2025-01-13 13:19:25,888 [INFO] Step[1350/2713]: training loss : 0.93404221534729 TRAIN  loss dict:  {'classification_loss': 0.93404221534729}
2025-01-13 13:19:38,438 [INFO] Step[1400/2713]: training loss : 0.9345489573478699 TRAIN  loss dict:  {'classification_loss': 0.9345489573478699}
2025-01-13 13:19:51,131 [INFO] Step[1450/2713]: training loss : 0.9330576872825622 TRAIN  loss dict:  {'classification_loss': 0.9330576872825622}
2025-01-13 13:20:04,051 [INFO] Step[1500/2713]: training loss : 0.9317441523075104 TRAIN  loss dict:  {'classification_loss': 0.9317441523075104}
2025-01-13 13:20:17,199 [INFO] Step[1550/2713]: training loss : 0.9336387777328491 TRAIN  loss dict:  {'classification_loss': 0.9336387777328491}
2025-01-13 13:20:30,263 [INFO] Step[1600/2713]: training loss : 0.9327349627017975 TRAIN  loss dict:  {'classification_loss': 0.9327349627017975}
2025-01-13 13:20:42,835 [INFO] Step[1650/2713]: training loss : 0.9329256415367126 TRAIN  loss dict:  {'classification_loss': 0.9329256415367126}
2025-01-13 13:20:55,415 [INFO] Step[1700/2713]: training loss : 0.9338457357883453 TRAIN  loss dict:  {'classification_loss': 0.9338457357883453}
2025-01-13 13:21:08,018 [INFO] Step[1750/2713]: training loss : 0.9318630146980286 TRAIN  loss dict:  {'classification_loss': 0.9318630146980286}
2025-01-13 13:21:20,559 [INFO] Step[1800/2713]: training loss : 0.9342754757404328 TRAIN  loss dict:  {'classification_loss': 0.9342754757404328}
2025-01-13 13:21:33,189 [INFO] Step[1850/2713]: training loss : 0.9318689334392548 TRAIN  loss dict:  {'classification_loss': 0.9318689334392548}
2025-01-13 13:21:45,758 [INFO] Step[1900/2713]: training loss : 0.9325518429279327 TRAIN  loss dict:  {'classification_loss': 0.9325518429279327}
2025-01-13 13:21:58,952 [INFO] Step[1950/2713]: training loss : 0.9327028584480286 TRAIN  loss dict:  {'classification_loss': 0.9327028584480286}
2025-01-13 13:22:11,939 [INFO] Step[2000/2713]: training loss : 0.9339190983772278 TRAIN  loss dict:  {'classification_loss': 0.9339190983772278}
2025-01-13 13:22:24,542 [INFO] Step[2050/2713]: training loss : 0.9325559031963349 TRAIN  loss dict:  {'classification_loss': 0.9325559031963349}
2025-01-13 13:22:37,613 [INFO] Step[2100/2713]: training loss : 0.9344505035877227 TRAIN  loss dict:  {'classification_loss': 0.9344505035877227}
2025-01-13 13:22:50,582 [INFO] Step[2150/2713]: training loss : 0.9342510545253754 TRAIN  loss dict:  {'classification_loss': 0.9342510545253754}
2025-01-13 13:23:03,140 [INFO] Step[2200/2713]: training loss : 0.933086267709732 TRAIN  loss dict:  {'classification_loss': 0.933086267709732}
2025-01-13 13:23:15,735 [INFO] Step[2250/2713]: training loss : 0.9329451072216034 TRAIN  loss dict:  {'classification_loss': 0.9329451072216034}
2025-01-13 13:23:28,311 [INFO] Step[2300/2713]: training loss : 0.934828530550003 TRAIN  loss dict:  {'classification_loss': 0.934828530550003}
2025-01-13 13:23:40,892 [INFO] Step[2350/2713]: training loss : 0.9328405010700226 TRAIN  loss dict:  {'classification_loss': 0.9328405010700226}
2025-01-13 13:23:53,926 [INFO] Step[2400/2713]: training loss : 0.9346269428730011 TRAIN  loss dict:  {'classification_loss': 0.9346269428730011}
2025-01-13 13:24:06,511 [INFO] Step[2450/2713]: training loss : 0.933996856212616 TRAIN  loss dict:  {'classification_loss': 0.933996856212616}
2025-01-13 13:24:19,037 [INFO] Step[2500/2713]: training loss : 0.9340629506111146 TRAIN  loss dict:  {'classification_loss': 0.9340629506111146}
2025-01-13 13:24:31,628 [INFO] Step[2550/2713]: training loss : 0.9341453504562378 TRAIN  loss dict:  {'classification_loss': 0.9341453504562378}
2025-01-13 13:24:44,356 [INFO] Step[2600/2713]: training loss : 0.9338356077671051 TRAIN  loss dict:  {'classification_loss': 0.9338356077671051}
2025-01-13 13:24:57,207 [INFO] Step[2650/2713]: training loss : 0.9334789979457855 TRAIN  loss dict:  {'classification_loss': 0.9334789979457855}
2025-01-13 13:25:09,969 [INFO] Step[2700/2713]: training loss : 0.9332455563545227 TRAIN  loss dict:  {'classification_loss': 0.9332455563545227}
2025-01-13 13:26:55,606 [INFO] Label accuracies statistics:
2025-01-13 13:26:55,606 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.5, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 0.75, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 0.75, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.5, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 13:26:55,609 [INFO] [108] TRAIN  loss: 0.9335394345918705 acc: 1.0
2025-01-13 13:26:55,609 [INFO] [108] TRAIN  loss dict: {'classification_loss': 0.9335394345918705}
2025-01-13 13:26:55,609 [INFO] [108] VALIDATION loss: 1.728745875053836 VALIDATION acc: 0.8351097178683385
2025-01-13 13:26:55,609 [INFO] [108] VALIDATION loss dict: {'classification_loss': 1.728745875053836}
2025-01-13 13:26:55,610 [INFO] 
2025-01-13 13:27:15,428 [INFO] Step[50/2713]: training loss : 0.9331353390216828 TRAIN  loss dict:  {'classification_loss': 0.9331353390216828}
2025-01-13 13:27:28,832 [INFO] Step[100/2713]: training loss : 0.933497120141983 TRAIN  loss dict:  {'classification_loss': 0.933497120141983}
2025-01-13 13:27:41,550 [INFO] Step[150/2713]: training loss : 0.9333915829658508 TRAIN  loss dict:  {'classification_loss': 0.9333915829658508}
2025-01-13 13:27:54,290 [INFO] Step[200/2713]: training loss : 0.9338263559341431 TRAIN  loss dict:  {'classification_loss': 0.9338263559341431}
2025-01-13 13:28:07,286 [INFO] Step[250/2713]: training loss : 0.9357402002811432 TRAIN  loss dict:  {'classification_loss': 0.9357402002811432}
2025-01-13 13:28:20,539 [INFO] Step[300/2713]: training loss : 0.9332535946369171 TRAIN  loss dict:  {'classification_loss': 0.9332535946369171}
2025-01-13 13:28:33,279 [INFO] Step[350/2713]: training loss : 0.9342900490760804 TRAIN  loss dict:  {'classification_loss': 0.9342900490760804}
2025-01-13 13:28:45,799 [INFO] Step[400/2713]: training loss : 0.9321496498584747 TRAIN  loss dict:  {'classification_loss': 0.9321496498584747}
2025-01-13 13:28:58,408 [INFO] Step[450/2713]: training loss : 0.9325825846195221 TRAIN  loss dict:  {'classification_loss': 0.9325825846195221}
2025-01-13 13:29:11,381 [INFO] Step[500/2713]: training loss : 0.9325304627418518 TRAIN  loss dict:  {'classification_loss': 0.9325304627418518}
2025-01-13 13:29:26,089 [INFO] Step[550/2713]: training loss : 0.9346290910243988 TRAIN  loss dict:  {'classification_loss': 0.9346290910243988}
2025-01-13 13:29:41,084 [INFO] Step[600/2713]: training loss : 0.9356298613548278 TRAIN  loss dict:  {'classification_loss': 0.9356298613548278}
2025-01-13 13:29:55,168 [INFO] Step[650/2713]: training loss : 0.9311862874031067 TRAIN  loss dict:  {'classification_loss': 0.9311862874031067}
2025-01-13 13:30:08,001 [INFO] Step[700/2713]: training loss : 0.9350229799747467 TRAIN  loss dict:  {'classification_loss': 0.9350229799747467}
2025-01-13 13:30:20,958 [INFO] Step[750/2713]: training loss : 0.9346205568313599 TRAIN  loss dict:  {'classification_loss': 0.9346205568313599}
2025-01-13 13:30:34,168 [INFO] Step[800/2713]: training loss : 0.9324414801597595 TRAIN  loss dict:  {'classification_loss': 0.9324414801597595}
2025-01-13 13:30:46,739 [INFO] Step[850/2713]: training loss : 0.934305180311203 TRAIN  loss dict:  {'classification_loss': 0.934305180311203}
2025-01-13 13:30:59,629 [INFO] Step[900/2713]: training loss : 0.932834665775299 TRAIN  loss dict:  {'classification_loss': 0.932834665775299}
2025-01-13 13:31:12,890 [INFO] Step[950/2713]: training loss : 0.9339929795265198 TRAIN  loss dict:  {'classification_loss': 0.9339929795265198}
2025-01-13 13:31:25,762 [INFO] Step[1000/2713]: training loss : 0.9328698146343232 TRAIN  loss dict:  {'classification_loss': 0.9328698146343232}
2025-01-13 13:31:38,909 [INFO] Step[1050/2713]: training loss : 0.9329485857486725 TRAIN  loss dict:  {'classification_loss': 0.9329485857486725}
2025-01-13 13:31:52,009 [INFO] Step[1100/2713]: training loss : 0.9348463296890259 TRAIN  loss dict:  {'classification_loss': 0.9348463296890259}
2025-01-13 13:32:04,952 [INFO] Step[1150/2713]: training loss : 0.9335564017295838 TRAIN  loss dict:  {'classification_loss': 0.9335564017295838}
2025-01-13 13:32:17,874 [INFO] Step[1200/2713]: training loss : 0.9336033093929291 TRAIN  loss dict:  {'classification_loss': 0.9336033093929291}
2025-01-13 13:32:30,999 [INFO] Step[1250/2713]: training loss : 0.9321609902381897 TRAIN  loss dict:  {'classification_loss': 0.9321609902381897}
2025-01-13 13:32:43,868 [INFO] Step[1300/2713]: training loss : 0.9337091970443726 TRAIN  loss dict:  {'classification_loss': 0.9337091970443726}
2025-01-13 13:32:56,730 [INFO] Step[1350/2713]: training loss : 0.9334603083133698 TRAIN  loss dict:  {'classification_loss': 0.9334603083133698}
2025-01-13 13:33:09,743 [INFO] Step[1400/2713]: training loss : 0.9322047376632691 TRAIN  loss dict:  {'classification_loss': 0.9322047376632691}
2025-01-13 13:33:22,783 [INFO] Step[1450/2713]: training loss : 0.9337171924114227 TRAIN  loss dict:  {'classification_loss': 0.9337171924114227}
2025-01-13 13:33:35,300 [INFO] Step[1500/2713]: training loss : 0.9354351007938385 TRAIN  loss dict:  {'classification_loss': 0.9354351007938385}
2025-01-13 13:33:48,161 [INFO] Step[1550/2713]: training loss : 0.9347705900669098 TRAIN  loss dict:  {'classification_loss': 0.9347705900669098}
2025-01-13 13:34:01,097 [INFO] Step[1600/2713]: training loss : 0.9322757089138031 TRAIN  loss dict:  {'classification_loss': 0.9322757089138031}
2025-01-13 13:34:14,331 [INFO] Step[1650/2713]: training loss : 0.9335790681838989 TRAIN  loss dict:  {'classification_loss': 0.9335790681838989}
2025-01-13 13:34:26,946 [INFO] Step[1700/2713]: training loss : 0.9327416574954986 TRAIN  loss dict:  {'classification_loss': 0.9327416574954986}
2025-01-13 13:34:39,814 [INFO] Step[1750/2713]: training loss : 0.9328988575935364 TRAIN  loss dict:  {'classification_loss': 0.9328988575935364}
2025-01-13 13:34:52,863 [INFO] Step[1800/2713]: training loss : 0.9329907989501953 TRAIN  loss dict:  {'classification_loss': 0.9329907989501953}
2025-01-13 13:35:05,838 [INFO] Step[1850/2713]: training loss : 0.9329651129245758 TRAIN  loss dict:  {'classification_loss': 0.9329651129245758}
2025-01-13 13:35:18,747 [INFO] Step[1900/2713]: training loss : 0.932682774066925 TRAIN  loss dict:  {'classification_loss': 0.932682774066925}
2025-01-13 13:35:31,748 [INFO] Step[1950/2713]: training loss : 0.9357639360427856 TRAIN  loss dict:  {'classification_loss': 0.9357639360427856}
2025-01-13 13:35:44,333 [INFO] Step[2000/2713]: training loss : 0.9340608859062195 TRAIN  loss dict:  {'classification_loss': 0.9340608859062195}
2025-01-13 13:35:57,247 [INFO] Step[2050/2713]: training loss : 0.9335706555843353 TRAIN  loss dict:  {'classification_loss': 0.9335706555843353}
2025-01-13 13:36:10,419 [INFO] Step[2100/2713]: training loss : 0.9332662725448608 TRAIN  loss dict:  {'classification_loss': 0.9332662725448608}
2025-01-13 13:36:23,874 [INFO] Step[2150/2713]: training loss : 0.9342825031280517 TRAIN  loss dict:  {'classification_loss': 0.9342825031280517}
2025-01-13 13:36:36,992 [INFO] Step[2200/2713]: training loss : 0.9336296832561493 TRAIN  loss dict:  {'classification_loss': 0.9336296832561493}
2025-01-13 13:36:50,001 [INFO] Step[2250/2713]: training loss : 0.933808445930481 TRAIN  loss dict:  {'classification_loss': 0.933808445930481}
2025-01-13 13:37:02,947 [INFO] Step[2300/2713]: training loss : 0.9326021373271942 TRAIN  loss dict:  {'classification_loss': 0.9326021373271942}
2025-01-13 13:37:15,638 [INFO] Step[2350/2713]: training loss : 0.933508722782135 TRAIN  loss dict:  {'classification_loss': 0.933508722782135}
2025-01-13 13:37:28,410 [INFO] Step[2400/2713]: training loss : 0.932713177204132 TRAIN  loss dict:  {'classification_loss': 0.932713177204132}
2025-01-13 13:37:42,116 [INFO] Step[2450/2713]: training loss : 0.9355286478996276 TRAIN  loss dict:  {'classification_loss': 0.9355286478996276}
2025-01-13 13:37:56,027 [INFO] Step[2500/2713]: training loss : 0.9344857060909271 TRAIN  loss dict:  {'classification_loss': 0.9344857060909271}
2025-01-13 13:38:09,665 [INFO] Step[2550/2713]: training loss : 0.9318890058994294 TRAIN  loss dict:  {'classification_loss': 0.9318890058994294}
2025-01-13 13:38:22,746 [INFO] Step[2600/2713]: training loss : 0.9334615576267242 TRAIN  loss dict:  {'classification_loss': 0.9334615576267242}
2025-01-13 13:38:35,356 [INFO] Step[2650/2713]: training loss : 0.9326313328742981 TRAIN  loss dict:  {'classification_loss': 0.9326313328742981}
2025-01-13 13:38:48,386 [INFO] Step[2700/2713]: training loss : 0.9321354746818542 TRAIN  loss dict:  {'classification_loss': 0.9321354746818542}
2025-01-13 13:40:32,110 [INFO] Label accuracies statistics:
2025-01-13 13:40:32,110 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.25, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.5, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.25, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 0.75, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 0.75, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 13:40:32,113 [INFO] [109] TRAIN  loss: 0.933516349732722 acc: 1.0
2025-01-13 13:40:32,113 [INFO] [109] TRAIN  loss dict: {'classification_loss': 0.933516349732722}
2025-01-13 13:40:32,113 [INFO] [109] VALIDATION loss: 1.706278543723257 VALIDATION acc: 0.8231974921630094
2025-01-13 13:40:32,113 [INFO] [109] VALIDATION loss dict: {'classification_loss': 1.706278543723257}
2025-01-13 13:40:32,114 [INFO] 
2025-01-13 13:40:50,788 [INFO] Step[50/2713]: training loss : 0.9328549563884735 TRAIN  loss dict:  {'classification_loss': 0.9328549563884735}
2025-01-13 13:41:03,371 [INFO] Step[100/2713]: training loss : 0.9333763909339905 TRAIN  loss dict:  {'classification_loss': 0.9333763909339905}
2025-01-13 13:41:15,964 [INFO] Step[150/2713]: training loss : 0.9352898728847504 TRAIN  loss dict:  {'classification_loss': 0.9352898728847504}
2025-01-13 13:41:28,781 [INFO] Step[200/2713]: training loss : 0.933051198720932 TRAIN  loss dict:  {'classification_loss': 0.933051198720932}
2025-01-13 13:41:41,651 [INFO] Step[250/2713]: training loss : 0.9327937591075898 TRAIN  loss dict:  {'classification_loss': 0.9327937591075898}
2025-01-13 13:41:54,657 [INFO] Step[300/2713]: training loss : 0.9336756920814514 TRAIN  loss dict:  {'classification_loss': 0.9336756920814514}
2025-01-13 13:42:07,163 [INFO] Step[350/2713]: training loss : 0.9333531796932221 TRAIN  loss dict:  {'classification_loss': 0.9333531796932221}
2025-01-13 13:42:20,430 [INFO] Step[400/2713]: training loss : 0.9349218463897705 TRAIN  loss dict:  {'classification_loss': 0.9349218463897705}
2025-01-13 13:42:33,599 [INFO] Step[450/2713]: training loss : 0.9329521489143372 TRAIN  loss dict:  {'classification_loss': 0.9329521489143372}
2025-01-13 13:42:46,487 [INFO] Step[500/2713]: training loss : 0.9331562530994415 TRAIN  loss dict:  {'classification_loss': 0.9331562530994415}
2025-01-13 13:42:59,698 [INFO] Step[550/2713]: training loss : 0.9334346449375153 TRAIN  loss dict:  {'classification_loss': 0.9334346449375153}
2025-01-13 13:43:12,693 [INFO] Step[600/2713]: training loss : 0.9340999329090118 TRAIN  loss dict:  {'classification_loss': 0.9340999329090118}
2025-01-13 13:43:25,805 [INFO] Step[650/2713]: training loss : 0.9325480914115906 TRAIN  loss dict:  {'classification_loss': 0.9325480914115906}
2025-01-13 13:43:38,854 [INFO] Step[700/2713]: training loss : 0.9339936089515686 TRAIN  loss dict:  {'classification_loss': 0.9339936089515686}
2025-01-13 13:43:51,671 [INFO] Step[750/2713]: training loss : 0.9337617790699005 TRAIN  loss dict:  {'classification_loss': 0.9337617790699005}
2025-01-13 13:44:04,860 [INFO] Step[800/2713]: training loss : 0.9327085888385773 TRAIN  loss dict:  {'classification_loss': 0.9327085888385773}
2025-01-13 13:44:17,989 [INFO] Step[850/2713]: training loss : 0.9334199666976929 TRAIN  loss dict:  {'classification_loss': 0.9334199666976929}
2025-01-13 13:44:30,893 [INFO] Step[900/2713]: training loss : 0.9330262815952302 TRAIN  loss dict:  {'classification_loss': 0.9330262815952302}
2025-01-13 13:44:43,814 [INFO] Step[950/2713]: training loss : 0.9335490179061889 TRAIN  loss dict:  {'classification_loss': 0.9335490179061889}
2025-01-13 13:44:56,464 [INFO] Step[1000/2713]: training loss : 0.933462553024292 TRAIN  loss dict:  {'classification_loss': 0.933462553024292}
2025-01-13 13:45:09,028 [INFO] Step[1050/2713]: training loss : 0.9329686367511749 TRAIN  loss dict:  {'classification_loss': 0.9329686367511749}
2025-01-13 13:45:21,693 [INFO] Step[1100/2713]: training loss : 0.9332372713088989 TRAIN  loss dict:  {'classification_loss': 0.9332372713088989}
2025-01-13 13:45:34,783 [INFO] Step[1150/2713]: training loss : 0.9341135299205781 TRAIN  loss dict:  {'classification_loss': 0.9341135299205781}
2025-01-13 13:45:47,970 [INFO] Step[1200/2713]: training loss : 0.9396624791622162 TRAIN  loss dict:  {'classification_loss': 0.9396624791622162}
2025-01-13 13:46:00,582 [INFO] Step[1250/2713]: training loss : 0.9335165989398956 TRAIN  loss dict:  {'classification_loss': 0.9335165989398956}
2025-01-13 13:46:13,642 [INFO] Step[1300/2713]: training loss : 0.9349140322208405 TRAIN  loss dict:  {'classification_loss': 0.9349140322208405}
2025-01-13 13:46:26,718 [INFO] Step[1350/2713]: training loss : 0.9329804813861847 TRAIN  loss dict:  {'classification_loss': 0.9329804813861847}
2025-01-13 13:46:39,737 [INFO] Step[1400/2713]: training loss : 0.9333843576908112 TRAIN  loss dict:  {'classification_loss': 0.9333843576908112}
2025-01-13 13:46:52,353 [INFO] Step[1450/2713]: training loss : 0.9347063887119293 TRAIN  loss dict:  {'classification_loss': 0.9347063887119293}
2025-01-13 13:47:04,999 [INFO] Step[1500/2713]: training loss : 0.9331323277950286 TRAIN  loss dict:  {'classification_loss': 0.9331323277950286}
2025-01-13 13:47:17,809 [INFO] Step[1550/2713]: training loss : 0.9328221714496613 TRAIN  loss dict:  {'classification_loss': 0.9328221714496613}
2025-01-13 13:47:30,562 [INFO] Step[1600/2713]: training loss : 0.9326870310306549 TRAIN  loss dict:  {'classification_loss': 0.9326870310306549}
2025-01-13 13:47:43,761 [INFO] Step[1650/2713]: training loss : 0.9329352593421936 TRAIN  loss dict:  {'classification_loss': 0.9329352593421936}
2025-01-13 13:47:56,899 [INFO] Step[1700/2713]: training loss : 0.9325937807559967 TRAIN  loss dict:  {'classification_loss': 0.9325937807559967}
2025-01-13 13:48:09,940 [INFO] Step[1750/2713]: training loss : 0.9326496732234955 TRAIN  loss dict:  {'classification_loss': 0.9326496732234955}
2025-01-13 13:48:22,557 [INFO] Step[1800/2713]: training loss : 0.9357617950439453 TRAIN  loss dict:  {'classification_loss': 0.9357617950439453}
2025-01-13 13:48:35,689 [INFO] Step[1850/2713]: training loss : 0.9330745065212249 TRAIN  loss dict:  {'classification_loss': 0.9330745065212249}
2025-01-13 13:48:48,830 [INFO] Step[1900/2713]: training loss : 0.9336436665058137 TRAIN  loss dict:  {'classification_loss': 0.9336436665058137}
2025-01-13 13:49:02,015 [INFO] Step[1950/2713]: training loss : 0.9335968589782715 TRAIN  loss dict:  {'classification_loss': 0.9335968589782715}
2025-01-13 13:49:14,804 [INFO] Step[2000/2713]: training loss : 0.9324718928337097 TRAIN  loss dict:  {'classification_loss': 0.9324718928337097}
2025-01-13 13:49:27,474 [INFO] Step[2050/2713]: training loss : 0.9337916457653046 TRAIN  loss dict:  {'classification_loss': 0.9337916457653046}
2025-01-13 13:49:40,681 [INFO] Step[2100/2713]: training loss : 0.9331674671173096 TRAIN  loss dict:  {'classification_loss': 0.9331674671173096}
2025-01-13 13:49:53,889 [INFO] Step[2150/2713]: training loss : 0.9332789945602417 TRAIN  loss dict:  {'classification_loss': 0.9332789945602417}
2025-01-13 13:50:06,766 [INFO] Step[2200/2713]: training loss : 0.9352613139152527 TRAIN  loss dict:  {'classification_loss': 0.9352613139152527}
2025-01-13 13:50:20,032 [INFO] Step[2250/2713]: training loss : 0.9318342125415802 TRAIN  loss dict:  {'classification_loss': 0.9318342125415802}
2025-01-13 13:50:32,740 [INFO] Step[2300/2713]: training loss : 0.9323549830913543 TRAIN  loss dict:  {'classification_loss': 0.9323549830913543}
2025-01-13 13:50:46,036 [INFO] Step[2350/2713]: training loss : 0.9335266089439392 TRAIN  loss dict:  {'classification_loss': 0.9335266089439392}
2025-01-13 13:50:58,813 [INFO] Step[2400/2713]: training loss : 0.9361763060092926 TRAIN  loss dict:  {'classification_loss': 0.9361763060092926}
2025-01-13 13:51:11,321 [INFO] Step[2450/2713]: training loss : 0.9348360109329223 TRAIN  loss dict:  {'classification_loss': 0.9348360109329223}
2025-01-13 13:51:23,962 [INFO] Step[2500/2713]: training loss : 0.9335740661621094 TRAIN  loss dict:  {'classification_loss': 0.9335740661621094}
2025-01-13 13:51:36,648 [INFO] Step[2550/2713]: training loss : 0.9333125376701354 TRAIN  loss dict:  {'classification_loss': 0.9333125376701354}
2025-01-13 13:51:49,804 [INFO] Step[2600/2713]: training loss : 0.932987949848175 TRAIN  loss dict:  {'classification_loss': 0.932987949848175}
2025-01-13 13:52:02,481 [INFO] Step[2650/2713]: training loss : 0.9324970805644989 TRAIN  loss dict:  {'classification_loss': 0.9324970805644989}
2025-01-13 13:52:15,234 [INFO] Step[2700/2713]: training loss : 0.9342313182353973 TRAIN  loss dict:  {'classification_loss': 0.9342313182353973}
2025-01-13 13:53:57,497 [INFO] Label accuracies statistics:
2025-01-13 13:53:57,497 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.25, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.5, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 0.75, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 0.75, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 1.0, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 0.75, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 13:53:57,499 [INFO] [110] TRAIN  loss: 0.9336073531948066 acc: 1.0
2025-01-13 13:53:57,499 [INFO] [110] TRAIN  loss dict: {'classification_loss': 0.9336073531948066}
2025-01-13 13:53:57,499 [INFO] [110] VALIDATION loss: 1.7222879787808971 VALIDATION acc: 0.8275862068965517
2025-01-13 13:53:57,499 [INFO] [110] VALIDATION loss dict: {'classification_loss': 1.7222879787808971}
2025-01-13 13:53:57,499 [INFO] 
2025-01-13 13:54:16,223 [INFO] Step[50/2713]: training loss : 0.9331290102005005 TRAIN  loss dict:  {'classification_loss': 0.9331290102005005}
2025-01-13 13:54:28,867 [INFO] Step[100/2713]: training loss : 0.9335691213607789 TRAIN  loss dict:  {'classification_loss': 0.9335691213607789}
2025-01-13 13:54:41,820 [INFO] Step[150/2713]: training loss : 0.9327353835105896 TRAIN  loss dict:  {'classification_loss': 0.9327353835105896}
2025-01-13 13:54:54,333 [INFO] Step[200/2713]: training loss : 0.9362617123126984 TRAIN  loss dict:  {'classification_loss': 0.9362617123126984}
2025-01-13 13:55:07,145 [INFO] Step[250/2713]: training loss : 0.9335148775577545 TRAIN  loss dict:  {'classification_loss': 0.9335148775577545}
2025-01-13 13:55:20,307 [INFO] Step[300/2713]: training loss : 0.9323617613315582 TRAIN  loss dict:  {'classification_loss': 0.9323617613315582}
2025-01-13 13:55:32,834 [INFO] Step[350/2713]: training loss : 0.934693877696991 TRAIN  loss dict:  {'classification_loss': 0.934693877696991}
2025-01-13 13:55:45,738 [INFO] Step[400/2713]: training loss : 0.9329389619827271 TRAIN  loss dict:  {'classification_loss': 0.9329389619827271}
2025-01-13 13:55:58,907 [INFO] Step[450/2713]: training loss : 0.934629294872284 TRAIN  loss dict:  {'classification_loss': 0.934629294872284}
2025-01-13 13:56:11,482 [INFO] Step[500/2713]: training loss : 0.9342454600334168 TRAIN  loss dict:  {'classification_loss': 0.9342454600334168}
2025-01-13 13:56:24,369 [INFO] Step[550/2713]: training loss : 0.9321971881389618 TRAIN  loss dict:  {'classification_loss': 0.9321971881389618}
2025-01-13 13:56:37,620 [INFO] Step[600/2713]: training loss : 0.9347679913043976 TRAIN  loss dict:  {'classification_loss': 0.9347679913043976}
2025-01-13 13:56:50,493 [INFO] Step[650/2713]: training loss : 0.9337615942955018 TRAIN  loss dict:  {'classification_loss': 0.9337615942955018}
2025-01-13 13:57:03,004 [INFO] Step[700/2713]: training loss : 0.9320257925987243 TRAIN  loss dict:  {'classification_loss': 0.9320257925987243}
2025-01-13 13:57:15,901 [INFO] Step[750/2713]: training loss : 0.9327647137641907 TRAIN  loss dict:  {'classification_loss': 0.9327647137641907}
2025-01-13 13:57:28,649 [INFO] Step[800/2713]: training loss : 0.9315480804443359 TRAIN  loss dict:  {'classification_loss': 0.9315480804443359}
2025-01-13 13:57:41,152 [INFO] Step[850/2713]: training loss : 0.9326577067375184 TRAIN  loss dict:  {'classification_loss': 0.9326577067375184}
2025-01-13 13:57:54,075 [INFO] Step[900/2713]: training loss : 0.9326547932624817 TRAIN  loss dict:  {'classification_loss': 0.9326547932624817}
2025-01-13 13:58:07,012 [INFO] Step[950/2713]: training loss : 0.932137954235077 TRAIN  loss dict:  {'classification_loss': 0.932137954235077}
2025-01-13 13:58:20,043 [INFO] Step[1000/2713]: training loss : 0.9339841401576996 TRAIN  loss dict:  {'classification_loss': 0.9339841401576996}
2025-01-13 13:58:32,521 [INFO] Step[1050/2713]: training loss : 0.9328628003597259 TRAIN  loss dict:  {'classification_loss': 0.9328628003597259}
2025-01-13 13:58:45,232 [INFO] Step[1100/2713]: training loss : 0.9353649401664734 TRAIN  loss dict:  {'classification_loss': 0.9353649401664734}
2025-01-13 13:58:58,151 [INFO] Step[1150/2713]: training loss : 0.9332345366477967 TRAIN  loss dict:  {'classification_loss': 0.9332345366477967}
2025-01-13 13:59:11,217 [INFO] Step[1200/2713]: training loss : 0.9335914337635041 TRAIN  loss dict:  {'classification_loss': 0.9335914337635041}
2025-01-13 13:59:24,473 [INFO] Step[1250/2713]: training loss : 0.9322514712810517 TRAIN  loss dict:  {'classification_loss': 0.9322514712810517}
2025-01-13 13:59:37,529 [INFO] Step[1300/2713]: training loss : 0.9338753616809845 TRAIN  loss dict:  {'classification_loss': 0.9338753616809845}
2025-01-13 13:59:50,536 [INFO] Step[1350/2713]: training loss : 0.9346373784542084 TRAIN  loss dict:  {'classification_loss': 0.9346373784542084}
2025-01-13 14:00:03,412 [INFO] Step[1400/2713]: training loss : 0.9331718945503235 TRAIN  loss dict:  {'classification_loss': 0.9331718945503235}
2025-01-13 14:00:16,847 [INFO] Step[1450/2713]: training loss : 0.9327564465999604 TRAIN  loss dict:  {'classification_loss': 0.9327564465999604}
2025-01-13 14:00:29,975 [INFO] Step[1500/2713]: training loss : 0.9349144732952118 TRAIN  loss dict:  {'classification_loss': 0.9349144732952118}
2025-01-13 14:00:42,977 [INFO] Step[1550/2713]: training loss : 0.9317889404296875 TRAIN  loss dict:  {'classification_loss': 0.9317889404296875}
2025-01-13 14:00:57,251 [INFO] Step[1600/2713]: training loss : 0.9328609466552734 TRAIN  loss dict:  {'classification_loss': 0.9328609466552734}
2025-01-13 14:01:12,025 [INFO] Step[1650/2713]: training loss : 0.9358858704566956 TRAIN  loss dict:  {'classification_loss': 0.9358858704566956}
2025-01-13 14:01:26,266 [INFO] Step[1700/2713]: training loss : 0.9328385436534882 TRAIN  loss dict:  {'classification_loss': 0.9328385436534882}
2025-01-13 14:01:39,289 [INFO] Step[1750/2713]: training loss : 0.9329270672798157 TRAIN  loss dict:  {'classification_loss': 0.9329270672798157}
2025-01-13 14:01:51,990 [INFO] Step[1800/2713]: training loss : 0.932771703004837 TRAIN  loss dict:  {'classification_loss': 0.932771703004837}
2025-01-13 14:02:04,584 [INFO] Step[1850/2713]: training loss : 0.933265665769577 TRAIN  loss dict:  {'classification_loss': 0.933265665769577}
2025-01-13 14:02:17,339 [INFO] Step[1900/2713]: training loss : 0.9332320499420166 TRAIN  loss dict:  {'classification_loss': 0.9332320499420166}
2025-01-13 14:02:30,523 [INFO] Step[1950/2713]: training loss : 0.9331288468837738 TRAIN  loss dict:  {'classification_loss': 0.9331288468837738}
2025-01-13 14:02:43,616 [INFO] Step[2000/2713]: training loss : 0.933432765007019 TRAIN  loss dict:  {'classification_loss': 0.933432765007019}
2025-01-13 14:02:56,691 [INFO] Step[2050/2713]: training loss : 0.9327575659751892 TRAIN  loss dict:  {'classification_loss': 0.9327575659751892}
2025-01-13 14:03:09,614 [INFO] Step[2100/2713]: training loss : 0.9336969280242919 TRAIN  loss dict:  {'classification_loss': 0.9336969280242919}
2025-01-13 14:03:22,132 [INFO] Step[2150/2713]: training loss : 0.9341466629505157 TRAIN  loss dict:  {'classification_loss': 0.9341466629505157}
2025-01-13 14:03:34,667 [INFO] Step[2200/2713]: training loss : 0.9329940032958984 TRAIN  loss dict:  {'classification_loss': 0.9329940032958984}
2025-01-13 14:03:47,515 [INFO] Step[2250/2713]: training loss : 0.9321079361438751 TRAIN  loss dict:  {'classification_loss': 0.9321079361438751}
2025-01-13 14:04:00,572 [INFO] Step[2300/2713]: training loss : 0.9323765647411346 TRAIN  loss dict:  {'classification_loss': 0.9323765647411346}
2025-01-13 14:04:13,536 [INFO] Step[2350/2713]: training loss : 0.9334914696216583 TRAIN  loss dict:  {'classification_loss': 0.9334914696216583}
2025-01-13 14:04:26,103 [INFO] Step[2400/2713]: training loss : 0.9325465059280396 TRAIN  loss dict:  {'classification_loss': 0.9325465059280396}
2025-01-13 14:04:38,600 [INFO] Step[2450/2713]: training loss : 0.9326804077625275 TRAIN  loss dict:  {'classification_loss': 0.9326804077625275}
2025-01-13 14:04:51,401 [INFO] Step[2500/2713]: training loss : 0.9327887833118439 TRAIN  loss dict:  {'classification_loss': 0.9327887833118439}
2025-01-13 14:05:04,128 [INFO] Step[2550/2713]: training loss : 0.9319704127311706 TRAIN  loss dict:  {'classification_loss': 0.9319704127311706}
2025-01-13 14:05:16,605 [INFO] Step[2600/2713]: training loss : 0.9324674606323242 TRAIN  loss dict:  {'classification_loss': 0.9324674606323242}
2025-01-13 14:05:30,137 [INFO] Step[2650/2713]: training loss : 0.9337617170810699 TRAIN  loss dict:  {'classification_loss': 0.9337617170810699}
2025-01-13 14:05:44,918 [INFO] Step[2700/2713]: training loss : 0.9323628199100494 TRAIN  loss dict:  {'classification_loss': 0.9323628199100494}
2025-01-13 14:07:27,881 [INFO] Label accuracies statistics:
2025-01-13 14:07:27,881 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.25, 64: 0.75, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 0.75, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 1.0, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 1.0, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 0.75, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 14:07:27,885 [INFO] [111] TRAIN  loss: 0.9332442906260534 acc: 1.0
2025-01-13 14:07:27,885 [INFO] [111] TRAIN  loss dict: {'classification_loss': 0.9332442906260534}
2025-01-13 14:07:27,885 [INFO] [111] VALIDATION loss: 1.7406482122894515 VALIDATION acc: 0.8175548589341692
2025-01-13 14:07:27,885 [INFO] [111] VALIDATION loss dict: {'classification_loss': 1.7406482122894515}
2025-01-13 14:07:27,885 [INFO] 
2025-01-13 14:07:46,665 [INFO] Step[50/2713]: training loss : 0.9322563552856445 TRAIN  loss dict:  {'classification_loss': 0.9322563552856445}
2025-01-13 14:07:59,502 [INFO] Step[100/2713]: training loss : 0.932600485086441 TRAIN  loss dict:  {'classification_loss': 0.932600485086441}
2025-01-13 14:08:12,300 [INFO] Step[150/2713]: training loss : 0.9335772335529328 TRAIN  loss dict:  {'classification_loss': 0.9335772335529328}
2025-01-13 14:08:25,378 [INFO] Step[200/2713]: training loss : 0.932464919090271 TRAIN  loss dict:  {'classification_loss': 0.932464919090271}
2025-01-13 14:08:38,567 [INFO] Step[250/2713]: training loss : 0.9327091526985168 TRAIN  loss dict:  {'classification_loss': 0.9327091526985168}
2025-01-13 14:08:51,479 [INFO] Step[300/2713]: training loss : 0.9320881640911103 TRAIN  loss dict:  {'classification_loss': 0.9320881640911103}
2025-01-13 14:09:04,495 [INFO] Step[350/2713]: training loss : 0.9327746772766113 TRAIN  loss dict:  {'classification_loss': 0.9327746772766113}
2025-01-13 14:09:17,483 [INFO] Step[400/2713]: training loss : 0.9324946796894074 TRAIN  loss dict:  {'classification_loss': 0.9324946796894074}
2025-01-13 14:09:30,126 [INFO] Step[450/2713]: training loss : 0.9335951447486878 TRAIN  loss dict:  {'classification_loss': 0.9335951447486878}
2025-01-13 14:09:43,132 [INFO] Step[500/2713]: training loss : 0.9332297503948211 TRAIN  loss dict:  {'classification_loss': 0.9332297503948211}
2025-01-13 14:09:56,113 [INFO] Step[550/2713]: training loss : 0.932974225282669 TRAIN  loss dict:  {'classification_loss': 0.932974225282669}
2025-01-13 14:10:09,097 [INFO] Step[600/2713]: training loss : 0.9332291221618653 TRAIN  loss dict:  {'classification_loss': 0.9332291221618653}
2025-01-13 14:10:22,066 [INFO] Step[650/2713]: training loss : 0.9327550053596496 TRAIN  loss dict:  {'classification_loss': 0.9327550053596496}
2025-01-13 14:10:34,884 [INFO] Step[700/2713]: training loss : 0.9343689787387848 TRAIN  loss dict:  {'classification_loss': 0.9343689787387848}
2025-01-13 14:10:47,801 [INFO] Step[750/2713]: training loss : 0.9325670278072358 TRAIN  loss dict:  {'classification_loss': 0.9325670278072358}
2025-01-13 14:11:01,013 [INFO] Step[800/2713]: training loss : 0.9336312234401702 TRAIN  loss dict:  {'classification_loss': 0.9336312234401702}
2025-01-13 14:11:13,677 [INFO] Step[850/2713]: training loss : 0.9325205862522126 TRAIN  loss dict:  {'classification_loss': 0.9325205862522126}
2025-01-13 14:11:26,972 [INFO] Step[900/2713]: training loss : 0.932543761730194 TRAIN  loss dict:  {'classification_loss': 0.932543761730194}
2025-01-13 14:11:39,770 [INFO] Step[950/2713]: training loss : 0.9317871499061584 TRAIN  loss dict:  {'classification_loss': 0.9317871499061584}
2025-01-13 14:11:52,387 [INFO] Step[1000/2713]: training loss : 0.9319579470157623 TRAIN  loss dict:  {'classification_loss': 0.9319579470157623}
2025-01-13 14:12:05,016 [INFO] Step[1050/2713]: training loss : 0.9325015771389008 TRAIN  loss dict:  {'classification_loss': 0.9325015771389008}
2025-01-13 14:12:18,276 [INFO] Step[1100/2713]: training loss : 0.9321377909183503 TRAIN  loss dict:  {'classification_loss': 0.9321377909183503}
2025-01-13 14:12:31,277 [INFO] Step[1150/2713]: training loss : 0.9331803107261658 TRAIN  loss dict:  {'classification_loss': 0.9331803107261658}
2025-01-13 14:12:43,806 [INFO] Step[1200/2713]: training loss : 0.9332562029361725 TRAIN  loss dict:  {'classification_loss': 0.9332562029361725}
2025-01-13 14:12:56,913 [INFO] Step[1250/2713]: training loss : 0.9332289385795594 TRAIN  loss dict:  {'classification_loss': 0.9332289385795594}
2025-01-13 14:13:09,472 [INFO] Step[1300/2713]: training loss : 0.9331754529476166 TRAIN  loss dict:  {'classification_loss': 0.9331754529476166}
2025-01-13 14:13:22,106 [INFO] Step[1350/2713]: training loss : 0.933349781036377 TRAIN  loss dict:  {'classification_loss': 0.933349781036377}
2025-01-13 14:13:34,946 [INFO] Step[1400/2713]: training loss : 0.9335535264015198 TRAIN  loss dict:  {'classification_loss': 0.9335535264015198}
2025-01-13 14:13:48,177 [INFO] Step[1450/2713]: training loss : 0.9337765026092529 TRAIN  loss dict:  {'classification_loss': 0.9337765026092529}
2025-01-13 14:14:01,093 [INFO] Step[1500/2713]: training loss : 0.9320826709270478 TRAIN  loss dict:  {'classification_loss': 0.9320826709270478}
2025-01-13 14:14:14,363 [INFO] Step[1550/2713]: training loss : 0.9328206086158752 TRAIN  loss dict:  {'classification_loss': 0.9328206086158752}
2025-01-13 14:14:27,614 [INFO] Step[1600/2713]: training loss : 0.9336349809169769 TRAIN  loss dict:  {'classification_loss': 0.9336349809169769}
2025-01-13 14:14:40,804 [INFO] Step[1650/2713]: training loss : 0.9358657634258271 TRAIN  loss dict:  {'classification_loss': 0.9358657634258271}
2025-01-13 14:14:53,762 [INFO] Step[1700/2713]: training loss : 0.9342579185962677 TRAIN  loss dict:  {'classification_loss': 0.9342579185962677}
2025-01-13 14:15:06,779 [INFO] Step[1750/2713]: training loss : 0.9329570305347442 TRAIN  loss dict:  {'classification_loss': 0.9329570305347442}
2025-01-13 14:15:19,693 [INFO] Step[1800/2713]: training loss : 0.9338097906112671 TRAIN  loss dict:  {'classification_loss': 0.9338097906112671}
2025-01-13 14:15:32,539 [INFO] Step[1850/2713]: training loss : 0.9333266270160675 TRAIN  loss dict:  {'classification_loss': 0.9333266270160675}
2025-01-13 14:15:45,491 [INFO] Step[1900/2713]: training loss : 0.935348562002182 TRAIN  loss dict:  {'classification_loss': 0.935348562002182}
2025-01-13 14:15:58,499 [INFO] Step[1950/2713]: training loss : 0.93310755610466 TRAIN  loss dict:  {'classification_loss': 0.93310755610466}
2025-01-13 14:16:11,522 [INFO] Step[2000/2713]: training loss : 0.9344190227985382 TRAIN  loss dict:  {'classification_loss': 0.9344190227985382}
2025-01-13 14:16:24,440 [INFO] Step[2050/2713]: training loss : 0.9323440301418304 TRAIN  loss dict:  {'classification_loss': 0.9323440301418304}
2025-01-13 14:16:37,342 [INFO] Step[2100/2713]: training loss : 0.9333313405513763 TRAIN  loss dict:  {'classification_loss': 0.9333313405513763}
2025-01-13 14:16:49,962 [INFO] Step[2150/2713]: training loss : 0.9330828201770782 TRAIN  loss dict:  {'classification_loss': 0.9330828201770782}
2025-01-13 14:17:02,503 [INFO] Step[2200/2713]: training loss : 0.9321608805656433 TRAIN  loss dict:  {'classification_loss': 0.9321608805656433}
2025-01-13 14:17:15,665 [INFO] Step[2250/2713]: training loss : 0.9329498660564423 TRAIN  loss dict:  {'classification_loss': 0.9329498660564423}
2025-01-13 14:17:28,859 [INFO] Step[2300/2713]: training loss : 0.9331983470916748 TRAIN  loss dict:  {'classification_loss': 0.9331983470916748}
2025-01-13 14:17:42,199 [INFO] Step[2350/2713]: training loss : 0.9340265548229217 TRAIN  loss dict:  {'classification_loss': 0.9340265548229217}
2025-01-13 14:17:54,866 [INFO] Step[2400/2713]: training loss : 0.9321105468273163 TRAIN  loss dict:  {'classification_loss': 0.9321105468273163}
2025-01-13 14:18:07,713 [INFO] Step[2450/2713]: training loss : 0.9316708087921143 TRAIN  loss dict:  {'classification_loss': 0.9316708087921143}
2025-01-13 14:18:21,277 [INFO] Step[2500/2713]: training loss : 0.932898952960968 TRAIN  loss dict:  {'classification_loss': 0.932898952960968}
2025-01-13 14:18:34,648 [INFO] Step[2550/2713]: training loss : 0.9322544550895691 TRAIN  loss dict:  {'classification_loss': 0.9322544550895691}
2025-01-13 14:18:47,267 [INFO] Step[2600/2713]: training loss : 0.940196408033371 TRAIN  loss dict:  {'classification_loss': 0.940196408033371}
2025-01-13 14:18:59,818 [INFO] Step[2650/2713]: training loss : 0.9341763603687286 TRAIN  loss dict:  {'classification_loss': 0.9341763603687286}
2025-01-13 14:19:12,879 [INFO] Step[2700/2713]: training loss : 0.9318223357200622 TRAIN  loss dict:  {'classification_loss': 0.9318223357200622}
2025-01-13 14:20:56,754 [INFO] Label accuracies statistics:
2025-01-13 14:20:56,754 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.5, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.5, 218: 1.0, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.25, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 1.0, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 0.75, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.5, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.5, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 1.0, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 14:20:56,756 [INFO] [112] TRAIN  loss: 0.9331780777090025 acc: 0.9998771347831429
2025-01-13 14:20:56,756 [INFO] [112] TRAIN  loss dict: {'classification_loss': 0.9331780777090025}
2025-01-13 14:20:56,756 [INFO] [112] VALIDATION loss: 1.6967426827527528 VALIDATION acc: 0.8294670846394985
2025-01-13 14:20:56,756 [INFO] [112] VALIDATION loss dict: {'classification_loss': 1.6967426827527528}
2025-01-13 14:20:56,756 [INFO] 
2025-01-13 14:21:15,438 [INFO] Step[50/2713]: training loss : 0.932743262052536 TRAIN  loss dict:  {'classification_loss': 0.932743262052536}
2025-01-13 14:21:28,503 [INFO] Step[100/2713]: training loss : 0.9322170662879944 TRAIN  loss dict:  {'classification_loss': 0.9322170662879944}
2025-01-13 14:21:41,023 [INFO] Step[150/2713]: training loss : 0.9328600490093231 TRAIN  loss dict:  {'classification_loss': 0.9328600490093231}
2025-01-13 14:21:53,678 [INFO] Step[200/2713]: training loss : 0.9327179598808288 TRAIN  loss dict:  {'classification_loss': 0.9327179598808288}
2025-01-13 14:22:06,849 [INFO] Step[250/2713]: training loss : 0.9339907848834992 TRAIN  loss dict:  {'classification_loss': 0.9339907848834992}
2025-01-13 14:22:19,621 [INFO] Step[300/2713]: training loss : 0.9333093822002411 TRAIN  loss dict:  {'classification_loss': 0.9333093822002411}
2025-01-13 14:22:32,872 [INFO] Step[350/2713]: training loss : 0.9326508271694184 TRAIN  loss dict:  {'classification_loss': 0.9326508271694184}
2025-01-13 14:22:46,219 [INFO] Step[400/2713]: training loss : 0.9326471948623657 TRAIN  loss dict:  {'classification_loss': 0.9326471948623657}
2025-01-13 14:22:59,515 [INFO] Step[450/2713]: training loss : 0.9328873372077942 TRAIN  loss dict:  {'classification_loss': 0.9328873372077942}
2025-01-13 14:23:12,512 [INFO] Step[500/2713]: training loss : 0.9340711081027985 TRAIN  loss dict:  {'classification_loss': 0.9340711081027985}
2025-01-13 14:23:25,712 [INFO] Step[550/2713]: training loss : 0.9322563660144806 TRAIN  loss dict:  {'classification_loss': 0.9322563660144806}
2025-01-13 14:23:39,214 [INFO] Step[600/2713]: training loss : 0.9330318105220795 TRAIN  loss dict:  {'classification_loss': 0.9330318105220795}
2025-01-13 14:23:52,449 [INFO] Step[650/2713]: training loss : 0.9313495016098022 TRAIN  loss dict:  {'classification_loss': 0.9313495016098022}
2025-01-13 14:24:05,887 [INFO] Step[700/2713]: training loss : 0.9321737694740295 TRAIN  loss dict:  {'classification_loss': 0.9321737694740295}
2025-01-13 14:24:19,726 [INFO] Step[750/2713]: training loss : 0.9337176775932312 TRAIN  loss dict:  {'classification_loss': 0.9337176775932312}
2025-01-13 14:24:33,505 [INFO] Step[800/2713]: training loss : 0.9327860176563263 TRAIN  loss dict:  {'classification_loss': 0.9327860176563263}
2025-01-13 14:24:46,637 [INFO] Step[850/2713]: training loss : 0.933127863407135 TRAIN  loss dict:  {'classification_loss': 0.933127863407135}
2025-01-13 14:24:59,192 [INFO] Step[900/2713]: training loss : 0.9319025814533234 TRAIN  loss dict:  {'classification_loss': 0.9319025814533234}
2025-01-13 14:25:12,054 [INFO] Step[950/2713]: training loss : 0.932559484243393 TRAIN  loss dict:  {'classification_loss': 0.932559484243393}
2025-01-13 14:25:25,236 [INFO] Step[1000/2713]: training loss : 0.931854487657547 TRAIN  loss dict:  {'classification_loss': 0.931854487657547}
2025-01-13 14:25:37,923 [INFO] Step[1050/2713]: training loss : 0.931781394481659 TRAIN  loss dict:  {'classification_loss': 0.931781394481659}
2025-01-13 14:25:51,222 [INFO] Step[1100/2713]: training loss : 0.9329759836196899 TRAIN  loss dict:  {'classification_loss': 0.9329759836196899}
2025-01-13 14:26:04,442 [INFO] Step[1150/2713]: training loss : 0.9322676920890808 TRAIN  loss dict:  {'classification_loss': 0.9322676920890808}
2025-01-13 14:26:17,330 [INFO] Step[1200/2713]: training loss : 0.9323208618164063 TRAIN  loss dict:  {'classification_loss': 0.9323208618164063}
2025-01-13 14:26:30,056 [INFO] Step[1250/2713]: training loss : 0.9335864877700806 TRAIN  loss dict:  {'classification_loss': 0.9335864877700806}
2025-01-13 14:26:42,827 [INFO] Step[1300/2713]: training loss : 0.9336383521556855 TRAIN  loss dict:  {'classification_loss': 0.9336383521556855}
2025-01-13 14:26:56,053 [INFO] Step[1350/2713]: training loss : 0.9330892014503479 TRAIN  loss dict:  {'classification_loss': 0.9330892014503479}
2025-01-13 14:27:08,706 [INFO] Step[1400/2713]: training loss : 0.9328367471694946 TRAIN  loss dict:  {'classification_loss': 0.9328367471694946}
2025-01-13 14:27:21,321 [INFO] Step[1450/2713]: training loss : 0.9316813552379608 TRAIN  loss dict:  {'classification_loss': 0.9316813552379608}
2025-01-13 14:27:33,958 [INFO] Step[1500/2713]: training loss : 0.9330143225193024 TRAIN  loss dict:  {'classification_loss': 0.9330143225193024}
2025-01-13 14:27:47,036 [INFO] Step[1550/2713]: training loss : 0.9343971908092499 TRAIN  loss dict:  {'classification_loss': 0.9343971908092499}
2025-01-13 14:27:59,681 [INFO] Step[1600/2713]: training loss : 0.9337292838096619 TRAIN  loss dict:  {'classification_loss': 0.9337292838096619}
2025-01-13 14:28:12,777 [INFO] Step[1650/2713]: training loss : 0.9322593784332276 TRAIN  loss dict:  {'classification_loss': 0.9322593784332276}
2025-01-13 14:28:25,927 [INFO] Step[1700/2713]: training loss : 0.9328455483913421 TRAIN  loss dict:  {'classification_loss': 0.9328455483913421}
2025-01-13 14:28:39,170 [INFO] Step[1750/2713]: training loss : 0.9405835270881653 TRAIN  loss dict:  {'classification_loss': 0.9405835270881653}
2025-01-13 14:28:52,418 [INFO] Step[1800/2713]: training loss : 0.9322886097431183 TRAIN  loss dict:  {'classification_loss': 0.9322886097431183}
2025-01-13 14:29:05,177 [INFO] Step[1850/2713]: training loss : 0.9361943852901459 TRAIN  loss dict:  {'classification_loss': 0.9361943852901459}
2025-01-13 14:29:18,007 [INFO] Step[1900/2713]: training loss : 0.9322360825538635 TRAIN  loss dict:  {'classification_loss': 0.9322360825538635}
2025-01-13 14:29:31,232 [INFO] Step[1950/2713]: training loss : 0.9335149705410004 TRAIN  loss dict:  {'classification_loss': 0.9335149705410004}
2025-01-13 14:29:43,812 [INFO] Step[2000/2713]: training loss : 0.9313878607749939 TRAIN  loss dict:  {'classification_loss': 0.9313878607749939}
2025-01-13 14:29:56,372 [INFO] Step[2050/2713]: training loss : 0.9318730235099792 TRAIN  loss dict:  {'classification_loss': 0.9318730235099792}
2025-01-13 14:30:09,283 [INFO] Step[2100/2713]: training loss : 0.932670179605484 TRAIN  loss dict:  {'classification_loss': 0.932670179605484}
2025-01-13 14:30:22,047 [INFO] Step[2150/2713]: training loss : 0.9329900670051575 TRAIN  loss dict:  {'classification_loss': 0.9329900670051575}
2025-01-13 14:30:35,059 [INFO] Step[2200/2713]: training loss : 0.9326923394203186 TRAIN  loss dict:  {'classification_loss': 0.9326923394203186}
2025-01-13 14:30:47,902 [INFO] Step[2250/2713]: training loss : 0.9321004390716553 TRAIN  loss dict:  {'classification_loss': 0.9321004390716553}
2025-01-13 14:31:01,147 [INFO] Step[2300/2713]: training loss : 0.9330471909046173 TRAIN  loss dict:  {'classification_loss': 0.9330471909046173}
2025-01-13 14:31:14,437 [INFO] Step[2350/2713]: training loss : 0.9329995465278625 TRAIN  loss dict:  {'classification_loss': 0.9329995465278625}
2025-01-13 14:31:27,657 [INFO] Step[2400/2713]: training loss : 0.9326519596576691 TRAIN  loss dict:  {'classification_loss': 0.9326519596576691}
2025-01-13 14:31:40,850 [INFO] Step[2450/2713]: training loss : 0.9334430623054505 TRAIN  loss dict:  {'classification_loss': 0.9334430623054505}
2025-01-13 14:31:53,720 [INFO] Step[2500/2713]: training loss : 0.9373993015289307 TRAIN  loss dict:  {'classification_loss': 0.9373993015289307}
2025-01-13 14:32:06,327 [INFO] Step[2550/2713]: training loss : 0.9323019731044769 TRAIN  loss dict:  {'classification_loss': 0.9323019731044769}
2025-01-13 14:32:19,218 [INFO] Step[2600/2713]: training loss : 0.9333262872695923 TRAIN  loss dict:  {'classification_loss': 0.9333262872695923}
2025-01-13 14:32:31,835 [INFO] Step[2650/2713]: training loss : 0.9327006435394287 TRAIN  loss dict:  {'classification_loss': 0.9327006435394287}
2025-01-13 14:32:44,825 [INFO] Step[2700/2713]: training loss : 0.934822062253952 TRAIN  loss dict:  {'classification_loss': 0.934822062253952}
2025-01-13 14:34:28,116 [INFO] Label accuracies statistics:
2025-01-13 14:34:28,116 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.5, 64: 0.75, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 0.75, 168: 0.75, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.5, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.25, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.5, 279: 1.0, 280: 1.0, 281: 0.75, 282: 1.0, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 0.75, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 14:34:28,118 [INFO] [113] TRAIN  loss: 0.9330800431397759 acc: 1.0
2025-01-13 14:34:28,118 [INFO] [113] TRAIN  loss dict: {'classification_loss': 0.9330800431397759}
2025-01-13 14:34:28,118 [INFO] [113] VALIDATION loss: 1.7214849842222113 VALIDATION acc: 0.8225705329153605
2025-01-13 14:34:28,118 [INFO] [113] VALIDATION loss dict: {'classification_loss': 1.7214849842222113}
2025-01-13 14:34:28,119 [INFO] 
2025-01-13 14:34:47,604 [INFO] Step[50/2713]: training loss : 0.9330053842067718 TRAIN  loss dict:  {'classification_loss': 0.9330053842067718}
2025-01-13 14:35:00,118 [INFO] Step[100/2713]: training loss : 0.9344234919548035 TRAIN  loss dict:  {'classification_loss': 0.9344234919548035}
2025-01-13 14:35:12,650 [INFO] Step[150/2713]: training loss : 0.9334870231151581 TRAIN  loss dict:  {'classification_loss': 0.9334870231151581}
2025-01-13 14:35:25,288 [INFO] Step[200/2713]: training loss : 0.9330226898193359 TRAIN  loss dict:  {'classification_loss': 0.9330226898193359}
2025-01-13 14:35:38,255 [INFO] Step[250/2713]: training loss : 0.9342731177806854 TRAIN  loss dict:  {'classification_loss': 0.9342731177806854}
2025-01-13 14:35:51,136 [INFO] Step[300/2713]: training loss : 0.9328264820575715 TRAIN  loss dict:  {'classification_loss': 0.9328264820575715}
2025-01-13 14:36:03,678 [INFO] Step[350/2713]: training loss : 0.9334231698513031 TRAIN  loss dict:  {'classification_loss': 0.9334231698513031}
2025-01-13 14:36:16,232 [INFO] Step[400/2713]: training loss : 0.9334559297561645 TRAIN  loss dict:  {'classification_loss': 0.9334559297561645}
2025-01-13 14:36:29,195 [INFO] Step[450/2713]: training loss : 0.9319813859462738 TRAIN  loss dict:  {'classification_loss': 0.9319813859462738}
2025-01-13 14:36:42,174 [INFO] Step[500/2713]: training loss : 0.9330175364017487 TRAIN  loss dict:  {'classification_loss': 0.9330175364017487}
2025-01-13 14:36:55,433 [INFO] Step[550/2713]: training loss : 0.933049920797348 TRAIN  loss dict:  {'classification_loss': 0.933049920797348}
2025-01-13 14:37:08,409 [INFO] Step[600/2713]: training loss : 0.9315093100070954 TRAIN  loss dict:  {'classification_loss': 0.9315093100070954}
2025-01-13 14:37:21,265 [INFO] Step[650/2713]: training loss : 0.9316687083244324 TRAIN  loss dict:  {'classification_loss': 0.9316687083244324}
2025-01-13 14:37:34,150 [INFO] Step[700/2713]: training loss : 0.9328501808643341 TRAIN  loss dict:  {'classification_loss': 0.9328501808643341}
2025-01-13 14:37:47,287 [INFO] Step[750/2713]: training loss : 0.9323681855201721 TRAIN  loss dict:  {'classification_loss': 0.9323681855201721}
2025-01-13 14:38:00,202 [INFO] Step[800/2713]: training loss : 0.9338098311424256 TRAIN  loss dict:  {'classification_loss': 0.9338098311424256}
2025-01-13 14:38:12,808 [INFO] Step[850/2713]: training loss : 0.9322255039215088 TRAIN  loss dict:  {'classification_loss': 0.9322255039215088}
2025-01-13 14:38:25,348 [INFO] Step[900/2713]: training loss : 0.9337700629234313 TRAIN  loss dict:  {'classification_loss': 0.9337700629234313}
2025-01-13 14:38:37,907 [INFO] Step[950/2713]: training loss : 0.9354985415935516 TRAIN  loss dict:  {'classification_loss': 0.9354985415935516}
2025-01-13 14:38:50,408 [INFO] Step[1000/2713]: training loss : 0.9339458608627319 TRAIN  loss dict:  {'classification_loss': 0.9339458608627319}
2025-01-13 14:39:03,382 [INFO] Step[1050/2713]: training loss : 0.9319796395301819 TRAIN  loss dict:  {'classification_loss': 0.9319796395301819}
2025-01-13 14:39:16,265 [INFO] Step[1100/2713]: training loss : 0.9308242464065551 TRAIN  loss dict:  {'classification_loss': 0.9308242464065551}
2025-01-13 14:39:28,797 [INFO] Step[1150/2713]: training loss : 0.931712715625763 TRAIN  loss dict:  {'classification_loss': 0.931712715625763}
2025-01-13 14:39:41,354 [INFO] Step[1200/2713]: training loss : 0.9332364523410797 TRAIN  loss dict:  {'classification_loss': 0.9332364523410797}
2025-01-13 14:39:53,888 [INFO] Step[1250/2713]: training loss : 0.9322457396984101 TRAIN  loss dict:  {'classification_loss': 0.9322457396984101}
2025-01-13 14:40:06,446 [INFO] Step[1300/2713]: training loss : 0.932319369316101 TRAIN  loss dict:  {'classification_loss': 0.932319369316101}
2025-01-13 14:40:19,213 [INFO] Step[1350/2713]: training loss : 0.9318146312236786 TRAIN  loss dict:  {'classification_loss': 0.9318146312236786}
2025-01-13 14:40:32,385 [INFO] Step[1400/2713]: training loss : 0.9322805452346802 TRAIN  loss dict:  {'classification_loss': 0.9322805452346802}
2025-01-13 14:40:45,646 [INFO] Step[1450/2713]: training loss : 0.9334098553657532 TRAIN  loss dict:  {'classification_loss': 0.9334098553657532}
2025-01-13 14:40:58,917 [INFO] Step[1500/2713]: training loss : 0.9333280634880066 TRAIN  loss dict:  {'classification_loss': 0.9333280634880066}
2025-01-13 14:41:11,577 [INFO] Step[1550/2713]: training loss : 0.9340760970115661 TRAIN  loss dict:  {'classification_loss': 0.9340760970115661}
2025-01-13 14:41:24,095 [INFO] Step[1600/2713]: training loss : 0.9313039958477021 TRAIN  loss dict:  {'classification_loss': 0.9313039958477021}
2025-01-13 14:41:37,086 [INFO] Step[1650/2713]: training loss : 0.9336578154563904 TRAIN  loss dict:  {'classification_loss': 0.9336578154563904}
2025-01-13 14:41:50,063 [INFO] Step[1700/2713]: training loss : 0.9329403829574585 TRAIN  loss dict:  {'classification_loss': 0.9329403829574585}
2025-01-13 14:42:03,175 [INFO] Step[1750/2713]: training loss : 0.9322678244113922 TRAIN  loss dict:  {'classification_loss': 0.9322678244113922}
2025-01-13 14:42:15,712 [INFO] Step[1800/2713]: training loss : 0.9347931110858917 TRAIN  loss dict:  {'classification_loss': 0.9347931110858917}
2025-01-13 14:42:28,343 [INFO] Step[1850/2713]: training loss : 0.9355996882915497 TRAIN  loss dict:  {'classification_loss': 0.9355996882915497}
2025-01-13 14:42:41,148 [INFO] Step[1900/2713]: training loss : 0.9335532593727112 TRAIN  loss dict:  {'classification_loss': 0.9335532593727112}
2025-01-13 14:42:53,675 [INFO] Step[1950/2713]: training loss : 0.9319925844669342 TRAIN  loss dict:  {'classification_loss': 0.9319925844669342}
2025-01-13 14:43:06,871 [INFO] Step[2000/2713]: training loss : 0.9321833181381226 TRAIN  loss dict:  {'classification_loss': 0.9321833181381226}
2025-01-13 14:43:20,068 [INFO] Step[2050/2713]: training loss : 0.9336313378810882 TRAIN  loss dict:  {'classification_loss': 0.9336313378810882}
2025-01-13 14:43:33,343 [INFO] Step[2100/2713]: training loss : 0.9345121502876281 TRAIN  loss dict:  {'classification_loss': 0.9345121502876281}
2025-01-13 14:43:46,267 [INFO] Step[2150/2713]: training loss : 0.9318092095851899 TRAIN  loss dict:  {'classification_loss': 0.9318092095851899}
2025-01-13 14:43:59,458 [INFO] Step[2200/2713]: training loss : 0.9357927370071412 TRAIN  loss dict:  {'classification_loss': 0.9357927370071412}
2025-01-13 14:44:12,593 [INFO] Step[2250/2713]: training loss : 0.9327191889286042 TRAIN  loss dict:  {'classification_loss': 0.9327191889286042}
2025-01-13 14:44:25,382 [INFO] Step[2300/2713]: training loss : 0.9319871640205384 TRAIN  loss dict:  {'classification_loss': 0.9319871640205384}
2025-01-13 14:44:38,601 [INFO] Step[2350/2713]: training loss : 0.9327276313304901 TRAIN  loss dict:  {'classification_loss': 0.9327276313304901}
2025-01-13 14:44:51,655 [INFO] Step[2400/2713]: training loss : 0.9323327326774598 TRAIN  loss dict:  {'classification_loss': 0.9323327326774598}
2025-01-13 14:45:04,279 [INFO] Step[2450/2713]: training loss : 0.9313248729705811 TRAIN  loss dict:  {'classification_loss': 0.9313248729705811}
2025-01-13 14:45:17,189 [INFO] Step[2500/2713]: training loss : 0.9334362006187439 TRAIN  loss dict:  {'classification_loss': 0.9334362006187439}
2025-01-13 14:45:29,944 [INFO] Step[2550/2713]: training loss : 0.9350578224658966 TRAIN  loss dict:  {'classification_loss': 0.9350578224658966}
2025-01-13 14:45:43,238 [INFO] Step[2600/2713]: training loss : 0.9338341128826141 TRAIN  loss dict:  {'classification_loss': 0.9338341128826141}
2025-01-13 14:45:56,485 [INFO] Step[2650/2713]: training loss : 0.9326150691509247 TRAIN  loss dict:  {'classification_loss': 0.9326150691509247}
2025-01-13 14:46:09,303 [INFO] Step[2700/2713]: training loss : 0.9338113856315613 TRAIN  loss dict:  {'classification_loss': 0.9338113856315613}
2025-01-13 14:48:16,924 [INFO] Label accuracies statistics:
2025-01-13 14:48:16,924 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.5, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.5, 61: 0.75, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 0.75, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 0.75, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 0.75, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 1.0, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.75, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 1.0, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 0.75, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 14:48:16,927 [INFO] [114] TRAIN  loss: 0.9330493199882409 acc: 1.0
2025-01-13 14:48:16,927 [INFO] [114] TRAIN  loss dict: {'classification_loss': 0.9330493199882409}
2025-01-13 14:48:16,927 [INFO] [114] VALIDATION loss: 1.6907003814340533 VALIDATION acc: 0.8344827586206897
2025-01-13 14:48:16,927 [INFO] [114] VALIDATION loss dict: {'classification_loss': 1.6907003814340533}
2025-01-13 14:48:16,927 [INFO] 
2025-01-13 14:48:35,883 [INFO] Step[50/2713]: training loss : 0.9329542386531829 TRAIN  loss dict:  {'classification_loss': 0.9329542386531829}
2025-01-13 14:48:48,340 [INFO] Step[100/2713]: training loss : 0.9334833109378815 TRAIN  loss dict:  {'classification_loss': 0.9334833109378815}
2025-01-13 14:49:01,164 [INFO] Step[150/2713]: training loss : 0.9341635012626648 TRAIN  loss dict:  {'classification_loss': 0.9341635012626648}
2025-01-13 14:49:14,352 [INFO] Step[200/2713]: training loss : 0.9335383749008179 TRAIN  loss dict:  {'classification_loss': 0.9335383749008179}
2025-01-13 14:49:27,180 [INFO] Step[250/2713]: training loss : 0.932447030544281 TRAIN  loss dict:  {'classification_loss': 0.932447030544281}
2025-01-13 14:49:40,166 [INFO] Step[300/2713]: training loss : 0.9340790975093841 TRAIN  loss dict:  {'classification_loss': 0.9340790975093841}
2025-01-13 14:49:53,157 [INFO] Step[350/2713]: training loss : 0.9340451037883759 TRAIN  loss dict:  {'classification_loss': 0.9340451037883759}
2025-01-13 14:50:06,349 [INFO] Step[400/2713]: training loss : 0.9326351070404053 TRAIN  loss dict:  {'classification_loss': 0.9326351070404053}
2025-01-13 14:50:19,538 [INFO] Step[450/2713]: training loss : 0.9345521450042724 TRAIN  loss dict:  {'classification_loss': 0.9345521450042724}
2025-01-13 14:50:32,551 [INFO] Step[500/2713]: training loss : 0.9330194187164307 TRAIN  loss dict:  {'classification_loss': 0.9330194187164307}
2025-01-13 14:50:45,128 [INFO] Step[550/2713]: training loss : 0.93207111120224 TRAIN  loss dict:  {'classification_loss': 0.93207111120224}
2025-01-13 14:50:57,632 [INFO] Step[600/2713]: training loss : 0.9318195343017578 TRAIN  loss dict:  {'classification_loss': 0.9318195343017578}
2025-01-13 14:51:10,172 [INFO] Step[650/2713]: training loss : 0.93271413564682 TRAIN  loss dict:  {'classification_loss': 0.93271413564682}
2025-01-13 14:51:23,417 [INFO] Step[700/2713]: training loss : 0.9326180422306061 TRAIN  loss dict:  {'classification_loss': 0.9326180422306061}
2025-01-13 14:51:36,250 [INFO] Step[750/2713]: training loss : 0.9329265511035919 TRAIN  loss dict:  {'classification_loss': 0.9329265511035919}
2025-01-13 14:51:48,914 [INFO] Step[800/2713]: training loss : 0.9329072248935699 TRAIN  loss dict:  {'classification_loss': 0.9329072248935699}
2025-01-13 14:52:01,503 [INFO] Step[850/2713]: training loss : 0.9333888971805573 TRAIN  loss dict:  {'classification_loss': 0.9333888971805573}
2025-01-13 14:52:14,484 [INFO] Step[900/2713]: training loss : 0.9335814261436463 TRAIN  loss dict:  {'classification_loss': 0.9335814261436463}
2025-01-13 14:52:27,413 [INFO] Step[950/2713]: training loss : 0.9355919206142426 TRAIN  loss dict:  {'classification_loss': 0.9355919206142426}
2025-01-13 14:52:40,595 [INFO] Step[1000/2713]: training loss : 0.9318007731437683 TRAIN  loss dict:  {'classification_loss': 0.9318007731437683}
2025-01-13 14:52:53,188 [INFO] Step[1050/2713]: training loss : 0.9331276273727417 TRAIN  loss dict:  {'classification_loss': 0.9331276273727417}
2025-01-13 14:53:05,707 [INFO] Step[1100/2713]: training loss : 0.9325052618980407 TRAIN  loss dict:  {'classification_loss': 0.9325052618980407}
2025-01-13 14:53:18,774 [INFO] Step[1150/2713]: training loss : 0.9331339263916015 TRAIN  loss dict:  {'classification_loss': 0.9331339263916015}
2025-01-13 14:53:31,317 [INFO] Step[1200/2713]: training loss : 0.9344147574901581 TRAIN  loss dict:  {'classification_loss': 0.9344147574901581}
2025-01-13 14:53:43,846 [INFO] Step[1250/2713]: training loss : 0.9320614326000214 TRAIN  loss dict:  {'classification_loss': 0.9320614326000214}
2025-01-13 14:53:56,675 [INFO] Step[1300/2713]: training loss : 0.9386459136009216 TRAIN  loss dict:  {'classification_loss': 0.9386459136009216}
2025-01-13 14:54:10,346 [INFO] Step[1350/2713]: training loss : 0.9331265223026276 TRAIN  loss dict:  {'classification_loss': 0.9331265223026276}
2025-01-13 14:54:25,401 [INFO] Step[1400/2713]: training loss : 0.9321999561786651 TRAIN  loss dict:  {'classification_loss': 0.9321999561786651}
2025-01-13 14:54:40,455 [INFO] Step[1450/2713]: training loss : 0.9327817273139953 TRAIN  loss dict:  {'classification_loss': 0.9327817273139953}
2025-01-13 14:54:53,761 [INFO] Step[1500/2713]: training loss : 0.9324811899662018 TRAIN  loss dict:  {'classification_loss': 0.9324811899662018}
2025-01-13 14:55:08,670 [INFO] Step[1550/2713]: training loss : 0.9326133346557617 TRAIN  loss dict:  {'classification_loss': 0.9326133346557617}
2025-01-13 14:55:21,459 [INFO] Step[1600/2713]: training loss : 0.9324219560623169 TRAIN  loss dict:  {'classification_loss': 0.9324219560623169}
2025-01-13 14:55:34,290 [INFO] Step[1650/2713]: training loss : 0.9317976653575897 TRAIN  loss dict:  {'classification_loss': 0.9317976653575897}
2025-01-13 14:55:47,364 [INFO] Step[1700/2713]: training loss : 0.9346537029743195 TRAIN  loss dict:  {'classification_loss': 0.9346537029743195}
2025-01-13 14:55:59,940 [INFO] Step[1750/2713]: training loss : 0.9333669567108154 TRAIN  loss dict:  {'classification_loss': 0.9333669567108154}
2025-01-13 14:56:12,782 [INFO] Step[1800/2713]: training loss : 0.9332523083686829 TRAIN  loss dict:  {'classification_loss': 0.9332523083686829}
2025-01-13 14:56:25,359 [INFO] Step[1850/2713]: training loss : 0.9329454290866852 TRAIN  loss dict:  {'classification_loss': 0.9329454290866852}
2025-01-13 14:56:37,895 [INFO] Step[1900/2713]: training loss : 0.933771071434021 TRAIN  loss dict:  {'classification_loss': 0.933771071434021}
2025-01-13 14:56:50,419 [INFO] Step[1950/2713]: training loss : 0.9329278826713562 TRAIN  loss dict:  {'classification_loss': 0.9329278826713562}
2025-01-13 14:57:02,939 [INFO] Step[2000/2713]: training loss : 0.9321902179718018 TRAIN  loss dict:  {'classification_loss': 0.9321902179718018}
2025-01-13 14:57:15,460 [INFO] Step[2050/2713]: training loss : 0.9328370404243469 TRAIN  loss dict:  {'classification_loss': 0.9328370404243469}
2025-01-13 14:57:28,570 [INFO] Step[2100/2713]: training loss : 0.9317504620552063 TRAIN  loss dict:  {'classification_loss': 0.9317504620552063}
2025-01-13 14:57:41,178 [INFO] Step[2150/2713]: training loss : 0.931908848285675 TRAIN  loss dict:  {'classification_loss': 0.931908848285675}
2025-01-13 14:57:54,145 [INFO] Step[2200/2713]: training loss : 0.9316859865188598 TRAIN  loss dict:  {'classification_loss': 0.9316859865188598}
2025-01-13 14:58:07,298 [INFO] Step[2250/2713]: training loss : 0.9324754798412322 TRAIN  loss dict:  {'classification_loss': 0.9324754798412322}
2025-01-13 14:58:20,013 [INFO] Step[2300/2713]: training loss : 0.9333675134181977 TRAIN  loss dict:  {'classification_loss': 0.9333675134181977}
2025-01-13 14:58:33,210 [INFO] Step[2350/2713]: training loss : 0.9326536154747009 TRAIN  loss dict:  {'classification_loss': 0.9326536154747009}
2025-01-13 14:58:46,267 [INFO] Step[2400/2713]: training loss : 0.9328844618797302 TRAIN  loss dict:  {'classification_loss': 0.9328844618797302}
2025-01-13 14:58:59,505 [INFO] Step[2450/2713]: training loss : 0.9330331039428711 TRAIN  loss dict:  {'classification_loss': 0.9330331039428711}
2025-01-13 14:59:12,706 [INFO] Step[2500/2713]: training loss : 0.9329957830905914 TRAIN  loss dict:  {'classification_loss': 0.9329957830905914}
2025-01-13 14:59:25,551 [INFO] Step[2550/2713]: training loss : 0.9324719417095184 TRAIN  loss dict:  {'classification_loss': 0.9324719417095184}
2025-01-13 14:59:38,635 [INFO] Step[2600/2713]: training loss : 0.9328331959247589 TRAIN  loss dict:  {'classification_loss': 0.9328331959247589}
2025-01-13 14:59:51,974 [INFO] Step[2650/2713]: training loss : 0.9332332956790924 TRAIN  loss dict:  {'classification_loss': 0.9332332956790924}
2025-01-13 15:00:04,828 [INFO] Step[2700/2713]: training loss : 0.9346802449226379 TRAIN  loss dict:  {'classification_loss': 0.9346802449226379}
2025-01-13 15:01:51,155 [INFO] Label accuracies statistics:
2025-01-13 15:01:51,155 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.75, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 0.75, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 0.75, 260: 0.5, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 1.0, 283: 0.75, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 0.75, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.0, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 0.75, 318: 0.75, 319: 0.75, 320: 1.0, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 15:01:51,157 [INFO] [115] TRAIN  loss: 0.9331110960392331 acc: 0.9998771347831429
2025-01-13 15:01:51,157 [INFO] [115] TRAIN  loss dict: {'classification_loss': 0.9331110960392331}
2025-01-13 15:01:51,157 [INFO] [115] VALIDATION loss: 1.697785893441143 VALIDATION acc: 0.8357366771159874
2025-01-13 15:01:51,157 [INFO] [115] VALIDATION loss dict: {'classification_loss': 1.697785893441143}
2025-01-13 15:01:51,157 [INFO] 
2025-01-13 15:02:09,842 [INFO] Step[50/2713]: training loss : 0.9340380430221558 TRAIN  loss dict:  {'classification_loss': 0.9340380430221558}
2025-01-13 15:02:23,375 [INFO] Step[100/2713]: training loss : 0.9332846140861512 TRAIN  loss dict:  {'classification_loss': 0.9332846140861512}
2025-01-13 15:02:40,630 [INFO] Step[150/2713]: training loss : 0.9319608008861542 TRAIN  loss dict:  {'classification_loss': 0.9319608008861542}
2025-01-13 15:02:59,615 [INFO] Step[200/2713]: training loss : 0.9340110325813293 TRAIN  loss dict:  {'classification_loss': 0.9340110325813293}
2025-01-13 15:03:20,724 [INFO] Step[250/2713]: training loss : 0.932208640575409 TRAIN  loss dict:  {'classification_loss': 0.932208640575409}
2025-01-13 15:03:44,990 [INFO] Step[300/2713]: training loss : 0.9325863456726075 TRAIN  loss dict:  {'classification_loss': 0.9325863456726075}
2025-01-13 15:04:06,944 [INFO] Step[350/2713]: training loss : 0.9332107245922089 TRAIN  loss dict:  {'classification_loss': 0.9332107245922089}
2025-01-13 15:04:26,323 [INFO] Step[400/2713]: training loss : 0.931868611574173 TRAIN  loss dict:  {'classification_loss': 0.931868611574173}
2025-01-13 15:04:46,211 [INFO] Step[450/2713]: training loss : 0.932355271577835 TRAIN  loss dict:  {'classification_loss': 0.932355271577835}
2025-01-13 15:05:08,513 [INFO] Step[500/2713]: training loss : 0.9392437303066253 TRAIN  loss dict:  {'classification_loss': 0.9392437303066253}
2025-01-13 15:05:25,550 [INFO] Step[550/2713]: training loss : 0.9329267978668213 TRAIN  loss dict:  {'classification_loss': 0.9329267978668213}
2025-01-13 15:05:43,522 [INFO] Step[600/2713]: training loss : 0.9327946603298187 TRAIN  loss dict:  {'classification_loss': 0.9327946603298187}
2025-01-13 15:06:02,320 [INFO] Step[650/2713]: training loss : 0.9333073413372039 TRAIN  loss dict:  {'classification_loss': 0.9333073413372039}
2025-01-13 15:06:23,856 [INFO] Step[700/2713]: training loss : 0.9332046461105347 TRAIN  loss dict:  {'classification_loss': 0.9332046461105347}
2025-01-13 15:06:47,562 [INFO] Step[750/2713]: training loss : 0.9317283570766449 TRAIN  loss dict:  {'classification_loss': 0.9317283570766449}
2025-01-13 15:07:08,135 [INFO] Step[800/2713]: training loss : 0.9321047019958496 TRAIN  loss dict:  {'classification_loss': 0.9321047019958496}
2025-01-13 15:07:26,585 [INFO] Step[850/2713]: training loss : 0.9321055698394776 TRAIN  loss dict:  {'classification_loss': 0.9321055698394776}
2025-01-13 15:07:48,042 [INFO] Step[900/2713]: training loss : 0.9325751328468322 TRAIN  loss dict:  {'classification_loss': 0.9325751328468322}
2025-01-13 15:08:08,645 [INFO] Step[950/2713]: training loss : 0.932352260351181 TRAIN  loss dict:  {'classification_loss': 0.932352260351181}
2025-01-13 15:08:24,917 [INFO] Step[1000/2713]: training loss : 0.9327501237392426 TRAIN  loss dict:  {'classification_loss': 0.9327501237392426}
2025-01-13 15:08:44,242 [INFO] Step[1050/2713]: training loss : 0.9328243780136108 TRAIN  loss dict:  {'classification_loss': 0.9328243780136108}
2025-01-13 15:09:03,830 [INFO] Step[1100/2713]: training loss : 0.932489218711853 TRAIN  loss dict:  {'classification_loss': 0.932489218711853}
2025-01-13 15:09:23,958 [INFO] Step[1150/2713]: training loss : 0.9333257818222046 TRAIN  loss dict:  {'classification_loss': 0.9333257818222046}
2025-01-13 15:09:46,040 [INFO] Step[1200/2713]: training loss : 0.9315839445590973 TRAIN  loss dict:  {'classification_loss': 0.9315839445590973}
2025-01-13 15:10:07,108 [INFO] Step[1250/2713]: training loss : 0.9321597814559937 TRAIN  loss dict:  {'classification_loss': 0.9321597814559937}
2025-01-13 15:10:25,821 [INFO] Step[1300/2713]: training loss : 0.9323471546173095 TRAIN  loss dict:  {'classification_loss': 0.9323471546173095}
2025-01-13 15:10:44,488 [INFO] Step[1350/2713]: training loss : 0.9324415850639344 TRAIN  loss dict:  {'classification_loss': 0.9324415850639344}
2025-01-13 15:11:05,563 [INFO] Step[1400/2713]: training loss : 0.9327906155586243 TRAIN  loss dict:  {'classification_loss': 0.9327906155586243}
2025-01-13 15:11:24,178 [INFO] Step[1450/2713]: training loss : 0.9339797639846802 TRAIN  loss dict:  {'classification_loss': 0.9339797639846802}
2025-01-13 15:11:39,381 [INFO] Step[1500/2713]: training loss : 0.9324361526966095 TRAIN  loss dict:  {'classification_loss': 0.9324361526966095}
2025-01-13 15:11:57,518 [INFO] Step[1550/2713]: training loss : 0.932495367527008 TRAIN  loss dict:  {'classification_loss': 0.932495367527008}
2025-01-13 15:12:14,996 [INFO] Step[1600/2713]: training loss : 0.932130275964737 TRAIN  loss dict:  {'classification_loss': 0.932130275964737}
2025-01-13 15:12:34,542 [INFO] Step[1650/2713]: training loss : 0.9338742351531982 TRAIN  loss dict:  {'classification_loss': 0.9338742351531982}
2025-01-13 15:12:59,700 [INFO] Step[1700/2713]: training loss : 0.9349570369720459 TRAIN  loss dict:  {'classification_loss': 0.9349570369720459}
2025-01-13 15:13:20,578 [INFO] Step[1750/2713]: training loss : 0.9343860757350921 TRAIN  loss dict:  {'classification_loss': 0.9343860757350921}
2025-01-13 15:13:40,799 [INFO] Step[1800/2713]: training loss : 0.932062977552414 TRAIN  loss dict:  {'classification_loss': 0.932062977552414}
2025-01-13 15:14:00,904 [INFO] Step[1850/2713]: training loss : 0.9333806371688843 TRAIN  loss dict:  {'classification_loss': 0.9333806371688843}
2025-01-13 15:14:22,064 [INFO] Step[1900/2713]: training loss : 0.9327710604667664 TRAIN  loss dict:  {'classification_loss': 0.9327710604667664}
2025-01-13 15:14:38,973 [INFO] Step[1950/2713]: training loss : 0.9321141624450684 TRAIN  loss dict:  {'classification_loss': 0.9321141624450684}
2025-01-13 15:14:57,451 [INFO] Step[2000/2713]: training loss : 0.9325560331344604 TRAIN  loss dict:  {'classification_loss': 0.9325560331344604}
2025-01-13 15:15:15,952 [INFO] Step[2050/2713]: training loss : 0.9340778040885925 TRAIN  loss dict:  {'classification_loss': 0.9340778040885925}
2025-01-13 15:15:37,199 [INFO] Step[2100/2713]: training loss : 0.9316688323020935 TRAIN  loss dict:  {'classification_loss': 0.9316688323020935}
2025-01-13 15:16:00,930 [INFO] Step[2150/2713]: training loss : 0.932608871459961 TRAIN  loss dict:  {'classification_loss': 0.932608871459961}
2025-01-13 15:16:23,758 [INFO] Step[2200/2713]: training loss : 0.9328617441654206 TRAIN  loss dict:  {'classification_loss': 0.9328617441654206}
2025-01-13 15:16:42,044 [INFO] Step[2250/2713]: training loss : 0.9327470993995667 TRAIN  loss dict:  {'classification_loss': 0.9327470993995667}
2025-01-13 15:17:04,331 [INFO] Step[2300/2713]: training loss : 0.9361013031005859 TRAIN  loss dict:  {'classification_loss': 0.9361013031005859}
2025-01-13 15:17:24,230 [INFO] Step[2350/2713]: training loss : 0.9331957685947418 TRAIN  loss dict:  {'classification_loss': 0.9331957685947418}
2025-01-13 15:17:39,695 [INFO] Step[2400/2713]: training loss : 0.9347732675075531 TRAIN  loss dict:  {'classification_loss': 0.9347732675075531}
2025-01-13 15:17:57,980 [INFO] Step[2450/2713]: training loss : 0.9325903069972992 TRAIN  loss dict:  {'classification_loss': 0.9325903069972992}
2025-01-13 15:18:17,532 [INFO] Step[2500/2713]: training loss : 0.9327498435974121 TRAIN  loss dict:  {'classification_loss': 0.9327498435974121}
2025-01-13 15:18:40,532 [INFO] Step[2550/2713]: training loss : 0.9330145740509033 TRAIN  loss dict:  {'classification_loss': 0.9330145740509033}
2025-01-13 15:19:03,617 [INFO] Step[2600/2713]: training loss : 0.9327826178073884 TRAIN  loss dict:  {'classification_loss': 0.9327826178073884}
2025-01-13 15:19:24,827 [INFO] Step[2650/2713]: training loss : 0.9320105230808258 TRAIN  loss dict:  {'classification_loss': 0.9320105230808258}
2025-01-13 15:19:44,968 [INFO] Step[2700/2713]: training loss : 0.931646386384964 TRAIN  loss dict:  {'classification_loss': 0.931646386384964}
2025-01-13 15:22:35,895 [INFO] Label accuracies statistics:
2025-01-13 15:22:35,895 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 0.75, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 0.75, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.5, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 1.0, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.5, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.75, 218: 0.75, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 1.0, 236: 0.75, 237: 0.5, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.5, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 0.5, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 0.75, 349: 0.75, 350: 0.75, 351: 0.75, 352: 0.5, 353: 0.25, 354: 0.5, 355: 1.0, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 1.0, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 0.75, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 15:22:35,897 [INFO] [116] TRAIN  loss: 0.9329850350057248 acc: 1.0
2025-01-13 15:22:35,897 [INFO] [116] TRAIN  loss dict: {'classification_loss': 0.9329850350057248}
2025-01-13 15:22:35,898 [INFO] [116] VALIDATION loss: 1.6939901148242162 VALIDATION acc: 0.831974921630094
2025-01-13 15:22:35,898 [INFO] [116] VALIDATION loss dict: {'classification_loss': 1.6939901148242162}
2025-01-13 15:22:35,898 [INFO] 
2025-01-13 15:23:06,884 [INFO] Step[50/2713]: training loss : 0.9314346957206726 TRAIN  loss dict:  {'classification_loss': 0.9314346957206726}
2025-01-13 15:23:29,937 [INFO] Step[100/2713]: training loss : 0.9327017331123352 TRAIN  loss dict:  {'classification_loss': 0.9327017331123352}
2025-01-13 15:23:47,417 [INFO] Step[150/2713]: training loss : 0.933503407239914 TRAIN  loss dict:  {'classification_loss': 0.933503407239914}
2025-01-13 15:24:05,252 [INFO] Step[200/2713]: training loss : 0.9319738733768463 TRAIN  loss dict:  {'classification_loss': 0.9319738733768463}
2025-01-13 15:24:23,260 [INFO] Step[250/2713]: training loss : 0.9337968897819519 TRAIN  loss dict:  {'classification_loss': 0.9337968897819519}
2025-01-13 15:24:45,481 [INFO] Step[300/2713]: training loss : 0.9322233271598815 TRAIN  loss dict:  {'classification_loss': 0.9322233271598815}
2025-01-13 15:25:09,378 [INFO] Step[350/2713]: training loss : 0.9328825056552887 TRAIN  loss dict:  {'classification_loss': 0.9328825056552887}
2025-01-13 15:25:31,904 [INFO] Step[400/2713]: training loss : 0.9328020513057709 TRAIN  loss dict:  {'classification_loss': 0.9328020513057709}
2025-01-13 15:25:52,008 [INFO] Step[450/2713]: training loss : 0.93246253490448 TRAIN  loss dict:  {'classification_loss': 0.93246253490448}
2025-01-13 15:26:14,157 [INFO] Step[500/2713]: training loss : 0.9338698947429657 TRAIN  loss dict:  {'classification_loss': 0.9338698947429657}
2025-01-13 15:26:34,121 [INFO] Step[550/2713]: training loss : 0.9326608788967132 TRAIN  loss dict:  {'classification_loss': 0.9326608788967132}
2025-01-13 15:26:49,450 [INFO] Step[600/2713]: training loss : 0.9340016889572144 TRAIN  loss dict:  {'classification_loss': 0.9340016889572144}
2025-01-13 15:27:08,327 [INFO] Step[650/2713]: training loss : 0.9324882364273072 TRAIN  loss dict:  {'classification_loss': 0.9324882364273072}
2025-01-13 15:27:28,655 [INFO] Step[700/2713]: training loss : 0.9332798314094544 TRAIN  loss dict:  {'classification_loss': 0.9332798314094544}
2025-01-13 15:27:53,183 [INFO] Step[750/2713]: training loss : 0.9329219710826874 TRAIN  loss dict:  {'classification_loss': 0.9329219710826874}
2025-01-13 15:28:16,274 [INFO] Step[800/2713]: training loss : 0.9321527969837189 TRAIN  loss dict:  {'classification_loss': 0.9321527969837189}
2025-01-13 15:28:35,966 [INFO] Step[850/2713]: training loss : 0.9322662055492401 TRAIN  loss dict:  {'classification_loss': 0.9322662055492401}
2025-01-13 15:28:56,049 [INFO] Step[900/2713]: training loss : 0.9324182736873626 TRAIN  loss dict:  {'classification_loss': 0.9324182736873626}
2025-01-13 15:29:17,756 [INFO] Step[950/2713]: training loss : 0.9349651873111725 TRAIN  loss dict:  {'classification_loss': 0.9349651873111725}
2025-01-13 15:29:34,922 [INFO] Step[1000/2713]: training loss : 0.9332985365390778 TRAIN  loss dict:  {'classification_loss': 0.9332985365390778}
2025-01-13 15:29:53,917 [INFO] Step[1050/2713]: training loss : 0.9322959458827973 TRAIN  loss dict:  {'classification_loss': 0.9322959458827973}
2025-01-13 15:30:11,608 [INFO] Step[1100/2713]: training loss : 0.932377713918686 TRAIN  loss dict:  {'classification_loss': 0.932377713918686}
2025-01-13 15:30:33,424 [INFO] Step[1150/2713]: training loss : 0.933121999502182 TRAIN  loss dict:  {'classification_loss': 0.933121999502182}
2025-01-13 15:30:57,731 [INFO] Step[1200/2713]: training loss : 0.9331463944911956 TRAIN  loss dict:  {'classification_loss': 0.9331463944911956}
2025-01-13 15:31:20,088 [INFO] Step[1250/2713]: training loss : 0.9328071057796479 TRAIN  loss dict:  {'classification_loss': 0.9328071057796479}
2025-01-13 15:31:38,789 [INFO] Step[1300/2713]: training loss : 0.931606148481369 TRAIN  loss dict:  {'classification_loss': 0.931606148481369}
2025-01-13 15:32:01,901 [INFO] Step[1350/2713]: training loss : 0.9333644688129425 TRAIN  loss dict:  {'classification_loss': 0.9333644688129425}
2025-01-13 15:32:21,806 [INFO] Step[1400/2713]: training loss : 0.9322945082187652 TRAIN  loss dict:  {'classification_loss': 0.9322945082187652}
2025-01-13 15:32:34,686 [INFO] Step[1450/2713]: training loss : 0.933425453901291 TRAIN  loss dict:  {'classification_loss': 0.933425453901291}
2025-01-13 15:32:47,706 [INFO] Step[1500/2713]: training loss : 0.932394208908081 TRAIN  loss dict:  {'classification_loss': 0.932394208908081}
2025-01-13 15:33:00,591 [INFO] Step[1550/2713]: training loss : 0.9325238585472106 TRAIN  loss dict:  {'classification_loss': 0.9325238585472106}
2025-01-13 15:33:13,591 [INFO] Step[1600/2713]: training loss : 0.9330611276626587 TRAIN  loss dict:  {'classification_loss': 0.9330611276626587}
2025-01-13 15:33:26,444 [INFO] Step[1650/2713]: training loss : 0.9323710715770721 TRAIN  loss dict:  {'classification_loss': 0.9323710715770721}
2025-01-13 15:33:39,196 [INFO] Step[1700/2713]: training loss : 0.9335836410522461 TRAIN  loss dict:  {'classification_loss': 0.9335836410522461}
2025-01-13 15:33:51,854 [INFO] Step[1750/2713]: training loss : 0.9326319336891175 TRAIN  loss dict:  {'classification_loss': 0.9326319336891175}
2025-01-13 15:34:04,646 [INFO] Step[1800/2713]: training loss : 0.9349038195610047 TRAIN  loss dict:  {'classification_loss': 0.9349038195610047}
2025-01-13 15:34:17,795 [INFO] Step[1850/2713]: training loss : 0.9320299279689789 TRAIN  loss dict:  {'classification_loss': 0.9320299279689789}
2025-01-13 15:34:31,019 [INFO] Step[1900/2713]: training loss : 0.9345203626155854 TRAIN  loss dict:  {'classification_loss': 0.9345203626155854}
2025-01-13 15:34:44,154 [INFO] Step[1950/2713]: training loss : 0.9324033999443054 TRAIN  loss dict:  {'classification_loss': 0.9324033999443054}
2025-01-13 15:34:58,247 [INFO] Step[2000/2713]: training loss : 0.9317409133911133 TRAIN  loss dict:  {'classification_loss': 0.9317409133911133}
2025-01-13 15:35:17,062 [INFO] Step[2050/2713]: training loss : 0.9318202638626099 TRAIN  loss dict:  {'classification_loss': 0.9318202638626099}
2025-01-13 15:35:34,262 [INFO] Step[2100/2713]: training loss : 0.932707144021988 TRAIN  loss dict:  {'classification_loss': 0.932707144021988}
2025-01-13 15:35:53,312 [INFO] Step[2150/2713]: training loss : 0.9314029622077942 TRAIN  loss dict:  {'classification_loss': 0.9314029622077942}
2025-01-13 15:36:16,364 [INFO] Step[2200/2713]: training loss : 0.9323216927051544 TRAIN  loss dict:  {'classification_loss': 0.9323216927051544}
2025-01-13 15:36:38,286 [INFO] Step[2250/2713]: training loss : 0.9329983341693878 TRAIN  loss dict:  {'classification_loss': 0.9329983341693878}
2025-01-13 15:36:57,463 [INFO] Step[2300/2713]: training loss : 0.9322690141201019 TRAIN  loss dict:  {'classification_loss': 0.9322690141201019}
2025-01-13 15:37:17,887 [INFO] Step[2350/2713]: training loss : 0.932515172958374 TRAIN  loss dict:  {'classification_loss': 0.932515172958374}
2025-01-13 15:37:39,669 [INFO] Step[2400/2713]: training loss : 0.9342883002758026 TRAIN  loss dict:  {'classification_loss': 0.9342883002758026}
2025-01-13 15:37:57,769 [INFO] Step[2450/2713]: training loss : 0.9321468126773834 TRAIN  loss dict:  {'classification_loss': 0.9321468126773834}
2025-01-13 15:38:14,953 [INFO] Step[2500/2713]: training loss : 0.9327086448669434 TRAIN  loss dict:  {'classification_loss': 0.9327086448669434}
2025-01-13 15:38:33,311 [INFO] Step[2550/2713]: training loss : 0.9326765477657318 TRAIN  loss dict:  {'classification_loss': 0.9326765477657318}
2025-01-13 15:38:54,987 [INFO] Step[2600/2713]: training loss : 0.933255455493927 TRAIN  loss dict:  {'classification_loss': 0.933255455493927}
2025-01-13 15:39:19,322 [INFO] Step[2650/2713]: training loss : 0.93246502161026 TRAIN  loss dict:  {'classification_loss': 0.93246502161026}
2025-01-13 15:39:41,882 [INFO] Step[2700/2713]: training loss : 0.9329182016849518 TRAIN  loss dict:  {'classification_loss': 0.9329182016849518}
2025-01-13 15:42:31,677 [INFO] Label accuracies statistics:
2025-01-13 15:42:31,677 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.75, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 0.75, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.75, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 0.75, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.75, 217: 0.75, 218: 0.75, 219: 1.0, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.25, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.5, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.5, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 0.75, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.0, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 1.0, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 15:42:31,686 [INFO] [117] TRAIN  loss: 0.9327951781520358 acc: 1.0
2025-01-13 15:42:31,686 [INFO] [117] TRAIN  loss dict: {'classification_loss': 0.9327951781520358}
2025-01-13 15:42:31,687 [INFO] [117] VALIDATION loss: 1.6952788911591796 VALIDATION acc: 0.8363636363636363
2025-01-13 15:42:31,687 [INFO] [117] VALIDATION loss dict: {'classification_loss': 1.6952788911591796}
2025-01-13 15:42:31,688 [INFO] 
2025-01-13 15:43:03,415 [INFO] Step[50/2713]: training loss : 0.932331919670105 TRAIN  loss dict:  {'classification_loss': 0.932331919670105}
2025-01-13 15:43:24,586 [INFO] Step[100/2713]: training loss : 0.9333130371570587 TRAIN  loss dict:  {'classification_loss': 0.9333130371570587}
2025-01-13 15:43:48,539 [INFO] Step[150/2713]: training loss : 0.9330358624458313 TRAIN  loss dict:  {'classification_loss': 0.9330358624458313}
2025-01-13 15:44:05,566 [INFO] Step[200/2713]: training loss : 0.9313383924961091 TRAIN  loss dict:  {'classification_loss': 0.9313383924961091}
2025-01-13 15:44:23,749 [INFO] Step[250/2713]: training loss : 0.9326393806934357 TRAIN  loss dict:  {'classification_loss': 0.9326393806934357}
2025-01-13 15:44:42,928 [INFO] Step[300/2713]: training loss : 0.9316722548007965 TRAIN  loss dict:  {'classification_loss': 0.9316722548007965}
2025-01-13 15:45:04,703 [INFO] Step[350/2713]: training loss : 0.9330320632457734 TRAIN  loss dict:  {'classification_loss': 0.9330320632457734}
2025-01-13 15:45:28,418 [INFO] Step[400/2713]: training loss : 0.9327118015289306 TRAIN  loss dict:  {'classification_loss': 0.9327118015289306}
2025-01-13 15:45:50,023 [INFO] Step[450/2713]: training loss : 0.93225381731987 TRAIN  loss dict:  {'classification_loss': 0.93225381731987}
2025-01-13 15:46:10,454 [INFO] Step[500/2713]: training loss : 0.9334065866470337 TRAIN  loss dict:  {'classification_loss': 0.9334065866470337}
2025-01-13 15:46:32,946 [INFO] Step[550/2713]: training loss : 0.9326951825618743 TRAIN  loss dict:  {'classification_loss': 0.9326951825618743}
2025-01-13 15:46:50,601 [INFO] Step[600/2713]: training loss : 0.9335950970649719 TRAIN  loss dict:  {'classification_loss': 0.9335950970649719}
2025-01-13 15:47:08,479 [INFO] Step[650/2713]: training loss : 0.9319284307956696 TRAIN  loss dict:  {'classification_loss': 0.9319284307956696}
2025-01-13 15:47:28,445 [INFO] Step[700/2713]: training loss : 0.9326953589916229 TRAIN  loss dict:  {'classification_loss': 0.9326953589916229}
2025-01-13 15:47:49,165 [INFO] Step[750/2713]: training loss : 0.9320571398735047 TRAIN  loss dict:  {'classification_loss': 0.9320571398735047}
2025-01-13 15:48:12,500 [INFO] Step[800/2713]: training loss : 0.9318800699710846 TRAIN  loss dict:  {'classification_loss': 0.9318800699710846}
2025-01-13 15:48:34,962 [INFO] Step[850/2713]: training loss : 0.9319729816913604 TRAIN  loss dict:  {'classification_loss': 0.9319729816913604}
2025-01-13 15:48:53,882 [INFO] Step[900/2713]: training loss : 0.933224561214447 TRAIN  loss dict:  {'classification_loss': 0.933224561214447}
2025-01-13 15:49:16,729 [INFO] Step[950/2713]: training loss : 0.9321855759620666 TRAIN  loss dict:  {'classification_loss': 0.9321855759620666}
2025-01-13 15:49:35,810 [INFO] Step[1000/2713]: training loss : 0.9323641967773437 TRAIN  loss dict:  {'classification_loss': 0.9323641967773437}
2025-01-13 15:49:51,503 [INFO] Step[1050/2713]: training loss : 0.9327472257614136 TRAIN  loss dict:  {'classification_loss': 0.9327472257614136}
2025-01-13 15:50:10,371 [INFO] Step[1100/2713]: training loss : 0.9335532999038696 TRAIN  loss dict:  {'classification_loss': 0.9335532999038696}
2025-01-13 15:50:29,949 [INFO] Step[1150/2713]: training loss : 0.9321389997005463 TRAIN  loss dict:  {'classification_loss': 0.9321389997005463}
2025-01-13 15:50:54,110 [INFO] Step[1200/2713]: training loss : 0.9325282943248748 TRAIN  loss dict:  {'classification_loss': 0.9325282943248748}
2025-01-13 15:51:16,109 [INFO] Step[1250/2713]: training loss : 0.9316970825195312 TRAIN  loss dict:  {'classification_loss': 0.9316970825195312}
2025-01-13 15:51:34,936 [INFO] Step[1300/2713]: training loss : 0.933488632440567 TRAIN  loss dict:  {'classification_loss': 0.933488632440567}
2025-01-13 15:51:56,528 [INFO] Step[1350/2713]: training loss : 0.9327694773674011 TRAIN  loss dict:  {'classification_loss': 0.9327694773674011}
2025-01-13 15:52:19,551 [INFO] Step[1400/2713]: training loss : 0.9330812406539917 TRAIN  loss dict:  {'classification_loss': 0.9330812406539917}
2025-01-13 15:52:38,436 [INFO] Step[1450/2713]: training loss : 0.9333737695217132 TRAIN  loss dict:  {'classification_loss': 0.9333737695217132}
2025-01-13 15:52:58,063 [INFO] Step[1500/2713]: training loss : 0.932273029088974 TRAIN  loss dict:  {'classification_loss': 0.932273029088974}
2025-01-13 15:53:18,843 [INFO] Step[1550/2713]: training loss : 0.9333234047889709 TRAIN  loss dict:  {'classification_loss': 0.9333234047889709}
2025-01-13 15:53:42,872 [INFO] Step[1600/2713]: training loss : 0.9327209460735321 TRAIN  loss dict:  {'classification_loss': 0.9327209460735321}
2025-01-13 15:54:04,709 [INFO] Step[1650/2713]: training loss : 0.9316258239746094 TRAIN  loss dict:  {'classification_loss': 0.9316258239746094}
2025-01-13 15:54:24,781 [INFO] Step[1700/2713]: training loss : 0.9314366328716278 TRAIN  loss dict:  {'classification_loss': 0.9314366328716278}
2025-01-13 15:54:47,758 [INFO] Step[1750/2713]: training loss : 0.9318739295005798 TRAIN  loss dict:  {'classification_loss': 0.9318739295005798}
2025-01-13 15:55:06,967 [INFO] Step[1800/2713]: training loss : 0.9330882573127747 TRAIN  loss dict:  {'classification_loss': 0.9330882573127747}
2025-01-13 15:55:24,445 [INFO] Step[1850/2713]: training loss : 0.9318046605587006 TRAIN  loss dict:  {'classification_loss': 0.9318046605587006}
2025-01-13 15:55:43,546 [INFO] Step[1900/2713]: training loss : 0.9325320088863372 TRAIN  loss dict:  {'classification_loss': 0.9325320088863372}
2025-01-13 15:56:03,809 [INFO] Step[1950/2713]: training loss : 0.9349879276752472 TRAIN  loss dict:  {'classification_loss': 0.9349879276752472}
2025-01-13 15:56:27,499 [INFO] Step[2000/2713]: training loss : 0.932843724489212 TRAIN  loss dict:  {'classification_loss': 0.932843724489212}
2025-01-13 15:56:50,707 [INFO] Step[2050/2713]: training loss : 0.9332417595386505 TRAIN  loss dict:  {'classification_loss': 0.9332417595386505}
2025-01-13 15:57:09,076 [INFO] Step[2100/2713]: training loss : 0.9312003183364869 TRAIN  loss dict:  {'classification_loss': 0.9312003183364869}
2025-01-13 15:57:30,263 [INFO] Step[2150/2713]: training loss : 0.9316692650318146 TRAIN  loss dict:  {'classification_loss': 0.9316692650318146}
2025-01-13 15:57:50,558 [INFO] Step[2200/2713]: training loss : 0.9329661571979523 TRAIN  loss dict:  {'classification_loss': 0.9329661571979523}
2025-01-13 15:58:06,621 [INFO] Step[2250/2713]: training loss : 0.9339053976535797 TRAIN  loss dict:  {'classification_loss': 0.9339053976535797}
2025-01-13 15:58:26,150 [INFO] Step[2300/2713]: training loss : 0.9316330230236054 TRAIN  loss dict:  {'classification_loss': 0.9316330230236054}
2025-01-13 15:58:48,002 [INFO] Step[2350/2713]: training loss : 0.9323239302635193 TRAIN  loss dict:  {'classification_loss': 0.9323239302635193}
2025-01-13 15:59:10,762 [INFO] Step[2400/2713]: training loss : 0.9336452090740204 TRAIN  loss dict:  {'classification_loss': 0.9336452090740204}
2025-01-13 15:59:30,827 [INFO] Step[2450/2713]: training loss : 0.9350870954990387 TRAIN  loss dict:  {'classification_loss': 0.9350870954990387}
2025-01-13 15:59:49,926 [INFO] Step[2500/2713]: training loss : 0.9321819472312928 TRAIN  loss dict:  {'classification_loss': 0.9321819472312928}
2025-01-13 16:00:09,632 [INFO] Step[2550/2713]: training loss : 0.932221052646637 TRAIN  loss dict:  {'classification_loss': 0.932221052646637}
2025-01-13 16:00:31,801 [INFO] Step[2600/2713]: training loss : 0.9324172389507294 TRAIN  loss dict:  {'classification_loss': 0.9324172389507294}
2025-01-13 16:00:53,626 [INFO] Step[2650/2713]: training loss : 0.9323683106899261 TRAIN  loss dict:  {'classification_loss': 0.9323683106899261}
2025-01-13 16:01:08,908 [INFO] Step[2700/2713]: training loss : 0.9321976375579833 TRAIN  loss dict:  {'classification_loss': 0.9321976375579833}
2025-01-13 16:03:59,287 [INFO] Label accuracies statistics:
2025-01-13 16:03:59,287 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 1.0, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 1.0, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.75, 69: 1.0, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.25, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.75, 123: 0.75, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 0.75, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 1.0, 157: 1.0, 158: 1.0, 159: 1.0, 160: 0.75, 161: 0.75, 162: 1.0, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 1.0, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.5, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.5, 274: 0.5, 275: 0.5, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 0.75, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.5, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 1.0, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 0.75, 370: 0.75, 371: 0.75, 372: 1.0, 373: 1.0, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 16:03:59,291 [INFO] [118] TRAIN  loss: 0.9326132740475268 acc: 1.0
2025-01-13 16:03:59,291 [INFO] [118] TRAIN  loss dict: {'classification_loss': 0.9326132740475268}
2025-01-13 16:03:59,291 [INFO] [118] VALIDATION loss: 1.7041364486952473 VALIDATION acc: 0.8357366771159874
2025-01-13 16:03:59,291 [INFO] [118] VALIDATION loss dict: {'classification_loss': 1.7041364486952473}
2025-01-13 16:03:59,292 [INFO] 
2025-01-13 16:04:24,372 [INFO] Step[50/2713]: training loss : 0.9325840210914612 TRAIN  loss dict:  {'classification_loss': 0.9325840210914612}
2025-01-13 16:04:37,507 [INFO] Step[100/2713]: training loss : 0.9317158210277557 TRAIN  loss dict:  {'classification_loss': 0.9317158210277557}
2025-01-13 16:04:50,227 [INFO] Step[150/2713]: training loss : 0.9322983419895172 TRAIN  loss dict:  {'classification_loss': 0.9322983419895172}
2025-01-13 16:05:03,439 [INFO] Step[200/2713]: training loss : 0.9311241412162781 TRAIN  loss dict:  {'classification_loss': 0.9311241412162781}
2025-01-13 16:05:16,340 [INFO] Step[250/2713]: training loss : 0.9347543537616729 TRAIN  loss dict:  {'classification_loss': 0.9347543537616729}
2025-01-13 16:05:29,472 [INFO] Step[300/2713]: training loss : 0.9325200915336609 TRAIN  loss dict:  {'classification_loss': 0.9325200915336609}
2025-01-13 16:05:42,614 [INFO] Step[350/2713]: training loss : 0.9315146458148956 TRAIN  loss dict:  {'classification_loss': 0.9315146458148956}
2025-01-13 16:05:55,637 [INFO] Step[400/2713]: training loss : 0.9324947202205658 TRAIN  loss dict:  {'classification_loss': 0.9324947202205658}
2025-01-13 16:06:08,568 [INFO] Step[450/2713]: training loss : 0.9324104690551758 TRAIN  loss dict:  {'classification_loss': 0.9324104690551758}
2025-01-13 16:06:21,435 [INFO] Step[500/2713]: training loss : 0.9310089778900147 TRAIN  loss dict:  {'classification_loss': 0.9310089778900147}
2025-01-13 16:06:34,415 [INFO] Step[550/2713]: training loss : 0.9317355668544769 TRAIN  loss dict:  {'classification_loss': 0.9317355668544769}
2025-01-13 16:06:47,152 [INFO] Step[600/2713]: training loss : 0.9326422345638276 TRAIN  loss dict:  {'classification_loss': 0.9326422345638276}
2025-01-13 16:07:01,383 [INFO] Step[650/2713]: training loss : 0.9316182243824005 TRAIN  loss dict:  {'classification_loss': 0.9316182243824005}
2025-01-13 16:07:16,497 [INFO] Step[700/2713]: training loss : 0.9318758118152618 TRAIN  loss dict:  {'classification_loss': 0.9318758118152618}
2025-01-13 16:07:30,233 [INFO] Step[750/2713]: training loss : 0.932076963186264 TRAIN  loss dict:  {'classification_loss': 0.932076963186264}
2025-01-13 16:07:43,110 [INFO] Step[800/2713]: training loss : 0.931906406879425 TRAIN  loss dict:  {'classification_loss': 0.931906406879425}
2025-01-13 16:07:55,812 [INFO] Step[850/2713]: training loss : 0.9309641897678376 TRAIN  loss dict:  {'classification_loss': 0.9309641897678376}
2025-01-13 16:08:09,060 [INFO] Step[900/2713]: training loss : 0.9329843020439148 TRAIN  loss dict:  {'classification_loss': 0.9329843020439148}
2025-01-13 16:08:21,651 [INFO] Step[950/2713]: training loss : 0.9321918118000031 TRAIN  loss dict:  {'classification_loss': 0.9321918118000031}
2025-01-13 16:08:34,501 [INFO] Step[1000/2713]: training loss : 0.9318120706081391 TRAIN  loss dict:  {'classification_loss': 0.9318120706081391}
2025-01-13 16:08:47,752 [INFO] Step[1050/2713]: training loss : 0.9322494423389435 TRAIN  loss dict:  {'classification_loss': 0.9322494423389435}
2025-01-13 16:09:00,576 [INFO] Step[1100/2713]: training loss : 0.9323660731315613 TRAIN  loss dict:  {'classification_loss': 0.9323660731315613}
2025-01-13 16:09:13,194 [INFO] Step[1150/2713]: training loss : 0.9324277126789093 TRAIN  loss dict:  {'classification_loss': 0.9324277126789093}
2025-01-13 16:09:25,979 [INFO] Step[1200/2713]: training loss : 0.9316484749317169 TRAIN  loss dict:  {'classification_loss': 0.9316484749317169}
2025-01-13 16:09:38,900 [INFO] Step[1250/2713]: training loss : 0.934010192155838 TRAIN  loss dict:  {'classification_loss': 0.934010192155838}
2025-01-13 16:09:51,658 [INFO] Step[1300/2713]: training loss : 0.9322669506072998 TRAIN  loss dict:  {'classification_loss': 0.9322669506072998}
2025-01-13 16:10:04,751 [INFO] Step[1350/2713]: training loss : 0.9313403844833374 TRAIN  loss dict:  {'classification_loss': 0.9313403844833374}
2025-01-13 16:10:17,663 [INFO] Step[1400/2713]: training loss : 0.9317894697189331 TRAIN  loss dict:  {'classification_loss': 0.9317894697189331}
2025-01-13 16:10:30,627 [INFO] Step[1450/2713]: training loss : 0.9323001432418824 TRAIN  loss dict:  {'classification_loss': 0.9323001432418824}
2025-01-13 16:10:43,443 [INFO] Step[1500/2713]: training loss : 0.9329206323623658 TRAIN  loss dict:  {'classification_loss': 0.9329206323623658}
2025-01-13 16:10:56,241 [INFO] Step[1550/2713]: training loss : 0.9322642338275909 TRAIN  loss dict:  {'classification_loss': 0.9322642338275909}
2025-01-13 16:11:09,284 [INFO] Step[1600/2713]: training loss : 0.9345373141765595 TRAIN  loss dict:  {'classification_loss': 0.9345373141765595}
2025-01-13 16:11:22,120 [INFO] Step[1650/2713]: training loss : 0.9322888612747192 TRAIN  loss dict:  {'classification_loss': 0.9322888612747192}
2025-01-13 16:11:34,903 [INFO] Step[1700/2713]: training loss : 0.9336095535755158 TRAIN  loss dict:  {'classification_loss': 0.9336095535755158}
2025-01-13 16:11:47,422 [INFO] Step[1750/2713]: training loss : 0.932498037815094 TRAIN  loss dict:  {'classification_loss': 0.932498037815094}
2025-01-13 16:12:00,346 [INFO] Step[1800/2713]: training loss : 0.9336164176464081 TRAIN  loss dict:  {'classification_loss': 0.9336164176464081}
2025-01-13 16:12:13,555 [INFO] Step[1850/2713]: training loss : 0.9323036110401154 TRAIN  loss dict:  {'classification_loss': 0.9323036110401154}
2025-01-13 16:12:26,832 [INFO] Step[1900/2713]: training loss : 0.9330837345123291 TRAIN  loss dict:  {'classification_loss': 0.9330837345123291}
2025-01-13 16:12:39,682 [INFO] Step[1950/2713]: training loss : 0.9321121382713318 TRAIN  loss dict:  {'classification_loss': 0.9321121382713318}
2025-01-13 16:12:52,691 [INFO] Step[2000/2713]: training loss : 0.932796082496643 TRAIN  loss dict:  {'classification_loss': 0.932796082496643}
2025-01-13 16:13:05,273 [INFO] Step[2050/2713]: training loss : 0.9313086700439454 TRAIN  loss dict:  {'classification_loss': 0.9313086700439454}
2025-01-13 16:13:18,204 [INFO] Step[2100/2713]: training loss : 0.9334589457511902 TRAIN  loss dict:  {'classification_loss': 0.9334589457511902}
2025-01-13 16:13:31,193 [INFO] Step[2150/2713]: training loss : 0.9340324366092682 TRAIN  loss dict:  {'classification_loss': 0.9340324366092682}
2025-01-13 16:13:43,773 [INFO] Step[2200/2713]: training loss : 0.9311556625366211 TRAIN  loss dict:  {'classification_loss': 0.9311556625366211}
2025-01-13 16:13:56,472 [INFO] Step[2250/2713]: training loss : 0.9301793110370636 TRAIN  loss dict:  {'classification_loss': 0.9301793110370636}
2025-01-13 16:14:09,301 [INFO] Step[2300/2713]: training loss : 0.9330536365509033 TRAIN  loss dict:  {'classification_loss': 0.9330536365509033}
2025-01-13 16:14:22,245 [INFO] Step[2350/2713]: training loss : 0.9317166459560394 TRAIN  loss dict:  {'classification_loss': 0.9317166459560394}
2025-01-13 16:14:35,255 [INFO] Step[2400/2713]: training loss : 0.9313336443901062 TRAIN  loss dict:  {'classification_loss': 0.9313336443901062}
2025-01-13 16:14:47,834 [INFO] Step[2450/2713]: training loss : 0.9316889750957489 TRAIN  loss dict:  {'classification_loss': 0.9316889750957489}
2025-01-13 16:15:00,739 [INFO] Step[2500/2713]: training loss : 0.9359777879714966 TRAIN  loss dict:  {'classification_loss': 0.9359777879714966}
2025-01-13 16:15:13,320 [INFO] Step[2550/2713]: training loss : 0.9327265202999115 TRAIN  loss dict:  {'classification_loss': 0.9327265202999115}
2025-01-13 16:15:26,228 [INFO] Step[2600/2713]: training loss : 0.9330270838737488 TRAIN  loss dict:  {'classification_loss': 0.9330270838737488}
2025-01-13 16:15:39,105 [INFO] Step[2650/2713]: training loss : 0.9310976827144622 TRAIN  loss dict:  {'classification_loss': 0.9310976827144622}
2025-01-13 16:15:52,083 [INFO] Step[2700/2713]: training loss : 0.93246621966362 TRAIN  loss dict:  {'classification_loss': 0.93246621966362}
2025-01-13 16:17:34,684 [INFO] Label accuracies statistics:
2025-01-13 16:17:34,684 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.75, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 0.75, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.25, 67: 1.0, 68: 0.75, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.25, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.75, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.25, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.5, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 1.0, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 0.5, 162: 1.0, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.5, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 1.0, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 1.0, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.25, 240: 0.75, 241: 1.0, 242: 0.25, 243: 0.75, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.5, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.75, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.5, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 1.0, 286: 1.0, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 1.0, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 1.0, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 1.0, 356: 0.5, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.5, 394: 0.75, 395: 0.5, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 16:17:34,687 [INFO] [119] TRAIN  loss: 0.9323638622891715 acc: 1.0
2025-01-13 16:17:34,687 [INFO] [119] TRAIN  loss dict: {'classification_loss': 0.9323638622891715}
2025-01-13 16:17:34,687 [INFO] [119] VALIDATION loss: 1.6860647500681698 VALIDATION acc: 0.8332288401253919
2025-01-13 16:17:34,687 [INFO] [119] VALIDATION loss dict: {'classification_loss': 1.6860647500681698}
2025-01-13 16:17:34,688 [INFO] 
2025-01-13 16:17:53,282 [INFO] Step[50/2713]: training loss : 0.9344353425502777 TRAIN  loss dict:  {'classification_loss': 0.9344353425502777}
2025-01-13 16:18:05,849 [INFO] Step[100/2713]: training loss : 0.9346680569648743 TRAIN  loss dict:  {'classification_loss': 0.9346680569648743}
2025-01-13 16:18:18,437 [INFO] Step[150/2713]: training loss : 0.9322981667518616 TRAIN  loss dict:  {'classification_loss': 0.9322981667518616}
2025-01-13 16:18:30,975 [INFO] Step[200/2713]: training loss : 0.9362575244903565 TRAIN  loss dict:  {'classification_loss': 0.9362575244903565}
2025-01-13 16:18:43,583 [INFO] Step[250/2713]: training loss : 0.9324838590621948 TRAIN  loss dict:  {'classification_loss': 0.9324838590621948}
2025-01-13 16:18:56,696 [INFO] Step[300/2713]: training loss : 0.931538577079773 TRAIN  loss dict:  {'classification_loss': 0.931538577079773}
2025-01-13 16:19:09,596 [INFO] Step[350/2713]: training loss : 0.9315729510784149 TRAIN  loss dict:  {'classification_loss': 0.9315729510784149}
2025-01-13 16:19:22,291 [INFO] Step[400/2713]: training loss : 0.9317361342906952 TRAIN  loss dict:  {'classification_loss': 0.9317361342906952}
2025-01-13 16:19:34,845 [INFO] Step[450/2713]: training loss : 0.9310993480682374 TRAIN  loss dict:  {'classification_loss': 0.9310993480682374}
2025-01-13 16:19:47,395 [INFO] Step[500/2713]: training loss : 0.9328081619739532 TRAIN  loss dict:  {'classification_loss': 0.9328081619739532}
2025-01-13 16:20:00,386 [INFO] Step[550/2713]: training loss : 0.9325864040851592 TRAIN  loss dict:  {'classification_loss': 0.9325864040851592}
2025-01-13 16:20:13,165 [INFO] Step[600/2713]: training loss : 0.9346230041980743 TRAIN  loss dict:  {'classification_loss': 0.9346230041980743}
2025-01-13 16:20:26,351 [INFO] Step[650/2713]: training loss : 0.9320530116558075 TRAIN  loss dict:  {'classification_loss': 0.9320530116558075}
2025-01-13 16:20:39,230 [INFO] Step[700/2713]: training loss : 0.931843739748001 TRAIN  loss dict:  {'classification_loss': 0.931843739748001}
2025-01-13 16:20:52,078 [INFO] Step[750/2713]: training loss : 0.9324486970901489 TRAIN  loss dict:  {'classification_loss': 0.9324486970901489}
2025-01-13 16:21:04,868 [INFO] Step[800/2713]: training loss : 0.9341197657585144 TRAIN  loss dict:  {'classification_loss': 0.9341197657585144}
2025-01-13 16:21:17,869 [INFO] Step[850/2713]: training loss : 0.9329057037830353 TRAIN  loss dict:  {'classification_loss': 0.9329057037830353}
2025-01-13 16:21:30,935 [INFO] Step[900/2713]: training loss : 0.9320002210140228 TRAIN  loss dict:  {'classification_loss': 0.9320002210140228}
2025-01-13 16:21:44,340 [INFO] Step[950/2713]: training loss : 0.9334096479415893 TRAIN  loss dict:  {'classification_loss': 0.9334096479415893}
2025-01-13 16:21:57,905 [INFO] Step[1000/2713]: training loss : 0.932050666809082 TRAIN  loss dict:  {'classification_loss': 0.932050666809082}
2025-01-13 16:22:11,013 [INFO] Step[1050/2713]: training loss : 0.9321901690959931 TRAIN  loss dict:  {'classification_loss': 0.9321901690959931}
2025-01-13 16:22:24,280 [INFO] Step[1100/2713]: training loss : 0.9322588038444519 TRAIN  loss dict:  {'classification_loss': 0.9322588038444519}
2025-01-13 16:22:37,436 [INFO] Step[1150/2713]: training loss : 0.9326284945011138 TRAIN  loss dict:  {'classification_loss': 0.9326284945011138}
2025-01-13 16:22:50,649 [INFO] Step[1200/2713]: training loss : 0.9322750675678253 TRAIN  loss dict:  {'classification_loss': 0.9322750675678253}
2025-01-13 16:23:03,802 [INFO] Step[1250/2713]: training loss : 0.9334251296520233 TRAIN  loss dict:  {'classification_loss': 0.9334251296520233}
2025-01-13 16:23:17,780 [INFO] Step[1300/2713]: training loss : 0.9322706484794616 TRAIN  loss dict:  {'classification_loss': 0.9322706484794616}
2025-01-13 16:23:31,363 [INFO] Step[1350/2713]: training loss : 0.9330665624141693 TRAIN  loss dict:  {'classification_loss': 0.9330665624141693}
2025-01-13 16:23:45,478 [INFO] Step[1400/2713]: training loss : 0.9325885224342346 TRAIN  loss dict:  {'classification_loss': 0.9325885224342346}
2025-01-13 16:24:00,586 [INFO] Step[1450/2713]: training loss : 0.9332303524017334 TRAIN  loss dict:  {'classification_loss': 0.9332303524017334}
2025-01-13 16:24:13,990 [INFO] Step[1500/2713]: training loss : 0.9327681457996368 TRAIN  loss dict:  {'classification_loss': 0.9327681457996368}
2025-01-13 16:24:26,891 [INFO] Step[1550/2713]: training loss : 0.9314218235015869 TRAIN  loss dict:  {'classification_loss': 0.9314218235015869}
2025-01-13 16:24:40,131 [INFO] Step[1600/2713]: training loss : 0.9334864950180054 TRAIN  loss dict:  {'classification_loss': 0.9334864950180054}
2025-01-13 16:24:53,365 [INFO] Step[1650/2713]: training loss : 0.9313384091854096 TRAIN  loss dict:  {'classification_loss': 0.9313384091854096}
2025-01-13 16:25:06,217 [INFO] Step[1700/2713]: training loss : 0.9358413922786712 TRAIN  loss dict:  {'classification_loss': 0.9358413922786712}
2025-01-13 16:25:19,357 [INFO] Step[1750/2713]: training loss : 0.9322079503536225 TRAIN  loss dict:  {'classification_loss': 0.9322079503536225}
2025-01-13 16:25:32,267 [INFO] Step[1800/2713]: training loss : 0.9323671627044677 TRAIN  loss dict:  {'classification_loss': 0.9323671627044677}
2025-01-13 16:25:45,482 [INFO] Step[1850/2713]: training loss : 0.9311384201049805 TRAIN  loss dict:  {'classification_loss': 0.9311384201049805}
2025-01-13 16:25:58,417 [INFO] Step[1900/2713]: training loss : 0.9321669960021972 TRAIN  loss dict:  {'classification_loss': 0.9321669960021972}
2025-01-13 16:26:11,281 [INFO] Step[1950/2713]: training loss : 0.9315638542175293 TRAIN  loss dict:  {'classification_loss': 0.9315638542175293}
2025-01-13 16:26:23,889 [INFO] Step[2000/2713]: training loss : 0.9319998705387116 TRAIN  loss dict:  {'classification_loss': 0.9319998705387116}
2025-01-13 16:26:36,792 [INFO] Step[2050/2713]: training loss : 0.9325260901451111 TRAIN  loss dict:  {'classification_loss': 0.9325260901451111}
2025-01-13 16:26:49,461 [INFO] Step[2100/2713]: training loss : 0.9315552091598511 TRAIN  loss dict:  {'classification_loss': 0.9315552091598511}
2025-01-13 16:27:02,251 [INFO] Step[2150/2713]: training loss : 0.9329370629787445 TRAIN  loss dict:  {'classification_loss': 0.9329370629787445}
2025-01-13 16:27:14,995 [INFO] Step[2200/2713]: training loss : 0.9329790341854095 TRAIN  loss dict:  {'classification_loss': 0.9329790341854095}
2025-01-13 16:27:27,968 [INFO] Step[2250/2713]: training loss : 0.9330153930187225 TRAIN  loss dict:  {'classification_loss': 0.9330153930187225}
2025-01-13 16:27:40,898 [INFO] Step[2300/2713]: training loss : 0.9324821591377258 TRAIN  loss dict:  {'classification_loss': 0.9324821591377258}
2025-01-13 16:27:53,656 [INFO] Step[2350/2713]: training loss : 0.9331955206394196 TRAIN  loss dict:  {'classification_loss': 0.9331955206394196}
2025-01-13 16:28:06,571 [INFO] Step[2400/2713]: training loss : 0.9323288524150848 TRAIN  loss dict:  {'classification_loss': 0.9323288524150848}
2025-01-13 16:28:19,337 [INFO] Step[2450/2713]: training loss : 0.9334586572647094 TRAIN  loss dict:  {'classification_loss': 0.9334586572647094}
2025-01-13 16:28:32,561 [INFO] Step[2500/2713]: training loss : 0.9325248694419861 TRAIN  loss dict:  {'classification_loss': 0.9325248694419861}
2025-01-13 16:28:45,212 [INFO] Step[2550/2713]: training loss : 0.9322950804233551 TRAIN  loss dict:  {'classification_loss': 0.9322950804233551}
2025-01-13 16:28:57,820 [INFO] Step[2600/2713]: training loss : 0.9333682012557983 TRAIN  loss dict:  {'classification_loss': 0.9333682012557983}
2025-01-13 16:29:10,803 [INFO] Step[2650/2713]: training loss : 0.931190128326416 TRAIN  loss dict:  {'classification_loss': 0.931190128326416}
2025-01-13 16:29:23,412 [INFO] Step[2700/2713]: training loss : 0.9325553286075592 TRAIN  loss dict:  {'classification_loss': 0.9325553286075592}
2025-01-13 16:31:07,368 [INFO] Label accuracies statistics:
2025-01-13 16:31:07,368 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 0.75, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 1.0, 35: 1.0, 36: 0.75, 37: 0.75, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.75, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.5, 58: 0.75, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.5, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 0.75, 117: 1.0, 118: 1.0, 119: 1.0, 120: 1.0, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 0.75, 157: 0.75, 158: 0.6666666666666666, 159: 1.0, 160: 0.75, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.5, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 1.0, 172: 0.75, 173: 1.0, 174: 1.0, 175: 1.0, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 0.75, 182: 0.75, 183: 0.75, 184: 0.75, 185: 1.0, 186: 1.0, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.75, 225: 0.75, 226: 0.75, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.5, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 0.75, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.75, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 0.75, 251: 0.75, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.5, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 0.75, 278: 0.75, 279: 1.0, 280: 1.0, 281: 0.75, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 0.75, 300: 1.0, 301: 1.0, 302: 0.75, 303: 0.75, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.5, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 1.0, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.75, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 1.0, 365: 0.75, 366: 0.75, 367: 0.75, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.75, 395: 0.0, 396: 0.75, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 16:31:07,371 [INFO] [120] TRAIN  loss: 0.932656436115009 acc: 1.0
2025-01-13 16:31:07,371 [INFO] [120] TRAIN  loss dict: {'classification_loss': 0.932656436115009}
2025-01-13 16:31:07,371 [INFO] [120] VALIDATION loss: 1.690151249555717 VALIDATION acc: 0.8338557993730408
2025-01-13 16:31:07,371 [INFO] [120] VALIDATION loss dict: {'classification_loss': 1.690151249555717}
2025-01-13 16:31:07,371 [INFO] 
2025-01-13 16:31:27,030 [INFO] Step[50/2713]: training loss : 0.9336833024024963 TRAIN  loss dict:  {'classification_loss': 0.9336833024024963}
2025-01-13 16:31:40,072 [INFO] Step[100/2713]: training loss : 0.9324755585193634 TRAIN  loss dict:  {'classification_loss': 0.9324755585193634}
2025-01-13 16:31:52,634 [INFO] Step[150/2713]: training loss : 0.9330140352249146 TRAIN  loss dict:  {'classification_loss': 0.9330140352249146}
2025-01-13 16:32:05,532 [INFO] Step[200/2713]: training loss : 0.9326336395740509 TRAIN  loss dict:  {'classification_loss': 0.9326336395740509}
2025-01-13 16:32:18,773 [INFO] Step[250/2713]: training loss : 0.9318099427223205 TRAIN  loss dict:  {'classification_loss': 0.9318099427223205}
2025-01-13 16:32:31,928 [INFO] Step[300/2713]: training loss : 0.9322656297683716 TRAIN  loss dict:  {'classification_loss': 0.9322656297683716}
2025-01-13 16:32:45,159 [INFO] Step[350/2713]: training loss : 0.9332024776935577 TRAIN  loss dict:  {'classification_loss': 0.9332024776935577}
2025-01-13 16:32:58,382 [INFO] Step[400/2713]: training loss : 0.9309788942337036 TRAIN  loss dict:  {'classification_loss': 0.9309788942337036}
2025-01-13 16:33:11,039 [INFO] Step[450/2713]: training loss : 0.9313409018516541 TRAIN  loss dict:  {'classification_loss': 0.9313409018516541}
2025-01-13 16:33:24,155 [INFO] Step[500/2713]: training loss : 0.9317931199073791 TRAIN  loss dict:  {'classification_loss': 0.9317931199073791}
2025-01-13 16:33:36,743 [INFO] Step[550/2713]: training loss : 0.9312840664386749 TRAIN  loss dict:  {'classification_loss': 0.9312840664386749}
2025-01-13 16:33:49,825 [INFO] Step[600/2713]: training loss : 0.9329833209514617 TRAIN  loss dict:  {'classification_loss': 0.9329833209514617}
2025-01-13 16:34:02,702 [INFO] Step[650/2713]: training loss : 0.9319872951507568 TRAIN  loss dict:  {'classification_loss': 0.9319872951507568}
2025-01-13 16:34:15,946 [INFO] Step[700/2713]: training loss : 0.9326338136196136 TRAIN  loss dict:  {'classification_loss': 0.9326338136196136}
2025-01-13 16:34:28,776 [INFO] Step[750/2713]: training loss : 0.9329433274269104 TRAIN  loss dict:  {'classification_loss': 0.9329433274269104}
2025-01-13 16:34:41,656 [INFO] Step[800/2713]: training loss : 0.9340189266204834 TRAIN  loss dict:  {'classification_loss': 0.9340189266204834}
2025-01-13 16:34:54,884 [INFO] Step[850/2713]: training loss : 0.9315938496589661 TRAIN  loss dict:  {'classification_loss': 0.9315938496589661}
2025-01-13 16:35:08,117 [INFO] Step[900/2713]: training loss : 0.9317904019355774 TRAIN  loss dict:  {'classification_loss': 0.9317904019355774}
2025-01-13 16:35:21,311 [INFO] Step[950/2713]: training loss : 0.9326607978343964 TRAIN  loss dict:  {'classification_loss': 0.9326607978343964}
2025-01-13 16:35:34,142 [INFO] Step[1000/2713]: training loss : 0.9323118734359741 TRAIN  loss dict:  {'classification_loss': 0.9323118734359741}
2025-01-13 16:35:46,792 [INFO] Step[1050/2713]: training loss : 0.931904182434082 TRAIN  loss dict:  {'classification_loss': 0.931904182434082}
2025-01-13 16:36:00,071 [INFO] Step[1100/2713]: training loss : 0.9338329875469208 TRAIN  loss dict:  {'classification_loss': 0.9338329875469208}
2025-01-13 16:36:13,131 [INFO] Step[1150/2713]: training loss : 0.93202214717865 TRAIN  loss dict:  {'classification_loss': 0.93202214717865}
2025-01-13 16:36:26,175 [INFO] Step[1200/2713]: training loss : 0.932797462940216 TRAIN  loss dict:  {'classification_loss': 0.932797462940216}
2025-01-13 16:36:39,393 [INFO] Step[1250/2713]: training loss : 0.933219952583313 TRAIN  loss dict:  {'classification_loss': 0.933219952583313}
2025-01-13 16:36:52,212 [INFO] Step[1300/2713]: training loss : 0.9327508962154388 TRAIN  loss dict:  {'classification_loss': 0.9327508962154388}
2025-01-13 16:37:05,182 [INFO] Step[1350/2713]: training loss : 0.9340974497795105 TRAIN  loss dict:  {'classification_loss': 0.9340974497795105}
2025-01-13 16:37:17,833 [INFO] Step[1400/2713]: training loss : 0.9324531412124634 TRAIN  loss dict:  {'classification_loss': 0.9324531412124634}
2025-01-13 16:37:30,901 [INFO] Step[1450/2713]: training loss : 0.9321320176124572 TRAIN  loss dict:  {'classification_loss': 0.9321320176124572}
2025-01-13 16:37:43,787 [INFO] Step[1500/2713]: training loss : 0.9322156333923339 TRAIN  loss dict:  {'classification_loss': 0.9322156333923339}
2025-01-13 16:37:56,992 [INFO] Step[1550/2713]: training loss : 0.9316233730316162 TRAIN  loss dict:  {'classification_loss': 0.9316233730316162}
2025-01-13 16:38:10,189 [INFO] Step[1600/2713]: training loss : 0.9335028660297394 TRAIN  loss dict:  {'classification_loss': 0.9335028660297394}
2025-01-13 16:38:23,037 [INFO] Step[1650/2713]: training loss : 0.9330333423614502 TRAIN  loss dict:  {'classification_loss': 0.9330333423614502}
2025-01-13 16:38:35,988 [INFO] Step[1700/2713]: training loss : 0.9321953749656677 TRAIN  loss dict:  {'classification_loss': 0.9321953749656677}
2025-01-13 16:38:48,908 [INFO] Step[1750/2713]: training loss : 0.9319875121116639 TRAIN  loss dict:  {'classification_loss': 0.9319875121116639}
2025-01-13 16:39:01,819 [INFO] Step[1800/2713]: training loss : 0.9317411828041077 TRAIN  loss dict:  {'classification_loss': 0.9317411828041077}
2025-01-13 16:39:14,868 [INFO] Step[1850/2713]: training loss : 0.9349847078323364 TRAIN  loss dict:  {'classification_loss': 0.9349847078323364}
2025-01-13 16:39:28,115 [INFO] Step[1900/2713]: training loss : 0.9320342326164246 TRAIN  loss dict:  {'classification_loss': 0.9320342326164246}
2025-01-13 16:39:41,389 [INFO] Step[1950/2713]: training loss : 0.9325480282306671 TRAIN  loss dict:  {'classification_loss': 0.9325480282306671}
2025-01-13 16:39:54,602 [INFO] Step[2000/2713]: training loss : 0.9319886970520019 TRAIN  loss dict:  {'classification_loss': 0.9319886970520019}
2025-01-13 16:40:07,673 [INFO] Step[2050/2713]: training loss : 0.933091492652893 TRAIN  loss dict:  {'classification_loss': 0.933091492652893}
2025-01-13 16:40:20,236 [INFO] Step[2100/2713]: training loss : 0.9315956711769104 TRAIN  loss dict:  {'classification_loss': 0.9315956711769104}
2025-01-13 16:40:32,811 [INFO] Step[2150/2713]: training loss : 0.9331005561351776 TRAIN  loss dict:  {'classification_loss': 0.9331005561351776}
2025-01-13 16:40:45,341 [INFO] Step[2200/2713]: training loss : 0.931692568063736 TRAIN  loss dict:  {'classification_loss': 0.931692568063736}
2025-01-13 16:40:58,031 [INFO] Step[2250/2713]: training loss : 0.9331396961212158 TRAIN  loss dict:  {'classification_loss': 0.9331396961212158}
2025-01-13 16:41:10,844 [INFO] Step[2300/2713]: training loss : 0.9317777645587921 TRAIN  loss dict:  {'classification_loss': 0.9317777645587921}
2025-01-13 16:41:23,438 [INFO] Step[2350/2713]: training loss : 0.9327882492542267 TRAIN  loss dict:  {'classification_loss': 0.9327882492542267}
2025-01-13 16:41:36,444 [INFO] Step[2400/2713]: training loss : 0.9405324947834015 TRAIN  loss dict:  {'classification_loss': 0.9405324947834015}
2025-01-13 16:41:49,619 [INFO] Step[2450/2713]: training loss : 0.9321195471286774 TRAIN  loss dict:  {'classification_loss': 0.9321195471286774}
2025-01-13 16:42:02,576 [INFO] Step[2500/2713]: training loss : 0.9314968478679657 TRAIN  loss dict:  {'classification_loss': 0.9314968478679657}
2025-01-13 16:42:15,165 [INFO] Step[2550/2713]: training loss : 0.931675409078598 TRAIN  loss dict:  {'classification_loss': 0.931675409078598}
2025-01-13 16:42:28,293 [INFO] Step[2600/2713]: training loss : 0.9333128261566163 TRAIN  loss dict:  {'classification_loss': 0.9333128261566163}
2025-01-13 16:42:40,846 [INFO] Step[2650/2713]: training loss : 0.9324421560764313 TRAIN  loss dict:  {'classification_loss': 0.9324421560764313}
2025-01-13 16:42:53,395 [INFO] Step[2700/2713]: training loss : 0.9333571588993073 TRAIN  loss dict:  {'classification_loss': 0.9333571588993073}
2025-01-13 16:44:36,756 [INFO] Label accuracies statistics:
2025-01-13 16:44:36,756 [INFO] {0: 0.3333333333333333, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.5, 17: 0.5, 18: 0.75, 19: 0.5, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.75, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.75, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 0.75, 62: 0.75, 63: 0.5, 64: 1.0, 65: 1.0, 66: 0.75, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.75, 84: 0.75, 85: 0.5, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 0.75, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.75, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 0.75, 155: 1.0, 156: 0.75, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.5, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 1.0, 206: 0.75, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 1.0, 213: 0.5, 214: 0.75, 215: 0.75, 216: 0.5, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 1.0, 227: 0.75, 228: 1.0, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 1.0, 240: 0.75, 241: 1.0, 242: 0.75, 243: 0.5, 244: 1.0, 245: 1.0, 246: 1.0, 247: 1.0, 248: 1.0, 249: 1.0, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 1.0, 261: 0.75, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 1.0, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 0.75, 286: 0.75, 287: 1.0, 288: 0.75, 289: 1.0, 290: 0.75, 291: 1.0, 292: 1.0, 293: 0.75, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.5, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 0.75, 308: 1.0, 309: 0.75, 310: 0.75, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 0.75, 320: 1.0, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 0.75, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.5, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.75, 357: 1.0, 358: 1.0, 359: 0.75, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 0.75, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.5, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.25, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 16:44:36,760 [INFO] [121] TRAIN  loss: 0.9326362664568613 acc: 1.0
2025-01-13 16:44:36,760 [INFO] [121] TRAIN  loss dict: {'classification_loss': 0.9326362664568613}
2025-01-13 16:44:36,760 [INFO] [121] VALIDATION loss: 1.6878230432818706 VALIDATION acc: 0.8413793103448276
2025-01-13 16:44:36,760 [INFO] [121] VALIDATION loss dict: {'classification_loss': 1.6878230432818706}
2025-01-13 16:44:36,761 [INFO] 
2025-01-13 16:44:58,959 [INFO] Step[50/2713]: training loss : 0.9320297801494598 TRAIN  loss dict:  {'classification_loss': 0.9320297801494598}
2025-01-13 16:45:12,175 [INFO] Step[100/2713]: training loss : 0.9324489247798919 TRAIN  loss dict:  {'classification_loss': 0.9324489247798919}
2025-01-13 16:45:25,507 [INFO] Step[150/2713]: training loss : 0.9318951082229614 TRAIN  loss dict:  {'classification_loss': 0.9318951082229614}
2025-01-13 16:45:38,780 [INFO] Step[200/2713]: training loss : 0.932440881729126 TRAIN  loss dict:  {'classification_loss': 0.932440881729126}
2025-01-13 16:45:52,027 [INFO] Step[250/2713]: training loss : 0.9317592811584473 TRAIN  loss dict:  {'classification_loss': 0.9317592811584473}
2025-01-13 16:46:05,446 [INFO] Step[300/2713]: training loss : 0.9328084206581115 TRAIN  loss dict:  {'classification_loss': 0.9328084206581115}
2025-01-13 16:46:18,686 [INFO] Step[350/2713]: training loss : 0.9325470507144928 TRAIN  loss dict:  {'classification_loss': 0.9325470507144928}
2025-01-13 16:46:32,074 [INFO] Step[400/2713]: training loss : 0.9319627952575683 TRAIN  loss dict:  {'classification_loss': 0.9319627952575683}
2025-01-13 16:46:45,276 [INFO] Step[450/2713]: training loss : 0.9324756300449372 TRAIN  loss dict:  {'classification_loss': 0.9324756300449372}
2025-01-13 16:46:58,027 [INFO] Step[500/2713]: training loss : 0.9319390642642975 TRAIN  loss dict:  {'classification_loss': 0.9319390642642975}
2025-01-13 16:47:10,763 [INFO] Step[550/2713]: training loss : 0.9324907410144806 TRAIN  loss dict:  {'classification_loss': 0.9324907410144806}
2025-01-13 16:47:23,465 [INFO] Step[600/2713]: training loss : 0.9351781761646271 TRAIN  loss dict:  {'classification_loss': 0.9351781761646271}
2025-01-13 16:47:36,635 [INFO] Step[650/2713]: training loss : 0.9334834098815918 TRAIN  loss dict:  {'classification_loss': 0.9334834098815918}
2025-01-13 16:47:49,814 [INFO] Step[700/2713]: training loss : 0.9317975628376007 TRAIN  loss dict:  {'classification_loss': 0.9317975628376007}
2025-01-13 16:48:02,851 [INFO] Step[750/2713]: training loss : 0.9333526241779327 TRAIN  loss dict:  {'classification_loss': 0.9333526241779327}
2025-01-13 16:48:15,434 [INFO] Step[800/2713]: training loss : 0.9328266179561615 TRAIN  loss dict:  {'classification_loss': 0.9328266179561615}
2025-01-13 16:48:27,982 [INFO] Step[850/2713]: training loss : 0.9331092941761017 TRAIN  loss dict:  {'classification_loss': 0.9331092941761017}
2025-01-13 16:48:40,889 [INFO] Step[900/2713]: training loss : 0.9419497740268707 TRAIN  loss dict:  {'classification_loss': 0.9419497740268707}
2025-01-13 16:48:53,828 [INFO] Step[950/2713]: training loss : 0.931663613319397 TRAIN  loss dict:  {'classification_loss': 0.931663613319397}
2025-01-13 16:49:06,834 [INFO] Step[1000/2713]: training loss : 0.9319861042499542 TRAIN  loss dict:  {'classification_loss': 0.9319861042499542}
2025-01-13 16:49:19,616 [INFO] Step[1050/2713]: training loss : 0.9319215369224548 TRAIN  loss dict:  {'classification_loss': 0.9319215369224548}
2025-01-13 16:49:32,590 [INFO] Step[1100/2713]: training loss : 0.9332614886760712 TRAIN  loss dict:  {'classification_loss': 0.9332614886760712}
2025-01-13 16:49:45,176 [INFO] Step[1150/2713]: training loss : 0.9327810144424439 TRAIN  loss dict:  {'classification_loss': 0.9327810144424439}
2025-01-13 16:49:58,298 [INFO] Step[1200/2713]: training loss : 0.9326880037784576 TRAIN  loss dict:  {'classification_loss': 0.9326880037784576}
2025-01-13 16:50:11,285 [INFO] Step[1250/2713]: training loss : 0.9326497006416321 TRAIN  loss dict:  {'classification_loss': 0.9326497006416321}
2025-01-13 16:50:23,850 [INFO] Step[1300/2713]: training loss : 0.9325589859485626 TRAIN  loss dict:  {'classification_loss': 0.9325589859485626}
2025-01-13 16:50:37,199 [INFO] Step[1350/2713]: training loss : 0.9324859046936035 TRAIN  loss dict:  {'classification_loss': 0.9324859046936035}
2025-01-13 16:50:49,936 [INFO] Step[1400/2713]: training loss : 0.9324013829231262 TRAIN  loss dict:  {'classification_loss': 0.9324013829231262}
2025-01-13 16:51:02,606 [INFO] Step[1450/2713]: training loss : 0.932301459312439 TRAIN  loss dict:  {'classification_loss': 0.932301459312439}
2025-01-13 16:51:15,755 [INFO] Step[1500/2713]: training loss : 0.933107568025589 TRAIN  loss dict:  {'classification_loss': 0.933107568025589}
2025-01-13 16:51:28,571 [INFO] Step[1550/2713]: training loss : 0.932107322216034 TRAIN  loss dict:  {'classification_loss': 0.932107322216034}
2025-01-13 16:51:41,245 [INFO] Step[1600/2713]: training loss : 0.9327488362789154 TRAIN  loss dict:  {'classification_loss': 0.9327488362789154}
2025-01-13 16:51:54,299 [INFO] Step[1650/2713]: training loss : 0.9328083169460296 TRAIN  loss dict:  {'classification_loss': 0.9328083169460296}
2025-01-13 16:52:07,121 [INFO] Step[1700/2713]: training loss : 0.93198357462883 TRAIN  loss dict:  {'classification_loss': 0.93198357462883}
2025-01-13 16:52:20,161 [INFO] Step[1750/2713]: training loss : 0.9320020532608032 TRAIN  loss dict:  {'classification_loss': 0.9320020532608032}
2025-01-13 16:52:33,024 [INFO] Step[1800/2713]: training loss : 0.9320438086986542 TRAIN  loss dict:  {'classification_loss': 0.9320438086986542}
2025-01-13 16:52:46,010 [INFO] Step[1850/2713]: training loss : 0.9359876084327697 TRAIN  loss dict:  {'classification_loss': 0.9359876084327697}
2025-01-13 16:52:59,185 [INFO] Step[1900/2713]: training loss : 0.9329376637935638 TRAIN  loss dict:  {'classification_loss': 0.9329376637935638}
2025-01-13 16:53:11,764 [INFO] Step[1950/2713]: training loss : 0.9314571928977966 TRAIN  loss dict:  {'classification_loss': 0.9314571928977966}
2025-01-13 16:53:24,283 [INFO] Step[2000/2713]: training loss : 0.9319176959991455 TRAIN  loss dict:  {'classification_loss': 0.9319176959991455}
2025-01-13 16:53:37,473 [INFO] Step[2050/2713]: training loss : 0.932092010974884 TRAIN  loss dict:  {'classification_loss': 0.932092010974884}
2025-01-13 16:53:50,389 [INFO] Step[2100/2713]: training loss : 0.9313472723960876 TRAIN  loss dict:  {'classification_loss': 0.9313472723960876}
2025-01-13 16:54:05,344 [INFO] Step[2150/2713]: training loss : 0.9326899027824402 TRAIN  loss dict:  {'classification_loss': 0.9326899027824402}
2025-01-13 16:54:19,545 [INFO] Step[2200/2713]: training loss : 0.9328562021255493 TRAIN  loss dict:  {'classification_loss': 0.9328562021255493}
2025-01-13 16:54:32,192 [INFO] Step[2250/2713]: training loss : 0.9337227940559387 TRAIN  loss dict:  {'classification_loss': 0.9337227940559387}
2025-01-13 16:54:44,737 [INFO] Step[2300/2713]: training loss : 0.9331123948097229 TRAIN  loss dict:  {'classification_loss': 0.9331123948097229}
2025-01-13 16:54:57,941 [INFO] Step[2350/2713]: training loss : 0.933037406206131 TRAIN  loss dict:  {'classification_loss': 0.933037406206131}
2025-01-13 16:55:10,722 [INFO] Step[2400/2713]: training loss : 0.9311084926128388 TRAIN  loss dict:  {'classification_loss': 0.9311084926128388}
2025-01-13 16:55:23,604 [INFO] Step[2450/2713]: training loss : 0.9400468873977661 TRAIN  loss dict:  {'classification_loss': 0.9400468873977661}
2025-01-13 16:55:36,358 [INFO] Step[2500/2713]: training loss : 0.935856351852417 TRAIN  loss dict:  {'classification_loss': 0.935856351852417}
2025-01-13 16:55:49,386 [INFO] Step[2550/2713]: training loss : 0.9340197789669037 TRAIN  loss dict:  {'classification_loss': 0.9340197789669037}
2025-01-13 16:56:02,200 [INFO] Step[2600/2713]: training loss : 0.9321952545642853 TRAIN  loss dict:  {'classification_loss': 0.9321952545642853}
2025-01-13 16:56:14,794 [INFO] Step[2650/2713]: training loss : 0.9327797996997833 TRAIN  loss dict:  {'classification_loss': 0.9327797996997833}
2025-01-13 16:56:27,707 [INFO] Step[2700/2713]: training loss : 0.9324935293197631 TRAIN  loss dict:  {'classification_loss': 0.9324935293197631}
2025-01-13 16:58:10,866 [INFO] Label accuracies statistics:
2025-01-13 16:58:10,866 [INFO] {0: 0.0, 1: 1.0, 2: 0.75, 3: 0.75, 4: 0.25, 5: 1.0, 6: 0.75, 7: 0.5, 8: 0.5, 9: 0.75, 10: 1.0, 11: 0.75, 12: 0.5, 13: 0.5, 14: 0.75, 15: 0.6666666666666666, 16: 0.75, 17: 0.5, 18: 0.75, 19: 0.75, 20: 0.75, 21: 0.75, 22: 0.75, 23: 1.0, 24: 1.0, 25: 0.75, 26: 1.0, 27: 0.75, 28: 1.0, 29: 1.0, 30: 0.75, 31: 1.0, 32: 0.75, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.75, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 0.75, 43: 1.0, 44: 0.5, 45: 0.75, 46: 1.0, 47: 1.0, 48: 1.0, 49: 0.5, 50: 0.75, 51: 0.75, 52: 1.0, 53: 0.75, 54: 0.5, 55: 0.75, 56: 0.75, 57: 0.75, 58: 1.0, 59: 0.75, 60: 0.75, 61: 1.0, 62: 0.75, 63: 0.75, 64: 1.0, 65: 1.0, 66: 0.5, 67: 1.0, 68: 0.5, 69: 0.75, 70: 0.75, 71: 0.5, 72: 1.0, 73: 1.0, 74: 0.5, 75: 1.0, 76: 0.75, 77: 0.75, 78: 1.0, 79: 0.75, 80: 1.0, 81: 1.0, 82: 0.75, 83: 0.5, 84: 0.75, 85: 0.25, 86: 0.75, 87: 0.75, 88: 0.75, 89: 0.5, 90: 0.5, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.5, 95: 1.0, 96: 0.5, 97: 0.5, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 1.0, 104: 1.0, 105: 1.0, 106: 1.0, 107: 0.5, 108: 1.0, 109: 1.0, 110: 1.0, 111: 1.0, 112: 1.0, 113: 0.5, 114: 0.5, 115: 1.0, 116: 1.0, 117: 1.0, 118: 1.0, 119: 1.0, 120: 0.75, 121: 0.75, 122: 0.5, 123: 1.0, 124: 1.0, 125: 1.0, 126: 1.0, 127: 1.0, 128: 1.0, 129: 1.0, 130: 1.0, 131: 1.0, 132: 0.75, 133: 1.0, 134: 1.0, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 1.0, 143: 1.0, 144: 1.0, 145: 1.0, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.5, 151: 1.0, 152: 1.0, 153: 0.75, 154: 1.0, 155: 1.0, 156: 1.0, 157: 1.0, 158: 0.6666666666666666, 159: 1.0, 160: 0.25, 161: 1.0, 162: 1.0, 163: 1.0, 164: 0.75, 165: 1.0, 166: 1.0, 167: 1.0, 168: 1.0, 169: 0.75, 170: 1.0, 171: 0.75, 172: 0.75, 173: 1.0, 174: 1.0, 175: 0.75, 176: 1.0, 177: 1.0, 178: 1.0, 179: 0.0, 180: 0.75, 181: 1.0, 182: 0.75, 183: 1.0, 184: 1.0, 185: 1.0, 186: 0.75, 187: 1.0, 188: 0.5, 189: 0.75, 190: 1.0, 191: 0.5, 192: 1.0, 193: 1.0, 194: 1.0, 195: 1.0, 196: 1.0, 197: 1.0, 198: 0.5, 199: 0.75, 200: 0.5, 201: 0.75, 202: 1.0, 203: 0.5, 204: 0.75, 205: 0.75, 206: 0.5, 207: 0.75, 208: 0.75, 209: 1.0, 210: 0.75, 211: 0.25, 212: 0.5, 213: 0.75, 214: 0.75, 215: 0.75, 216: 0.25, 217: 0.75, 218: 1.0, 219: 0.75, 220: 0.75, 221: 1.0, 222: 0.75, 223: 0.75, 224: 0.5, 225: 0.75, 226: 0.75, 227: 0.75, 228: 0.75, 229: 0.75, 230: 0.25, 231: 0.5, 232: 0.5, 233: 0.75, 234: 1.0, 235: 0.75, 236: 0.75, 237: 1.0, 238: 0.75, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.75, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 1.0, 248: 1.0, 249: 0.75, 250: 1.0, 251: 0.75, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.5, 259: 1.0, 260: 0.75, 261: 0.5, 262: 1.0, 263: 1.0, 264: 1.0, 265: 1.0, 266: 1.0, 267: 0.5, 268: 0.25, 269: 1.0, 270: 1.0, 271: 0.75, 272: 1.0, 273: 0.75, 274: 0.75, 275: 0.75, 276: 0.75, 277: 1.0, 278: 0.75, 279: 1.0, 280: 1.0, 281: 1.0, 282: 0.75, 283: 1.0, 284: 0.75, 285: 1.0, 286: 1.0, 287: 0.75, 288: 0.75, 289: 1.0, 290: 1.0, 291: 1.0, 292: 1.0, 293: 1.0, 294: 1.0, 295: 1.0, 296: 0.75, 297: 0.75, 298: 0.75, 299: 1.0, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.25, 305: 1.0, 306: 1.0, 307: 1.0, 308: 1.0, 309: 0.75, 310: 0.5, 311: 0.75, 312: 1.0, 313: 1.0, 314: 0.75, 315: 0.75, 316: 0.75, 317: 0.75, 318: 0.75, 319: 0.75, 320: 0.75, 321: 1.0, 322: 1.0, 323: 0.75, 324: 1.0, 325: 0.75, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 0.75, 331: 1.0, 332: 1.0, 333: 0.75, 334: 0.75, 335: 1.0, 336: 1.0, 337: 0.75, 338: 1.0, 339: 0.75, 340: 1.0, 341: 1.0, 342: 1.0, 343: 1.0, 344: 0.5, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 0.75, 352: 0.75, 353: 0.5, 354: 0.75, 355: 0.75, 356: 0.5, 357: 1.0, 358: 1.0, 359: 1.0, 360: 1.0, 361: 0.75, 362: 0.75, 363: 0.75, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 1.0, 369: 1.0, 370: 0.75, 371: 0.75, 372: 1.0, 373: 0.75, 374: 1.0, 375: 0.75, 376: 1.0, 377: 1.0, 378: 0.75, 379: 1.0, 380: 1.0, 381: 0.25, 382: 1.0, 383: 1.0, 384: 0.75, 385: 1.0, 386: 1.0, 387: 0.75, 388: 1.0, 389: 0.75, 390: 0.75, 391: 1.0, 392: 1.0, 393: 0.75, 394: 0.75, 395: 0.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 0.75}

2025-01-13 16:58:10,871 [INFO] [122] TRAIN  loss: 0.9329518959656704 acc: 0.9998771347831429
2025-01-13 16:58:10,871 [INFO] [122] TRAIN  loss dict: {'classification_loss': 0.9329518959656704}
2025-01-13 16:58:10,871 [INFO] [122] VALIDATION loss: 1.6949230657708376 VALIDATION acc: 0.8363636363636363
2025-01-13 16:58:10,871 [INFO] [122] VALIDATION loss dict: {'classification_loss': 1.6949230657708376}
2025-01-13 16:58:10,871 [INFO] 
2025-01-13 16:58:10,872 [INFO] 

***Stop training***


2025-01-13 16:58:10,872 [INFO] 
Testing checkpointed models starting...

2025-01-13 16:58:10,872 [INFO] 
Evaluating checkpoint with best validation loss...

2025-01-13 16:59:43,106 [INFO] Label accuracies statistics:
2025-01-13 16:59:43,106 [INFO] {0: 0.3333333333333333, 1: 0.75, 2: 1.0, 3: 0.75, 4: 0.5, 5: 0.5, 6: 0.75, 7: 0.5, 8: 0.75, 9: 1.0, 10: 1.0, 11: 1.0, 12: 1.0, 13: 0.75, 14: 1.0, 15: 1.0, 16: 1.0, 17: 0.6666666666666666, 18: 0.5, 19: 1.0, 20: 0.75, 21: 0.5, 22: 1.0, 23: 0.75, 24: 1.0, 25: 1.0, 26: 0.75, 27: 1.0, 28: 1.0, 29: 0.75, 30: 1.0, 31: 0.75, 32: 1.0, 33: 0.75, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.5, 42: 0.75, 43: 1.0, 44: 1.0, 45: 1.0, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 1.0, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.75, 55: 1.0, 56: 0.5, 57: 0.75, 58: 0.75, 59: 1.0, 60: 1.0, 61: 1.0, 62: 1.0, 63: 1.0, 64: 0.25, 65: 1.0, 66: 1.0, 67: 0.75, 68: 0.75, 69: 1.0, 70: 0.75, 71: 1.0, 72: 1.0, 73: 1.0, 74: 0.75, 75: 0.75, 76: 1.0, 77: 1.0, 78: 1.0, 79: 1.0, 80: 1.0, 81: 1.0, 82: 0.5, 83: 0.75, 84: 1.0, 85: 0.75, 86: 0.75, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 1.0, 95: 0.75, 96: 0.75, 97: 1.0, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 0.75, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 1.0, 113: 0.0, 114: 0.5, 115: 0.75, 116: 1.0, 117: 0.75, 118: 0.75, 119: 0.75, 120: 1.0, 121: 1.0, 122: 1.0, 123: 0.75, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.5, 130: 0.75, 131: 1.0, 132: 0.25, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 1.0, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.5, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 0.75, 149: 1.0, 150: 1.0, 151: 1.0, 152: 1.0, 153: 1.0, 154: 1.0, 155: 1.0, 156: 0.5, 157: 0.5, 158: 1.0, 159: 0.5, 160: 1.0, 161: 0.75, 162: 0.75, 163: 0.75, 164: 0.75, 165: 0.5, 166: 0.5, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.25, 172: 1.0, 173: 0.75, 174: 1.0, 175: 0.5, 176: 0.75, 177: 0.75, 178: 1.0, 179: 0.75, 180: 1.0, 181: 1.0, 182: 0.75, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 0.75, 188: 0.75, 189: 0.75, 190: 1.0, 191: 1.0, 192: 1.0, 193: 0.75, 194: 1.0, 195: 0.75, 196: 1.0, 197: 0.75, 198: 1.0, 199: 1.0, 200: 0.25, 201: 1.0, 202: 0.75, 203: 0.75, 204: 0.6666666666666666, 205: 1.0, 206: 0.75, 207: 1.0, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 0.75, 214: 1.0, 215: 0.75, 216: 0.5, 217: 0.75, 218: 0.75, 219: 1.0, 220: 1.0, 221: 1.0, 222: 1.0, 223: 0.75, 224: 0.75, 225: 1.0, 226: 1.0, 227: 1.0, 228: 0.5, 229: 0.5, 230: 0.75, 231: 1.0, 232: 0.5, 233: 1.0, 234: 1.0, 235: 1.0, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.75, 240: 0.75, 241: 1.0, 242: 0.5, 243: 1.0, 244: 0.6666666666666666, 245: 0.75, 246: 1.0, 247: 0.5, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 0.75, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 1.0, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 0.75, 268: 1.0, 269: 0.75, 270: 1.0, 271: 0.75, 272: 1.0, 273: 1.0, 274: 1.0, 275: 1.0, 276: 0.5, 277: 1.0, 278: 0.75, 279: 0.75, 280: 1.0, 281: 1.0, 282: 1.0, 283: 0.75, 284: 1.0, 285: 0.75, 286: 1.0, 287: 1.0, 288: 1.0, 289: 0.75, 290: 1.0, 291: 1.0, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.5, 296: 0.75, 297: 0.75, 298: 0.75, 299: 0.5, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.75, 305: 1.0, 306: 1.0, 307: 1.0, 308: 0.75, 309: 1.0, 310: 1.0, 311: 0.75, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.5, 316: 1.0, 317: 1.0, 318: 0.75, 319: 1.0, 320: 0.75, 321: 1.0, 322: 1.0, 323: 1.0, 324: 1.0, 325: 1.0, 326: 1.0, 327: 0.75, 328: 0.25, 329: 1.0, 330: 1.0, 331: 0.75, 332: 1.0, 333: 0.5, 334: 0.6666666666666666, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 1.0, 340: 0.75, 341: 0.25, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 0.75, 348: 1.0, 349: 0.75, 350: 1.0, 351: 1.0, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.75, 356: 0.75, 357: 1.0, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 1.0, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.5, 371: 0.5, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.5, 376: 0.6666666666666666, 377: 1.0, 378: 1.0, 379: 1.0, 380: 1.0, 381: 1.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 0.75, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.0, 391: 1.0, 392: 0.75, 393: 0.75, 394: 0.75, 395: 1.0, 396: 0.25, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-13 16:59:43,108 [INFO] 
Testing accuracy (Best Loss Checkpoint): 0.8487341772151898
2025-01-13 16:59:43,108 [INFO] 
Evaluating checkpoint with best validation accuracy...

2025-01-13 17:01:15,635 [INFO] Label accuracies statistics:
2025-01-13 17:01:15,635 [INFO] {0: 0.3333333333333333, 1: 0.75, 2: 1.0, 3: 1.0, 4: 0.5, 5: 0.5, 6: 0.75, 7: 0.75, 8: 0.75, 9: 1.0, 10: 1.0, 11: 0.75, 12: 0.75, 13: 0.75, 14: 1.0, 15: 0.75, 16: 1.0, 17: 0.6666666666666666, 18: 0.5, 19: 1.0, 20: 0.75, 21: 0.5, 22: 1.0, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 1.0, 28: 1.0, 29: 0.75, 30: 1.0, 31: 0.75, 32: 1.0, 33: 1.0, 34: 0.75, 35: 1.0, 36: 0.5, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.75, 42: 1.0, 43: 1.0, 44: 1.0, 45: 1.0, 46: 1.0, 47: 0.75, 48: 1.0, 49: 0.75, 50: 1.0, 51: 1.0, 52: 1.0, 53: 0.5, 54: 0.75, 55: 1.0, 56: 0.5, 57: 1.0, 58: 0.75, 59: 1.0, 60: 1.0, 61: 1.0, 62: 1.0, 63: 1.0, 64: 0.5, 65: 1.0, 66: 1.0, 67: 0.75, 68: 0.75, 69: 1.0, 70: 0.5, 71: 0.75, 72: 1.0, 73: 1.0, 74: 1.0, 75: 0.75, 76: 1.0, 77: 1.0, 78: 1.0, 79: 1.0, 80: 1.0, 81: 0.75, 82: 0.5, 83: 0.5, 84: 1.0, 85: 0.5, 86: 1.0, 87: 1.0, 88: 0.75, 89: 1.0, 90: 0.75, 91: 1.0, 92: 1.0, 93: 1.0, 94: 0.75, 95: 0.75, 96: 1.0, 97: 0.75, 98: 1.0, 99: 1.0, 100: 1.0, 101: 1.0, 102: 1.0, 103: 0.75, 104: 1.0, 105: 0.75, 106: 1.0, 107: 0.5, 108: 1.0, 109: 0.75, 110: 0.75, 111: 1.0, 112: 0.75, 113: 0.0, 114: 0.5, 115: 0.75, 116: 1.0, 117: 0.75, 118: 0.5, 119: 0.75, 120: 1.0, 121: 1.0, 122: 1.0, 123: 1.0, 124: 1.0, 125: 0.75, 126: 1.0, 127: 1.0, 128: 1.0, 129: 0.5, 130: 0.75, 131: 1.0, 132: 0.5, 133: 1.0, 134: 0.5, 135: 1.0, 136: 1.0, 137: 1.0, 138: 0.75, 139: 1.0, 140: 1.0, 141: 1.0, 142: 0.75, 143: 1.0, 144: 1.0, 145: 0.75, 146: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 0.75, 151: 1.0, 152: 0.75, 153: 1.0, 154: 1.0, 155: 1.0, 156: 1.0, 157: 0.5, 158: 0.75, 159: 0.75, 160: 1.0, 161: 1.0, 162: 1.0, 163: 0.75, 164: 0.75, 165: 1.0, 166: 0.75, 167: 1.0, 168: 0.75, 169: 0.75, 170: 1.0, 171: 0.25, 172: 1.0, 173: 0.75, 174: 1.0, 175: 0.75, 176: 1.0, 177: 0.75, 178: 1.0, 179: 0.5, 180: 0.75, 181: 1.0, 182: 0.5, 183: 1.0, 184: 0.75, 185: 1.0, 186: 0.75, 187: 0.75, 188: 0.75, 189: 0.5, 190: 0.75, 191: 1.0, 192: 0.75, 193: 0.75, 194: 1.0, 195: 1.0, 196: 1.0, 197: 0.75, 198: 1.0, 199: 1.0, 200: 0.25, 201: 1.0, 202: 0.75, 203: 0.5, 204: 0.6666666666666666, 205: 1.0, 206: 0.75, 207: 1.0, 208: 1.0, 209: 1.0, 210: 0.75, 211: 0.5, 212: 1.0, 213: 1.0, 214: 1.0, 215: 1.0, 216: 0.5, 217: 0.75, 218: 1.0, 219: 1.0, 220: 1.0, 221: 1.0, 222: 1.0, 223: 0.75, 224: 0.75, 225: 1.0, 226: 0.75, 227: 1.0, 228: 0.5, 229: 0.5, 230: 0.75, 231: 1.0, 232: 0.75, 233: 1.0, 234: 1.0, 235: 1.0, 236: 1.0, 237: 1.0, 238: 1.0, 239: 0.25, 240: 1.0, 241: 1.0, 242: 0.5, 243: 1.0, 244: 1.0, 245: 0.75, 246: 1.0, 247: 0.75, 248: 1.0, 249: 1.0, 250: 0.75, 251: 1.0, 252: 1.0, 253: 1.0, 254: 1.0, 255: 1.0, 256: 0.75, 257: 1.0, 258: 0.75, 259: 1.0, 260: 0.75, 261: 1.0, 262: 1.0, 263: 0.75, 264: 1.0, 265: 1.0, 266: 0.75, 267: 1.0, 268: 0.6666666666666666, 269: 0.75, 270: 1.0, 271: 1.0, 272: 1.0, 273: 1.0, 274: 0.75, 275: 1.0, 276: 0.5, 277: 1.0, 278: 1.0, 279: 0.75, 280: 1.0, 281: 1.0, 282: 1.0, 283: 0.75, 284: 1.0, 285: 0.75, 286: 1.0, 287: 1.0, 288: 0.75, 289: 0.75, 290: 0.75, 291: 1.0, 292: 0.75, 293: 0.75, 294: 1.0, 295: 0.5, 296: 1.0, 297: 0.75, 298: 0.75, 299: 0.5, 300: 1.0, 301: 1.0, 302: 0.75, 303: 1.0, 304: 0.75, 305: 1.0, 306: 0.75, 307: 1.0, 308: 0.5, 309: 1.0, 310: 1.0, 311: 1.0, 312: 1.0, 313: 0.75, 314: 1.0, 315: 0.75, 316: 1.0, 317: 1.0, 318: 0.75, 319: 1.0, 320: 1.0, 321: 1.0, 322: 1.0, 323: 1.0, 324: 0.75, 325: 1.0, 326: 1.0, 327: 1.0, 328: 0.75, 329: 1.0, 330: 1.0, 331: 1.0, 332: 1.0, 333: 0.5, 334: 1.0, 335: 0.75, 336: 1.0, 337: 0.75, 338: 1.0, 339: 1.0, 340: 0.75, 341: 0.75, 342: 1.0, 343: 1.0, 344: 0.75, 345: 1.0, 346: 0.75, 347: 1.0, 348: 1.0, 349: 1.0, 350: 1.0, 351: 1.0, 352: 0.75, 353: 0.5, 354: 0.25, 355: 0.5, 356: 0.75, 357: 0.75, 358: 0.75, 359: 1.0, 360: 1.0, 361: 0.75, 362: 1.0, 363: 1.0, 364: 0.75, 365: 0.75, 366: 1.0, 367: 1.0, 368: 0.75, 369: 1.0, 370: 0.75, 371: 0.75, 372: 0.75, 373: 1.0, 374: 1.0, 375: 0.75, 376: 0.6666666666666666, 377: 1.0, 378: 1.0, 379: 1.0, 380: 1.0, 381: 1.0, 382: 1.0, 383: 1.0, 384: 1.0, 385: 1.0, 386: 1.0, 387: 0.75, 388: 0.75, 389: 0.75, 390: 0.25, 391: 1.0, 392: 0.75, 393: 0.5, 394: 0.5, 395: 1.0, 396: 0.5, 397: 1.0, 398: 1.0, 399: 1.0}

2025-01-13 17:01:15,637 [INFO] 
Testing accuracy (Best Acc Checkpoint): 0.8607594936708861
